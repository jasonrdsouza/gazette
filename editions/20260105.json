{
    "articles": [
        {
            "content": [
                "<div class=\"trix-content\">\n  <div>My folks are in town visiting us for a couple months so we rented them a house nearby.<br /><br />It\u2019s new construction. No one has lived in it yet. It\u2019s amped up with state of the art systems. You know, the ones with touchscreens of various sizes, IoT appliances, and interfaces that try too hard.<br /><br />And it\u2019s terrible. What a regression.<br /><br />The lights are powered by Control4. And require a demo to understand how to use the switches, understand which ones control what, and to be sure not to hit THAT ONE because it\u2019ll turn off all the lights in the house when you didn\u2019t mean to. Worse.<br /><br />The TV is the latest Samsung which has a baffling UI just to watch CNN. My parents aren\u2019t idiots, but definitely feel like they\u2019re missing something obvious. They aren\u2019t \u2014 TVs have simply gotten worse. You don\u2019t turn them on anymore, you boot them up.<br /><br />The Miele dishwasher is hidden flush with the counters. That part is fine, but here\u2019s what isn\u2019t: It wouldn\u2019t even operate the first time without connecting it with an app. This meant another call to the house manager to have them install an app they didn\u2019t know they needed either. An app to clean some peanut butter off a plate? For serious? Worse.<br /><br />Thermostats... Nest would have been an upgrade, but these other propriety ones from some other company trying to be nest-like are baffling. Round touchscreens that take you into a dark labyrinth of options just to be sure it\u2019s set at 68. Or is it 68 now? Or is that what we want it at, but it\u2019s at 72? Wait... What? Which number is this? Worse.<br /><br />The alarm system is essentially a 10\u201d iPad bolted to the wall that has the fucking weather forecast on it. And it\u2019s bright! I\u2019m sure there\u2019s a way to turn that off, but then the screen would be so barren that it would be filled with the news instead. Why can\u2019t the alarm panel just be an alarm panel? Worse.<br /><br />And the lag. Lag everywhere. Everything feels a beat or two behind. Everything. Lag is the giveaway that the system is working too hard for too little. Real-time must be the hardest problem.<br /><br />Now look... I\u2019m no luddite. But this experience is close to conversion therapy. Tech can make things better, but I simply can\u2019t see in these cases. I\u2019ve heard the pitches too \u2014 you can set up scenes and one button can change EVERYTHING. Not buying it. It actually feels primitive, like we haven\u2019t figured out how to make things easy yet. That some breakthrough will eventually come when you can simply knock a switch up or down and it\u2019ll all makes sense. But we haven\u2019t evolved to that point yet.<br /><br />It\u2019s really the contrast that makes it alarming. We just got back from a vacation in Montana. Rented a house there. They did have a fancy TV \u2014 seems those can\u2019t be avoided these days \u2014 but everything else was old school and clear. Physical up/down light switches in the right places. Appliances without the internet. Buttons with depth and physically-confirmed state change rather than surfaces that don\u2019t obviously register your choice. More traditional round rotating Honeywell thermostats that are just clear and obvious. No tours, no instructions, no questions, no fearing you\u2019re going to do something wrong, no wondering how something works. Useful and universally clear. That\u2019s human that\u2019s modern.<br /><br /></div><div>-Jason</div>\n</div>"
            ],
            "link": "https://world.hey.com/jason/the-big-regression-da7fc60d",
            "publishedAt": "2026-01-05",
            "source": "Jason Fried",
            "summary": "<div class=\"trix-content\"> <div>My folks are in town visiting us for a couple months so we rented them a house nearby.<br /><br />It\u2019s new construction. No one has lived in it yet. It\u2019s amped up with state of the art systems. You know, the ones with touchscreens of various sizes, IoT appliances, and interfaces that try too hard.<br /><br />And it\u2019s terrible. What a regression.<br /><br />The lights are powered by Control4. And require a demo to understand how to use the switches, understand which ones control what, and to be sure not to hit THAT ONE because it\u2019ll turn off all the lights in the house when you didn\u2019t mean to. Worse.<br /><br />The TV is the latest Samsung which has a baffling UI just to watch CNN. My parents aren\u2019t idiots, but definitely feel like they\u2019re missing something obvious. They aren\u2019t \u2014 TVs have simply gotten worse. You don\u2019t turn them on anymore, you boot them up.<br /><br />The Miele dishwasher is hidden flush with the counters. That part is fine, but here\u2019s what isn\u2019t: It wouldn\u2019t even operate the first time without connecting it with an app. This meant another call to the house manager to have them",
            "title": "The big regression"
        },
        {
            "content": [
                "<p>I was reading Macintosh Human Interface Guidelines <a href=\"https://dn721903.ca.archive.org/0/items/apple-hig/Macintosh_HIG_1992.pdf\">from 1992</a> and found this nice illustration:</p>\n<figure>\n<img class=\"\" src=\"https://tonsky.me/blog/tahoe-icons/hig_icons@2x.webp\" /></figure>\n<p>accompanied by explanation:</p>\n<figure>\n<img class=\"\" src=\"https://tonsky.me/blog/tahoe-icons/hig_quote@2x.webp\" /></figure>\n<p>Fast forward to 2025. Apple releases macOS Tahoe. Main attraction? Adding unpleasant, distracting, illegible, messy, cluttered, confusing, frustrating icons (their words, not mine!) to every menu item:</p>\n<figure>\n<img class=\"\" src=\"https://tonsky.me/blog/tahoe-icons/sequoia_tahoe_textedit@2x.webp\" /><figcaption>Sequoia \u2192 Tahoe</figcaption></figure>\n<p>It\u2019s bad. But why exactly is it bad? Let\u2019s delve into it!</p>\n<p>Disclaimer: screenshots are a mix from macOS 26.1 and 26.2, taken   from stock Apple apps only that come pre-installed with the system. No system settings were modified.</p>\n<h1 id=\"icons-should-differentiate\">Icons should differentiate</h1>\n<p>The main function of an icon is to help you find what you are looking for faster.</p>\n<p>Perhaps counter-intuitively, adding an icon to everything is exactly the wrong thing to do. To stand out, things need to be different. But if everything has an icon, nothing stands out.</p>\n<p>The same applies to color: black-and-white icons look clean, but they don\u2019t help you find things faster!</p>\n<p>Microsoft used to know this:</p>\n<figure>\n<a href=\"https://tonsky.me\"><img class=\"noround\" src=\"https://tonsky.me/blog/tahoe-icons/word@2x.webp\" /></a></figure>\n<p>Look how much faster you can find Save or Share in the right variant:</p>\n<figure>\n<img class=\"\" src=\"https://tonsky.me/blog/tahoe-icons/menu_cleanup@2x.webp\" /></figure>\n<p>It also looks cleaner. Less cluttered.</p>\n<p>A colored version would be even better (clearer separation of text from icon, faster to find):</p>\n<figure>\n<img class=\"\" src=\"https://tonsky.me/blog/tahoe-icons/menu_cleanup_color@2x.webp\" /></figure>\n<p>I know you won\u2019t like how it looks. I don\u2019t like it either. These icons are hard to work with. You\u2019ll have to actually design for color to look nice. But the principle stands: it is way easier to use.</p>\n<h1 id=\"consistency-between-apps\">Consistency between apps</h1>\n<p>If you want icons to work, they need to be <em>consistent</em>. I need to be able to learn what to look for.</p>\n<p>For example, I see a \u201cCut\u201d command and <img class=\"inline\" src=\"https://tonsky.me/blog/tahoe-icons/scissors.svg\" /> next to it. Okay, I think. Next time I\u2019m looking for \u201cCut,\u201d I might save some time and start looking for <img class=\"inline\" src=\"https://tonsky.me/blog/tahoe-icons/scissors.svg\" /> instead.</p>\n<p>How is Tahoe doing on that front? I present to you: Fifty Shades of \u201cNew\u201d:</p>\n<figure>\n<img class=\"\" src=\"https://tonsky.me/blog/tahoe-icons/menu_new@2x.webp\" /></figure>\n<p>I even collected them all together, so the absurdity of the situation is more obvious.</p>\n<figure>\n<img class=\"\" src=\"https://tonsky.me/blog/tahoe-icons/icons_new@2x.webp\" /></figure>\n<p>Granted, some of them are different operations, so they have different icons. I guess creating a smart folder is different from creating a journal entry. But this?</p>\n<figure>\n<img class=\"\" src=\"https://tonsky.me/blog/tahoe-icons/menu_new_object@2x.webp\" /></figure>\n<p>Or this:</p>\n<figure>\n<img class=\"\" src=\"https://tonsky.me/blog/tahoe-icons/icons_new_smart_folder@2x.webp\" /></figure>\n<p>Or this:</p>\n<figure>\n<img class=\"\" src=\"https://tonsky.me/blog/tahoe-icons/icons_new_window@2x.webp\" /></figure>\n<p>There is no excuse.</p>\n<p>Same deal with open:</p>\n<figure>\n<img class=\"\" src=\"https://tonsky.me/blog/tahoe-icons/menu_open@2x.webp\" /></figure>\n<p>Save:</p>\n<figure>\n<img class=\"\" src=\"https://tonsky.me/blog/tahoe-icons/menu_save@2x.webp\" /></figure>\n<p>Yes. One of them is a checkmark. And they can\u2019t even agree on the direction of an arrow!</p>\n<p>Close:</p>\n<figure>\n<img class=\"\" src=\"https://tonsky.me/blog/tahoe-icons/menu_close@2x.webp\" /></figure>\n<p>Find (which is sometimes called Search, and sometimes Filter):</p>\n<figure>\n<img class=\"\" src=\"https://tonsky.me/blog/tahoe-icons/menu_find@2x.webp\" /></figure>\n<p>Delete (from Cut-Copy-Paste-Delete fame):</p>\n<figure>\n<img class=\"\" src=\"https://tonsky.me/blog/tahoe-icons/menu_delete@2x.webp\" /></figure>\n<p>Minimize window.</p>\n<figure>\n<img class=\"\" src=\"https://tonsky.me/blog/tahoe-icons/menu_minimize@2x.webp\" /></figure>\n<p>These are not some obscure, unique operations. These are OS basics, these are foundational. Every app has them, and they are always in the same place. They shouldn\u2019t look different!</p>\n<h1 id=\"consistency-inside-the-same-app\">Consistency inside the same app</h1>\n<p>Icons are also used in toolbars. Conceptually, operations in a toolbar are identical to operations called through the menu, and thus should use the same icons. That\u2019s the simplest case to implement: inside the same app, often on the same screen. How hard can it be to stay consistent?</p>\n<p>Preview:</p>\n<figure>\n<img class=\"\" src=\"https://tonsky.me/blog/tahoe-icons/preview@2x.webp\" /></figure>\n<p>Photos: same <img class=\"inline\" src=\"https://tonsky.me/blog/tahoe-icons/info_circle.svg\" /> and <img class=\"inline\" src=\"https://tonsky.me/blog/tahoe-icons/info.svg\" /> mismatch, but reversed \u00af\\_(\u30c4)_/\u00af</p>\n<figure>\n<img class=\"\" src=\"https://tonsky.me/blog/tahoe-icons/photos@2x.webp\" /></figure>\n<p>Maps and others often use different symbols for zoom:</p>\n<figure>\n<img class=\"\" src=\"https://tonsky.me/blog/tahoe-icons/consistency_maps@2x.webp\" /></figure>\n<h1 id=\"icon-reuse\">Icon reuse</h1>\n<p>Another cardinal sin is to use the same icon for different actions. Imagine: I have learned that <img class=\"inline\" src=\"https://tonsky.me/blog/tahoe-icons/square_and_pencil.svg\" /> means \u201cNew\u201d:</p>\n<figure>\n<img class=\"\" src=\"https://tonsky.me/blog/tahoe-icons/new_note@2x.webp\" /></figure>\n<p>Then I open an app and see<img class=\"inline\" src=\"https://tonsky.me/blog/tahoe-icons/square_and_pencil.svg\" />. \u201cCool\u201d, I think, \u201cI already know what it means\u201d:</p>\n<figure>\n<img class=\"\" src=\"https://tonsky.me/blog/tahoe-icons/edit_address@2x.webp\" /></figure>\n<p>Gotcha!</p>\n<p>You\u2019d think: okay, <img class=\"inline\" src=\"https://tonsky.me/blog/tahoe-icons/eye.svg\" /> means quick look:</p>\n<figure>\n<img class=\"\" src=\"https://tonsky.me/blog/tahoe-icons/quick_look@2x.webp\" /></figure>\n<p>Sometimes, sure. Some other times, <img class=\"inline\" src=\"https://tonsky.me/blog/tahoe-icons/eye.svg\" /> means \u201cShow completed\u201d:</p>\n<figure>\n<img class=\"\" src=\"https://tonsky.me/blog/tahoe-icons/show_completed@2x.webp\" /></figure>\n<p>Sometimes <img class=\"inline\" src=\"https://tonsky.me/blog/tahoe-icons/square_and_arrow_down.svg\" /> is \u201cImport\u201d:</p>\n<figure>\n<img class=\"\" src=\"https://tonsky.me/blog/tahoe-icons/import@2x.webp\" /></figure>\n<p>Sometimes <img class=\"inline\" src=\"https://tonsky.me/blog/tahoe-icons/square_and_arrow_down.svg\" /> is \u201cUpdates\u201d:</p>\n<figure>\n<img class=\"\" src=\"https://tonsky.me/blog/tahoe-icons/update@2x.webp\" /></figure>\n<p>Same as with consistency, icon reuse doesn\u2019t only happen between apps. Sometimes you see <img class=\"inline\" src=\"https://tonsky.me/blog/tahoe-icons/rectangle_pencil_ellipsis.svg\" /> in a toolbar:</p>\n<figure>\n<img class=\"\" src=\"https://tonsky.me/blog/tahoe-icons/form_filling_toolbar@2x.webp\" /></figure>\n<p>Then go to the menu <em>in the same app</em> and see <img class=\"inline\" src=\"https://tonsky.me/blog/tahoe-icons/rectangle_pencil_ellipsis.svg\" /> means something else:</p>\n<figure>\n<img class=\"\" src=\"https://tonsky.me/blog/tahoe-icons/autofill@2x.webp\" /></figure>\n<p>Sometimes identical icons meet in the same menu.</p>\n<figure>\n<img class=\"\" src=\"https://tonsky.me/blog/tahoe-icons/save_export@2x.webp\" /></figure>\n<p>Sometimes next to each other.</p>\n<figure>\n<img class=\"\" src=\"https://tonsky.me/blog/tahoe-icons/passwords@2x.webp\" /></figure>\n<p>Sometimes they put an entire barrage of identical icons in a row:</p>\n<figure>\n<img class=\"\" src=\"https://tonsky.me/blog/tahoe-icons/photos_export@2x.webp\" /></figure>\n<p>This doesn\u2019t help anyone. No user will find a menu item faster or will understand the function better if all icons are the same.</p>\n<p>The worst case of icon reuse so far has been the Photos app:</p>\n<figure>\n<img class=\"\" src=\"https://tonsky.me/blog/tahoe-icons/photos_copy@2x.webp\" /></figure>\n<p>It feels like the person tasked with choosing a unique icon for every menu item just ran out of ideas.</p>\n<p>Understandable.</p>\n<h1 id=\"too-much-nuance\">Too much nuance</h1>\n<p>When looking at icons, we usually allow for slight differences in execution. That lets us, for example, understand that these <em>technically different</em> road signs mean the same thing:</p>\n<figure>\n<img class=\"\" src=\"https://tonsky.me/blog/tahoe-icons/pedestrians.webp\" /></figure>\n<p>Same applies for icons: if you draw an arrow going out of the box in one place and also an arrow and the box but at a slightly different angle, or with different stroke width, or make one filled, we will understand them as meaning the same thing.</p>\n<p>Like, <img class=\"inline\" src=\"https://tonsky.me/blog/tahoe-icons/info_circle.svg\" /> is supposed to mean something else from <img class=\"inline\" src=\"https://tonsky.me/blog/tahoe-icons/info_circle_fill.svg\" />? Come on!</p>\n<figure>\n<img class=\"\" src=\"https://tonsky.me/blog/tahoe-icons/similar_i@2x.webp\" /></figure>\n<p>Or two-letter As that only slightly differ in the font size:</p>\n<figure>\n<img class=\"\" src=\"https://tonsky.me/blog/tahoe-icons/similar_font_size@2x.webp\" /></figure>\n<p>A pencil is \u201cRename\u201d but a slightly thicker pencil is \u201cHighlight\u201d?</p>\n<figure>\n<img class=\"\" src=\"https://tonsky.me/blog/tahoe-icons/similar_pencil@2x.webp\" /></figure>\n<p>Arrows that use different diagonals?</p>\n<figure>\n<img class=\"\" src=\"https://tonsky.me/blog/tahoe-icons/similar_actual_size@2x.webp\" /></figure>\n<p>Three dots occupying \u2154 of space vs three dots occupying everything. Seriously?</p>\n<figure>\n<img class=\"\" src=\"https://tonsky.me/blog/tahoe-icons/similar_sidebar@2x.webp\" /></figure>\n<p>Slightly darker dots?</p>\n<figure>\n<img class=\"\" src=\"https://tonsky.me/blog/tahoe-icons/similar_quality@2x.webp\" /></figure>\n<p>The sheet of paper that changes meaning depending on if its corner is folded or if there are lines inside?</p>\n<figure>\n<img class=\"\" src=\"https://tonsky.me/blog/tahoe-icons/similar_sheet@2x.webp\" /></figure>\n<p>But the final boss are arrows. They are all different:</p>\n<figure>\n<img class=\"\" src=\"https://tonsky.me/blog/tahoe-icons/similar_arrows@2x.webp\" /></figure>\n<p>Supposedly, a user must become an expert at noticing how squished the circle is, if it starts top to right or bottom to right, and how far the arrow\u2019s end goes.</p>\n<p>Do I care? Honestly, no. I could\u2019ve given it a shot, maybe, if Apple applied these consistently. But Apple considers <img class=\"inline\" src=\"https://tonsky.me/blog/tahoe-icons/square_and_pencil.svg\" /> and <img class=\"inline\" src=\"https://tonsky.me/blog/tahoe-icons/plus.svg\" /> to mean the same thing in one place, and expects me to notice minute details like this in another?</p>\n<p>Sorry, I can\u2019t trust you. Not after everything I\u2019ve seen.</p>\n<h1 id=\"detalization\">Detalization</h1>\n<p>Icons are supposed to be easily recognizable from a distance. Every icon designer knows: small details are no-go. You can have them sometimes, maybe, for aesthetic purposes, but you can\u2019t <em>rely</em> on them.</p>\n<p>And icons in Tahoe menus are <em>tiny</em>. Most of them fit in a 12\u00d712 pixel square (actual resolution is 24\u00d724 because of Retina), and because many of them are not square, one dimension is usually even less than 12.</p>\n<p>It\u2019s not a lot of space to work with! Even Windows 95 had 16\u00d716 icons. If we take the typical DPI of that era at 72 dots per inch, we get a physical icon size of 0.22 inches (5.6 mm). On a modern MacBook Pro with 254\u00a0DPI, Tahoe\u2019s 24\u00d724 icons are 0.09 inches (2.4 mm). Sure, 24 is bigger than 16, but in reality, these icons\u2019 area is 4 times as small!</p>\n<figure>\n<img class=\"\" src=\"https://tonsky.me/blog/tahoe-icons/dpi_comparison@2x.webp\" /><figcaption>Simulated physical size comparison between 16\u00d716\u00a0at\u00a072\u00a0DPI (left) and 24\u00d724\u00a0at\u00a0254\u00a0DPI (right)</figcaption></figure>\n<p>So when I see this:</p>\n<figure>\n<img class=\"\" src=\"https://tonsky.me/blog/tahoe-icons/details_zoom@2x.webp\" /></figure>\n<p>I struggle. I can tell they are different. But I definitely struggle to tell what\u2019s being drawn.</p>\n<p>Even zoomed in 20\u00d7, it\u2019s still a mess:</p>\n<figure>\n<img class=\"\" src=\"https://tonsky.me/blog/tahoe-icons/details_zoomed@2x.webp\" /></figure>\n<p>Or here. These are three different icons:</p>\n<figure>\n<img class=\"\" src=\"https://tonsky.me/blog/tahoe-icons/details_lists@2x.webp\" /></figure>\n<p>Am I supposed to tell plus sign from sparkle here?</p>\n<figure>\n<img class=\"\" src=\"https://tonsky.me/blog/tahoe-icons/details_sparkle@2x.webp\" /></figure>\n<p>Some of these lines are half the pixel thicker than the other lines, and that\u2019s supposed to be the main point:</p>\n<figure>\n<img class=\"\" src=\"https://tonsky.me/blog/tahoe-icons/details_redact@2x.webp\" /></figure>\n<p>Is this supposed to be an arrow?</p>\n<figure>\n<img class=\"\" src=\"https://tonsky.me/blog/tahoe-icons/details_original@2x.webp\" /></figure>\n<p>A paintbrush?</p>\n<figure>\n<img class=\"\" src=\"https://tonsky.me/blog/tahoe-icons/details_paste@2x.webp\" /></figure>\n<p>Look, a tiny camera.</p>\n<figure>\n<img class=\"\" src=\"https://tonsky.me/blog/tahoe-icons/details_screenshot@2x.webp\" /></figure>\n<p>It even got an even tinier viewfinder, which you can almost see if you zoom in 20\u00d7:</p>\n<figure>\n<img class=\"\" src=\"https://tonsky.me/blog/tahoe-icons/details_screenshot_zoomed@2x.webp\" /></figure>\n<p>Or here. There is a box, inside that box is a circle, and inside it is a tiny letter. <code>i</code> with a total height of 2 pixels:</p>\n<figure>\n<img class=\"\" src=\"https://tonsky.me/blog/tahoe-icons/details_properties@2x.webp\" /></figure>\n<p>Don\u2019t see it?</p>\n<figure>\n<img class=\"\" src=\"https://tonsky.me/blog/tahoe-icons/details_properties_zoomed@2x.webp\" /></figure>\n<p>I don\u2019t. But it\u2019s there...</p>\n<p>And this is a window! It even has traffic lights! How adorable:</p>\n<figure>\n<img class=\"\" src=\"https://tonsky.me/blog/tahoe-icons/details_window@2x.webp\" /></figure>\n<p>Remember: these are retina pixels, \u00bc of a real pixel. Steve Jobs himself claimed they were invisible.</p>\n<blockquote>\n  <p>It turns out there\u2019s a magic number right around 300 pixels per inch, that when you hold something around to 10 to 12 inches away from your eyes, is the limit of the human retina to differentiate the pixels.</p>\n</blockquote>\n<p>And yet, Tahoe icons rely on you being able to see them.</p>\n<h1 id=\"pixel-grid\">Pixel grid</h1>\n<p>When you have so little space to work with, every pixel matters. You can make a good icon, but you have to choose your pixels very carefully.</p>\n<p>For Tahoe icons, Apple decided to use vector fonts instead of good old-fashioned bitmaps. It saves Apple resources\u2014draw once, use everywhere. Any size, any display resolution, any font width.</p>\n<p>But there\u2019re downsides: fonts are hard to position vertically, their size <a href=\"https://tonsky.me/blog/font-size/\">doesn\u2019t map directly to pixels</a>, stroke width doesn\u2019t map 1-to-1 to pixel grid, etc. So, they work everywhere, but they also look blurry and mediocre everywhere:</p>\n<figure>\n<img class=\"\" src=\"https://tonsky.me/blog/tahoe-icons/details_clean_up@2x.webp\" /><figcaption>Tahoe icon (left) and its pixel-aligned version (right).</figcaption></figure>\n<p>They certainly start to work better once you give them more pixels.</p>\n<figure>\n<img class=\"\" src=\"https://tonsky.me/blog/tahoe-icons/ipad_comparison@2x.webp\" /><figcaption>iPad OS 26 vs macOS 26</figcaption></figure>\n<p>or make graphics simpler. But the combination of small details and tiny icon size is deadly. So, until Apple releases MacBooks with 380+ DPI, unfortunately, we still have to care about the pixel grid.</p>\n<h1 id=\"confusing-metaphors\">Confusing metaphors</h1>\n<p>Icons might serve another function: to help users understand the meaning of the command.</p>\n<p>For example, once you know the context (move window), these icons explain what\u2019s going on faster than words:</p>\n<figure>\n<img class=\"\" src=\"https://tonsky.me/blog/tahoe-icons/window@2x.webp\" /></figure>\n<p>But for this to work, the user must understand what\u2019s drawn on the icon. It must be a familiar object with a clear translation to computer action (like Trash can \u2192 Delete), a widely used symbol, or an easy-to-understand diagram. HIG:</p>\n<figure>\n<img class=\"\" src=\"https://tonsky.me/blog/tahoe-icons/hig_metaphor@2x.webp\" /></figure>\n<p>A rookie mistake would be to misrepresent the object. For example, this is how selection looks like:</p>\n<figure>\n<img class=\"\" src=\"https://tonsky.me/blog/tahoe-icons/metaphor_selection@2x.webp\" /></figure>\n<p>But its icon looks like this:</p>\n<figure>\n<img class=\"\" src=\"https://tonsky.me/blog/tahoe-icons/metaphor_select@2x.webp\" /></figure>\n<p>Honestly, I\u2019ve been writing this essay for a week, and I still have zero ideas why it looks like that. There\u2019s an object that looks like this, but it\u2019s a text block in Freeform/Preview:</p>\n<figure>\n<img class=\"\" src=\"https://tonsky.me/blog/tahoe-icons/metaphor_text_block@2x.webp\" /></figure>\n<p>It\u2019s called <code>character.textbox</code> in SF Symbols:</p>\n<figure>\n<img class=\"\" src=\"https://tonsky.me/blog/tahoe-icons/character_textbox@2x.webp\" /></figure>\n<p>Why did it become a metaphor for \u201cSelect all\u201d? My best guess is it\u2019s a mistake.</p>\n<p>Another place uses text selection from iOS as a metaphor. On a Mac!</p>\n<figure>\n<img class=\"\" src=\"https://tonsky.me/blog/tahoe-icons/metaphor_text_selection@2x.webp\" /></figure>\n<p>Some concepts have obvious or well-established metaphors. In that case, it\u2019s a mistake not to use them. For example, bookmarks: <img class=\"inline\" src=\"https://tonsky.me/blog/tahoe-icons/bookmark.svg\" />. Apple, for some reason, went with a book:</p>\n<figure>\n<img class=\"\" src=\"https://tonsky.me/blog/tahoe-icons/metaphor_bookmarks@2x.webp\" /></figure>\n<p>Sometimes you already have an interface element and can use it for an icon. However, try not to confuse your users. Dots in a rectangle look like password input, not permissions:</p>\n<figure>\n<img class=\"\" src=\"https://tonsky.me/blog/tahoe-icons/metaphor_permissions@2x.webp\" /></figure>\n<p>Icon here says \u201cCheck\u201d but the action is \u201cUncheck\u201d.</p>\n<figure>\n<img class=\"\" src=\"https://tonsky.me/blog/tahoe-icons/metaphor_mark_incomplete@2x.webp\" /></figure>\n<p>Terrible mistake: icon doesn\u2019t help, it actively confuses the user.</p>\n<p>It\u2019s also tempting to construct a two-level icon: an object and some sort of indicator. Like, a checkbox and a cross, meaning \u201cDelete checkbox\u201d:</p>\n<figure>\n<img class=\"\" src=\"https://tonsky.me/blog/tahoe-icons/metaphor_mark_unchecked@2x.webp\" /></figure>\n<p>Or a user and a checkmark, like \u201cCheck the user\u201d:</p>\n<figure>\n<img class=\"\" src=\"https://tonsky.me/blog/tahoe-icons/metaphor_manage@2x.webp\" /></figure>\n<p>Unfortunately, constructs like this rarely work. Users don\u2019t build sentences from building blocks you provide; they have no desire to solve these puzzles.</p>\n<p>Finding metaphors is hard. Nouns are easier than verbs, and menu items are mostly verbs. How does open look? Like an arrow pointing to the top right? Why?</p>\n<figure>\n<img class=\"\" src=\"https://tonsky.me/blog/tahoe-icons/metaphor_open@2x.webp\" /></figure>\n<p>I\u2019m not saying there\u2019s an obvious metaphor for \u201cOpen\u201d Apple missed. There isn\u2019t. But that\u2019s the point: if you can\u2019t find a good metaphor, using no icon is better than using a bad, confusing, or nonsensical icon.</p>\n<p>There\u2019s a game I like to play to test the quality of the metaphor. Remove the labels and try to guess the meaning. Give it a try:</p>\n<figure>\n<img class=\"\" src=\"https://tonsky.me/blog/tahoe-icons/metaphor_guess@2x.webp\" /></figure>\n<p>It\u2019s delusional to think that there\u2019s a good icon for every action if you think hard enough. There isn\u2019t. It\u2019s a lost battle from the start. No amount of money or \u201cmanagement decisions\u201d is going to change that. The problems are 100% self-inflicted.</p>\n<p>All this being said, I gotta give Apple credit where credit is due. When they are good at choosing metaphors, they are good:</p>\n<figure>\n<img class=\"\" src=\"https://tonsky.me/blog/tahoe-icons/metaphor_up_down@2x.webp\" /></figure>\n<h1 id=\"symmetrical-actions\">Symmetrical actions</h1>\n<p>A special case of a confusing metaphor is using different metaphors for actions that are direct opposites of one another. Like Undo/Redo, Open/Close, Left/Right.</p>\n<p>It\u2019s good when their icons use the same metaphor:</p>\n<figure>\n<img class=\"\" src=\"https://tonsky.me/blog/tahoe-icons/symmetry_import_export_right@2x.webp\" /></figure>\n<p>Because it saves you time and cognitive resources. Learn one, get another one for free.</p>\n<p>Because of that, it\u2019s a mistake not to use common metaphors for related actions:</p>\n<figure>\n<img class=\"\" src=\"https://tonsky.me/blog/tahoe-icons/symmetry_select@2x.webp\" /></figure>\n<p>Or here:</p>\n<figure>\n<img class=\"\" src=\"https://tonsky.me/blog/tahoe-icons/symmetry_clipboard@2x.webp\" /></figure>\n<p>Another mistake is to create symmetry where there is none. \u201cBack\u201d and \u201cSee all\u201d?</p>\n<figure>\n<img class=\"\" src=\"https://tonsky.me/blog/tahoe-icons/symmetry_app_store@2x.webp\" /></figure>\n<p>Some menus in Tahoe make both mistakes. E.g. lack of symmetry between Show/Hide and false symmetry between completed/subtasks:</p>\n<figure>\n<img class=\"\" src=\"https://tonsky.me/blog/tahoe-icons/symmetry_eye@2x.webp\" /></figure>\n<p>Import not mirrored by Export but by Share:</p>\n<figure>\n<img class=\"\" src=\"https://tonsky.me/blog/tahoe-icons/symmetry_import_export@2x.webp\" /></figure>\n<h1 id=\"text-in-icons\">Text in icons</h1>\n<p>HIG again:</p>\n<figure>\n<img class=\"\" src=\"https://tonsky.me/blog/tahoe-icons/hig_text_icons@2x.webp\" /></figure>\n<p>Authors of HIG are arguing against including text as a part of an icon. So something like this:</p>\n<figure>\n<img class=\"\" src=\"https://tonsky.me/blog/tahoe-icons/metaphor_select@2x.webp\" /></figure>\n<p>or this:</p>\n<figure>\n<img class=\"\" src=\"https://tonsky.me/blog/tahoe-icons/similar_i@2x.webp\" /></figure>\n<p>would not fly in 1992.</p>\n<p>I agree, but Tahoe has more serious problems: icons consisting <em>only</em> of text. Like this:</p>\n<figure>\n<img class=\"\" src=\"https://tonsky.me/blog/tahoe-icons/text_font@2x.webp\" /></figure>\n<p>It\u2019s unclear where \u201cmetaphorical, abstract icon text that is not supposed to be read literally\u201d ends and actual text starts. They use the same font, the same color, so how am I supposed to differentiate? Icons just get in a way: A...Complete? AaFont? What does it mean?</p>\n<p>I can maybe understand <img class=\"inline\" src=\"https://tonsky.me/blog/tahoe-icons/textformat_characters_dottedunderline.svg\" /> and <img class=\"inline\" src=\"https://tonsky.me/blog/tahoe-icons/a_ellipsis.svg\" />. Dots are supposed to represent something. I can imagine thinking that led to <img class=\"inline\" src=\"https://tonsky.me/blog/tahoe-icons/aa.svg\" />. But <img class=\"inline\" src=\"https://tonsky.me/blog/tahoe-icons/textformat_characters.svg\" />? No decorations. No effects. Just plain Abc. Really?</p>\n<h1 id=\"text-transformations\">Text transformations</h1>\n<p>One might think that using icons to illustrate text transformations is a better idea.</p>\n<p>Like, you look at this:</p>\n<figure>\n<img class=\"\" src=\"https://tonsky.me/blog/tahoe-icons/text_transformations@2x.webp\" /></figure>\n<p>or this:</p>\n<figure>\n<img class=\"\" src=\"https://tonsky.me/blog/tahoe-icons/text_size@2x.webp\" /></figure>\n<p>or this:</p>\n<figure>\n<img class=\"\" src=\"https://tonsky.me/blog/tahoe-icons/text_styles@2x.webp\" /></figure>\n<p>and just from the icon alone understand what will happen with the text. Icon <em>illustrates</em> the action.</p>\n<p>Also, BIU are well-established in word processing, so all upside?</p>\n<p>Not exactly. The problem is the same\u2014text icon looks like text, not icon. Plus, these icons are <em>excessive</em>. What\u2019s the point of taking the first letter and repeating it? The word \u201cBold\u201d already starts with a letter \u201cB\u201d, it reads just as easily, so why double it? Look at it again:</p>\n<figure>\n<img class=\"\" src=\"https://tonsky.me/blog/tahoe-icons/text_styles@2x.webp\" /></figure>\n<p>It\u2019s also repeated once more as a shortcut...</p>\n<p>There is a better way to design this menu:</p>\n<figure>\n<img class=\"\" src=\"https://tonsky.me/blog/tahoe-icons/text_styles_inline@2x.webp\" /></figure>\n<p>And it was known to Apple for at least 33 years.</p>\n<figure>\n<img class=\"\" src=\"https://tonsky.me/blog/tahoe-icons/hig_style@2x.webp\" /></figure>\n<h1 id=\"system-elements-in-icons\">System elements in icons</h1>\n<p>Operating system, of course, uses some visual elements for its own purposes. Like window controls, resize handles, cursors, shortcuts, etc. It would be a mistake to use those in icons.</p>\n<figure>\n<img class=\"\" src=\"https://tonsky.me/blog/tahoe-icons/hig_standard_elements@2x.webp\" /></figure>\n<p>Unfortunately, Apple fell into this trap, too. They reused arrows.</p>\n<figure>\n<img class=\"\" src=\"https://tonsky.me/blog/tahoe-icons/text_arrow@2x.webp\" /></figure>\n<p>Key shortcuts:</p>\n<figure>\n<img class=\"\" src=\"https://tonsky.me/blog/tahoe-icons/text_encoding@2x.webp\" /></figure>\n<p>HIG has an entire section on ellipsis specifically and how dangerous it is to use it anywhere else in the menu.</p>\n<figure>\n<img class=\"\" src=\"https://tonsky.me/blog/tahoe-icons/hig_ellipsis@2x.webp\" /></figure>\n<p>And this exact problem is in Tahoe, too.</p>\n<figure>\n<img class=\"\" src=\"https://tonsky.me/blog/tahoe-icons/text_ellipsis@2x.webp\" /></figure>\n<h1 id=\"icons-break-scanning\">Icons break scanning</h1>\n<p>Without icons, you can just scan the menu from top to bottom, reading only the first letters. Because they all align:</p>\n<figure>\n<img class=\"\" src=\"https://tonsky.me/blog/tahoe-icons/align_sequoia@2x.webp\" /><figcaption>macOS Sequoia</figcaption></figure>\n<p>In Tahoe, though, some menu items have icons, some don\u2019t, and they are aligned differently:</p>\n<figure>\n<img class=\"\" src=\"https://tonsky.me/blog/tahoe-icons/align_tahoe@2x.webp\" /></figure>\n<p>Some items can have both checkmarks <em>and</em> icons, or have only one of them, or have neither, so we get situations like this:</p>\n<figure>\n<img class=\"\" src=\"https://tonsky.me/blog/tahoe-icons/align_holes@2x.webp\" /></figure>\n<p>Ugh.</p>\n<h1 id=\"special-mention\">Special mention</h1>\n<p>This menu deserves its own category:</p>\n<figure>\n<img class=\"\" src=\"https://tonsky.me/blog/tahoe-icons/writing_direction@2x.webp\" /></figure>\n<p>Same icon for different actions. Missing the obvious metaphor. Somehow making the first one slightly smaller than the second and third. Congratulations! It got it all.</p>\n<h1 id=\"is-hig-still-relevant\">Is HIG still relevant?</h1>\n<p>I\u2019ve been mentioning HIG a lot, and you might be wondering: is an interface manual from 1992 still relevant today? Haven\u2019t computers changed so much that entirely new principles, designs, and idioms apply?</p>\n<p>Yes and no. Of course, advice on how to adapt your icons to black-and-white displays is obsolete. But the principles\u2014as long as they are good principles\u2014still apply, because they are based on how humans work, not how computers work.</p>\n<p>Humans don\u2019t get a new release every year. Our memory doesn\u2019t double. Our eyesight doesn\u2019t become sharper. Attention works the same way it always has. Visual recognition, motor skills\u2014all of this is exactly as it was in 1992.</p>\n<p>So yeah, until we get a direct chip-to-brain interface, HIG will stay relevant.</p>\n<h1 id=\"conclusion\">Conclusion</h1>\n<p>In my opinion, Apple took on an impossible task: to add an icon to every menu item. There are just not enough good metaphors to do something like that.</p>\n<p>But even if there were, the premise itself is questionable: if everything has an icon, it doesn\u2019t mean users will find what they are looking for faster.</p>\n<p>And even if the premise was solid, I still wish I could say: they did the best they could, given the goal. But that\u2019s not true either: they did a poor job consistently applying the metaphors and designing the icons themselves.</p>\n<p>I hope this article would be helpful in avoiding common mistakes in icon design, which Apple managed to collect all in one OS release. I love computers, I love interfaces, I love visual communication. It makes me sad seeing perfectly good knowledge already accessible 30 years ago being completely ignored or thrown away today.</p>\n<p>On the upside: it\u2019s not that hard anymore to design better than Apple! Let\u2019s drink to that. Happy New year!</p>\n<figure>\n<img class=\"\" src=\"https://tonsky.me/blog/tahoe-icons/smiley@2x.webp\" /><figcaption>From SF Symbols: a smiley face calling somebody on the phone</figcaption></figure>\n<h1 id=\"notes\">Notes</h1>\n<p>During review of this post I was made familiar with <a href=\"https://blog.jim-nielsen.com/2025/icons-in-menus/\">Jim Nielsen\u2019s article</a>, which hits a lot of the same points as I do. I take that as a sign there\u2019s some common truth behind our reasoning.</p>\n<p>Also note: Safari \u2192 File menu got worse since 26.0. Used to have only 4 icons, now it\u2019s 18!</p>\n<p>Thanks Kevin, Ryan, and Nicki for reading drafts of this post.</p>"
            ],
            "link": "https://tonsky.me/blog/tahoe-icons/",
            "publishedAt": "2026-01-05",
            "source": "Nikita Prokopov",
            "summary": "<p>I was reading Macintosh Human Interface Guidelines <a href=\"https://dn721903.ca.archive.org/0/items/apple-hig/Macintosh_HIG_1992.pdf\">from 1992</a> and found this nice illustration:</p> <figure> <img class=\"\" src=\"https://tonsky.me/blog/tahoe-icons/hig_icons@2x.webp\" /></figure> <p>accompanied by explanation:</p> <figure> <img class=\"\" src=\"https://tonsky.me/blog/tahoe-icons/hig_quote@2x.webp\" /></figure> <p>Fast forward to 2025. Apple releases macOS Tahoe. Main attraction? Adding unpleasant, distracting, illegible, messy, cluttered, confusing, frustrating icons (their words, not mine!) to every menu item:</p> <figure> <img class=\"\" src=\"https://tonsky.me/blog/tahoe-icons/sequoia_tahoe_textedit@2x.webp\" /><figcaption>Sequoia \u2192 Tahoe</figcaption></figure> <p>It\u2019s bad. But why exactly is it bad? Let\u2019s delve into it!</p> <p>Disclaimer: screenshots are a mix from macOS 26.1 and 26.2, taken from stock Apple apps only that come pre-installed with the system. No system settings were modified.</p> <h1 id=\"icons-should-differentiate\">Icons should differentiate</h1> <p>The main function of an icon is to help you find what you are looking for faster.</p> <p>Perhaps counter-intuitively, adding an icon to everything is exactly the wrong thing to do. To stand out, things need to be different. But if everything has an icon, nothing stands out.</p> <p>The same applies to color: black-and-white icons look clean, but they don\u2019t help you find things faster!</p> <p>Microsoft used to know this:</p> <figure> <a href=\"https://tonsky.me\"><img class=\"noround\" src=\"https://tonsky.me/blog/tahoe-icons/word@2x.webp\" /></a></figure> <p>Look how much faster you can find Save or Share in the right variant:</p> <figure> <img class=\"\" src=\"https://tonsky.me/blog/tahoe-icons/menu_cleanup@2x.webp\" /></figure>",
            "title": "It\u2019s hard to justify Tahoe icons"
        },
        {
            "content": [],
            "link": "https://www.robinsloan.com/lab/pop-up-2026/",
            "publishedAt": "2026-01-05",
            "source": "Robin Sloan",
            "summary": "<p>The Winter Garden beckons. <a href=\"https://www.robinsloan.com/lab/pop-up-2026/\">Read here.</a></p>",
            "title": "Popping up!"
        },
        {
            "content": [
                "<p>This is the weekly visible open thread. Post about anything you want, ask random questions, whatever. ACX has an unofficial <a href=\"https://www.reddit.com/r/slatestarcodex/\">subreddit</a>, <a href=\"https://discord.gg/RTKtdut\">Discord</a>, and <a href=\"https://www.datasecretslox.com/index.php\">bulletin board</a>, and <a href=\"https://www.lesswrong.com/community?filters%5B0%5D=SSC\">in-person meetups around the world</a>. Most content is free, some is subscriber only; you can subscribe <strong><a href=\"https://astralcodexten.substack.com/subscribe\">here</a></strong>. Also:</p><div><hr /></div><p><strong>1: </strong>New subscriber-only post, <a href=\"https://www.astralcodexten.com/p/learn-phrygian-in-zero-days\">Learn Phrygian In Zero Days</a>, about toddlers&#8217; linguistic quirks.</p><p><strong>2: </strong>ACX grantee Jacob Arbeid is looking for a cofounder for his AI safety lab. He writes: </p><blockquote><p>I&#8217;m looking for a strong software or ML engineer to cofound the world&#8217;s first &#8216;automation-first&#8217; AI safety lab. As a founding member of the UK&#8217;s AI Safety Institute, I saw firsthand how organisational, engineering and research bottlenecks limit humanity&#8217;s ability to build the safety tooling we need. To keep pace with AI&#8217;s rapid capability advances, we&#8217;ll need to go all-in on augmenting safety research and engineering with AI. I&#8217;m betting that a different kind of organisation - lean, flexible, relentlessly focused on automation with AI agents - can capture these gains to build at scales that would have been unimaginable a few years ago. I&#8217;ve received a generous grant from ACX to build this full-time, starting with AI evaluations. If this is something you feel should exist (no AI safety background required), reach out <a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfTiK6SoKrbeyOrWbVq0uwkLonFJcKN_Yt4VYje_fLtd4r75w/viewform\">here</a> or via <a href=\"https://www.linkedin.com/in/jacob-arbeid-a0947b159/\">LinkedIn</a>.</p></blockquote><p><strong>3: </strong>Astronomer/engineer/writer <a href=\"https://en.wikipedia.org/wiki/Clifford_Stoll\">Clifford Stoll</a> (mentioned on last month&#8217;s <a href=\"https://www.astralcodexten.com/p/links-for-december-2025\">links post</a> for his book <em>Silicon Snake Oil</em>, but also famous for cybersecurity adventures and Klein bottle making) will be doing a Q&amp;A at Mox in SF on January 13, <a href=\"https://partiful.com/e/ij02O44LOstHOuopedNy\">see here for details/signup</a>.</p><p><strong>4: </strong>Many good responses to <a href=\"https://www.astralcodexten.com/p/highlights-from-the-comments-on-vibecession\">Highlights On The Comments On Vibecession</a>, but I most appreciated the ones pushing back against my claim that, since China&#8217;s economy had dectupled in 25 years, Chinese economic nostalgia had to be baseless. One reader wrote (I haven&#8217;t confirmed):</p><blockquote><p>[China has] estimated 50% youth unemployment, general unemployment now illegal to publish (but you can still approximate by falling commuter numbers), some government workers haven&#8217;t been paid in more than a year (living entirely off extortion/bribery; includes police, teachers, and all healthcare), common experience in private sector to have your paycheck delayed by &#8220;2 weeks&#8221; that turns out to be 16 weeks. Starting salary offer for software engineer in Shanghai is &lt;1/3rd legal minimum wage.  You cannot partake of public services without home &#8220;ownership&#8221; (hukou system, and technically its a land lease). Private health insurance is overwhelmingly likely to be a scam and rarely used. Public coverage primarily covers TCM; most western medicine is out of pocket. If you cannot afford to pay - even in a trauma situation - you will be escorted off the property so your death doesn&#8217;t lower hospital mortality statistics.</p></blockquote><p>I apologize for telling the Chinese their complaints were invalid, and this is an interesting look at how massively increasing wealth can coexist with people&#8217;s lives getting bad (worse?), maybe relevant to last week&#8217;s <em>other</em> post&#8230;</p><p><strong>5: </strong>Thanks to everyone who commented on the <a href=\"https://www.astralcodexten.com/p/you-have-only-x-years-to-escape-permanent\">Permanent Moon Ownership</a> post. I was trying to inspire people to think bigger than B2B SAAS employment in the New Year, but I think I got the tone wrong, and also said things that required more explanation than the literary form could offer. I might talk about them at more length, but here&#8217;s a brief summary of what I would have said in more declarative form:</p><ol><li><p> This post was intended to counter <a href=\"https://www.newyorker.com/culture/infinite-scroll/will-ai-trap-you-in-the-permanent-underclass\">a specific meme going around in Silicon Valley</a>, and addressed primarily to the people spreading it. Poor people continue to have a hard time and a natural interest in becoming less poor, as always. The post, and everything below, is aimed at neurotic well-off people.</p></li><li><p>If we don&#8217;t get a crazy AI future, then human labor won&#8217;t be obsolete, and you won&#8217;t be in a permanent underclass (at least for that reason)</p></li><li><p>If we do get a crazy AI future, and the economy grows 100x (Industrial Revolution scale) or 1000000x (solar system colonization scale) in your lifetime, then you only need a little capital to remain as absolutely well-off as you are today. For example, after 100x growth, anyone with $25,000 in the stock market now would have $2.5 million.</p></li><li><p>If you don&#8217;t put away $25,000, then in order to stay equally well-off you only need for 1% (industrial scale) to 0.0001% (solar scale) of wealth to be redistributed through some combination of private charity and government welfare. Currently about 2% of income is redistributed via charity, and 25% via government (in the US). I glossed this as &#8220;you can get a moon in one of Dario Amodei&#8217;s galaxies&#8221;, and people had strong opinions on that exact example, but many people getting rich in AI have expressed interest in post-singularity charity, and I expect the 0.0001% - 1% target to be reached.</p></li><li><p>If you&#8217;re in one of the early industries to be affected by AI, you may have a very bad time before the economy can grow 100x or 1000000x. I wouldn&#8217;t describe this as a &#8220;permanent underclass&#8221; - it&#8217;s a subset of people, and their suffering is temporary - but it might be a very large subset, and it might continue longer than you can remain solvent. I agree it&#8217;s worth having savings ready to prepare against this scenario.</p></li><li><p>Some people have argued that you have to find a way to join an AI company, because AI company employees will form the new ruling class, with everyone else as serfs. I disagree. The main thing an AI company employee has that you don&#8217;t is AI company stock. But you can buy stock in Google, you may soon be able to buy stock in OpenAI and Anthropic, and even if not, you can get indirect exposure to these companies via stock in Amazon and Microsoft. I don&#8217;t recommend putting all your money in these stocks. But there&#8217;s no fundamental difference between a Google employee having 75% of their money in Google stock because they didn&#8217;t cash out their equity vs. you having 75% of your money in Google stock because you&#8217;re crazy and fail at diversification. So either put 75% of your money in Google stock or don&#8217;t (I recommend don&#8217;t), and don&#8217;t worry about how you need to join an AI company or be left out of the future oligarchy. </p></li><li><p>Gradual disempowerment (by humans or AI), coups by AI company executives, and techno-oligarchy by a tiny number of people are serious concerns. But you won&#8217;t join the oligarch class by starting a B2B SAAS company, and these concerns are more about democracy, freedom, inequality, and the meaning of life than about you personally being poor. The proper response to these scenarios (as the original post tried to argue) is to fight heroically against them and live forever in the pantheon of the benefactors of humanity - not to start a B2B SAAS company.</p></li><li><p>One way to think about all of this is that it&#8217;s important not to be very poor (you want enough capital to last through the transition period, aided by ballooning stock markets), and it might be extra good to be very rich (if you can be a literal oligarch, you have some new options available), but I don&#8217;t think going from 6-digit to 8-digit pre-singularity net worth gets you much more than pride.</p></li><li><p>Pride might matter - there could be permanent monetary inequality in a post-AGI world, and even if everyone is richer than today, your neighbor could be permanently richer than you (&#8221;a 20% bigger moon&#8221;), and this might grate for some people. But wealth is only one kind of potentially-permanent-inequality: Jeff Bezos, Bernie Sanders, Malala Yousafzai, Tyler Cowen, Dolly Parton, Dustin Moskovitz, and Chesley Sullenberger all have different types of status. Even if you&#8217;re thinking in terms of how you&#8217;ll be better than the Joneses in the distant changeless future, I encourage you to think bigger.</p></li></ol>"
            ],
            "link": "https://www.astralcodexten.com/p/open-thread-415",
            "publishedAt": "2026-01-05",
            "source": "SlateStarCodex",
            "summary": "<p>This is the weekly visible open thread. Post about anything you want, ask random questions, whatever. ACX has an unofficial <a href=\"https://www.reddit.com/r/slatestarcodex/\">subreddit</a>, <a href=\"https://discord.gg/RTKtdut\">Discord</a>, and <a href=\"https://www.datasecretslox.com/index.php\">bulletin board</a>, and <a href=\"https://www.lesswrong.com/community?filters%5B0%5D=SSC\">in-person meetups around the world</a>. Most content is free, some is subscriber only; you can subscribe <strong><a href=\"https://astralcodexten.substack.com/subscribe\">here</a></strong>. Also:</p><div><hr /></div><p><strong>1: </strong>New subscriber-only post, <a href=\"https://www.astralcodexten.com/p/learn-phrygian-in-zero-days\">Learn Phrygian In Zero Days</a>, about toddlers&#8217; linguistic quirks.</p><p><strong>2: </strong>ACX grantee Jacob Arbeid is looking for a cofounder for his AI safety lab. He writes: </p><blockquote><p>I&#8217;m looking for a strong software or ML engineer to cofound the world&#8217;s first &#8216;automation-first&#8217; AI safety lab. As a founding member of the UK&#8217;s AI Safety Institute, I saw firsthand how organisational, engineering and research bottlenecks limit humanity&#8217;s ability to build the safety tooling we need. To keep pace with AI&#8217;s rapid capability advances, we&#8217;ll need to go all-in on augmenting safety research and engineering with AI. I&#8217;m betting that a different kind of organisation - lean, flexible, relentlessly focused on automation with AI agents - can capture these gains to build at scales that would have been unimaginable a few years ago. I&#8217;ve received a generous grant from ACX to build this full-time, starting with AI evaluations. If this",
            "title": "Open Thread 415"
        },
        {
            "content": [
                "<p>This week, <a href=\"https://t.co/TPwNA6zWqm\">Philip Trammell and Dwarkesh Patel wrote Capital in the 22nd Century</a>.</p>\n<p>One of my goals for Q1 2026 is to write unified explainer posts for all the standard economic debates around potential AI futures in a systematic fashion. These debates tend to repeatedly cover the same points, and those making economic arguments continuously assume you must be misunderstanding elementary economic principles, or failing to apply them for no good reason. Key assumptions are often unstated and even unrealized, and also false or even absurd. Reference posts are needed.</p>\n<p>That will take longer, so instead this post covers the specific discussions and questions around the post by Trammell and Patel. My goal is to both meet that post on its own terms, and also point out the central ways its own terms are absurd, and the often implicit assumptions they make that are unlikely to hold.</p>\n<div>\n\n\n<span id=\"more-25005\"></span>\n\n\n</div>\n\n\n<h4 class=\"wp-block-heading\">What Trammell and Patel Are Centrally Claiming As A Default Outcome</h4>\n\n\n<p>They affirm, as do I, that Piketty was centrally wrong about capital accumulation in the past, for many well understood reasons, many of which they lay out.</p>\n<p>They then posit that Piketty could have been unintentionally describing our AI future.</p>\n<p>As in, IF, as they say they expect is likely:</p>\n<ol>\n<li>AI is used to \u2018lock in a more stable world\u2019 where wealth is passed to descendants.</li>\n<li>There are high returns on capital, with de facto increasing returns to scale due to superior availability of investment opportunities.</li>\n<li>AI and robots become true substitutes for all labor.</li>\n<li>(Implicit) This universe continues to support humanity and allows us to thrive.</li>\n<li>(Implicit) The humans continue to be the primary holders of capital.</li>\n<li>(Implicit) The humans are able to control their decisions and make essentially rational investment decisions in a world in which their minds are overmatched.</li>\n<li>We indefinitely do not do a lot of progressive redistribution.</li>\n<li>(Implicit) Private property claims are indefinitely respected at unlimited scale.</li>\n</ol>\n<p>THEN:</p>\n<ol>\n<li>Inequality grows without bound, the Gini coefficient approaches 1.</li>\n<li>Those who invest wisely, with eyes towards maximizing long term returns, end up with increasingly large shares of wealth.</li>\n<li>As in, they end up owning galaxies.</li>\n</ol>\n<blockquote><p>Patel and Trammell: But once robots and computers are capable enough that labor is no longer a bottleneck, we will be in the second scenario. The robots will stay useful even as they multiply, and the share of total income paid to robot-owners will rise to 1. (This would be the \u201cJevons paradox\u201d.)\u200b</p></blockquote>\n<p>Later on, to make the discussions make sense, we need to add:</p>\n<ol>\n<li>There is a functioning human government that can impose taxes, including on capital, in ways that end up actually getting paid.</li>\n<li>(Unclear) This government involves some form of Democratic control?</li>\n</ol>\n\n\n<h4 class=\"wp-block-heading\">Does The Above Conclusion Follow From The Above Premises?</h4>\n\n\n<p>If you include the implicit assumptions?</p>\n<p>Then yes. Very, very obviously yes. This is basic math.</p>\n\n\n<h4 class=\"wp-block-heading\">Sounds Like This Is Not Our Main Problem In This Scenario?</h4>\n\n\n<p>In this scenario, sufficiently capable AIs and robots are multiplying without limit and are perfect substitutes for human labor.</p>\n<p>Perhaps \u2018what about the distribution of wealth among humans\u2019 is the wrong question?</p>\n<p>I notice I have much more important questions about such worlds where the share of profits that goes to some combination AI, robots and capital rises to all of it.</p>\n<p>Why should the implicit assumptions hold? Why should we presume humans retain primary or all ownership of capital over time? Why should we assume humans are able to retain control over this future and make meaningful decisions? Why should we assume the humans remain able to even physically survive let alone thrive?</p>\n<p>Note especially the assumption that AIs don\u2019t end up with substantial private property. The best returns on capital in such worlds would obviously go to \u2018the AIs that are, directly or indirectly, instructed to do that.\u2019 So if AI is allowed to own capital, <a href=\"https://x.com/jankulveit/status/2005797088697934221\">the AIs end up with control over all the capital</a>, and the robots, and everything else. It\u2019s funny to me that they consider charitable trusts as a potential growing source of capital, but not the AIs.</p>\n<p>Even if we assumed all of that, why should we assume that private property rights would be indefinitely respected at limitless scale, on the level of owning galaxies? Why should we even expect property rights to be long term respected under normal conditions, here on Earth? Especially in a post calling for aggressive taxation on wealth, which is kind of the central \u2018nice\u2019 case of not respecting private property.</p>\n<p>Expecting current private property rights to indefinitely survive into the transformational superintelligence age seems, frankly, rather unwise?</p>\n<blockquote><p>Eliezer Yudkowsky: \u200bWhat is with this huge, bizarre, and unflagged presumption that property rights, as assigned by human legal systems, are inviolable laws of physics? That ASIs remotely care? You might as well write \u201cI OWN YOU\u201d on an index card in crayon, and wave it at the sea.</p>\n<p>Oliver Habryka: I really don\u2019t get where this presumption that property ownership is a robust category against changes of this magnitude. It certainly hasn\u2019t been historically!</p>\n<p><a href=\"https://x.com/jankulveit/status/2006676138106798253\">Jan Kulveit</a>: Cope level 1: My labour will always be valuable!<br />\nCope level 2: That\u2019s naive. My AGI companies stock will always be valuable, may be worth galaxies! We may need to solve some hard problems with inequality between humans, but private property will always be sacred and human.</p></blockquote>\n<p>Then, if property rights do hold, did we <a href=\"https://guive.substack.com/p/the-case-for-ai-property-rights?r=j837a&amp;utm_campaign=post&amp;utm_medium=web&amp;triedRedirect=true\">give AIs property rights</a>, as Guive Assadi suggests (and as others have suggested) we should do to give them a \u2018stake in the legal system\u2019 or simply for functional purposes? If not, that makes it very difficult for AIs to operate and transact, or for our system of property rights to remain functional. If we do, then the AIs end up with all the capital, even if human property rights remain respected. It also seems right to at some point, if the humans are not losing their wealth fast enough, <a href=\"https://x.com/ohabryka/status/2007040531008790891\">to expect AIs coordinating to expropriate</a> human property rights while respecting AI property rights, as has happened commonly throughout the history of property rights when otherwise disempowered groups had a large percentage of wealth.</p>\n<p>The hidden \u2018libertarian human essentialist\u2019 assumptions continue. For example, who are these \u2018descendants\u2019 and what are the \u2018inheritances\u2019? In these worlds one would expect aging and disease to be solved problems for both humans and AIs.</p>\n<p><a href=\"https://x.com/allTheYud/status/2006336954947006489\">Such talk and economic analysis often sounds remarkably parallel to this</a>:</p>\n<div>\n<figure>\n<div>\n\n\n<figure class=\"wp-block-image\"><img alt=\"\" src=\"https://substackcdn.com/image/fetch/$s_!a6lo!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F69753afe-9a54-4548-a10d-50687b02b23e_900x491.jpeg\" /></figure>\n\n\n<div></div>\n</div>\n</figure>\n</div>\n\n\n<h4 class=\"wp-block-heading\">To Be Clear This Scenario Doesn\u2019t Make Sense</h4>\n\n\n<p>The world described here has AIs that are no longer normal technology (while it tries to treat them as normal in other places anyway), it is not remotely at equilibrium, there is no reason to expect its property rights to endorse or to stay meaningful, it would be dominated by its AIs, and it would not long endure.</p>\n<p>If humans really are no longer useful, that breaks most of the assumptions and models of traditional econ along with everyone else\u2019s models, and people typically keep assuming actually humans will still be useful for something sufficiently for comparative advantage to rescue us, and can\u2019t actually wrap their heads around it not being true and humans being true zero marginal product workers given costs.</p>\n<blockquote><p><a href=\"https://x.com/ciphergoth/status/2007187293350789214\">Paul Crowley</a>: A lot of these stories from economics about how people will continue to be valuable make assumptions that don&#8217;t apply. If the models can do everything I do, and do it better, and faster, and for less than it costs me to eat, why would someone employ me?</p>\n<p>It&#8217;s really hard for people to take in the idea of an AI that&#8217;s better than any human at *every* task. Many just jump to some idea of an uber-task that they implicitly assume humans are better at. Satya Nadella made exactly this mistake on Dwarkesh.</p>\n<p><a href=\"https://x.com/dwarkesh_sp/status/2005766138907771356\">Dwarkesh Patel</a>: If labor is the bottleneck to all the capital growth. I don\u2019t see why sports and restaurants would bottleneck the Dyson sphere though.\u200b</p></blockquote>\n<p>That\u2019s the thing. If we\u2019re talking about a Dyson sphere world, why are we pretending any of these questions are remotely important or ultimately matter? At some point you have to stop playing with toys.</p>\n<p>A lot of this makes more sense if we don\u2019t think it involves Dyson spheres.</p>\n<p>Under a long enough time horizon, <a href=\"https://x.com/robinhanson/status/2006760732692750790\">I do think we can know roughly what the technologies</a> will look like barring the unexpected discovery of new physics, so I\u2019m with Robin Hanson here rather than Andrew Cote, today is not like 1850:</p>\n<blockquote><p>Andrew Cote: This kind of reasoning &#8211; that the future of humanity will be rockets, robots, and dyson swarms indefinitely into the future, assumes an epistemological completeness that we already know the future trade-space of all possible technologies.</p>\n<p>It is as wrong as it would be to say, in 1850, that in two hundred years any nation that does not have massive coal reserves will be unfathomably impoverished. What could there be besides coal, steel, rail, and steam engines?</p>\n<p>Physics is far from complete, we are barely at the beginning of what technology can be, and the most valuable things that can be done in physical reality can only be done by conscious observers, and this gets to the very heart of interpretations of quantum mechanics and physical theory itself.</p>\n<p>Robin Hanson: \u200bNo, more likely than not, we are constrained to a 3space-1time space-time where the speed of light is a hard limit on travel/influence, thermodynamics constrains the work we can do, &amp; we roughly know what are the main sources of neg-entropy. We know a lot more than in 1850.</p></blockquote>\n<p>Even in the places were the assumptions aren\u2019t obviously false, or you want to think they\u2019re not obviously false, and also you want to assume various miracles occur such that we dodge outright ruin, certainly there\u2019s no reason to think the future situation will be sufficiently analogous to make these analyses actually make sense?</p>\n<blockquote><p><a href=\"https://x.com/daniel_271828/status/2006500364980466030\">Daniel Eth</a>: This feels overly confident for advising a world completely transformed. I have no idea if post-AGI we\u2019d be better off taxing wealth vs consumption vs something else. Sure, you can make the Econ 101 argument for taxing consumption, but will the relevant assumptions hold? Who knows.</p>\n<p><a href=\"https://x.com/sebkrier/status/2006704128819343612\">Seb Krier</a>: I also don&#8217;t have particularly good intuitions about what a world with ASI, nanotechnology and Dyson swarms looks like either.</p>\n<p>Futurist post-AGI discussions often revolve around thinking at the edge of what&#8217;s in principle plausible/likely and extrapolating more and more. This is useful, but the compounding assumptions necessary to support a particular take contain so many moving parts that can individually materially affect a prediction.</p>\n<p>It&#8217;s good to then unpack and question these, and this creates all sorts of interesting discussions. But what&#8217;s often lost in discussions is the uncertainty and fragility of the scaffolding that supports a particular prediction. Some variant of the conjunction fallacy.</p>\n<p>Which is why even though I find long term predictions interesting and useful to expand the option space, I rarely find them particularly informative or sufficient to act on decisively now. In practice I feel like we&#8217;re basically hill-climbing on a fitness landscape we cannot fully see.</p>\n<p>Brian Albrecht: I appreciate Dwarkesh and Philip\u2019s piece. I responded to one tiny part.</p>\n<p>But I\u2019ll admit I don\u2019t have a good intuition for what will happen in 1000 years across galaxies. So I think building from the basics seems reasonable.</p></blockquote>\n<p>I don\u2019t even know that \u2018wealth\u2019 and \u2018consumption\u2019 would be meaningful concepts that look similar to how they look now, among other even bigger questions. I don\u2019t expect \u2018the basics\u2019 to hold and I think we have good reasons to expect many of them not to.</p>\n<blockquote><p><a href=\"https://stratechery.com/2026/ai-and-the-human-condition/?access_token=eyJhbGciOiJSUzI1NiIsImtpZCI6InN0cmF0ZWNoZXJ5LnBhc3Nwb3J0Lm9ubGluZSIsInR5cCI6IkpXVCJ9.eyJhdWQiOiJzdHJhdGVjaGVyeS5wYXNzcG9ydC5vbmxpbmUiLCJhenAiOiJIS0xjUzREd1Nod1AyWURLYmZQV00xIiwiZW50Ijp7InVyaSI6WyJodHRwczovL3N0cmF0ZWNoZXJ5LmNvbS8yMDI2L2FpLWFuZC10aGUtaHVtYW4tY29uZGl0aW9uLyJdfSwiZXhwIjoxNzcwMjAzMjIyLCJpYXQiOjE3Njc2MTEyMjIsImlzcyI6Imh0dHBzOi8vYXBwLnBhc3Nwb3J0Lm9ubGluZS9vYXV0aCIsInNjb3BlIjoiZmVlZDpyZWFkIGFydGljbGU6cmVhZCBhc3NldDpyZWFkIGNhdGVnb3J5OnJlYWQgZW50aXRsZW1lbnRzIiwic3ViIjoiMDE5NjQwYTctM2NjNS03NzUzLTgzNjgtZmIyODkxMjRjZjEzIiwidXNlIjoiYWNjZXNzIn0.QHaFRs3k-VirD1RZE_MKg5MxAUXRuALP6EEjiHpYhwcNHjc1SmZUCDfZttwCOuco8AKQ3TE8zgVKEJW7Mj109V1h3EH7FWntnkJKOgv2NN8pYYaBnjbudaEiCTKtTSvjOLetGRQVE59tolXsgpODLZBAvFd1FziFCEH21SayscB1-nvKxbF4RPhfNraCLrb-7Hxg07tP4ZkCyYjXy5-tI4ztUWzl9jiX19c-prwe88A0HjIoejdbT8LXeJxi0wajkUAUOEsl7Qe4nNnO4DzXr_5tDzVBP99kQk345JFVRGTT-10QGlvjj4X6IAPOkkQGT_ME3FEHZGHckVQ7fpjjOw\">Ben Thompson</a>: \u200bThis world also sounds implausible. It seems odd that AI would acquire such fantastic capabilities and yet still be controlled by humans and governed by property laws as commonly understood in 2025. I find the AI doomsday scenario \u2014 where this uber-capable AI is no longer controllable by humans \u2014 to be more realistic; on the flipside, if we start moving down this path of abundance, I would expect our collective understanding of property rights to shift considerably.</p></blockquote>\n<p><a href=\"https://x.com/BjarturTomas/status/2006614753309765758\">Ultimately all of this, as Tomas Bjartur puts it, imagines an absurd world</a>, assuming away all of the dynamics that matter most. Which still leaves something fun and potentially insightful to argue about, I\u2019m happy to do that, but don\u2019t lose sight of it not being a plausible future world, and taking as a given that all our \u2018real\u2019 problems mysteriously turn out fine despite us having no way to even plausibly describe what that would look like, let alone any idea how to chart a path towards making it happen.</p>\n\n\n<h4 class=\"wp-block-heading\">Ad Argumento</h4>\n\n\n<p>Thus from this point on, this post accepts the premises listed above, ad argumento.</p>\n<p>I don\u2019t think that world actually makes a lot of sense on reflection, as an actual world. Even if all associated technical and technological problems are solved, including but not limited to all senses of AI alignment, I do not see a path arriving at this outcome.</p>\n<p>I also have lots of problems with parts the economic baseline case under this scenario.</p>\n<p>The discussion is still worth having, but one needs to understand all this up front.</p>\n<p>It\u2019s even worth having that discussion even if the economists are mostly rather dense and smg and trotting out their standard toolbox as if nothing else ever applies to anything. <a href=\"https://x.com/yonashav/status/2006378453826736496\">I agree with Yo Shavit that it is good that this and other writing and talks from Dwarkesh Patel are generating serious economic engagement</a> at all.</p>\n\n\n<h4 class=\"wp-block-heading\">The Baseline Scenario</h4>\n\n\n<p>If meaningful democratic human control over capital persisted in a world trending towards extreme levels of inequality, I would expect to see massive wealth redistribution, including taxes on or confiscation of extreme concentrations of wealth.</p>\n<p>If meaningful democratic control didn\u2019t persist, then I would expect the future to be determined by whatever forces had assumed de facto control. By default I would presume this would be \u2018the AIs,\u2019 but the same applies if some limited human group managed to retain control, including over the AIs, despite superintelligence. Then it would be up to that limited group what happened after that. My expectation would be that most such groups would do some redistribution, but not attempt to prevent the Gini coefficient going to ~1, and they would want to retain control.</p>\n<p><a href=\"https://x.com/jankulveit/status/2005797088697934221\">Jan Kelveit\u2019s pushback here seems good</a>. In this scenario, human share of capital will go to zero, our share of useful capability for violence will also go to zero, the use of threats as leverage won\u2019t work and will go to zero, and our control over the state will follow. <a href=\"https://x.com/LedermanHarvey/status/2006006050194112771\">Harvey Lederman also points out related key flaws</a>.</p>\n<p><a href=\"https://x.com/nikolaj2030/status/2007839615382667632\">As Nikola Jurkovic notes</a>, if superintelligence shows up and if we presume we get to a future with tons of capital and real wealth but human labor loses market value, and if humans are still alive and in control over what to do with the atoms (big ifs), then as he points out we fundamentally are going to either do charity for those who don\u2019t have capital, or those people perish.</p>\n<p>That charity can take the form of government redistribution, and one hopes that we do some amount of this, but once those people have no leverage it is charity. It could also take the form of private charity, as \u2018the bill\u2019 here will not be so large compared to total wealth.</p>\n\n\n<h4 class=\"wp-block-heading\">Would We Have An Inequality Problem?</h4>\n\n\n<p>It is not obvious that we would.</p>\n<p>Inequality of wealth is not inherently a problem. Why should we care that one man has a million dollars and a nice apartment, while another has the Andromeda galaxy?What exactly are you going to do with the Andromeda galaxy?</p>\n<p>A metastudy in Nature released last week concluded that <a href=\"https://www.nature.com/articles/d41586-025-03833-8?utm_source=x&amp;utm_medium=social&amp;utm_campaign=nature&amp;linkId=31730782\">economic inequality does not equate to poor well-being or mental health</a>.</p>\n<p><a href=\"https://x.com/paulnovosad/status/2007089824126099865\">I also agree with Paul Novosad that it seems like our appetite</a> for a circle of concern and generous welfare state is going down, not up. I\u2019d like to hope that this is mostly about people feeling they themselves don\u2019t have enough, and this would reverse if we had true abundance, but I\u2019d predict only up to a point, <a href=\"https://x.com/littmath/status/2007472815952265654\">no we\u2019re not going to demand something resembling equality</a> and I don\u2019t think anyone needs a story to justify it.</p>\n<p><a href=\"https://x.com/dwarkesh_sp/status/2006505366423745004\">Dwarkesh\u2019s addendum that people are misunderstanding him</a>, and emphasizing the inequality is inherently the problem, makes me even more confused. It seems like, yes, he is saying that wealth levels get locked in by early investment choices, and then that it is \u2018hard to justify\u2019 high levels of \u2018inequality\u2019 and that even if you can make 10 million a year in real income in the post-abundance future Larry Page\u2019s heirs owning galaxies is not okay.</p>\n<p>I say, actually, yes that\u2019s perfectly okay, provided there is stable political economy and we\u2019ve solved the other concerns so you can enjoy that 10 million a year in peace. The idea that there is a basic unit, physical human minds, that all have rights to roughly equal wealth, whereas the more capable AI minds and other entities don\u2019t, and anything else is unacceptable? That doesn\u2019t actually make a lot of sense, even if you accept the entire premise.</p>\n<p><a href=\"https://x.com/t_holden/status/2006370711363829945\">Tom Holden\u2019s pushback is that we only care</a> about consumption inequality, not wealth inequality, and when capital is the only input taking capital hurts investment, so what you really want is a consumption tax.</p>\n<p>Similar thinking causes Brian Albrecht to say \u2018redistribution doesn\u2019t help\u2019 when the thing that\u2019s trying to be \u2018helped\u2019 is inequality. Of course redistribution can \u2018help\u2019 with that. Whereas I think Brian is presuming what you actually care about is the absolute wealth or consumption level of the workers, which of course can also be \u2018helped\u2019 by transfers, so I notice I\u2019m still confused.</p>\n<p>But either way, no, that\u2019s not what anyone is asking in this scenario &#8211; the pie doth overfloweth, so it\u2019s very easy for a very small tax to create quite a lot of consumption, if you can actually stay in control and enforce that tax.</p>\n<p>I agree that in \u2018normal\u2019 situations among humans consumption inequality is what matters, and I would go further and say absolute consumption levels are what matters most. You don\u2019t have to care so much about how much others consume so long as you have plenty, although I agree that people often do. I have 1000x what I have now and I don\u2019t age or die, and my loved ones don\u2019t age or die, but other people own galaxies? Sign me the hell up. Do happy dance.</p>\n<p>Dwarkesh explicitly disagrees and many humans have made it clear they disagree.</p>\n<p>Framing this as \u2018consumption\u2019 drags in a lot of assumptions that will break in such worlds even if they are otherwise absurdly normal. We need to question this idea that meaningful use of wealth involves \u2018consumption,\u2019 whereas many forms of investment or other such spending are in this sense de facto consumption. Also AIs don\u2019t \u2018consume\u2019 is this sense so again this type of strategy only accelerates disempowerment.</p>\n<p>The good counter argument is that sufficient wealth soon becomes power.</p>\n<blockquote><p><a href=\"https://x.com/paulg/status/2007079764964892917\">Paul Graham</a>: \u200bThe rational fear of those who dislike economic inequality is that the rich will convert their economic power into political power: that they&#8217;ll tilt elections, or pay bribes for pardons, or buy up the news media to promote their views.</p>\n<p>I used to be able to claim that tech billionaires didn&#8217;t actually do this \u2014 that they just wanted to refine their gadgets. But unfortunately in the current administration we&#8217;ve seen all three.</p>\n<p>It&#8217;s still rare for tech billionaires to do this. Most do just want to refine their gadgets. That habit is what made them billionaires. But unfortunately I can no longer say that they all do.</p></blockquote>\n<p>I don\u2019t think the inequality being \u2018hard to justify\u2019 is important. I do think \u2018humans, often correctly, beware inequality because it leads to power\u2019 is important.</p>\n\n\n<h4 class=\"wp-block-heading\">No You Can\u2019t Simply Let Markets Handle Everything</h4>\n\n\n<p><a href=\"https://x.com/drydenwtbrown/status/2006519311670956346\">Garry Tan\u2019s pushback</a> of \u2018<a href=\"https://x.com/garrytan/status/2006439007333527604\">whoa Dwarkesh, open markets are way better</a> than redistribution\u2019 and all the standard anti-redistribution rhetoric and faith that competition means everyone wins, a pure blind faith in markets to deliver all of us from everything and the only thing we have to fear is government regulations and taxes and redistribution, attacking Dwarkesh for daring to suggest redistribution could ever help with anything, is a maximally terrible response.</p>\n<p>Not only is it suicidal in the face of the problems Dwarkesh is ignoring, it is also very literally suicidal in the face of labor income dropping to zero. Yes, prices fall and quality rises, and then anyone without enough capital starves anyway. Free markets don\u2019t automagically solve everything. Mostly free markets are mostly the best solutions to most problems. There\u2019s a difference.</p>\n<p>You can decide that \u2018inequality\u2019 is not in and of itself a problem. You do still need to do some amount of \u2018non-market\u2019 redistribution if you want humans whose labor is not valuable to survive other than off capital, because otherwise they won\u2019t. Maybe Garry Tan is fine with that if it boosts the growth rate. I\u2019m not fine with it. The good news is that in this scenario we will be supremely wealthy, so a very small tax regime will enable all existing humans to live indefinitely in material wealth we cannot dream of.</p>\n\n\n<h4 class=\"wp-block-heading\">Proposed Solutions</h4>\n\n\n<p>Okay, suppose we do want to address the \u2018inequality\u2019 problem. What are our options?</p>\n<p>Their first proposed solution are large inheritance taxes. As noted above, I would not expect these ultra wealthy people or AIs to die, so I don\u2019t expect there to be inheritances to tax. If we lean harder into \u2018premise!\u2019 and ignore that issue, then I agree that applying taxes on death rather than continuously has some incentive advantages but also it introduces an insane level of distorted incentives if you tried to make this revenue source actually matter versus straight wealth taxes.</p>\n<p>The proposed secondary solution of a straight up large wealth tax is justified by \u2018the robots will work just as hard no matter the tax rate,\u2019 to argue that this won\u2019t do too much economic damage, but to the extent they are minds or minds are choosing the robot behaviors this simply is not effectively true, as most economists will tell you. They might work as hard, but they won\u2019t work in the same ways towards the same ends, because either humans or AIs will be directing what the robots do and the optimization targets have changed. Communist utopia is still communist utopia, it\u2019s weird to see it snuck in here as if it isn\u2019t what it is.</p>\n<p>The tertiary solution, a minimum \u2018spending\u2019 requirement, starts to get weird quickly if you try to pin it down. What is spending? What is consumption? This would presumably be massively destructive, causing massive wasteful consumption, on the level of \u2018destroying a large portion of the available mass-energy in the lightcone for no effect.\u2019 It\u2019s a cool new thing to think about. Ultimately I don\u2019t think it works, due to mismatched conceptual assumptions.</p>\n<p>They also suggest taxing \u2018natural resources.\u2019 In a galactic scenario this seems like an incoherent proposal when applied to very large concentrations of wealth, not functionally different than straight up taxing wealth. If it is confined to Earth, then you can get some mileage out of this, but that\u2019s solving your efficient government revenue problems, not your inequality problems. Do totally do it anyway.</p>\n<p>The real barriers to implementing massive redistribution are \u2018can the sources of power choose to do that?\u2019 and \u2018are we willing to take the massive associated hits to growth?\u2019</p>\n<p>The good news for the communist utopia solution (aka the wealth tax) is that <a href=\"https://x.com/daniel_271828/status/2006146956385530221\">it would be quite doable to implement it</a> on a planetary scale, or in \u2018AI as normal technology\u2019 near term worlds, if the main sources of power wanted to do that. Capital controls are a thing, as is imposing your will on less powerful jurisdictions. \u2018Capital\u2019 is not magic.</p>\n<p>The problem on a planetary scale is that the main sources of real power are unlikely to be the democratic electorate, once that electorate no longer is a source of either economic or military power. If the major world powers (or unified world government) want something, and remain the major world powers, they get it.</p>\n<p>When you\u2019re going into the far future and talking about owning galaxies, you then have some rather large \u2018laws of physics\u2019 problems with enforcement? How are you going to collect or enforce a tax on a galaxy? What would it even mean to tax it? In what sense do they \u2018own\u2019 the galaxy?</p>\n<p>A universe with only speed-of-light travel, where meaningful transfers require massive expenditures of energy, and essentially solved technological possibilities, functions very, very differently in many ways. I don\u2019t think they\u2019re being thought through. If you\u2019re living in a science fiction story for real, best believe in them.</p>\n\n\n<h4 class=\"wp-block-heading\">Wealth Taxes Today Are Grade-A Stupid</h4>\n\n\n<p>As Tyler Cowen noted in his response to Dwarkesh, there are those who want to implement wealth taxes a lot sooner than when AI sends human labor income to zero.</p>\n<p>As in, they want to implement it now, including now in California, where there is a serious proposal for a wealth tax, including on unrealized capital gains, including illiquid ones in startups as assessed by the state.</p>\n<p>That would be supremely, totally, grade-A stupid and destructive if implemented, on the level of \u2018no actually this would destroy San Francisco as the tech capital.\u2019</p>\n<p>Tech and venture capital like to talk the big cheap talk about how every little slight is going to cause massive capital flight, and how everything cool will happen in Austin and Miami instead of San Francisco and New York Real Soon Now because Socialism. Mostly this is cheap talk. They are mostly bluffing. The considerations matter on the margin, but not enough to give up the network effects or actually move.</p>\n<p>They said SB 1047 would \u2018destroy California\u2019s AI industry\u2019 when its practical effect would have been precisely zero. Many are saying similar things about Mamdani, who could cause real problems for New York City in this fashion, but chances are he won\u2019t. And so on, there\u2019s always something, usually many somethings.</p>\n<p>So there is most definitely a \u2018<a href=\"https://x.com/daniel_271828/status/2006089802907877510\">boy who cried wolf\u2019 problem</a>, but no, <a href=\"https://x.com/zoink/status/2005093365243908226\">seriously, wolf</a>.</p>\n<p>I believe it would be a 100% full wolf even if you could pay in kind with illiquid assets, or otherwise have a workaround. It would <a href=\"https://x.com/stuartbuck1/status/2005402631695376515\">still be obviously unworkable</a> including due to flight. Without a workaround for illiquid assets, this isn\u2019t even a question, the ecosystem is forced to flee overnight.</p>\n<p>Looking at historical examples, a good rule of thumb is:</p>\n<ol>\n<li>High taxes on realized capital gains or high incomes do drive people away, but if you offer sufficient value most of them suck it up and stay anyway. <a href=\"https://x.com/stuartbuck1/status/2005402631695376515\">There is a lot of room</a>, especially nationally, to ensure billionaires get taxed on their income.</li>\n<li>Wealth taxes are different. Impacted people flee and take their capital with them.</li>\n</ol>\n<p>The good news is <a href=\"https://x.com/mrp/status/2004710044278161633\">California Governor Gavin Newsom is opposed</a>, but this Manifold market still <a href=\"https://manifold.markets/NicholasWeininger/will-the-proposed-california-wealth\">gives the proposed \u20182026 Billionaires Tax Act\u2019 a 19% chance of collecting over a billion in revenue</a>. That\u2019s probably too high, but even if it\u2019s more like 10%, that\u2019s only the first attempts, and that\u2019s high enough to have a major chilling effect already.</p>\n\n\n<h4 class=\"wp-block-heading\">Tyler Cowen Responds With Econ Equations</h4>\n\n\n<p>To be fair to Tyler Cowen, his analysis assumes a far more near term, very much like today scenario rather than Dyson spheres and galaxies, and if you assume AI is having sufficiently minor impact and things don\u2019t change much, then his statements, and his treating the future world as ours in a trenchcoat, makes a lot more sense.</p>\n<p><a href=\"https://marginalrevolution.com/marginalrevolution/2026/01/taxation-in-a-strong-ai-world.html\">Tyler Cowen offered more of the</a> \u2018assume everything important doesn\u2019t matter and then apply traditional economic principles to the situation\u2019 analysis, try to point to equations that suggest real wages could go up in worlds where labor doesn\u2019t usefully accomplish anything, and look at places humans would look to increase consumption so you can tax health care spending or quality home locations to pay for your redistribution, as if this future world is ours in a trenchcoat.</p>\n<p><a href=\"https://x.com/GarettJones/status/2007239386967204198\">Similarly, here Garett Jones claims (in a not directly related post) that </a>if there is astronomical growth in \u2018capital\u2019 (read: AI) such that it\u2019s \u2018unpriced like air\u2019, and labor and capital are perfect substitutes, then capital share of profits would be zero. Except, unless I and Claude are missing something rather obvious, that makes the price of labor zero. So what in the world?</p>\n<p>That leaves the other scenario, which he also lists, where labor and \u2018capital\u2019 are perfect complements, as in you assume human labor is mysteriously uniquely valuable and rule of law and general conditions and private property hold, in which case by construction yes labor does fine, as you\u2019ve assumed your conclusion. That\u2019s not the scenario being considered by the OP, indeed the OP directly assumes the opposite.</p>\n<p>No, do not assume the returns stay with capital, but why are you assuming returns stay with humans at all? Why would you think that most consumption is going to human consumption of ordinary goods like housing and healthcare? There are so many levels of scenario absurdity at play. I\u2019d also note that Cowen\u2019s ideas all involve taxing humans in ways that do not tax AIs, accelerating our disempowerment.</p>\n\n\n<h4 class=\"wp-block-heading\">Brian Albrecht Responds With More Equations</h4>\n\n\n<p>As another example of economic toolbox response <a href=\"https://x.com/BrianCAlbrecht/status/2005986194262241694\">we have Brian Albrecht here usefully trotting out supply and demand</a> to engage with these questions, to ask whether we can effectively tax capital which depends on capital supply elasticity and so on, talking about substituting capital and labor, except the whole point is that labor (if we presume AI is of the form \u2018capital\u2019 rather than labor, and that the only two relevant forms of production are capital and labor, which smuggles in quite a lot of additional assumptions I expect to likely become false in ways I doubt Brian is realizing) is now irrelevant and strictly dominated by capital. I would ask, why are we asking about the rate of \u2018capital substitution for labor\u2019 in a world in which capital has fully replaced labor?</p>\n<p>So this style engagement is great compared to not engaging, but on another level also completely misses the point? When they get to talking downthread it seems like the point is missed even more, with statements like \u2018capital never gets to share of 1 because of depreciation, you get finite K*.\u2019 I\u2019m sorry, what? The forest has been lost, models are being applied to scenarios where they don\u2019t make sense.</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p>"
            ],
            "link": "https://thezvi.wordpress.com/2026/01/05/dos-capital/",
            "publishedAt": "2026-01-05",
            "source": "TheZvi",
            "summary": "This week, Philip Trammell and Dwarkesh Patel wrote Capital in the 22nd Century. One of my goals for Q1 2026 is to write unified explainer posts for all the standard economic debates around potential AI futures in a systematic fashion. &#8230; <a href=\"https://thezvi.wordpress.com/2026/01/05/dos-capital/\">Continue reading <span class=\"meta-nav\">&#8594;</span></a>",
            "title": "Dos Capital"
        },
        {
            "content": [],
            "link": "https://xkcd.com/3190/",
            "publishedAt": "2026-01-05",
            "source": "XKCD",
            "summary": "<img alt=\"Some people argue that the tension and compression in the human skeleton is technically tensegrity, but it's missing the defining characteristic: making people say 'wtf, how is that thing floating?' when they see it.\" src=\"https://imgs.xkcd.com/comics/tensegrity.png\" title=\"Some people argue that the tension and compression in the human skeleton is technically tensegrity, but it's missing the defining characteristic: making people say 'wtf, how is that thing floating?' when they see it.\" />",
            "title": "Tensegrity"
        }
    ],
    "lookbackDays": 1,
    "publishDate": "2026-01-05"
}