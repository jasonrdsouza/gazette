{
    "articles": [
        {
            "content": [
                "<p>I manage my bookmarks <a href=\"https://alexwlchan.net/2025/bookmarks-static-site/\">with a static website</a>.\nI\u2019ve bookmarked over 2000 pages, and I keep a local snapshot of every page.\nThese snapshots are stored alongside the bookmark data, so I always have access, even if the original website goes offline or changes.</p>\n\n\n\n<figure class=\"comparison\">\n  \n\n\n\n\n  \n  <source type=\"image/png\" />\n  \n  <img alt=\"Screenshot of a web page showing a 504 Gateway Timeout error.\" class=\"screenshot\" src=\"https://alexwlchan.net/images/2025/bookmarks/thirty_years_broken_1x.png\" width=\"300\" />\n\n\n\n\n  \n\n\n\n\n  \n  <source type=\"image/png\" />\n  \n  <img alt=\"Screenshot of the same page, from a local snapshot, showing the headline \u201c30\u00a0years on, what\u2019s next for the web?\u201d\" class=\"screenshot\" src=\"https://alexwlchan.net/images/2025/bookmarks/thirty_years_archived_1x.png\" width=\"300\" />\n\n\n\n\n</figure>\n\n<p>I\u2019ve worked on web archives in a professional setting, but this one is strictly personal.\nThis gives me more freedom to make different decisions and trade-offs.\nI can focus on the pages I care about, spend more time on quality control, and delete parts of a page I don\u2019t need \u2013 without worrying about institutional standards or long-term public access.</p>\n\n<p>In this post, I\u2019ll show you how I built this personal archive of the web: how I save pages, why I chose to do it by hand, and what I do to make sure every page is properly preserved.</p>\n\n<blockquote class=\"toc\">\n  <p>This article is the second in a four part bookmarking mini-series:</p>\n  <ol>\n    <li>\n      <a href=\"https://alexwlchan.net/2025/bookmarks-static-site/\"><strong>Creating a static site for all my bookmarks</strong></a> \u2013 why I bookmark, why I use a static site, and how it works.\n    </li>\n    <li>\n      <strong>Creating a local archive of all my bookmarks</strong> (this article)\n      <ul>\n        <li><a href=\"https://alexwlchan.net/2025/personal-archive-of-the-web/#requirements\">What do I want from a web archive?</a></li>\n        <li>\n          <a href=\"https://alexwlchan.net/2025/personal-archive-of-the-web/#what_it_looks_like\">What does my web archive look like?</a>\n          <ul>\n            <li><a href=\"https://alexwlchan.net/2025/personal-archive-of-the-web/#static_folders\">A static folder for every page</a></li>\n            <li><a href=\"https://alexwlchan.net/2025/personal-archive-of-the-web/#warc_or_wacz\">Why not WARC or WACZ?</a></li>\n          </ul>\n        </li>\n        <li>\n          <a href=\"https://alexwlchan.net/2025/personal-archive-of-the-web/#howto\">How do I save a local copy of each web page?</a>\n          <ul>\n            <li><a href=\"https://alexwlchan.net/2025/personal-archive-of-the-web/#saving_one_page\">Saving a single web page by hand</a></li>\n            <li><a href=\"https://alexwlchan.net/2025/personal-archive-of-the-web/#deleting_the_junk\">Deleting all the junk</a></li>\n            <li><a href=\"https://alexwlchan.net/2025/personal-archive-of-the-web/#templates\">Using templates for repeatedly-bookmarked sites</a></li>\n            <li><a href=\"https://alexwlchan.net/2025/personal-archive-of-the-web/#backfilling\">Backfilling my existing bookmarks</a></li>\n            <li><a href=\"https://alexwlchan.net/2025/personal-archive-of-the-web/#backups\">Backing up the backups</a></li>\n          </ul>\n        </li>\n        <li>\n          <a href=\"https://alexwlchan.net/2025/personal-archive-of-the-web/#why_not_automation\">Why not use automated tools?</a>\n        </li>\n        <li>\n          <a href=\"https://alexwlchan.net/2025/personal-archive-of-the-web/#what_i_learnt\">What I learnt about archiving the web</a>\n          <ul>\n            <li><a href=\"https://alexwlchan.net/2025/personal-archive-of-the-web/#defunct_services\">Lots of the web is built on now-defunct services</a></li>\n            <li><a href=\"https://alexwlchan.net/2025/personal-archive-of-the-web/#silent_changes\">Just because the site is up, doesn\u2019t mean it\u2019s right</a></li>\n            <li><a href=\"https://alexwlchan.net/2025/personal-archive-of-the-web/#link_rot\">Many sites do a poor job of redirects</a></li>\n            <li><a href=\"https://alexwlchan.net/2025/personal-archive-of-the-web/#lazy_loading\">Images are becoming more efficient, but harder to preserve</a></li>\n            <li><a href=\"https://alexwlchan.net/2025/personal-archive-of-the-web/#boundary\">There\u2019s no clearly-defined boundary of what to collect</a></li>\n          </ul>\n        </li>\n        <li><a href=\"https://alexwlchan.net/2025/personal-archive-of-the-web/#conclusion\">Should you do this?</a></li>\n      </ul>\n    </li>\n    <li>\n      <a href=\"https://alexwlchan.net/2025/personal-archive-of-the-web/\"><strong>Learning how to make websites by reading two thousand web pages</strong></a> (coming 26 May) \u2013 everything I learnt from reading the source code of the web pages I saved.\n    </li>\n    <li>\n      <a href=\"https://alexwlchan.net/2025/personal-archive-of-the-web/\"><strong>Some cool websites from my bookmark collection</strong></a> (coming 2 June) \u2013 some websites which are doing especially fun or clever things with the web.\n    </li>\n  </ol>\n</blockquote>\n\n\n\n<hr />\n\n<h2 id=\"requirements\">What do I want from a web archive?</h2>\n\n<p>I\u2019m building a <em>personal</em> web archive \u2013 it\u2019s just for me.\nI can be very picky about what it contains and how it works, because I\u2019m the only person who\u2019ll read it or save pages.\nIt\u2019s not a public resource, and nobody else will ever see it.</p>\n\n<p>This means it\u2019s quite different to what I\u2019d do in a professional or institutional setting.\nThere, the priorities are different: automation over manual effort, immutability over editability, and a strong preference for content that can be shared or made public.</p>\n\n<h3 id=\"i-want-a-complete-copy-of-every-web-page\">I want a complete copy of every web page</h3>\n\n<p>I want my archive to have a copy of every page I\u2019ve bookmarked, and for each copy to be a good substitute for the original.\nIt should include everything I need to render the page \u2013 text, images, videos, styles, and so on.</p>\n\n<p>If the original site changes or goes offline, I should still be able to see the page as I saved it.</p>\n\n<h3 id=\"i-want-the-archive-to-live-on-my-computer\">I want the archive to live on my computer</h3>\n\n<p>I don\u2019t want to rely on an online service which could break, change, or be shut down.</p>\n\n<p>I learnt this the hard way with Pinboard.\nI was paying for an <a href=\"https://pinboard.in/faq/#archiving\">archiving account</a>, which promised to save a copy of all my bookmarks.\nBut in recent years it became unreliable \u2013 sometimes it would fail to archive a page, and sometimes I couldn\u2019t retrieve a supposedly saved page.</p>\n\n<h3 id=\"it-should-be-easy-to-save-new-pages\">It should be easy to save new pages</h3>\n\n<p>I save a couple of new bookmarks a week.\nI want to keep this archive up-to-date, and I don\u2019t want adding pages to be a chore.</p>\n\n<h3 id=\"it-should-support-private-or-paywalled-pages\">It should support private or paywalled pages</h3>\n\n<p>I read a lot of pages which aren\u2019t on the public web, stuff behind paywalls or login screens.\nI want to include these in my web archive.</p>\n\n<p>Many web archives only save public content \u2013 either because they can\u2019t access private content to save, or they couldn\u2019t share if it they did.\nThis makes it even more important that I keep my own copy of private pages, because I may not find another.</p>\n\n<h3 id=\"it-should-be-possible-to-edit-snapshots\">It should be possible to edit snapshots</h3>\n\n<p>This is both additive and subtractive.</p>\n\n<p>Web pages can embed external resources, and sometimes I want those resources in my archive.\nFor example, suppose somebody publishes a blog post about a conference talk, and embeds a YouTube video of them giving the talk.\nI want to download the video, not just the YouTube embed code.</p>\n\n<p>Web pages also contain a lot of junk that I don\u2019t care about saving \u2013 ads, tracking, pop-ups, and more.\nI want to cut all that stuff out, and just keep the useful parts.\nIt\u2019s like taking clippings from a magazine: I want the article, not the ads wrapped around it.</p>\n\n<hr />\n\n<h2 id=\"what_it_looks_like\">What does my web archive look like?</h2>\n\n<p>I treat my archived bookmarks like the bookmarks site itself: as static files, saved in folders on my local filesystem.</p>\n\n<h3 id=\"static_folders\">A static folder for every page</h3>\n\n<p>For every page, I have a folder with the HTML, stylesheets, images, and other linked files.\nEach folder is a self-contained \u201cmini-website\u201d.\nIf I want to look at a saved page, I can just open the HTML file in my web browser.</p>\n\n<figure>\n  \n\n\n\n\n  \n  <source type=\"image/png\" />\n  \n  <source media=\"(prefers-color-scheme: dark)\" type=\"image/png\" />\n  \n  <img alt=\"Screnshot of a folder containing an HTML page, and two folders with linked resources: images and static.\" class=\"screenshot dark_aware\" src=\"https://alexwlchan.net/images/2025/bookmarks/mini_website_1x.png\" width=\"600\" />\n\n\n\n\n  <figcaption>\n    The files for <a href=\"https://preshing.com/20110926/high-resolution-mandelbrot-in-obfuscated-python/\">a\u00a0single web page</a>, saved in a folder in my archive.\n    I flatten the structure of each website into top-level folders like <code>images</code> and <code>static</code>, which keeps things simple and readable.\n    I don\u2019t care about the exact URL paths from the original site.\n  </figcaption>\n</figure>\n\n<p>Any time the HTML refers to an external file, I\u2019ve changed it to fetch the file from the local folder rather than the original website.\nFor example, the original HTML might have an <code class=\"language-plaintext highlighter-rouge\">&lt;img&gt;</code> tag that loads an image from <code class=\"language-plaintext highlighter-rouge\">https://preshing.com/~img/poster-wall.jpg</code>, but in my local copy I\u2019d change the <code class=\"language-plaintext highlighter-rouge\">&lt;img&gt;</code> tag to load from <code class=\"language-plaintext highlighter-rouge\">images/poster-wall.jpg</code>.</p>\n\n<p>I like this approach because it\u2019s using open, standards-based web technology, and this structure is simple, durable, and easy to maintain.\nThese folder-based snapshots will likely remain readable for the rest of my life.</p>\n\n<h3 id=\"warc_or_wacz\">Why not WARC or WACZ?</h3>\n\n<p>Many institutions store their web archives as <a href=\"https://en.wikipedia.org/wiki/WARC_(file_format)\">WARC</a> or <a href=\"https://specs.webrecorder.net/wacz/1.1.1/\">WACZ</a>, which are file formats specifically designed to store preserved web pages.</p>\n\n<p>These files contain the saved page, as well as extra information about how the archive was created \u2013 useful context for future researchers.\nThis could include the HTTP headers, the IP\u00a0address, or the name of the software that created the archive.</p>\n\n<p>You can only open WARC or WACZ files with specialist \u201cplayback\u201d software, or by unpacking the files from the archive.\nBoth file formats are open standards, so theoretically you could write your own software to read them \u2013 archives saved this way aren\u2019t trapped in a proprietary format \u2013 but in practice, you\u2019re picking from a small set of tools.</p>\n\n<p>In my personal archive, I don\u2019t need that extra context, and I don\u2019t want to rely on a limited set of tools.\nIt\u2019s also difficult to edit WARC files, which is one of my requirements.\nI can\u2019t easily open them up and delete all the ads, or add extra files.</p>\n\n<p>I prefer the flexibility of files and folders \u2013 I can open HTML files in any web browser, make changes with ease, and use whatever tools I like.</p>\n\n<hr />\n\n<h2 id=\"howto\">How do I save a local copy of each web page?</h2>\n\n<p>I save every page by hand, then I check it looks good \u2013 that I\u2019ve saved all the external resources like images and stylesheets.</p>\n\n<p>This manual inspection gives me the peace of mind to know that I really have saved each web page, and that it\u2019s a high quality copy.\nI\u2019m not going to open a snapshot in two years time only to discover that I\u2019m missing a key image or illustration.</p>\n\n<p>Let\u2019s go through that process in more detail.</p>\n\n<h3 id=\"saving_one_page\">Saving a single web page by hand</h3>\n\n<p>I start by saving the HTML file, using the \u201cSave As\u201d button in my web browser.</p>\n\n<p>I open that file in my web browser and my text editor.\nUsing the browser\u2019s developer tools, I look for external files that I need to save locally \u2013 stylesheets, fonts, images, and so on.\nI download the missing files, edit the HTML in my text editor to point at the local copy, then reload the page in the browser to see the result.\nI keep going until I\u2019ve downloaded everything, and I have a self-contained, offline copy of the page.</p>\n\n<p>Most of my time in the developer tools is spent in two tabs.</p>\n\n<p>I look at the <em>Network tab</em> to see what files the page is loading.\nAre they being served from my local disk, or fetched from the Internet?\nI want everything to come from disk.</p>\n\n<figure>\n  \n\n\n\n\n  \n  <source type=\"image/png\" />\n  \n  <source media=\"(prefers-color-scheme: dark)\" type=\"image/png\" />\n  \n  <img alt=\"The network tab in my browser developer tools, which has a list of files loaded by the page, and the domain they were loaded from. A lot of these files were loaded from remote servers.\" class=\"screenshot dark_aware\" src=\"https://alexwlchan.net/images/2025/bookmarks/network_tab_1x.png\" width=\"600\" />\n\n\n\n\n  <figcaption>\n    This HTML file is making a lot of external network requests \u2013 I have more work to do!\n  </figcaption>\n</figure>\n\n<p>I check the <em>Console tab</em> for any errors loading the page \u2013 some image that can\u2019t be found, or a JavaScript file that didn\u2019t load properly.\nI want to fix all these errors.</p>\n\n<figure>\n  \n\n\n\n\n  \n  <source type=\"image/png\" />\n  \n  <source media=\"(prefers-color-scheme: dark)\" type=\"image/png\" />\n  \n  <img alt=\"The console tab in my browser developer tools, which has a lot of messages highlighted in red about resources that weren't loaded properly.\" class=\"screenshot dark_aware\" src=\"https://alexwlchan.net/images/2025/bookmarks/console_errors_1x.png\" width=\"600\" />\n\n\n\n\n  <figcaption>\n    So much red!\n  </figcaption>\n</figure>\n\n<p>I spend a lot of time reading and editing HTML by hand.\nI\u2019m fairly comfortable working with other people\u2019s code, and it typically takes me a few minutes to save a page.\nThis is fine for the handful of new pages I save every week, but it wouldn\u2019t scale for a larger archive.</p>\n\n<p>Once I\u2019ve downloaded everything the page needs, eliminated external requests, and fixed the errors, I have my snapshot.</p>\n\n<h3 id=\"deleting_the_junk\">Deleting all the junk</h3>\n\n<p>As I\u2019m saving a page, I cut away all the stuff I don\u2019t want.\nThis makes my snapshots smaller, and pages often shrank by 10\u201320\u00d7.\nThe junk I deleted includes:</p>\n\n<ul>\n  <li>\n    <p><strong>Ads.</strong>\nSo many ads.\nI found one especially aggressive plugin that inserted an advertising <code class=\"language-plaintext highlighter-rouge\">&lt;div&gt;</code> between every single paragraph.</p>\n  </li>\n  <li>\n    <p><strong>Banners for time-sensitive events.</strong>\nNews tickers, announcements, limited-time promotions, and in one case, a museum\u2019s bank holiday opening hours.</p>\n  </li>\n  <li>\n    <p><strong>Inline links to related content.</strong>\nThere are many articles where, every few paragraphs, you get a promo for a different article.\nI find this quite distracting, especially as I\u2019m already reading the site!\nI deleted all those, so my saved articles are just the text.</p>\n  </li>\n  <li>\n    <p><strong>Cookie notices, analytics, tracking, and other services for gathering \u201cconsent\u201d.</strong>\nI don\u2019t care what tracking tools a web page was using when I saved it, and they\u2019re a complete waste of space in my personal archive.</p>\n  </li>\n</ul>\n\n<!-- My \"favourite\" was a Squarespace site that loaded over 25MB of JavaScript to render a 400-word essay with no images. -->\n\n<p>As I was editing the page in my text editor, I\u2019d look for <code class=\"language-plaintext highlighter-rouge\">&lt;script&gt;</code> and <code class=\"language-plaintext highlighter-rouge\">&lt;iframe&gt;</code> elements.\nThese are good indicators of the stuff I want to remove \u2013 for example, most ads are loaded in iframes.\nA lot of what I save is static content, where I don\u2019t need the interactivity of JavaScript.\nI can remove it from the page and still have a useful snapshot.</p>\n\n<p>In my personal archive, I think these deletions are a clear improvement.\nSnapshots load faster, they\u2019re easier to read, and I\u2019m not preserving megabytes of junk I\u2019ll never use.\nBut I\u2019d be a lot more cautious doing this in a public context.</p>\n\n<p>Institutional web archives try to preserve web pages exactly as they were.\nThey want researchers to trust that they\u2019re seeing an authentic representation of the original page, unchanged in content or meaning.\nDeleting anything from the page, however well-intentioned, might undermine that trust \u2013 who decides what gets deleted?\nWhat\u2019s cruft to me might be a crucial clue to someone else.</p>\n\n<h3 id=\"templates\">Using templates for repeatedly-bookmarked sites</h3>\n\n<p>For big, complex websites that I bookmark often, I\u2019ve created simple HTML templates.</p>\n\n<p>When I want to save a new page, I discard the original HTML, and I just copy the text and images into the template.\nIt\u2019s a lot faster than unpicking the site\u2019s HTML every time, and I\u2019m saving the content of the article, which is what I really care about.</p>\n\n<figure class=\"comparison\">\n  \n\n\n\n\n  \n  <source type=\"image/png\" />\n  \n  <img alt=\"Screenshot of an article on the New York Times website. You can only see a headline \u2013 most of the page is taken up by an ad and a cookie banner.\" class=\"screenshot\" src=\"https://alexwlchan.net/images/2025/bookmarks/nytimes_theirs_1x.png\" width=\"300\" />\n\n\n\n\n  \n\n\n\n\n  \n  <source type=\"image/png\" />\n  \n  <img alt=\"Screenshot of the same article saved in my archive. You can see the main illustration, the headline, and two paragraphs of the article.\" class=\"screenshot\" src=\"https://alexwlchan.net/images/2025/bookmarks/nytimes_mine_1x.png\" width=\"300\" />\n\n\n\n\n  <figcaption>\n    Here\u2019s an example from the New York Times.\n    You can tell which page is the <a href=\"https://www.nytimes.com/2016/03/27/opinion/sunday/my-mothers-garden.html\">real article</a>, because you have to click through two dialogs and scroll past an ad before you see any text.\n  </figcaption>\n</figure>\n\n<p>I was inspired by AO3 (the Archive of Our Own), a popular fanfiction site.\nYou can <a href=\"https://archiveofourown.org/faq/downloading-fanworks?language_id=en\">download copies</a> of every story in multiple formats, and they believe in it so strongly that <em>everything</em> published on their site can be downloaded.\nAuthors don\u2019t get to opt out.</p>\n\n<p>An HTML download from AO3 looks different to the styled version you\u2019d see browsing the web:</p>\n\n<figure class=\"comparison\">\n  \n\n\n\n\n  \n  <source type=\"image/png\" />\n  \n  <img alt=\"Screenshot of a story \u2018The Jacket Bar\u2019 on AO3. There are styles and colours, and a red AO3 site header at the top fo the page.\" class=\"screenshot\" src=\"https://alexwlchan.net/images/2025/bookmarks/ao3_theirs_1x.png\" width=\"300\" />\n\n\n\n\n  \n\n\n\n\n  \n  <source type=\"image/png\" />\n  \n  <img alt=\"Screenshot of the same story, as an HTML download. It\u2019s an unstyled HTML page, with Times New Roman font and default blue links.\" class=\"screenshot\" src=\"https://alexwlchan.net/images/2025/bookmarks/ao3_mine_1x.png\" width=\"300\" />\n\n\n\n\n</figure>\n\n<p>But the difference is only cosmetic \u2013 both files contain the full text of the story, which is what I really care about.\nI don\u2019t care about saving a visual snapshot of what AO3 looks like.</p>\n\n<p>Most sites don\u2019t offer a plain HTML download of their content, but I know enough HTML and CSS to create my own templates.\nI have a dozen or so of these templates, which make it easy to create snapshots of sites I visit often \u2013 sites like Medium, Wired, and the New York Times.</p>\n\n<h3 id=\"backfilling\">Backfilling my existing bookmarks</h3>\n\n<p>When I decided to build a new web archive by hand, I already had partial collections from several sources \u2013 Pinboard, the Wayback Machine, and some personal scripts.</p>\n\n<p>I gradually consolidated everything into my new archive, tackling a few bookmarks a day: fixing broken pages, downloading missing files, deleting ads and other junk.\nI had over 2000\u00a0bookmarks, and it took about a year to migrate all of them.\nNow I have a collection where I\u2019ve checked everything by hand, and I know I have a complete set of local copies.</p>\n\n<p>I wrote some Python scripts to automate common cleanup tasks, and I used regular expressions to help me clean up the mass of HTML.\nThis code is too scrappy and specific to be worth sharing, but I wanted to acknowledge my use of automation, albeit at a lower level than most archiving tools.\nThere was a lot of manual effort involved, but it wasn\u2019t entirely by hand.</p>\n\n<p>Now I\u2019m done, there\u2019s only one bookmark that seems conclusively lost \u2013 a review of <em>Rogue One</em> on Dreamwidth, where the only capture I can find is a content warning interstitial.</p>\n\n<p>I consider this a big success, but it was also a reminder of how fragmented our internet archives are.\nMany of my snapshots are \u201cfranken-archives\u201d \u2013 stitched together from multiple sources, combining files that were saved years apart.</p>\n\n<h3 id=\"backups\">Backing up the backups</h3>\n\n<p>Once I have a website saved as a folder, that folder gets backed up like all my other files.</p>\n\n<p>I use <a href=\"https://en.wikipedia.org/wiki/Time_Machine_(macOS)\">Time Machine</a> and <a href=\"https://bombich.com/\">Carbon Copy Cloner</a> to back up to a pair of external SSDs, and <a href=\"https://secure.backblaze.com/r/01h8yj\">Backblaze</a> to create a cloud backup that lives outside my house.</p>\n\n<hr />\n\n<h2 id=\"why_not_automation\">Why not use automated tools?</h2>\n\n<p>I\u2019m a big fan of automated tools for archiving the web, I think they\u2019re an essential part of web preservation, and I\u2019ve used many of them in the past.</p>\n\n<p>Tools like <a href=\"https://archivebox.io/\">ArchiveBox</a>, <a href=\"https://webrecorder.net/\">Webrecorder</a>, and the <a href=\"https://web.archive.org/\">Wayback Machine</a> have preserved enormous chunks of the web \u2013 pages that would otherwise be lost.\nI paid for a Pinboard <a href=\"https://pinboard.in/faq/#archiving\">archiving account</a> for a decade, and I search the Internet Archive at least once a week.\nI\u2019ve used command-line tools like <a href=\"https://www.gnu.org/software/wget/manual/wget.html#Recursive-Download\">wget</a>, and last year I wrote my own tool to <a href=\"https://alexwlchan.net/2024/creating-a-safari-webarchive/\">create Safari webarchives</a>.</p>\n\n<p>The size and scale of today\u2019s web archives are only possible because of automation.</p>\n\n<p>But automation isn\u2019t a panacea, it\u2019s a trade-off.\nYou\u2019re giving up accuracy for speed and volume.\nIf nobody is reviewing pages as they\u2019re archived, it\u2019s more likely that they\u2019ll contain mistakes or be missing essential files.</p>\n\n<p>When I reviewed my old Pinboard archive, I found a lot of pages that <a href=\"https://pinboard.in/faq/#archival_quality\">weren\u2019t archived properly</a> \u2013 they had missing images, or broken styles, or relied on JavaScript from the original site.\nThese were web pages I really care about, and I thought I had them saved, but that was a false sense of security.\nI\u2019ve found issues like this whenever I\u2019ve used automated tools to archive the web.</p>\n\n<p>That\u2019s why I decided to create my new archive manually \u2013 it\u2019s much slower, but it gives me the comfort of knowing that I have a good copy of every page.</p>\n\n<hr />\n\n<h2 id=\"what_i_learnt\">What I learnt about archiving the web</h2>\n\n<h3 id=\"defunct_services\">Lots of the web is built on now-defunct services</h3>\n\n<p>I found many pages that rely on third-party services that no longer exist, like:</p>\n\n<ul>\n  <li>Photo sharing sites \u2013 some I\u2019d heard of (Twitpic, Yfrog), others that were new to me (phto.ch)</li>\n  <li>Link redirection services \u2013 URL shorteners and sponsored redirects</li>\n  <li>Social media sharing buttons and embeds</li>\n</ul>\n\n<p>This means that if you load the live site, the main page loads, but key resources like images and scripts are broken or missing.</p>\n\n<h3 id=\"silent_changes\">Just because the site is up, doesn\u2019t mean it\u2019s right</h3>\n\n<p>One particularly insidious form of breakage is when the page still exists, but the content has changed.\nHere\u2019s an example: a screenshot from an iTunes tutorial on LiveJournal that\u2019s been replaced with an \u201c18+ warning\u201d:</p>\n\n<figure>\n  <div>\n    \n\n\n\n\n  \n  <source type=\"image/png\" />\n  \n  <img alt=\"Screenshot of a text box in the iTunes UI, where you can enter a year.\" class=\"screenshot\" src=\"https://alexwlchan.net/images/2025/bookmarks/001akef1_1x.png\" width=\"78\" />\n\n\n\n\n    \n\n\n\n\n  \n  <source type=\"image/avif\" />\n  \n  <source type=\"image/webp\" />\n  \n  <source type=\"image/png\" />\n  \n  <img alt=\"A warning from LiveJournal that this image is \u201c18+\u201d.\" src=\"https://alexwlchan.net/images/2025/bookmarks/122942_original_1x.png\" width=\"300\" />\n\n\n\n\n  </div>\n</figure>\n\n<p>This kind of failure is hard to detect automatically \u2013 the server is returning a valid response, just not the one you want.\nThat\u2019s why I wanted to look at every web page with my eyes, and not rely on a computer to tell me it was saved correctly.</p>\n\n<h3 id=\"link_rot\">Many sites do a poor job of redirects</h3>\n\n<p>I was surprised by how many web pages still exist, but the original URLs no longer work, especially on large news sites.\nMany of my old bookmarks now return a 404, but if you search for the headline, you can find the story at a completely different URL.</p>\n\n<p>I find this frustrating and disappointing.\nWhenever I\u2019ve restructured this site, I always set up redirects because I\u2019m an old-school web nerd and I <a href=\"https://www.w3.org/Provider/Style/URI.html\">think URLs are cool</a> \u2013 but redirects aren\u2019t just about making me feel good.\nKeeping links alive makes it easier to find stuff in your back catalogue \u2013 without redirects, most people who encounter a broken link will assume the page was deleted, and won\u2019t dig further.</p>\n\n<h3 id=\"lazy_loading\">Images are getting easier to serve, harder to preserve</h3>\n\n<p>When the web was young, images were simple.\nYou wrote an <code>&lt;img\u00a0src=\"\u2026\"&gt;</code> tag in your HTML, and that was that.</p>\n\n<p>Today, images are more complicated.\nYou can provide multiple versions of the same image, or control when images are loaded.\nThis can make web pages more efficient and accessible, but harder to preserve.</p>\n\n<p>There are two features that stood out to me:</p>\n\n<ol>\n  <li>\n    <p><a href=\"https://developer.mozilla.org/en-US/docs/Web/Performance/Guides/Lazy_loading\"><em>Lazy loading</em></a> is a technique where a web page doesn\u2019t load images or resources until they\u2019re needed \u2013 for example, not loading an image at the bottom of an article until you scroll down.</p>\n\n    <p>Modern lazy loading is easy with <a href=\"https://developer.mozilla.org/en-US/docs/Web/HTML/Reference/Elements/img#loading\"><code class=\"language-plaintext highlighter-rouge\">&lt;img loading=\"lazy\"&gt;</code></a>, but there are lots of sites that were built before that attribute was widely-supported.\nThey have their own code for lazy loading, and every site behaves a bit differently.\nFor example, a page might load a low-res image first, then swap it out for a high-res version with JavaScript.\nBut automated tools can\u2019t always run that JavaScript, so they only capture the low-res image.</p>\n  </li>\n  <li>\n    <p>The <a href=\"https://developer.mozilla.org/en-US/docs/Web/HTML/Reference/Elements/picture\"><em><code class=\"language-plaintext highlighter-rouge\">&lt;picture&gt;</code> tag</em></a> allows pages to specify multiple versions of an image.\nFor example:</p>\n\n    <ul>\n      <li>A page could send a high-res image to laptops, and a low-res images to phones.\nThis is more efficient; you\u2019re not sending an unnecessarily large image to a small screen.</li>\n      <li>A page could send different images based on your colour scheme.\nYou could see a graph on a white background if you use light mode, or on black if you use dark mode.</li>\n    </ul>\n\n    <p>If you preserve a page, which images should you get?\nAll of them?\nJust one?\nIf so, which one?\nFor my personal archive, I always saved the highest resolution copy of each image, but I\u2019m not sure that\u2019s the best answer in every case.</p>\n\n    <p>On the modern web, pages may not look the same for everyone \u2013 different people can see different things.\nWhen you\u2019re preserving a page, you need to decide which version of it you want to save.</p>\n  </li>\n</ol>\n\n<h3 id=\"boundary\">There\u2019s no clearly-defined boundary of what to collect</h3>\n\n<p>Once you\u2019ve saved the initial HTML page, what else do you save?</p>\n\n<p>Some automated tools will aggressively follow every link on the page, and every link on those pages, and every link after that, and so on.\nOthers will follow simple heuristics, like \u201csave everything linked from the first page, but no further\u201d, or \u201csave everything up to a fixed size limit\u201d.</p>\n\n<p>I struggled to come up with a good set of heuristics for my own approach, and I was often making decisions on a case-by-case basis.\nHere are two examples:</p>\n\n<ul>\n  <li>\n    <p>I\u2019ve bookmarked blog posts about conferences talks, where authors embed a YouTube video of them giving the talk.\nI think the video is a key part of the page, so I want to download it \u2013 but \u201cdownload all embeds and links\u201d would be a very expensive rule.</p>\n  </li>\n  <li>\n    <p>I\u2019ve bookmarked blog posts that comment on scientific papers.\nUsually the link to the original paper doesn\u2019t go directly to the PDF, but to a landing page on a site like arXiv.</p>\n\n    <p>I want to save the PDF because it\u2019s important context for the blog post, but now I\u2019m saving something two clicks away from the original post \u2013 which would be even more expensive if applied as a universal rule.</p>\n  </li>\n</ul>\n\n<p>This is another reason why I\u2019m really glad I build my archive by hand \u2013 I could make different decisions based on the content and context.</p>\n\n<hr />\n\n<h2 id=\"conclusion\">Should you do this?</h2>\n\n<p>I can recommend having a personal web archive.\nJust like I keep paper copies of my favourite books, I now keep local copies of my favourite web pages.\nI know that I\u2019ll always be able to read them, even if the original website goes away.</p>\n\n<p>It\u2019s harder to recommend following my exact approach.\nBuilding my archive by hand took nearly a year, probably hundreds of hours of my free time.\nI\u2019m very glad I did it, I enjoyed doing it, and I like the result \u2013 but it\u2019s a big time commitment, and it was only possible because I have a lot of experience building websites.</p>\n\n<p>Don\u2019t let that discourage you \u2013 a web archive doesn\u2019t have to be fancy or extreme.</p>\n\n<p>If you take a few screenshots, save some PDFs, or download HTML copies of your favourite fic, that\u2019s a useful backup.\nYou\u2019ll have something to look at if the original web page goes away.</p>\n\n<p>If you want to scale up your archive, look at automated tools.\nFor most people, they\u2019re a better balance of cost and reward than saving and editing HTML by hand.\nBut you don\u2019t need to, and even a folder with just a few files is better than nothing.</p>\n\n<p>When I was building my archive \u2013 and reading all those web pages \u2013 I learnt a lot about how the web is built.\nIn part\u00a03 of this series, I\u2019ll share what that process taught me about making websites.</p>\n\n<p>If you\u2019d like to know when that article goes live, <a href=\"https://alexwlchan.net/subscribe/\">subscribe to my RSS feed or newsletter</a>!</p>\n\n\n    <p>[If the formatting of this post looks odd in your feed reader, <a href=\"https://alexwlchan.net/2025/personal-archive-of-the-web/\">visit the original article</a>]</p>"
            ],
            "link": "https://alexwlchan.net/2025/personal-archive-of-the-web/?utm_source=rss",
            "publishedAt": "2025-05-19",
            "source": "Alex Chan",
            "summary": "How I built a web archive by hand, the tradeoffs between manual and automated archiving, and what I learnt about preserving the web.",
            "title": "Building a personal archive of the web, the\u00a0slow\u00a0way"
        },
        {
            "content": [
                "<div class=\"trix-content\">\n  <div>Have you thought about doing the opposite of whatever you're doing or considering? It's a really helpful way to test your assumptions and your values. What does the opposite look like, how would it work?</div><div><br />It's so easy to get stuck in a groove of what works, what you believe to be right. But helpful assumptions have a half-life, just like facts. And it's ever so easy to miss the shift when circumstances change, if you're not regularly stress-testing your core beliefs.</div><div><br />That doesn't mean you're just a flag in the wind, blowing whichever way. But it does mean having enough intellectual humility and creative flexibility to consider that what you believe to be true about your business, about your team, about your technology might not be so.<br /><br /></div><div>We did this a while back with full-time managers. We'd been working for nearly two decades without any, but exactly because it'd been so long, <a href=\"https://world.hey.com/dhh/we-once-more-have-no-full-time-managers-at-37signals-f8611085\">we were drawn to try the opposite</a>, just to see what we might have missed. So we did. Hired a few full-time managers to help us test that assumption for a few years.</div><div><br />In the end, we decided that <a href=\"https://signalvnoise.com/posts/1430-hire-managers-of-one\">our managers-of-one culture</a> worked better, but it wasn't a given at the outset. To try the opposite, you really have to believe that you might have been wrong.<br /><br /></div><div>Because you're wrong about something. I guarantee it. We all are.</div>\n</div>"
            ],
            "link": "https://world.hey.com/dhh/have-you-tried-the-exact-opposite-1d55b7b5",
            "publishedAt": "2025-05-19",
            "source": "DHH",
            "summary": "<div class=\"trix-content\"> <div>Have you thought about doing the opposite of whatever you're doing or considering? It's a really helpful way to test your assumptions and your values. What does the opposite look like, how would it work?</div><div><br />It's so easy to get stuck in a groove of what works, what you believe to be right. But helpful assumptions have a half-life, just like facts. And it's ever so easy to miss the shift when circumstances change, if you're not regularly stress-testing your core beliefs.</div><div><br />That doesn't mean you're just a flag in the wind, blowing whichever way. But it does mean having enough intellectual humility and creative flexibility to consider that what you believe to be true about your business, about your team, about your technology might not be so.<br /><br /></div><div>We did this a while back with full-time managers. We'd been working for nearly two decades without any, but exactly because it'd been so long, <a href=\"https://world.hey.com/dhh/we-once-more-have-no-full-time-managers-at-37signals-f8611085\">we were drawn to try the opposite</a>, just to see what we might have missed. So we did. Hired a few full-time managers to help us test that assumption for a few years.</div><div><br />In the end, we decided that <a href=\"https://signalvnoise.com/posts/1430-hire-managers-of-one\">our managers-of-one culture</a>",
            "title": "Have you tried the exact opposite?"
        },
        {
            "content": [
                "<p>Alice and Bob are driving through the desert.</p>\n\n<p><strong>Alice</strong>: Looks dry.</p>\n\n<p><strong>Bob</strong>: That\u2019s wrong, what we see ahead is caused by the sun heating up the road. While the speed of light is constant in vacuum, when light moves through matter, the atoms emit new light that destructively interferes with the old light, in effect causing a \u201cdelay\u201d. This happens more with more atoms, meaning light travels slower through denser media.</p>\n\n<p><strong>Alice</strong>: OK, but\u2014</p>\n\n<p><strong>Bob</strong>: So when the sun heats up the road, this creates a layer of thin warm air with denser cooler air higher up. As light passes through these layers, it refracts upwards towards the denser cooler layer. If conditions are right, this can bend the light back up towards your eyes. Does that make sense?</p>\n\n<p><strong>Alice</strong>: Yeah, but\u2014</p>\n\n<p><strong>Bob</strong>: Now you might ask, \u201cWhy does this look like <em>water</em>?\u201d The situations seem similar at first, with two fluids of different densities. But think about it: With an ocean, the dense water is <em>below</em> the thin air. While, in front of you, the dense cool air is <em>above</em> the thin warm air. The situations are actually reversed!</p>\n\n<p><strong>Alice</strong>: Please just\u2014</p>\n\n<p><strong>Bob</strong>: With an ocean, there\u2019s a sharp increase in density where the air meets the water. The Fresnel equations say that when light hits such a discontinuity at an angle, most is <em>reflected</em> off the surface. Whereas with the air in front of us, there\u2019s a small and gradual decrease in density, meaning the light is slowly <em>refracted</em> <em>through</em> the lighter warm air and gradually bent back up. In both cases, the mixing in the fluids causes a shimmering effect.</p>\n\n<p><strong>Alice</strong>:</p>\n\n<p><strong>Bob</strong>: So yeah. Wrong. That\u2019s just a heat mirage.</p>\n\n<p>Meanwhile, the desert flies by, bone dry.</p>\n\n<p><img alt=\"mirage\" src=\"https://dynomight.net/img/heat/a_mirage_arizona_1986.jpg\" /></p>\n\n<p>Often on the internet, someone will make a thing and someone else will make a reply with this pattern:</p>\n\n<ol>\n  <li>\u201cThat\u2019s wrong.\u201d</li>\n  <li>(Various smart-sounding factual statements.)</li>\n</ol>\n\n<p>The problem, of course, is that this never explains <em>why</em> the original thing is wrong. This is a fully-general pattern for refuting anything, one that does not require the existence of any mistake. As far as I can tell, this pattern doesn\u2019t have a name. So I suggest <strong>\u201cHeat Mirage\u201d</strong>.</p>\n\n<p>Often, all the smart-sounding factual statements are totally correct and insightful. So other people tend to see them and think, \u201cWow, this person really knows what they\u2019re talking about. Clearly the original thing is dumb and bad. I\u2019m not going to waste my time going through <em>that</em>!\u201d</p>\n\n<p>Sometimes this leads to a whole gang of people agreeing with the smart-sounding statements and lamenting the original thing\u2019s wrongness. Someone might chime in with \u201cUhh, nothing you said contradicts anything?\u201d or \u201cIt seems like you\u2019re just annoyed that your personal fixation wasn\u2019t the main subject of the thing?\u201d But this rarely turns the tide.</p>\n\n<p>What makes the Heat Mirage pattern so pernicious is that there\u2019s seldom any ill intent. Someone just neglected for some reason to explain the \u201cmistake\u201d they were refuting. They might even be trying to be \u201cnice\u201d by avoiding too much criticism or negativity.</p>\n\n<p>One of the tough lessons of life is that when you try to be \u201cnice\u201d to someone instead of being direct, you often end up hurting them, because you don\u2019t understand their needs. If in doubt, it\u2019s best to err on the side of (tactful) directness. And, on the internet, one should always be in doubt.</p>\n\n<p>Trust me, anyone who makes a thing on the internet is <em>100% fully aware</em> that other people are likely to point out flaws. I think that\u2019s great, and we should all get comfortable with our own fallibility. But it\u2019s best when the flaws that are pointed out are <em>real</em> flaws that actually exist.</p>\n\n<p>So, if you want to say someone is wrong, say why. Or, if you don\u2019t want to do that, consider starting off with, \u201cHere\u2019s something I find interesting\u2026\u201d</p>"
            ],
            "link": "https://dynomight.net/heat/",
            "publishedAt": "2025-05-19",
            "source": "Dynomight",
            "summary": "<p>Alice and Bob are driving through the desert.</p> <p><strong>Alice</strong>: Looks dry.</p> <p><strong>Bob</strong>: That\u2019s wrong, what we see ahead is caused by the sun heating up the road. While the speed of light is constant in vacuum, when light moves through matter, the atoms emit new light that destructively interferes with the old light, in effect causing a \u201cdelay\u201d. This happens more with more atoms, meaning light travels slower through denser media.</p> <p><strong>Alice</strong>: OK, but\u2014</p> <p><strong>Bob</strong>: So when the sun heats up the road, this creates a layer of thin warm air with denser cooler air higher up. As light passes through these layers, it refracts upwards towards the denser cooler layer. If conditions are right, this can bend the light back up towards your eyes. Does that make sense?</p> <p><strong>Alice</strong>: Yeah, but\u2014</p> <p><strong>Bob</strong>: Now you might ask, \u201cWhy does this look like <em>water</em>?\u201d The situations seem similar at first, with two fluids of different densities. But think about it: With an ocean, the dense water is <em>below</em> the thin air. While, in front of you, the dense cool air is <em>above</em> the thin warm air. The situations are actually reversed!</p> <p><strong>Alice</strong>: Please just\u2014</p> <p><strong>Bob</strong>: With an ocean, there\u2019s a sharp",
            "title": "The Heat Mirage: My least-favorite internet maneuver"
        },
        {
            "content": [
                "<h1>LLM Memory</h1>\n<p>I've been thinking about LLM memory since GPT3 came out.</p>\n<p>Back then, my LLM side project was story generation (i.e. fiction). Context windows for LLMs were tiny back then: 4K tokens, input + output. So just a few pages of text. How to write a novella if your entire knowledge of the text is only a few pages, as if you were an amnesiac author?</p>\n<p>Suppose you're writing a scene. Where does the scene take place? Your description must match any previous description of the same place and be consistent with the setting at a whole.</p>\n<p>Who's in the scene? What's the emotional state of the characters given recent events? What's some sample dialogue for those characters so you know their speaking style? What are their motivations?</p>\n<p>You start to enumerate these things you need to know just to write the simplest scenes, and you pretty quickly blow your 4K token budget. This doesn't even get into the complexity of indexing and retrieving the relevant information.</p>\n<p>How do you store a character's hair color? What if they dye their hair in the middle of the story? What if they have a flashback to when they were a blonde baby, but it darkened with age?</p>\n<p>How do you represent that the character's new acquaintance can see their current hair color, but has no way of knowing they were a blonde baby?</p>\n<p>The more you think about these questions, the more you can find edge cases where any particular memory system is limited.</p>\n<p>This post contains a wandering train of thought through some of these systems.</p>\n<h2>Reference frames</h2>\n<p><em>All knowledge has an explicit or implicit reference frame for which it is valid.</em></p>\n<p>This observation didn't come close to first in this region of thought, but I think it's among the most important, so I'm putting it first in this document.</p>\n<p>You might think &quot;Berlin is the capital of Germany&quot; is a trivial piece of knowledge you could capture in a key-value store type repository. <code>capital(germany) = berlin</code>, or <code>country(berlin) = germany</code>.</p>\n<p>But what about from 1945-1990? The capital of West Germany was Bonn. And before German unification, the question is nonsensical, though Berlin was the capital of Prussia.</p>\n<p>But the capital of Germany is Flensburg in the sci-fi alternate history <a href=\"https://en.wikipedia.org/wiki/Colonization_%28series%29\">Colonization</a> series. But only for some parts of the story, before Flensburg, it's still Berlin.</p>\n<p>If you wrote a naive knowledge-ingestion pipeline, and I fed that sci-fi book to it, it would naively create a key-value entry for the capital of Germany being Flensburg. And another one for Berlin.</p>\n<p>Temporal reference frames are the most common, i.e. facts associated with some span of time. Bonn is the capital of West Germany from 1945 to 1990. And then works of fiction essentially create alternate timelines.</p>\n<p>The other huge on is spatial reference frames. Animal brains like ours are evolved to store knowledge spatially.</p>\n<p>It's interesting that you don't have one global spatial reference frame. Like the floorplan of your house is a reference frame, but that reference frame is mostly disconnected from the reference frame of your neighborhood, and that one might be disconnected from the one of your city.</p>\n<p>You can kind of view temporal reference frames a sort of 1D spatial reference frame.</p>\n<p>You can also spatialize more abstract things, too. Imagine placing foods onto a 2D grid with saltiness on one axis and spiciness on the other axis.</p>\n<p>So there is probably a memory system out there where everything is reference frames, meta-reference frames (where the embedded objects are themselves reference frames), etc.</p>\n<p>This vein of thought is inspired by some of Jeff Hawkins' <a href=\"https://www.numenta.com/resources/books/a-thousand-brains-by-jeff-hawkins/\">Thousand Brains book</a>. The book is worth reading despite it being possible it's entirely wrong.</p>\n<h2>Vector embeddings</h2>\n<p>For the uninitiated, a vector embedding is basically where you take some text and convert it to an N-dimensional point, with the property that semantically similar pieces of text map to closer points in the N-dimensional vector space.</p>\n<p>So if you were to compute the embedding for &quot;I ate a sandwich&quot; and find all the nearest neighbors to that vector in your database, you'd find other pieces of text about eating sandwiches, and then as you get a little further, you'll find stuff about eating in general, or perhaps sandwiches in general, and then even further you'll find sentences about doing things in the first person, etc.</p>\n<p>In the GPT3 era, when I started playing with LLMs, I thought vector databases were all you need to build AGI. I changed my mind as soon as I actually tried it.</p>\n<p>It's maybe possible with some extreme engineering, but I am pretty sure there are <em>much</em> better ways to do it.</p>\n<p>An example of something vector embeddings struggle with is episodic memories. That is, you want to remember a chain of memories in series-order. How do you store the link between the memories in your vector DB? Is it another memory? What text do you embed to query it?</p>\n<p>There's probably <em>some</em> way to do it, but it's a hell of a lot easier to just put an edge between two nodes in a knowledge graph.</p>\n<p>Another major downside of vector embeddings is they are just hard to reason about. If two vectors are anomalously close or far, you don't really have much recourse or explanation. Adjust the training data of your embedding network and try again? Use a different distance metric than cosine similarity? Something else?</p>\n<p>And we have just completely ignored the reference frame problem, and I have no idea how to bolt it on to vector DBs in a generalizable way.</p>\n<h2>Knowledge graphs</h2>\n<p>Knowledge graphs are a broad set of ideas, but the basic notion is you have memories that are somehow linked together, so &quot;Berlin&quot; is somehow connected to &quot;Germany&quot;, and also to &quot;List of world capitals&quot;, and also a million other things. Major European Cities. WW2. Cold war. Prussia. Brandenberg. Etc.</p>\n<p>There are a bunch of possible implementation details.</p>\n<p>Do edges have semantic value? e.g. is &quot;Jerry Stiller&quot; connected to &quot;Ben Stiller&quot; with a &quot;father of&quot; edge? And is there a reverse edge called &quot;son of&quot;?</p>\n<p>Or are edges unlabeled, indirected, just vague connections?</p>\n<p>The answers to these questions inform your design about how much goes in the nodes, too.</p>\n<p>If you have highly semantic edges, your nodes can be tiny. e.g. &quot;Boston&quot; and &quot;1630&quot; nodes connected by a &quot;was-founded-in-year&quot; edge.</p>\n<p>If you have unlabeled connections, this is no good, because &quot;Boston&quot; will be connected to tons of nodes that look like years. Instead, you really have to store whole documents that are useful individually.</p>\n<p>You'd have a document (or many!) about the founding of Boston, those are connected to other documents about Boston's history, they might be connected to documents like &quot;Everything interesting that happened in 1630&quot; or a &quot;Big American Cities&quot; document. All of those documents are individually useful.</p>\n<p>I tend to favor the document-based approach. I feel it's more <a href=\"http://www.incompleteideas.net/IncIdeas/BitterLesson.html\">Bitter Lesson</a>-pilled. In the fullness of time, it won't be costly to deploy thousands of little cheap agents to crawl the unlabeled local graph looking for relevant information.</p>\n<p>The document approach can also pair well with vector embeddings, since you can embed the documents (or embed questions the document answers, etc) and use those embeddings to jump to a bunch of candidate nodes to start the graph traversal from.</p>\n<p>This isn't to say I hate semantic edges. Certain semantic edge feel very natural. Suppose you have two nodes that describe two events, and one event happened after another. It feels right to encode that &quot;happened after&quot; semantic information in the edge.</p>\n<p>To recover that information without that semantic edge, the nodes would need to internally contain ordering information such as the timestamp the event in the node occurred, or the latter node needs to explicitly refer to the events of the former node.</p>\n<p>In the case of AI systems, simply knowing the timestamp is probably always an option, but still feels wrong. Lots of human memories don't have timestamps but still have happens-before/after relationships for encoding episodic information.</p>\n<p>An alternative is to have both nodes connected to some document that is like &quot;Timeline of events at...&quot; that has an ordered list, a brief description of each event, and a connection to the document describing each event. So if you have any event, you can traverse up to the meta-document, and trace that through the chronology, etc.</p>\n<h2>Meta-documents</h2>\n<p>Over time, I think the majority of the items in your knowledge graph actually become meta-documents instead of &quot;source&quot; documents.</p>\n<p>Every time you run a query over the knowledge graph, you can store the results of that thing as a meta-document.</p>\n<p>Like suppose someone asks me what my 5 favorite European cities are. I've never considered that question before. I query my memory and conjure a large amount of information about European cities. Once I've loaded all this, I weight cities against each other and produce my list.</p>\n<p>I can now store that list, along with the reasoning process, as a new document! And connect it to the all those source documents.</p>\n<p>Now if someone asks me &quot;what are your 5 favorite cities in the world&quot; I can just use this document as a partial cache \u2014 it at least covers me for any European results.</p>\n<p>I've read that when you recall a memory, you actually recall the last time you recalled it (which can cause it to distort over time!). This process of producing and recalling meta-documents is kind of analogous.</p>\n<h2>Making connections</h2>\n<p>It's unclear to me if you want the connection-making process to be automatic, i.e. any documents you have in context when you produce a new document just automatically connect to the new document (and perhaps each other!), or you want something more explicit.</p>\n<p>A more explicit strategy: after producing a new document, take every pair of documents in context and ask the model if they should probably be connected. Ask what future queries could be helped by having those documents connected. Are those queries likely? If so, make the connection.</p>\n<h2>Forgetting</h2>\n<p>You actually don't want unbounded growth in connections, because then your graph becomes a lot harder to navigate.</p>\n<p>Since you will create spurious connections, because your connection-creator code won't be perfect, you need some way to garbage collect connections.</p>\n<p>There's probably a bunch of clever techniques here, but I bet you can get pretty far by just reinforcing connections you travel the most, and letting others decay and be deleted. The risk of this is there might be a few connections that always <em>look</em> promising to follow but never are. They'll get pointlessly reinforced.</p>\n<p>But don't humans do the same thing? There's stuff you just can't forget despite being useless?</p>\n<p>A smarter strategy might be to employ the LLM itself to somehow judge connections as being poor, unused, etc and just surgically excise them. If the network is getting too dense in a place, just visit a node and all its neighbors and ask &quot;which connection do you predict will be the least useful in the future&quot; and delete that one.</p>\n<p>If the graph is truly dense there, the deleted connection will just be one extra hop away through a sibling node, so the connection can always be remade if it turns out it was useful.</p>\n<p>You can also imagine some forgetting logic based on spaced-repetition type engineering. That is, the probability of a connection being deleted has some refreshable decay property.</p>\n<h2>Episodic Memory</h2>\n<p>Presumably the primary means of sensory-input document creation is the authoring of &quot;episodes&quot;. Just a narrative of what happens to the agent as it happens, with some kind of narrative start and stop points to the document.</p>\n<p>Presumably, each document is connected to the episode before and after it, so you can traverse the leaf-level timeline of sensory input if needed.</p>\n<p>But also, when the day is done, the model might do a process that looks remarkably like sleep and mull through the last day's episodes. It'll create a meta-document that is &quot;everything that happened on 2025-05-17&quot; with a summary of all the episodes, connected to all those episodes, and the meta-document from yesterday. It might go update the document &quot;Everything that has happened in 2025 so far&quot; if something particularly notable happened.</p>\n<p>It might pull out some common themes from the last day's episodes and make meta-documents about synthesized learnings on just those specific topics. Then connect to past documents about related concepts.</p>\n<p>It's worth noting that querying memory can itself be an episode. You could probably engineer this system where the vast majority (all?) documents are episodes.</p>\n<p>For example, you don't have a document titled &quot;List of European Cities I've Visited&quot;, instead, you have a document like &quot;In May 2025, I was asked by Josh what European cities I've visited, and I compiled the following list&quot;. That is, the list is individually useful, but also embedded within an episode.</p>\n<h2>Traversal</h2>\n<p>As intelligence gets cheaper and context windows get larger, it makes sense to be really liberal with what you throw into context.</p>\n<p>This strategy is dead simple but probably surprisingly good:</p>\n<ul>\n<li>All the day's episodes are in context</li>\n<li>The last 15 daily summaries</li>\n<li>The last 15 weekly summaries</li>\n<li>The last 15 monthly summaries</li>\n<li>All the yearly summaries</li>\n<li>Identify all nodes connected to any of those within 2 jumps that look promising</li>\n</ul>\n<p>This would be super wasteful, so you can improve it by:</p>\n<p>As you scan stuff, you don't need to put the full document into context. You really want to scan the document and <em>extract</em> relevant quotes and context from it.</p>\n<p>You can make the node expansion more efficient with more of a priority queue type situation than a simple breadth-first search. Just maintain a numbered list and when you expand a node, ask the model to provide insert points for each node based on how promising it looks relative to what's in the queue.</p>\n<h2>The traversal agent</h2>\n<p>How to actually design the agent doing the traversal?</p>\n<p>Consider a situation where you're looking for information about the query &quot;Which famous European queen was murdered by her nephew?&quot;. Suppose you find a document about a queen who was murdered, but it doesn't say anything about the murderer.</p>\n<p>Is this document important? Maybe! Maybe not. We need to traverse to adjacent documents now to learn more, see if anything mentions a nephew, etc.</p>\n<p>It's probably possible to do this in a straightforward agentic approach. You give the model context on what it's looking for, and it has tools to load documents, take notes, un-load documents (clear up context), can see its own action log, and can return query results when ready.</p>\n<p>You can imagine a more sophisticated system where query agents can themselves spin off sub-agents to answer sub-queries, e.g. &quot;who was the nephew of Queen Whatever?&quot;. Each agent might be given a search budget that they can allocate to sub-agents (who themselves might delegate further!).</p>\n<h2>Databases</h2>\n<p>What about SQLite? A lot of memories would work great in an SQLite table. List of world capitals. The kings of England. German-Spanish bilingual dictionary.</p>\n<p>I think it's simplest to just model these as external tool uses rather than part of the core memory system. Similar to humans! You store a memory &quot;I have an SQLite table with all the world capitals&quot; and upon retrieving that memory into context, the agent stops querying memory and goes to use that tool.</p>\n<h2>Scratchpads</h2>\n<p>For certain applications, all the memory and state you could need fits in the context window. The simplest possible memory system is just a single scratchpad of text that you append new memories to. When it fills up, you ask the model to select irrelevant stuff to delete or compact.</p>\n<p>I think in the fullness of time, this will work for those applications. But today's models are mostly not smart enough to do it well yet. It's better to impose more structure on them.</p>\n<p>For example, suppose the agent tried some course of action to accomplish the goal, but failed. That failure goes in the flat log. But later on, the log must be pruned for space. Some models will decide to prune the failure log, after all, what good is it?</p>\n<p>But then they try the failed course of action again as soon as it's pruned. You can get really dumb, loopy behavior for non-trivial tasks with this system.</p>\n<p>I think most humans would not do well if they had admin edit access to their own memories. Delete some childhood trauma? Whoops there goes a core part of your personality, now you're a different person.</p>\n<h2>Layers</h2>\n<p>It's tempting to represent all knowledge in one system, and I bet it <em>is</em> possible eventually, but from an engineering point of view, it seems to good to separate some of these.</p>\n<p>Things core to identity, motivations, personality, temperament, etc feel fundamentally different than episodic memories. I feel like they should probably be represented differently, but at least separately.</p>\n<h2>Control</h2>\n<p>How much control should an agent have over <em>what</em> it remembers? Should memory be a <em>tool</em> the agent has control over? Or something that happens implicitly?</p>\n<p>I kind of lean towards preferring implicit. Both because that's how memory feels like it works in my own mind, but also because the models aren't smart enough to have explicit control of it yet.</p>\n<p>Models are prone to overestimating their own abilities. You ask them some historical fact, give them access to a memory tool, they are just as likely to conjure an answer from within the model weights as they are to make extensive use of the tool.</p>\n<p>This is mostly fine for a sufficiently large model and sufficiently well-established historical facts, but totally insufficient for recent events that aren't in the training data.</p>\n<h2>Neural methods</h2>\n<p>Of course, all of the techniques in this post will probably <em>eventually</em> be subsumed by some fully-learned, end-to-end memory approach that is somehow just represented in vectors/weights.</p>\n<p>I have no idea what this looks like. It might be some ultra-sparse mixture of experts where you can train new experts every night and plug them in the next day. Might be some recurrent thing. Who knows.</p>\n<p>I don't have a lot to say here, so instead focus more on the things I have line-of-sight on.</p>\n<h2>Conclusion</h2>\n<p>There's no grand thesis statement to this post, it's just a ramble. Email me if you have any other memory techniques to add.</p>"
            ],
            "link": "https://grantslatton.com/llm-memory",
            "publishedAt": "2025-05-19",
            "source": "Grant Slatton",
            "summary": "<h1>LLM Memory</h1> <p>I've been thinking about LLM memory since GPT3 came out.</p> <p>Back then, my LLM side project was story generation (i.e. fiction). Context windows for LLMs were tiny back then: 4K tokens, input + output. So just a few pages of text. How to write a novella if your entire knowledge of the text is only a few pages, as if you were an amnesiac author?</p> <p>Suppose you're writing a scene. Where does the scene take place? Your description must match any previous description of the same place and be consistent with the setting at a whole.</p> <p>Who's in the scene? What's the emotional state of the characters given recent events? What's some sample dialogue for those characters so you know their speaking style? What are their motivations?</p> <p>You start to enumerate these things you need to know just to write the simplest scenes, and you pretty quickly blow your 4K token budget. This doesn't even get into the complexity of indexing and retrieving the relevant information.</p> <p>How do you store a character's hair color? What if they dye their hair in the middle of the story? What if they have a flashback to when they were a blonde",
            "title": "LLM Memory"
        },
        {
            "content": [
                "<p>This is the weekly visible open thread. Post about anything you want, ask random questions, whatever. ACX has an unofficial <a href=\"https://www.reddit.com/r/slatestarcodex/\">subreddit</a>, <a href=\"https://discord.gg/RTKtdut\">Discord</a>, and <a href=\"https://www.datasecretslox.com/index.php\">bulletin board</a>, and <a href=\"https://www.lesswrong.com/community?filters%5B0%5D=SSC\">in-person meetups around the world</a>. Most content is free, some is subscriber only; you can subscribe <strong><a href=\"https://astralcodexten.substack.com/subscribe\">here</a></strong>. Also:</p><p><strong>1: </strong>ACX meetups this week in Oxford, Shanghai, and Austin. See <a href=\"https://www.astralcodexten.com/p/meetups-everywhere-spring-2025-times\">the post</a> for details.</p><p><strong>2: </strong>Thanks to the 152 of you who turned in non-book reviews. We are working on collating them and will have them available for voting soon.</p><p><strong>3: </strong>Constellation is an AI safety coworking space in Berkeley. They offer <a href=\"https://www.constellation.org/programs/visiting-fellows\">a fellowship where you can work at their office for 3-6 months</a>. I&#8217;m there sometimes with the AI 2027 team and recommend it as a great place to work and meet people. Applications close June 13. </p><p><strong>4: </strong>There&#8217;s also the <a href=\"https://www.flf.org/fellowship\">Fellowship on AI For Human Reasoning</a>, intended to &#8220;help talented researchers and builders start working on AI tools for coordination and epistemics&#8221;. Three months, &#8220;$25-50K stipend&#8221;, and a coworking space in the SF Bay Area (maybe also Constellation, I don&#8217;t know). Applications close June 9.</p><p><strong>5: </strong>&#8230;and also, FIRE and Cosmos are offering <a href=\"https://cosmosgrants.org/truth\">fast grants</a> (total pot $1 million, expected size per grant $1,000 - $10,000 in cash + compute) to projects on how &#8220;AI can empower open inquiry, not suppress it&#8221;. </p><p><strong>6: </strong>Two new posts on AI Futures blog, <a href=\"https://blog.ai-futures.org/p/make-the-prompt-public\">Make The Prompt Public</a> and <a href=\"https://blog.ai-futures.org/p/slow-corporations-as-an-intuition\">Slow Corporations As An Intuition Pump For AI R&amp;D Automation</a>.</p>"
            ],
            "link": "https://www.astralcodexten.com/p/open-thread-382",
            "publishedAt": "2025-05-19",
            "source": "SlateStarCodex",
            "summary": "<p>This is the weekly visible open thread. Post about anything you want, ask random questions, whatever. ACX has an unofficial <a href=\"https://www.reddit.com/r/slatestarcodex/\">subreddit</a>, <a href=\"https://discord.gg/RTKtdut\">Discord</a>, and <a href=\"https://www.datasecretslox.com/index.php\">bulletin board</a>, and <a href=\"https://www.lesswrong.com/community?filters%5B0%5D=SSC\">in-person meetups around the world</a>. Most content is free, some is subscriber only; you can subscribe <strong><a href=\"https://astralcodexten.substack.com/subscribe\">here</a></strong>. Also:</p><p><strong>1: </strong>ACX meetups this week in Oxford, Shanghai, and Austin. See <a href=\"https://www.astralcodexten.com/p/meetups-everywhere-spring-2025-times\">the post</a> for details.</p><p><strong>2: </strong>Thanks to the 152 of you who turned in non-book reviews. We are working on collating them and will have them available for voting soon.</p><p><strong>3: </strong>Constellation is an AI safety coworking space in Berkeley. They offer <a href=\"https://www.constellation.org/programs/visiting-fellows\">a fellowship where you can work at their office for 3-6 months</a>. I&#8217;m there sometimes with the AI 2027 team and recommend it as a great place to work and meet people. Applications close June 13. </p><p><strong>4: </strong>There&#8217;s also the <a href=\"https://www.flf.org/fellowship\">Fellowship on AI For Human Reasoning</a>, intended to &#8220;help talented researchers and builders start working on AI tools for coordination and epistemics&#8221;. Three months, &#8220;$25-50K stipend&#8221;, and a coworking space in the SF Bay Area (maybe also Constellation, I don&#8217;t know). Applications close June 9.</p><p><strong>5: </strong>&#8230;and also, FIRE and Cosmos are offering <a href=\"https://cosmosgrants.org/truth\">fast grants</a> (total pot $1",
            "title": "Open Thread 382"
        },
        {
            "content": [],
            "link": "https://xkcd.com/3091/",
            "publishedAt": "2025-05-19",
            "source": "XKCD",
            "summary": "<img alt=\"Applying renormalization to bullies successfully transformed Pete &amp; Pete's Endless Mike into Finite Mike.\" src=\"https://imgs.xkcd.com/comics/renormalization.png\" title=\"Applying renormalization to bullies successfully transformed Pete &amp; Pete's Endless Mike into Finite Mike.\" />",
            "title": "Renormalization"
        }
    ],
    "lookbackDays": 1,
    "publishDate": "2025-05-19"
}