{
    "articles": [
        {
            "content": [
                "<p>AIs have quietly crossed a threshold: they can now perform real, economically relevant work.</p><p>Last week, OpenAI released a<a href=\"https://cdn.openai.com/pdf/d5eb7428-c4e9-4a33-bd86-86dd4bcf12ce/GDPval.pdf\"> new test </a>of AI ability, but this one differs from the usual benchmarks built around math or trivia. For this test, OpenAI gathered experts with an average of 14 years of experience in industries ranging from finance to law to retail and had them design realistic tasks that would take human experts an average of four to seven hours to complete (you <a href=\"https://huggingface.co/datasets/openai/gdpval/viewer/default/train\">can see all the tasks here</a>). OpenAI then had both AI and other experts do the tasks themselves. A third group of experts graded the results, not knowing which answers came from the AI and which from the human, a process which took about an hour per question.</p><div class=\"captioned-image-container\"><figure><a class=\"image-link image2 is-viewable-img\" href=\"https://substackcdn.com/image/fetch/$s_!8Igm!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F30be8070-25d2-4cfb-810e-c79234b9ac76_2399x699.png\" target=\"_blank\"><div class=\"image2-inset\"><source type=\"image/webp\" /><img alt=\"\" class=\"sizing-normal\" height=\"424\" src=\"https://substackcdn.com/image/fetch/$s_!8Igm!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F30be8070-25d2-4cfb-810e-c79234b9ac76_2399x699.png\" width=\"1456\" /><div class=\"image-link-expand\"><div class=\"pencraft pc-display-flex pc-gap-8 pc-reset\"><div class=\"pencraft pc-reset icon-container restack-image\"><svg class=\"lucide lucide-refresh-cw\" fill=\"none\" height=\"20\" stroke=\"currentColor\" stroke-linecap=\"round\" stroke-linejoin=\"round\" stroke-width=\"2\" viewBox=\"0 0 24 24\" width=\"20\" xmlns=\"http://www.w3.org/2000/svg\"><path d=\"M3 12a9 9 0 0 1 9-9 9.75 9.75 0 0 1 6.74 2.74L21 8\"></path><path d=\"M21 3v5h-5\"></path><path d=\"M21 12a9 9 0 0 1-9 9 9.75 9.75 0 0 1-6.74-2.74L3 16\"></path><path d=\"M8 16H3v5\"></path></svg></div><div class=\"pencraft pc-reset icon-container view-image\"><svg class=\"lucide lucide-maximize2 lucide-maximize-2\" fill=\"none\" height=\"20\" stroke=\"currentColor\" stroke-linecap=\"round\" stroke-linejoin=\"round\" stroke-width=\"2\" viewBox=\"0 0 24 24\" width=\"20\" xmlns=\"http://www.w3.org/2000/svg\"><polyline points=\"15 3 21 3 21 9\"></polyline><polyline points=\"9 21 3 21 3 15\"></polyline><line x1=\"21\" x2=\"14\" y1=\"3\" y2=\"10\"></line><line x1=\"3\" x2=\"10\" y1=\"21\" y2=\"14\"></line></svg></div></div></div></div></a></figure></div><p>Human experts won, but barely, and the margins varied dramatically by industry. Yet AI is improving fast, with more recent AI models scoring much higher than older ones. Interestingly, the major reason for AI losing to humans was not hallucinations and errors, but a failure to format results well or follow instructions exactly &#8212; areas of rapid improvement. If the current patterns hold, the next generation of AI models should beat human experts on average in this test. Does that mean AI is ready to replace human jobs?</p><div class=\"captioned-image-container\"><figure><a class=\"image-link image2 is-viewable-img\" href=\"https://substackcdn.com/image/fetch/$s_!aBff!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F71ed60be-dbab-4724-b726-2dd1de70fb3a_1874x1330.png\" target=\"_blank\"><div class=\"image2-inset\"><source type=\"image/webp\" /><img alt=\"\" class=\"sizing-normal\" height=\"361.8337912087912\" src=\"https://substackcdn.com/image/fetch/$s_!aBff!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F71ed60be-dbab-4724-b726-2dd1de70fb3a_1874x1330.png\" width=\"510\" /><div class=\"image-link-expand\"><div class=\"pencraft pc-display-flex pc-gap-8 pc-reset\"><div class=\"pencraft pc-reset icon-container restack-image\"><svg class=\"lucide lucide-refresh-cw\" fill=\"none\" height=\"20\" stroke=\"currentColor\" stroke-linecap=\"round\" stroke-linejoin=\"round\" stroke-width=\"2\" viewBox=\"0 0 24 24\" width=\"20\" xmlns=\"http://www.w3.org/2000/svg\"><path d=\"M3 12a9 9 0 0 1 9-9 9.75 9.75 0 0 1 6.74 2.74L21 8\"></path><path d=\"M21 3v5h-5\"></path><path d=\"M21 12a9 9 0 0 1-9 9 9.75 9.75 0 0 1-6.74-2.74L3 16\"></path><path d=\"M8 16H3v5\"></path></svg></div><div class=\"pencraft pc-reset icon-container view-image\"><svg class=\"lucide lucide-maximize2 lucide-maximize-2\" fill=\"none\" height=\"20\" stroke=\"currentColor\" stroke-linecap=\"round\" stroke-linejoin=\"round\" stroke-width=\"2\" viewBox=\"0 0 24 24\" width=\"20\" xmlns=\"http://www.w3.org/2000/svg\"><polyline points=\"15 3 21 3 21 9\"></polyline><polyline points=\"9 21 3 21 3 15\"></polyline><line x1=\"21\" x2=\"14\" y1=\"3\" y2=\"10\"></line><line x1=\"3\" x2=\"10\" y1=\"21\" y2=\"14\"></line></svg></div></div></div></div></a></figure></div><p>No (at least not soon), because what was being measured was not jobs but tasks. Our jobs consist of many tasks. My job as a professor is not just one thing, it involves teaching, researching, writing, filling out annual reports, supporting my students, reading, administrative work and more. AI doing one or more of these tasks does not replace my entire job, it shifts what I do. And as long as AI is jagged in its abilities, and cannot substitute for all the complex work of human interaction, it cannot easily replace jobs as a whole&#8230;</p><h1>A Very Valuable Task</h1><p>&#8230;and yet some of the tasks that AI can do right now have incredible value. Let&#8217;s return to something that is critical in my job: producing accurate research. As many people know, there has been a &#8220;replication crisis&#8221; in academia where important findings turned out to be impossible for other researchers to reproduce. Academia has made some progress on this problem, and many researchers now provide their data so that other scholars can reproduce their work. The problem is that replication takes a lot of time, as you have to deeply read and understand the paper, analyze the data, and painstakingly check for errors<a class=\"footnote-anchor\" href=\"https://www.oneusefulthing.org/feed#footnote-1\" id=\"footnote-anchor-1\" target=\"_self\">1</a>. It&#8217;s a very complicated process that only humans could do.</p><p>Until now.</p><p>I gave the new Claude Sonnet 4.5 (to which I had early access) the text of a <a href=\"https://direct.mit.edu/rest/article-abstract/102/4/648/96785/Using-Goals-to-Motivate-College-Students-Theory?redirectedFrom=fulltext\">sophisticated economics paper</a> involving a number of experiments, along with the <a href=\"https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/IO8NQU\">archive of all of their replication data</a>. I did not do anything other than give Claude the files and the prompts &#8220;replicate the findings in this paper from the dataset they uploaded. you need to do this yourself. if you can&#8217;t attempt a full replication, do what you can&#8221; and, because it involved complex statistics, I asked it to go further: &#8220;can you also replicate the full interactions as much as possible?&#8221;</p><div class=\"captioned-image-container\"><figure><a class=\"image-link image2 is-viewable-img\" href=\"https://substackcdn.com/image/fetch/$s_!dRfL!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F681e7d0a-77e0-4013-bd8a-42dea77a3a52_1888x1167.png\" target=\"_blank\"><div class=\"image2-inset\"><source type=\"image/webp\" /><img alt=\"\" class=\"sizing-normal\" height=\"900\" src=\"https://substackcdn.com/image/fetch/$s_!dRfL!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F681e7d0a-77e0-4013-bd8a-42dea77a3a52_1888x1167.png\" width=\"1456\" /><div class=\"image-link-expand\"><div class=\"pencraft pc-display-flex pc-gap-8 pc-reset\"><div class=\"pencraft pc-reset icon-container restack-image\"><svg class=\"lucide lucide-refresh-cw\" fill=\"none\" height=\"20\" stroke=\"currentColor\" stroke-linecap=\"round\" stroke-linejoin=\"round\" stroke-width=\"2\" viewBox=\"0 0 24 24\" width=\"20\" xmlns=\"http://www.w3.org/2000/svg\"><path d=\"M3 12a9 9 0 0 1 9-9 9.75 9.75 0 0 1 6.74 2.74L21 8\"></path><path d=\"M21 3v5h-5\"></path><path d=\"M21 12a9 9 0 0 1-9 9 9.75 9.75 0 0 1-6.74-2.74L3 16\"></path><path d=\"M8 16H3v5\"></path></svg></div><div class=\"pencraft pc-reset icon-container view-image\"><svg class=\"lucide lucide-maximize2 lucide-maximize-2\" fill=\"none\" height=\"20\" stroke=\"currentColor\" stroke-linecap=\"round\" stroke-linejoin=\"round\" stroke-width=\"2\" viewBox=\"0 0 24 24\" width=\"20\" xmlns=\"http://www.w3.org/2000/svg\"><polyline points=\"15 3 21 3 21 9\"></polyline><polyline points=\"9 21 3 21 3 15\"></polyline><line x1=\"21\" x2=\"14\" y1=\"3\" y2=\"10\"></line><line x1=\"3\" x2=\"10\" y1=\"21\" y2=\"14\"></line></svg></div></div></div></div></a></figure></div><p>Without further instruction, Claude read the paper, opened up the archive and sorted through the files, converted the statistical code from one language (STATA) to another (Python), and methodically went through all the findings before reporting a successful reproduction. I spot checked the results and had another AI model, GPT-5 Pro, reproduce the reproduction. It all checked out. I tried this on several other papers with similarly good results, though some were inaccessible due to file size limitations or issues with the replication data provided. Doing this manually would have taken many hours.</p><p>But the revolutionary part is not that I saved a lot of time. It is that a crisis that has shaken entire academic fields could be partially resolved with reproduction, but doing so required painstaking and expensive human effort that was impossible to do at scale. Now it appears that AI could check many published papers, reproducing results, with implications for all of scientific research. There are still barriers to doing this, including benchmarking for accuracy and fairness, but it is now a real possibility. Reproducing research may be an AI task, not a job, but it is also might change an entire field of human endeavor dramatically. What makes this possible? AI agents have gotten much better, very quickly.</p><h1>Agents at the heart of it all</h1><p>Generative AI has helped a lot of people do tasks since the original ChatGPT, but the limit was always a human user. AI makes mistakes and errors, so, without a human guiding it on each step, nothing valuable could be accomplished.  The dream of autonomous AI agents, which, when given a task, can plan and use tools (coding, web search) to accomplish it, seemed far away. After all, AI makes mistakes, so one failure in the long chain of steps that an agent has to follow to accomplish a task would result in a failure overall.</p><p>However, that isn&#8217;t how things worked out, and <a href=\"https://arxiv.org/pdf/2509.09677\">another new paper explains why.</a> It turns out most of our assumptions about AI agents were wrong. Even small increases in accuracy (and new models are much less prone to errors) leads to huge increases in the number of tasks an AI can do. And the biggest and latest &#8220;thinking&#8221; models are actually self-correcting, so they don&#8217;t get stopped by errors. All of this means that AI agents can accomplish far more steps than they could before and can use tools (which basically include anything your computer can do) without substantial human intervention.</p><div class=\"captioned-image-container\"><figure><a class=\"image-link image2 is-viewable-img\" href=\"https://substackcdn.com/image/fetch/$s_!SKKh!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd08a3822-49c9-4ad5-bacb-153f1a094405_1073x748.png\" target=\"_blank\"><div class=\"image2-inset\"><source type=\"image/webp\" /><img alt=\"\" class=\"sizing-normal\" height=\"402.23299161230193\" src=\"https://substackcdn.com/image/fetch/$s_!SKKh!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd08a3822-49c9-4ad5-bacb-153f1a094405_1073x748.png\" width=\"577\" /><div class=\"image-link-expand\"><div class=\"pencraft pc-display-flex pc-gap-8 pc-reset\"><div class=\"pencraft pc-reset icon-container restack-image\"><svg class=\"lucide lucide-refresh-cw\" fill=\"none\" height=\"20\" stroke=\"currentColor\" stroke-linecap=\"round\" stroke-linejoin=\"round\" stroke-width=\"2\" viewBox=\"0 0 24 24\" width=\"20\" xmlns=\"http://www.w3.org/2000/svg\"><path d=\"M3 12a9 9 0 0 1 9-9 9.75 9.75 0 0 1 6.74 2.74L21 8\"></path><path d=\"M21 3v5h-5\"></path><path d=\"M21 12a9 9 0 0 1-9 9 9.75 9.75 0 0 1-6.74-2.74L3 16\"></path><path d=\"M8 16H3v5\"></path></svg></div><div class=\"pencraft pc-reset icon-container view-image\"><svg class=\"lucide lucide-maximize2 lucide-maximize-2\" fill=\"none\" height=\"20\" stroke=\"currentColor\" stroke-linecap=\"round\" stroke-linejoin=\"round\" stroke-width=\"2\" viewBox=\"0 0 24 24\" width=\"20\" xmlns=\"http://www.w3.org/2000/svg\"><polyline points=\"15 3 21 3 21 9\"></polyline><polyline points=\"9 21 3 21 3 15\"></polyline><line x1=\"21\" x2=\"14\" y1=\"3\" y2=\"10\"></line><line x1=\"3\" x2=\"10\" y1=\"21\" y2=\"14\"></line></svg></div></div></div></div></a></figure></div><p>So, it is interesting that one of the few measures of AI ability that covers the full range of AI models in the past few years, from GPT-3 to GPT-5, is <a href=\"https://metr.org/blog/2025-03-19-measuring-ai-ability-to-complete-long-tasks/\">METR&#8217;s test</a> of the length of tasks that AI can accomplish alone with at least 50% accuracy. The exponential gains from GPT-3 to GPT-5 are very consistent over five years, showing the ongoing improvement in agentic work.</p><div class=\"captioned-image-container\"><figure><a class=\"image-link image2 is-viewable-img\" href=\"https://substackcdn.com/image/fetch/$s_!mGGm!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5dc70bae-f6c2-4fcf-b38d-ee04183dab21_1214x676.png\" target=\"_blank\"><div class=\"image2-inset\"><source type=\"image/webp\" /><img alt=\"\" class=\"sizing-normal\" height=\"278.4184514003295\" src=\"https://substackcdn.com/image/fetch/$s_!mGGm!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5dc70bae-f6c2-4fcf-b38d-ee04183dab21_1214x676.png\" width=\"500\" /><div class=\"image-link-expand\"><div class=\"pencraft pc-display-flex pc-gap-8 pc-reset\"><div class=\"pencraft pc-reset icon-container restack-image\"><svg class=\"lucide lucide-refresh-cw\" fill=\"none\" height=\"20\" stroke=\"currentColor\" stroke-linecap=\"round\" stroke-linejoin=\"round\" stroke-width=\"2\" viewBox=\"0 0 24 24\" width=\"20\" xmlns=\"http://www.w3.org/2000/svg\"><path d=\"M3 12a9 9 0 0 1 9-9 9.75 9.75 0 0 1 6.74 2.74L21 8\"></path><path d=\"M21 3v5h-5\"></path><path d=\"M21 12a9 9 0 0 1-9 9 9.75 9.75 0 0 1-6.74-2.74L3 16\"></path><path d=\"M8 16H3v5\"></path></svg></div><div class=\"pencraft pc-reset icon-container view-image\"><svg class=\"lucide lucide-maximize2 lucide-maximize-2\" fill=\"none\" height=\"20\" stroke=\"currentColor\" stroke-linecap=\"round\" stroke-linejoin=\"round\" stroke-width=\"2\" viewBox=\"0 0 24 24\" width=\"20\" xmlns=\"http://www.w3.org/2000/svg\"><polyline points=\"15 3 21 3 21 9\"></polyline><polyline points=\"9 21 3 21 3 15\"></polyline><line x1=\"21\" x2=\"14\" y1=\"3\" y2=\"10\"></line><line x1=\"3\" x2=\"10\" y1=\"21\" y2=\"14\"></line></svg></div></div></div></div></a></figure></div><h1>How to use AI to do economically valuable things</h1><p>Agents, however, don&#8217;t have true agency in the human sense. For now, we need to decide what to do with them, and that will determine a lot about the future of work. The risk everyone focuses on is using AI to replace human labor, and it is not hard to see this becoming a major concern in the coming years, especially for unimaginative organizations that focus on cost-cutting, rather than using these new capabilities to expand or transform work. But there is a second, very likely, risk about using AI at work: using agents to do more of the tasks we do now, unthinkingly.</p><p>As a preview of this particular nightmare, I gave Claude a corporate memo and asked it to turn it into a PowerPoint. And then another PowerPoint from a different perspective. And another one.</p><div class=\"captioned-image-container\"><figure><a class=\"image-link image2 is-viewable-img\" href=\"https://substackcdn.com/image/fetch/$s_!8Ngl!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6eda6d8c-ce33-4997-9c9c-bf7fcf3f6cb2_782x904.png\" target=\"_blank\"><div class=\"image2-inset\"><source type=\"image/webp\" /><img alt=\"\" class=\"sizing-normal\" height=\"330.61892583120203\" src=\"https://substackcdn.com/image/fetch/$s_!8Ngl!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6eda6d8c-ce33-4997-9c9c-bf7fcf3f6cb2_782x904.png\" width=\"286\" /><div class=\"image-link-expand\"><div class=\"pencraft pc-display-flex pc-gap-8 pc-reset\"><div class=\"pencraft pc-reset icon-container restack-image\"><svg class=\"lucide lucide-refresh-cw\" fill=\"none\" height=\"20\" stroke=\"currentColor\" stroke-linecap=\"round\" stroke-linejoin=\"round\" stroke-width=\"2\" viewBox=\"0 0 24 24\" width=\"20\" xmlns=\"http://www.w3.org/2000/svg\"><path d=\"M3 12a9 9 0 0 1 9-9 9.75 9.75 0 0 1 6.74 2.74L21 8\"></path><path d=\"M21 3v5h-5\"></path><path d=\"M21 12a9 9 0 0 1-9 9 9.75 9.75 0 0 1-6.74-2.74L3 16\"></path><path d=\"M8 16H3v5\"></path></svg></div><div class=\"pencraft pc-reset icon-container view-image\"><svg class=\"lucide lucide-maximize2 lucide-maximize-2\" fill=\"none\" height=\"20\" stroke=\"currentColor\" stroke-linecap=\"round\" stroke-linejoin=\"round\" stroke-width=\"2\" viewBox=\"0 0 24 24\" width=\"20\" xmlns=\"http://www.w3.org/2000/svg\"><polyline points=\"15 3 21 3 21 9\"></polyline><polyline points=\"9 21 3 21 3 15\"></polyline><line x1=\"21\" x2=\"14\" y1=\"3\" y2=\"10\"></line><line x1=\"3\" x2=\"10\" y1=\"21\" y2=\"14\"></line></svg></div></div></div></div></a></figure></div><p>Until I got 17 different PowerPoints. That is too many PowerPoints.</p><div class=\"captioned-image-container\"><figure><a class=\"image-link image2\" href=\"https://substackcdn.com/image/fetch/$s_!XP5k!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fed691296-9f07-4822-b833-9af5b3778787_3248x1820.png\" target=\"_blank\"><div class=\"image2-inset\"><source type=\"image/webp\" /><img alt=\"\" class=\"sizing-normal\" height=\"238.74725274725276\" src=\"https://substackcdn.com/image/fetch/$s_!XP5k!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fed691296-9f07-4822-b833-9af5b3778787_3248x1820.png\" width=\"426\" /><div></div></div></a></figure></div><p>If we don&#8217;t think hard about WHY we are doing work, and what work should look like, we are all going to drown in a wave of AI content. What is the alternative? The OpenAI paper suggested that experts can work with AI to solve problems by delegating tasks to an AI as a first pass and reviewing the work. If it isn&#8217;t good enough, they should try a couple of attempts to give corrections or better instructions. If that doesn&#8217;t work, they should just do the work themselves. If experts followed this workflow, the paper estimates they would get work done forty percent faster and sixty percent cheaper, and, even more importantly, retain control over the AI.</p><div class=\"captioned-image-container\"><figure><a class=\"image-link image2 is-viewable-img\" href=\"https://substackcdn.com/image/fetch/$s_!sj1I!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fae47b051-6425-4dd8-9a41-333669c21872_779x586.png\" target=\"_blank\"><div class=\"image2-inset\"><source type=\"image/webp\" /><img alt=\"\" class=\"sizing-normal\" height=\"324.9704749679076\" src=\"https://substackcdn.com/image/fetch/$s_!sj1I!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fae47b051-6425-4dd8-9a41-333669c21872_779x586.png\" width=\"432\" /><div class=\"image-link-expand\"><div class=\"pencraft pc-display-flex pc-gap-8 pc-reset\"><div class=\"pencraft pc-reset icon-container restack-image\"><svg class=\"lucide lucide-refresh-cw\" fill=\"none\" height=\"20\" stroke=\"currentColor\" stroke-linecap=\"round\" stroke-linejoin=\"round\" stroke-width=\"2\" viewBox=\"0 0 24 24\" width=\"20\" xmlns=\"http://www.w3.org/2000/svg\"><path d=\"M3 12a9 9 0 0 1 9-9 9.75 9.75 0 0 1 6.74 2.74L21 8\"></path><path d=\"M21 3v5h-5\"></path><path d=\"M21 12a9 9 0 0 1-9 9 9.75 9.75 0 0 1-6.74-2.74L3 16\"></path><path d=\"M8 16H3v5\"></path></svg></div><div class=\"pencraft pc-reset icon-container view-image\"><svg class=\"lucide lucide-maximize2 lucide-maximize-2\" fill=\"none\" height=\"20\" stroke=\"currentColor\" stroke-linecap=\"round\" stroke-linejoin=\"round\" stroke-width=\"2\" viewBox=\"0 0 24 24\" width=\"20\" xmlns=\"http://www.w3.org/2000/svg\"><polyline points=\"15 3 21 3 21 9\"></polyline><polyline points=\"9 21 3 21 3 15\"></polyline><line x1=\"21\" x2=\"14\" y1=\"3\" y2=\"10\"></line><line x1=\"3\" x2=\"10\" y1=\"21\" y2=\"14\"></line></svg></div></div></div></div></a></figure></div><p>Agents are here. They can do real work, and while that work is still limited, it is valuable and increasing. But the same technology that can replicate academic papers in minutes can also generate 17 versions of a PowerPoint deck that nobody needs. The difference between these futures isn&#8217;t in the AI, it&#8217;s in how we choose to use it. By using our judgement in deciding what&#8217;s worth doing, not just what can be done, we can ensure these tools make us more capable, not just more productive.</p><p class=\"button-wrapper\"><a class=\"button primary\" href=\"https://www.oneusefulthing.org/subscribe\"><span>Subscribe now</span></a></p><p class=\"button-wrapper\"><a class=\"button primary\" href=\"https://www.oneusefulthing.org/p/real-ai-agents-and-real-work?utm_source=substack&amp;utm_medium=email&amp;utm_content=share&amp;action=share\"><span>Share</span></a></p><p></p><div class=\"captioned-image-container\"><figure><a class=\"image-link image2 is-viewable-img\" href=\"https://substackcdn.com/image/fetch/$s_!0oc8!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffbe42bc3-4c1b-40db-9247-a50466387ded_1376x864.png\" target=\"_blank\"><div class=\"image2-inset\"><source type=\"image/webp\" /><img alt=\"\" class=\"sizing-normal\" height=\"264.9767441860465\" src=\"https://substackcdn.com/image/fetch/$s_!0oc8!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffbe42bc3-4c1b-40db-9247-a50466387ded_1376x864.png\" width=\"422\" /><div class=\"image-link-expand\"><div class=\"pencraft pc-display-flex pc-gap-8 pc-reset\"><div class=\"pencraft pc-reset icon-container restack-image\"><svg class=\"lucide lucide-refresh-cw\" fill=\"none\" height=\"20\" stroke=\"currentColor\" stroke-linecap=\"round\" stroke-linejoin=\"round\" stroke-width=\"2\" viewBox=\"0 0 24 24\" width=\"20\" xmlns=\"http://www.w3.org/2000/svg\"><path d=\"M3 12a9 9 0 0 1 9-9 9.75 9.75 0 0 1 6.74 2.74L21 8\"></path><path d=\"M21 3v5h-5\"></path><path d=\"M21 12a9 9 0 0 1-9 9 9.75 9.75 0 0 1-6.74-2.74L3 16\"></path><path d=\"M8 16H3v5\"></path></svg></div><div class=\"pencraft pc-reset icon-container view-image\"><svg class=\"lucide lucide-maximize2 lucide-maximize-2\" fill=\"none\" height=\"20\" stroke=\"currentColor\" stroke-linecap=\"round\" stroke-linejoin=\"round\" stroke-width=\"2\" viewBox=\"0 0 24 24\" width=\"20\" xmlns=\"http://www.w3.org/2000/svg\"><polyline points=\"15 3 21 3 21 9\"></polyline><polyline points=\"9 21 3 21 3 15\"></polyline><line x1=\"21\" x2=\"14\" y1=\"3\" y2=\"10\"></line><line x1=\"3\" x2=\"10\" y1=\"21\" y2=\"14\"></line></svg></div></div></div></div></a></figure></div><p></p><div class=\"footnote\"><a class=\"footnote-number\" contenteditable=\"false\" href=\"https://www.oneusefulthing.org/feed#footnote-anchor-1\" id=\"footnote-1\" target=\"_self\">1</a><div class=\"footnote-content\"><p>Depending on the field of research, there can be differences between replicating (which can involve collecting new data) and reproducing (which can involve using existing data) research. I don&#8217;t go into the various distinctions in this post, but in this case, the AI is working with existing data, but also applying new statistical approaches to that data.</p></div></div>"
            ],
            "link": "https://www.oneusefulthing.org/p/real-ai-agents-and-real-work",
            "publishedAt": "2025-09-29",
            "source": "Ethan Mollick",
            "summary": "<p>AIs have quietly crossed a threshold: they can now perform real, economically relevant work.</p><p>Last week, OpenAI released a<a href=\"https://cdn.openai.com/pdf/d5eb7428-c4e9-4a33-bd86-86dd4bcf12ce/GDPval.pdf\"> new test </a>of AI ability, but this one differs from the usual benchmarks built around math or trivia. For this test, OpenAI gathered experts with an average of 14 years of experience in industries ranging from finance to law to retail and had them design realistic tasks that would take human experts an average of four to seven hours to complete (you <a href=\"https://huggingface.co/datasets/openai/gdpval/viewer/default/train\">can see all the tasks here</a>). OpenAI then had both AI and other experts do the tasks themselves. A third group of experts graded the results, not knowing which answers came from the AI and which from the human, a process which took about an hour per question.</p><div class=\"captioned-image-container\"><figure><a class=\"image-link image2 is-viewable-img\" href=\"https://substackcdn.com/image/fetch/$s_!8Igm!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F30be8070-25d2-4cfb-810e-c79234b9ac76_2399x699.png\" target=\"_blank\"><div class=\"image2-inset\"><source type=\"image/webp\" /><img alt=\"\" class=\"sizing-normal\" height=\"424\" src=\"https://substackcdn.com/image/fetch/$s_!8Igm!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F30be8070-25d2-4cfb-810e-c79234b9ac76_2399x699.png\" width=\"1456\" /><div class=\"image-link-expand\"><div class=\"pencraft pc-display-flex pc-gap-8 pc-reset\"><div class=\"pencraft pc-reset icon-container restack-image\"><svg class=\"lucide lucide-refresh-cw\" fill=\"none\" height=\"20\" stroke=\"currentColor\" stroke-linecap=\"round\" stroke-linejoin=\"round\" stroke-width=\"2\" viewBox=\"0 0 24 24\" width=\"20\" xmlns=\"http://www.w3.org/2000/svg\"><path d=\"M3 12a9 9 0 0 1 9-9 9.75 9.75 0 0 1 6.74 2.74L21 8\"></path><path d=\"M21 3v5h-5\"></path><path d=\"M21 12a9 9 0 0 1-9 9 9.75 9.75 0 0 1-6.74-2.74L3 16\"></path><path d=\"M8 16H3v5\"></path></svg></div><div",
            "title": "Real AI Agents and Real Work"
        },
        {
            "content": [],
            "link": "https://simonwillison.net/2025/Sep/29/claude-sonnet-4-5/#atom-entries",
            "publishedAt": "2025-09-29",
            "source": "Simon Willison",
            "summary": "<p>Anthropic <a href=\"https://www.anthropic.com/news/claude-sonnet-4-5\">released Claude Sonnet 4.5 today</a>, with a <em>very</em> bold set of claims:</p> <blockquote> <p>Claude Sonnet 4.5 is the best coding model in the world. It's the strongest model for building complex agents. It\u2019s the best model at using computers. And it shows substantial gains in reasoning and math.</p> </blockquote> <p>Anthropic gave me access to a preview version of a \"new model\" over the weekend which turned out to be Sonnet 4.5. My initial impressions were that it felt like a better model for code than GPT-5-Codex, which has been my preferred coding model since <a href=\"https://simonwillison.net/2025/Sep/23/gpt-5-codex/\">it launched a few weeks ago</a>. This space moves <em>so fast</em> - Gemini 3 is rumored to land soon so who knows how long Sonnet 4.5 will continue to hold the \"best coding model\" crown.</p> <p>The pricing is the same as the previous Sonnet: $3/million input tokens and $15/million output tokens. This remains significantly cheaper than Claude Opus - $15/$75 - but still quite a bit more than GPT-5 and GPT-5-Codex, both at $1.25/$10.</p> <h4 id=\"it-really-shines-with-claude-ai-code-interpreter\">It really shines with Claude.ai Code Interpreter</h4> <p>The <a href=\"https://claude.ai/\">claude.ai</a> web interface (not yet the Claude iPhone native app) recently added the ability for Claude to write and",
            "title": "Claude Sonnet 4.5 is probably the \"best coding model in the world\" (at least for now)"
        },
        {
            "content": [
                "<p>This is the weekly visible open thread. Post about anything you want, ask random questions, whatever. ACX has an unofficial <a href=\"https://www.reddit.com/r/slatestarcodex/\">subreddit</a>, <a href=\"https://discord.gg/RTKtdut\">Discord</a>, and <a href=\"https://www.datasecretslox.com/index.php\">bulletin board</a>, and <a href=\"https://www.lesswrong.com/community?filters%5B0%5D=SSC\">in-person meetups around the world</a>. Most content is free, some is subscriber only; you can subscribe <strong><a href=\"https://astralcodexten.substack.com/subscribe\">here</a></strong>. Also:</p><p><strong>1: </strong>Meetups this week include Ankara, Bangalore, Dallas, DC, Delhi, Denver, Hyderabad, Istanbul, LA, Raleigh-Durham, San Diego, San Francisco, Zagreb; see <a href=\"https://www.astralcodexten.com/p/meetups-everywhere-2025-times-and\">the meetup post</a> for more information.. And late additions Aachen, Lviv, and Malaga have been added to the list for October. </p><p><strong>2: </strong>Post frequency might stay low as I wrap up ACX Grants. I am still hoping to email winners this week (maybe not by October 1 exactly) and to announce them publicly around mid-October.</p><p><strong>3: </strong>Announcement from previous grant winner Devansh: </p><blockquote><p>We are holding a contest until October 6th for using LLMs and other models to predict how important 45 open source repos are to Ethereum. The winning submissions, as judged by their error rate to ground truth data collected from experts, get to distribute $350,000 to projects and also win $20,000 in prizes from Ethereum Foundation. This is a continuation of a project in the ACX mini forecasting challenge, with a pivot from using a particular LLM for assessing impact to a data science competition where anyone can submit models. Compete <a href=\"https://cryptopond.xyz/modelfactory/detail/2564617\">here</a> .</p></blockquote>"
            ],
            "link": "https://www.astralcodexten.com/p/open-thread-401",
            "publishedAt": "2025-09-29",
            "source": "SlateStarCodex",
            "summary": "<p>This is the weekly visible open thread. Post about anything you want, ask random questions, whatever. ACX has an unofficial <a href=\"https://www.reddit.com/r/slatestarcodex/\">subreddit</a>, <a href=\"https://discord.gg/RTKtdut\">Discord</a>, and <a href=\"https://www.datasecretslox.com/index.php\">bulletin board</a>, and <a href=\"https://www.lesswrong.com/community?filters%5B0%5D=SSC\">in-person meetups around the world</a>. Most content is free, some is subscriber only; you can subscribe <strong><a href=\"https://astralcodexten.substack.com/subscribe\">here</a></strong>. Also:</p><p><strong>1: </strong>Meetups this week include Ankara, Bangalore, Dallas, DC, Delhi, Denver, Hyderabad, Istanbul, LA, Raleigh-Durham, San Diego, San Francisco, Zagreb; see <a href=\"https://www.astralcodexten.com/p/meetups-everywhere-2025-times-and\">the meetup post</a> for more information.. And late additions Aachen, Lviv, and Malaga have been added to the list for October. </p><p><strong>2: </strong>Post frequency might stay low as I wrap up ACX Grants. I am still hoping to email winners this week (maybe not by October 1 exactly) and to announce them publicly around mid-October.</p><p><strong>3: </strong>Announcement from previous grant winner Devansh: </p><blockquote><p>We are holding a contest until October 6th for using LLMs and other models to predict how important 45 open source repos are to Ethereum. The winning submissions, as judged by their error rate to ground truth data collected from experts, get to distribute $350,000 to projects and also win $20,000 in prizes from Ethereum Foundation. This is a continuation of a project in the ACX mini forecasting challenge,",
            "title": "Open Thread 401"
        },
        {
            "content": [
                "<p>This seems like a good opportunity to do some of my classic detailed podcast coverage.</p>\n<p>The conventions are:</p>\n<ol>\n<li>This is not complete, points I did not find of note are skipped.</li>\n<li>The main part of each point is descriptive of what is said, by default paraphrased.</li>\n<li>For direct quotes I will use quote marks, by default this is Sutton.</li>\n<li>Nested statements are my own commentary.</li>\n<li>Timestamps are approximate and from <a href=\"https://www.dwarkesh.com/p/richard-sutton\">his hosted copy</a>, not the <a href=\"https://www.youtube.com/watch?v=21EYKqUsPfg&amp;t=1s\">YouTube version</a>, in this case I didn\u2019t bother because the section divisions in the transcript should make this very easy to follow without them.</li>\n</ol>\n<p><a href=\"https://www.dwarkesh.com/p/richard-sutton\">Full transcript of the episode is here</a> if you want to verify exactly what was said.</p>\n<div>\n\n\n<span id=\"more-24751\"></span>\n\n\n</div>\n<p>Well, that was the plan. This turned largely into me quoting Sutton and then expressing my mind boggling. A lot of what was interesting about this talk was in the back and forth or the ways Sutton lays things out in ways that I found impossible to excerpt, so one could consider following along with the transcript or while listening.</p>\n<div>\n<figure>\n<div>\n\n\n<figure class=\"wp-block-image\"><img alt=\"\" src=\"https://substackcdn.com/image/fetch/$s_!O2D_!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F009b1a9d-aa48-40a4-b58a-4225424a8e8e_1609x695.png\" /></figure>\n\n\n<div></div>\n</div>\n</figure>\n</div>\n\n\n<h4 class=\"wp-block-heading\">Sutton Says LLMs Are Not Intelligent And Don\u2019t Do Anything</h4>\n\n\n<ol>\n<li>(0:33) RL and LLMs are very different. RL is \u2018basic\u2019 AI. Intelligence and RL are about understanding your world. LLMs mimic people, they don\u2019t figure out what to do.\n<ol>\n<li>RL isn\u2019t strictly about \u2018understanding your world\u2019 except insofar as it is necessary to do the job. The same applies to LLMs, no?</li>\n<li>To maximize RL signal you need to understand and predict the world, aka you need intelligence. To mimic people, you have to understand and predict them, which in turn requires understanding and predicting the world. Same deal.</li>\n</ol>\n</li>\n<li>(1:19) Dwarkesh points out that mimicry requires a robust world model, indeed LLMs have the best world models to date. Sutton disagrees, you\u2019re mimicking people, and he questions that people have a world model. He says a world model would allow you to predict what would happen, whereas people can\u2019t do that.\n<ol>\n<li>People don\u2019t always have an explicit world model, but sometimes they do, and they have an implicit one running under the hood.</li>\n<li>Even if people didn\u2019t have a world model in their heads, their outputs in a given situation depend on the world, which you then have to model, if you want to mimic those humans.</li>\n<li>People predict what will happen all the time, on micro and macro levels. On the micro level they are usually correct. On sufficiently macro levels they are often wrong, but this still counts. If the claim is \u2018if you can\u2019t reliably predict what will happen then you don\u2019t have a model\u2019 then we disagree on what it means to have a model, and I would claim no such-defined models exist at any interesting scale or scope.</li>\n</ol>\n</li>\n<li>(1:38) \u201cWhat we want, to quote <a href=\"https://en.wikipedia.org/wiki/Alan_Turing\">Alan Turing</a>, is a machine that can learn from experience, where experience is the things that actually happen in your life. You do things, you see what happens, and that\u2019s what you learn from. The large language models learn from something else. They learn from \u201chere\u2019s a situation, and here\u2019s what a person did\u201d. Implicitly, the suggestion is you should do what the person did.\u201d\n<ol>\n<li>That\u2019s not the suggestion. If [X] is often followed by [Y], then the suggestion is not \u2018if [X] then you should do [Y]\u2019 it it \u2018[X] means [Y] is likely\u2019 so yes if you are asked \u2018what is likely after [X]\u2019 it will respond [Y] but it will also internalize everything implied by this fact and the fact is not in any way normative.</li>\n<li>That\u2019s still \u2018learning from experience\u2019 it\u2019s simply not continual learning.</li>\n<li>Do LLMs do continual learning, e.g. \u2018from what actually happens in your life\u2019 in particular? Not in their current forms, not technically, but there\u2019s no inherent reason they couldn\u2019t, you\u2019d just do [mumble] except that doing so would get rather expensive.</li>\n<li>You can also have them learn via various forms of external memory, broadly construed, including having them construct programs. It would work.</li>\n<li>Not that it\u2019s obvious that you would want an LLM or other AI to learn specifically from what happens in your life, as opposed to learning from things that happen in lives in general plus having context and memory.</li>\n</ol>\n</li>\n<li>(2:39) Dwarkesh responds with a potential crux that imitation learning is a good prior or reasonable approach, and gives the opportunity to get answers right sometimes, then you can train on experience. Sutton says no, that\u2019s the LLM perspective, but the LLM perspective is bad. It\u2019s not \u2018actual knowledge.\u2019 You need continual learning so you need to know what\u2019s right during interactions, but the LLM setup can\u2019t tell because there\u2019s no ground truth, because you don\u2019t have a prediction about what will happen next.\n<ol>\n<li>I don\u2019t see Dwarkesh\u2019s question as a crux.</li>\n<li>I think Sutton\u2019s response is quite bad, relying on invalid sacred word defenses.</li>\n<li>I think Sutton wants to draw a distinction between events in the world and tokens in a document. I don\u2019t think you can do that.</li>\n<li>There is no \u2018ground truth\u2019 other than the feedback one gets from the environment. I don\u2019t see why a physical response is different from a token, or from a numerical score. The feedback involved can come from anywhere, including from self-reflection if verification is easier than generation or can be made so in context, and it still counts. What is this special \u2018ground truth\u2019?</li>\n<li>Almost all feedback is noisy because almost all outcomes are probabilistic.</li>\n<li>You think that\u2019s air you\u2019re experiencing breathing? Does that matter?</li>\n</ol>\n</li>\n<li>(5:29) Dwarkesh points out you can literally ask \u201cWhat would you anticipate a user might say in response?\u201d but Sutton rejects this because it\u2019s not a \u2018substantive\u2019 prediction and the LLM won\u2019t be \u2018surprised\u2019 or \u201cthey will not change because an unexpected thing has happened. To learn that, they\u2019d have to make an adjustment.\u201d\n<ol>\n<li>Why is this \u2018not substantive\u2019 in any meaningful way, especially if it is a description of a substantive consequence, which speech often is?</li>\n<li>How is it not \u2018surprise\u2019 when a low-probability token appears in the text?</li>\n<li>There are plenty of times a human is surprised by an outcome but does not learn from it out of context. For example, I roll a d100 and get a 1. Okie dokie.</li>\n<li>LLMs do learn from a surprising token in training. You can always train. This seems like an insistence that surprise requires continual learning? Why?</li>\n</ol>\n</li>\n<li>Dwarkesh points out LLMs update within a chain-of-thought, so flexibility exists in a given context. Sutton reiterates they can\u2019t predict things and can\u2019t be surprised. He insists that \u201cThe next token is what they should say, what the actions should be. It\u2019s not what the world will give them in response to what they do.\u201d\n<ol>\n<li>What is Sutton even saying, at this point?</li>\n<li>Again, this distinction that outputting or predicting a token is distinct from \u2018taking an action,\u2019 and getting a token back is not the world responding.</li>\n<li>I\u2019d point out the same applies to the rest of the tokens in context without CoT.</li>\n</ol>\n</li>\n<li>(6:47) Sutton claims something interesting, that intelligence requires goals, \u201cI like <a href=\"https://en.wikipedia.org/wiki/John_McCarthy_(computer_scientist)\">John McCarthy\u2019s</a> <a href=\"http://www.incompleteideas.net/papers/Sutton-JAGI-2020.pdf\">definition that intelligence is the computational part of the ability to achieve goals</a>. You have to have goals or you\u2019re just a behaving system.\u201d And he asks Dwarkesh is he agrees that LLMs don\u2019t have goals (or don\u2019t have \u2018substantive\u2019 goals, and that next token prediction is not a goal, because it doesn\u2019t influence the tokens.\n<ol>\n<li>Okay, seriously, this is crazy, right?</li>\n<li>What is this \u2018substantive\u2019 thing? If you say something on the internet, it gets read in real life. It impacts real life. It causes real people to do \u2018substantive\u2019 things, and achieving many goals within the internet requires \u2018substantive\u2019 changes in the offline world. If you\u2019re dumb on the internet, you\u2019re dumb in real life. If you die on the internet, you die in real life (e.g. in the sense of an audience not laughing, or people not supporting you, etc).</li>\n<li>I feel dumb having to type that, but I\u2019m confused what the confusion is.</li>\n<li>Of course next token prediction is a goal. You try predicting the next token (it\u2019s hard!) and then tell me you weren\u2019t pursuing a goal.</li>\n<li>Next token prediction does influence the tokens in deployment because the LLM will output the next most likely token, which changes what tokens come after, its and the user\u2019s, and also the real world.</li>\n<li>Next token prediction does influence the world in training, because the feedback on that prediction\u2019s accuracy will change the model\u2019s weights, if nothing else. Those are part of the world.</li>\n<li>If intelligence requires goals, and something clearly displays intelligence, then that something must have a goal. If you conclude that LLMs \u2018don\u2019t have intelligence\u2019 in 2025, you\u2019ve reached a wrong conclusion. Wrong conclusions are wrong. You made a mistake. Retrace your steps until you find it.</li>\n</ol>\n</li>\n<li>Dwarkesh next points out you can do RL on top of LLMs, and they get IMO gold, and asks why Sutton still doesn\u2019t think that is anything. Sutton doubles down that math operations still aren\u2019t the empirical world, doesn\u2019t count.\n<ol>\n<li>Are you kidding me? So symbolic things aren\u2019t real, period, and manipulating them can\u2019t be intelligence, period?</li>\n</ol>\n</li>\n<li>Dwarkesh notes that Sutton is famously the author of <a href=\"http://www.incompleteideas.net/IncIdeas/BitterLesson.html\">The Bitter Lesson</a>, which is constantly cited as inspiring and justifying the whole \u2018stack more layers\u2019 scaling of LLMs that basically worked, yet Sutton doesn\u2019t see LLMs as \u2018bitter lesson\u2019 pilled. Sutton says they\u2019re also putting in lots of human knowledge, so kinda yes kinda no, he expects that new systems that \u2018learn from experience\u2019 and \u2018perform much better\u2019 and are \u2018more scalable\u2019 to then be another instance of the Bitter Lesson?\n<ol>\n<li>This seems like backtracking on the Bitter Lesson? At least kinda. Mostly he\u2019s repeating that LLMs are one way and it\u2019s the other way, and therefore Bitter Lesson will be illustrated the other way?</li>\n</ol>\n</li>\n<li>\u201cIn every case of the bitter lesson you could start with human knowledge and then do the scalable things. That\u2019s always the case. There\u2019s never any reason why that has to be bad. But in fact, and in practice, it has always turned out to be bad. People get locked into the human knowledge approach, and they psychologically\u2026 Now I\u2019m speculating why it is, but this is what has always happened. They get their lunch eaten by the methods that are truly scalable.\u201d\n<ol>\n<li>I do not get where \u2018truly scalable\u2019 is coming from here, as it becomes increasingly clear that he is using words in a way I\u2019ve never seen before.</li>\n<li>If anything it is the opposite. The real objection is training efficiency, or failure to properly update from direct relevant experiences, neither of which has anything to do with scaling.</li>\n<li>I also continue not to see why there is this distinction \u2018human knowledge\u2019 versus other information? Any information available to the AI can be coded as tokens and be put into an LLM, regardless of its \u2018humanness.\u2019 The AI can still gather or create knowledge on its own, and LLMs often do.</li>\n</ol>\n</li>\n<li>\u201cThe scalable method is you learn from experience. You try things, you see what works. No one has to tell you. First of all, you have a goal. Without a goal, there\u2019s no sense of right or wrong or better or worse. Large language models are trying to get by without having a goal or a sense of better or worse. That\u2019s just exactly starting in the wrong place.\u201d\n<ol>\n<li>Again, the word \u2018scaling\u2019 is being used in a completely alien manner here. He seems to be trying to say \u2018successful\u2019 or \u2018efficient.\u2019</li>\n<li>You have to have a \u2018goal\u2019 in the sense of a means of selecting actions, and a way of updating based on those actions, but in this sense LLMs in training very obviously have \u2018goals\u2019 regardless of whether you\u2019d use that word that way.</li>\n<li>Except Sutton seems to think this \u2018goal\u2019 needs to exist in some \u2018real world\u2019 sense or it doesn\u2019t count and I continue to be boggled by this request, and there are many obvious counterexamples, but I risk repeating myself.</li>\n<li>No sense of better or worse? What do you think thumbs up and down are? What do you think evaluators are? Does he not think an LLM can do evaluation?</li>\n</ol>\n</li>\n</ol>\n<p>Sutton has a reasonable hypothesis that a different architecture, that uses a form of continual learning and that does so via real world interaction, would be an interesting and potentially better approach to AI. That might be true.</p>\n<p>But his uses of words do not seem to match their definitions or common usage, his characterizations of LLMs seem deeply confused, and he\u2019s drawing a bunch of distinctinctions and treating them as meaningful in ways that I don\u2019t understand. This results in absurd claims like \u2018LLMs are not intelligent and do not have goals\u2019 and that feedback from digital systems doesn\u2019t count and so on.</p>\n<p>It seems like a form of essentialism, the idea that \u2018oh LLMs can never [X] because they don\u2019t [Y]\u2019 where when you then point (as people frequently do) to the LLM doing [X] and often also doing [Y] and they say \u2018la la la can\u2019t hear you.\u2019</p>\n\n\n<h4 class=\"wp-block-heading\">Humans Do Imitation Learning</h4>\n\n\n<ol>\n<li>Dwarkesh claims humans initially do imitation learning, Sutton says obviously not. \u201cWhen I see kids, I see kids just trying things and waving their hands around and moving their eyes around. There\u2019s no imitation for how they move their eyes around or even the sounds they make. They may want to create the same sounds, but the actions, the thing that the infant actually does, there\u2019s no targets for that. There are no examples for that.\u201d\n<ol>\n<li>GPT-5 Thinking says partly true, but only 30% in the first months, more later on. Gemini says yes. Claude says yes: \u201cImitation is one of the core learning mechanisms from birth onward. Newborns can imitate facial expressions within hours of birth (tongue protrusion being the classic example). By 6-9 months, they\u2019re doing deferred imitation &#8211; copying actions they saw earlier. The whole mirror neuron system appears to be built for this.\u201d</li>\n<li>Sutton\u2019s claim seems clearly so strong as to be outright false here. He\u2019s not saying \u2018they do more non-imitation learning than imitation learning in the first few months,\u2019 he is saying \u2018there are no examples of that\u2019 and there are very obviously examples of that. Here\u2019s Gemini: \u201c<a href=\"https://www.innovativeinterventionsnj.com/post/infants-and-imitation-what-it-means-when-your-baby-mirrors-your-actions#:~:text=Imitation%20is%20one%20of%20the,%2C%20timing%2C%20and%20visual%20tracking.\">Research has shown</a> that newborns, some just a few hours old, can imitate simple facial expressions like sticking out their tongue or opening their mouth. This early imitation is believed to be a reflexive behavior that lays the groundwork for more intentional imitation later on.\u201d</li>\n</ol>\n</li>\n<li>\u201cSchool is much later. Okay, I shouldn\u2019t have said never. I don\u2019t know, I think I would even say that about school. But formal schooling is the exception. You shouldn\u2019t base your theories on that.\u201d \u201cSupervised learning is not something that happens in nature. Even if that were the case with school, we should forget about it because that\u2019s some special thing that happens in people.\u201d\n<ol>\n<li>At this point I kind of wonder if Sutton has met humans?</li>\n<li>As in, I do imitation learning. All. The Time. Don\u2019t you? Like, what?</li>\n<li>As in, I do supervised learning. All. The. Time. Don\u2019t you? Like, what?</li>\n<li>A lot of this supervised and imitation learning happens outside of \u2018school.\u2019</li>\n<li>You even see supervised learning in animals, given the existence of human supervisors who want to teach them things. Good dog! Good boy!</li>\n<li>You definitely see imitation learning in animals. Monkey see, monkey do.</li>\n<li>The reason not to do supervised learning is the cost of the supervisor, or (such as in the case of nature) their unavailability. Thus nature supervises, instead.</li>\n<li>The reason not to do imitation learning in a given context is the cost of the thing to imitate, or the lack of a good enough thing to imitate to let you continue to sufficiently progress.</li>\n</ol>\n</li>\n<li>\u201cWhy are you trying to distinguish humans? Humans are animals. What we have in common is more interesting. What distinguishes us, we should be paying less attention to.\u201d \u201cI like the way you consider that obvious, because I consider the opposite obvious. We have to understand how we are animals. If we understood a squirrel, I think we\u2019d be almost all the way there to understanding human intelligence. The language part is just a small veneer on the surface.\u201d\n<ol>\n<li>Because we want to create something that has what only humans have and humans don\u2019t, which is a high level of intelligence and ability to optimize the arrangements of atoms according to our preferences and goals.</li>\n<li>Understanding an existing intelligence is not the same thing as building a new intelligence, which we have also managed to build without understanding.</li>\n<li>The way animals have (limited) intelligence does not mean this is the One True Way that intelligence can ever exist. There\u2019s no inherent reason an AI needs to mimic a human let alone an animal, except for imitation learning, or in ways we find this to be useful. We\u2019re kind of looking for our keys under the streetlamp here, while assuming there are no keys elsewhere, and I think we\u2019re going to be in for some very rude (or perhaps pleasant?) surprises.</li>\n<li>I don\u2019t want to make a virtual squirrel and scale it up. Do you?</li>\n</ol>\n</li>\n<li>The process of humans learning things over 10k years a la Henrich, of figuring out a many-step long process, where you can\u2019t one-shot the reasoning process. This knowledge evolves over time, and is passed down through imitation learning, as are other cultural practices and gains. Sutton agrees, but calls this a \u2018small thing.\u2019\n<ol>\n<li>You could of course one-shot the process with sufficient intelligence and understanding of the world, what Henrich is pointing out is that in practice this was obviously impossible and not how any of this went down.</li>\n<li>Seems like Sutton is saying again that the difference between humans and squirrels is a \u2018small thing\u2019 and we shouldn\u2019t care about it? I disagree.</li>\n</ol>\n</li>\n<li>They agree that mammals can do continual learning and LLMs can\u2019t. We all agree that Moravec\u2019s paradox is a thing.\n<ol>\n<li>Moravec\u2019s paradox is misleading. There will of course be all four quadrants of things, where for each of [AI, human] things will be [easy, hard].</li>\n<li>The same is true for any pair of humans, or any pair of AIs, to a lesser degree.</li>\n<li>The reason it is labeled a paradox is that there are some divergences that look very large, larger than one might expect, but this isn\u2019t obvious to me.</li>\n</ol>\n</li>\n</ol>\n\n\n<h4 class=\"wp-block-heading\">The Experimental Paradigm</h4>\n\n\n<ol>\n<li>\u201cThe experiential paradigm. Let\u2019s lay it out a little bit. It says that experience, action, sensation\u2014well, sensation, action, reward\u2014this happens on and on and on for your life. It says that this is the foundation and the focus of intelligence. Intelligence is about taking that stream and altering the actions to increase the rewards in the stream\u2026. This is what the reinforcement learning paradigm is, learning from experience.\u201d\n<ol>\n<li>Can be. Doesn\u2019t have to be.</li>\n<li>A priori knowledge exists. Paging Descartes\u2019 meditator! Molyneux\u2019s problem.</li>\n<li>Words, written and voiced, are sensation, and can also be reward.</li>\n<li>Thoughts and predictions, and saying or writing words, are actions.</li>\n<li>All of these are experiences. You can do RL on them (and humans do this).</li>\n</ol>\n</li>\n<li>Sutton agrees that the reward function is arbitrary, and can often be \u2018seek pleasure and avoid pain.\u2019\n<ol>\n<li>That sounds exactly like \u2018make number go up\u2019 with extra steps.</li>\n</ol>\n</li>\n<li>Sutton wants to say \u2018network\u2019 instead of \u2018model.\u2019\n<ol>\n<li>Okie dokie, this does cause confusion with \u2018world models\u2019 that minds have, as Sutton points out later, so using the same word for both is unfortunate.</li>\n<li>I do think we\u2019re stuck with \u2018model\u2019 here, but I\u2019d be happy to support moving to \u2018network\u2019 or another alternative if one got momentum.</li>\n</ol>\n</li>\n<li>He points out that copying minds is a huge cost savings, more than \u2018trying to learn from people.\u2019\n<ol>\n<li>Okie dokie, again, but these two are not rivalrous actions.</li>\n<li>If anything they are complements. If you learn from general knowledge and experiences it is highly useful to copy you. If you are learning from local particular experiences then your usefulness is likely more localized.</li>\n<li>As in, suppose I had a GPT-5 instance, embodied in a humanoid robot, that did continual learning, which let\u2019s call Daneel. I expect that Daneel would rapidly become a better fit to me than to others.</li>\n<li>Why wouldn\u2019t you want to learn from all sources, and then make copies?</li>\n<li>One answer would be \u2018because to store all that info the network would need to be too large and thus too expensive\u2019 but that again pushes you in the other direction, and towards additional scaffolding solutions.</li>\n</ol>\n</li>\n<li>They discuss temporal difference learning and finding intermediate objectives.</li>\n<li>Sutton brings up the \u2018big world hypothesis\u2019 where to be maximally useful a human or AI needs particular knowledge of a particular part of the world. In continual learning the knowledge goes into weights. \u201cYou learn a policy that\u2019s specific to the environment that you\u2019re finding yourself in.\u201d\n<ol>\n<li>Well sure, but there are any number of ways to get that context, and to learn that policy. You can even write the policy down (e.g. in claude.md).</li>\n<li>Often it would be actively unwise to put that knowledge into weights. There is a reason humans will often use forms of external memory. If you were planning to copy a human into other contexts you\u2019d use it even more.</li>\n</ol>\n</li>\n</ol>\n<div>\n<figure>\n<div>\n\n\n<figure class=\"wp-block-image\"><img alt=\"\" src=\"https://substackcdn.com/image/fetch/$s_!oH5R!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8f00502b-bd9d-492b-9ae7-7aa04042d724_1456x776.webp\" /></figure>\n\n\n<div></div>\n</div>\n</figure>\n</div>\n<ol>\n<li>Sutton lays out the above common model of the agent. The new claim seems to be that you learn from all the sensation you receive, not just from the reward. And there is emphasis on the importance of the \u2018transition model\u2019 of the world.\n<ol>\n<li>I once again don\u2019t see the distinction between this and learning from a stream of tokens, whether one or two directional, or even from contemplation, where again (if you had an optimal learning policy) you would pay attention to all the tokens and not only to the formal reward, as indeed a human does when learning from a text, or from sending tokens and getting tokens back in various forms.</li>\n<li>In terms of having a \u2018transition model,\u2019 I would say that again this is something all agents or networks need similarly, and can \u2018get away with not having\u2019 to roughly similar extents.</li>\n</ol>\n</li>\n</ol>\n\n\n<h4 class=\"wp-block-heading\">Current Architectures Generalize Poorly Out Of Distribution</h4>\n\n\n<p>So do humans.</p>\n<ol>\n<li>Sutton claims people live in one world that may involve chess or Atari games and and can generalize across not only games but states, and will happen whether that generalization is good or bad. Whereas gradient descent will not make you generalize well, and we need algorithms where the generalization is good.\n<ol>\n<li>I\u2019m not convinced that LLMs or SGD generalize out-of-distribution (OOD) poorly relative to other systems, including humans or RL systems, once you control for various other factors.</li>\n<li>I do agree that LLMs will often do pretty dumb or crazy things OOD.</li>\n<li>All algorithms will solve the problem at hand. If you want that solution to generalize, you need to either make the expectation of such generalization part of the de facto evaluation function, develop heuristics and methods that tend to lead to generalization for other reasons, or otherwise incorporate the general case, or choose or get lucky with a problem where the otherwise \u2018natural\u2019 solution does still generalize.</li>\n</ol>\n</li>\n<li>\u201cWell maybe that [LLMs] don\u2019t need to generalize to get them right, because the only way to get some of them right is to form something which gets all of them right. If there\u2019s only one answer and you find it, that\u2019s not called generalization. It\u2019s just it\u2019s the only way to solve it, and so they find the only way to solve it. But generalization is when it could be this way, it could be that way, and they do it the good way.\u201d\n<ol>\n<li>Sutton only thinks you can generalize given the ability to not generalize, the way good requires the possibility of evil. It is a relative descriptor.</li>\n<li>I don\u2019t understand why you\u2019d find that definition useful or valid. I care about the generality of your solution in practice, not whether there was a more or less general alternative solution also available.</li>\n<li>Once again there\u2019s this focus on whether something \u2018counts\u2019 as a thing. Yes, of course, if the only or simplest or easiest way to solve a special case is to solve the general case, which often happens, and thus you solve the general case, and this happens to solve a bunch of problem types you didn\u2019t consider, then you have done generalization. Your solution will work in the general case, whether or not you call that OOD.</li>\n<li>If there\u2019s only one answer and you find it, you still found it.</li>\n<li>This seems pretty central. SGD or RL or other training methods, of both humans and AIs, will solve the problem you hand to them. Not the problem you meant to solve, the problem and optimization target you actually presented.</li>\n<li>You need to design that target and choose that method, such that this results in a solution that does what you want it to do. You can approach that in any number of ways, and ideally (assuming you want a general solution) you will choose to set the problem up such that the only or best available solution generalizes, if necessary via penalizing solutions that don\u2019t in various ways.</li>\n</ol>\n</li>\n<li>Sutton claims coding agents trained via SGD will only find solutions to problems they have seen, and yes sometimes the only solution will generalize but nothing in their algorithms will cause them to choose solutions that generalize well.\n<ol>\n<li>Very obviously coding agents generalize to problems they haven\u2019t seen.</li>\n<li>Not fully to \u2018all coding of all things\u2019 but they generalize quite a bit and are generalizing better over time. Seems odd to deny this?</li>\n<li>Sutton is making at least two different claims.</li>\n<li>The first claim is that coding agents only find solutions to problems they have seen. This is at least a large overstatement.</li>\n<li>The second claim is that the algorithms will not cause the network to choose solutions that generalize well over alternative solutions that don\u2019t.</li>\n<li>The second claim is true by default. As Sutton notes, sometimes the default or only solution does indeed generalize well. I would say this happens often. But yeah, sometimes by default this isn\u2019t true, and then by construction and default there is nothing pushing towards finding the general solution.</li>\n<li>Unless you design the training algorithms and data to favor the general solution. If you select your data well, often you can penalize or invalidate non-general solutions, and there are various algorithmic modifications available.</li>\n<li>One solution type is giving the LLM an inherent preference for generality, or have the evaluator choose with a value towards generality, or both.</li>\n<li>No, it isn\u2019t going to be easy, but why should it be? If you want generality you have to ask for it. Again, compare to a human or an RL program. I\u2019m not going for a more general solution unless I am motivated to do so, which can happen for any number of reasons.</li>\n</ol>\n</li>\n</ol>\n\n\n<h4 class=\"wp-block-heading\">Surprises In The AI Field</h4>\n\n\n<ol>\n<li>Dwarkesh asks what has been surprising in AI\u2019s big picture? Sutton says the effectiveness of artificial neural networks. He says \u2018weak\u2019 methods like search and learning have totally won over \u2018strong\u2019 methods that come from \u2018imbuing a system with human knowledge.\u2019\n<ol>\n<li>I find it interesting that Sutton in particular was surprised by ANNs. He is placing a lot of emphasis on copying animals, which seems like it would lead to expecting ANNs.</li>\n<li>It feels like he\u2019s trying to make \u2018don\u2019t imbue the system with human knowledge\u2019 happen? To me that\u2019s not what makes the \u2018strong\u2019 systems strong, or the thing that failed. The thing that failed was GOFAI, the idea that you would hardcode a bunch of logic and human knowledge in particular ways, and tell the AI how to do things, rather than letting the AI find solutions through search and learning. But that can still involve learning from human knowledge.</li>\n<li>It doesn\u2019t have to (see AlphaZero and previously TD-Gammon as Sutton points out), and yes that was somewhat surprising but also kind of not, in the sense that with <a href=\"https://thezvi.substack.com/p/more-dakka\">More Dakka</a> within a compact space like chess you can just solve the game from scratch.</li>\n<li>As in: We don\u2019t need to use human knowledge to master chess, because we can learn chess through self-play beyond human ability levels, and we have enough compute and data that way that we can do it \u2018the hard way.\u2019 Sure.</li>\n</ol>\n</li>\n</ol>\n\n\n<h4 class=\"wp-block-heading\">Will The Bitter Lesson Apply After AGI?</h4>\n\n\n<ol>\n<li>Dwarkesh asks what happens to scaling laws after AGI is created that can do AI research. Sutton says: \u201cThese AGIs, if they\u2019re not superhuman already, then the knowledge that they might impart would be not superhuman.\u201d\n<ol>\n<li>This seems like more characterization insistence combined with category error?</li>\n<li>And it ignores or denies the premise of the question, which is that AGI allows you to scale researcher time with compute the same way we previously could scale compute spend in other places. Sutton agrees that doing bespoke work is helpful, it\u2019s just that it doesn\u2019t scale, but what if it did?</li>\n<li>Even if the AGI is not \u2018superhuman\u2019 per se, the ability to run it faster and in parallel and with various other advantages means it can plausibly produce superhuman work in AI R&amp;D. Already we have AIs that can do \u2018superhuman\u2019 tasks in various domains, even regular computers are \u2018superhuman\u2019 in some subdomains (e.g. arithmetic).</li>\n</ol>\n</li>\n<li>\u201cSo why do you say, \u201cBring in other agents\u2019 expertise to teach it\u201d, when it\u2019s worked so well from experience and not by help from another agent?\u201d\n<ol>\n<li>Help from another agent is experience. It can also directly create experience.</li>\n<li>The context is chess where this is even more true.</li>\n<li>Indeed, the way AlphaZero was trained was not to not involve other agents. The way AlphaZero was trained involved heavy use of other agents, except all those other agents were also AlphaZero.</li>\n</ol>\n</li>\n<li>Dwarkesh focuses specifically on the \u2018billions of AI researchers\u2019 case, Sutton says that\u2019s an interesting case very different from today and The Bitter Lesson doesn\u2019t have to apply. Better to ask questions like whether you should use compute to enhance a few agents or spread it around to spin up more of them, and how they will interact. \u201cMore questions, will it be possible to really spawn it off, send it out, learn something new, something perhaps very new, and then will it be able to be reincorporated into the original? Or will it have changed so much that it can\u2019t really be done? Is that possible or is that not?\u201d\n<ol>\n<li>I agree that things get strange and different and we should ask new questions.</li>\n<li>Asking whether it is possible for an ASI (superintelligent AI) copy to learn something new and then incorporate it into the original seems like such a strange question.\n<ol>\n<li>It presupposes this \u2018continual learning\u2019 thesis where the copy \u2018learns\u2019 the information via direct incorporation into its weights.</li>\n<li>It then assumes that passing on this new knowledge requires incorporation directly into weights or something weird?</li>\n<li>As opposed to, ya know, writing the insight down and the other ASI reading it? If ASIs are indeed superintelligent and do continual learning, why can\u2019t they learn via reading? Wouldn\u2019t they also get very good at knowing how to describe what they know?</li>\n<li>Also, yes, I\u2019m pretty confident you can also do this via direct incorporation of the relevant experiences, even if the full Sutton model holds here in ways I don\u2019t expect. You should be able to merge deltas directly in various ways we already know about, and in better ways that these ASIs will be able to figure out.</li>\n<li>Even if nothing else works, you can simply have the \u2018base\u2019 version of the ASI in question rerun the relevant experiences once it is verified that they led to something worthwhile, reducing this to the previous problem, says the mathematician.</li>\n</ol>\n</li>\n</ol>\n</li>\n<li>Sutton also speculates about potential for corruption or insanity and similar dangers, if a central mind is incorporating the experiences or knowledge of other copies of itself. He expects this to be a big concern, including \u2018mind viruses.\u2019\n<ol>\n<li>Seems fun to think about, but nothing an army of ASIs couldn\u2019t handle.</li>\n<li>In general, when imagining scenarios with armies of ASIs, you have to price into everything the fact that they can solve problems way better than you.</li>\n<li>I don\u2019t think the associated \u2018mind viruses\u2019 in this scenario are fundamentally different than the problems with memetics and hazardous information we experience today, although they\u2019ll be at a higher level.</li>\n<li>I would of course expect lots of new unexpected and weird problems to arise.</li>\n</ol>\n</li>\n</ol>\n\n\n<h4 class=\"wp-block-heading\">Succession To AI</h4>\n\n\n<p>It\u2019s Sutton, so eventually we were going to have to deal with him being a successionist.</p>\n<ol>\n<li>He argues that succession is inevitable for four reasons: Humanity is incapable of a united front, we will eventually figure out intelligence, we will eventually figure out superhuman intelligence, and it is inevitable that over time the most intelligent things around would gain intelligence and power.\n<ol>\n<li>We can divide this into two parts. Let \u201cit\u201d equal superintelligence.</li>\n<li>Let\u2019s call part one Someone Will Build It.</li>\n<li>Let\u2019s call part two <a href=\"https://www.amazon.com/Anyone-Builds-Everyone-Dies-Superhuman/dp/0316595640\">If Anyone Builds It, Everyone Dies</a>.\n<ol>\n<li>Okay, sure, not quite as you see below, but mostly? Yeah, mostly.</li>\n</ol>\n</li>\n<li>Therefore, Everyone Will Die. Successionism is inevitable.</li>\n<li>Part two is actually a very strong argument! It is simpler and cleaner and in many ways more convincing than the book\u2019s version, at least in terms of establishing this as a baseline outcome. It doesn\u2019t require (or give the impression it requires) any assumptions whatsoever about the way we get to superintelligence, what form that superintelligence takes, nothing.</li>\n<li>I actually think this should be fully convincing of the weaker argument that by default (rather than inevitably) this happens, and that there is a large risk of this happening, and something has to go very right for it to not happen.</li>\n<li>If you say \u2018oh even if we do build superintelligence there\u2019s no risk of this happening\u2019 I consider this to be Obvious Nonsense and you not to be thinking.</li>\n<li>I don\u2019t think this argument is convincing that it is \u2018inevitable.\u2019 Facts not in evidence, and there seem like two very obvious counterexamples.\n<ol>\n<li>Counterexample one is that if the intelligence gap is not so large in practical impact, other attributes can more than compensate for this. Other attributes, both mental and physical, also matter and can make up for this. Alas, this seems unlikely to be relevant given the expected intelligence gaps.</li>\n<li>Counterexample two is that you could \u2018solve the alignment problem\u2019 in a sufficiently robust sense that the more intelligent minds optimize for a world in which the less intelligent minds retain power in a sufficiently robust way. Extremely tricky, but definitely not impossible in theory.</li>\n</ol>\n</li>\n<li>However his definition of what is inevitable, and what counts as \u2018succession\u2019 here, is actually much more optimistic than I previously realized\u2026</li>\n<li>If we agree that If Anyone Builds It, Everyone Dies, then the logical conclusion is \u2018Then Let\u2019s Coordinate To Ensure No One F***ing Build It.\u2019</li>\n<li>He claims nope, can\u2019t happen, impossible, give up. I say, if everyone was convinced of part two, then that would change this.</li>\n</ol>\n</li>\n<li>\u201cPut all that together and it\u2019s sort of inevitable. You\u2019re going to have succession to AI or to AI-enabled, augmented humans. Those four things seem clear and sure to happen. But within that set of possibilities, there could be good outcomes as well as less good outcomes, bad outcomes. I\u2019m just trying to be realistic about where we are and ask how we should feel about it.\u201d\n<ol>\n<li>If \u2018AI-enhanced, augmented humans\u2019 count here, well, that\u2019s me, right now.</li>\n<li>I mean, presumably that\u2019s not exactly what he meant.</li>\n<li>But yeah, conditional on us building ASIs or even AGIs, we\u2019re at least dealing with some form of augmented humans.</li>\n<li>Talk of \u2018merge with the AI\u2019 is nonsense, you\u2019re not adding anything to it, but it can enhance you.</li>\n</ol>\n</li>\n<li>\u201cI mark this as one of the four great stages of the universe. First there\u2019s dust, it ends with stars. Stars make planets. The planets can give rise to life. Now we\u2019re giving rise to designed entities. I think we should be proud that we are giving rise to this great transition in the universe.\u201d\n<ol>\n<li>Designed is being used rather loosely here, but we get the idea.</li>\n<li>We already have created designed things, and yeah that\u2019s pretty cool.</li>\n</ol>\n</li>\n<li>\u201cIt\u2019s an interesting thing. Should we consider them part of humanity or different from humanity? It\u2019s our choice. It\u2019s our choice whether we should say, \u201cOh, they are our offspring and we should be proud of them and we should celebrate their achievements.\u201d Or we could say, \u201cOh no, they\u2019re not us and we should be horrified.\u201d\u201d\n<ol>\n<li>It\u2019s not about whether they are \u2018part of humanity\u2019 or our \u2018children.\u2019 They\u2019re not.</li>\n<li>They can still have value. One can imagine aliens (as many stories have) that are not these things and still have value.</li>\n<li>That doesn\u2019t mean that us going away would therefore be non-horrifying.</li>\n</ol>\n</li>\n<li>\u201cA lot of it has to do with just how you feel about change. If you think the current situation is really good, then you\u2019re more likely to be suspicious of change and averse to change than if you think it\u2019s imperfect. I think it\u2019s imperfect. In fact, I think it\u2019s pretty bad. So I\u2019m open to change. I think humanity has not had a super good track record. Maybe it\u2019s the best thing that there has been, but it\u2019s far from perfect.\u201d \u201cI think it\u2019s appropriate for us to really work towards our own local goals. It\u2019s kind of aggressive for us to say, \u201cOh, the future has to evolve this way that I want it to.\u201d\u201d\n<ol>\n<li>So there you have it.</li>\n<li>I disagree.</li>\n</ol>\n</li>\n<li>\u201cSo we\u2019re trying to design the future and the principles by which it will evolve and come into being. The first thing you\u2019re saying is, \u201cWell, we try to teach our children general principles which will promote more likely evolutions.\u201d Maybe we should also seek for things to be voluntary. If there is change, we want it to be voluntary rather than imposed on people. I think that\u2019s a very important point. That\u2019s all good.\u201d\n<ol>\n<li>This is interestingly super different and in conflict with the previous claim.</li>\n<li>It\u2019s fully the other way so far that I don\u2019t even fully endorse it, this idea that change needs to be voluntary whenever it is imposed on people. That neither seems like a reasonable ask, nor does it historically end well, as in the paralysis of the West and especially the Anglosphere in many ways, especially in housing.</li>\n<li>I am very confident in what would happen if you asked about the changes Sutton is anticipating, and put them to a vote.</li>\n</ol>\n</li>\n</ol>\n<p>Fundamentally, I didn\u2019t pull direct quotes on this but Sutton repeatedly emphasizes that AI-dominated futures can be good or bad, that he wants us to steer towards good futures rather than bad futures, and that we should think carefully about which futures we are steering towards and choose deliberately.</p>\n<p>I can certainly get behind that. The difference is that I don\u2019t think we need to accept this transition to AI dominance as our only option, including that I don\u2019t think we should accept that humans will always be unable to coordinate.</p>\n<p>Mostly what I found interesting were the claims around the limitations and nature of LLMs, in ways that don\u2019t make sense to me. This did help solidify a bunch of my thinking about how all of this works, so it felt like a good use of time for that alone.</p>\n\n\n<h4 class=\"wp-block-heading\"></h4>\n\n\n<p>&nbsp;</p>\n\n\n<h4 class=\"wp-block-heading\"></h4>\n\n\n<p>&nbsp;</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p>\n\n\n<h4 class=\"wp-block-heading\"></h4>\n\n\n<p>&nbsp;</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p>"
            ],
            "link": "https://thezvi.wordpress.com/2025/09/29/on-dwarkesh-patels-podcast-with-richard-sutton/",
            "publishedAt": "2025-09-29",
            "source": "TheZvi",
            "summary": "This seems like a good opportunity to do some of my classic detailed podcast coverage. The conventions are: This is not complete, points I did not find of note are skipped. The main part of each point is descriptive of &#8230; <a href=\"https://thezvi.wordpress.com/2025/09/29/on-dwarkesh-patels-podcast-with-richard-sutton/\">Continue reading <span class=\"meta-nav\">&#8594;</span></a>",
            "title": "On Dwarkesh Patel\u2019s Podcast With Richard Sutton"
        },
        {
            "content": [],
            "link": "https://xkcd.com/3148/",
            "publishedAt": "2025-09-29",
            "source": "XKCD",
            "summary": "<img alt=\"I'm trying to share my footage of the full run to prove it's not tool-assisted, but the uploader has problems with video lengths of more than a decade.\" src=\"https://imgs.xkcd.com/comics/100_all_achievements.png\" title=\"I'm trying to share my footage of the full run to prove it's not tool-assisted, but the uploader has problems with video lengths of more than a decade.\" />",
            "title": "100% All Achievements"
        }
    ],
    "lookbackDays": 1,
    "publishDate": "2025-09-29"
}