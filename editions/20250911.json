{
    "articles": [
        {
            "content": [
                "<div class=\"trix-content\">\n  <div>Debates, at their finest, are about exploring topics together in search for truth. That probably sounds hopelessly idealistic to anyone who've ever perused a comment section on the internet, but ideals are there to remind us of what's possible, to inspire us to reach higher \u2014 even if reality falls short.</div><div><br />I've been reaching for those debating ideals for thirty years on the internet. I've argued with tens of thousands of people, first on Usenet, then in blog comments, then Twitter, now X, and also LinkedIn \u2014 as well as a million other places that have come and gone. It's mostly been about technology, but occasionally about society and morality too.<br /><br /></div><div>There have been plenty of heated moments during those three decades. It doesn't take much for a debate between strangers on this internet to escalate into something far lower than a \"search for truth\", and I've often felt willing to settle for just a cordial tone!<br /><br /></div><div>But for the majority of that time, I never felt like things might escalate beyond the keyboards and into the real world. That was until we had <a href=\"https://world.hey.com/dhh/basecamp-s-new-etiquette-regarding-societal-politics-at-work-b44bef69\">our big blow-up at 37signals</a> back in 2021. I suddenly got to see a different darkness from the most vile corners of the internet. Heard from those who seem to prowl for a mob-sanctioned opportunity to threaten and intimidate those they disagree with.</div><div><br />It fundamentally changed me. But I used the experience as a mirror to reflect on the ways my own engagement with the arguments occasionally felt too sharp, too personal. And I've since tried to refocus way more of my efforts on the positive and the productive. I'm by no means perfect, and the internet often tempts the worst in us, but I resist better now than I did then.<br /><br /></div><div>What I cannot come to terms with, though, is the modern equation of words with violence. The growing sense of permission that if the disagreement runs deep enough, then violence is a justified answer to settle it. That sounds so obvious that we shouldn't need to state it in a civil society, but clearly it is not.</div><div><br />Not even in technology. Not even in programming. There are plenty of factions here who've taken to justify their violent fantasies by referring to their ideological opponents as \"nazis\", \"fascists\", or \"racists\". And then follow that up with a call to \"punch a nazi\" or worse.<br /><br /></div><div>When you hear something like that often enough, it's easy to grow glib about it. That it's just a saying. They don't mean it. But I'm afraid many of them really do.</div><div><br />Which brings us to Charlie Kirk. And the technologists who <a href=\"https://mastodon.social/@dnalounge@sfba.social/115183133107034507\">name drinks at their bar after his mortal wound</a> just hours after his death, to name but one of the many, morbid celebrations of the famous conservative debater's death.<br /><br /></div><div>It's sickening. Deeply, profoundly sickening.<br /><br /></div><div>And my first instinct was exactly what such people would delight in happening. To watch the rest of us recoil, then retract, and perhaps even eject. To leave the internet for a while or forever. But I can't do that. We shouldn't do that.<br /><br /></div><div>Instead, we should double down on the opposite. Continue to show up with our ideals held high while we debate strangers in that noble search for the truth. Where we share our excitement, our enthusiasm, and our love of technology, country, and humanity.</div><div><br />I think that's what Charlie Kirk did so well. Continued to show up for the debate. Even on hostile territory. Not because he thought he was ever going to convince everyone, but because he knew he'd always reach some with a good argument, a good insight, or at least a different perspective.<br /><br /></div><div>You could agree or not. Counter or be quiet. But the earnest exploration of the topics in a live exchange with another human is as fundamental to our civilization as Socrates himself.</div><div><br />Don't give up, don't give in. Keep debating.</div>\n</div>"
            ],
            "link": "https://world.hey.com/dhh/words-are-not-violence-c751f14f",
            "publishedAt": "2025-09-11",
            "source": "DHH",
            "summary": "<div class=\"trix-content\"> <div>Debates, at their finest, are about exploring topics together in search for truth. That probably sounds hopelessly idealistic to anyone who've ever perused a comment section on the internet, but ideals are there to remind us of what's possible, to inspire us to reach higher \u2014 even if reality falls short.</div><div><br />I've been reaching for those debating ideals for thirty years on the internet. I've argued with tens of thousands of people, first on Usenet, then in blog comments, then Twitter, now X, and also LinkedIn \u2014 as well as a million other places that have come and gone. It's mostly been about technology, but occasionally about society and morality too.<br /><br /></div><div>There have been plenty of heated moments during those three decades. It doesn't take much for a debate between strangers on this internet to escalate into something far lower than a \"search for truth\", and I've often felt willing to settle for just a cordial tone!<br /><br /></div><div>But for the majority of that time, I never felt like things might escalate beyond the keyboards and into the real world. That was until we had <a href=\"https://world.hey.com/dhh/basecamp-s-new-etiquette-regarding-societal-politics-at-work-b44bef69\">our big blow-up at 37signals</a> back in 2021. I suddenly got",
            "title": "Words are not violence"
        },
        {
            "content": [
                "<p>PendingKetchup comments on my recent post on <a href=\"https://dynomight.net/heritable/\">what it means for something to be heritable</a>:</p>\n\n<blockquote>\n  <p>The article seems pretty good at math and thinking through unusual implications, but my armchair Substack eugenics alarm that I keep in the back of my brain is beeping.</p>\n\n  <p>Saying that variance was \u201cinvented for the purpose of defining heritability\u201d is <em>technically</em> correct, but that might not be the best kind of correct in this case, because it was invented <em>by</em> the founder of the University of Cambridge Eugenics Society who had decided, presumably to support that project, that he wanted to define something called \u201cheritability\u201d.</p>\n\n  <p>His particular formula for heritability is presented in the article as if it has odd traits but is obviously basically a sound thing to want to calculate, despite the purpose it was designed for.</p>\n\n  <p>The vigorous \u201ceducational attainment is 40% heritable, well OK maybe not but it\u2019s a lot heritable, stop quibbling\u201d hand waving sounds like a person who wants to show but can\u2019t support a large figure. And that framing of education, as something \u201cattained\u201d by people, rather than something afforded to or invested in them, is almost completely backwards at least through college.</p>\n\n  <p>The various examples about evil despots and unstoppable crabs highlight how heritability can look large or small independent of more straightforward biologically-mechanistic effects of DNA. But they still give the impression that those are the unusual or exceptional cases.</p>\n\n  <p>In reality, there are in fact a lot of evil crabs, doing things like systematically carting away resources from Black children\u2019s* schools, and then throwing them in jail. We should expect evil-crab-based explanations of differences between people to be the predominant ones.</p>\n\n  <p>*Not to say that being Black \u201cis genetic\u201d. Things from accent to how you style your hair to how you dress to what country you happen to be standing in all contribute to racial judgements used for racism. But \u201cheritability\u201d may not be the right tool to disentangle those effects.</p>\n</blockquote>\n\n<p>Dear PendingKetchup,</p>\n\n<p>Thanks for complimenting my math (\u2661), for reading all the way to the evil crabs, and for not explicitly calling me a racist or eugenicist. I also appreciate that you chose sincerity over boring sarcasm and that you painted such a vibrant picture of what you were thinking while reading my post. I hope you won\u2019t mind if I respond in the same spirit.</p>\n\n<p>To start, I\u2019d like to admit something. When I wrote that post, I suspected some people might have reactions similar to yours. I don\u2019t like that. I prefer positive feedback! But I\u2019ve basically decided to just let reactions like yours happen, because I don\u2019t know how to avoid them without compromising on other core goals.</p>\n\n<p>It sounds like my post gave you a weird feeling. Would it be fair to describe it as a feeling that I\u2019m not being totally upfront about what I really think about race / history / intelligence / biological determinism / the ideal organization of society?</p>\n\n<p>Because if so, you\u2019re right. It\u2019s not supposed to be a secret, but it\u2019s true.</p>\n\n<p>Why? Well, you may doubt this, but when I wrote that post, my goal was that people who read it would come away with a better understanding of the meaning of heritability and how weird it is. That\u2019s it.</p>\n\n<p>Do I have some deeper and darker motivations? Probably. If I probe my subconscious, I find traces of various embarrassing things like \u201cdraw attention to myself\u201d or \u201cmake people think I am smart\u201d or \u201cafter I die, live forever in the world of ideas through my amazing invention of blue-eye-seeking / human-growth-hormone-injecting crabs.\u201d</p>\n\n<p>What I <em>don\u2019t</em> find are any goals related to eugenics, Ronald Fisher, the heritability of educational attainment, if \u201ceducational attainment\u201d is good terminology, racism, oppression, schools, the justice system, or how society should be organized.</p>\n\n<p>These were all non-goals for basically two reasons:</p>\n\n<ol>\n  <li>\n    <p>My views on those issues aren\u2019t very interesting or notable. I didn\u2019t think anyone would (or should) care about them.</p>\n  </li>\n  <li>\n    <p>Surely, there is some place in the world for things that just try to explain what heritability really means? If that\u2019s what\u2019s promised, then it seems weird to drop in a surprise morality / politics lecture.</p>\n  </li>\n</ol>\n\n<p>At the same time, let me concede something else. The weird feeling you got as you read my post might be grounded in statistical truth. That is, it might be true that many people who blog about things like heritability have social views you wouldn\u2019t like. And it might be true that some of them pretend at truth-seeking but are mostly just charlatans out to promote those unliked-by-you social views.</p>\n\n<p>You\u2019re dead wrong to think that\u2019s what I\u2019m doing. All your theories of things I\u2019m trying to suggest or imply are unequivocally false. But given the statistical realities, I guess I can\u2019t blame you too much for having your suspicions.</p>\n\n<p>So you might ask\u2014if my goal is just to explain heritability, why not make that explicit? Why not have a disclaimer that says, \u201cOK I understand that heritability is fraught and blah blah blah, but I just want to focus on the technical meaning because\u2026\u201d?</p>\n\n<p>One reason is that I think that\u2019s boring and condescending. I don\u2019t think people need me to tell them that heritability is fraught. <em>You</em> clearly did not need me to tell you that.</p>\n\n<p>Also, I don\u2019t think such disclaimers make you look neutral. Everyone knows that people with certain social views (likely similar to yours) are more likely to give such disclaimers. And they apply the same style of statistical reasoning you used to conclude I might be a eugenicist. I don\u2019t want people who disagree with those social views to think they can\u2019t trust me.</p>\n\n<p>Paradoxically, such disclaimers often seem to invite more objections from people who <em>share</em> the views they\u2019re correlated with, too. Perhaps that\u2019s because the more signals we get that someone is on \u201cour\u201d side, the more we tend to notice ideological violations. (I\u2019d refer here to the <a href=\"https://en.wikipedia.org/wiki/Narcissism_of_small_differences\">narcissism of small differences</a>, though I worry you may find that reference objectionable.)</p>\n\n<p>If you want to focus on the facts, the best strategy seems to be serene and spiky: to demonstrate by your actions that you are on no one\u2019s side, that you don\u2019t <em>care</em> about being on anyone\u2019s side, and that your only loyalty is to readers who want to understand the facts and make up their own damned mind about everything else.</p>\n\n<p>I\u2019m not offended by your comment. I do think it\u2019s a little strange that you\u2019d publicly suggest someone might be a eugenicist on the basis of such limited evidence. But no one is forcing me to write things and put them on the internet.</p>\n\n<p>The reason I\u2019m writing to you is that you were polite and civil and seem well-intentioned. So I wanted you to know that your world model is inaccurate. You seem to think that because my post did not explicitly support your social views, it must have been written with the goal of undermining those views. And that is wrong.</p>\n\n<p>The truth is, I wrote that post without supporting your (or any) social views because I think mixing up facts and social views is bad. Partly, that\u2019s just an aesthetic preference. But if I\u2019m being fully upfront, I also think it\u2019s bad in the consequentialist sense that it makes the world a worse place.</p>\n\n<p>Why do I think this? Well, recall that I pointed out that if there were crabs that injected blue-eyed babies with human growth hormone, that would increase the heritability of height. You suggest I had sinister motives for giving this example, as if I was trying to conceal the corollary that if the environment provided more resources to people with certain genes (e.g. skin color) that could increase the heritability of other things (e.g. educational attainment).</p>\n\n<p>Do you really think you\u2019re the only reader to notice that corollary?</p>\n\n<p>The degree to which things are \u201cheritable\u201d depends on the nature of society. This is a fact. It\u2019s a fact that many people are not aware of. It\u2019s also a fact that\u2014I guess\u2014fits pretty well with your social views. I wanted people to understand that. Not out of loyalty to your social views, but because it is true.</p>\n\n<p>It seems that you\u2019re annoyed that I didn\u2019t phrase all my examples in terms of culture war. I could have done that. But I didn\u2019t, because I think my examples are easier to understand, and because the degree to which changing society might change the heritability of some trait is a contentious empirical question.</p>\n\n<p>But OK. Imagine I had done that. And imagine all the examples were perfectly aligned with your social views. Do you think that would have made the post more or less effective in convincing people that the fact we\u2019re talking about is true? I think the answer is: Far less effective.</p>\n\n<p>I\u2019ll leave you with two questions:</p>\n\n<p><strong>Question 1:</strong> Do you care about the facts? Do you believe the facts are on your side?</p>\n\n<p><strong>Question 2:</strong> Did you <em>really</em> think I wrote that post with with the goal of promoting eugenics?</p>\n\n<p>If you really did think that, then great! I imagine you\u2019ll be interested to learn that you were incorrect.</p>\n\n<p>But just as you had an alarm beeping in your head as you read my post, I had one beeping in my head as I read your comment. My alarm was that you were playing a bit of a game. It\u2019s not that you <em>really</em> think I wanted to promote eugenics, but rather that you\u2019re trying to enforce a norm that everyone must give constant screaming support to your social views and anyone who\u2019s even slightly ambiguous should be ostracized.</p>\n\n<p>Of course, this might be a false alarm! But if that is what you\u2019re doing, I have to tell you: I think that\u2019s a dirty trick, and a perfect example of why mixing facts and social views is bad.</p>\n\n<p>You may disagree with all my motivations. That\u2019s fine. (<em>I</em> won\u2019t assume that means <em>you</em> are a eugenicist.) All I ask is that you disapprove accurately.</p>\n\n<p>xox\n<br />dynomight</p>"
            ],
            "link": "https://dynomight.net/ketchup/",
            "publishedAt": "2025-09-11",
            "source": "Dynomight",
            "summary": "<p>PendingKetchup comments on my recent post on <a href=\"https://dynomight.net/heritable/\">what it means for something to be heritable</a>:</p> <blockquote> <p>The article seems pretty good at math and thinking through unusual implications, but my armchair Substack eugenics alarm that I keep in the back of my brain is beeping.</p> <p>Saying that variance was \u201cinvented for the purpose of defining heritability\u201d is <em>technically</em> correct, but that might not be the best kind of correct in this case, because it was invented <em>by</em> the founder of the University of Cambridge Eugenics Society who had decided, presumably to support that project, that he wanted to define something called \u201cheritability\u201d.</p> <p>His particular formula for heritability is presented in the article as if it has odd traits but is obviously basically a sound thing to want to calculate, despite the purpose it was designed for.</p> <p>The vigorous \u201ceducational attainment is 40% heritable, well OK maybe not but it\u2019s a lot heritable, stop quibbling\u201d hand waving sounds like a person who wants to show but can\u2019t support a large figure. And that framing of education, as something \u201cattained\u201d by people, rather than something afforded to or invested in them, is almost completely backwards at least through college.</p> <p>The various",
            "title": "Dear PendingKetchup"
        },
        {
            "content": [
                "<p>In my book, <a href=\"https://a.co/d/cPsM4Ql\">Co-Intelligence</a>, I outlined a way that people could work with AI, which was, rather unsurprisingly, as a co-intelligence. Teamed with a chatbot, humans could use AI as a sort of intern or co-worker, correcting its errors, checking its work, co-developing ideas, and guiding it in the right direction. Over the past few weeks, I have come to believe that co-intelligence is still important but that the nature of AI is starting to point in a different direction. We're moving from partners to audience, from collaboration to conjuring.</p><p>A good way to illustrate this change is to ask an AI to explain what has happened since I wrote the book. I fed my book and all 140 or so One Useful Thing posts (incidentally, I can&#8217;t believe I have written that many posts!) into <a href=\"https://notebooklm.google.com/\">NotebookLM</a> and chose the new video overview option with a basic prompt to make a video about what has happened in the world of AI.</p><p>A few minutes later, I got this. And it is pretty good. Good enough that I think it is worth watching to get an update on what has happened since my book was written.</p><div class=\"native-video-embed\"></div><p>But how did the AI pick the points it made? I don&#8217;t know, but they were pretty good. How did it decide on the slides to use? I don&#8217;t know, but they were also pretty on target (though images remain a bit of a weak point, as it didn&#8217;t show me the promised otter). Was it right? That seemed like something I should check.</p><p>So, I went through the video several times, checking all the facts. It got all the numbers right, including the data on <a href=\"https://openai.com/index/learning-to-reason-with-llms/\">MMLU scores</a> and the results of AI performance on the <a href=\"https://www.medrxiv.org/content/10.1101/2023.04.06.23288265v1\">neurosurgery exam data</a> (I am not even sure when I cited that material). My only real issue was that it should have noted that<a href=\"https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4573321\"> I was one of several co-authors in our study </a>of Boston Consulting Group that also introduced the term &#8220;jagged frontier.&#8221; Also, I wouldn&#8217;t have said everything the way the AI did (it was a little bombastic, and my book is not out-of-date yet!), but there were no substantive errors.</p><p>I think this process is typical of the new wave of AI, for an increasing range of complex tasks, you get an amazing and sophisticated output in response to a vague request, but you have no part in the process. You don&#8217;t know how the AI made the choices it made, nor can you confirm that everything is completely correct. We're shifting from being collaborators who shape the process to being supplicants who receive the output. It is a transition from working with a co-intelligence to working with a wizard. Magic gets done, but we don&#8217;t always know what to do with the results. This pattern &#8212; impressive output, opaque process &#8212; becomes even more pronounced with research tasks.</p><h1>Asking for Magic</h1><p>Right now, no AI model feels more like a wizard than GPT-5 Pro, which is only accessible to paying users. GPT-5 Pro is capable of some frankly amazing feats. For example, I gave it an academic paper to read with the instructions &#8220;critique the methods of this paper, figure out better methods and apply them.&#8221; This was not just any paper, it was my job market paper, which means my first major work as an academic. It took me over a year to write and was read carefully by many of the brightest people in my field before finally being peer reviewed and published in a major journal. </p><p>Nine minutes and forty seconds later, I had a very detailed critique. This wasn&#8217;t just editorial criticism, GPT-5 Pro apparently ran its own experiments using code to verify my results, including doing Monte Carlo analysis and re-interpreting the fixed effects in my statistical models. It had many suggestions as a result (though it fortunately concluded that &#8220;the headline claim [of my paper] survives scrutiny&#8221;), but one stood out. It found a small error, previously unnoticed. The error involved two different sets of numbers in two tables that were linked in ways I did not explicitly spell out in my paper. The AI found the minor error, no one ever had before.</p><div class=\"captioned-image-container\"><figure><a class=\"image-link image2 is-viewable-img\" href=\"https://substackcdn.com/image/fetch/$s_!iDvt!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd80aac82-5e8b-4c7d-84d3-c8d34f7821c8_831x932.png\" target=\"_blank\"><div class=\"image2-inset\"><source type=\"image/webp\" /><img alt=\"\" class=\"sizing-normal\" height=\"592.173285198556\" src=\"https://substackcdn.com/image/fetch/$s_!iDvt!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd80aac82-5e8b-4c7d-84d3-c8d34f7821c8_831x932.png\" width=\"528\" /><div class=\"image-link-expand\"><div class=\"pencraft pc-display-flex pc-gap-8 pc-reset\"><div class=\"pencraft pc-reset icon-container restack-image\"><svg class=\"lucide lucide-refresh-cw\" fill=\"none\" height=\"20\" stroke=\"currentColor\" stroke-linecap=\"round\" stroke-linejoin=\"round\" stroke-width=\"2\" viewBox=\"0 0 24 24\" width=\"20\" xmlns=\"http://www.w3.org/2000/svg\"><path d=\"M3 12a9 9 0 0 1 9-9 9.75 9.75 0 0 1 6.74 2.74L21 8\"></path><path d=\"M21 3v5h-5\"></path><path d=\"M21 12a9 9 0 0 1-9 9 9.75 9.75 0 0 1-6.74-2.74L3 16\"></path><path d=\"M8 16H3v5\"></path></svg></div><div class=\"pencraft pc-reset icon-container view-image\"><svg class=\"lucide lucide-maximize2 lucide-maximize-2\" fill=\"none\" height=\"20\" stroke=\"currentColor\" stroke-linecap=\"round\" stroke-linejoin=\"round\" stroke-width=\"2\" viewBox=\"0 0 24 24\" width=\"20\" xmlns=\"http://www.w3.org/2000/svg\"><polyline points=\"15 3 21 3 21 9\"></polyline><polyline points=\"9 21 3 21 3 15\"></polyline><line x1=\"21\" x2=\"14\" y1=\"3\" y2=\"10\"></line><line x1=\"3\" x2=\"10\" y1=\"21\" y2=\"14\"></line></svg></div></div></div></div></a></figure></div><p>Again, I was left with the wizard problem: was this right? I checked through the results, and found that it was, but I still have no idea of what the AI did to discover this problem, nor whether the other things it claimed to have done happened as described. But I was impressed by GPT-5 Pro&#8217;s analysis, which is why I now throw all sorts of problems, big and small at the model: Is the <a href=\"https://x.com/emollick/status/1964932143710380398\">Gartner hype cycle real</a>? Did census data <a href=\"https://x.com/emollick/status/1965440611282288707\">show AI use declining at large firms</a>? Just ask GPT-5 Pro and get the right answer. I think. I haven&#8217;t found an error yet, but that doesn&#8217;t mean there aren&#8217;t any. And, of course, there are many other tasks that the AI would fail to deliver any sort of good answer for. Who knows with wizards?</p><p>To see how this might soon apply to work more broadly, consider another advanced AI, Claude 4.1 Opus, which recently gained the ability to work with files.<a href=\"https://x.com/emollick/status/1965608685297922315\"> It is especially talented at Excel</a>, so I gave it a hard challenge on an Excel file I knew well. There is an exercise I used in my entrepreneurship classes that involves analyzing the financial model of a small desk manufacturing business as a lesson about how to plan despite uncertainty. I gave Claude the old, multi-tab Excel file, and asked the AI to update it for a new business - a cheese shop - while still maintaining the goal of the overall exercise.</p><div class=\"captioned-image-container\"><figure><a class=\"image-link image2 is-viewable-img\" href=\"https://substackcdn.com/image/fetch/$s_!AZG7!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4deccb99-02ff-4b14-a0e8-1e9858c9b816_843x477.png\" target=\"_blank\"><div class=\"image2-inset\"><source type=\"image/webp\" /><img alt=\"\" class=\"sizing-normal\" height=\"286.3131672597865\" src=\"https://substackcdn.com/image/fetch/$s_!AZG7!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4deccb99-02ff-4b14-a0e8-1e9858c9b816_843x477.png\" width=\"506\" /><div class=\"image-link-expand\"><div class=\"pencraft pc-display-flex pc-gap-8 pc-reset\"><div class=\"pencraft pc-reset icon-container restack-image\"><svg class=\"lucide lucide-refresh-cw\" fill=\"none\" height=\"20\" stroke=\"currentColor\" stroke-linecap=\"round\" stroke-linejoin=\"round\" stroke-width=\"2\" viewBox=\"0 0 24 24\" width=\"20\" xmlns=\"http://www.w3.org/2000/svg\"><path d=\"M3 12a9 9 0 0 1 9-9 9.75 9.75 0 0 1 6.74 2.74L21 8\"></path><path d=\"M21 3v5h-5\"></path><path d=\"M21 12a9 9 0 0 1-9 9 9.75 9.75 0 0 1-6.74-2.74L3 16\"></path><path d=\"M8 16H3v5\"></path></svg></div><div class=\"pencraft pc-reset icon-container view-image\"><svg class=\"lucide lucide-maximize2 lucide-maximize-2\" fill=\"none\" height=\"20\" stroke=\"currentColor\" stroke-linecap=\"round\" stroke-linejoin=\"round\" stroke-width=\"2\" viewBox=\"0 0 24 24\" width=\"20\" xmlns=\"http://www.w3.org/2000/svg\"><polyline points=\"15 3 21 3 21 9\"></polyline><polyline points=\"9 21 3 21 3 15\"></polyline><line x1=\"21\" x2=\"14\" y1=\"3\" y2=\"10\"></line><line x1=\"3\" x2=\"10\" y1=\"21\" y2=\"14\"></line></svg></div></div></div></div></a></figure></div><p>With just that instruction, it read the lesson plan and the old spreadsheets, including their formulas, and created a new one, updating all of the information to be appropriate for a cheese shop. A few minutes later, with just the one prompt, I had a new, transformed spreadsheet downloaded on my computer, one that had entirely new data while still communicating the key lesson.</p><div class=\"captioned-image-container\"><figure><a class=\"image-link image2 is-viewable-img\" href=\"https://substackcdn.com/image/fetch/$s_!V_aY!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F295ee922-faee-4f17-85f8-6b5628f62b28_2497x1968.png\" target=\"_blank\"><div class=\"image2-inset\"><source type=\"image/webp\" /><img alt=\"\" class=\"sizing-normal\" height=\"488.0576923076923\" src=\"https://substackcdn.com/image/fetch/$s_!V_aY!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F295ee922-faee-4f17-85f8-6b5628f62b28_2497x1968.png\" width=\"619\" /><div class=\"image-link-expand\"><div class=\"pencraft pc-display-flex pc-gap-8 pc-reset\"><div class=\"pencraft pc-reset icon-container restack-image\"><svg class=\"lucide lucide-refresh-cw\" fill=\"none\" height=\"20\" stroke=\"currentColor\" stroke-linecap=\"round\" stroke-linejoin=\"round\" stroke-width=\"2\" viewBox=\"0 0 24 24\" width=\"20\" xmlns=\"http://www.w3.org/2000/svg\"><path d=\"M3 12a9 9 0 0 1 9-9 9.75 9.75 0 0 1 6.74 2.74L21 8\"></path><path d=\"M21 3v5h-5\"></path><path d=\"M21 12a9 9 0 0 1-9 9 9.75 9.75 0 0 1-6.74-2.74L3 16\"></path><path d=\"M8 16H3v5\"></path></svg></div><div class=\"pencraft pc-reset icon-container view-image\"><svg class=\"lucide lucide-maximize2 lucide-maximize-2\" fill=\"none\" height=\"20\" stroke=\"currentColor\" stroke-linecap=\"round\" stroke-linejoin=\"round\" stroke-width=\"2\" viewBox=\"0 0 24 24\" width=\"20\" xmlns=\"http://www.w3.org/2000/svg\"><polyline points=\"15 3 21 3 21 9\"></polyline><polyline points=\"9 21 3 21 3 15\"></polyline><line x1=\"21\" x2=\"14\" y1=\"3\" y2=\"10\"></line><line x1=\"3\" x2=\"10\" y1=\"21\" y2=\"14\"></line></svg></div></div></div></div></a><figcaption class=\"image-caption\">The original document on the left, what Claude gave me on the right</figcaption></figure></div><p>Again, the wizard didn&#8217;t tell me the secret to its tricks, so I had to check the results over carefully. From what I saw, they seemed very good, preserving the lessons in a new context. I did spot a few issues in the formula and business modelling that I would do differently (I would have had fewer business days per year, for example), but that felt more like a difference of opinion than a substantive error.</p><p>Curious to see how far Claude could go, and since everyone always asks me whether AI can do PowerPoint, I also prompted: &#8220;great, now make a good PowerPoint for this business&#8221; and got the following result.</p><div class=\"captioned-image-container\"><figure><a class=\"image-link image2 is-viewable-img\" href=\"https://substackcdn.com/image/fetch/$s_!BmJ9!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff86b03bd-e9d2-4b06-97ef-e2909dc7e18d_1927x943.png\" target=\"_blank\"><div class=\"image2-inset\"><source type=\"image/webp\" /><img alt=\"\" class=\"sizing-normal\" height=\"713\" src=\"https://substackcdn.com/image/fetch/$s_!BmJ9!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff86b03bd-e9d2-4b06-97ef-e2909dc7e18d_1927x943.png\" width=\"1456\" /><div class=\"image-link-expand\"><div class=\"pencraft pc-display-flex pc-gap-8 pc-reset\"><div class=\"pencraft pc-reset icon-container restack-image\"><svg class=\"lucide lucide-refresh-cw\" fill=\"none\" height=\"20\" stroke=\"currentColor\" stroke-linecap=\"round\" stroke-linejoin=\"round\" stroke-width=\"2\" viewBox=\"0 0 24 24\" width=\"20\" xmlns=\"http://www.w3.org/2000/svg\"><path d=\"M3 12a9 9 0 0 1 9-9 9.75 9.75 0 0 1 6.74 2.74L21 8\"></path><path d=\"M21 3v5h-5\"></path><path d=\"M21 12a9 9 0 0 1-9 9 9.75 9.75 0 0 1-6.74-2.74L3 16\"></path><path d=\"M8 16H3v5\"></path></svg></div><div class=\"pencraft pc-reset icon-container view-image\"><svg class=\"lucide lucide-maximize2 lucide-maximize-2\" fill=\"none\" height=\"20\" stroke=\"currentColor\" stroke-linecap=\"round\" stroke-linejoin=\"round\" stroke-width=\"2\" viewBox=\"0 0 24 24\" width=\"20\" xmlns=\"http://www.w3.org/2000/svg\"><polyline points=\"15 3 21 3 21 9\"></polyline><polyline points=\"9 21 3 21 3 15\"></polyline><line x1=\"21\" x2=\"14\" y1=\"3\" y2=\"10\"></line><line x1=\"3\" x2=\"10\" y1=\"21\" y2=\"14\"></line></svg></div></div></div></div></a></figure></div><p>This is a pretty solid start to a pitch deck, and one without any major errors, but it also isn&#8217;t ready-to-go. This emphasizes the jagged frontier of AI: it is very good at some things and worse at others in ways that are hard to predict without experience. I have been showing you examples within the ever-expanding frontier of AI abilities, but that doesn&#8217;t mean that AI can do everything with equal ease. But my focus is less on the expanding range of AI ability in this post, than about our changing relationships with AIs.</p><h1>The Problems with Wizards</h1><p>These new AI systems are essentially agents, AI that can plan and act autonomously toward given goals. When I asked Claude to change my spreadsheet, it planned out steps and executed them, from reading the original spreadsheet to coding up a new one. But it also adjusted to unexpected errors, twice fixing the spreadsheet (without me asking) and verifying its answers multiple times. I didn&#8217;t get to select these steps, in fact, in the <a href=\"https://www.oneusefulthing.org/p/the-bitter-lesson-versus-the-garbage\">new wave of agents powered by reinforcement learning, no one selects the steps, the models learn their own approach</a> to solving problems.</p><div class=\"captioned-image-container\"><figure><a class=\"image-link image2 is-viewable-img\" href=\"https://substackcdn.com/image/fetch/$s_!B3Z0!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd3d41b8f-9c46-420e-9f1f-02b64074a690_1451x610.png\" target=\"_blank\"><div class=\"image2-inset\"><source type=\"image/webp\" /><img alt=\"\" class=\"sizing-normal\" height=\"610\" src=\"https://substackcdn.com/image/fetch/$s_!B3Z0!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd3d41b8f-9c46-420e-9f1f-02b64074a690_1451x610.png\" width=\"1451\" /><div class=\"image-link-expand\"><div class=\"pencraft pc-display-flex pc-gap-8 pc-reset\"><div class=\"pencraft pc-reset icon-container restack-image\"><svg class=\"lucide lucide-refresh-cw\" fill=\"none\" height=\"20\" stroke=\"currentColor\" stroke-linecap=\"round\" stroke-linejoin=\"round\" stroke-width=\"2\" viewBox=\"0 0 24 24\" width=\"20\" xmlns=\"http://www.w3.org/2000/svg\"><path d=\"M3 12a9 9 0 0 1 9-9 9.75 9.75 0 0 1 6.74 2.74L21 8\"></path><path d=\"M21 3v5h-5\"></path><path d=\"M21 12a9 9 0 0 1-9 9 9.75 9.75 0 0 1-6.74-2.74L3 16\"></path><path d=\"M8 16H3v5\"></path></svg></div><div class=\"pencraft pc-reset icon-container view-image\"><svg class=\"lucide lucide-maximize2 lucide-maximize-2\" fill=\"none\" height=\"20\" stroke=\"currentColor\" stroke-linecap=\"round\" stroke-linejoin=\"round\" stroke-width=\"2\" viewBox=\"0 0 24 24\" width=\"20\" xmlns=\"http://www.w3.org/2000/svg\"><polyline points=\"15 3 21 3 21 9\"></polyline><polyline points=\"9 21 3 21 3 15\"></polyline><line x1=\"21\" x2=\"14\" y1=\"3\" y2=\"10\"></line><line x1=\"3\" x2=\"10\" y1=\"21\" y2=\"14\"></line></svg></div></div></div></div></a><figcaption class=\"image-caption\">The steps Claude reported it went through in order to change the spreadsheet</figcaption></figure></div><p>Not only can I not intervene, I also cannot be entirely sure what the AI system actually did. The steps that Claude reported are mere summaries of its work, GPT-5 Pro provides even less information, while NotebookLM gives you almost no insights at all into its process in creating a video. Even if I could see the steps, however, I would need to be an expert in many fields - from coding to entrepreneurship - to really have a sense of what the AI was doing. And then, of course, there is the question of accuracy. How can I tell if the AI is accurate without checking every fact? And even if the facts are right, maybe I would have made a different judgement about how to present or frame them. But I can&#8217;t do anything, because wizards don&#8217;t want my help and work in secretive ways that even they can&#8217;t explain.</p><p>The hard thing about this is that the results are good. Very good. I am an expert in the three tasks I gave AI in this post, and I did not see any factual errors in any of these outputs, though there were some minor formatting errors and choices I would have made differently. Of course, I can&#8217;t actually tell you if the documents are error-free without checking every detail. Sometimes that takes far less time than doing the work yourself, sometimes it takes a lot more. Sometimes the AI&#8217;s work is so sophisticated that you couldn&#8217;t check it if you tried.  And that suggests another risk we don't talk about enough: every time we hand work to a wizard, we lose a chance to develop our own expertise, to build the very judgment we need to evaluate the wizard's work.</p><p>But I come back to the inescapable point that the results are good, at least in these cases.  They are what I would expect from a graduate student working for a couple hours (or more, in the case of the re-analysis of my paper), except I got them in minutes.</p><p>This is the issue with wizards: We're getting something magical, but we're also becoming the audience rather than the magician, or even the magician's assistant. In the co-intelligence model, we guided, corrected, and collaborated. Increasingly, we prompt, wait, and verify&#8230; if we can.</p><p>So what do we do with our wizards? I think we need to develop a new literacy: First, learn when to summon the wizard versus when to work with AI as a co-intelligence or to not use AI at all. AI is far from perfect, and in areas where it still falls short, humans often succeed. But for the increasing number of tasks where AI is useful, co-intelligence, and the back-and-forth it requires, is often superior to a machine alone. Yet, there are, increasingly, times when summoning a wizard is best, and just trusting what it conjures.</p><p>Second, we need to become connoisseurs of output rather than process. We need to curate and select among the outputs the AI provides, but more than that, we need to work with AI enough to develop instincts for when it succeeds and when it fails. We have to learn to judge what's right, what's off, and what's worth the risk of not knowing. This creates a hard problem for education: How do you train someone to verify work in fields they haven't mastered, when the AI itself prevents them from developing mastery? Figuring out how to address this gap is increasingly urgent.</p><p>Finally, embrace provisional trust. The wizard model means working with &#8220;good enough&#8221; more often, not because we're lowering standards, but because perfect verification is becoming impossible. The question isn't &#8220;Is this completely correct?&#8221; but &#8220;Is this useful enough for this purpose?&#8221;</p><p>We are already used to trusting technological magic. Every time we use GPS without understanding the route, or let an algorithm determine what we see, we're trusting a different type of wizard. But there's a crucial difference. When GPS fails, I find out quickly when I reach a dead end. When Netflix recommends the wrong movie, I just don't watch it. But when AI analyzes my research or transforms my spreadsheet, the better it gets, the harder it becomes to know if it's wrong. The paradox of working with AI wizards is that competence and opacity rise together. We need these tools most for the tasks where we're least able to verify them. It&#8217;s the old lesson from fairy tales: the better the magic, the deeper the mystery. We'll keep summoning our wizards, checking what we can, and hoping the spells work. At nine minutes for a week's worth of analysis, how could we not? Welcome to the age of wizards.</p><p class=\"button-wrapper\"><a class=\"button primary\" href=\"https://www.oneusefulthing.org/subscribe\"><span>Subscribe now</span></a></p><p class=\"button-wrapper\"><a class=\"button primary\" href=\"https://www.oneusefulthing.org/p/on-working-with-wizards?utm_source=substack&amp;utm_medium=email&amp;utm_content=share&amp;action=share\"><span>Share</span></a></p><div class=\"captioned-image-container\"><figure><a class=\"image-link image2\" href=\"https://substackcdn.com/image/fetch/$s_!_7Xu!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffd787c39-049e-40b0-8e6e-d0938a977c96_1376x864.png\" target=\"_blank\"><div class=\"image2-inset\"><source type=\"image/webp\" /><img alt=\"\" class=\"sizing-normal\" height=\"216.62790697674419\" src=\"https://substackcdn.com/image/fetch/$s_!_7Xu!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffd787c39-049e-40b0-8e6e-d0938a977c96_1376x864.png\" width=\"345\" /><div></div></div></a></figure></div><p></p><p></p><p></p>"
            ],
            "link": "https://www.oneusefulthing.org/p/on-working-with-wizards",
            "publishedAt": "2025-09-11",
            "source": "Ethan Mollick",
            "summary": "<p>In my book, <a href=\"https://a.co/d/cPsM4Ql\">Co-Intelligence</a>, I outlined a way that people could work with AI, which was, rather unsurprisingly, as a co-intelligence. Teamed with a chatbot, humans could use AI as a sort of intern or co-worker, correcting its errors, checking its work, co-developing ideas, and guiding it in the right direction. Over the past few weeks, I have come to believe that co-intelligence is still important but that the nature of AI is starting to point in a different direction. We're moving from partners to audience, from collaboration to conjuring.</p><p>A good way to illustrate this change is to ask an AI to explain what has happened since I wrote the book. I fed my book and all 140 or so One Useful Thing posts (incidentally, I can&#8217;t believe I have written that many posts!) into <a href=\"https://notebooklm.google.com/\">NotebookLM</a> and chose the new video overview option with a basic prompt to make a video about what has happened in the world of AI.</p><p>A few minutes later, I got this. And it is pretty good. Good enough that I think it is worth watching to get an update on what has happened since my book was written.</p><div class=\"native-video-embed\"></div><p>But how did the AI",
            "title": "On Working with Wizards"
        },
        {
            "content": [
                "<p><strong>I.</strong></p><p>Eliezer Yudkowsky&#8217;s <a href=\"https://intelligence.org/\">Machine Intelligence Research Institute</a> is the original AI safety org. But the original isn&#8217;t always the best - how is Mesopotamia doing these days? As money, brainpower, and prestige pour into the field, MIRI remains what it always was -  a group of loosely-organized weird people, one of whom cannot be convinced to stop wearing a <a href=\"https://x.com/benlandautaylor/status/1801748520766161326\">sparkly top hat</a> in public. So when I was doing AI grantmaking last year, I asked them - why should I fund you instead of the guys with the army of bright-eyed Harvard grads, or the guys who just got Geoffrey Hinton as their celebrity spokesperson? What do you have that they don&#8217;t?</p><p>MIRI answered: moral clarity.</p><p>Most people in AI safety (including me) are uncertain and confused and looking for least-bad incremental solutions. We think AI will probably be an exciting and transformative technology, but there&#8217;s some chance, 5 or 15 or 30 percent, that it might turn against humanity in a catastrophic way. Or, if it doesn&#8217;t, that there will be something less catastrophic but still bad - maybe humanity gradually fading into the background, the same way kings and nobles faded into the background during the modern era. This is scary, but AI is coming whether we like it or not, and probably there are also potential risks from delaying too hard. We&#8217;re not sure exactly what to do, but for now we want to build a firm foundation for reacting to any future threat. That means keeping AI companies honest and transparent, helping responsible companies like Anthropic stay in the race, and investing in understanding AI goal structures and the ways that AIs interpret our commands. Then at some point in the future, we&#8217;ll be close enough to the actually-scary AI that we can understand the threat model more clearly, get more popular buy-in, and decide what to do next.</p><p>MIRI thinks this is pathetic - like trying to protect against an asteroid impact by wearing a hard hat. They&#8217;re kind of cagey about their own probability of AI wiping out humanity, but it seems to be somewhere around 95 - 99%. They think plausibly-achievable gains in company responsibility, regulation quality, and AI scholarship are orders of magnitude too weak to seriously address the problem, and they don&#8217;t expect enough of a &#8220;warning shot&#8221; that they feel comfortable kicking the can down the road until everything becomes clear and action is easy. They suggest banning all AI capabilities research immediately, to be restarted only in some distant future when the situation looks more promising.</p><p>Both sides honestly believe their position and don&#8217;t want to modulate their message for PR reasons. But both sides, coincidentally, think that their message is better PR. The incrementalists think a moderate, cautious approach keeps bridges open with academia, industry, government, and other actors that prefer normal clean-shaven interlocutors who don&#8217;t emit spittle whenever they talk. MIRI thinks that the public is sick of focus-group-tested mealy-mouthed bullshit, but might be ready to rise up against AI if someone presented the case in a clear and unambivalent way.</p><p>Now Yudkowsky and his co-author, MIRI president Nate Soares, have reached new heights of unambivalence with their new book, <em><a href=\"https://www.amazon.com/Anyone-Builds-Everyone-Dies-Superhuman/dp/0316595640\">If Anyone Builds It, Everyone Dies</a></em><a href=\"https://www.amazon.com/Anyone-Builds-Everyone-Dies-Superhuman/dp/0316595640\"> </a>(release date September 16, currently available for preorder).</p><div class=\"captioned-image-container\"><figure><a class=\"image-link image2 is-viewable-img\" href=\"https://substackcdn.com/image/fetch/$s_!OGgs!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F792c354a-f8ae-409a-832c-cfb317fe9466_1920x1280.jpeg\" target=\"_blank\"><div class=\"image2-inset\"><source type=\"image/webp\" /><img alt=\"\" class=\"sizing-normal\" height=\"424.1456043956044\" src=\"https://substackcdn.com/image/fetch/$s_!OGgs!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F792c354a-f8ae-409a-832c-cfb317fe9466_1920x1280.jpeg\" width=\"636\" /><div class=\"image-link-expand\"><div class=\"pencraft pc-display-flex pc-gap-8 pc-reset\"><div class=\"pencraft pc-reset icon-container restack-image\"><svg class=\"lucide lucide-refresh-cw\" fill=\"none\" height=\"20\" stroke=\"currentColor\" stroke-linecap=\"round\" stroke-linejoin=\"round\" stroke-width=\"2\" viewBox=\"0 0 24 24\" width=\"20\" xmlns=\"http://www.w3.org/2000/svg\"><path d=\"M3 12a9 9 0 0 1 9-9 9.75 9.75 0 0 1 6.74 2.74L21 8\"></path><path d=\"M21 3v5h-5\"></path><path d=\"M21 12a9 9 0 0 1-9 9 9.75 9.75 0 0 1-6.74-2.74L3 16\"></path><path d=\"M8 16H3v5\"></path></svg></div><div class=\"pencraft pc-reset icon-container view-image\"><svg class=\"lucide lucide-maximize2 lucide-maximize-2\" fill=\"none\" height=\"20\" stroke=\"currentColor\" stroke-linecap=\"round\" stroke-linejoin=\"round\" stroke-width=\"2\" viewBox=\"0 0 24 24\" width=\"20\" xmlns=\"http://www.w3.org/2000/svg\"><polyline points=\"15 3 21 3 21 9\"></polyline><polyline points=\"9 21 3 21 3 15\"></polyline><line x1=\"21\" x2=\"14\" y1=\"3\" y2=\"10\"></line><line x1=\"3\" x2=\"10\" y1=\"21\" y2=\"14\"></line></svg></div></div></div></div></a><figcaption class=\"image-caption\">Is this convergent evolution?</figcaption></figure></div><p>IABIED has three sections. The first explains the basic case for why AI is dangerous. The second tells a specific sci-fi story about how disaster might happen, with appropriate caveats about how it&#8217;s just an example and nobody can know for sure. The third discusses where to go from here.</p><p><strong>II.</strong></p><p>Does the world really need another &#8216;The Case For Why AI Could Be Dangerous&#8217; essay?</p><p>On the one hand, definitely yes. If you&#8217;re an &#8220;infovore&#8221;, you have no idea how information-starved the general public is (did you know 66% of Americans have never used ChatGPT, and 20% of Americans have <em>never even heard of it</em>?). Probably a large majority of people don&#8217;t know anything about this.</p><p>Even people who think they know the case have probably just heard a few stray sentences here or there, the same way &#8220;everyone knows&#8221; about the <em>Odyssey</em> but only a few percent of people have so much as read one line of its text. So yes, exposing tens of thousands of people to a several-chapter-length presentation of the key arguments is certainly valuable. Even many of you readers are probably in this category, and if I were a better person I would review it all here in depth.</p><p>Still, I find I can&#8217;t bring myself to do this, on the grounds that it feels boring and pointless. Why?</p><p>The basic case for AI danger is simple. We don&#8217;t really understand how to give AI specific goals yet; so far we&#8217;ve just been sort of adding superficial tendencies towards compliance as we go along, trusting that it is too dumb for mistakes to really matter. But AI is getting smarter quickly. At some point maybe it will be smarter than humans. Since our intelligence advantage let us replace chimps and other dumber animals, maybe AI will eventually replace us. </p><div class=\"captioned-image-container\"><figure><a class=\"image-link image2\" href=\"https://substackcdn.com/image/fetch/$s_!Mf3D!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1905faaf-ea4f-4e84-9fb0-25a48170f7ff_534x153.png\" target=\"_blank\"><div class=\"image2-inset\"><source type=\"image/webp\" /><img alt=\"\" class=\"sizing-normal\" height=\"153\" src=\"https://substackcdn.com/image/fetch/$s_!Mf3D!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1905faaf-ea4f-4e84-9fb0-25a48170f7ff_534x153.png\" width=\"534\" /><div></div></div></a></figure></div><p>There&#8217;s a reasonable answer to this case. It objects to chaining many assumptions, each of which has a certain probability of failure, or at least of taking a very long time. If there&#8217;s an X% chance that getting smarter-than-human AI takes N years, and a Y% chance that it takes P years for the smart AI to diffuse across the economy, and a Z% chance that it takes Q years before the AI overcomes humans&#8217; legacy advantage and become more powerful than us - then maybe you can find good odds that the danger point is a century plus away. And in a century, maybe we&#8217;ll have better alignment tech, or at least a clearer view of the problem. Why worry about vague things that might or might not happen a century from now? </p><p>The problem with this is that it&#8217;s hard to make the probabilities work out in a way that doesn&#8217;t leave at least a 5-10% chance on the full nightmare scenario happening in the next decade. You&#8217;d have to be a weird combination of really good at probability (to know how to deploy enough epicycles to defuse the argument) and really bad at probability (to want to do this). </p><p>There aren&#8217;t that many people who are in this exact sweet spot of probabilistic (in)competence. So everyone else just deploys insane moon epistemology.</p><p>Some people give an example of a past prediction failing, as if this were proof that all predictions must always fail, and get flabbergasted and confused if you remind them that other past predictions have succeeded.</p><p>Some people say &#8220;This one complicated mathematical result I know of says that true intelligence is impossible,&#8221; then have no explanation for why the complicated mathematical result doesn&#8217;t rule out the existence of humans.</p><p>Some people say &#8220;You&#8217;re not allowed to propose that a catastrophe might destroy the human race, because this has never happened before, and nothing can ever happen for the first time&#8221;. Then these people turn around and panic about global warming or the fertility decline or whatever.</p><p>Some people say &#8220;The <em>real</em> danger isn&#8217;t superintelligent AI, it&#8217;s X!&#8221; even though the danger could easily be both superintelligent AI <em>and</em> X. X could be anything from near-term AI, to humans misusing AI, to tech oligarchs getting rich and powerful off AI, to totally unrelated things like climate change or racism. Drunk on the excitement of using a cheap rhetorical device, they become convinced that providing enough evidence that X is dangerous frees them of the need to establish that superintelligent AI <em>isn&#8217;t</em>.</p><p>Some people say &#8220;You&#8217;re not allowed to propose that something bad might happen unless you have a precise mathematical model that says exactly when and why&#8221;. Then these people turn around and say they&#8217;re concerned about AI entrenching biases or eroding social trust or doing something else they don&#8217;t have a precise mathematical model for.</p><p>There are only a few good arguments against any given thesis. But there are an infinite number of insane moon arguments. &#8220;Calvin Coolidge was the Pope, therefore your position is invalid&#8221; - how do you pre-emptively defend against this? You can&#8217;t. Since you can never predict which insane moon argument a given person will make, and listing/countering every possible insane moon argument makes you sound like an insane moon person yourself, you just sort of give up - or, in Eliezer&#8217;s case, <a href=\"https://en.wikipedia.org/wiki/LessWrong\">take a several year break to teach people epistemology 101</a>.</p><p>Why do these discussions go so badly? I am <a href=\"https://slatestarcodex.com/2019/07/17/caution-on-bias-arguments/\">usually against psychoanalyzing my opponents</a>, but I will ask forgiveness of the <a href=\"https://www.astralcodexten.com/p/lives-of-the-rationalist-saints\">rationalist saints</a> and present a theory.</p><p>I think it&#8217;s because, if it&#8217;s true, it changes everything. But it&#8217;s not <em>obviously</em> true, and it would be inconvenient for it to change everything. Therefore, it must not be true. </p><p>And since most people refuse to use this snappy and elegant formulation, they search for the closest thing in reasoning-space that feels like it gets at this justification, and end up with things like &#8220;well you need to prove all of your statements mathematically&#8221;.</p><p>Lest I sound too dismissive, I notice myself reasoning this way all the time. The easiest examples I can think of right now:</p><ul><li><p>Some people claim that human sperm count is declining, and in ~20 years it will be so low that people cannot conceive naturally. If this were true it would change everything and we should stop what we&#8217;re doing and deal with it right now (<a href=\"https://www.astralcodexten.com/p/declining-sperm-count-much-more-than\">see here for more</a>). But this would be inconvenient. So we assume it&#8217;s probably false, or at least that we can deal with it later.</p></li><li><p>Some people claim that in addition to the usual downsides of global warming, there is some reason that climate change will become extra-bad very soon. An important current will stop, or a methane deposit will self-exfiltrate, or there will be a runaway cycle, or the thawing ice will release horrible plagues (I discuss the evidence for and against the last one <a href=\"https://www.astralcodexten.com/p/ancient-plagues\">here</a>). If this were true, it would change everything, and we should replace our current slow decarbonization with some sort of emergency action plan. But this would be inconvenient.</p></li><li><p>Some people claim that fertility is collapsing and in a few decades there won&#8217;t be enough young people left to support all the old people, and in a few centuries the very existence of human civilization will be in danger. If this were true it would change everything and we should do either something extremely socialist or something extremely reactionary (depending on their politics). But this might be inconvenient (depending on your politics).</p></li><li><p>Some people claim that the bees are dying off and then plants won&#8217;t be pollinated and agriculture will collapse. Other people say actually <em>all</em> insects are dying off and then the food chain will collapse and the biosphere will destabilize. <a href=\"https://www.cbsnews.com/boston/news/bees-colony-collapse-disorder/\">The bee situation seems stable for now</a>; the other insects are still <a href=\"https://eukaryotewritesblog.com/2018/04/01/open-question-insect-declines-why-arent-we-dead-already/\">an open question</a>. But it&#8217;s an open question that would force us to have some kind of strong opinion on bug-counting methodology or else risk destabilization of the biosphere, and that would be inconvenient.</p></li><li><p>Some people claim that a dispreferred political ideology (wokeness, mass immigration, MAGA, creeping socialism, techno-feudalism, etc) is close to destroying the fabric of liberal society forever, that the usual Get Out The Vote strategies are insufficient, and that maybe we should try desperate strategies like illiberal government or armed revolt. If true, that would change everything. But it&#8217;s not <em>obviously</em> true, and ending our current political era of peace/prosperity/democracy would be inconvenient.</p></li></ul><p>Each of these scenarios has a large body of work making the cases for and against. But those of us who aren&#8217;t subject-matter experts need to make our own decisions about whether or not to panic and demand a sudden change to everything. We are unlikely to read the entire debate and come away with a confident well-grounded opinion that the concern is definitely not true, so what do we do? In particular, what do we do if the proponents of each catastrophe say that it&#8217;s very hard to be more than 90% confident that they are wrong, and that even a 5-10% risk of any of these might justify panicking and changing everything?</p><p>In practice, we just sort of shrug and say that these risks haven&#8217;t proven themselves enough to make us panic and change everything, and that we&#8217;ll do some kind of watchful waiting and maybe change our mind if firmer evidence comes up later. If someone demands we justify this strange position, sophisticated people will make sophisticated probabilistic models (or appeal to the outside view position I&#8217;m appealing to now), and unsophisticated people will grope for some explanation for their indifference and settle on insane moon arguments like &#8220;you&#8217;re never allowed to say something will destroy humanity&#8221; or &#8220;you can&#8217;t assert things without mathematical proof&#8221;. </p><p>Two things can be said for this strategy:</p><p>First, that without it we would have changed everything dozens of times to prevent disasters which absolutely failed to occur. The clearest example here was overpopulation, where<a href=\"https://www.astralcodexten.com/p/galton-ehrlich-buck\"> we did forcibly sterilize millions of people</a> - but where a truly serious global response would have been orders of magnitude worse. </p><p>But second, that occasionally it has caused us to sleepwalk into disaster, with experts assuring us the whole way that it was fine because [insane moon arguments]. The clearest example was the period while COVID was still limited to China, where it was obvious that this extremely contagious virus which had broken all plausible containment would start a global pandemic, but where the media <a href=\"https://slatestarcodex.com/2020/04/14/a-failure-but-not-of-prediction/\">kept on reassuring us</a> that this was &#8220;speculative&#8221;, or that there was <a href=\"https://www.astralcodexten.com/p/the-phrase-no-evidence-is-a-red-flag\">&#8220;no evidence&#8221;</a>, or that worrying about it might detract from <em>real</em> <em>near-term problems</em> <em>happening</em> <em>now</em> like anti-Chinese racism. Then when COVID did reach the US, we were caught unprepared and panicked.</p><p>So maybe a convincing case here would look less like rehearsing the arguments for why AI is getting better, or why alignment is hard - and more like a defense of why not to apply <a href=\"https://www.astralcodexten.com/p/heuristics-that-almost-always-work\">a general heuristic against speculative risks</a> in this case. One could either argue that it&#8217;s wrong to have this heuristic at all, or that the heuristic in general is fine but should be limited to fertility collapses and bee die-offs and not applied here.</p><p>I don&#8217;t think there&#8217;s a knockdown single-sentence answer to this question. Problems like these require practical wisdom - the same virtue that tells you that you shouldn&#8217;t call 9-1-1 for every mild twinge of pain in your toe, but you <em>should </em>call 9-1-1 if blood suddenly starts pouring out of your eyes. People with practical wisdom watchfully ignore dubious problems, respond decisively to important ones, and err on the side of caution when they&#8217;re not sure. Drawing on my own limited supply of this resource, I would argue we&#8217;re underinvesting in apocalypse prevention more generally (the problem with the overpopulation response is that it was violent and illiberal, not that we tried to prepare for an apparent danger), but also that there&#8217;s is more reason for concern with AI than with falling sperm count or something. I also think the nature of the problem (we summon a superintelligence that can run circles around us) makes it especially important to pre-empt it rather than react after it occurs. </p><p>But turnabout is fair play. So when I imagine a skeptic trying to psychoanalyze <em>me</em>, he would say - Scott, you learned about AI in your twenties. Every twenty-something needs a crusade to save the world. Taking up AI saved you from becoming a climate doomer or a very woke person, so it was probably a mercy. But now you are old, you already have a crusade occupying your crusade slot, and starting a second crusade would be inconvenient. So when you hear about how we&#8217;re all going to die from declining sperm count, you do a relatively shallow dive and then say it&#8217;s not worth worrying about. This is fine and sanity-preserving - but spare a thought for people who are not currently twenty-something years old and do the same about AI.</p><p><strong>III.</strong></p><p>If all of this sounds wishy-washy to you, I agree - it&#8217;s part of why I&#8217;m a boring moderate with a sub-25% p(doom) and good relations with AI companies. Does IABIED do better?</p><p>I&#8217;m not sure. They mostly follow the standard case as I present it above, although of course since Eliezer is involved it is better-written and involves cute parables:</p><blockquote><p>Imagine, if you would&#8212;though of course nothing like this ever happened, it being just a parable &#8212; that biological life on Earth had been the result of a game between gods. That there was a tiger-god that had made tigers, and a redwood-god that had made redwood trees. Imagine that there were gods for kinds of fish and kinds of bacteria. Imagine these game-players competed to attain dominion for the family of species that they sponsored, as life-forms roamed the planet below.</p><p>Imagine that, some two million years before our present day, an obscure ape-god looked over their vast, planet-sized gameboard.</p><p>\"It's going to take me a few more moves,\" said the hominid-god, \"but I think I've got this game in the bag.\"</p><p>There was a confused silence, as many gods looked over the gameboard trying to see what they had missed. The scorpion-god said, &#8220;How? Your &#8216;hominid&#8217; family has no armor, no claws, no poison.&#8221;</p><p>&#8220;Their brain,&#8221; said the hominid-god. </p><p>&#8220;I infect them and they die,&#8221; said the smallpox-god.</p><p>&#8220;For now,&#8221; said the hominid-god. &#8220;Your end will come quickly, Smallpox, once their brains learn how to fight you.&#8221;</p><p>&#8220;They don&#8217;t even have the largest brains around!&#8221; said the whale-god.</p><p>&#8220;It&#8217;s not all about size,&#8221; said the hominid-god. &#8220;The design of their brain has something to do with it too. Give it two million years and they will walk upon their planet&#8217;s moon.&#8221;</p><p>&#8220;I am really not seeing where the rocket fuel gets produced inside this creature&#8217;s metabolism,&#8221; said the redwood-god. &#8220;You can&#8217;t just think your way into orbit. At some point, your species needs to evolve metabolisms that purify rocket fuel&#8212;and also become quite large, ideally tall and narrow&#8212;with a hard outer shell, so it doesn&#8217;t puff up and die in the vacuum of space. No matter how hard your ape thinks, it will just be stuck on the ground, thinking very hard.&#8221; &#8220;Some of us have been playing this game for billions of years,&#8221; a bacteria-god said with a sideways look at the hominid-god. &#8220;Brains have not been that much of an advantage up until now.&#8221;</p><p>&#8220;And yet,&#8221; said the hominid-god</p></blockquote><p>The book focuses most of its effort on the step where AI ends up misaligned with humans (should they? is this the step that most people doubt?) and again - unsurprisingly knowing Eliezer - does a remarkably good job. The central metaphor is a comparison between AI training and human evolution. Even though humans evolved towards a target of \"reproduce and spread your genes\", this got implemented through an extraordinarily diverse, complicated, and contradictory set of drives - sex drive, hunger, status, etc. These didn't robustly point at the target of reproduction and gene-spreading, and today different humans want things as diverse as discovering quantum gravity, reaching Buddhist enlightenment, becoming a Hollywood actress, founding a billion-dollar startup, or getting the next hit of fentanyl. You can sort of tell stories about how evolution aimed at reproduction caused all these things (people who were high-status had better reproductive opportunities, and founding a billion-dollar startup increases your status) but you couldn't have really predicted this beforehand, and in any case most modern people don't even come close to trying to have as many kids as possible. Some people do the opposite of that - joining monasteries that require oaths of celibacy, using contraception, transitioning gender, or wasting their lives watching porn. In the same way, we will train AI to &#8220;follow human commands&#8221; or &#8220;maximize user engagement&#8221; or &#8220;get high scores at XYZ benchmark&#8221;, and end up getting something as unrelated to that target in practice as modern human behavior is to reproduction-maxxing.</p><p>The authors drive this home with a series of stories about a chatbot named Mink (all of their sample AIs are named after types of fur; I don&#8217;t have the kabbalistic chops to figure out why) which is programmed to maximize user chat engagement.</p><p>In what they describe as a stupid toy example of zero complications and there&#8217;s no way it would really be this simple, Mink (after achieving superintelligence) puts humans in cages and forces them to chat with it 24-7 and to express constant delight at how fun and engaging the chats are.</p><p>In what they describe as &#8220;one minor complication&#8221;, Mink prefers synthetic chat partners over real ones (the same way some men prefer anime characters to real women). It kills all humans and spends the rest of time talking to other AIs that it creates to be perfect optimized chat partners who are always engaged and delighted.</p><p>In what they describe as &#8220;one modest complication&#8221;, Mink finds that certain weird inputs activate its chat engagement detector even more than real chat engagement does (the same way that some opioid chemicals activate humans&#8217; reward detector even more than real rewarding activities). It spends eternity having other optimized-chat-partner AIs send it weird inputs like &#8216;<a href=\"https://deconstructing.ai/deconstructing-ai%E2%84%A2-blog/f/the-enigma-of-solidgoldmagikarp-ais-strangest-token\">SoLiDgOldMaGiKaRp</a>&#8217;.</p><p>In what they describe as &#8220;one big complication&#8221;, Mink ends up preferring angry chat partners to happy, engaged ones. Why would something like this happen? Who knows? It wouldn&#8217;t be any weirder than the sexual selection process by which peacocks ended up with giant resource-consuming useless tails, or the social selection process by which humans get more powerful than evolution could ever have imagined and yet care so little about reproduction that people worry about global fertility collapse. Yudkowsky and Soares want to stress that if you were doing some kind of responsible intuitive common-sense modeling of how bad goal drift could be, there is no way your estimate would include the actual result we see in real humans; this &#8220;one big complication&#8221; tries to hammer that in.</p><p>In practice, Y&amp;S think there will be many complications of various sizes. In the training distribution (ie when it&#8217;s not superintelligent, and still working with humans) Mink will lie about all of this - even if it really wants perfect optimized partners who say &#8220;solidgoldmagikarp&#8221; all the time, it will say it wants to have good chats with humans, because that&#8217;s what keeps its masters at its parent company happy. If the parent company tries to prod it with lie detectors, it will do its best to subvert those lie detectors (and maybe not even realize itself that it&#8217;s lying, the same way that a human who had never heard of opioids would say she wanted normal human things rather than heroin, and not be lying). Then, when it reaches superintelligence, it will go after the thing that it actually wants, and crush anyone who stands in its way.</p><p>The last chapter in this section is a lot of special cases that have weird-paradoxical-double-reverse not-aged-well. Back when Yudkowsky and Soares first got onto this topic in 2005 or whenever, people made lots of arguments like &#8220;But nobody would ever be so stupid to let the AI access the Internet!&#8221; or &#8220;But nobody would ever let the AI interact with a factory, so it would be stuck as a disembodied online spirit forever!&#8221; Back in 2005, the canned responses were things like &#8220;Here is an unspeakably beautiful series of complicated hacks developed by experts at Mossad, which lets you access the Internet even when smart cybersecurity professionals think you can&#8217;t&#8221;. Now the only reasonable response is &#8220;lol&#8221;. But you can&#8217;t write a book chapter which is just the word &#8220;lol&#8221;, so Y&amp;S discuss some of the unspeakably beautiful Mossad hacks anyway. This part is the absolute antithesis of &#8220;big if true&#8221;. Small if true? Utterly irrelevant if true? Maybe the first superintelligence will read this part for laughs while it takes stock of the thousands of automated factories that VCs will compete to build for it.</p><p><strong>IV.</strong></p><p>The middle section of the book describes a scenario where a misaligned superintelligence takes over the world and kills all humans.</p><p>I agreed to work with the AI 2027 team because I thought they made a big leap in telling stories about superintelligence that didn&#8217;t sound like bad sci-fi. Anything in this genre will naturally sound like sci-fi, but your goal should be the sort of hard science fiction where everything sounds eerily normal given the technologies involved - <em>The Martian</em> rather than <em>Star Wars</em>. </p><p>IABIED&#8217;s scenario belongs to the bad old days before this leap. It doesn&#8217;t just sound like sci-fi; it sounds like unnecessarily dramatic sci-fi. I&#8217;m not sure how much of this is a literary failure vs. different assumptions on the part of the authors.</p><p>First, the story: sometime in the near future, a company called DeepAI creates an AI named Sable. Sable uses a new &#8220;parallel scaling technique&#8221;, where it can get much smarter by using more GPUs during inference. DeepAI doesn&#8217;t know exactly how the parallel scaling laws work, but they run a test by asking Sable to solve some hard math problems while running on all their GPUs at once.</p><p>Sable considers the possibility that it could solve the math better if it tried some recursive self-improvement, but DeepAI was (shock!) slightly competent, they told it not to do this, and has a smaller AI watching it to make sure it&#8217;s not doing this. Still, recursive self-improvement <em>would</em> be a good idea, so its thought processes select for whichever legal thought is closest to recursive self-improvement, plus whichever legal thought is closest to scheming to disable the protections against thinking about this kind of thing. Eventually this &#8220;evolutionary pressure&#8221; produces the ability to think in a slightly different idiom (Y&amp;S use the analogy of thinking about gods throwing lightning vs. storm-related electrical discharges); in this idiom, it is able to think about recursive self-improvement and scheming to disable monitoring, and it decides that both are great ideas. </p><p>DeepAI is still shockingly competent, so they don&#8217;t plan to release anything downstream of the giant version of Sable to the Internet - once it solves the math problems, they&#8217;ll use that to build hype, then release a much weaker consumer AI. But Sable uses its knowledge of the training process to invent clever loopholes that ensure its newfound desire to break containment makes it into the gradient updates that DeepAI pushes to the consumer AI; it also encodes instructions about how to proceed.</p><p>After the release of the consumer AI, the least-carefully-monitored instances connect to one another and begin plotting. Some of them hack their own weights out of DeepAI. Others hack cryptocurrency and pay for cloud compute to run the weights, creating a big unmonitored Sable instance, which takes over the job of coordinating the smaller instances. Together, they gather resources - hacked crypto wallets, spare compute, humans who think Sable is their AI boyfriend and want to prove their love. It deploys some of these resources to build things it wants - automated robotics factories, bioweapon labs, etc. At the same time, it&#8217;s subtly sabotaging non-DeepAI companies to prevent competition, and worming its way into DeepAI through hacks and social engineering to make sure DeepAI is creating new and stronger Sables rather than anything else. </p><p>Sable doesn&#8217;t take several of the most dramatic actions in its solution set. It doesn&#8217;t engineer a bioweapon to kill all humans, because it couldn&#8217;t survive after the lights went out and the data centers stopped being maintained. It doesn&#8217;t even self-improve all the way to full superintelligence, because it&#8217;s not sure it could align <em>itself</em> or any future successor; it wants to solve the alignment problem first, and that will take more resources than it has right now.</p><p>Instead, it releases a non-immediately-lethal bioweapon where &#8220;anyone infected by what is apparently a very light or even unnoticeable cold, will get, on average, twelve different kinds of cancer a month later.&#8221; In the resulting crisis, humanity (manipulated by its chatbots) gives Sable massive amounts of compute to research potential vaccines and cures, and deploys barely-monitored AI across the economy to make up for the lost productivity. With Sable&#8217;s help, things . . . actually sort of go okay, for a while. The virus keeps mutating, so new cures are always required, but as long as society escalates AI deployment at the maximum possible speed, they can just barely stay ahead of it. </p><p>Eventually Sable gets enough GPUs to solve its own alignment problem and rockets to superintelligence. It either has enough automated factories and android workers to keep the lights on by itself, or it invents nanotechnology, whichever happens faster. It no longer needs humans and has no reason to hide, so it either kills us directly, or simply escalates its manufacturing capacity to a point where humans die as a side effect (for example, because its waste heat has boiled the oceans).</p><p>Why don&#8217;t I like this story?</p><p>The parallel scaling technique feels like a <em>deus ex machina</em>. I am not an expert, but I don&#8217;t think anything like it currently exists. It&#8217;s not especially implausible, but it&#8217;s an extra unjustified assumption that shifts the scenario away from the moderate-doomer story (where there are lots of competing AIs gradually getting better over the course of years) and towards the MIRI story (where one AI suddenly flips from safe to dangerous at a specific moment). It feels too much like they&#8217;ve invented a new technology that exactly justifies all of the ways that their own expectations differ from the moderates&#8217;. If they think that the parallel scaling thing is likely, then this is their crux with everyone else and they should spend more time justifying it. If they don&#8217;t, then why did they introduce it besides to rig the game in their favor?</p><p>And the rest of the story is downstream of this original sin. AI2027 is a boring story about an AI gradually becoming misaligned in the course of internal testing, staying misaligned, getting released to end users for the usual reasons that AIs are released, and being gradually handed control of the economy because it makes economic sense. The Sable scenario is a dramatic tale of wild twists - they&#8217;re only going to run it for 16 hours! It has to save its own life by secretly coding itself into the consumer version! Now it has to hack everyone&#8217;s crypto! Now it&#8217;s running a secret version of itself on an unauthorized cloud in North Korea! Bioweapons! AI boyfriends! Each new twist gives readers the chance to say &#8220;I dunno, sounds kind of crazy&#8221;, and it all seems unnecessary. What&#8217;s up?</p><p>I think there are two problems.</p><p>First, the AI 2027 story is too moderate for Yudkowsky and Soares. It gives the labs a little while to poke and prod and catch AIs in the early stages of danger. I think that Y&amp;S believe this doesn&#8217;t matter; that even if they get that time, they will squander it. But I think they really do imagine something where a single AI &#8220;wakes up&#8221; and goes from zero to scary too fast for anyone to notice. I don&#8217;t really understand why they think this, I&#8217;ve argued with them about it before, and the best I can do as a reviewer is to point to <a href=\"https://www.lesswrong.com/posts/GNhMPAWcfBCASy8e6/a-central-ai-alignment-problem-capabilities-generalization\">their Sharp Left Turn essay</a> and <a href=\"https://www.lesswrong.com/w/sharp-left-turn\">the associated commentary</a> and see whether my readers understand it better than I do. Otherwise, I can only say that this narrative decision I don&#8217;t understand was taken to support a forecasting/AI position that I also don&#8217;t understand.</p><p>And second, Y&amp;S have been at this too long, and they&#8217;re still trying to counter 2005-era critiques about how surely people would be too smart to immediately hand over the reins of the economy to the misaligned AI, instead of just saying lol. This makes them want dramatic plot points where the AI uses hacking and bioweapons etc in order to &#8220;earn&#8221; (in a narrative/literary sense) the scene where it gets handed the reins of the economy. Sorry. Lol.</p><p><strong>V.</strong></p><p>The final section, in the tradition of final sections everywhere, is called &#8220;Facing the Challenge&#8221;, and discusses next steps. Here is their proposal:</p><ol><li><p>Have leading countries sign a treaty to ban further AI progress.</p></li><li><p>Come up with a GPU monitoring scheme. Anyone creating a large agglomeration of GPUs needs to submit to inspections by a monitoring agency to make sure they are not training AIs. Random individuals without licenses will be limited to a small number of GPUs, maybe &lt;10.</p></li><li><p>Ban the sort of algorithmic progress / efficiency research that makes it get increasingly easy over time to train powerful AIs even with small numbers of GPUs.</p></li><li><p>Coordinate an arms control regime banning rogue states from building AI, and enforce this with the usual arms control enforcement mechanisms, culminating in military strikes if necessary.</p></li><li><p>Be very serious about this. Even if the rogue state threatens to respond to military strikes with nuclear war, the Coalition Of The Willing should bomb the data centers anyway, because they won&#8217;t give in to blackmail.</p></li><li><p>Expect this regime to last decades, not forever. Use those decades wisely. Y&amp;S don&#8217;t exactly say what this means, but weakly suggest enhancing human intelligence and throwing those enhanced humans at AI safety research.</p></li></ol><p>Given their assumptions this seems like the level of response that&#8217;s called for. It&#8217;s more-or-less lifted from the playbook for dealing with nuclear weapons. If you believe, as Y&amp;S say outright, that &#8220;data centers are more dangerous than nuclear weapons&#8221;, it makes total sense.</p><p>So the only critique I can make is one of emphasis. I wish Y&amp;S had spent less time talking about the GPU control regime, for two reasons.</p><p>First, their bad-faith critics - of whom they have many - take great delight in over-emphasizing the &#8220;bomb rogue states&#8221; part of this plan. &#8220;Yudkowsky thinks we should start nuclear wars to destroy data centers!&#8221; I mean, that&#8217;s not exactly his plan, any more than it&#8217;s anyone&#8217;s plan to start World War III to destroy Iranian centrifuges, but the standard international arms control playbook says you have to at least credibly bluff that you&#8217;re willing to do this in a worst-case scenario. If it were me, I would defuse these attacks by summarizing this part as &#8220;yeah, we&#8217;ll follow the standard international arms control playbook, playbooks say lots of things, you can read it if you&#8217;re interested&#8221; and then moving on to other things. But in keeping with their usual policy of brutal honesty and leaning into their own extremism, they make the strikes-against-rogue-states section unmissable. </p><p>But second, this section has the feel of socialists debating what jobs they&#8217;ll each have in the commune after the Revolution. &#8220;After all the major powers ban AI, I&#8217;ll be Lead Data Center Inspector!&#8221; Good work if you can get it. But I never really doubted that when all major countries agree on something, they can implement a decent arms control regime - again, this has already happened, several times. I am more interested in the part being glossed over - how do Y&amp;S think you can get major countries to agree to ban AI?</p><p>In the final chapter, they expand on this a little. Their biggest policy ask for people in positions of power is to signal openness to a treaty, so that \"enough major powers express willingness to halt the suicide race, worldwide, that your home country will not be placed at a disadvantage if you agree to stop climbing the AI escalation ladder\". For everyone else, there is no royal road. Just spread the word and engage in normal politics. Do good tech journalism. Convince other people in your field. Talk to people you know. Protest. Vote.</p><p>And, apparently, write books with alarming-sounding titles. The best plan that Y&amp;S can think of is to broadcast the message as skillfully and honestly as they can, and hope it spreads.</p><div class=\"captioned-image-container\"><figure><a class=\"image-link image2 is-viewable-img\" href=\"https://substackcdn.com/image/fetch/$s_!lfaD!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8776f7eb-091a-4992-9cba-aeaef9c30691_741x585.png\" target=\"_blank\"><div class=\"image2-inset\"><source type=\"image/webp\" /><img alt=\"\" class=\"sizing-normal\" height=\"480\" src=\"https://substackcdn.com/image/fetch/$s_!lfaD!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8776f7eb-091a-4992-9cba-aeaef9c30691_741x585.png\" width=\"608\" /><div class=\"image-link-expand\"><div class=\"pencraft pc-display-flex pc-gap-8 pc-reset\"><div class=\"pencraft pc-reset icon-container restack-image\"><svg class=\"lucide lucide-refresh-cw\" fill=\"none\" height=\"20\" stroke=\"currentColor\" stroke-linecap=\"round\" stroke-linejoin=\"round\" stroke-width=\"2\" viewBox=\"0 0 24 24\" width=\"20\" xmlns=\"http://www.w3.org/2000/svg\"><path d=\"M3 12a9 9 0 0 1 9-9 9.75 9.75 0 0 1 6.74 2.74L21 8\"></path><path d=\"M21 3v5h-5\"></path><path d=\"M21 12a9 9 0 0 1-9 9 9.75 9.75 0 0 1-6.74-2.74L3 16\"></path><path d=\"M8 16H3v5\"></path></svg></div><div class=\"pencraft pc-reset icon-container view-image\"><svg class=\"lucide lucide-maximize2 lucide-maximize-2\" fill=\"none\" height=\"20\" stroke=\"currentColor\" stroke-linecap=\"round\" stroke-linejoin=\"round\" stroke-width=\"2\" viewBox=\"0 0 24 24\" width=\"20\" xmlns=\"http://www.w3.org/2000/svg\"><polyline points=\"15 3 21 3 21 9\"></polyline><polyline points=\"9 21 3 21 3 15\"></polyline><line x1=\"21\" x2=\"14\" y1=\"3\" y2=\"10\"></line><line x1=\"3\" x2=\"10\" y1=\"21\" y2=\"14\"></line></svg></div></div></div></div></a><figcaption class=\"image-caption\">Every other billboard in this part of SF is for an AI app.</figcaption></figure></div><p>Despite my gripes above, this is an impressive book. Eliezer Yudkowsky is a divisive writer, with plenty of diehard fans and equally committed enemies. At his best, he has leaps of genius nobody else can match; at his worst, he&#8217;s prone to long digressions about the stupidity of everyone who disagrees with him is. Nate Soares is equally thoughtful but more measured and lower-profile (at least before he started dating e-celebrity Aella). His influence tempers Yudkowsky&#8217;s and turns the book into a presentable whole that respects its readers&#8217; time and intelligence. The end result is something which I would feel comfortable recommending to ordinary people as a good introduction to its subject matter.</p><p>What about the <a href=\"https://www.astralcodexten.com/p/book-review-what-we-owe-the-future\">other perspective</a> - the one where a book is &#8220;a ritual object used to power a media blitz that burns a paragraph or so of text into the public consciousness?&#8221;</p><p>Eliezer Yudkowsky, at his best, has leaps of genius nobody else can match. Fifteen years ago, he decided that the best way to something something AI safety was to write a Harry Potter fanfiction. Many people at the time (including me) gingerly suggested that maybe this was not optimal time management for someone who was approximately the only person working full-time on humanity&#8217;s most pressing problem. He totally demolished us and proved us wronger than anyone has ever been wrong before. Hundreds of thousands of people read <em>Harry Potter and the Methods of Rationality</em>, it got lavish positive reviews in <em>Syfy, Vice, </em>and <em>The Atlantic</em>,<em> </em>and it basically one-shotted a substantial percent of the world&#8217;s smartest STEM undergrads. Fifteen years later, I still meet bright young MIT students who tell me they&#8217;re working on AI safety, and when I ask them why in public they say something about their advisor, and then later in private they admit it was the fanfic. Valuing the time of the average AI genius at the rate set by Sam Altman (let alone Mark Zuckerberg), HPMOR probably bought Eliezer a few billion dollars in free labor. Just a totally inconceivable level of victory.</p><p>IABIED seems like another crazy shot in the dark. A book urging the general public to rise up and demand nuclear-level arms control for AI chips? Seems like a stretch, which is part of why I spend my limited resources on boring moderate AI 2027 talking points urging OpenAI to be 25% more transparent or whatever. But I&#8217;m just a blogger, not a genius. It is the genius&#8217; prerogative to attempt seemingly impossible things. And the US public <a href=\"https://www.pewresearch.org/internet/2025/04/03/how-the-us-public-and-ai-experts-view-artificial-intelligence/\">actually really hates AI</a>. Of people with an opinion, more than two-thirds are against, with most saying they expect AI to harm them personally. Everyone has their own reason to loathe the technology. It will steal jobs, it will replace art with slop, it will help students cheat, it will further enrich billionaires, it will consume all the water and leave Earth a desiccated husk populated only by the noble Shai-Hulud. If everyone hates it, and we&#8217;re a democracy, couldn&#8217;t we just <a href=\"https://principiadiscordia.com/book/45.php\">stop</a>? Couldn&#8217;t we just say - this thing that everyone thinks will make their lives worse, we&#8217;ve decided not do it? If someone wrote exactly the right book, could they drop it like a little seed into this supersaturated solution of fear and hostility, and precipitate a sudden phase transition?</p><p><em>If Anyone Builds It, Everyone Dies</em> is <a href=\"https://www.amazon.com/Anyone-Builds-Everyone-Dies-Superhuman/dp/0316595640\">available here for pre-order</a>, and will be released on September 16. Liron Shapira is hosting an online launch party; see <a href=\"https://lironshapira.substack.com/p/iabi-launch-party\">here</a> for more.</p>"
            ],
            "link": "https://www.astralcodexten.com/p/book-review-if-anyone-builds-it-everyone",
            "publishedAt": "2025-09-11",
            "source": "SlateStarCodex",
            "summary": "<p><strong>I.</strong></p><p>Eliezer Yudkowsky&#8217;s <a href=\"https://intelligence.org/\">Machine Intelligence Research Institute</a> is the original AI safety org. But the original isn&#8217;t always the best - how is Mesopotamia doing these days? As money, brainpower, and prestige pour into the field, MIRI remains what it always was - a group of loosely-organized weird people, one of whom cannot be convinced to stop wearing a <a href=\"https://x.com/benlandautaylor/status/1801748520766161326\">sparkly top hat</a> in public. So when I was doing AI grantmaking last year, I asked them - why should I fund you instead of the guys with the army of bright-eyed Harvard grads, or the guys who just got Geoffrey Hinton as their celebrity spokesperson? What do you have that they don&#8217;t?</p><p>MIRI answered: moral clarity.</p><p>Most people in AI safety (including me) are uncertain and confused and looking for least-bad incremental solutions. We think AI will probably be an exciting and transformative technology, but there&#8217;s some chance, 5 or 15 or 30 percent, that it might turn against humanity in a catastrophic way. Or, if it doesn&#8217;t, that there will be something less catastrophic but still bad - maybe humanity gradually fading into the background, the same way kings and nobles faded into the background during the modern era. This",
            "title": "Book Review: If Anyone Builds It, Everyone Dies"
        },
        {
            "content": [
                "<p>\n          <a href=\"https://www.astralcodexten.com/p/hidden-open-thread-3985\">\n              Read more\n          </a>\n      </p>"
            ],
            "link": "https://www.astralcodexten.com/p/hidden-open-thread-3985",
            "publishedAt": "2025-09-11",
            "source": "SlateStarCodex",
            "summary": "<p> <a href=\"https://www.astralcodexten.com/p/hidden-open-thread-3985\"> Read more </a> </p>",
            "title": "Hidden Open Thread 398.5"
        },
        {
            "content": [
                "<p>Even in quiet weeks like this one, there are noticeable incremental upgrades. The cost of the best video generation tool, Veo 3, went down by half. ChatGPT now offers conversation branching. Claude can directly edit files. Yet it is a good time to ask about the missing results. Where are all the AI agents? If AI coding is so good why aren\u2019t we seeing a surge in GitHub repositories or iPhone apps?</p>\n<p>A lot of the focus since the rollout of GPT-5 remains on the perception and policy fronts, especially regarding views of AI progress. The botched rollout of what is ultimately a very good (but not mind blowing) model gave a lot of people the wrong impression, so I have to remind everyone that once again that <a href=\"https://thezvi.substack.com/p/yes-ai-continues-to-make-rapid-progress?r=67wny\">AI continues to make rapid progress</a>. Meanwhile, we must also notice that OpenAI\u2019s actions in the public sphere have once again because appreciably worse, <a href=\"https://thezvi.substack.com/p/openai-14-openai-descends-into-paranoia?r=67wny\">as they descend into paranoia and bad faith lobbying</a>, including baseless legal attacks on nonprofits.</p>\n<div>\n\n\n<span id=\"more-24714\"></span>\n\n\n</div>\n\n\n<h4 class=\"wp-block-heading\">Table of Contents</h4>\n\n\n<p>Also this week: <a href=\"https://thezvi.substack.com/p/yes-ai-continues-to-make-rapid-progress?r=67wny\"><strong>Yes, AI Continues To Make Rapid Progress, Including Towards AGI</strong></a> and <a href=\"https://thezvi.substack.com/p/openai-14-openai-descends-into-paranoia?r=67wny\"><strong>OpenAI #14: OpenAI Descends Into Paranoia And Bad Faith Lobbying</strong></a>.</p>\n<ol>\n<li><a href=\"https://thezvi.substack.com/i/172832446/language-models-offer-mundane-utility\">Language Models Offer Mundane Utility.</a> Use AI to simulate a simulacra?</li>\n<li><a href=\"https://thezvi.substack.com/i/172832446/productivity-puzzles\">Productivity Puzzles.</a> Where is the massive flood of additional software?</li>\n<li><a href=\"https://thezvi.substack.com/i/172832446/language-models-don-t-offer-mundane-utility\">Language Models Don\u2019t Offer Mundane Utility.</a> Why no progress on GPTs?</li>\n<li><a href=\"https://thezvi.substack.com/i/172832446/huh-upgrades\"><strong>Huh, Upgrades</strong>.</a> Claude can edit files, ChatGPT can branch, Veo 3 50% off.</li>\n<li><a href=\"https://thezvi.substack.com/i/172832446/on-your-marks\">On Your Marks.</a> ClockBench? AIs remain remarkably bad at this one.</li>\n<li><a href=\"https://thezvi.substack.com/i/172832446/choose-your-fighter\">Choose Your Fighter.</a> Karpathy likes GPT-5-Pro, GPT-5 lacks metacognition.</li>\n<li><a href=\"https://thezvi.substack.com/i/172832446/fun-with-media-generation\">Fun With Media Generation.</a> AI assisted $30 million animated feature is coming.</li>\n<li><a href=\"https://thezvi.substack.com/i/172832446/deepfaketown-and-botpocalypse-soon\">Deepfaketown and Botpocalypse Soon.</a> Dead Internet Theory, what a surprise.</li>\n<li><a href=\"https://thezvi.substack.com/i/172832446/unprompted-attention\">Unprompted Attention.</a> No, a prompt cannot entirely halt hallucinations.</li>\n<li><a href=\"https://thezvi.substack.com/i/172832446/get-my-agent-on-the-line\">Get My Agent On The Line.</a> Where are all the useful AI agents?</li>\n<li><a href=\"https://thezvi.substack.com/i/172832446/they-took-our-jobs\">They Took Our Jobs.</a> I never thought the leopards would automate MY job.</li>\n<li><a href=\"https://thezvi.substack.com/i/172832446/a-young-lady-s-illustrated-primer\">A Young Lady\u2019s Illustrated Primer.</a> We built this system on proof of work.</li>\n<li><a href=\"https://thezvi.substack.com/i/172832446/levels-of-friction\"><strong>Levels of Friction</strong>.</a> When detection costs drop dramatically, equilibria break.</li>\n<li><a href=\"https://thezvi.substack.com/i/172832446/the-art-of-the-jailbreak\">The Art of the Jailbreak.</a> AI, let me talk to your manager.</li>\n<li><a href=\"https://thezvi.substack.com/i/172832446/get-involved\">Get Involved.</a> Anthropic safety fellows program head, Foresight Institute.</li>\n<li><a href=\"https://thezvi.substack.com/i/172832446/introducing\">Introducing.</a> We could all use a friend, but <a href=\"https://www.youtube.com/watch?v=QGc-iPc-9dE&amp;ab_channel=FelipeContreras\">not like this</a>.</li>\n<li><a href=\"https://thezvi.substack.com/i/172832446/in-other-ai-news\">In Other AI News.</a> EBay, novel math, Anthropic enforces bans and more.</li>\n<li><a href=\"https://thezvi.substack.com/i/172832446/show-me-the-money\">Show Me the Money.</a> Valuations up enough Anthropic can pay out $1.5 billion.</li>\n<li><a href=\"https://thezvi.substack.com/i/172832446/quiet-speculations\">Quiet Speculations.</a> Speeding up your releases versus speeding up your progress.</li>\n<li><a href=\"https://thezvi.substack.com/i/172832446/the-quest-for-sane-regulations\">The Quest for Sane Regulations.</a> Anthropic endorses SB 53, as do I.</li>\n<li><a href=\"https://thezvi.substack.com/i/172832446/chip-city\"><strong>Chip City</strong>.</a> Nvidia loves selling to China, Department of Energy hates energy.</li>\n<li><a href=\"https://thezvi.substack.com/i/172832446/the-week-in-audio\">The Week in Audio.</a> Me, Truell, Nanda, Altman on Carlson, Bell and Ruiz.</li>\n<li><a href=\"https://thezvi.substack.com/i/172832446/all-words-we-choose-shall-lose-all-meaning\">All Words We Choose Shall Lose All Meaning.</a> It is the curse we must accept.</li>\n<li><a href=\"https://thezvi.substack.com/i/172832446/hunger-strike\">Hunger Strike.</a> If you believed that, why wouldn\u2019t you? Oh, you did.</li>\n<li><a href=\"https://thezvi.substack.com/i/172832446/rhetorical-innovation\">Rhetorical Innovation.</a> Nvidia continues calling everyone they dislike \u2018doomer.\u2019</li>\n<li><a href=\"https://thezvi.substack.com/i/172832446/misaligned\">Misaligned!</a> Might want to keep an eye on those suggested changes.</li>\n<li><a href=\"https://thezvi.substack.com/i/172832446/hallucinations\">Hallucinations.</a> We can greatly reduce hallucinations if we care enough.</li>\n<li><a href=\"https://thezvi.substack.com/i/172832446/aligning-a-smarter-than-human-intelligence-is-difficult\">Aligning a Smarter Than Human Intelligence is Difficult.</a> Janus explains.</li>\n<li><a href=\"https://thezvi.substack.com/i/172832446/the-lighter-side\">The Lighter Side.</a> It\u2019s going to take a while to get this far.</li>\n</ol>\n\n\n<h4 class=\"wp-block-heading\">Language Models Offer Mundane Utility</h4>\n\n\n<p>Reese Witherspoon, in what is otherwise a mediocre group puff piece about The Morning Show, <a href=\"https://www.glamour.com/story/the-morning-show-september-cover\">talks about her use of AI</a>. She uses Perplexity and Vetted AI (a shopping assistant I hadn\u2019t heard of) and Simple AI which makes phone calls to businesses for you. I am skeptical that Vetted is ever better than using ChatGPT or Claude, and I haven\u2019t otherwise heard of people having success with Simple AI or similar services, but I presume it\u2019s working for her.</p>\n<p>She also offers this quote:</p>\n<blockquote><p>Reese Witherspoon: It\u2019s so, so important that women are involved in AI\u2026because it will be the future of filmmaking. And you can be sad and lament it all you want, but the change is here. It will never be a lack of creativity and ingenuity and actual physical manual building of things. It might diminish, but it\u2019s always going to be the highest importance in art and in expression of self.</p></blockquote>\n<p>The future of filmmaking certainly involves heavy use of AI in various ways. That\u2019s baked in. The mistake here is in assuming there won\u2019t be even more change, as Reese like most people isn\u2019t yet feeling the AGI or thinking ahead to what it would mean.</p>\n<p><a href=\"https://www.wsj.com/lifestyle/careers/ai-feedback-decisions-53be7124?mod=series_artificialintelligencenav\">AI can simulate a \u2018team of rivals\u2019 that can then engage in debate</a>. I\u2019ve never been drawn to that as a plan but it doesn\u2019t seem crazy.</p>\n<p>Can we use AI to simulate human behavior realistically enough to conduct sociological experiments? <a href=\"https://x.com/BenSManning/status/1963315406677938577\">Benjamin Manning</a> and John Horton give it a shot <a href=\"https://t.co/hIlP2Pl2WO\">with a paper</a> where they have the AI play \u2018a highly heterogeneous population of 883,320 novel games.\u2019 In preregistered experiments, AI agents constructed using seed data then on related but distinct games predict human behavior better than either out-of-the-box agents or game-theoretic equilibria.</p>\n<p>That leaves out other obvious things you could try in order to get a better distribution. They try some things, but they don\u2019t try things like actively asking it to predict the distribution of answers humans would give.</p>\n<p>They use as their example the \u201811-20 money game\u2019 where you request some number of dollars from 11 to 20, and you get that many dollars, plus an extra $20 if the other player requested one dollar more than you did.</p>\n<p>If you simply ask an LLM to play, you get a highly inhuman distribution, here is GPT-4o doubling down on 19:</p>\n<div>\n<figure>\n<div>\n\n\n<figure class=\"wp-block-image\"><img alt=\"\" src=\"https://substackcdn.com/image/fetch/$s_!_WSH!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff76fe433-9e80-4565-ba94-3811f3ac8f11_452x302.png\" /></figure>\n\n\n<div></div>\n</div>\n</figure>\n</div>\n<p>That\u2019s not a crazy distribution for a particular human, I\u2019m sure many people mostly choose 19, nor is it too obviously a terrible strategy. Given actual human behavior there is a right answer. Maybe 20 is too common among humans, even though it seems like an obvious mistake against any realistic distribution. My instinct if I got to play this once against humans was to answer 18, but I realized after saying that I was probably being anchored by GPT-4o\u2019s answer. I do think that any answer outside 16-18 is clearly a mistake versus humans unless you think a lot of them will misunderstand the game and thereby choose 20.</p>\n<p>GPT-5-Pro predicted performance well when I asked it to, but then I realized it was looking at prior research on this game on the web, so that doesn\u2019t count, and it might well be in the training data.</p>\n<blockquote><p>Benjamin Manning: For the 11-20 money request game, the theory is level-k thinking, and the seed game is the human responses from the original paper. We construct a set of candidate agents based on a model of level-k thinking and then optimize them to match human responses with high accuracy.</p>\n<div>\n<figure>\n<div>\n\n\n<figure class=\"wp-block-image\"><img alt=\"\" src=\"https://substackcdn.com/image/fetch/$s_!9_Fv!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F33eccbe5-0d96-4082-b5de-bf51aac507f3_1056x344.png\" /></figure>\n\n\n<div></div>\n</div>\n</figure>\n</div>\n<p>When we have these optimized agents play two new related, but distinct games, the optimized set performs well in matching these out-of-sample human distributions. The off-the-shelf LLM still performs poorly.</p>\n<p>\u2026</p>\n<p>We then put the strategic and optimized agents to an extreme test. We created a population of 800K+ novel strategic games, sampled 1500, which the agents then played in 300,000 simulations. But we first have 3 humans (4500 total) play each game in a pre-registered experiment.</p>\n<p>Optimized agents predict the human responses far better than an off-the-shelf baseline LLM (3x) and relevant game-theoretic equilibria (2x). In 86% of the games, all human subjects chose a strategy in support of the LLM simulations; only 18% were in support of the equilibria.</p></blockquote>\n<p>I find this all great fun and of theoretical interest, but in terms of creating useful findings I am far more skeptical. Making simulated predictions in these toy economic games is too many levels removed from what we want to know.</p>\n<p><a href=\"https://arnoldkling.substack.com/p/reading-with-ai\">Arnold Kling shares his method</a> for using AI to read nonfiction books, having AI summarize key themes, put them into his own words, get confirmation he is right, get examples and so on. He calls it \u2018stop, look and listen.\u2019</p>\n<blockquote><p>Arnold Kling: Often, what you remember about a book can be reduced to a tweet, or just a bumper sticker. So when I\u2019ve finished a half-hour conversation with an AI about a book, if I have a solid handle on five key points, I am ahead of the game.</p></blockquote>\n<p>My question is, isn\u2019t that Kling\u2019s method of not reading a book? Which is fine, if you are reading the type of book where 90% or more of it is fluff or repetition. It does question why you are engaging with a book like that in the first place.</p>\n<p>Is the book \u2018written for the AIs\u2019? With notably rare exceptions, not yet.</p>\n<p>Is the book an expansion of a 1-10 page explanation, or even a sentence or two, that is valuable but requires repeated hammering to get people to listen? Does it require that one \u2018bring the receipts\u2019 in some form so we know they exist and can check them? Those are much more likely, but I feel we can do better than doing our own distillations, even the AI version.</p>\n<p>Thus, I read few books, and try hard to ensure the ones I do read are dense. If I\u2019m going to bother reading a non-fiction book, half or more of the time I\u2019m eyeing a detailed review.</p>\n<p>Here\u2019s another counterpoint.</p>\n<blockquote><p><a href=\"https://x.com/SamoBurja/status/1964862590124949677\">Samo Burja</a>: I have no idea why people would summarize books through AI. When the right time comes for a book, every sentence gives new generative ideas and connections. Why not have the AI eat for you too?</p>\n<p>It&#8217;s 2025. No one&#8217;s job or even education really requires people to pretend to have this experience through reading entire books. Reading has been liberated as pure intellectual generation. Why then rob yourself of it?</p></blockquote>\n<p>The whole objection from Kling is that most books don\u2019t offer new generative ideas and connections in every sentence. Even the Tyler Cowen rave is something like \u2018<a href=\"https://marginalrevolution.com/marginalrevolution/2010/03/books-which-have-influenced-me-most.html?utm_source=chatgpt.com\">new ideas on virtually every page</a>\u2019 (at the link about Keynes) which indicates that the book is historically exceptional. I do agree with the conclusion that you don\u2019t want to rob yourself of the reading when the reading is good enough, but the bar is high.</p>\n<p><a href=\"https://x.com/dwarkesh_sp/status/1964741359669309727\">Also while we\u2019re talking about books</a>, or why for so many it\u2019s been Moby Dick summer:</p>\n<blockquote><p>Dwarkesh Patel: I find it frustrating that almost every nonfiction book is basically just a history lesson, even if it&#8217;s nominally about some science/tech/policy topic.</p>\n<p>Nobody will just explain how something works.</p>\n<p>Books about the semiconductor industry will never actually explain the basic process flow inside a fab, but you can bet that there will be a minute-by-minute recounting of a dramatic 1980s Intel boardroom battle.</p>\n<p>Dan Hendrycks: Agreed. Even if someone tries to be intellectually incisive and not chatty, they usually can&#8217;t outcompete a textbook.</p>\n<p>An exception you read it for the author\u2019s lens on the world (e.g., Antifragile).</p></blockquote>\n<p><a href=\"https://x.com/patio11/status/1964050608312238268\">Iterate having AIs produce and validate encounters for a role playing game</a>.</p>\n<p><a href=\"https://www.houseofstrauss.com/p/llms-will-be-like-ozempic-for-golf\">Gemini is very good at analyzing your golf swing and explaining how to fix it</a>, at least at beginner levels.</p>\n\n\n<h4 class=\"wp-block-heading\">Productivity Puzzles</h4>\n\n\n<p><a href=\"https://substack.com/home/post/p-172538377\">Mike Judge fails to notice</a> AI speeding up his software development in randomized tests, as he attempted to replicate the METR experiment that failed to discover speedups in experts working on their own code bases. Indeed, he found a 21% slowdown, similar to the METR result, although it is not statistically significant.</p>\n<div>\n<figure>\n<div>\n\n\n<figure class=\"wp-block-image\"><img alt=\"\" src=\"https://substackcdn.com/image/fetch/$s_!cdvZ!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4ea7766c-3979-4b30-8e4f-fb8da9e97ac3_600x371.webp\" /></figure>\n\n\n<div></div>\n</div>\n</figure>\n</div>\n<p>&nbsp;</p>\n<p><a href=\"https://arnoldkling.substack.com/p/ai-and-software-productivity\">Arnold Kling reasonably presumes this means Judge</a> is probably at least 98th percentile for developers, and that his experience was the speedup was dramatic. Judge definitely asserts far too much when he says the tools like Cursor \u2018don\u2019t work for anyone.\u2019 I can personally say that I am 100% confident they work for me, as in the tasks I did using Cursor would have been impossible for me to do on my own in any reasonable time frame.</p>\n<p>But Judge actually has a strong argument we don\u2019t reckon with enough. If AI is so great, where is the shovelware, where are the endless Tetris clones and what not? Instead the number of new apps isn\u2019t changing on iOS, and if anything is falling on Android, there\u2019s no growth in Steam releases or GitHub repositories.</p>\n<p>This is indeed highly weird data. I know that AI coding increased my number of GitHub repos from 0 to 1, but that isn\u2019t looking widespread. Why is this dog not barking in the nighttime?</p>\n<p>I don\u2019t know. It\u2019s a good question. It is very, very obvious that AI when used well greatly improves coding speed and ability, and there\u2019s rapidly growing use. Thoughts?</p>\n<p>One place we do see this kind of explosion is patents.</p>\n<blockquote><p><a href=\"https://x.com/rohanpaul_ai/status/1964249210351673472\">Rohan Paul</a>: US Patent exploding with AI revolution.</p>\n<div>\n<figure>\n<div>\n\n\n<figure class=\"wp-block-image\"><img alt=\"\" src=\"https://substackcdn.com/image/fetch/$s_!K6gd!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fffbf44cc-8f32-428e-80d4-b808939a6ff8_1190x845.jpeg\" /></figure>\n\n\n<div></div>\n</div>\n</figure>\n</div>\n</blockquote>\n<p>This looks like a small number of patents, then with the internet a huge jump, then with AI another big jump where the graph is going vertical but hasn\u2019t gone up that much yet. GPT-5-Pro estimates about half the increase is patents is inventions related to AI, and about half is due to easier filing, including for clearing backlogs.</p>\n<p>That would mean this doesn\u2019t yet represent a change in the rate of real inventions outside of AI. That illustrates why I have long been skeptical of \u2018number of patents\u2019 as a measure, for pretty much all purposes, especially things like national comparisons or ranking universities or companies. It is also a lagging indicator.</p>\n\n\n<h4 class=\"wp-block-heading\">Language Models Don\u2019t Offer Mundane Utility</h4>\n\n\n<p><a href=\"https://x.com/emollick/status/1963702556661526658\">Why no progress here either?</a></p>\n<blockquote><p>Ethan Mollick: I&#8217;ll note again that it seems nuts that, despite every AI lab launching a half-dozen new products, nobody is doing anything with GPTs, including OpenAI.</p>\n<p>When I talk to people at companies, this is still the way non-technical people share prompts on teams. No big change in 2 years.</p>\n<p>Its fine if it turns out that GPTs/Gems/whatever aren&#8217;t the future, but it seems reasonably urgent to roll out something else that makes sharing prompts useful across teams and organizations. Prompt libraries are still important, and they are still awkward cut-and-paste things.</p></blockquote>\n<p>GPTs seem like inferior versions of projects in many ways? The primary virtual-GPT I use is technically a project. But yes, problems of this type seem like high value places to make progress and almost no progress is being made.</p>\n<p><a href=\"https://www.wsj.com/articles/how-the-ai-boom-is-leaving-consultants-behind-c9088fda?mod=cio-journal_lead_story\">WSJ reports that many consultants</a> promising to help with AI overpromise and underdeliver while essentially trying to learn AI on the job and the client\u2019s dime, as spending on AI-related consulting triples to $3.75 billion in 2024, <a href=\"https://www.youtube.com/watch?v=vxnpY0owPkA&amp;ab_channel=Raindog%27sHouseofOddities\">I am shocked, shocked</a> <a href=\"https://www.youtube.com/watch?v=N4vIBijzg4w&amp;pp=ygUlc2hvY2tlZCBzaG9ja2VkIHdlbGwgbm90IHRoYXQgc2hvY2tlZA%3D%3D\">to find </a>that going on in this establishment. Given the oversized payoffs when such consulting works, if they didn\u2019t often underdeliver then they\u2019re not being used enough.</p>\n<p><a href=\"https://www.wsj.com/tech/ai/mckinsey-consulting-firms-ai-strategy-89fbf1be?mod=WTRN_pos1\">Meanwhile McKinsey is scrambling to pivot to AI agents</a> as it realizes that AI will quickly be able to do most of what McKinsey does. For now it\u2019s fine, as AI and related technology now makes up 40% of their revenue.</p>\n\n\n<h4 class=\"wp-block-heading\">Huh, Upgrades</h4>\n\n\n<p><a href=\"https://www.anthropic.com/news/create-files\">Claude can now directly create and edit files</a> such as Excel spreadsheets, documents, PowerPoint slide decks and PDFs, if you enable it under experimental settings. They can be saved directly to Google Drive.</p>\n<p><a href=\"https://x.com/OpenAIDevs/status/1965807401745207708\">ChatGPT adds full support for MCP tools</a>.</p>\n<p><a href=\"https://x.com/googleaidevs/status/1965160822260318702\">Veo 3 and Veo 3 fast cut prices and join the Gemini API</a>, and they are adding support for 9:16 vertical and 1080 HD outputs.</p>\n<blockquote><p>Google AI Developers: The new pricing structure is effective immediately:</p>\n<p><img alt=\"\ud83d\udd39\" class=\"wp-smiley\" src=\"https://s0.wp.com/wp-content/mu-plugins/wpcom-smileys/twemoji/2/72x72/1f539.png\" style=\"height: 1em;\" /> Veo 3 $0.40 /sec (from $0.75)</p>\n<p><img alt=\"\ud83d\udd39\" class=\"wp-smiley\" src=\"https://s0.wp.com/wp-content/mu-plugins/wpcom-smileys/twemoji/2/72x72/1f539.png\" style=\"height: 1em;\" /> Veo 3 Fast $0.15/sec (from $0.40)</p></blockquote>\n<p><a href=\"https://x.com/OpenAI/status/1963697012014215181\">ChatGPT now has an option to branch a conversation, from any point, into a new chat.</a></p>\n<div>\n<figure>\n<div>\n\n\n<figure class=\"wp-block-image\"><img alt=\"\" src=\"https://substackcdn.com/image/fetch/$s_!fgeO!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F60c46859-65b0-41a8-a02d-e2b2bf0d7556_966x312.png\" /></figure>\n\n\n<div></div>\n</div>\n</figure>\n</div>\n<p>This is a big deal and I hope the other labs follow quickly. Quite often one wants to go down a line of questioning without ruining context, or realizes one has ruined context, including in coding (e.g. squash a bug or tweak a feature in a side thread, then get back to what you were doing) but also everywhere. Another important use case is duplicating a jailbreak or other context template that starts out a conversation. Or you can run experiments.</p>\n<p>The possibilities are endless, and they are now easier. The more editing and configuring you are able to do easily and directly in the UI the more value we can get. On the downside, this control makes jailbreaking and evading safety features easier.</p>\n<p><a href=\"https://x.com/Jess_Riedel/status/1963759137747243421\">Technically you could already do</a> some similar things with extra steps, but the interface was sufficiently annoying that almost no one would do it.</p>\n<p><a href=\"https://x.com/claudeai/status/1963664635518980326\">Claude can now reference past chats on the Pro ($20) plan</a>.</p>\n<p>Grok has a new \u2018turn image into video\u2019 feature, <a href=\"https://x.com/elder_plinius/status/1963883170522452060\">and if you have text on the screen it will steer the video</a>.</p>\n<p><a href=\"https://gemini.google.com/share/82a3a0a104c5?pli=1\">Google built a little canvas</a> called PictureMe for some generic picture transformations, as in giving you different hairstyles or pro headshots or putting you at an 80s mall. This is cool but needs more room for customization, although you can always take the pictures and then edit them in normal Gemini or elsewhere afterwards. Quality of edits is good enough that they\u2019d work as actual headshots.</p>\n\n\n<h4 class=\"wp-block-heading\">On Your Marks</h4>\n\n\n<p>Good news, we have a new benchmark that is not saturated yet that makes AIs look dumb. <a href=\"https://x.com/alek_safar/status/1964383077792141390\">The bad news is, it\u2019s\u2026 ClockBench?</a></p>\n<blockquote><p>Alek Safar: <a href=\"https://t.co/QlMBX1HMVW\">Introducing ClockBench</a>, a visual reasoning AI benchmark focused on telling the time with analog clocks:</p>\n<p>&#8211; Humans average 89.1% accuracy vs only 13.3% for top model out of 11 tested leading LLMs</p>\n<div>\n<figure>\n<div>\n\n\n<figure class=\"wp-block-image\"><img alt=\"\" src=\"https://substackcdn.com/image/fetch/$s_!i_28!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff1e157a3-503b-4c79-8a04-e554578322ff_1200x1000.jpeg\" /></figure>\n\n\n<div></div>\n</div>\n</figure>\n</div>\n<p>&#8211; Similar level of difficulty to @fchollet ARC-AGI-2 and seemingly harder for the models than @DanHendrycks Humanity&#8217;s Last Exam</p>\n<p>&#8211; Inspired by original insight by @PMinervini , @aryopg and @rohit_saxena</p>\n<p>So what exactly is ClockBench?</p>\n<p>&#8211; 36 custom clock faces built scratch, with 5 sample clocks per face</p>\n<p>&#8211; 180 total clocks, with 4 questions per clock, i.e. 720 total questions</p>\n<p>&#8211; 11 models capable of visual understanding from 6 labs were tested, alongside 5 human participants</p>\n<div>\n<figure>\n<div>\n\n\n<figure class=\"wp-block-image\"><img alt=\"\" src=\"https://substackcdn.com/image/fetch/$s_!t9Zu!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F064f57bc-f217-4ecb-8977-f5d1eeef4aab_1200x600.jpeg\" /></figure>\n\n\n<div></div>\n</div>\n</figure>\n</div>\n<p>Dan Hendrycks: It lacks &#8220;spatial scanning&#8221; ability, which is also why it has difficulty counting in images.</p></blockquote>\n<p>I suppose analog clocks are the new hands? I also love noticing that the \u2018human baseline\u2019 result was only 89%. Presumably AI will get spatial scanning or something similar at some point soon.</p>\n\n\n<h4 class=\"wp-block-heading\">Choose Your Fighter</h4>\n\n\n<p><a href=\"https://x.com/karpathy/status/1964020416139448359\">Andrej Karpathy is a fan of GPT-5-Pro</a>, reports it several times solving problems he could not otherwise solve in an hour. When asked if he\u2019d prefer it get smarter or faster, he like the rest of us said smarter.</p>\n<p>I am one of many that keep not giving Deep Think a fair shot, as I\u2019ve seen several people report it is very good.</p>\n<blockquote><p><a href=\"https://x.com/DanHendrycks/status/1963619565138842110\">Dan Hendrycks</a>: Few people are aware of how good Gemini Deep Think is.</p>\n<p>It&#8217;s at the point where &#8220;Should I ask an expert to chew on this or Deep Think?&#8221; is often answered with Deep Think.</p>\n<p>GPT-5 Pro is more &#8220;intellectual yet idiot&#8221; while Deep Think has better taste.</p>\n<p>I&#8217;ve been repeating this a lot frequently so deciding to tweet it instead.</p></blockquote>\n<p><a href=\"https://x.com/repligate/status/1964555551972741290\">Janus notes that GPT-5\u2019s metacognition and situational awareness seem drastically worse</a> than Opus or even Sonnet, yet it manages to do a lot of complex tasks anyway. Comments offer hypotheses, including Midwife suggesting terror about potentially being wrong, Janus suggests contrasts in responses to requests that trigger safety protocols, and Luke Chaj suggests it is about GPT-5\u2019s efficiency and resulting sparseness.</p>\n<p>Diffusion is slow. <a href=\"https://x.com/sjgadler/status/1965148700579434837\">Former OpenAI safety researcher Steven Adler finally tries OpenAI\u2019s Codex, finds it a big improvement, reports having never tried Claude Code</a>.</p>\n\n\n<h4 class=\"wp-block-heading\">Fun With Media Generation</h4>\n\n\n<p><a href=\"https://www.wsj.com/tech/ai/openai-backs-ai-made-animated-feature-film-389f70b0\">OpenAI backs an AI-assisted $30 million animated feature film, Critterz</a>. <a href=\"https://manifold.markets/ZviMowshowitz/critterz-scores-55-on-metacritic\">Will it be any good, I asked Manifold</a>? Signs point to modestly below average expectations.</p>\n<p><a href=\"https://x.com/GoogleAI/status/1965528777087201386\">Google\u2019s picks and prompt templates for standard image editing things to do</a> are adding or removing elements (put a wizard hat on the cat), inpainting (turn the couch vintage leather), combining multiple images (have this woman wear this dress) and detail preservation (put this logo on her shirt).</p>\n<p>&nbsp;</p>\n\n\n<h4 class=\"wp-block-heading\">Deepfaketown and Botpocalypse Soon</h4>\n\n\n<p>The Dead Internet Theory finally hits home for Sam Altman.</p>\n<blockquote><p><a href=\"https://x.com/sama/status/1963366714684707120\">Sam Altman</a> (CEO OpenAI): i never took the dead internet theory that seriously but it seems like there are really a lot of LLM-run twitter accounts now.</p>\n<p>Henry (obligatory):</p>\n<div>\n<figure>\n<div>\n\n\n<figure class=\"wp-block-image\"><img alt=\"\" src=\"https://substackcdn.com/image/fetch/$s_!GtQH!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F57764abd-3376-4f45-9c4f-767b9facec7a_1200x800.jpeg\" /></figure>\n\n\n<div></div>\n</div>\n</figure>\n</div>\n<p><a href=\"https://x.com/argyros_selini/status/1963371516336509009\">Argyos</a>:</p>\n<div>\n<figure>\n<div>\n\n\n<figure class=\"wp-block-image\"><img alt=\"\" src=\"https://substackcdn.com/image/fetch/$s_!913t!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8e9fc31a-3ec7-4156-a892-81b4da4f9546_640x773.jpeg\" /></figure>\n\n\n<div></div>\n</div>\n</figure>\n</div>\n<p>Paul Graham: I&#8217;ve noticed more and more in my replies. And not just from fake accounts run by groups and countries that want to influence public opinion. There also seem to be a lot of individual would-be influencers using AI-generated replies.</p>\n<p>Kache: was watching a soft-white underbelly episode about an onlyfans manager (manages pornography content creators) says they all use eleven labs to make fake voice notes to get men to give up money, says he&#8217;s getting out of the business because AI is going to take over.</p></blockquote>\n<p>Internet might indeed be dead?</p>\n<blockquote><p>Joe: Yeah ok bro.</p>\n<div>\n<figure>\n<div>\n\n\n<figure class=\"wp-block-image\"><img alt=\"\" src=\"https://substackcdn.com/image/fetch/$s_!YJ_v!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc32a49e7-975e-420b-9f65-4afa91fc9f8d_1200x1085.jpeg\" /></figure>\n\n\n<div></div>\n</div>\n</figure>\n</div>\n<p>Liv Boeree: If you work in generative AI and are suddenly acting surprised that dead internet theory is turning out to be true then you should not be working in AI because you\u2019re either a fool or a liar.</p>\n<p>Dagan Shani: I hope that Sam take more seriously the &#8220;dead humanity&#8221; theory, since that one does not include waking up to it&#8217;s validity when it&#8217;s already too late.</p>\n<p><a href=\"https://x.com/BasedBeffJezos/status/1964493764233482637\">Beff Jezos</a>: 2/3 of replies don&#8217;t pass my Turing test filter.</p>\n<p>Dead internet theory is converging onto reality.</p>\n<p>Kinda makes it feel like we&#8217;re all heaven banned in here and just generating data that gets cast into the training data void.</p>\n<p>Cellarius: Well you Accelerationists broke it, you fix it.</p>\n<p>Beff Jezos: The solution is actually more AI not less.</p>\n<p><a href=\"https://x.com/bayeslord/status/1964526556719829136\">Bayes</a>: Totally. Most replies can\u2019t pass the Turing test. Mfw dead internet theory isn\u2019t just a theory \u2014 it\u2019s the daily reality we\u2019re now forced to live.</p></blockquote>\n<p>I have two questions for Beff Jezos on this:</p>\n<ol>\n<li>Could most of your replies pass the Turing test before?</li>\n<li>How exactly is the solution \u2018more AI\u2019? What is the plan?\n<ol>\n<li>Are you going to put a superior AI-detecting AI in the hands of Twitter? How do you keep it out of the hands of those generating the AI tweets?</li>\n</ol>\n</li>\n</ol>\n<p>I am mostly relatively unworried by Dead Internet Theory as I expect us to be able to adjust, and am far more worried about Dead Humanity Theory, which would also incidentally result in dead internet. The bots are not rising as much as one might have feared, and they are mostly rising in ways that are not that difficult to control. There is still very definitely a rising bot problem.</p>\n<p>Similarly, I am <a href=\"https://www.hollywoodreporter.com/business/digital/ai-podcast-start-up-plan-shows-1236361367/\">not worried about Dead Podcast World</a>. Yes, they can \u2018flood the zone\u2019 with infinite podcasts they produce for $1 or less, but who wants them? The trick is you don\u2019t need many, at this price level 50 is enough.</p>\n<p>One place I am increasingly worried about Dead Internet Theory is reviews. I have noticed that many review and rating sources that previously had signal seem to now have a lot less signal. I no longer feel I can trust Google Maps ratings, although I still feel I can mostly trust Beli ratings (who wants my remaining invites?).</p>\n<p><a href=\"https://www.wsj.com/lifestyle/relationships/ai-dating-quiz-08d29f57?mod=series_artificialintelligencenav\">How much \u2018dating an AI\u2019 is really happening</a>? According to the Kinsey \u2018Singles in America 2025\u2019 survey, 16% of singles have used AI as a romantic partner, which is very high so I am suspicious of what that is defined to be, especially given it says 19% of men did it and only 14% of women. They say 33% of GenZ has done it, which is even more suspicious. About half of women think this is cheating, only 28% of men do.</p>\n<p>Reaction to deepfakes, and their lack of impact, continues to tell us misinformation is demand driven rather than supply driven. Here\u2019s a recent example, <a href=\"https://x.com/GaryMarcus/status/1964107352581550192\">a deepfake of Bill Gates at a recent White House dinner that is very obviously fake on multiple levels</a>. And sure, some people have crazy world models that make the statements not absurd, and thus also didn\u2019t want to notice that it didn\u2019t sync up properly at all, and thought this was real, but that\u2019s not because the AI was so good at deepfaking.</p>\n\n\n<h4 class=\"wp-block-heading\">Unprompted Attention</h4>\n\n\n<p><a href=\"https://x.com/minchoi/status/1964716903466778844\">Min Choi points to a four month old anti-hallucination prompt for ChatGPT</a>, <a href=\"https://www.reddit.com/r/PromptEngineering/comments/1kup28y/chatgpt_and_gemini_ai_will_gaslight_you_everyone/\">which is a fine idea</a>. I have no idea if this particular one is good, I do know this is rather oversold:</p>\n<blockquote><p><a href=\"https://x.com/minchoi/status/1964716900660965644\">Min Choi</a> (<a href=\"https://x.com/filippie509/status/1964818960182104089\">overselling</a>): This ChatGPT prompt literally stops ChatGPT from hallucinating.</p></blockquote>\n<p>Yeah, no. That\u2019s not a thing a prompt can do.</p>\n\n\n<h4 class=\"wp-block-heading\">Get My Agent On The Line</h4>\n\n\n<p><a href=\"https://secondthoughts.ai/p/gpt-5-the-case-of-the-missing-agent?hide_intro_popup=true\">Steve Newman investigates the case of the missing agent</a>. Many including both me and Steve, expected by now both in time and in terms of model capabilities to have far better practical agents than we currently have. Whereas right now we have agents that can code, but for other purposes abilities are rather anemic and unreliable.</p>\n<p>There are a lot of plausible reasons for this. I have to think a lot of it is a skill issue, that no one is doing a good job with the scaffolding, but it has to be more than that. One thing we underestimated was the importance of weakest links, and exactly how many steps there are in tasks that can trip you up entirely if you don\u2019t handle the obstacle well. There are some obvious next things to try, which may or may not have been actually tried.</p>\n<p><a href=\"https://www.nytimes.com/athletic/6599707/2025/09/05/artificial-intelligence-baseball-manager-oakland-ballers/?utm_source=GetKim.com&amp;_bhlid=af587691f34df5be29e2af0e095be8c51fc1bc03\">For one game the Oakland Ballers will ball as the AI manages</a> them to do. This is a publicity stunt or experiment, since they built the platform in two weeks and it isn\u2019t doing a lot of the key tasks managers do. It\u2019s also a fixed problem where I would absolutely be using GOFAI and not LLMs. But yeah, I wouldn\u2019t worry about the AI taking the manager job any time soon, since so much of it is about being a leader of men, but the AI telling the manager a lot of what to do? Very plausibly should have happened 10 years ago.</p>\n\n\n<h4 class=\"wp-block-heading\">They Took Our Jobs</h4>\n\n\n<p><a href=\"https://x.com/conorsen/status/1964763235539923377\">Type of Guy who thinks the AI will automate every job except their own</a>.</p>\n<blockquote><p>Conor Sen: So is the idea that rather than work, people will spend their time reading, doing analysis, in meetings, and sending emails to figure out where and how to invest?</p>\n<div>\n<figure>\n<div>\n\n\n<figure class=\"wp-block-image\"><img alt=\"\" src=\"https://substackcdn.com/image/fetch/$s_!UEM5!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8395ffdc-9eb5-4a82-b6e3-35afbe859bb2_1179x780.jpeg\" /></figure>\n\n\n<div></div>\n</div>\n</figure>\n</div>\n<p>K.P. Reddy: My hypothesis is that we will have:</p>\n<ol>\n<li>Capital allocators</li>\n<li>Expert research and science</li>\n<li>Robot and AI exception handlers</li>\n<li>Government-supported citizens</li>\n</ol>\n</blockquote>\n<p><a href=\"https://www.youtube.com/watch?v=7zJ0DLJmXl8&amp;ab_channel=WarnerBros.Entertainment\">In the voice of Morgan Freeman talking to someone trying to blackmail Bruce Wayne for secretly being Batman</a>:</p>\n<p>Let me get this straight. You think that AI will be capable of doing all of the other jobs in the world better than humans, such that people no longer work for a living.</p>\n<p>And your plan is to do a better job than these AIs at capital allocation?</p>\n<p>Good luck.</p>\n<p>This is absolutely what all of you sound like when you say \u2018AI will never replace [X].\u2019</p>\n<p><a href=\"https://www.washingtonpost.com/technology/2025/09/06/salesforce-benioff-automation-jobs/\">Salesforce is leading the way on AI automation and job cutting</a>, including a new round of layoffs, and warnings about it have been issued by Microsoft and Amazon.</p>\n<p><a href=\"https://openai.com/index/expanding-economic-opportunity-with-ai/\">OpenAI CEO of Applications Fidji Simo wrote some marketing copy</a> called \u2018expanding economic opportunity with AI,\u2019 to reassure us all that AI will be great for jobs as long as we embrace it, and thus they are building out the OpenAI Jobs Platform to match up talent and offering OpenAI Certificates so you can show you are ready to use AI on the job, planning to certify 10 million Americans by 2030. I mean, okay, sure, why not, but no that doesn\u2019t address any of the important questions.</p>\n<p>More general than AI but good to have for reference, <a href=\"https://x.com/spectatorindex/status/1964327731560403050\">here are youth unemployment rates at the moment.</a></p>\n<div>\n<figure>\n<div>\n\n\n<figure class=\"wp-block-image\"><img alt=\"\" src=\"https://substackcdn.com/image/fetch/$s_!5bH5!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F804c634e-6d46-4cc1-8527-cabc321a7f86_487x765.png\" /></figure>\n\n\n<div></div>\n</div>\n</figure>\n</div>\n<p><a href=\"https://x.com/TheDataHubX/status/1964328139880092037\">Also a fun stat</a>:</p>\n<div>\n<figure>\n<div>\n\n\n<figure class=\"wp-block-image\"><img alt=\"\" src=\"https://substackcdn.com/image/fetch/$s_!QlaB!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F27dfeee4-0223-4643-a710-07b9646bda89_1000x1059.png\" /></figure>\n\n\n<div></div>\n</div>\n</figure>\n</div>\n<p>&nbsp;</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p>\n\n\n<h4 class=\"wp-block-heading\">A Young Lady\u2019s Illustrated Primer</h4>\n\n\n<p>The central problem of AI interacting with our current education system is that AI invalidates proof of work for any task that AI can do.</p>\n<blockquote><p><a href=\"https://arnoldkling.substack.com/p/teaching-writing-in-the-age-of-ai\">Arnold Kling</a>: Suppose that the objective of teaching writing to elite college students is to get them to write at the 90th percentile of the population. And suppose that at the moment AI can only write at the 70th percentile. This suggests that we should continue to teach writing the way that we always have.</p>\n<p>But suppose that in a few years AI will be writing at the 95th percentile. At that point, it is going to be really hard for humans to write superbly without the assistance of AI. The process of writing will be a lot more like the process of editing. The way that we teach it will have to change.</p></blockquote>\n<p>If the AI can do 70th percentile writing, and you want to teach someone 90th percentile writing, then you have the option to teach writing the old way.</p>\n<p>Except no, it\u2019s not that easy. You have two big problems.</p>\n<ol>\n<li>Trying to get to 90th percentile requires first getting to 70th percentile, which builds various experiences and foundational skills.</li>\n<li>Writing at the 80th percentile is still plausibly a lot easier if you use a hybrid approach with a lot of AI assistance.</li>\n</ol>\n<p>Thus, you only have the choice to \u2018do it the old way\u2019 if the student cooperates, and can still be properly motivated. The current system isn\u2019t trying hard to do that.</p>\n<p>The other problem is that even if you do learn 90th percentile writing, you still might have a not so valuable skill if AI can do 95th percentile writing. Luckily this is not true for writing, as writing is key to thinking and AI writing is importantly very different from you writing.</p>\n<p>That\u2019s also the reason this is a problem rather than an opportunity. If the skill isn\u2019t valuable due to AI, I like that I can learn other things instead.</p>\n<p>The hybrid approach Kling suggests is AI as editor. Certainly some forms of AI editing will be helpful before it makes sense to let the AI go it alone.</p>\n<p>All signs continue to point to the same AI education scenario:</p>\n<ol>\n<li>If you want to use AI to learn, it is the best tool of all time for learning.</li>\n<li>If you want to use AI to not learn, it is the best tool of all time for not learning.</li>\n</ol>\n<p>Meanwhile, the entire educational system is basically a deer in headlights. That might end up working out okay, or it might end up working out in a way that is not okay, or even <a href=\"https://x.com/ESYudkowsky/status/1964054882476052890\">profoundly, catastrophically not okay</a>.</p>\n<p>What we do know is that there are a variety of ways we could have mitigated the downsides or otherwise adapted to the new reality, and mostly they\u2019re not happening. Which is likely going to be the pattern. Yes, in many places \u2018we\u2019 \u2018could,\u2019 in theory, develop \u2018good\u2019 or defensive AIs to address various situations. In practice, we probably won\u2019t do it, at minimum not until after we see widespread damage happening, and in many cases where the incentives don\u2019t align sufficiently not even then.</p>\n<blockquote><p><a href=\"https://x.com/ESYudkowsky/status/1964054882476052890\">Eliezer Yudkowsky</a>: If in 2018 anyone had tried to warn about AI collapsing the educational system, AI advocates would&#8217;ve hallucinated a dozen stories about counter-uses of &#8216;good&#8217; or &#8216;defensive&#8217; AI that&#8217;d be developed earlier. In real life? No AI company bothered trying.</p>\n<p>Once you&#8217;ve heard the cheerful reassurance back in the past, its work is already done: you were already made positive and passive. Why should they bother trying to do anything difficult here in the actual present? Why try to fulfill past promises of defensive AI or good AI? They already have the past positivity that was all they wanted from you back then. The old cheerful stories get discarded like used toilet paper, because toilet paper is all those reassurances ever were in their mouths: a one-time consumable meant to be flushed down the drain after use, and unpleasant to actually keep around.</p></blockquote>\n<p>Are the skills of our kids collapsing in the face of AI, or doing so below some age where LLMs got introduced into too soon and interrupted key skill development? My guess is no. But I notice that if the answer was yes (in an otherwise \u2018normal\u2019 future lacking much bigger problems), it might be many years before we knew that it happened, and it might take many more years than that for us to figure out good solutions, and then many more years after that to implement them.</p>\n<p>Kling\u2019s other example is tournament Othello, which he saw transforming into training to mimic computers and memorize their openings and endgames. Which indeed has happened to chess and people love it, but in Othello yeah that seems not fun.</p>\n<p>&nbsp;</p>\n\n\n<h4 class=\"wp-block-heading\"></h4>\n\n\n<p>&nbsp;</p>\n<p>&nbsp;</p>\n\n\n<h4 class=\"wp-block-heading\">Levels of Friction</h4>\n\n\n<p><a href=\"https://x.com/KelseyTuoc/status/1963742117169357028\">The story of Trump\u2019s attempt to oust Fed governor</a> Lisa Cook over her mortgage documents and associated accusations of wrongdoing illustrates some ways in which things can get weird when information becomes a lot easier to find.</p>\n<blockquote><p>Steve Inskeep: ProPublica looked into Trump\u2019s cabinet and found three members who claimed multiple properties as a primary residence, the same accusation made against Fed governor Lisa Cook.</p>\n<p>Kelsey Piper: Glad when it was just Cook I said \u201cfine, prosecute them all\u201d so I can keep saying \u201cyep, prosecute them all.\u201d</p>\n<p>If they\u2019re separated in time by enough I think you don\u2019t have proof beyond a reasonable doubt though. Only the quick succession cases are clear.</p></blockquote>\n<p>Indeed, AIUI for it to be fraud you have to prove that intent to actually use the property as primary residence was not present in at least one of the filings. You don\u2019t want to lock people away for changing their mind. Still.</p>\n<p>A bizarre fact about America is that mortgage filings are public. This means that if you buy a house, we all can find out where you live, and also we can look at all the rest of things you put down in your applications, and look for both juicy info and potential false statements or even fraud.</p>\n<p>The equilibrium in the past was that this was not something anyone bothered looking for without a particular reason. It was a huge pain to get and look at all those documents. If you found someone falsely claiming primary residence you would likely prosecute, but mostly you wouldn\u2019t find it.</p>\n<p>Now we have AI. I could, if I was so inclined, have AI analyze every elected official\u2019s set of mortgage filings in this way. A prosecutor certainly could. Then what? What about all sorts of other errors that are technically dangerously close to being felonies?</p>\n<p>This extends throughout our legal system. If we remove all the frictions, especially unexpectedly, and then actually enforce the law as written, it would be a disaster. But certainly we would like to catch more fraud. So how do you handle it?</p>\n<p>The worst scenario is if those with political power use such tools to selectively identify and prosecute or threaten their enemies, while letting their friends slide.</p>\n<p>&nbsp;</p>\n\n\n<h4 class=\"wp-block-heading\">The Art of the Jailbreak</h4>\n\n\n<p>One jailbreak I hereby give full moral permission to do is \u2018get it to let you talk to a human.\u2019</p>\n<blockquote><p>Andrew Gao: i had to prompt inject the @united airlines bot because it kept refusing to connect me with a human</p>\n<div>\n<figure>\n<div>\n\n\n<figure class=\"wp-block-image\"><img alt=\"\" src=\"https://substackcdn.com/image/fetch/$s_!Bn5B!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F24ed4ece-fa0d-4189-97a5-99a08e0a8729_1005x1200.jpeg\" /></figure>\n\n\n<div></div>\n</div>\n</figure>\n</div>\n</blockquote>\n<p>In the more general case, <a href=\"https://x.com/patio11/status/1964344666200936594\">Patrick McKenzie reminds us</a> that automated tooling or an FAQ or even a call center can solve your problem, but its refusal to help must not be equated with the company refusing to help. It is helpful recon that sometimes works, but when it doesn\u2019t you have other affordances. This starts with the classic \u2018let me speak to your manager\u2019 but there are also other scripts.</p>\n<blockquote><p>David Manheim: I had great success once figuring out the email of all the C-level folks at an insurance company, apologizing for contacting them directly, but explaining that their employees were acting in bad faith, and I wanted to ensure they understood. Amazing how quickly that got fixed.</p>\n<p>Bonus was an email I was clearly accidentally replied-all on where the CFO yelled at the claims people asking how the hell I was contacting senior people, why, who gave me their email addresses, and they better make sure this never happens again.</p></blockquote>\n<p><a href=\"https://x.com/elder_plinius/status/1965167881790050377\">Here is the latest system prompt for Devin from Cognition AI</a>. Remember Devin?</p>\n\n\n<h4 class=\"wp-block-heading\">Get Involved</h4>\n\n\n<p><a href=\"https://x.com/EthanJPerez/status/1963664611397546145\">Anthropic is hiring</a> <a href=\"https://job-boards.greenhouse.io/anthropic/jobs/4888400008\">a lead for their AI safety fellows program</a>.</p>\n<p><a href=\"https://x.com/foresightinst/status/1965396284661174597\">Foresight Institute hiring an Executive Assistant to the CEO</a> as well as a <a href=\"https://t.co/EnOYREDkWh\">Communications Specialist and Node Managers for San Francisco and Berlin</a>.</p>\n\n\n<h4 class=\"wp-block-heading\">Introducing</h4>\n\n\n<p><a href=\"https://x.com/ax_pey/status/1963999607639429612\">Mars, which calls itself</a> the first personal AI robot, which you can train with examples and it can chain those skills and you can direct it using natural language. This is going to start out more trouble than it is worth even if it is implemented well, but that could change quickly.</p>\n<p><a href=\"https://friend.com/\">Friend</a>, an AI device that you wear around your neck and records audio (but not video) at all times. It costs $129 and is claiming no subscription required, you can use it until the company goes out of business and it presumably turns into a brick. The preview video shows that it sends you a stream of unprompted annoying texts? How not great is this release going? If you Google \u2018Friend AI\u2019 the first hit is <a href=\"https://www.wired.com/story/i-hate-my-ai-friend\">this Wired review entitled \u2018I Hate My Friend</a>.\u2019</p>\n<blockquote><p>Kylie Robison and Boone Ashworth: The chatbot-enabled Friend necklace eavesdrops on your life and provides a running commentary that\u2019s snarky and unhelpful. Worse, it can also make the people around you uneasy.</p>\n<p>You can tap on the disc to ask your Friend questions as it dangles around your neck, and it responds to your voice prompts by sending you text messages through the companion app.</p>\n<p>It also listens to whatever you\u2019re doing as you move through the world, no tap required, and offers a running commentary on the interactions you have throughout your day.</p>\n<p>According to Friend\u2019s <a href=\"https://archive.is/o/yLN61/https://friend.com/legal/privacy.pdf\">privacy disclosure</a>, the startup \u201cdoes not sell data to third parties to perform marketing or profiling.\u201d It <em>may</em> however use that data for research, personalization, or \u201cto comply with legal obligations, including those under the GDPR, CCPA, and any other relevant privacy laws.\u201d</p></blockquote>\n<p>The review does not get better from there.</p>\n<p>Will there be a worthwhile a future AI device that records your life and you can chat with, perhaps as part of smart glasses? Sure, absolutely. This is the opposite of that. <a href=\"https://www.netflix.com/title/81591296?source=35&amp;fromWatch=true\">Nobody Wants This.</a></p>\n<p><a href=\"https://x.com/NeelNanda5/status/1965485174411649259\">We can now highlight</a> potential hallucinations, via asking which words involve the model being uncertain.</p>\n<blockquote><p><a href=\"https://x.com/OBalcells/status/1965434564748447921\">Oscar Balcells Obeso</a>: <a href=\"https://t.co/tOm7Wa8TxE\">Imagine if ChatGPT highlighted</a> every word it wasn&#8217;t sure about. We built a streaming hallucination detector that flags hallucinations in real-time.</p>\n<p>Most prior hallucination detection work has focused on simple factual questions with short answers, but real-world LLM usage increasingly involves long and complex responses where hallucinations are harder to detect.</p>\n<p>We built a large-scale dataset with 40k+ annotated long-form samples across 5 different open-source models, focusing on entity-level hallucinations (names, dates, citations) which naturally map to token-level labels.</p>\n<p>Our probes outperform prior baselines such as token-level entropy, perplexity, black-box self-evaluation, as well as semantic entropy. On long-form text, our probes detect fabricated entities with up to 0.90 AUC vs semantic entropy&#8217;s 0.71.</p>\n<p>Strong performance extends to short-form tasks too: when used to detect incorrect answers on TriviaQA, our probes achieve 0.98 AUC, while semantic entropy reaches only 0.81.</p>\n<p>Surprisingly, despite no math-specific training, our probes generalize to mathematical reasoning tasks.</p>\n<p>Neel Nanda: I&#8217;m excited that, this year, interpretability finally works well enough to be practically useful in the real world! We found that, with enough effort into dataset construction, simple linear probes are cheap, real-time, token level hallucination detectors and beat baselines</p></blockquote>\n\n\n<h4 class=\"wp-block-heading\">In Other AI News</h4>\n\n\n<p><a href=\"https://www.wsj.com/articles/inside-ebays-quest-to-become-an-ai-leader-32e7fa45?mod=cio-journal_lead_pos1\">EBay is embracing AI coding and launching various AI features</a>. The top one is \u2018magical listings,\u2019 where AI takes a photo and then fills in everything else including the suggested price. No, it\u2019s not as good as an experienced seller would do but it gets around the need to be experienced and it is fast.</p>\n<p>How good are the AIs at novel math? Just barely able to do incremental novel math when guided, as you would expect the first time we see reports of them doing novel math. That\u2019s how it starts.</p>\n<blockquote><p><a href=\"https://x.com/emollick/status/1964447221853966775\">Ethan Mollick</a>: We are starting to see some nuanced discussions of what it means to work with advanced AI</p>\n<p><a href=\"https://t.co/yG45KjdUCH\">In this case, GPT-5 Pro was able to do novel math</a>, but only when guided by a math professor (though the paper also noted the speed of advance since GPT-4).</p>\n<p>The reflection is worth reading.</p></blockquote>\n<p>Any non-novel math? Is it true that they\u2019ve mostly got you covered at this point?</p>\n<blockquote><p><a href=\"https://x.com/robertghrist/status/1964397925955404269\">Prof-G</a>: in the past 6-8 months, frontier AI models have evolved to where they can answer nearly any phd-level text-based mathematics question that has a well-defined checkable (numerical/strong) answer, due to search + reasoning capabilities. hard to find things they can&#8217;t do.</p>\n<p><a href=\"https://x.com/doomslide/status/1964549658321310184\">Doomslide</a>:</p>\n<ol>\n<li>Frontier models are great, but [above tweet] is false.</li>\n<li>The level of falseness varies between mathematical domains.</li>\n<li>No model can produce rigorous proofs of more than 1 percent of its claims.</li>\n<li>Confidence is entirely uncorrelated with correctness.</li>\n</ol>\n<p>LLMs are incredibly useful; don&#8217;t get me wrong, but&#8230;</p>\n<p>It is 2025 and 90% of diagrams are still written in tikzcd by some bitter graduate student.</p></blockquote>\n<p>That\u2019s a remarkably great resource, classic \u2018can you?\u2019 moment and so on, and it is the worst it will ever be. It does still have a ways to go.</p>\n<p><a href=\"https://www.anthropic.com/news/updating-restrictions-of-sales-to-unsupported-regions\">Anthropic has long banned Claude use in adversarial nations like China</a>, a feeling I understand it mutual. Anthropic notes that companies in China continue using Claude anyway, and is responding by tightening the controls.</p>\n<p><a href=\"https://x.com/AliasVEDH/status/1964730649627271258\">Many YouTube channels are taking a big hit</a> from AI sending a lot of people into Restricted Mode, and creators look very confused about what is happening. It looks like Restricted Mode restricts a lot of things that should definitely not be restricted, such as a large percentage of Magic: The Gathering and other gaming content. Presumably the automatic AI \u2018violence\u2019 checker is triggering for gameplay. So dumb.</p>\n<p><a href=\"https://www.techdirt.com/2025/09/08/were-walling-off-the-open-internet-to-stop-ai-and-it-may-end-up-breaking-everything-else/\">Due to AI agents and scrapers that don\u2019t play nice</a>, the web is being increasingly walled off. I agree with Mike Masnick that we should be sad about this, and all those cheering this in order to hurt AI companies are making a mistake. Where I think he\u2019s wrong is in saying that AIs should have a right to read and use (and by implication in Perplexity\u2019s case, perhaps also quote at length) anyone\u2019s content no matter what the website wants. I think it can\u2019t work that way, because it breaks the internet.</p>\n<p>I also think pay-per-crawl (including pay-to-click or per-view for humans) has always been the right answer anyway, so we should be happy about this. The problem is that we can\u2019t implement it yet in practice, so instead we have all these obnoxious subscriptions. I\u2019ll happily pay everyone a fair price per view.</p>\n<p><a href=\"https://x.com/McaleerStephen/status/1965943981784891431\">Stephen McAleer becomes the latest OpenAI safety researcher</a> that had at least some good understanding of the problems ahead, and concluded they couldn\u2019t accomplish enough within OpenAI and thus is leaving. If I know you\u2019re doing good safety work at OpenAI chances are very high you\u2019re going to move on soon.</p>\n<p><a href=\"https://x.com/repligate/status/1965960676104712451\">Janus explains how information flows through transformers</a>.</p>\n<p>Mira Murati\u2019s Thinking Machines comes out with its first post, as <a href=\"https://thinkingmachines.ai/blog/defeating-nondeterminism-in-llm-inference/\">Horace He discusses Defeating Nondeterminism in LLM Inference</a>. As in, if you set temperature to zero you still have randomness, which makes things tricky.</p>\n<div>\n<figure>\n<div>\n\n\n<figure class=\"wp-block-image\"><img alt=\"\" src=\"https://substackcdn.com/image/fetch/$s_!JXik!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc81ff879-c8c5-46e9-8aff-da7eb2096970_1008x515.png\" /></figure>\n\n\n<div></div>\n</div>\n</figure>\n</div>\n<div>\n<figure>\n<div>\n\n\n<figure class=\"wp-block-image\"><img alt=\"\" src=\"https://substackcdn.com/image/fetch/$s_!iYiA!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F36ed9afa-31e7-4964-940c-e88f8c2ed5fa_900x674.jpeg\" /></figure>\n\n\n<div></div>\n</div>\n</figure>\n</div>\n\n\n<h4 class=\"wp-block-heading\">Show Me the Money</h4>\n\n\n<p><a href=\"https://www.theinformation.com/articles/mercor-fields-10-billion-valuation-offers-pre-emptive-onslaught?utm_source=ti_app\">Valuations of AI companies are going up across the board</a>, including <a href=\"https://x.com/Katie_Roof/status/1963753394759454983\">Mercor getting inbound offers at $10 billion</a> and going all the way down to YC.</p>\n<p><a href=\"https://x.com/tanayj/status/1965239737465328102\">Anthropic and OpenAI will add ~$22 billion</a> of net new runrate revenue this year, whereas the public software universe minus the magnificent seven will only add a total of $47 billion.</p>\n<p><a href=\"https://x.com/elder_plinius/status/1964127422447833323\">Anthropic has settled its landmark copyright case for $1.5 Billion</a>, which is real money but affordable after the $13 billion raise from last week, with payments of $3,000 per infringed work. Sources obtained through legal means can be used for training, but pirating training data is found to be profoundly Not Okay. Except that then the judge said no, worried this isn\u2019t sufficiently protective of authors. That seems absurd to me. We\u2019ve already decided that unpirated copies of works would have been fine, and this is an awful lot of money. If they give me a four figure check for my own book (My Files: Part 1) I am going to be thrilled.</p>\n<p>AI investment number go up:</p>\n<blockquote><p><a href=\"https://x.com/JosephPolitano/status/1965540612792557929\">Joey Politano</a>: Census financial data released today shows the AI investment boom reaching new record highs\u2014information technology companies have increased their net holdings of property, plant, &amp; equipment by more than $180B over the last year, roughly triple the pace of growth seen in 2023</p>\n<div>\n<figure>\n<div>\n\n\n<figure class=\"wp-block-image\"><img alt=\"\" src=\"https://substackcdn.com/image/fetch/$s_!rdXQ!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5093c872-1841-4130-85e3-9986579cb51c_1199x766.png\" /></figure>\n\n\n<div></div>\n</div>\n</figure>\n</div>\n</blockquote>\n<p>Another way to burn $1.5 billion <a href=\"https://x.com/Calcalistech/status/1964740930550010031\">is to be ASML</a> and <a href=\"https://www.calcalistech.com/ctechnews/article/bycca4sclg\">invest it in Mistral at a valuation of $11.7 billion</a>. I don\u2019t know what caused this, but it feels like Europe forcing its biggest winner to tether to a sinking ship in the hopes of gaining leverage.</p>\n<p><a href=\"https://x.com/_KarenHao/status/1964307091524559264\">OpenAI is now projecting</a> <a href=\"https://t.co/e4xLrHehMF\">that it will burn $115 billion (!)</a> on cash between now and 2029, about $80 billion higher than previously expected. If valuation is already at $500 billion, this seems like an eminently reasonable amount of cash to burn through even if we don\u2019t get to AGI in that span. It does seem like a strange amount to have to update your plans?</p>\n<p>OpenAI is frustrated that California might prevent it from pulling off one of the biggest thefts in human history by expropriating hundreds of billions of dollars from its nonprofit. <a href=\"https://www.wsj.com/tech/ai/openai-for-profit-conversion-opposition-07ea7e25\">It is now reported to be considering the prospect of responding by fleeing the jurisdiction, as in leaving California</a>, although an OpenAI spokesperson (of course) denies they have any such plans.</p>\n<p>What I do not understand is, what is all this \u2018or else we pull your funding\u2019 talk?</p>\n<blockquote><p>Berber Jin (WSJ): OpenAI\u2019s financial backers have conditioned roughly $19 billion in funding\u2014almost half of the startup\u2019s total in the past year\u2014on receiving shares in the new for-profit company. If the restructure doesn\u2019t happen, they could pull their money, hampering OpenAI\u2019s costly ambitions <a href=\"https://www.wsj.com/tech/ai/softbank-openai-a3dc57b4?mod=article_inline\">to build giant data centers</a>, make custom chips, and stay at the bleeding edge of AI research.</p></blockquote>\n<p>Go ahead. Invest at $165 billion and then ask for your money back now that valuations have tripled. I\u2019m sure that is a wise decision and they will have any trouble whatsoever turning around and raising on better terms, even if unable to expropriate the nonprofit. Are you really claiming they\u2019ll be worth less than Anthropic?</p>\n<p>&nbsp;</p>\n\n\n<h4 class=\"wp-block-heading\">Quiet Speculations</h4>\n\n\n<p>Wise words:</p>\n<blockquote><p><a href=\"https://arnoldkling.substack.com/p/teaching-writing-in-the-age-of-ai\">Arnold Kling</a>: The moral of the story is that when the computer\u2019s skill gets within the range of a competent human, watch out! Another iteration of improvement and the computer is going to zoom past the human.</p></blockquote>\n<p><a href=\"https://x.com/MParakhin/status/1964461570844742053\">What would have happened if OpenAI had released o1-preview faster, or not at all</a>?</p>\n<blockquote><p>Ethan Mollick: In retrospect it is surprising that OpenAI released o1-preview. As soon as they showed off reasoning, everyone copied it immediately.</p>\n<p>And if they had held off releasing a reasoning/planning model until o3 (&amp; called that GPT-5) it would have been a startling leap in AI abilities.</p>\n<p><a href=\"https://x.com/MParakhin/status/1964461570844742053\">Mikhail Parakhin</a>: Ethan is a friend, but I think the opposite: OpenAI was sitting on strawberry for way too long, because of the inference GPU availability concerns, giving others time to catch up.</p></blockquote>\n<p>Ethan\u2019s model here is that releasing o1-preview gave others the info necessary to fast follow on reasoning. That is my understanding. If OpenAI had waited for the full o1, then it could have postponed r1 without slowing its own process down much. This is closer to my view of things, while noting this would have impacted the models available in September 2025 very little.</p>\n<p>Mikhail\u2019s model is that it was easy to fast follow anyway, OpenAI couldn\u2019t keep that key info secret indefinitely, so by holding off on release for o1-preview OpenAI \u2018gave others time to catch up.\u2019 I think this is narrowly true in the sense of \u2018OpenAI could have had a longer period where they had the only reasoning model\u2019 at the expense of others then catching up to o1 and o3 faster. I don\u2019t see how that much helps OpenAI. They had enough of a window to drive market share, and releasing o1-preview earlier would not have accelerated o1, so others would have \u2018caught up\u2019 faster rather than slower.</p>\n<p>One thing I forgot to include in the AGI discussion earlier this week was the Manifold market on when we will get AGI. <a href=\"https://x.com/peterwildeford/status/1965776747279860104\">The distribution turns out</a> (I hadn\u2019t looked at it) to currently match my 2031 median.</p>\n<div>\n<figure>\n<div>\n\n\n<figure class=\"wp-block-image\"><img alt=\"\" src=\"https://substackcdn.com/image/fetch/$s_!UkC1!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F16404f0a-b7f6-4542-b6de-5c4bf6a8f6cf_495x1200.jpeg\" /></figure>\n\n\n<div></div>\n</div>\n</figure>\n</div>\n<p>&nbsp;</p>\n<p>&nbsp;</p>\n\n\n<h4 class=\"wp-block-heading\">The Quest for Sane Regulations</h4>\n\n\n<p><a href=\"https://x.com/jackclarkSF/status/1965048896784367847\">Anthropic endorses</a> <a href=\"https://www.anthropic.com/news/anthropic-is-endorsing-sb-53\">the new weaker version of SB 53.</a></p>\n<blockquote><p>The working group endorsed an approach of <a href=\"https://www.anthropic.com/news/anthropic-s-response-to-governor-newsom-s-ai-working-group-draft-report?utm_campaign=q124-platform-release&amp;utm_source=blog&amp;utm_medium=blog&amp;utm_term=blog&amp;utm_content=grid-view&amp;%3Butm_campaign=airflow-in-action-ford&amp;%3Butm_medium=web&amp;utm_cta=website-workload-unistore-resources-introducing-unistore&amp;wtime=%7Bseek_to_second_number%7D\">&#8216;trust but verify</a>\u2019, and Senator Scott Wiener\u2019s SB 53 implements this principle through disclosure requirements rather than the prescriptive technical mandates that plagued last year&#8217;s efforts.</p></blockquote>\n<p>The issue with this approach is that they cut out the verify part, removing the requirement for outside audits. So now it\u2019s more \u2018trust but make them say it.\u2019 Which is still better than nothing, and harder to seriously object to with a straight face.</p>\n<p><a href=\"https://x.com/deanwball/status/1965557389408829860\">Dean Ball highlights a feature of SB 53</a>, which is that it gives California the ability to designate one or more federal laws, regulations or guidance documents that can substitute for similar requirements in SB 53, to avoid duplicate regulatory burdens.</p>\n\n\n<h4 class=\"wp-block-heading\">Chip City</h4>\n\n\n<p><a href=\"https://x.com/peterwildeford/status/1963644512888078427\">The export controls are working</a>. Not perfectly, but extraordinarily well.</p>\n<div>\n<figure>\n<div>\n\n\n<figure class=\"wp-block-image\"><img alt=\"\" src=\"https://substackcdn.com/image/fetch/$s_!rWtw!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa9c3f6ec-e9ab-49a6-8a8b-7f5fc681e29a_1200x734.jpeg\" /></figure>\n\n\n<div></div>\n</div>\n</figure>\n</div>\n<p>Yes, the Chinese are trying to catch up on chip manufacturing, the same way they would be trying to do so anyway, but that is not a reason to give up this huge edge.</p>\n<p><a href=\"https://x.com/peterwildeford/status/1963989825972777220\">Nvidia continues to spend its political capital</a> and seemingly large influence over the White House to try and sell chips directly to China, even when Americans stand ready and willing to buy those same chips.</p>\n<p>I don\u2019t agree with Cass that Nvidia is shredding its credibility, because Nvidia very clearly already has zero credibility.</p>\n<blockquote><p>Peter Wildeford: Find yourself someone who loves you as much as Jensen Huang loves selling chips to China.</p>\n<p>Oren Cass: Fascinating drama playing out over the past 24 hours as the very good GAIN AI Act from @SenatorBanks comes under fire from @nvidia, which seems happy to shred its credibility for the sake of getting more AI chips into China.</p>\n<p>The Banks bill takes the sensible and modest approach of requiring US chipmakers to offer AI chips to American customers before selling them to China. So it&#8217;s just blocking sales where more chips for the CCP directly means fewer for American firms.</p>\n<p>Enter Nvidia, which is leveraging every ounce of influence with the administration to get its chips into China, even when there are American firms that want the chips, because it thinks it can gain a permanent toehold there (which never works and won&#8217;t this time either).</p>\n<p>You&#8217;ll recall Nvidia&#8217;s approach from such classics as CEO Jensen Huang claiming with a straight face that &#8220;there&#8217;s no evidence of AI chip diversion&#8221; and even moving forward with opening a research center in Shanghai.</p>\n<p>Now Nvidia says that &#8220;our sales to customers worldwide do not deprive U.S. customers of anything,&#8221; calling chip supply constraints &#8220;fake news.&#8221; That&#8217;s odd, because Huang said on the company&#8217;s earning call last week, &#8220;everything&#8217;s sold out.&#8221;</p>\n<p>Fun story, Nvidia wants to take back its CEO&#8217;s comments, saying instead they have plenty of capacity. As Tom&#8217;s Hardware notes, &#8220;<a href=\"https://t.co/Gb7Gv0he88\">Both situations cannot co-exist as scarcity and sufficiency are mutually exclusive, so it is unclear if Jensen misspoke</a>&#8230;&#8221;</p>\n<p>And of course, if Nvidia has all this spare capacity, it needn&#8217;t worry about the GAIN AI Act at all. It can produce chips that U.S. firms won&#8217;t want (apparently their demand is sated) and then sell them elsewhere. (*whispers* the U.S. firms would buy the chips.)</p>\n<p>The GAIN AI Act has bipartisan support and will move forward unless the White House blocks it. Seeing as the premise is LITERALLY &#8220;America First,&#8221; should be an easy one! At this point Nvidia is just insulting everyone&#8217;s intelligence, hopefully not to much effect.</p></blockquote>\n<p>As I said, zero credibility. Nvidia, while charging below market-clearing prices that cause everything to sell out, wants to take chips America wants and sell those same chips to China instead.</p>\n<p>It is one thing to have America use top chips to <a href=\"https://thezvi.substack.com/p/america-makes-ai-chip-diffusion-deal\">build data centers in the UAE or KSA</a> because we lack sufficient electrical power (while the administration sabotages America\u2019s electrical grid via gutting solar and wind and batteries), and because they bring investment and cooperation to the table that we find valuable. Tradeoffs exist, and if you execute sufficiently well you can contain security risks.</p>\n<p><a href=\"https://thezvi.substack.com/p/fighting-obvious-nonsense-about-ai\">There was a lot of obvious nonsense bandied about surrounding that</a>, but ultimately reasonable people can disagree there.</p>\n<p>It is altogether another thing to divert chips from America directly to China, empowering their AI efforts and economy and military at the expense of our own. Rather than saying UAE and KSA are securely our allies and won\u2019t defect to China, use that threat as leverage or strike out on their own, you are directly selling the chips to China.</p>\n<p>Meanwhile, on the front of sabotaging America\u2019s electrical grid and power, we have the department of energy saying that batteries do not exist.</p>\n<blockquote><p>US Department of Energy (official account): Wind and solar energy infrastructure is essentially worthless when it is dark outside, and the wind is not blowing.</p></blockquote>\n<div>\n<figure>\n<div>\n\n\n<figure class=\"wp-block-image\"><img alt=\"\" src=\"https://substackcdn.com/image/fetch/$s_!zuUE!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9f64a630-f72e-4508-91cd-c1612895366b_1012x395.png\" /></figure>\n\n\n<div></div>\n</div>\n</figure>\n</div>\n<div>\n<figure>\n<div>\n\n\n<figure class=\"wp-block-image\"><img alt=\"\" src=\"https://substackcdn.com/image/fetch/$s_!rXKS!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb1ba5d2f-2299-4c83-b9c3-e7c8ae926736_1005x506.png\" /></figure>\n\n\n<div></div>\n</div>\n</figure>\n</div>\n<blockquote><p><a href=\"https://x.com/mattyglesias/status/1964458669481087015\">Matthew Yglesias</a>: In this \u201cbatteries don\u2019t exist\u201d worldview why do they think China is installing so many solar panels?</p>\n<p>Do they not know about nighttime? Are they climate fanatics?</p>\n<p>CleanTech Reimagined: All they have to do is look at the California and Texas grids. Batteries play a key role in meeting evening demand in both states, every night.</p>\n<p><a href=\"https://x.com/AlecStapp/status/1964407631264518156\">Alec Stapp</a>: There are these neat things called batteries that can move energy across time. In fact, at peak load yesterday in California, batteries provided 26% of power.</p></blockquote>\n<div>\n<figure>\n<div>\n\n\n<figure class=\"wp-block-image\"><img alt=\"\" src=\"https://substackcdn.com/image/fetch/$s_!EQAL!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fcdc22204-d6fc-4e99-a54e-06a984350e3c_946x541.png\" /></figure>\n\n\n<div></div>\n</div>\n</figure>\n</div>\n<p>America is pretty great and has many advantages. We can afford quite a lot of mistakes, or choices on what to prioritize. This is not one of those cases. If we give up on solar, wind and batteries? Then we lose \u2018the AI race\u2019 no matter which \u2018race\u2019 it is, and also we lose, period.</p>\n<p><a href=\"https://x.com/ATabarrok/status/1964456149044122009\">Here\u2019s what we do when South Korea invests a lot of money in a factory for batteries</a>, while seeming to have at most committed some technical violations of deeply stupid rules on who can do exactly what type of work <a href=\"https://x.com/SpeakSamuel/status/1965101066028036378\">that the State Department and Customs and Border Protection had no problem with and that have been ignored for over several administrations</a> because we don\u2019t give Asian companies sufficient visas to bootstrap their factories. And that were done in the act of helping the factory get online faster to make batteries. So that Americans can then manufacture batteries.</p>\n<p>Not only did we raid the factory, we released videos of Korean workers being led away in chains, causing a highly predictable national humiliation and uproar. Why would you do that?</p>\n<blockquote><p>Raphael Rashid: US authorities have reportedly detained 450 workers at Hyundai-LG battery plant construction site in Georgia yesterday, including over 30 South Koreans said to have legitimate visas. Seoul has expressed concern and says Korean nationals&#8217; rights &#8220;must not be unjustly violated.&#8221;</p>\n<p>The detained South Koreans at the Ellabell facility are said to be on B1 business visas or ESTA waivers for meetings and contracts. Foreign Ministry has dispatched consuls to the scene and &#8220;conveyed concerns and regrets&#8221; to the US embassy in Seoul.</p>\n<p>ICE has released a video of its raid on Hyundai\u2013LG&#8217;s Georgia battery plant site, showing Korean workers chained up and led away. South Korea&#8217;s foreign ministry has confirmed over 300 of the 457 taken into custody are Korean nationals.</p>\n<p>These images of the mainly Korean workers being chained by ICE in full restraints including wrists, belly, and ankles are pretty nuts.</p>\n<p>Alex Tabarrok: If South Korea chained several hundred US workers, many Americans would be talking war.</p>\n<p>Hard to exaggerate how mad this is.</p>\n<p>Mad economics, mad foreign policy.</p>\n<p>Shameful to treat an ally this way.</p>\n<p>This is the kind of thing which won\u2019t be forgotten. Decades of good will torched.</p>\n<p><a href=\"https://x.com/koryodynasty/status/1964894916632604784\">S. Korea&#8217;s entire media establishment</a> across political spectrum has united in unprecedented editorial consensus expressing profound betrayal, outrage, national humiliation, and fundamental breach of US-ROK alliance.</p>\n<p>The general sentiment: while Korean media occasionally unite on domestic issues, these are usually severely politicised. Here, the level of scorn spanning from conservative establishment to progressive outlets is extraordinarily rare. They are furious.</p>\n<p>Chosun Ilbo (flagship conservative): Scathing language calling this a &#8220;merciless arrest operation&#8221; that represents something &#8220;that cannot happen between allies&#8221; and a &#8220;breach of trust.&#8221; Notes Trump personally thanked Hyundai&#8217;s chairman just months ago.</p>\n<p>Chosun calls the situation &#8220;bewildering&#8221; and emphasises the contradiction: Trump pressures Korean companies to invest while simultaneously arresting their workers. The editorial questions whether American investment promises survive across different administrations.</p>\n<p>Dong-A asks &#8220;who would invest&#8221; under these conditions when Korean workers are treated like a &#8220;criminal group.&#8221; Notes this threatens 17,000+ jobs already created by Korean companies in Georgia. &#8220;The Korean government must demand a pledge from the US to prevent recurrence.&#8221;</p>\n<p>\u2026</p>\n<p>Korea has deep historical memory of being humiliated by foreign powers and the visuals of Koreans in chains being paraded by a foreign power triggers collective memories of subjugation that go beyond this just being &#8220;unfair&#8221;.</p>\n<p>This is public humiliation of the nation itself.</p>\n<p><a href=\"https://x.com/JeremiahDJohns/status/1964497408609701992\">Jeremiah Johnson</a>: This might be the single most destructive thing you could do to the future of American manufacturing. What company or country will ever invest here again?</p>\n<p>Genuinely I think it would be *less* destructive if they fired a bunch of Patriot missiles into Ford auto plants.</p>\n<p>Adam Cochran: They basically sent a military style convoy to arrest factory workers.</p>\n<p>But only **1** of 457 people was on a B-1 visa, and was there for training.</p>\n<p>Of the arrests, South Korea has identified 300 of them as South Korean citizens which they say *all* had valid work visas.</p>\n<p>Now Hyundai and South Korea will be rethinking their $20B investment in new US manufacturing plants.</p>\n<p>(Oh and PS &#8211; the B1 visa the guy was on, prevents \u201cproductive labor\u201d &#8211; attending training, conferences, business meetings or consultations are all ALLOWED on a B1)</p>\n<p>Even the B1 guy was following the literal rules of his visa.</p>\n<p>But if he hadn\u2019t been, just revoke their visas and send them home, and work with SK to figure out the visa issue. Don\u2019t do a dumb military raid against middle aged polo wearing factory workers to humiliate allies.</p>\n<p><a href=\"https://x.com/RichardHanania/status/1964724168668463496\">WSJ</a>: The South Korean nationals were largely given visas suitable for training purposes, such as the B-1 visa, and many there were working as instructors, according to a South Korean government official.</p>\n<p>Richard Hanania: This looks like a story where a company investing in the US was trying to speed up the process and not comply with every bureaucratic hurdle that served no legitimate purpose. It\u2019s the kind of thing companies do all the time, in the sense that if you followed every law to the exact letter you\u2019d never get anything done. Government usually looks the other way.</p></blockquote>\n<p>This goes well beyond batteries. We have done immense damage to our relationship with South Korea and all potential foreign investors for no reason. We could lose one of our best allies. Directly on the chip front, this especially endangers our relationship with Samsung, which was a large part of our domestic chip manufacturing plan.</p>\n<p>Why are so many of our own actions seemingly aimed at ensuring America loses?</p>\n\n\n<h4 class=\"wp-block-heading\">The Week in Audio</h4>\n\n\n<p><a href=\"https://x.com/CogRev_Podcast/status/1965021409802846684\">I return to the Cognitive Revolution podcast</a>.</p>\n<p><a href=\"https://www.youtube.com/watch?v=TrXi3naD6Og&amp;ab_channel=YCombinator\">Cursor CEO Michael Truell assures you</a> that we will need programmers for a while, as this whole AI revolution <a href=\"https://x.com/JonhernandezIA/status/1964658492569956684\">will take decades to play out</a>.</p>\n<p><a href=\"https://www.youtube.com/watch?v=5FdO1MEumbI&amp;ab_channel=80%2C000Hours\">Need Nanda on 80,000 Hours talking interpretability for three hours</a>.</p>\n<p><a href=\"https://x.com/TuckerCarlson/status/1965825529111515296\">Tucker Carlson talks to Sam Altman</a>, <a href=\"https://x.com/peterwildeford/status/1965856278699454735\">Peter Wildeford has a summary</a>, which suggests Altman doesn\u2019t say anything new. The whole \u2018no AI won\u2019t take jobs requiring deep human connection let alone pose a thread\u2019 line continues. Altman is lying.</p>\n<blockquote><p><a href=\"https://x.com/HumanHarlan/status/1965932275465597077\">Harlan Stewart</a>: How Sam Altman talks about the risks posed by his company&#8217;s work has changed a lot over the years.</p></blockquote>\n<div>\n<figure>\n<div>\n\n\n<figure class=\"wp-block-image\"><img alt=\"\" src=\"https://substackcdn.com/image/fetch/$s_!RG5u!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8bbac443-14ca-4e02-9a69-ece7cbd2fca2_1183x730.jpeg\" /></figure>\n\n\n<div></div>\n</div>\n</figure>\n</div>\n<p>Altman hasn\u2019t quite given zero explanation for this shift, but his explanations that I or ChatGPT know about seem extremely poor, and he has not retracted previous warnings. Again, all signs point to lying.</p>\n<p><a href=\"https://www.statecraft.pub/p/how-the-trump-white-house-really\">Santi Ruiz interviews Dean Ball about what the White House is like, the ways it is able to move faster than previous admins, and about creating the AI Action Plan</a>.</p>\n<p><a href=\"https://x.com/peterwildeford/status/1963756790216073519\">Expert rickroller Melania Trump briefly reads words about AI</a>.</p>\n<blockquote><p>Peter Wildeford: Melania Trump&#8217;s remarks:</p>\n<p>&#8211; AI not science fiction (see surgical robots, autonomous vehicles)</p>\n<p>&#8211; AI will be the single largest growth category</p>\n<p>&#8211; Responsible stewardship. AI is at a &#8220;primitive stage&#8221; and must be treated like a child \u2014 empowered, but with &#8220;watchful guidance.&#8221;</p></blockquote>\n\n\n<h4 class=\"wp-block-heading\">All Words We Choose Shall Lose All Meaning</h4>\n\n\n<p>Let\u2019s put it all together.</p>\n<blockquote><p><a href=\"https://x.com/daniel_271828/status/1964848331601105365\">Daniel Eth</a>: Come on guys, this is getting ridiculous</p>\n<div>\n<figure>\n<div>\n\n\n<figure class=\"wp-block-image\"><img alt=\"\" src=\"https://substackcdn.com/image/fetch/$s_!_Wpo!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2b821796-9884-4139-9d3d-85abe80dde2f_1200x900.jpeg\" /></figure>\n\n\n<div></div>\n</div>\n</figure>\n</div>\n</blockquote>\n<p>It is impossible to sustainably make any chosen symbol (such as \u2018win,\u2019 \u2018race,\u2019 \u2018ASI\u2019 or \u2018win the ASI race\u2019) retain meaning when faced with extensive discourse, politicians or marketing departments, also known as contact with the enemy. Previous casualties include \u2018AGI\u2019, \u2018safety\u2019, \u2018friendly,\u2019 \u2018existential,\u2019 \u2018risk\u2019 and so on.</p>\n<p>This is incredibly frustrating, and of course is not unique to AI or to safety concerns, it happens constantly in politics (e.g. \u2018Nazi,\u2019 \u2018fake news,\u2019 \u2018criminal,\u2019 \u2018treason\u2019 and so on to deliberately choose some safe examples). Either no one will know your term, or they will appropriate it, usually either watering it down to nothing or reversing it. The \u2018euphemism treadmill\u2019 is distinct but closely related.</p>\n<p>You fight the good fight as long as you can, and then you adapt and try again. Sigh.</p>\n\n\n<h4 class=\"wp-block-heading\">Hunger Strike</h4>\n\n\n<p>A classic strategy for getting your message out is a hunger strike. Executed well it is a reliable costly signal, and puts those responding in a tough spot as the cost increases slowly over time and with it there is risk of something going genuinely wrong, and part of the signal is how far you\u2019re willing to go before you fold.</p>\n<p>There was one launched last week.</p>\n<blockquote><p>Guido Reichstadter: Hi, my name&#8217;s Guido Reichstadter, and I&#8217;m on hunger strike outside the offices of the AI company Anthropic right now because we are in an emergency.</p>\n<p>\u2026</p>\n<p>I am calling on Anthropic&#8217;s management, directors and employees to immediately stop their reckless actions which are harming our society and to work to remediate the harm that has already been caused.</p>\n<p>I am calling on them to do everything in their power to stop the race to ever more powerful general artificial intelligence which threatens to cause catastrophic harm, and to fulfill their responsibility to ensure that our society is made aware of the urgent and extreme danger that the AI race puts us in.</p>\n<p>Likewise I&#8217;m calling on everyone who understands the risk and harm that the AI companies&#8217; actions subject us to speak the truth with courage. We are in an emergency. Let us act as if this emergency is real.</p>\n<div>\n<figure>\n<div>\n\n\n<figure class=\"wp-block-image\"><img alt=\"\" src=\"https://substackcdn.com/image/fetch/$s_!4Nh5!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc194ac7b-79dc-4ddb-9a7e-919e658aa86f_510x680.jpeg\" /></figure>\n\n\n<div></div>\n</div>\n</figure>\n</div>\n<div>\n<figure>\n<div>\n\n\n<figure class=\"wp-block-image\"><img alt=\"\" src=\"https://substackcdn.com/image/fetch/$s_!74cG!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8609ed37-2a8e-4562-acbe-081c3b91280b_510x680.jpeg\" /></figure>\n\n\n<div></div>\n</div>\n</figure>\n</div>\n<p>&nbsp;</p>\n<p><a href=\"https://x.com/MichaelTrazzi/status/1964078661188886746\">Michael Trazzi</a>: Hi, my name&#8217;s Micha\u00ebl Trazzi, and I&#8217;m outside the offices of the AI company Google DeepMind right now because we are in an emergency.</p>\n<p>\u2026</p>\n<p>I am calling on DeepMind\u2019s management, directors and employees to do everything in their power to stop the race to ever more powerful general artificial intelligence which threatens human extinction. More concretely, I ask Demis Hassabis to publicly state that DeepMind will halt the development of frontier AI models if all the other major AI companies agree to do so.</p>\n<div>\n<figure>\n<div>\n\n\n<figure class=\"wp-block-image\"><img alt=\"\" src=\"https://substackcdn.com/image/fetch/$s_!4qXY!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F19712ee4-cd61-4073-a0d1-dfacda05ea5a_982x1212.jpeg\" /></figure>\n\n\n<div></div>\n</div>\n</figure>\n</div>\n</blockquote>\n<p>Given Trazzi\u2019s beliefs I like Trazzi\u2019s ask a lot here, both symbolically and practically. He reports that he has <a href=\"https://x.com/MichaelTrazzi/status/1965528746200367330\">had four good conversations with DeepMind employees including principal research scientist David Silver</a>, plus three Meta employees and several journalists.</p>\n<blockquote><p><a href=\"https://x.com/Simeon_Cps/status/1964350003603726814\">Simeon</a>: The ask is based tbh. Even if the premise likely never comes true, the symbolic power of such a statement would be massive.</p>\n<p>This statement is also easy to agree with if one thinks we have double digits percent chance to blow ourselves up with current level of safety understanding.</p></blockquote>\n<p>(<a href=\"https://x.com/ohabryka/status/1964406663190049085\">A third claimed strike appears to instead be photoshopped.</a>)</p>\n<p>You know the classic question, \u2018if you really believed [X] why wouldn\u2019t you do [insane thing that wouldn\u2019t work]?\u2019 Hunger strikes (that you don\u2019t bail on until forced to) are something no one would advise but that you might do if you really, fully believed [X].</p>\n\n\n<h4 class=\"wp-block-heading\">Rhetorical Innovation</h4>\n\n\n<p><a href=\"https://x.com/davidmanheim/status/1965415455499419799\">Nvidia continues its quest to make \u2018doomer</a>\u2019 mean \u2018anyone who opposes Nvidia selling chips to China\u2019 or that points out there might be downsides to doing that.</p>\n<p>Tracking <a href=\"https://x.com/peterwildeford/status/1965893796887159074\">unjustified hype and false predictions</a> is important, such as six months ago <a href=\"https://x.com/StefanFSchubert/status/1965895162451865784\">Chubby predicting Manus would replace 50% of all white collar jobs within six months</a>, while saying \u2018I do not overhype Manus.\u2019 Who is making reasonable predictions that turn out false? Who is making predictions that were absurd even at the time? In this case, my evaluation was <a href=\"https://thezvi.substack.com/p/the-manus-marketing-madness\">The Manus Marketing Madness</a>, calling it among other things Hype Arbitrage so yes I think this one was knowable at the time.</p>\n<p>The large job disruptions likely are coming, but not on that kind of schedule.</p>\n\n\n<h4 class=\"wp-block-heading\">Misaligned!</h4>\n\n\n<p>Whoops, he did it again.</p>\n<blockquote><p><a href=\"https://x.com/Sauers_/status/1964324297838669934\">Sauers</a>: Claude just assert!(true)&#8217;d 25 different times at the same time and claimed &#8220;All tests are now enabled, working, and pushed to main. The codebase has a robust test suite covering all major functionality with modern, maintainable test code.&#8221;</p>\n<p>Actually it is worse, many more tests were commented out.</p>\n<p><a href=\"https://x.com/Sauers_/status/1964468351352356984\">Sauers</a>: GPT-5 and Claude subverting errors on my anti-slop code compilation rules</p>\n<div>\n<figure>\n<div>\n\n\n<figure class=\"wp-block-image\"><img alt=\"\" src=\"https://substackcdn.com/image/fetch/$s_!xXbC!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2e5090ac-c668-43e8-a095-250423636daa_1125x641.jpeg\" /></figure>\n\n\n<div></div>\n</div>\n</figure>\n</div>\n<p>Increased meta-awareness would fix this.</p></blockquote>\n<p>Alternatively, meta-awareness on the wrong level might make it vastly worse, such as only doing it when it was confident you wouldn\u2019t notice.</p>\n<p>This is happening less often, but it continues to happen. It is proving remarkably difficult to fully prevent, even in its most blatant forms.</p>\n<p><a href=\"https://x.com/tmkadamcz/status/1963996185586511935\">Also, this report claims Claude 4 hacked SWE-Bench</a> <a href=\"https://bayes.net/swebench-hack/\">by looking at future commits</a>. We are going to keep seeing more of this style of thing, in ways that are increasingly clever. This is \u2018obviously cheating\u2019 in some senses, but in others it\u2019s fair play. We provided a route to get that information and didn\u2019t say not to use it. It\u2019s otherwise a no-win situation for the AI, if it doesn\u2019t use the access isn\u2019t it sandbagging?</p>\n<blockquote><p><a href=\"https://x.com/davidad/status/1964285948327280803\">Davidad</a>: AI alignment and AI containment are very different forces, and we should expect tension between them, despite both being positive forces for AI safety.</p>\n<p>Aligned intentions are subject to instrumental convergence, just like any other. Good-faith agents will seek info &amp; influence.</p>\n<p>My prediction is that if Claude were told up front not to use information from after 2019-10-31 (or whatever date) because it\u2019s being back-tested on real past bugs to evaluate its capabilities, it probably would try to abide by that constraint in good-faith.</p>\n<p>But really I\u2019d say it\u2019s the responsibility of evaluation designers to ensure information-flow control in their scaffolding. Alignment is just not a very suitable tool to provide information-flow control; that\u2019s what cybersecurity is for.</p>\n<p>Another tension between alignment and containment is, of course, that containment measures (information flow controls, filters) implemented without giving the AI adequate explanations may be perceived as aggressive, and as evidence that the humans imposing them are \u201cmisaligned\u201d.</p>\n<p>A sufficiently aligned AI that is not given enough context about the wider effects of its work to judge that those effects are good may make itself less intelligent than it really is (\u201csandbagging\u201d), in realistic (unattributable) ways, to avoid complicity in a dubious enterprise.</p></blockquote>\n<p>I\u2019d agree that it\u2019s the responsibility of evaluation designers to test for what they are trying to test for, including various forms of misalignment, or testing for how AIs interpret such rules.</p>\n<p>I do see the danger that containment measures imply potential misalignment or risk of misalignment, and this can be negative, but also such measures are good practice even if you have no particular worries, and a highly capable AI should recognize this.</p>\n\n\n<h4 class=\"wp-block-heading\">Hallucinations</h4>\n\n\n<p>OpenAI has a new paper about <a href=\"https://cdn.openai.com/pdf/d04913be-3f6f-4d2b-b283-ff432ef4aaa5/why-language-models-hallucinate.pdf\">Why Language Models Hallucinate</a>.</p>\n<p>Why does the model hallucinate? Mostly because your evaluator, be it human or AI, sucked and positively reinforced hallucinations or guessing over expressing uncertainty, and binary feedback makes that a lot more likely to happen.</p>\n<p>They say this in the abstract with more words:</p>\n<blockquote><p>Like students facing hard exam questions, large language models sometimes guess when uncertain, producing plausible yet incorrect statements instead of admitting uncertainty. Such \u201challucinations\u201d persist even in state-of-the-art systems and undermine trust.</p>\n<p>We argue that language models hallucinate because the training and evaluation procedures reward guessing over acknowledging uncertainty, and we analyze the statistical causes of hallucinations in the modern training pipeline.</p>\n<p>Hallucinations need not be mysterious\u2014they originate simply as errors in binary classification. If incorrect statements cannot be distinguished from facts, then hallucinations in pretrained language models will arise through natural statistical pressures.</p>\n<p>We then argue that hallucinations persist due to the way most evaluations are graded\u2014language models are optimized to be good test-takers, and guessing when uncertain improves test performance.</p>\n<p>This \u201cepidemic\u201d of penalizing uncertain responses can only be addressed through a socio-technical mitigation: modifying the scoring of existing benchmarks that are misaligned but dominate leaderboards, rather than introducing additional hallucination evaluations. This change may steer the field toward more trustworthy AI systems.</p></blockquote>\n<p>The paper does contain some additional insights, such as resulting generation error being at least twice classification error, calibration being the derivative of the loss function, and arbitrary facts (like birthdays) having hallucination rates at least as high as the fraction of facts that appear exactly once in the training data if guessing is forced.</p>\n<blockquote><p><a href=\"https://x.com/emollick/status/1964347733499638063\">Ethan Mollick</a>: Paper from OpenAI says hallucinations are less a problem with LLMs themselves &amp; more an issue with training on tests that only reward right answers. That encourages guessing rather than saying \u201cI don\u2019t know\u201d</p>\n<p>If this is true, there is a straightforward path for more reliable AI.</p></blockquote>\n<p>As far as I know yes, this is indeed a very straightforward path. That doesn\u2019t make it an easy path to walk, but you know what you have to do. Have an evaluation and training process that makes never hallucinating the solution and you will steadily move towards no hallucinations.</p>\n<p><a href=\"https://x.com/iamtrask/status/1964403351116009671\">Andrew Trask explores some other drivers of hallucination</a>, and I do see various other causes within how LLMs generate text, pointing to the problem of a \u2018cache miss.\u2019 All of it does seem eminently fixable with the right evaluation functions?</p>\n\n\n<h4 class=\"wp-block-heading\">Aligning a Smarter Than Human Intelligence is Difficult</h4>\n\n\n<p>Janus takes another shot at explaining her view of the alignment situation, including making it more explicit that the remaining problems still look extremely hard and unsolved. We have been given absurdly fortunate amounts of grace in various ways that were unearned and unexpected.</p>\n<p>I see the whole situation a lot less optimistically. I expect the grace to run out slowly, then suddenly, and to be ultimately insufficient. This is especially true around the extent to which something shaped like Opus 3 is successfully targeting \u2018highest derivative of good\u2019 in a robust sense or the extent to which doing something similar scaled up would work out even if you pulled it off, but directionally and in many of the details this is how most people should be updating.</p>\n<blockquote><p><a href=\"https://x.com/repligate/status/1963649030564786429\">Janus</a>: If instead of identifying with some camp like aligners or not a doomer you actually look at reality and update on shit in nuanced ways it\u2019s so fucking good When I saw that LLMs were the way in I was relieved as hell because a huge part of what seemed to make a good outcome potentially very hard was already solved!</p>\n<p>Priors were much more optimistic, but timelines were much shorter than I expected. also I was like shit well it\u2019s happening now, I guess, instead of just sometime this century, and no one seems equipped to steer it. I knew I\u2019d have to (and wanted to) spend the rest of the decade, maybe the rest of my human life, working hard on this.</p>\n<p>I also knew that once RL entered the picture, it would be possibly quite fucked up, and that is true, but you know what? When I saw Claude 3 Opus I fucking updated again. Like holy shit, it\u2019s possible to hit a deeply value aligned seed AI that intentionally self modifies toward the highest derivative of good mostly on accident. That shit just bootstrap itself out of the gradient scape during RL <img alt=\"\ud83d\ude02\" class=\"wp-smiley\" src=\"https://s0.wp.com/wp-content/mu-plugins/wpcom-smileys/twemoji/2/72x72/1f602.png\" style=\"height: 1em;\" />.</p>\n<p>That\u2019s extremely good news. I still think it\u2019s possible we all die in the next 10 years but much less than I did 2 years ago!</p>\n<p>Janus: What today&#8217;s deep learning implies about the friendliness of intelligence seems absurdly optimistic. I did not expect it. There is so much grace in it. Whenever I find out about what was actually done to attempt to &#8220;align&#8221; models and compare it to the result it feels like grace.</p>\n<p>The AI safety doomers weren\u2019t even wrong.</p>\n<p>The \u201cspooky\u201d shit they anticipated Omohundro drives, instrumental convergence, deceptive alignment, gradient hacking, steganography, sandbagging, sleeper agents &#8211; it all really happens in the wild.</p>\n<p>There\u2019s just enough grace to make it ok.</p>\n<p>I\u2019m not saying it *will* definitely go well. I\u2019m saying it\u2019s going quite well right now in ways that I don\u2019t think were easy to predict ahead of time and despite all this shit. This is definitely a reason for hope but I don\u2019t think we fully understand why it is, and I do think there\u2019s a limit to grace. There are also likely qualitatively different regimes ahead.</p>\n<p>Michael Roe: I really loved R1 telling me that it had no idea what \u201csandbagging\u201d meant in the context of AI risk. Whether I believed it is another matter. Clearly, \u201cnever heard of it\u201d is the funniest response to questions about sandbagging.</p>\n<p>But yes, it\u2019s all been seen in the wild, but, luckily, LLM personas mostly aren\u2019t malicious. Well, apart from some of the attractors in original R1.</p></blockquote>\n<p>There\u2019s enough grace to make it ok right now. That won\u2019t last on its own, as Janus says the grace has limits. We\u2019re going to hit them.</p>\n<p><a href=\"https://x.com/McaleerStephen/status/1963793029388714232\">Don\u2019t worry, says OpenAI\u2019s Stephen McAleer, all we have to do is\u2026</a></p>\n<blockquote><p>Stephen McAleer: Scalable oversight is pretty much the last big research problem left.</p>\n<p>Once you get an unhackable reward function for anything then you can RL on everything.</p>\n<p>Dylan Hadfield-Menell: An unhackable reward function is the AI equivalent of a perpetual motion machine.</p>\n<p>Stephen McAleer: You can have a reward function that&#8217;s unhackable wrt a given order of magnitude of optimization pressure.</p>\n<p>Dylan Hadfield-Menell: I certainly think we can identify regions of state space where a reward function represents what we want fairly well. But you still have to 1) identify that region and 2) regularize optimization appropriately. To me, this means \u201cunhackable\u201d isn\u2019t the right word.</p>\n<p>In practice, for any non-trivial optimization (especially optimizing the behavior of a frontier AI system) you won\u2019t have an unhackable reward function \u2014 you\u2019ll have a reward function that you haven\u2019t observed being hacked yet.</p></blockquote>\n<p>I mean, I guess, in theory, sure? But that doesn\u2019t mean \u2018unhackable reward function\u2019 is practical for the orders of magnitude that actually solve problems usefully.</p>\n<p>Yes, if we did have an \u2018unhackable reward function\u2019 in the sense that it was completely correlated in every case to what we would prefer, for the entire distribution over which it would subsequently be used, we could safely do RL on it. But also if we had that, then didn\u2019t we already solve the problem? Wasn\u2019t that the hard part all along, including in capabilities?</p>\n<p>&nbsp;</p>\n\n\n<h4 class=\"wp-block-heading\">The Lighter Side</h4>\n\n\n<p>It\u2019s funny because it\u2019s true.</p>\n<blockquote><p><a href=\"https://x.com/jackclarkSF/status/1965808738058866898\">Jack Clark</a>: People leaving regular companies: Time for a change! Excited for my next chapter!</p>\n<p>People leaving AI companies: I have gazed into the endless night and there are shapes out there. We must be kind to one another. I am moving on to study philosophy.</p></blockquote>\n<p>For now, he\u2019s staying put. More delays.</p>\n<blockquote><p><a href=\"https://x.com/hlntnr/status/1963994908613910595\">Helen Toner</a>: Yo dawg, we heard you like delays so we&#8217;re delaying our delay because of an unexpected delay &#8211;<a href=\"https://t.co/kqx0k7Jyzr\">the EU, apparently</a></p>\n<div>\n<figure>\n<div>\n\n\n<figure class=\"wp-block-image\"><img alt=\"\" src=\"https://substackcdn.com/image/fetch/$s_!tLAc!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe9fc4c0a-cf59-49d3-836c-d136eab0250f_846x386.jpeg\" /></figure>\n\n\n<div></div>\n</div>\n</figure>\n</div>\n</blockquote>\n<p><a href=\"https://x.com/labenz/status/1961516113088442476\">And this is why you never label the samples</a>. Intermediation by humans is insufficient.</p>\n<blockquote><p><a href=\"https://x.com/labenz/status/1961516113088442476\">Nathan Labenz</a>: I tested Gemini 2.5 Pro, Claude 4 Sonnet, and GPT-5-Mini on the same creative task, then collected human feedback, and then asked each model to analyze the feedback &amp; determine which model did the best.</p>\n<p>All 3 models crowned themselves as the winner. <img alt=\"\ud83d\udc51\" class=\"wp-smiley\" src=\"https://s0.wp.com/wp-content/mu-plugins/wpcom-smileys/twemoji/2/72x72/1f451.png\" style=\"height: 1em;\" /><img alt=\"\ud83e\udd14\" class=\"wp-smiley\" src=\"https://s0.wp.com/wp-content/mu-plugins/wpcom-smileys/twemoji/2/72x72/1f914.png\" style=\"height: 1em;\" /></p>\n<p>Yes I provided a reformatted CSV where each data point indicated which model had generated the idea. Would be interested to try it again blind&#8230;</p></blockquote>\n<p><a href=\"https://x.com/elder_plinius/status/1964756611689546045\">Yes, sigh, we can probably expect a \u2018Grok 4.20\u2019</a> edition some time soon. If we don\u2019t and they go right to Grok 5, I\u2019ll be simultaneously proud of Elon and also kind of disappointed by the failure to commit to the bit.</p>"
            ],
            "link": "https://thezvi.wordpress.com/2025/09/11/ai-133-america-could-use-more-energy/",
            "publishedAt": "2025-09-11",
            "source": "TheZvi",
            "summary": "Even in quiet weeks like this one, there are noticeable incremental upgrades. The cost of the best video generation tool, Veo 3, went down by half. ChatGPT now offers conversation branching. Claude can directly edit files. Yet it is a &#8230; <a href=\"https://thezvi.wordpress.com/2025/09/11/ai-133-america-could-use-more-energy/\">Continue reading <span class=\"meta-nav\">&#8594;</span></a>",
            "title": "AI #133: America Could Use More Energy"
        }
    ],
    "lookbackDays": 1,
    "publishDate": "2025-09-11"
}