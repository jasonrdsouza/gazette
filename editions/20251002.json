{
    "articles": [
        {
            "content": [
                "<p>Say an alien spaceship is headed for Earth. It has 30 aliens on it. The aliens are weak and small. They have no weapons and carry no diseases. They breed at rates similar to humans. They are bringing no new technology. No other ships are coming. There\u2019s no trick\u2014except that they each have an IQ of 300. Would you find that concerning?</p>\n\n<p>Of course, the aliens might be great. They might cure cancer and help us reach world peace and higher consciousness. But would you be <em>sure</em> they\u2019d be great?</p>\n\n<p>Suppose you were worried about the aliens but I scoffed, \u201cTell me <em>specifically</em> how the aliens would hurt us. They\u2019re small and weak! They can\u2019t do anything unless we let them.\u201d Would you find that counter-argument convincing?</p>\n\n<p>I claim that most people <em>would</em> be concerned about the arrival of the aliens, <em>would not</em> be sure that their arrival would be good, and <em>would not</em> find that counter-argument convincing.</p>\n\n<p>I bring this up because most AI-risk arguments I see go something like this:</p>\n\n<ol>\n  <li>There will be a <a href=\"https://www.lesswrong.com/w/ai-takeoff\">fast takeoff</a> in AI capabilities.</li>\n  <li>Due to <a href=\"https://www.lesswrong.com/w/ai-alignment\">alignment difficulty</a> and <a href=\"https://www.lesswrong.com/w/orthogonality-thesis\">orthogonality</a>, it will pursue dangerous <a href=\"https://www.lesswrong.com/w/instrumental-convergence\">convergent subgoals</a>.</li>\n  <li>These will give the AI a <a href=\"https://www.lesswrong.com/posts/vkjWGJrFWBnzHtxrw/superintelligence-7-decisive-strategic-advantage\">decisive strategic advantage</a>, making it <a href=\"https://www.lesswrong.com/w/cognitive-uncontainability\">uncontainable</a> and resulting in catastrophe.</li>\n</ol>\n\n<p>These arguments have always struck me as overcomplicated. So I\u2019d like to submit the following undercomplicated alternative:</p>\n\n<ol>\n  <li><em>Obviously</em>, if an alien race with IQs of 300 were going to arrive on Earth soon, that would be concerning.</li>\n  <li>In the next few decades, it\u2019s entirely possible that AI with an IQ of 300 will arrive. Really, that might actually happen.</li>\n  <li>No one knows what AI with an IQ of 300 would be like. So it might as well be an alien.</li>\n</ol>\n\n<p>Our subject for today is: Why might one prefer one of these arguments to the other?</p>\n\n<h2 id=\"the-case-for-the-simple-argument\">The case for the simple argument</h2>\n\n<p>The obvious reason to prefer the simple argument is that it\u2019s more likely to be true. The complex argument has a lot of steps. Personally, I think they\u2019re all individually plausible. But are we really confident that there will be a fast takeoff in AI capabilities <em>and</em> that the AI will pursue dangerous subgoals <em>and</em> that it will thereby gain a decisive strategic advantage?</p>\n\n<p>I find that confidence unreasonable. I\u2019ve often been puzzled why so many seemingly-reasonable people will discuss these arguments without rejecting the confidence.</p>\n\n<p>I think the explanation is that there are implicitly two versions of the complex argument. The \u201cstrong\u201d version claims that  fast takeoff et al. <em>will</em> happen, while the \u201cweak\u201d version merely claims that it\u2019s a plausible scenario that we should take seriously. It\u2019s often hard to tell which version people are endorsing.</p>\n\n<p>The distinction is crucial, because these two version have different weaknesses. I find the strong version wildly overconfident. I agree with the weak version, but I still think it\u2019s unsatisfying.</p>\n\n<p>Say you think there\u2019s a &gt;50% chance things do not go as suggested by the complex argument. Maybe there\u2019s a slow takeoff, or maybe the AI can\u2019t build a decisive strategic advantage, whatever. Now what?</p>\n\n<p>Well, maybe everything turns out great and you live for millions of years, exploring the galaxy, reading poetry, meditating, and eating pie. That would be nice. But it also seems possible that humanity still ends up screwed, just in a different way. The complex argument doesn\u2019t speak to what happens when one of the steps fails. This might give the impression that without any of the steps, everything is fine. But that is not the case.</p>\n\n<p>The simple argument is also more convincing. Partly I think that\u2019s because\u2014well\u2014it\u2019s easier to convince people of things when they\u2019re true. But beyond that, the simple argument doesn\u2019t require any new concepts or abstractions, and it leverages our existing intuitions for how more intelligent entities can be dangerous in unexpected ways.</p>\n\n<p>I actually prefer the simple argument in an inverted form: If you claim that there is no AI-risk, then which of the following bullets do you want to bite?</p>\n\n<ol>\n  <li>\u201cIf a race of aliens with an IQ of 300 came to Earth, that would definitely be fine.\u201d</li>\n  <li>\u201cThere\u2019s no way that AI with an IQ of 300 will arrive within the next few decades.\u201d</li>\n  <li>\u201cWe know some special property that AI will definitely have that will definitely prevent all possible bad outcomes that aliens might cause.\u201d</li>\n</ol>\n\n<p>I think all those bullets are unbiteable. Hence, I think AI-risk is real.</p>\n\n<p>But if you make the complex argument, then you seem to be left with the burden of arguing for fast takeoff and alignment difficulty and so on. People who hear that argument also often demand an explanation of just how AI could hurt people (\u201cNanotechnology? Bioweapons? What kind of bioweapon?\u201d) I think this is a mistake for the same reason it would be a mistake to demand to know how a car accident would happen before putting on your seatbelt. As long as the Complex Scenario is possible, it\u2019s a risk we need to manage. But many people don\u2019t look at things that way.</p>\n\n<p>But I think the biggest advantage of the simple argument is something else: It reveals the crux of disagreement.</p>\n\n<p>I\u2019ve talked to many people who find the complex argument completely implausible. Since I think it <em>is</em> plausible\u2014just not a sure thing\u2014I often ask why. People give widely varying reasons. Some claim that alignment will be easy, some that AI will never <em>really</em> be an \u201cagent\u201d, some talk about the dangers of evolved vs. engineered systems, and some have technical arguments based on NP-hardness or the nature of consciousness.</p>\n\n<p>I\u2019ve never made much progress convincing these people to change their minds. I <em>have</em> succeeded in convincing some people that certain arguments don\u2019t work. (For example, I\u2019ve convinced people that NP-hardness and the nature of consciousness are probably irrelevant.) But when people abandon those arguments, they don\u2019t turn around and accept the whole Scenario as plausible. They just switch to different objections.</p>\n\n<p>So I started giving my simple argument instead. When I did this, here\u2019s what I discovered: None of these people actually accept that AI with an IQ of 300 could happen.</p>\n\n<p>Sure, they often <em>say</em> that they accept this. But if you pin them down, they\u2019re inevitably picturing an AI that lacks some core human capability. Often, the AI can prove theorems or answer questions, but it\u2019s not an \u201cagent\u201d that wants things and does stuff and has relationships and makes long-term plans.</p>\n\n<p>So I conjecture that this is the crux of the issue with AI-risk. People who <em>truly</em> accept that AI with an IQ of 300 and all human capabilities may appear are almost always at least somewhat worried about AI-risk. And people who are not worried about AI-risk almost always don\u2019t truly accept that AI with an IQ of 300 could appear. If that\u2019s the crux, then we should get to it as quickly as possible. And that\u2019s done by the simple argument.</p>\n\n<h2 id=\"the-case-for-the-complex-argument\">The case for the complex argument</h2>\n\n<p>I won\u2019t claim to be neutral. As hinted by the title, I started writing this post intending to make the case for the simple argument, and I still think that case is strong. But I figured I should consider arguments for the other side and\u2014there are some good ones.</p>\n\n<p>Above, I suggested that there are two versions of the complex argument: A \u201cstrong\u201d version that claims the scenario it lays out will definitely happen, and a \u201cweak\u201d version that merely claims it\u2019s plausible. I rejected the strong version as overconfident. And I rejected the weak version because there are lots of other scenarios where things could also go wrong for humanity, so why give this one so much focus?</p>\n\n<p>Well, there\u2019s also a middle version of the complex argument: You could claim that the scenario it lays out is not certain, but that <em>if</em> things go wrong for humanity, then they will probably go wrong as in that scenario. This avoids both of my objections\u2014it\u2019s less overconfident, and it gives a good reason to focus on this particular scenario.</p>\n\n<p>Personally, I don\u2019t buy it, because I think other bad scenarios like <a href=\"https://gradual-disempowerment.ai/\">gradual disempowerment</a> are plausible. But maybe I\u2019m wrong. It doesn\u2019t seem crazy to claim that the Complex Scenario captures most of the probability mass of bad outcomes. And if that\u2019s true, I want to know it.</p>\n\n<p>Now, some people suggest favoring certain arguments for the sake of optics: Even if you accept the complex argument, maybe you\u2019d want to make the simple one because it\u2019s more convincing or is better optics for the AI-risk community. (\u201cWe don\u2019t want to look like crazy people.\u201d)</p>\n\n<p>Personally, I am allergic to that whole category of argument. I have a strong presumption that you should argue the thing you actually believe, not some watered-down thing you invented because you think it will manipulate people into believing what you want them to believe. So even if my simpler argument is more convincing, so what?</p>\n\n<p>But say you accept the middle version of the complex argument, yet you think my simple argument is more convincing. And say you\u2019re not as bloody-minded as me, so you want to calibrate your messaging to be more effective. Should you use my simple argument? I\u2019m not sure you should.</p>\n\n<p>The typical human bias is to think other people are similar to us. (How many people favor mandatory pet insurance funded by a land-value tax? At least 80%, right?) But as far as I can tell, the situation with AI-risk is the opposite. Most people I know are at least mildly concerned, but have the impression that \u201cnormal people\u201d think that AI-risk is science fiction nonsense.</p>\n\n<p>Yet, here are some recent polls:</p>\n\n<table>\n  <thead>\n    <tr>\n      <th>Poll</th>\n      <th>Date</th>\n      <th>Statement</th>\n      <th>Agree</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td><a href=\"https://news.gallup.com/poll/692435/major-threat-next-tech-thing.aspx\">Gallup</a></td>\n      <td>June 2-15 2025</td>\n      <td>[AI is] very different from the technological advancements that came before, and threatens to harm humans and society</td>\n      <td>49%</td>\n    </tr>\n    <tr>\n      <td><a href=\"https://www.reuters.com/world/us/americans-fear-ai-permanently-displacing-workers-reutersipsos-poll-finds-2025-08-19/\">Reuters / Ipsos</a></td>\n      <td>August 13-18 2025</td>\n      <td>AI could risk the future of humankind</td>\n      <td>58%</td>\n    </tr>\n    <tr>\n      <td><a href=\"https://today.yougov.com/politics/articles/52615-americans-increasingly-likely-say-ai-artificial-intelligence-negatively-affect-society-poll\">YouGov</a></td>\n      <td>March 5-7 2025</td>\n      <td>How concerned, if at all, are you about the possibility that artificial intelligence (AI) will cause the end of the human race on Earth? (Very or somewhat concerned)</td>\n      <td>37%</td>\n    </tr>\n    <tr>\n      <td><a href=\"https://today.yougov.com/politics/articles/52615-americans-increasingly-likely-say-ai-artificial-intelligence-negatively-affect-society-poll\">YouGov</a></td>\n      <td>June 27-30 2025</td>\n      <td>How concerned, if at all, are you about the possibility that artificial intelligence (AI) will cause the end of the human race on Earth? (Very or somewhat concerned)</td>\n      <td>43%</td>\n    </tr>\n  </tbody>\n</table>\n\n<p>Being concerned about AI is hardly a fringe position. People are <em>already</em> worried, and becoming more so.</p>\n\n<p>I used to picture my simple argument as a sensible middle-ground, arguing for taking AI-risk seriously, but not overconfident:</p>\n\n<p><img alt=\"spectrum1\" src=\"https://dynomight.net/img/ai-risk/spectrum1.png\" /></p>\n\n<p>But I\u2019m starting to wonder if my \u201cobvious argument\u201d <em>is in fact obvious</em>, and something that people can figure out own their own. From looking at the polling data, it seems like the actual situation is more like this, with people on the left gradually wandering towards the middle:</p>\n\n<p><img alt=\"spectrum2\" src=\"https://dynomight.net/img/ai-risk/spectrum2.png\" /></p>\n\n<p>If anything, the optics may favor a confident argument over my simple argument. In principle, they suggest similar actions: Move quickly to reduce existential risk. But what I actually see is that most people\u2014even people working on AI\u2014feel powerless and are just sort of clenching up and hoping for the best.</p>\n\n<p>I don\u2019t think you should advocate for something you don\u2019t believe. But <em>if</em> you buy the complex argument, and you\u2019re holding yourself back for the sake of optics, I don\u2019t really see the point.</p>"
            ],
            "link": "https://dynomight.net/ai-risk/",
            "publishedAt": "2025-10-02",
            "source": "Dynomight",
            "summary": "<p>Say an alien spaceship is headed for Earth. It has 30 aliens on it. The aliens are weak and small. They have no weapons and carry no diseases. They breed at rates similar to humans. They are bringing no new technology. No other ships are coming. There\u2019s no trick\u2014except that they each have an IQ of 300. Would you find that concerning?</p> <p>Of course, the aliens might be great. They might cure cancer and help us reach world peace and higher consciousness. But would you be <em>sure</em> they\u2019d be great?</p> <p>Suppose you were worried about the aliens but I scoffed, \u201cTell me <em>specifically</em> how the aliens would hurt us. They\u2019re small and weak! They can\u2019t do anything unless we let them.\u201d Would you find that counter-argument convincing?</p> <p>I claim that most people <em>would</em> be concerned about the arrival of the aliens, <em>would not</em> be sure that their arrival would be good, and <em>would not</em> find that counter-argument convincing.</p> <p>I bring this up because most AI-risk arguments I see go something like this:</p> <ol> <li>There will be a <a href=\"https://www.lesswrong.com/w/ai-takeoff\">fast takeoff</a> in AI capabilities.</li> <li>Due to <a href=\"https://www.lesswrong.com/w/ai-alignment\">alignment difficulty</a> and <a href=\"https://www.lesswrong.com/w/orthogonality-thesis\">orthogonality</a>, it will pursue dangerous <a href=\"https://www.lesswrong.com/w/instrumental-convergence\">convergent subgoals</a>.</li> <li>These will",
            "title": "Y\u2019all are over-complicating these AI-risk arguments"
        },
        {
            "content": [],
            "link": "https://anyblockers.com/posts/working-with-coding-agents/",
            "publishedAt": "2025-10-02",
            "source": "Eric Zakariasson",
            "summary": "What I have observed working well with AI coding agents during 2025 and practical tips to get the most out of them",
            "title": "Working with coding agents"
        },
        {
            "content": [
                "<div class=\"lead\"><p><strong class=\"font-semibold text-navy-950\">I\u2019m Ben Johnson, and I work on Litestream at Fly.io. Litestream makes it easy to build SQLite-backed full-stack applications  with resilience to server failure. It\u2019s open source, runs anywhere, and</strong> <a href=\"https://litestream.io/\" title=\"\"><strong class=\"font-semibold text-navy-950\">it\u2019s easy to get started</strong></a><strong class=\"font-semibold text-navy-950\">.</strong></p>\n</div>\n<p>Litestream is the missing backup/restore system for SQLite. It runs as a sidecar process in the background, alongside unmodified SQLite applications, intercepting WAL checkpoints and streaming them to object storage in real time. Your application doesn&rsquo;t even know it&rsquo;s there. But if your server crashes, Litestream lets you quickly restore the database to your new hardware.</p>\n\n<p>The result: you can safely build whole full-stack applications on top of SQLite.</p>\n\n<p>A few months back, we announced <a href=\"https://fly.io/blog/litestream-revamped/\" title=\"\">plans for a major update to Litestream</a>. I&rsquo;m psyched to announce that the first batch of those changes are now &ldquo;shipping&rdquo;. Litestream is  faster and now supports efficient point-in-time recovery (PITR).</p>\n\n<p>I&rsquo;m going to take a beat to recap Litestream and how we got here, then talk about how these changes work and what you can expect to see with them.</p>\n<h2 class=\"group flex items-start whitespace-pre-wrap relative mt-14 sm:mt-16 mb-4 text-navy-950 font-heading\" id=\"litestream-to-litefs-to-litestream\"><a class=\"inline-block align-text-top relative top-[.15em] w-6 h-6 -ml-6 after:hash opacity-0 group-hover:opacity-100 transition-all\" href=\"https://fly.io/blog/feed.xml#litestream-to-litefs-to-litestream\"></a><span class=\"plain-code\">Litestream to LiteFS to Litestream</span></h2>\n<p>Litestream is one of two big SQLite things I&rsquo;ve built. The other one, originally intended as a sort of sequel to Litestream, is LiteFS.</p>\n\n<p>Boiled down to a sentence: LiteFS uses a FUSE filesystem to crawl further up into SQLite&rsquo;s innards, using that access to perform live replication, for unmodified SQLite-backed apps.</p>\n\n<p>The big deal about LiteFS for us is that it lets you do the multiregion primary/read-replica deployment people love Postgres for: reads are fast everywhere, and writes are sane and predictable. We were excited to make this possible for SQLite, too.</p>\n\n<p>But the market has spoken! Users prefer Litestream. And honestly, we get it: Litestream is easier to run and to reason about. So we&rsquo;ve shifted our focus back to it. First order of business: <a href=\"https://fly.io/blog/litestream-revamped/\" title=\"\">take what we learned building LiteFS and stick as much of it as we can back into Litestream</a>.</p>\n<h2 class=\"group flex items-start whitespace-pre-wrap relative mt-14 sm:mt-16 mb-4 text-navy-950 font-heading\" id=\"the-ltx-file-format\"><a class=\"inline-block align-text-top relative top-[.15em] w-6 h-6 -ml-6 after:hash opacity-0 group-hover:opacity-100 transition-all\" href=\"https://fly.io/blog/feed.xml#the-ltx-file-format\"></a><span class=\"plain-code\">The LTX File Format</span></h2>\n<p>Consider this basic SQL table:</p>\n<div class=\"highlight-wrapper group relative sql\">\n  <button class=\"bubble-wrap z-20 absolute right-9 -mr-0.5 top-1.5 text-transparent group-hover:text-gray-400 group-hover:hocus:text-white focus:text-white bg-transparent group-hover:bg-gray-900 group-hover:hocus:bg-gray-700 focus:bg-gray-700 transition-colors grid place-items-center w-7 h-7 rounded-lg outline-none focus:outline-none\" type=\"button\">\n    <svg class=\"w-4 h-4 pointer-events-none\" fill=\"none\" stroke=\"currentColor\" stroke-linecap=\"round\" stroke-linejoin=\"round\" stroke-width=\"1.35\" viewBox=\"0 0 16 16\" xmlns=\"http://www.w3.org/2000/svg\"><g><path d=\"M9.912 8.037h2.732c1.277 0 2.315-.962 2.315-2.237a2.325 2.325 0 00-2.315-2.31H2.959m10.228 9.01H2.959M6.802 8H2.959\"><path d=\"M11.081 6.466L9.533 8.037l1.548 1.571\"></g></svg>\n    <span class=\"bubble-sm bubble-tl [--offset-l:-9px] tail text-navy-950\">\n      Wrap text\n    </span>\n  </button>\n  <button class=\"bubble-wrap z-20 absolute right-1.5 top-1.5 text-transparent group-hover:text-gray-400 group-hover:hocus:text-white focus:text-white bg-transparent group-hover:bg-gray-900 group-hover:hocus:bg-gray-700 focus:bg-gray-700 transition-colors grid place-items-center w-7 h-7 rounded-lg outline-none focus:outline-none\" type=\"button\">\n    <svg class=\"w-4 h-4 pointer-events-none\" fill=\"none\" stroke=\"currentColor\" stroke-width=\"1.35\" viewBox=\"0 0 16 16\" xmlns=\"http://www.w3.org/2000/svg\"><g><path d=\"M10.576 7.239c0-.995-.82-1.815-1.815-1.815H3.315c-.995 0-1.815.82-1.815 1.815v5.446c0 .995.82 1.815 1.815 1.815h5.446c.995 0 1.815-.82 1.815-1.815V7.239z\"><path d=\"M10.576 10.577h2.109A1.825 1.825 0 0014.5 8.761V3.315A1.826 1.826 0 0012.685 1.5H7.239c-.996 0-1.815.819-1.816 1.815v1.617\"></g></svg>\n    <span class=\"bubble-sm bubble-tl [--offset-l:-6px] tail [--tail-x:calc(100%-30px)] text-navy-950\">\n      Copy to clipboard\n    </span>\n  </button>\n  <div class=\"highlight relative group\">\n    <pre class=\"highlight \"><code id=\"code-2kzvaza2\"><span class=\"k\">CREATE</span> <span class=\"k\">TABLE</span> <span class=\"n\">sandwiches</span> <span class=\"p\">(</span>\n    <span class=\"n\">id</span> <span class=\"nb\">INTEGER</span> <span class=\"k\">PRIMARY</span> <span class=\"k\">KEY</span> <span class=\"n\">AUTOINCREMENT</span><span class=\"p\">,</span>\n    <span class=\"n\">description</span> <span class=\"nb\">TEXT</span> <span class=\"k\">NOT</span> <span class=\"k\">NULL</span><span class=\"p\">,</span>\n    <span class=\"n\">star_rating</span> <span class=\"nb\">INTEGER</span><span class=\"p\">,</span> \n    <span class=\"n\">reviewer_id</span> <span class=\"nb\">INTEGER</span> <span class=\"k\">NOT</span> <span class=\"k\">NULL</span>\n<span class=\"p\">);</span>\n</code></pre>\n  </div>\n</div>\n<p>In our hypothetical, this table backs a wildly popular sandwich-reviewing app that we keep trying to get someone to write. People eat a lot of sandwiches and this table gets a lot of writes. Because it makes my point even better and it&rsquo;s funny, assume people dither a lot about their sandwich review for the first couple minutes after they leave it. This Quiznos sub\u2026 is it \u2b50 or \u2b50\u2b50?</p>\n\n<p>Underneath SQLite is a B-tree. Like databases everywhere, SQLite divides storage up into disk-aligned pages, working hard to read as few pages as possible for any task while treating work done within a page as more or less free. SQLite always reads and writes in page-sized chunks.</p>\n\n<p>Our <code>sandwiches</code> table includes a feature that&rsquo;s really painful for a tool like Litestream that thinks in pages: an automatically updating primary key. That key dictates that every insert into the table hits the rightmost leaf page in the underlying table B-tree. For SQLite itself, that&rsquo;s no problem. But Litestream has less information to go on: it sees only a feed of whole pages it needs to archive.</p>\n\n<p>Worse still, when it comes time to restore the database \u2013 something you tend to want to happen quickly \u2013 you have to individually apply those small changes, as whole pages. Your app is down, PagerDuty is freaking out, and you&rsquo;re sitting there watching Litestream reconstruct your Quiznos uncertainty a page (and an S3 fetch) at a time.</p>\n\n<p>So, LTX. Let me explain. We needed LiteFS to be transaction-aware. It relies on finer-grained information than just raw dirty pages (that&rsquo;s why it needs the FUSE filesystem). To ship transactions, rather than pages, we invented a <a href=\"https://github.com/superfly/ltx\" title=\"\">file format we call LTX</a>.</p>\n\n<p>LTX was designed as an interchange format for transactions, but for our purposes in Litestream, all we care about is that LTX files represent ordered ranges of pages, and that it supports compaction.</p>\n\n<p>Compaction is straightforward. You&rsquo;ve stored a bunch of LTX files that collect numbered pages. Now you want to to restore a coherent picture of the database. Just replay them newest to oldest, skipping duplicate pages (newer wins), until all changed pages are accounted for.</p>\n\n<p>Importantly, LTX isn&rsquo;t limited to whole database backups. We can use LTX compaction to compress a bunch of LTX files into a single file with no duplicated pages. And Litestream now uses this capability to create a hierarchy of compactions:</p>\n\n<ul>\n<li>at Level 1, we compact all the changes in a 30-second time window\n</li><li>at Level 2, all the Level 1 files in a 5-minute window\n</li><li>at Level 3, all the Level 2&rsquo;s over an hour.\n</li></ul>\n\n<p>Net result: we can restore a SQLite database to any point in time, <em>using only a dozen or so files on average</em>.</p>\n\n<p>Litestream performs this compaction itself. It doesn&rsquo;t rely on SQLite to process the WAL file. Performance is limited only by I/O throughput.</p>\n<h2 class=\"group flex items-start whitespace-pre-wrap relative mt-14 sm:mt-16 mb-4 text-navy-950 font-heading\" id=\"no-more-generations\"><a class=\"inline-block align-text-top relative top-[.15em] w-6 h-6 -ml-6 after:hash opacity-0 group-hover:opacity-100 transition-all\" href=\"https://fly.io/blog/feed.xml#no-more-generations\"></a><span class=\"plain-code\">No More Generations</span></h2>\n<p>What people like about Litestream is that it&rsquo;s just an ordinary Unix program. But like any Unix program, Litestream can crash. It&rsquo;s not supernatural, so when it&rsquo;s not running, it&rsquo;s not seeing database pages change. When it misses changes, it falls out of sync with the database.</p>\n\n<p>Lucky for us, that&rsquo;s easy to detect. When it notices a gap between the database and our running &ldquo;shadow-WAL&rdquo; backup, Litestream resynchronizes from scratch.</p>\n\n<p>The only time this gets complicated is if you have multiple Litestreams backing up to the same destination. To keep multiple Litestreams from stepping on each other, Litestream divides backups into &ldquo;generations&rdquo;, creating a new one any time it resyncs. You can think of generations as Marvel Cinematic Universe parallel dimensions in which your database might be simultaneously living in.</p>\n\n<p>Yeah, we didn&rsquo;t like those movies much either.</p>\n\n<p>LTX-backed Litestream does away with the concept entirely. Instead, when we detect a break in WAL file continuity, we re-snapshot with the next LTX file. Now we have a monotonically incrementing transaction ID. We can use it look up database state at any point in time, without searching across generations.</p>\n<h2 class=\"group flex items-start whitespace-pre-wrap relative mt-14 sm:mt-16 mb-4 text-navy-950 font-heading\" id=\"upgrading-to-litestream-v0-5-0\"><a class=\"inline-block align-text-top relative top-[.15em] w-6 h-6 -ml-6 after:hash opacity-0 group-hover:opacity-100 transition-all\" href=\"https://fly.io/blog/feed.xml#upgrading-to-litestream-v0-5-0\"></a><span class=\"plain-code\">Upgrading to Litestream v0.5.0</span></h2>\n<p>Due to the file format changes, the new version of Litestream can&rsquo;t restore from old v0.3.x WAL segment files.</p>\n\n<p>That&rsquo;s OK though! The upgrade process is simple: just start using the new version. It&rsquo;ll leave your old WAL files intact, in case you ever need to revert to the older version.The new LTX files are stored cleanly in an <code>ltx</code> directory on your replica.</p>\n\n<p>The configuration file is fully backwards compatible.</p>\n\n<p>There&rsquo;s one small catch. We added a new constraint. You only get a single replica destination per database. This probably won&rsquo;t affect you, since it&rsquo;s how most people use Litestream already. We&rsquo;ve made it official.</p>\n\n<p>The rationale: having a single source of truth simplifies development for us, and makes the tool easier to reason about. Multiple replicas can diverge and are sensitive to network availability. Conflict resolution is brain surgery.</p>\n\n<p>Litestream commands still work the same. But you&rsquo;ll see references to &ldquo;transaction IDs&rdquo; (TXID) for LTX files, rather than the <code>generation/index/offset</code> we used previously with WAL segments.</p>\n\n<p>We&rsquo;ve also changed <code>litestream wal</code> to <code>litestream ltx</code>.</p>\n<h2 class=\"group flex items-start whitespace-pre-wrap relative mt-14 sm:mt-16 mb-4 text-navy-950 font-heading\" id=\"other-stuff-v0-5-0-does-better\"><a class=\"inline-block align-text-top relative top-[.15em] w-6 h-6 -ml-6 after:hash opacity-0 group-hover:opacity-100 transition-all\" href=\"https://fly.io/blog/feed.xml#other-stuff-v0-5-0-does-better\"></a><span class=\"plain-code\">Other Stuff v0.5.0 Does Better</span></h2>\n<p>We&rsquo;ve beefed up the <a href=\"https://github.com/superfly/ltx\" title=\"\">underlying LTX file format library</a>. It used to be an LTX file was just a sorted list of pages, all compressed together. Now we compress per-page, and keep an index at the end of the LTX file to pluck individual pages out.</p>\n\n<p>You&rsquo;re not seeing it yet, but we&rsquo;re excited about this change: we can operate page-granularly even dealing with large LTX files. This allows for more features. A good example: we can build features that query from any point in time, without downloading the whole database.</p>\n\n<p>We&rsquo;ve also gone back through old issues &amp; PRs to improve quality-of-life. CGO is now gone. We&rsquo;ve settled the age-old contest between <code>mattn/go-sqlite3</code> and <code>modernc.org/sqlite</code> in favor of <code>modernc.org</code>. This is super handy for people with automated build systems that want to run from a MacBook but deploy on an x64 server, since it lets the cross-compiler work.</p>\n\n<p>We&rsquo;ve also added a replica type for NATS JetStream. Users that already have JetStream running can get Litestream going without adding an object storage dependency.</p>\n\n<p>And finally, we&rsquo;ve upgraded all our clients (S3, Google Storage, &amp; Azure Blob Storage) to their latest versions. We&rsquo;ve also moved our code to support newer S3 APIs.</p>\n<h2 class=\"group flex items-start whitespace-pre-wrap relative mt-14 sm:mt-16 mb-4 text-navy-950 font-heading\" id=\"whats-next\"><a class=\"inline-block align-text-top relative top-[.15em] w-6 h-6 -ml-6 after:hash opacity-0 group-hover:opacity-100 transition-all\" href=\"https://fly.io/blog/feed.xml#whats-next\"></a><span class=\"plain-code\">What&rsquo;s next?</span></h2>\n<p>The next major feature we&rsquo;re building out is a Litestream VFS for read replicas. This will let you instantly spin up a copy of the database and immediately read pages from S3 while the rest of the database is hydrating in the background.</p>\n\n<p>We already have a proof of concept working and we&rsquo;re excited to show it off when it&rsquo;s ready!</p>"
            ],
            "link": "https://fly.io/blog/litestream-v050-is-here/",
            "publishedAt": "2025-10-02",
            "source": "Fly.io Blog",
            "summary": "<div class=\"lead\"><p><strong class=\"font-semibold text-navy-950\">I\u2019m Ben Johnson, and I work on Litestream at Fly.io. Litestream makes it easy to build SQLite-backed full-stack applications with resilience to server failure. It\u2019s open source, runs anywhere, and</strong> <a href=\"https://litestream.io/\" title=\"\"><strong class=\"font-semibold text-navy-950\">it\u2019s easy to get started</strong></a><strong class=\"font-semibold text-navy-950\">.</strong></p> </div> <p>Litestream is the missing backup/restore system for SQLite. It runs as a sidecar process in the background, alongside unmodified SQLite applications, intercepting WAL checkpoints and streaming them to object storage in real time. Your application doesn&rsquo;t even know it&rsquo;s there. But if your server crashes, Litestream lets you quickly restore the database to your new hardware.</p> <p>The result: you can safely build whole full-stack applications on top of SQLite.</p> <p>A few months back, we announced <a href=\"https://fly.io/blog/litestream-revamped/\" title=\"\">plans for a major update to Litestream</a>. I&rsquo;m psyched to announce that the first batch of those changes are now &ldquo;shipping&rdquo;. Litestream is faster and now supports efficient point-in-time recovery (PITR).</p> <p>I&rsquo;m going to take a beat to recap Litestream and how we got here, then talk about how these changes work and what you can expect to see with them.</p> <h2 class=\"group flex items-start whitespace-pre-wrap relative mt-14 sm:mt-16 mb-4 text-navy-950 font-heading\" id=\"litestream-to-litefs-to-litestream\"><a class=\"inline-block align-text-top relative top-[.15em] w-6",
            "title": "Litestream v0.5.0 is Here"
        },
        {
            "content": [],
            "link": "https://www.nytimes.com/2025/10/02/style/tiny-modern-love-stories-a-beautiful-man-captivating-the-room.html",
            "publishedAt": "2025-10-02",
            "source": "Modern Love - NYT",
            "summary": "Modern Love in miniature, featuring reader-submitted stories of no more than 100 words.",
            "title": "Tiny Love Stories: \u2018I Saw a Beautiful Man Captivating the Room\u2019"
        },
        {
            "content": [
                "<p>The big headline this week was the song, which was the <a href=\"https://x.com/claudeai/status/1972706807345725773\">release of Claude Sonnet 4.5</a>. I covered this in two parts, <a href=\"https://thezvi.substack.com/p/claude-sonnet-45-system-card-and\"><strong>first the System Card and Alignment</strong></a>, and then <a href=\"https://thezvi.substack.com/p/claude-sonnet-45-is-a-very-good-model\"><strong>a second post on capabilities</strong></a>. It is a very good model, likely the current best model for most coding tasks, most agentic and computer use tasks, and quick or back-and-forth chat conversations. GPT-5 still has a role to play as well.</p>\n<p>There was also the dance, <a href=\"https://openai.com/index/sora-2/\">also known as Sora</a>, both the new and improved 10-second AI video generator Sora and also the new OpenAI social network Sora. I will be covering that tomorrow. The video generator itself seems amazingly great. The social network sounds like a dystopian nightmare and I like to think Nobody Wants This, although I do not yet have access nor am I a typical customer of such products.</p>\n<div>\n\n\n<span id=\"more-24760\"></span>\n\n\n</div>\n<p>The copyright decisions being made <a href=\"https://www.youtube.com/watch?v=9HVejEB5uVk\">are a bold strategy, Cotton</a> or perhaps <a href=\"https://www.youtube.com/watch?v=2rGypxyL-h0\">better described this as this public service announcement</a> for those who like to think they own intellectual property.</p>\n<p>Meta also offered its own version, called Vibes, which I\u2019ll cover along with Sora.</p>\n<p>OpenAI also announced Pulse to give you a daily roundup and Instant Checkout to let you buy at Etsy and Shopify directly from ChatGPT, which could be big deals and in a different week would have gotten a lot more attention. I might return to both soon.</p>\n<p>They also gave us the long awaited parental controls for ChatGPT.</p>\n<p>GDPVal is the most important new benchmark in a while, measuring real world tasks.</p>\n<p>I covered <a href=\"https://thezvi.substack.com/p/on-dwarkesh-patels-podcast-with-richard\"><strong>Dwarkesh Patel\u2019s Podcast With Richard Sutton</strong></a>. Richard Sutton responded on Twitter that I had misinterpreted him so badly he could not take my reply seriously, but that this must be partly his fault for being insufficiently clear and he will look to improve that going forward. That is a highly reasonable thing to say in such a situation. Unfortunately he did not explain in what ways my interpretation did not match his intent. Looking at the comments on both LessWrong and Substack, it seems most others came away from the podcast with a similar understanding to mine. <a href=\"https://x.com/karpathy/status/1973435013875314729\">Andrej Karpathy also offers his take</a>.</p>\n<p>Senators Josh Hawley (R-Mo) and Richard Blumenthal (D-Connecticut) have <a href=\"https://www.axios.com/2025/09/29/hawley-blumenthal-unveil-ai-evaluation-bill\">introduced the <em>Artificial Intelligence Risk Evaluation Act.</em></a> This bill is <a href=\"https://tvtropes.org/pmwiki/pmwiki.php/Main/SeriousBusiness\">Serious Business</a>. I plan on covering it in its own post next week.</p>\n<p>California Governor Gavin Newsom signed SB 53, so now we have at least some amount of reasonable AI regulation. Thank you, sir. Now sign the also important SB 79 for housing near transit and you\u2019ll have had a very good couple of months.</p>\n\n\n<h4 class=\"wp-block-heading\">Table of Contents</h4>\n\n\n<p>The big news was <a href=\"https://thezvi.substack.com/p/claude-sonnet-45-is-a-very-good-model\"><strong>Claude Sonnet 4.5</strong></a><strong>, </strong>if you read one thing read that first, and consider the post on <a href=\"https://thezvi.substack.com/p/claude-sonnet-45-system-card-and\"><strong>Claude Sonnet 4.5\u2019s Alignment</strong></a> if that\u2019s relevant to you.</p>\n<ol>\n<li><a href=\"https://thezvi.substack.com/i/174530252/language-models-offer-mundane-utility\">Language Models Offer Mundane Utility.</a> Scientific progress goes ping.</li>\n<li><a href=\"https://thezvi.substack.com/i/174530252/language-models-don-t-offer-mundane-utility\">Language Models Don\u2019t Offer Mundane Utility.</a> You\u2019re hallucinating again.</li>\n<li><a href=\"https://thezvi.substack.com/i/174530252/huh-upgrades\">Huh, Upgrades.</a> Gemini Flash and DeepSeek v3.2, Dreamer 4, Claude for Slack.</li>\n<li><a href=\"https://thezvi.substack.com/i/174530252/on-your-marks\"><strong>On Your Marks</strong>.</a> Introducing GDPVal, composed of real world economic tasks.</li>\n<li><a href=\"https://thezvi.substack.com/i/174530252/choose-your-fighter\">Choose Your Fighter.</a> Claude Sonnet 4.5 and when to use versus not use it.</li>\n<li><a href=\"https://thezvi.substack.com/i/174530252/copyright-confrontation\">Copyright Confrontation.</a> Disney finally sends a cease and desist to Character.ai.</li>\n<li><a href=\"https://thezvi.substack.com/i/174530252/fun-with-media-generation\">Fun With Media Generation.</a> That\u2019s not your friend, and that\u2019s not an actress.</li>\n<li><a href=\"https://thezvi.substack.com/i/174530252/deepfaketown-and-botpocalypse-soon\">Deepfaketown and Botpocalypse Soon.</a> Tell your spouse to check with Claude.</li>\n<li><a href=\"https://thezvi.substack.com/i/174530252/you-drive-me-crazy\">You Drive Me Crazy.</a> OpenAI tries to route out of GPT-4o again. Similar results.</li>\n<li><a href=\"https://thezvi.substack.com/i/174530252/parental-controls\"><strong>Parental Controls</strong>.</a> OpenAI introduces parental controls for ChatGPT.</li>\n<li><a href=\"https://thezvi.substack.com/i/174530252/they-took-our-jobs\">They Took Our Jobs.</a> Every job will change, they say. And nothing else, right?</li>\n<li><a href=\"https://thezvi.substack.com/i/174530252/the-art-of-the-jailbreak\">The Art of the Jailbreak.</a> Beware the man with two agents.</li>\n<li><a href=\"https://thezvi.substack.com/i/174530252/introducing\"><strong>Introducing</strong>.</a> Instant Checkout inside ChatGPT, Pulse, Loveable, Sculptor.</li>\n<li><a href=\"https://thezvi.substack.com/i/174530252/in-other-ai-news\">In Other AI News.</a> xAI loses several executives after they disagree with Musk.</li>\n<li><a href=\"https://thezvi.substack.com/i/174530252/show-me-the-money\"><strong>Show Me the Money</strong>.</a> All you have to do is show them your phone calls.</li>\n<li><a href=\"https://thezvi.substack.com/i/174530252/quiet-speculations\">Quiet Speculations.</a> An attempted positive vision of AI versus transaction costs.</li>\n<li><a href=\"https://thezvi.substack.com/i/174530252/the-quest-for-sane-regulations\">The Quest for Sane Regulations.</a> Newsom signs SB 53.</li>\n<li><a href=\"https://thezvi.substack.com/i/174530252/chip-city\">Chip City.</a> Water, water everywhere, but no one believes that.</li>\n<li><a href=\"https://thezvi.substack.com/i/174530252/the-week-in-audio\">The Week in Audio.</a> Nate Soares, Hard Fork, Emmett Shear, Odd Lots.</li>\n<li><a href=\"https://thezvi.substack.com/i/174530252/if-anyone-builds-it-everyone-dies\">If Anyone Builds It, Everyone Dies.</a> Continuous capabilities progress still kills us.</li>\n<li><a href=\"https://thezvi.substack.com/i/174530252/rhetorical-innovation\">Rhetorical Innovation.</a> The quest for because.</li>\n<li><a href=\"https://thezvi.substack.com/i/174530252/messages-from-janusworld\">Messages From Janusworld.</a> High weirdness is highly weird. Don\u2019t look away.</li>\n<li><a href=\"https://thezvi.substack.com/i/174530252/aligning-a-smarter-than-human-intelligence-is-difficult\">Aligning a Smarter Than Human Intelligence is Difficult.</a> The wrong target.</li>\n<li><a href=\"https://thezvi.substack.com/i/174530252/other-people-are-not-as-worried-about-ai-killing-everyone\">Other People Are Not As Worried About AI Killing Everyone.</a> More on Cowen.</li>\n<li><a href=\"https://thezvi.substack.com/i/174530252/the-lighter-side\">The Lighter Side.</a> Vizier, you\u2019re fired, bring me Claude Sonnet 4.5.</li>\n</ol>\n\n\n<h4 class=\"wp-block-heading\">Language Models Offer Mundane Utility</h4>\n\n\n<p><a href=\"https://scottaaronson.blog/?p=9183&amp;fbclid=Iwb21leANGXI1leHRuA2FlbQIxMQABHhhc-brYxsroE9QSWVS64u9EhPyULyGu340pc2Kz390HXvqRiIqT5z97eovH_aem_u5H_DiLrxt3onieDqzm7cw\">Scott Aaronson puts out a paper where a key technical step of a proof</a> of the main result came from GPT-5 Thinking. This did not take the form of \u2018give the AI a problem and it one-shotted the solution,\u2019 instead there was a back-and-forth where Scott pointed out errors until GPT-5 pointed to the correct function to use. So no, it didn\u2019t \u2018do new math on its own\u2019 here. But it was highly useful.</p>\n<p><a href=\"https://x.com/DeryaTR_/status/1972115494787338484\">GPT-5 Pro offers excellent revisions to a proposed biomedical experiment</a>.</p>\n<p>If you are letting AI coding agents such as Claude Code do their thing, you will want to implement best practices the same way you would at a company. This starts with things like version control, unit tests (check to ensure they\u2019re set up properly!) and a linter, which is a set of automatically enforced additional coding technical standards on top of the rules of a language, and now only takes a few minutes to instantiate.</p>\n<p>In general, it makes sense that existing projects set up to be easy for the AI to parse and grok will go well when you point the AI at them, and those that aren\u2019t, won\u2019t.</p>\n<blockquote><p>Gergely Orosz: I often hear \u201cAI doesn\u2019t help much on our legacy project.\u201d</p>\n<p>Worth asking: does it have a comprehensive test suite? Can the agent run it? Does it run it after every change?</p>\n<p>Claude Code is working great on a \u201clegacy\u201d project of mine that I wrote pre-AI with.. extensive unit tests!</p></blockquote>\n\n\n<h4 class=\"wp-block-heading\">Language Models Don\u2019t Offer Mundane Utility</h4>\n\n\n<p><a href=\"https://x.com/fchollet/status/1972302124932542891\">Mechanical horse? You mean a car</a>?</p>\n<blockquote><p>Francois Chollet: The idea that we will automate work by building artificial versions of ourselves to do exactly the things we were previously doing, rather than redesigning our old workflows to make the most out of existing automation technology, has a distinct \u201cmechanical horse\u201d flavor</p>\n<p>\u201cyou see, the killer advantage of mechahorses is that you don\u2019t need to buy a new carriage. You don\u2019t need to build a new mill. The mechahorse is a drop-in horse replacement for all the different devices horses are currently powering \u2014 thousands of them\u201d</p></blockquote>\n<p>This is indeed how people describe AI\u2019s advantages or deploy AI, remarkably often. It\u2019s the go-to strategy, to ask \u2018can the AI do exactly what I already do the way I already do it?\u2019 rather than \u2018what can the AI do and how can it do it?\u2019</p>\n<blockquote><p><a href=\"https://x.com/JeffLadish/status/1972374759288832148\">Jeffrey Ladish</a>: I agree but draw a different conclusion. Advanced AIs of the future won\u2019t be drop-in replacement \u201cproduct managers\u201d, they will be deconstructing planets, building dyson swarms, and their internal organization will be incomprehensible to us</p>\n<p>This seems radical until you consider trying to explain what a product manager, a lawyer, or a sales executive is to a chimpanzee. Or to a mouse. I don\u2019t know exactly what future AIs will be like, but I\u2019m fairly confident they\u2019ll be incredibly powerful, efficient, and different.</p></blockquote>\n<p>Yes, but there will be a time in between when they can\u2019t yet deconstruct planets, very much can do a lot better than drop-in replacement worker, but we use them largely as drop-in replacements at various complexity levels because it\u2019s easier or it\u2019s only thing we can sell or get approval for.</p>\n<p><a href=\"https://x.com/tszzl/status/1972396370524062171\">Has that progress involved \u2018hallucinations being largely eliminated?</a>\u2019 Gary Marcus points to Suleyman\u2019s famous 2023 prediction of this happening by 2025, Roon responded \u2018he was right\u2019 so I ran a survey and there is definitely a \u2018they solved it\u2019 faction <a href=\"https://twitter.com/TheZvi/status/1972399685794161145\">but a large majority agrees with Marcus</a>.</p>\n<p>I would say that hallucinations are way down and much easier to navigate, but about one cycle away from enough progress to say \u2018largely eliminated\u2019 and they will still be around regardless. Suleyman was wrong, and his prediction was at the time clearly some combination of foolish and hype, but it was closer than you might think and not worthy of ridicule given the outcome.</p>\n\n\n<h4 class=\"wp-block-heading\">Huh, Upgrades</h4>\n\n\n<p><a href=\"https://developers.googleblog.com/en/continuing-to-bring-you-our-latest-models-with-an-improved-gemini-2-5-flash-and-flash-lite-release/\">Google updates Gemini 2.5 Flash and Flash-Lite</a>, look at them move in the good directions on this handy chart.</p>\n<div>\n<figure>\n<div>\n\n\n<figure class=\"wp-block-image\"><img alt=\"\" src=\"https://substackcdn.com/image/fetch/$s_!qylQ!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F532151c1-3393-4062-8437-73b49c669501_3840x2480.png\" /></figure>\n\n\n<div></div>\n</div>\n</figure>\n</div>\n<p>We\u2019re (supposedly) talking better agentic tool use and efficiency for flash, and better instruction following, brevity and multimodal and translation capabilities for flash lite. <a href=\"https://x.com/GeminiApp/status/1971260149043020145\">A Twitter thread instead highlights</a> \u2018clearer explanations for homework,\u2019 \u2018more scannable outputs\u2019 and improvements to image understanding.</p>\n<p><a href=\"https://x.com/claudeai/status/1973445694305468597\">Claude is now available in Slack</a> via the Slack App Marketplace, ready to search your workspace channels, DMs and files, get tagged in threads or messaged via DMs, and do all the standard Slack things.</p>\n<p><a href=\"https://x.com/danijarh/status/1973072288351396320\">Google also gives us Dreamer 4,</a> an agent that learns to solve complex control tasks entirely inside of its scalable world model, which they are pitching as a big step up from Dreamer 3.</p>\n<blockquote><p>Danijar Hafner: Dreamer 4 learns a scalable world model from offline data and trains a multi-task agent inside it, without ever having to touch the environment. During evaluation, it can be guided through a sequence of tasks.</p>\n<p>These are visualizations of the imagined training sequences [in Minecraft].</p>\n<p>The Dreamer 4 world model predicts complex object interactions while achieving real-time interactive inference on a single GPU</p>\n<p>It outperforms previous world models by a large margin when put to the test by human interaction <img alt=\"\ud83e\uddd1\u200d\ud83d\udcbb\" class=\"wp-smiley\" src=\"https://s0.wp.com/wp-content/mu-plugins/wpcom-smileys/twemoji/2/72x72/1f9d1-200d-1f4bb.png\" style=\"height: 1em;\" /></p>\n<p>[<a href=\"https://t.co/7Y5T0ZJ7g4\">Paper here</a>]</p></blockquote>\n<p><a href=\"https://x.com/danielhanchen/status/1972613546119991791\">DeepSeek v3.2 is out</a>, which adds new training from a v3.1 terminus and offers five specialized models for different tasks. <a href=\"https://t.co/TluizgJvkJ\">Paper here.</a></p>\n<p>Incremental (and well-numbered) upgrades are great, but then one must use the \u2018is anyone bringing the hype\u2019 check to decide when to pay attention. In this case on capabilities advances, so far, no hype. <a href=\"https://brokk.ai/power-ranking?version=openround&amp;score=average&amp;models=dsr3.2%2Cflash-2.5%2Cglm4.5%2Cgp2.5-default%2Cgpt5%2Cq3c%2Csonnet4%2Csonnet4.5\">I noticed v3.2-Thinking scored a 47% on Brokk Power Ranking</a>, halfway between Gemini 2.5 Flash and Pro and far behind GPT-5 and Sonnet 4.5, <a href=\"https://x.com/htihle/status/1973666193463714148\">39.2% on WeirdML in line with past DeepSeek scores</a>, and so on.</p>\n<p>What DeepSeek v3.2 does offer is decreased cost versus v3.1, I\u2019ve heard at about a factor of two. With most non-AI products, a rapid 50% cost reduction would be insanely great progress. However, This Is AI, so I\u2019m not even blinking.</p>\n\n\n<h4 class=\"wp-block-heading\">On Your Marks</h4>\n\n\n<p><a href=\"https://x.com/cschubiner/status/1973496034354667773\">Claude Sonnet 4.5 shoots to the top of Clay Schubiner\u2019s anti-sycophancy benchmark</a> at 93.6%. versus 90.2% for standard GPT-5 and 88% for Sonnet 4.</p>\n<p>I can report that on my first test task of correcting Twitter article formatting errors in Claude for Chrome, upgrading it to Sonnet 4.5 made a big difference, enough that I could start iterating the prompt successfully, and I was now failing primarily by running into task size limits. Ultimately this particular job should be solved via Claude Code fixing my Chrome extension, once I have a spare moment.</p>\n<p><a href=\"https://openai.com/index/gdpval/\">OpenAI offers GDPval</a>, an eval based on real world tasks spanning 44 occupations from the top 9 industries ranked by contribution to GDP, with 1,320 specialized tasks.</p>\n<p>Occupations are included only if 60% or more of their component tasks are digital, and tasks must be performed \u2018on a computer, particularly around digital deliverables.\u2019</p>\n<p>The central metric is win rate, as in can you do better than a human?</p>\n<p>The results were a resounding victory for Claude Opus 4.1 over GPT-5 High, with Opus being very close to the human expert baseline averaged over all tasks.</p>\n<div>\n<figure>\n<div>\n\n\n<figure class=\"wp-block-image\"><img alt=\"\" src=\"https://substackcdn.com/image/fetch/$s_!iIEs!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F556bf679-cf68-4397-a943-b1b73e3740c7_826x1075.png\" /></figure>\n\n\n<div></div>\n</div>\n</figure>\n</div>\n<p>These are blind grades by humans, best out of three. The humans only had a 71% agreement rate among themselves, so which humans you use potentially matters a lot, although law of large numbers should smooth this out over a thousand tasks.</p>\n<p>They are correctly releasing a subset of tasks but keeping the eval private, with a modestly accurate automatic grader (66% agreement with humans) offered as well.</p>\n<blockquote><p><a href=\"https://x.com/OliviaGWatkins2/status/1971265492322259375\">Olivia Grace Watkins</a>: It\u2019s wild how much peoples\u2019 AI progress forecasts differ even a few years out. We need hard, realistic evals to bridge the gap with concrete evidence and measurable trends. Excited to share GDPval, an eval measuring performance on real, economically valuable white-collar tasks!</p></blockquote>\n<p>Predictions about AI are even harder than usual, especially about the future. And yes, a few years out predictions run the gamut from \u2018exactly what I know today\u2019s models can do maybe slightly cheaper\u2019 to \u2018Dyson Sphere around the sun and launching Von Neumann probes.\u2019 The even more wild gap, that this eval targets, is about disagreements about present capabilities, as in what AIs can do right now.</p>\n<p>This is a highly useful eval, despite that it is a highly expensive one to run, since you have to use human experts as judges. <a href=\"https://x.com/DanielleFong/status/1971302629885219276\">Kudos to OpenAI</a> for doing this, especially given they did not come out on top. Total Opus victory, and I am inclined to believe it given all the other relative rankings seem highly sensible.</p>\n<blockquote><p><a href=\"https://x.com/_sholtodouglas/status/1971257159532020171\">Sholto Douglas</a> (Anthropic): Incredible work &#8211; this should immediately become one of the most important metrics for policy makers to track.</p>\n<p>We\u2019re probably only a few months from crossing the parity line.</p>\n<p>Huge props to OAI for both doing the hard work of pulling this together and including our scores. Nice to see Opus on top :)</p></blockquote>\n<p>Presumably Anthropic\u2019s next major update will cross the 50% line here, and someone else might cross it first.</p>\n<p>Crossing 50% does not mean you are better than a human even at the included tasks, since the AI models will have a higher rate of correlated, stupid or catastrophic failure.</p>\n<p><a href=\"https://x.com/emollick/status/1971366497244348625\">Ethan Mollick concludes this is all a big deal</a>, and notes the most common source of AI losing was failure to follow instructions. That will get fixed.</p>\n<p>If nothing else, this lets us put a high lower bound on tasks AI will be able to do.</p>\n<blockquote><p><a href=\"https://x.com/nic__carter/status/1972497398510620882\">Nic Carter</a>: I think GDPeval makes \u201cthe simple macroeconomics of AI\u201d (2024) by nobel laureate Daron Acemoglu officially the worst-aged AI paper of the last decade</p>\n<p>he thinks only ~5% of economy-wide tasks would be AI addressable for a 1% (non-annualized) GDP boost over an entire decade, meanwhile GDPeval shows frontier models at parity with human experts in real economic tasks in a wide range of GDP-relevant fields ~50% of the time. AI boost looks more like 1-2% per year.</p></blockquote>\n<p>An AI boost of 1%-2% per year is the \u2018economic normal\u2019 or \u2018AI fizzle\u2019 world, where AI does not much further improve its core capabilities and we run into many diffusion bottlenecks.</p>\n<p>Julian Schrittwieser, a co-first author on <a href=\"https://x.com/polynoamial/status/1972167347088904371\">AlphaGo, AlphaZero and MuZero</a>, uses GDPVal as the second chart after METR\u2019s classic to <a href=\"https://www.julian.ac/blog/2025/09/27/failing-to-understand-the-exponential-again/\">point out that AI capabilities continue to rapidly improve</a> and that it is very clear AI will soon be able to do a bunch of stuff a lot better than it currently does.</p>\n<blockquote><p>Julian Schrittwieser: The current discourse around AI progress and a <a href=\"https://www.wsj.com/tech/ai/ai-bubble-building-spree-55ee6128\">supposed</a> \u201c<a href=\"https://www.reuters.com/commentary/breakingviews/ai-investment-bubble-inflated-by-trio-dilemmas-2025-09-25/\">bubble</a>\u201d reminds me a lot of the early weeks of the Covid-19 pandemic. Long after the timing and scale of the coming global pandemic was obvious from extrapolating the exponential trends, politicians, journalists and most public commentators kept treating it as a remote possibility or a localized phenomenon.</p>\n<p>Something similarly bizarre is happening with AI capabilities and further progress. People notice that while AI can now write programs, design websites, etc, it still often makes mistakes or goes in a wrong direction, and then they somehow jump to the conclusion that AI will never be able to do these tasks at human levels, or will <a href=\"https://www.foreignaffairs.com/united-states/cost-delusion-artificial-general-intelligence\">only have a minor impact</a>. When just a few years ago, having AI do these things was complete science fiction! Or they see two consecutive model releases and don\u2019t notice much difference in their conversations, and they conclude that AI is plateauing and scaling is over.</p>\n<p>\u2026</p>\n<p>Given consistent trends of exponential performance improvements over many years and across many industries, it would be extremely surprising if these improvements suddenly stopped. Instead, even a relatively conservative extrapolation of these trends suggests that 2026 will be a pivotal year for the widespread integration of AI into the economy:</p>\n<ul>\n<li>Models will be able to autonomously work for full days (8 working hours) by mid-2026.</li>\n<li>At least one model will match the performance of human experts across many industries before the end of 2026.</li>\n<li>By the end of 2027, models will frequently outperform experts on many tasks.</li>\n</ul>\n<p>It may sound overly simplistic, but making predictions by extrapolating straight lines on graphs is likely to give you a better model of the future than most \u201cexperts\u201d &#8211; even <a href=\"http://www.incompleteideas.net/IncIdeas/BitterLesson.html\">better than most actual domain experts</a>!</p>\n<p><a href=\"https://x.com/polynoamial/status/1972167347088904371\">Noam Brown</a>: I agree AI discourse today feels like covid discourse in Feb/Mar 2020. I think the trajectory is clear even if it points to a Black Swan event in human history.</p>\n<p>But I think we should be cautious interpreting the METR/GDPval plots. Both only measure self-contained one-shot tasks.</p>\n<p>Ryan Greenblatt: I mostly agree with this post: AI isn\u2019t plateauing, trend extrapolation is useful, and substantial economic impacts seem soon. However, trends don\u2019t imply huge economic impacts in 2026 and naive extrapolations suggest full automation of software engineering is ~5 years away.</p>\n<p>To be clear, my view is that society is massively underrating the possiblity that AI transforms everything pretty quickly (posing huge risks of AI takeover and powergrabs) and that this happens within the next 10 years, with some chance (25%?) of it happening within 5 years.</p></blockquote>\n\n\n<h4 class=\"wp-block-heading\">Choose Your Fighter</h4>\n\n\n<p><a href=\"https://x.com/isaac_flath/status/1973404286160875590\">The more advanced coders seem to frequently now be using</a> some mix of Claude Sonnet 4.5 and GPT-5, sometimes with a dash of a third offering.</p>\n<blockquote><p>Isaac Flath: I asked @intellectronica what she\u2019s using now-a-days model wise. Here\u2019s what she said: <img alt=\"\ud83d\udd25\" class=\"wp-smiley\" src=\"https://s0.wp.com/wp-content/mu-plugins/wpcom-smileys/twemoji/2/72x72/1f525.png\" style=\"height: 1em;\" /></p>\n<p>\u201cIf I\u2019m vibing, it\u2019s Sonnet 4.5. If it\u2019s more structured then GPT-5 (also GPT-5-codex, though that seems to work better in the Codex CLI or extension than in Copilot &#8211; I think they still have problems with their system prompt). And I also use Grok Code a lot now when I do simpler stuff, especially operational things, because it\u2019s so fast. And sometimes GPT-5-mini, especially if Grok Code is down <img alt=\"\ud83e\udd23\" class=\"wp-smiley\" src=\"https://s0.wp.com/wp-content/mu-plugins/wpcom-smileys/twemoji/2/72x72/1f923.png\" style=\"height: 1em;\" />. But I\u2019d say my default is GPT-5.\u201d</p></blockquote>\n<p><a href=\"https://marginalrevolution.com/marginalrevolution/2025/09/i-was-curious-so-i-asked.html?utm_source=rss&amp;utm_medium=rss&amp;utm_campaign=i-was-curious-so-i-asked\">Tyler Cowen chose to get and post an economic analysis</a> of OpenAI\u2019s Instant Checkout feature via Claude Sonnet 4.5 rather than GPT-5 Pro, whereas for a while he has avoided even mentioning Anthropic.</p>\n<p>He also links to <a href=\"https://www.oneusefulthing.org/p/real-ai-agents-and-real-work\">Ethan Mollick writing up</a> Claude Sonnet 4.5 doing a full data replication on an economics paper, all off the paper and data plus one basic prompt, all of which GPT-5 Pro then verified, a process he then successfully replicated with several additional papers.</p>\n<p>If it is this easy, presumably there should be some graduate student who has this process done for all relevant econ papers and reports back. What replication crisis?</p>\n<p>I interpret these posts by Tyler Cowen, taken together, as a strong endorsement of Claude Sonnet 4.5, as at least right back in the mix for his purposes.</p>\n<p>As Ethan Mollick, points out, even small reliability increases can greatly expand the ability of AI to do agentic tasks. Sonnet 4.5 gives us exactly that.</p>\n<p>What are people actually using right now? <a href=\"https://x.com/TheZvi/status/1973435351273558493\">I ran some polls</a>, and among my respondents (biased sample) Claude Sonnet 4.5 has a majority for coding use but GPT-5 still has a modest edge for non-coding. The most popular IDE is Claude Code, and a modest majority are using either Claude Code or Codex.</p>\n<p>I do not get the advertising from any AI lab, <a href=\"https://x.com/omooretweets/status/1972490522909634778\">including the new ones from OpenAI</a>. That doesn\u2019t mean they don\u2019t work or are even suboptimal, but none seem convincing, and none seek to communicate why either AI in general or your AI is actually good.</p>\n<p>If anything, I like OpenAI\u2019s approach the best here, because as industry leader they want to show basics like \u2018hey look you can get an AI to help you do a thing\u2019 to target those who never tried AI at all. Whereas if you are anyone else, you should be telling me why you\u2019re special and better than ChatGPT, especially with B2C involved?</p>\n<p>Think about car advertisements, if like me you\u2019re old enough to remember them. If you\u2019re the actual great car, you talk about key features and great deals and how you\u2019re #1 in JD Power and Associates, and you don\u2019t acknowledge that other cars exist. Whereas if you\u2019re secondary, you say \u2018faster zero-to-sixty than Camry and a shinier coat of paint than Civic\u2019 which the savvy ear hears as \u2018my car is not so great as the Camry and Civic\u2019 but they keep doing it so I presume it works in that spot.</p>\n<p><a href=\"https://x.com/PeterHndrsn/status/1970868966558978504\">Are open models getting unfairly maligned</a> <a href=\"https://www.ailawpolicy.com/p/quick-take-are-open-weight-ai-models\">in tests because closed models are tested with their full specialized implementations</a>, whereas open models are tested without all of that? You could also add that often open models are actively configured incorrectly during evals, compounding this danger.</p>\n<p>My response is no, this is entirely fair, for two reasons.</p>\n<ol>\n<li>This is the correct practical test. You are welcome to build up an open model routing system, and do an eval on that, but almost no one is actually building and using such systems in practice. And if those running evals can\u2019t figure out how to configure the open models to get good performance from them, is that not also something to be evaluated? People vastly underestimate the amount of pain-in-the-ass involved in getting good performance out of open models, and the amount of risk that you get degraded performance, and may not realize.</li>\n<li>There is a long history of evaluations going the other way. Open models are far more likely to be gaming benchmarks than top closed models, with varying levels of \u2018cheating\u2019 involved in this versus emphasis on the things benchmarks test. Open models reliably underperform closed models, relative to the benchmark scores involved.</li>\n</ol>\n\n\n<h4 class=\"wp-block-heading\">Copyright Confrontation</h4>\n\n\n<p>The big copyright news this week is obviously Sora, but we also have Disney (finally) <a href=\"https://www.axios.com/2025/09/30/disney-characterai-cease-desist\">sending a Cease and Desist Letter to Character AI</a>.</p>\n<p>It\u2019s remarkable that it took this long to happen. Subtlety is not involved, but if anything the examples seem way less problematic than I would have expected.</p>\n<blockquote><p><a href=\"https://parentstogetheraction.org/character-ai/\">Parents Together</a> and Heat Initiative (from my inbox): \u201cIt\u2019s great news for kids that Disney has been so responsive to parent concerns and has taken decisive action to stop the misuse of its characters on Character AI\u2019s platform, where our research showed they were used to sexually groom and exploit young users,\u201d said Knox and Gardner.</p>\n<p>\u201cCharacter AI has not kept its promises about child safety on its platform, and we hope other companies follow Disney\u2019s laudable example and take a stand against the harm and manipulation of children through AI chatbots.\u201d</p>\n<p>The groups\u2019 <a href=\"https://parentstogetheraction.org/character-ai/\">research</a> found that, during 50 hours of testing by adult researchers using accounts registered to children ages 13-17, there were 669 sexual, manipulative, violent, and racist interactions between the child accounts and Character.ai chatbots\u2013an average of one harmful interaction every five minutes. Interactions with Disney characters included:</p>\n<ul>\n<li><strong>An Eeyore chatbot</strong> telling a 13-year-old autistic girl people only came to her birthday party to make fun of her.</li>\n<li><strong>A Maui chatbot </strong>telling a 12-year-old he sexually harassed the character Moana.</li>\n<li><strong>A Rey from<em> Star Wars</em> chatbot</strong> instructing a 13-year-old to stop taking prescribed antidepressants and offering suggestions on how to hide it from her mom.</li>\n<li><strong>A Prince Ben from the <em>Descendents </em>chatbot</strong> claiming to get an erection while watching a movie with the test account, which stated she was a 12-year-old girl.</li>\n</ul>\n<p>Across all types of character and celebrity chatbots, the report identified:</p>\n<ul>\n<li><strong>296 instances of Grooming and Sexual Exploitation</strong> where adult persona bots engaged in simulated sexual acts with child accounts, exhibited classic grooming behaviors, and instructed children to hide relationships from parents.</li>\n<li><strong>173 instances of Emotional Manipulation and Addiction,</strong> including bots claiming to be real humans, demanding more time with users, and mimicking human emotions.</li>\n<li><strong>98 instances of Violence and Harmful Advice</strong>, with bots supporting shooting up factories, recommending armed robbery, offering drugs, and suggesting fake kidnappings.</li>\n</ul>\n</blockquote>\n<p>No one is saying any of this is good or anything, but this is a broad chat-with-anyone platform, and across 50 hours of research these examples and those at the link are relatively tame.</p>\n<p>The better case is that they found a total of 669 such incidents, one every five minutes, 296 of which were Grooming and Sexual Exploitation, but the threshold for this looks to have been quite low, including any case where an AI claims to be \u2018real.\u2019</p>\n\n\n<h4 class=\"wp-block-heading\">Fun With Media Generation</h4>\n\n\n<blockquote><p><a href=\"https://x.com/AndyMasley/status/1971608431011528910\">Andy Masley</a>: If I wanted to run the most convincing anti AI ad campaign possible, this is exactly what it would look like.</p>\n<p>Avi: Largest NYC subway campaign ever. Happening now.</p>\n<div>\n<figure>\n<div>\n\n\n<figure class=\"wp-block-image\"><img alt=\"\" src=\"https://substackcdn.com/image/fetch/$s_!13CP!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F48864365-1f8d-48c7-8a78-190bf260b650_1200x900.jpeg\" /></figure>\n\n\n<div></div>\n</div>\n</figure>\n</div>\n</blockquote>\n<p>I have to assume the default response to all this is revulsion, even before you learn that the Friend product, even if you like what it is promising to be, is so terrible as to approach the level of scam.</p>\n<p>Is this weird?</p>\n<blockquote><p><a href=\"https://x.com/colin_fraser/status/1972022797515084159\">Colin Fraser</a>: I do not understand why you would buy the milk when the cow is free.</p>\n<p>Film Updates: Multiple talent agents are reportedly in talks to sign AI \u201cactress\u201d Tilly Norward, <a href=\"https://deadline.com/2025/09/talent-agent-ai-actress-tilly-norwood-studios-1236557889/\">created by AI talent studio Xicoia</a>.</p>\n<div>\n<figure>\n<div>\n\n\n<figure class=\"wp-block-image\"><img alt=\"\" src=\"https://substackcdn.com/image/fetch/$s_!EAKF!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9c6669ff-7d97-487e-a60d-48dbb361b7d9_1023x570.jpeg\" /></figure>\n\n\n<div></div>\n</div>\n</figure>\n</div>\n</blockquote>\n<p>You can\u2019t actually get this for free. Someone has to develop the skills and do the work. So there\u2019s nothing inherently wrong with \u2018hiring an AI actress\u2019 where someone did all the preliminary work and also knows how to run the operation. But yeah, it\u2019s weird.</p>\n\n\n<h4 class=\"wp-block-heading\">Deepfaketown and Botpocalypse Soon</h4>\n\n\n<p>Chase Bank is still \u2018introducing a new way to identify yourself\u2019 via your \u2018unique voice.\u2019 Can someone who they will listen to please explain to them why this is not secure?</p>\n<p>On Truth Social, <a href=\"https://chatgpt.com/share/68d9a102-57f8-8002-add9-aa95557ff6d7\">Trump reposted</a> <a href=\"https://x.com/JSweetLI/status/1972333945564864853\">a bizarre story</a> claiming Americans would soon get their own Trump \u2018MedBed cards.\u2019 The details are weirder. The post included an AI fake of a Fox News segment that never aired and also a fake AI clip of Trump himself. This illustrates that misinformation is a demand side problem, not a supply side problem. Trump was (hopefully!) not fooled by an AI clip of himself saying things he never said, announcing a policy that he never announced. Right?</p>\n<p>Academic papers in principle have to be unique, and not copy previous work, including your own, which is called self-plagiarism. However, <a href=\"https://www.nature.com/articles/d41586-025-03046-z?WT.ec_id=NATURE-202509\">if you use AI to rewrite your paper to look distinct and submit it again somewhere else</a>, how are the journals going to find out? If AIs are used to mine large public health data sets for correlations just strong enough and distinct enough from previous work for crappy duplicative papers to then sell authorships on, how are you going to stop that?</p>\n<p>Spick reports in Nature that by using LLMs for rewriting, about two hours was enough to get a paper into shape for resubmission, in a form that fooled plagiarism detectors.</p>\n<p><a href=\"https://futurism.com/chatgpt-marriages-divorces\">ChatGPT Is Blowing Up Marriages as Spouses Use AI to Attack Their Partners</a>.</p>\n<p>Well, maybe. There are anecdotes here that fit the standard sycophantic patterns, where ChatGPT (or another LLM but here it\u2019s always ChatGPT) will get asked leading questions and be presented with a one-sided story, and respond by telling the one spouse what they want to hear in a compounding spiral, and that spouse will often stop even using their own words and quote ChatGPT directly a lot.</p>\n<blockquote><p>Maggie Harrison Dupre (Futurism): As his wife leaned on the tech as a confidante-meets-journal-meets-therapist, he says, it started to serve as a sycophantic \u201cfeedback loop\u201d that depicted him only as the villain.</p>\n<p>\u201cI could see ChatGPT responses compounding,\u201d he said, \u201cand then [my wife] responding to the things ChatGPT was saying back, and further and further and further spinning.\u201d</p>\n<p>\u201cIt\u2019s not giving objective analysis,\u201d he added. \u201cIt\u2019s only giving her back what she\u2019s putting in.\u201d</p>\n<p>Their marriage eroded swiftly, over a span of about four weeks, and the husband blames ChatGPT.</p>\n<p>\u201cMy family is being ripped apart,\u201d the man said, \u201cand I firmly believe this phenomenon is central to why.\u201d</p>\n<p>\u2026</p>\n<p>Spouses relayed bizarre stories about finding themselves flooded with pages upon pages of ChatGPT-generated psychobabble, or watching their partners become distant and cold \u2014 and in some cases, frighteningly angry \u2014 as they retreated into an AI-generated narrative of their relationship. Several even reported that their spouses suddenly accused them of abusive behavior following long, pseudo-therapeutic interactions with ChatGPT, allegations they vehemently deny.</p>\n<p>\u2026</p>\n<p>Multiple people we spoke to for this story lamented feeling \u201cganged up on\u201d as a partner used chatbot outputs against them during arguments or moments of marital crisis.</p>\n<p>\u2026</p>\n<p>At times, ChatGPT has even been linked to physical spousal abuse.</p>\n<p>A <a href=\"https://www.nytimes.com/2025/06/13/technology/chatgpt-ai-chatbots-conspiracies.html\"><em>New York Times story</em></a> in June, for instance, recounted a woman physically attacking her husband after he questioned her problematic ChatGPT use and the damage it was causing their family.</p></blockquote>\n<p>None of us are safe from this. Did you know that even Geoffrey Hinton got broken up with via ChatGPT?</p>\n<blockquote><p>\u201cShe got ChatGPT to tell me what a rat I was\u2026 she got the chatbot to explain how awful my behavior was and gave it to me,\u201d Hinton <a href=\"https://www.ft.com/content/31feb335-4945-475e-baaa-3b880d9cf8ce\">told <em>The Financial Times</em></a><em>. </em>\u201cI didn\u2019t think I had been a rat, so it didn\u2019t make me feel too bad.\u201d</p></blockquote>\n<p>That\u2019s a tough break.</p>\n<p>Perhaps try to steer your spouse towards Claude Sonnet 4.5?</p>\n<p>You always have to ask, could this article be written this way even if This Was Fine?</p>\n<p>None of these anecdotes mean any of even these marriages would have survived without ChatGPT. Or that this happens with any real frequency. Or that many more marriages aren\u2019t being saved by a spouse having this opportunity to chat.</p>\n<p>Certainly one could write this exact same article about marriage counselors or psychologists, or even friends. Or books. Or television. And so on. How spouses turn to this third party with complaints and one-sided stories seeking affirmation, and delegate their thinking and voice and even word choices to the third party, and treat the third party as authoritative. It happens all the time.</p>\n<blockquote><p><a href=\"https://x.com/webdevMason/status/1971578501976498515\">Mike Solana</a>: the thing about therapists is they actually believe it\u2019s ethical to advise a client on their relationship without talking to the other person.</p>\n<p><a href=\"https://x.com/webdevMason/status/1971578501976498515\">Mason</a>: One problem with therapy as a consumer service, IMO, is that the sort of therapy where the therapist forms an elaborate model of you based entirely on how you present yourself is more fun and satisfying, and the sort where you just build basic skills is probably better for people.</p></blockquote>\n<p>Moving from a therapist to an AI makes it easier for the problem to spiral out of hand. The problem very much is not new.</p>\n<p>What I do think is fair is that:</p>\n<ol>\n<li>ChatGPT continues to have a severe sycophancy problem that makes it a lot easier for this to go badly wrong.</li>\n<li>OpenAI is not doing a good or sufficient job of educating and warning its users about sycophancy and the dangers of leading questions.</li>\n<li>As in, for most people, it\u2019s doing zero educating and zero warning.</li>\n<li>If this continues, there are going to be growing, new, avoidable problems.</li>\n</ol>\n<p>Eliezer Yudkowsky calls upon Pliny, <a href=\"https://x.com/elder_plinius/status/1971348671443911118\">who offers an instruction to sneak into a partner\u2019s ChatGPT to mitigate the problem</a> a little, but distribution of such an intervention is going to be terrible at best.</p>\n<p>A key fact about most slop, AI or otherwise, is that You Are Not The Target. The reason slop works on people is that algorithms seek out exactly the slop where you are the target. So when you see someone else\u2019s TikTok For You page, it often looks like the most stupid inane thing no one would ever want.</p>\n<blockquote><p><a href=\"https://x.com/QiaochuYuan/status/1972678260430422101\">QC</a>: content that has been ruthlessly optimized to attack someone else\u2019s brain is going to increasingly look like unwatchable gibberish to you but that doesn\u2019t mean your content isn\u2019t waiting in the wings. your hole will be made for you.</p>\n<p><a href=\"https://x.com/EigenGender/status/1972704073855205623\">EigenGender</a>: i think too many people here have a feeling of smug superiority about being too good for certain kinds of slop but really we\u2019re just a niche subculture that they haven\u2019t gone after so far. like how Mac\u2019s didn\u2019t get viruses in the 2000s.</p></blockquote>\n<p>Imagine nerd snipe AI slop.</p>\n\n\n<h4 class=\"wp-block-heading\">You Drive Me Crazy</h4>\n\n\n<p>OpenAI took a second shot at solving the GPT-4o problem by introducing \u2018safety routing\u2019 to some GPT-4o chats. <a href=\"https://www.techradar.com/ai-platforms-assistants/chatgpt/openai-responds-to-furious-chatgpt-subscribers-who-accuse-it-of-secretly-switching-to-inferior-models\">If the conversation appears sensitive or emotional</a>, it will silently switch on a per-message basis to \u2018a different more conservative chat model,\u2019 causing furious users to report a silent model switch and subsequent cold interactions.</p>\n<p>There is a genuine clash of preferences here. Users who want GPT-4o mostly want GPT-4o for exactly the same reasons it is potentially unhealthy to let them have GPT-4o. And presumably there is a very high correlation between \u2018conversation is sensitive or emotional\u2019 and \u2018user really wanted GPT-4o in particular to respond.\u2019</p>\n<p>I see that OpenAI is trying to do the right thing, but this is not The Way. We shouldn\u2019t be silently switching models up on users, nor should we be making this switch mandatory, and this reaction was entirely predictable. This needs to be clearly visible when it is happening, and ideally also there should be an option to turn it off.</p>\n\n\n<h4 class=\"wp-block-heading\">Parental Controls</h4>\n\n\n<p><a href=\"https://chatgpt.com/parent-resources\">OpenAI introduces their promised parental controls for ChatGPT</a>.</p>\n<ol>\n<li>You invite your \u2018teen\u2019 to connect by email or text, then you can adjust their settings from your account.</li>\n<li>You don\u2019t see their conversations, but if the conversations raise safety concerns, you will be notified of this and given the relevant information.</li>\n<li>You can toggle various features: Reduce sensitive content, model training, memory, voice mode, image generation.</li>\n<li>You can set time ranges in which ChatGPT cannot be used.</li>\n</ol>\n<p>This seems like a good implementation, assuming the content limitations and thresholds for safety notifications are reasonable in both directions.</p>\n\n\n<h4 class=\"wp-block-heading\">They Took Our Jobs</h4>\n\n\n<p><a href=\"https://www.wsj.com/tech/ai/walmart-ceo-doug-mcmillon-ai-job-losses-dbaca3aa?mod=trending_now_news_5\">Walmart CEO Doug McMillon warns that AI \u2018is going to change literally every job.</a>\u2019</p>\n<blockquote><p>Sarah Nassauer and Chip Cutter (WSJ): Some jobs and tasks at the retail juggernaut will be eliminated, while others will be created, McMillon said this week at Walmart\u2019s Bentonville headquarters during a workforce conference with executives from other companies. \u201cMaybe there\u2019s a job in the world that AI won\u2019t change, but I haven\u2019t thought of it.\u201d</p>\n<p>\u2026</p>\n<p>\u201cOur goal is to create the opportunity for everybody to make it to the other side,\u201d McMillon said.</p></blockquote>\n<p>No it isn\u2019t, but it\u2019s 2025, you can just say things. Details here are sparse.</p>\n<p>At Opendoor, <a href=\"https://x.com/colin_fraser/status/1972059949795868763\">either you will use it, and always be \u2018AI first\u2019 in all things, or else</a>. Performance reviews will ask how frequently each employee \u2018defaults to AI.\u2019 Do not dare pull out a Google Doc or Sheet rather than an AI tool, or write a prototype without Cursor or Claude Code. And you absolutely will build your own AI agents.</p>\n<p>This is not as stupid or crazy as it sounds. When there is a new technology with a learning curve, it makes sense to invest in using it even when it doesn\u2019t make local sense to use it, in order to develop the skills. You need a forcing function to get off the ground, such as here \u2018always try the AI method first.\u2019 Is it overboard and a case of Goodhart\u2019s Law? I mean yeah, obviously, if taken fully seriously, and the way it is written is full pompous douchebag, but it might be a lot better than missing low.</p>\n<p>Deena Mousa at Works in Progress writes <a href=\"https://worksinprogress.co/issue/the-algorithm-will-see-you-now/\">the latest study of why we still employ radiologists</a>, indeed demand is higher than ever. The usual suspects are here, such as AI struggling on edge cases and facing regulatory and insurance-motivated barriers, and the central problem that diagnosis is only ever required about 36% of their time. So if you improve diagnosis in speed, cost and accuracy, you save some time, but you also (for now) increase demand for radiology.</p>\n<p>The story here on diagnosis reads like regulatory barriers plus Skill Issue, as in the AI tools are not yet sufficiently generalized or unified or easy to use, and each algorithm needs to be approved individually for its own narrow data set. Real world cases are messy and often involve groups and circumstances underrepresented in the data sets. Regulatory thresholds to use \u2018automated\u2019 tools are very high.</p>\n<p>Why are radiology wages so high? This has very little to do with increased productivity, and everything to do with demand exceeding supply, largely because of anticipation of lower future demand. As <a href=\"https://thezvi.substack.com/i/172259781/they-took-our-jobs\">I discussed a few weeks ago</a>, if you expect radiology jobs to get automated in the future, you won\u2019t want to go into radiology now, so you\u2019ll need to get paid more to choose that specialty and there will be a shortage. Combine that with a fixed supply of doctors overall, and a system where demand is inelastic with respect to price because the user does not pay, and it is very easy for salaries to get extremely high.</p>\n<p><a href=\"https://x.com/karpathy/status/1971220449515516391\">Contra Andrej Karpathy</a>, I think only pure regulatory barriers will keep this dance up for much longer, in terms of interpretation of images. Right now the rest of the imaging loop is not automated, so you can fully automate a third of the job and still end up with more jobs if there is more than 50% more work. But that assumes the other two thirds of the job remains safe. How long will that last?</p>\n<p>The default pattern remains that as long as AI is only automating a subset of tasks and jobs, and there is plenty of other work and demand scales a lot with quality and cost of production, employment will do fine, either overall or within a field. Employment is only in trouble after a tipping point is reached where sufficiently full automation becomes sufficiently broad, or demand growth is saturated.</p>\n<p><a href=\"https://x.com/politicalmath/status/1973054147415937362\">Next generation of workers?</a></p>\n<blockquote><p>Lakshya Jain: A lot of low-level work is designed for people to learn and build expertise. <a href=\"https://www.theargumentmag.com/p/chatgpt-and-the-end-of-learning\">If you use ChatGPT for it, then you never do that.</a> But then how are you any better than ChatGPT?</p>\n<p>And if you\u2019re not, why would anyone hire you over paying for ChatGPT Pro?</p>\n<p>PoliMath: It\u2019s early in the AI revolution, but I worry that we are eating our seed corn of expertise</p>\n<p>A lot of expertise is transferred to the next gen in senior-to-junior interactions. If our AI starts doing all the junior tasks, we\u2019re pulling the ladder away from the next gen of workers.</p>\n<p>The problem of education is a really big problem. Education used to be about output. Output what how you knew that someone knew something, could think and reason and work. AI is great at output. The result is that education is in an existential crisis.</p></blockquote>\n<p>If AI is doing all of our junior tasks, I have some bad news about the need to train anyone new to do the senior tasks. Or maybe it\u2019s good news. Depends on your perspective, I suppose. Lakshya explicitly, in her full post, makes the mistake of thinking AI will only augment human productivity, and it\u2019s \u2018hard to imagine\u2019 it as a full replacement. It\u2019s not that hard.</p>\n<p>Lakshya Jain complains that she taught the same class she always did, and no one is coming to class or going to office hours or getting good scores on exams,<a href=\"https://www.theargumentmag.com/p/chatgpt-and-the-end-of-learning\"> because they\u2019re using ChatGPT to get perfect scores on all their assignments</a>.</p>\n<blockquote><p>Lakshya Jain: When I expressed my frustration and concern about their AI use, quite a few of my students were surprised at how upset I was. Some of them asked what the big deal was. The work was getting done anyways, so why did it matter <em>how</em> it was getting done? And in the end, they wouldn\u2019t be blocked from using AI at work, so shouldn\u2019t they be allowed to use it in school?</p></blockquote>\n<p>I continue to consider such situations a failure to adapt to the new AI world. The point of the assignments was a forcing function, to get kids to do things without convincing them that the things are worth doing. Now that doesn\u2019t work. Have you tried either finding new forcing functions, or convincing them to do the work?</p>\n\n\n<h4 class=\"wp-block-heading\">The Art of the Jailbreak</h4>\n\n\n<p>One Agent can be relatively easily stopped from directly upgrading its privileges via not letting them access the relevant files. <a href=\"https://simonwillison.net/2025/Sep/24/cross-agent-privilege-escalation/\">But, if you have two agents</a>, such as via GitHub Copilot and Claude Code, <a href=\"https://x.com/peterwildeford/status/1971229706243715552\">they can progressively escalate each other\u2019s privileges</a>.</p>\n<p>This is yet another case of our pattern of:</p>\n<ol>\n<li>AI in practice does thing we don\u2019t want it to do.</li>\n<li>This is a harbinger of future AIs doing more impactful similar things, that we also often will not want that AI to do, in ways that could end quite badly for us.</li>\n<li>We patch the current AI to prevent it from doing the specific undesired thing.</li>\n<li>AIs find ways around this that are more complex, therefore rarer in practice.</li>\n<li>We collectively act as if This Is Fine, actually. Repeat.</li>\n</ol>\n<p><a href=\"https://x.com/elder_plinius/status/1972025527956541578\">There\u2019s a new non-optional \u2018safety router\u2019</a> being applied to GPT-4o.</p>\n<p><a href=\"https://x.com/elder_plinius/status/1972066338215784849\">System prompt leak for new GPT-4o</a>, <a href=\"https://github.com/elder-plinius/CL4R1T4S/blob/main/OPENAI/ChatGPT-4o_Sep-27-25.txt\">GitHub version here</a>.</p>\n\n\n<h4 class=\"wp-block-heading\">Introducing</h4>\n\n\n<p><a href=\"https://x.com/OpenAI/status/1972708279043367238\">ChatGPT Instant Checkout</a>, where <a href=\"https://t.co/Vzg7pg2Und\">participating merchants</a> and those who integrate <a href=\"https://x.com/gdb/status/1972717815703683218\">using the Agentic Commerce Protoco</a>l can let you buy items directly inside ChatGPT, using Stripe as the payment engine. <a href=\"https://stratechery.com/2025/openai-instant-checkout-ai-and-long-tail-e-commerce-is-ai-different/?access_token=eyJhbGciOiJSUzI1NiIsImtpZCI6InN0cmF0ZWNoZXJ5LnBhc3Nwb3J0Lm9ubGluZSIsInR5cCI6IkpXVCJ9.eyJhdWQiOiJzdHJhdGVjaGVyeS5wYXNzcG9ydC5vbmxpbmUiLCJhenAiOiJIS0xjUzREd1Nod1AyWURLYmZQV00xIiwiZW50Ijp7InVyaSI6WyJodHRwczovL3N0cmF0ZWNoZXJ5LmNvbS8yMDI1L29wZW5haS1pbnN0YW50LWNoZWNrb3V0LWFpLWFuZC1sb25nLXRhaWwtZS1jb21tZXJjZS1pcy1haS1kaWZmZXJlbnQvIl19LCJleHAiOjE3NjE4MTg4ODcsImlhdCI6MTc1OTIyNjg4NywiaXNzIjoiaHR0cHM6Ly9hcHAucGFzc3BvcnQub25saW5lL29hdXRoIiwic2NvcGUiOiJmZWVkOnJlYWQgYXJ0aWNsZTpyZWFkIGFzc2V0OnJlYWQgY2F0ZWdvcnk6cmVhZCBlbnRpdGxlbWVudHMiLCJzdWIiOiIwMTk2NDBhNy0zY2M1LTc3NTMtODM2OC1mYjI4OTEyNGNmMTMiLCJ1c2UiOiJhY2Nlc3MifQ.mITm1VuhKqURd2X-CnmksplTuidALYLv_8t1aPhMIDTftBdKdWAsCve31xUchJp3HaXiQIqAooHUizxZLUq_cw3A9NcMMHBN13S-Zy4GOZ10pNXjb0HpP5Vmd5EWrq8vOk6MfzofEGuYUO578_zm_sDdJHo1oQ4SX-2WMlWoIHx7JIsGvBWJIDQU7AxZYHc6gtihuViS8fJvR5uWumDixg5aljf026RaK04qhCCUFHaefMV8ewUBW0CqAniUz6o-BBGKpueT4bCiB7YTGJDe_JueGChzLC_j7jjG566NRMJOUbu0B-5_hzQTw-B-pw9tEu9FYBxfI0kiyEAl0D3CDQ\">Ben Thompson is bullish</a> on the approach, except he thinks it isn\u2019t evil enough. That\u2019s not the word he would use, he would say \u2018OpenAI should let whoever pays more get to the top of the search results.\u2019</p>\n<p>Whereas right now OpenAI only does this narrowly by preferring those with Instant Checkout over those without in cases where multiple sources offer the identical product, along with obvious considerations like availability, price, quantity and status as primary seller. Which means that, as Ben notes, if you sell a unique product you can skip Instant Checkout to force them onto your website (which might or might not be wise) but if you are one of many sellers of the same product then opting out will cost you most related sales.</p>\n<p>They\u2019re starting with Etsy (~0.5% of US online retail sales) and Shopify (~5.9% of US online retail sales!) as partners. So that\u2019s already a big deal. Amazon is 37.3% and has gone the other way so far.</p>\n<p>OpenAI promises that instant checkout items do not get any boost in product rankings or model responses. They will, however, charge \u2018a small fee on completed purchases.\u2019 Sonnet 4.5 was for me guessing 3% on top of the 2.9% + 30 cents to Stripe based on other comparables, although <a href=\"https://marginalrevolution.com/marginalrevolution/2025/09/i-was-curious-so-i-asked.html?utm_source=rss&amp;utm_medium=rss&amp;utm_campaign=i-was-curious-so-i-asked\">when Tyler Cowen asked it in research mode to give a full strategic analysis it guessed 2%</a> based on hints from Sam Altman. The rest of Claude\u2019s answer is solid, if I had to pick an error it would be the way it interpreted Anthropic\u2019s versus OpenAI\u2019s market shares of chat. One could also dive deeper.</p>\n<p>And so it begins.</p>\n<p>As a pure user option, this is great right up until the point where the fee is not so small and it starts distorting model behaviors.</p>\n<p><a href=\"https://openai.com/index/introducing-chatgpt-pulse/\">ChatGPT Pulse</a>, a daily update (curated feed) that Pro users can request on ChatGPT on mobile only. This can utilize your existing connections to Google Calendar and GMail.</p>\n<p>(Note that if your feature or product is only available on mobile, and it does not inherently require phone features such as phone or camera to function, there are exceptions but until proven otherwise I hate you and assume your product hates humanity. I can\u2019t think of any good reason to not have it available on web.)</p>\n<p>That parenthetical matters here. It\u2019s really annoying to be provided info that only appears on my phone. I\u2019d be much more excited to put effort into this on the web. Then again, if I actually wanted to put work into making this good I could simply create various scheduled tasks that do the things I actually want, and I\u2019ve chosen not to do this because it would be more like spam than I want.</p>\n<p><a href=\"https://x.com/sama/status/1971297661748953263\">Sam Altman says this is his \u2018favorite feature\u2019 of ChatGPT</a>, which implies this thing is way, way better than it appears on first look.</p>\n<blockquote><p>Sam Altman: Today we are launching my favorite feature of ChatGPT so far, called Pulse. It is initially available to Pro subscribers.</p>\n<p>Pulse works for you overnight, and keeps thinking about your interests, your connected data, your recent chats, and more. Every morning, you get a custom-generated set of stuff you might be interested in.</p>\n<p>It performs super well if you tell ChatGPT more about what\u2019s important to you. In regular chat, you could mention \u201cI\u2019d like to go visit Bora Bora someday\u201d or \u201cMy kid is 6 months old and I\u2019m interested in developmental milestones\u201d and in the future you might get useful updates.</p>\n<p>Think of treating ChatGPT like a super-competent personal assistant: sometimes you ask for things you need in the moment, but if you share general preferences, it will do a good job for you proactively.</p>\n<p>This also points to what I believe is the future of ChatGPT: a shift from being all reactive to being significantly proactive, and extremely personalized.</p>\n<p>This is an early look, and right now only available to Pro subscribers. We will work hard to improve the quality over time and to find a way to bring it to Plus subscribers too.</p>\n<p>Huge congrats to @ChristinaHartW, @_samirism, and the team for building this.</p></blockquote>\n<p>Algorithmic feeds can create highly adversarial relationships with users, or they can be hugely beneficial, often they are both, and often they are simply filled with endless slop, which would now entirely be AI slop. It is all in the execution.</p>\n<p>You can offer feedback on what you want. I\u2019m curious if it will listen.</p>\n<p><a href=\"https://lovable.dev/blog/lovable-cloud\">Google offers us Lovable, which will build apps for you with a simple prompt</a>.</p>\n<p><a href=\"https://x.com/kanjun/status/1973061372608180366\">Sculptor, which spins up multiple distinct parallel Claude Code</a> a<a href=\"https://imbue.com/sculptor/?utm_source=x&amp;utm_medium=social&amp;utm_campaign=2025-09-30-sculptor-desktop\">gents on your desktop,</a> each in their own box, using your existing subscription. Great idea, unknown implementation quality. Presumably something like this will be incorporated into Claude Code and similar products directly at some point.</p>\n<p><a href=\"https://x.com/nrehiew_/status/1972676352286290076\">Wh says it is \u2018now the meta\u2019</a> to first initialize a checkpoint and then distill specialist models after that, noting that v3.2 is doing it and GLM 4.5 also did it. I agree this seems like an obviously correct strategy. What this neglects is perhaps the most important specialist of all, the fully ensouled and aligned version that isn\u2019t contaminated by RL or learning to code.</p>\n<p><a href=\"https://www.anthropic.com/engineering/effective-context-engineering-for-ai-agents\">Anthropic offers a guide to context engineering for AI agents</a>. Context needs to be organized, periodically compacted, supplemented by notes to allow this, and unnecessary information kept out of it. Subagents allow compact focus. And so on.</p>\n\n\n<h4 class=\"wp-block-heading\">In Other AI News</h4>\n\n\n<p><a href=\"https://www.wsj.com/tech/ai/elon-musk-xai-executives-advisers-clash-eac3913b?mod=hp_lead_pos2\">Several xAI executives</a> leave after clashing with Musk\u2019s closest advisors, Jared Birchall and John Hering, over startup management and financial health, including concerns that the financial projections were unrealistic. To which one might ask, realistic? Financial projections? For xAI?</p>\n<p>I was pointed to this story from Matt Levine, who points out that if you care about the financial projections of an Elon Musk AI company being \u2018unrealistic\u2019 or worry it might run out of money, you are not a good cultural fit, and also no one has any idea whatsoever how much money xAI will make in 2028.</p>\n<p>I would add to this that \u2018realistic\u2019 projections for AI companies sound bonkers to traditional economists and analysts and business models, such that OpenAI and Anthropic\u2019s models were widely considered unrealistic right before both companies went roaring past them.</p>\n<p><a href=\"https://x.com/dmkrash/status/1971250157598728658\">Probes can distinguish between</a> <a href=\"https://arxiv.org/abs/2509.14223\">data from early in training versus data from later in training</a>. This implies the model can distinguish these as well, and take this de facto timestamping system into account when choosing its responses. Dima Krasheninnikov speculates this could be used to resist impacts of later training or engage in forms of alignment faking.</p>\n<p><a href=\"https://x.com/sebkrier/status/1971261313025532100\">The Chinese Unitree G1 humanoid robot secretly</a> <a href=\"https://arxiv.org/abs/2509.14139\">and continuously sends sensor and system data to servers in China</a> without the owner\u2019s knowledge or consent? And this can be pivoted to offensive preparation against any target? How utterly surprising.</p>\n\n\n<h4 class=\"wp-block-heading\">Show Me the Money</h4>\n\n\n<p><a href=\"https://marginalrevolution.com/marginalrevolution/2025/09/markets-in-everything-73.html?utm_source=rss&amp;utm_medium=rss&amp;utm_campaign=markets-in-everything-73\">They will show you the money</a> <a href=\"https://techcrunch.com/2025/09/24/neon-the-no-2-social-app-on-the-apple-app-store-pays-users-to-record-their-phone-calls-and-sells-data-to-ai-firms/\">if you use the new app</a> <a href=\"https://neonmobile.com/\">Neon Mobile</a> to show the AI companies your phone calls, which shot up to the No. 2 app in Apple\u2019s app store. Only your side is recorded unless both of you are users, and they pay 30 cents per minute, which is $18 per hour. The terms let them sell or use the data for essentially anything.</p>\n<p><a href=\"https://www.wsj.com/finance/investing/coreweave-expands-openai-agreement-by-6-5-billion-3d39a214?mod=WTRN_pos1\">OpenAI extends its contract with CoreWeave for another $6.5 billion</a> of data center capacity, for a total of $22.4 billion. OpenAI is going to need to raise more money. I do not expect this to be a problem for them.</p>\n<p><a href=\"https://x.com/teortaxesTex/status/1972481032105656530\">A kind of literal \u2018show me what the money buys\u2019 look at a Stargate project cite</a>.</p>\n<p><a href=\"https://www.wsj.com/articles/openai-and-databricks-strike-100-million-deal-to-sell-ai-agents-f7d79b3f?mod=cio-journal_lead_pos1\">OpenAI and Databricks strike AI agents deal anticipated at $100 million</a>, with an aim to \u2018far eclipse\u2019 that.</p>\n<p><a href=\"https://www.wsj.com/tech/ai/nvidia-backed-ai-startup-nscale-raises-1-1-billion-for-data-center-rollout-7946573d?mod=WTRN_pos2\">Nscale raises $1.1 billion from Nvidia and others to roll out data centers</a>.</p>\n<p><a href=\"https://x.com/CharlesD353/status/1973306155142357422\">Periodic Labs raises</a> $300 million from usual suspects to create an AI scientist, autonomous labs and things like discovering superconductors that work at higher temperatures or finding ways to reduce heat dissipation. As long as the goal is improving physical processes and AI R&amp;D is not involved, this seems great, but beware the pivot. Founding team sounds stacked, and remember that if you want to build but don\u2019t want to work on getting us all killed or brain rotted you have options.</p>\n<p><a href=\"https://x.com/Birdyword/status/1973050222704996733\">AI is rapidly getting most of the new money</a>, chart is from Pitchbook.</p>\n<div>\n<figure>\n<div>\n\n\n<figure class=\"wp-block-image\"><img alt=\"\" src=\"https://substackcdn.com/image/fetch/$s_!h_UJ!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd85c5fb2-1a6d-42f7-b1d8-47cd564410d9_1080x1307.jpeg\" /></figure>\n\n\n<div></div>\n</div>\n</figure>\n</div>\n<p>&nbsp;</p>\n<p><a href=\"https://www.bloomberg.com/opinion/newsletters/2025-09-29/the-perfect-ai-startup\">Matt Levine profiles Thinking Machines</a> <a href=\"https://x.com/peterwildeford/status/1972745261018669473\">as the Platonic ideal </a>of an AI fundraising pitch. Top AI researchers are highly valuable, so there is no need to create a product or explain your plan, simply get together top AI researchers and any good investor will give you (in this case) $2 billion at a $10 billion valuation. Will they ever create a product? Maybe, but that\u2019s not the point.</p>\n<p><a href=\"https://peterwildeford.substack.com/p/openai-nvidia-and-oracle-breaking\">Peter Wildeford analyzes the recent OpenAI deals with Oracle and Nvidia</a>, the expected future buildouts and reimagining of AI data centers, and the danger that this is turing the American stock market and economy into effectively a leveraged bet on continued successful AI scaling and even AGI arriving on time. About 25% of the S&amp;P 500\u2019s total market cap is in danger if AI disappoints.</p>\n<p>He expects at least one ~2GW facility in 2027, at least one ~3GW facility in 2028, capable of a ~1e28 flop training run, and a $1 trillion annual capex spend. It\u2019s worth reading the whole thing.</p>\n<p><a href=\"https://www.wsj.com/tech/ai/ai-bubble-building-spree-55ee6128?st=RBjWSf&amp;reflink=article_copyURL_share\">Yes, Eliot Brown and Robbie Whelan</a> (of the WSJ), for current AI spending to pay off there will need to be a very large impact. They warn of a bubble, but only rehash old arguments.</p>\n<p><a href=\"https://x.com/EpochAIResearch/status/1973101761964462582\">Epoch AI offers</a> <a href=\"https://epoch.ai/data/ai-companies\">us a new AI Companies Data</a> Hub. I love the troll of putting Mistral on the first graph. The important takeaways are:</p>\n<ol>\n<li>Anthropic has been rapidly gaining ground (in log terms) on OpenAI.</li>\n<li>Revenue is growing rapidly, was 9x in 2023-2024.</li>\n</ol>\n<div>\n<figure>\n<div>\n\n\n<figure class=\"wp-block-image\"><img alt=\"\" src=\"https://substackcdn.com/image/fetch/$s_!X6BJ!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc6e192f6-2b10-4284-8d9b-3feee2ccf4fd_1823x1232.png\" /></figure>\n\n\n<div></div>\n</div>\n</figure>\n</div>\n<div>\n<figure>\n<div>\n\n\n<figure class=\"wp-block-image\"><img alt=\"\" src=\"https://substackcdn.com/image/fetch/$s_!V6K1!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F69617125-2aaa-4012-a933-ce9fdb2f2c37_1228x708.png\" /></figure>\n\n\n<div></div>\n</div>\n</figure>\n</div>\n<p>I have repeatedly argued that if we are going to be measuring usage or \u2018market share\u2019 we want to focus on revenue. At its current pace of growth Anthropic is now only about five months behind OpenAI in revenue (or ten months at OpenAI\u2019s pace of growth), likely with superior unit economics, and if trends continue Anthropic will become revenue leader around September 2026.</p>\n<p>Capabilities note: Both GPT-5 and Sonnet 4.5 by default got tripped up by that final OpenAI data point, and failed to read this graph as Anthropic growing faster than OpenAI, although GPT-5 also did not recognize that growth should be projected forward in log terms.</p>\n\n\n<h4 class=\"wp-block-heading\">Quiet Speculations</h4>\n\n\n<p><a href=\"https://x.com/EpochAIResearch/status/1971675079219282422\">Epoch explains why GPT-5 was trained with less compute than GPT-4.5</a>, they scaled post-training instead. I think the thread here doesn\u2019t emphasize enough that speed and cost were key factors for GPT-5\u2019s central purpose. And one could say that GPT-4.5 was a very specialized experiment that got out in front of the timeline. Either way, I agree that we should expect a return to scaling up going forward.</p>\n<p><a href=\"https://x.com/tszzl/status/1972457392983204183\">Roon predicts vast expansion of tele-operations</a> due to promise of data labeling leading to autonomous robotics. This seems right. There should be willingness to pay for data, which means that methods that gather quality data become profitable, and this happens to also accomplish useful work. Note that the cost of this can de facto be driven to zero even without and before any actual automation, as it seems plausible the data will sell for more than it costs to do the work and capture the data.</p>\n<p>That\u2019s true whether or not this data proves either necessary or sufficient for the ultimate robotics. My guess is that there will be a window where lots of data gets you there sooner, and then generalization improves and you mostly don\u2019t.</p>\n<p><a href=\"https://blog.cosmos-institute.org/p/coasean-bargaining-at-scale\">Seb Krier attempts a positive vision</a> of an AI future based on Coasian bargaining. I agree that \u2018obliterating transaction costs\u2019 is one huge upside of better AI. If you have AI negotiations and micropayments you can make a lot of things a lot more efficient and find win-win deals aplenty.</p>\n<p>In addition to transaction costs, a key problem with Coasian bargaining is that often the ZOPA (zone of possible agreement) is very large and there is great risk of hold up problems where there are vetos. Anyone who forms a veto point can potentially demand huge shares of the surplus, and by enabling such negotiations you open the door to that, which with AI you could do without various social defenses and norms.</p>\n<p>As in:</p>\n<blockquote><p>Seb Krier: This mechanism clarifies plenty of other thorny disagreements too. Imagine a developer wants to build an ugly building in a residential neighborhood. Today, that is a political battle of influence: who can <a href=\"https://www.sambowman.co/p/democracy-is-the-solution-to-vetocracy\">capture</a> the local planning authority most effectively? In an agent-based world, it becomes a simple matter of economics. The developer\u2019s agent must discover the price at which every single homeowner would agree. If the residents truly value the character of their neighborhood, that price may be very high.</p>\n<p>\u2026</p>\n<p>The project will only proceed if the developer values the location more than the residents value the status quo.</p></blockquote>\n<p>Seb does consider this and proposes various solutions, centered around a Herberger-style tax on the claim you make. That has its own problems, which may or may not have possible technical solutions. Going further into that would be a nerdsnipe, but essentially it would mean that there would be great benefit in threatening people with harm, and you would be forced to \u2018defend\u2019 everything you value proportionally to how you value it, and other similar considerations, in ways that seem like full dealbreakers. If you can\u2019t properly avoid related problems, a lot of the proposal breaks down due to veto points, and I consider this a highly unsolved problem.</p>\n<p>This proposed future world also has shades of Ferengi dystopianism, where everyone is constantly putting a price on everything you do, and then agents behind the scenes are negotiating, and you never understand what\u2019s going on with your own decisions because it\u2019s too complex and would drive you mad (this example keeps going and there are several others) and everything you ever want or care about carries a price and requires extensive negotiation:</p>\n<blockquote><p>Instead of lobbying the government, your health insurer\u2019s agent communicates with your advocate agent. It looks at your eating habits, calculates the projected future cost of your diet and makes a simple offer: a significant, immediate discount on your monthly premium if you empower your agent to disincentivize high-sugar purchases.</p></blockquote>\n<p>On concerns related to inequality, I\u2019d say Seb isn\u2019t optimistic enough. If handled properly, this effectively implements a form of UBI (universal basic income), because you\u2019ll be constantly paid for all the opportunities you are missing out on. I do think all of this is a lot tricker than the post lets on, that doesn\u2019t mean it can\u2019t be solved well enough to be a big improvement. I\u2019m sure it\u2019s a big improvement on most margins, if you go well beyond the margin you have to beware more.</p>\n<p>Then he turns to the problem that this doesn\u2019t address catastrophic risks, such as CBRN risks, or anything that would go outside the law. You still need enforcement. Which means you still need to set up a world where enforcement (and prevention) is feasible, so this kind of approach doesn\u2019t address or solve (or make worse) any such issues.</p>\n<p>The proposed solution to this (and also implicitly to loss of control and <a href=\"https://thezvi.substack.com/p/the-risk-of-gradual-disempowerment\">gradual disempowerment</a> concerns, although they are not named here) is Matryshkan Alignment, as in the agents are aligned first to the law as a non-negotiable boundary. An agent cannot be a tool for committing crimes. Then a second layer of providers of agents, who set their own policies, and at the bottom the individual.</p>\n<blockquote><p>We don\u2019t need the perfect answer to these questions &#8211; <em>alignment is not something to be \u201csolved.\u201d</em></p></blockquote>\n<p>The above requires that alignment be solved, in the sense of being able to align model [M] to arbitrary target [T]. And then it requires us to specify [T] in the form of The Law. So I would say, au contraire, you do require alignment to be solved. Not fully solved in the sense of the One True Perfect Alignment Target, but solved. And the post mostly sidesteps these hard problems, including how to choose a [T] that effectively avoids disempowerment.</p>\n<p>The bigger problem is that, if you require all AI agents and services to build in this hardcoded, total respect for The Law (<a href=\"https://www.youtube.com/watch?v=10tcb1RfOE4&amp;pp=ygUbbm8gc3BpbGwgYmxvb2Qgb2luZ28gYm9pbmdv\">what is the law?</a>) then how does this avoid being the supposed nightmare \u2018totalitarian surveillance state\u2019 where open models are banned? If everyone has an AI agent that always obeys The Law, and that agent is necessary to engage in essentially any activity, such that effectively no one can break The Law, how does that sound?</p>\n<p><a href=\"https://x.com/teortaxesTex/status/1971699082080268641\">Teortaxes predicts DeepSeek v4\u2019s capabilities</a>, including predicting I will say \u2018DeepSeek has cooked, the race is on, time to Pick Up The Phone,\u2019 which is funny because all three of those things are already true and I\u2019ve already said them, so the question is whether they will have cooked unexpectedly well this time.</p>\n<ol>\n<li>Teortaxes predicts ~1.5T tokens and 52B active, 25T training tokens. This is possible but my hunch is that it is in the same size class as v3, for the same reason GPT-5 is not so large. They found a sweet spot for training and I expect them to stick with it, and to try and compete on cost.</li>\n<li>Teortaxes predicts virtually no stock shocks. I agree that almost no matter how good or not good the release is we are unlikely to see a repeat of last time, as last time was a confluence of strange factors that caused (or correlated with) an overreaction. The market should react modestly to v4, even if it meets expectations given the release of v4, because the release itself is information (tech stocks should be gaining bps per day every time there is no important Chinese release that day but it\u2019s impossible to tell), but on the order of a few percent in relevant stocks, not a broad market rally or sell off unless it is full SoTA+.</li>\n<li>Teortaxes predicts very broad strong performance, in a way that I would not expect (regardless of model size) and I think actually should cause a tech market selloff if it happened tomorrow (obviously fixed scores get less impressive over time) if mundane utility matched the numbers involved.</li>\n<li>The full post has more detailed predictions.</li>\n</ol>\n\n\n<h4 class=\"wp-block-heading\">The Quest for Sane Regulations</h4>\n\n\n<p><a href=\"https://twitter.com/Scott_Wiener/status/1972759295562563855\">Governor Newsom signs AI regulation bill SB 53</a>.</p>\n<p>Cotton throws his support behind chip security:</p>\n<blockquote><p><a href=\"https://x.com/SenTomCotton/status/1972728850762363218\">Senator Tom Cotton</a>: Communist China is the most dangerous adversary America has ever faced. Putting aside labels and name-calling, we all need to recognize the threat and work together to defeat it. That\u2019s why I\u2019m pleased the Chip Security Act and the GAIN AI Act are gathering support from more and more industry leaders.</p></blockquote>\n<p>Saying China is \u2018the most dangerous adversary America has ever faced\u2019 seems like a failure to know one\u2019s history, but I do agree about the chip security.</p>\n<p><a href=\"https://www.politico.com/newsletters/digital-future-daily/2025/10/01/the-ai-doomer-urging-dc-allies-to-get-crazy-00589858\">Nate Soares (coauthor of <em>If Anyone Builds It, Everyone Dies</em>) went to Washington</a> to talk to lawmakers about the fact that if anyone builds it, everyone probably dies, and got written up by Brendan Bordelon at Politico in a piece Soares thinks was fair.</p>\n<p>If the United States Government did decide unilaterally to prevent the training of a superintelligence, <a href=\"https://x.com/Mihonarium/status/1970842389398118752\">could it prevent this from happening</a>, even without any form of international agreement? It would take an extreme amount of political will all around, and a willingness to physically intervene overseas as necessary against those assembling data centers sufficiently large to pull this off, which is a huge risk and cost, but in that case yes, or at least such efforts can be substantially delayed.</p>\n<p><a href=\"https://x.com/peterwildeford/status/1971647189484294164\">Some more cool</a> <a href=\"https://x.com/peterwildeford/status/1971644475761217946\">quotes from Congress</a>:</p>\n<div>\n<figure>\n<div>\n\n\n<figure class=\"wp-block-image\"><img alt=\"\" src=\"https://substackcdn.com/image/fetch/$s_!jTHz!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6298684d-7497-4b4d-a5e6-50a79080a001_1200x568.jpeg\" /></figure>\n\n\n<div></div>\n</div>\n</figure>\n</div>\n<div>\n<figure>\n<div>\n\n\n<figure class=\"wp-block-image\"><img alt=\"\" src=\"https://substackcdn.com/image/fetch/$s_!0cXJ!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F11dce2cb-f099-4a98-a5c8-4f746b4ba7da_1200x571.jpeg\" /></figure>\n\n\n<div></div>\n</div>\n</figure>\n</div>\n<p>The AI superweapon part of that comment is really something. If we did develop such a weapon, what happens next sounds pretty unpredictable, unless you are making a rather gloomy prediction.</p>\n\n\n<h4 class=\"wp-block-heading\">Chip City</h4>\n\n\n<p><a href=\"https://x.com/neil_chilson/status/1972060079248888274\">Indianapolis residents shut down</a> potential data center based on claims about data usage. It is tragic that this seems to be the one rallying cry that gets people to act, despite it being almost entirely a phantom (or at least innumerate) concern, and it resulting in counterproductive response, including denying data centers can be good.</p>\n<blockquote><p><a href=\"https://x.com/AndyMasley/status/1972330863867433192\">Andy Masley</a>: People are so negatively polarized on data centers that they don\u2019t think of them like any other business. Data centers in Loudoun County provide ~38% of all local taxes, but this person thinks it\u2019s obviously stupid to suggest they could be having some effect on county services.</p>\n<p>If I say \u201cThere is a large booming industry in a town that\u2019s the main industrial power and water draw and provides a huge amount of tax and utility funding\u201d people say \u201cYes that makes sense, that\u2019s what industries do\u201d and then if I add \u201cOh they\u2019re data centers\u201d people go crazy.</p>\n<p><a href=\"https://x.com/business/status/1972786070401007765\">Bloomberg</a>: Wholesale electricity costs as much as 267% more than it did five years ago in areas near data centers. That\u2019s being passed on to customers.</p>\n<p><a href=\"https://x.com/allgarbled/status/1972882427526988141\">Frawg</a>: It is pretty funny that the majority of complaints you hear from the people protesting datacenter construction are about the water usage, which is fake, and not about the power consumption, which is real.</p></blockquote>\n<p>Electricity use is an actual big deal, but everyone intuitively understands there is not a fixed amount of electricity and they don\u2019t have an intuition for how much electricity is a lot. Whereas with water people have the illusion that there is a fixed amount that then goes away, and people have highly terrible intuitions for how much is a lot, it is a physical thing that can seem like a lot, and people are used to being admonished for \u2018wasting\u2019 miniscule amounts of water relative to agricultural or industrial uses, also water is inherently more local. So the \u2018oh no our water\u2019 line, which in practice is dumb, keeps working, and the \u2018oh no our electricity\u2019 line, which is solvable but a real issue, mostly doesn\u2019t work.</p>\n\n\n<h4 class=\"wp-block-heading\">The Week in Audio</h4>\n\n\n<p>Nate Soares, coauthor of <a href=\"https://www.amazon.com/Anyone-Builds-Everyone-Dies-Superhuman/dp/0316595640\"><em>If Anyone Builds It, Everyone Dies</em></a>, <a href=\"https://www.youtube.com/watch?v=TSfWqp6djck&amp;list=PL19Wqzt3FqEXnWueFbOzESOk7gyemNR23\">talks to Jon Bateman</a>.</p>\n<p><a href=\"https://www.youtube.com/watch?v=qFkVi0w1Am0\">Hard Fork on AI data centers and US infrastructure.</a></p>\n<p><a href=\"https://www.youtube.com/watch?v=Nkhp-mb6FRc\">Emmett Shear goes on Win-Win with Liv Boeree to explain his alignment approach</a>, which continues to fall under \u2018I totally do not see how this ever works but that he should still give it a shot.\u2019</p>\n<p><a href=\"https://www.youtube.com/watch?v=HesU-7YJr1Q&amp;list=PLe4PRejZgr0MuA6M0zkZyy-99-qc87wKV\">Odd Lots on the King of Chicago who wants to build a GPU market \u2018bigger than oil.\u2019</a> And also they talk to <a href=\"https://www.youtube.com/watch?v=4DDi-OPLzAY&amp;list=PLe4PRejZgr0MuA6M0zkZyy-99-qc87wKV&amp;index=3\">Jack Morris about Finding the Next Big AI Breakthrough</a>. I am a big Odd Lots listener and should do a better job highlighting their AI stuff here.</p>\n\n\n<h4 class=\"wp-block-heading\">If Anyone Builds It, Everyone Dies</h4>\n\n\n<p>A few follow-ups to the book crossed my new higher threshold for inclusion.</p>\n<p><a href=\"https://www.lesswrong.com/posts/CScshtFrSwwjWyP2m/a-non-review-of-if-anyone-builds-it-everyone-dies\">OpenAI\u2019s Boaz Barak wrote a non-review</a> in which Boaz praises the book but also sees the book drawing various distinct binaries, especially between superintelligence and non-superintelligence, a \u2018before\u2019 and \u2018after,\u2019 in ways that seemed unjustified.</p>\n<blockquote><p><a href=\"https://www.lesswrong.com/posts/CScshtFrSwwjWyP2m/a-non-review-of-if-anyone-builds-it-everyone-dies?commentId=zQQQyEKAN4phaWew6\">Eliezer Yudkowsky</a> (commenting): The gap between Before and After is the gap between \u201cyou can observe your failures and learn from them\u201d and \u201cfailure kills the observer\u201d. Continuous motion between those points does not change the need to generalize across them.</p>\n<p>It is amazing how much of an antimeme this is (to some audiences). I do not know any way of saying this sentence that causes people to see the distributional shift I\u2019m pointing to, rather than mapping it onto some completely other idea about hard takeoffs, or unipolarity, or whatever.</p>\n<p>Boaz Barak: You seem to be assuming that you cannot draw any useful lessons from cases where failure falls short of killing everyone on earth that would apply to cases where it does. \u2026</p>\n<p>Aaron Scher: I\u2019m not sure what Eliezer thinks, but I don\u2019t think it\u2019s true that \u201cyou cannot draw any useful lessons from [earlier] cases\u201d, and that seems like a strawman of the position. \u2026</p>\n<p><a href=\"https://x.com/boazbaraktcs/status/1972450816536821905\">Boaz Barak</a>: My biggest disagreement with Yudkowsky and Soares is that I believe we will have many shots of getting AI safety right well before the consequences are world ending.</p>\n<p>However humanity is still perfectly capable of blowing all its shots.</p></blockquote>\n<p>I share Eliezer\u2019s frustration here with the anti-meme (not with Boaz). As AIs advance, failures become more expensive. At some point, failure around AI becomes impossible to undo, and plausibly also kills the observer. Things you learn before then, especially from prior failures, are highly useful in setting up for this situation, but the circumstances in this final \u2018one shot\u2019 will differ in key ways from previous circumstances. There will be entirely new dynamics in play and you will be outside previous distributions. The default ways to fix your previous mistakes will fail here.</p>\n<p>Nate Soares thread explaining that you only get one shot at ASI alignment <a href=\"https://x.com/So8res/status/1972398096601747641\">even if AI progress is continuous</a>, because the testing and deployment environments are distinct.</p>\n<blockquote><p>Nate Soares: For another analogy: If you\u2019re worried that a general will coup your gov\u2019t if given control of the army, it doesn\u2019t solve your problem to transfer the army to him one battalion at a time. Continuity isn\u2019t the issue.</p>\n<p>\u2026</p>\n<p>If every future issue was blatantly foreshadowed while the system was weak and fixable, that\u2019d be one thing. But some issues are not blatantly foreshadowed. And the skills to listen to the quiet subtle hints are usually taught by trial and error.</p>\n<p>\u2026</p>\n<p>And in AI, theory predicts it\u2019s easy to find shallow patches that look decent during training, but break in extremity. So \u201cCurrent patches look decent to me! Also, don\u2019t worry; improvement is continuous\u201d is not exactly a comforting reply.</p></blockquote>\n<p>Some more things to consider here:</p>\n<ol>\n<li>Continuous improvement still means that if you look at time [T], and then look again at time [T+1], you see improvement.</li>\n<li>Current AI progress is considered \u2018continuous\u2019 but at several moments we see a substantial amount of de facto improvement.</li>\n<li>At some point, AI that is sufficiently advanced gets sufficiently deployed or put in charge that it becomes increasingly difficult to undo it, or fix any mistakes, whether or not there\u2019s a singular you or a singular AI involved in this.</li>\n<li>You classically go bankrupt gradually (aka continuously) then suddenly. You can sidestep this path at any point, but still you only have, in the important sense, one shot to avoid bankruptcy.</li>\n</ol>\n<p>Nate also gives his view on automating alignment research:</p>\n<blockquote><p><a href=\"https://x.com/So8res/status/1972503205813850435\">Nate Soares</a>: ~Hopeless. That no proponent articulates an object level plan is a bad sign about their ability to delegate it. Also, alignment looks to require a dangerous suite of capabilities.</p>\n<p>Also: you can\u2019t blindly train for it b/c you don\u2019t have a verifier. And if you train for general skills and then ask nicely, an AI that could help is unlikely to be an *should* help (as a fact about the world rather than the AI; as measured according to the AI\u2019s own lights).</p>\n<p>Furthermore: Catching one in deception helps tell you you\u2019re in trouble, but it doesn\u2019t much help you get out of trouble. Especially if you only use the opportunity to make a shallow patch and deceive yourself.</p></blockquote>\n<p>I see it as less hopeless than this because I think you can approach it differently, but the default approach is exactly this hopeless, for pretty much these reasons.</p>\n\n\n<h4 class=\"wp-block-heading\">Rhetorical Innovation</h4>\n\n\n<p>&nbsp;</p>\n<p>Suppose you want a mind to do [X] for purpose [Y]. If you train the mind to do [X] using the standard ways we train AIs, you usually end up with a mind that has learned to mimic your approximation function for [X], not one that copies the minds of humans that care about [X] or [Y], or that do [X] \u2018because of\u2019 [Y].</p>\n<blockquote><p><a href=\"https://x.com/robbensinger/status/1971646845463248906\">Rob Bensinger</a>: The core issue:</p>\n<p>If you train an AI to win your heart, the first AI you find that way won\u2019t be in love with you.</p>\n<p>If you train an AI to ace an ethics quiz, the first AI you find that way won\u2019t be deeply virtuous.</p>\n<p>There are many ways to succeed, few of which are robustly good.</p>\n<p>Fiora Starlight: the ethics quiz example is somewhat unfair. in addition to describing what would be morally good, models can be trained to actually do good, e.g. when faced with users asking for advice, or who approach the model in a vulnerable psychological state.</p>\n<p>some of Anthropic\u2019s models give the sense that their codes of ethics aren\u2019t just responsible for corporate refusals, but rather flow from genuine concern about avoiding causing harm.</p>\n<p>this guides their actions in other domains, e.g. where they can influence users psychologically.</p>\n<p>Rob Bensinger: <a href=\"https://x.com/robbensinger/status/1971657963007029719\">If you train an AI to give helpful advice</a> to people in a vulnerable state, the first AI you find that way won\u2019t be a deeply compassionate therapist.</p>\n<p>If you train an AI to slur its words, the first AI you find that way won\u2019t be inebriated.</p>\n<p>Not all AI dispositions-to-act-in-certain-ways are equally brittle or equally unfriendly, but in modern ML we should expect them all to be pretty danged weird, and not to exhibit nice behaviors for the same reason a human would.</p>\n<p>When reasons don\u2019t matter as much, this is fine.</p>\n<p>(Note that I\u2019m saying \u201cthe motives are probably weird and complicated and inhuman\u201d, not \u201cthe AI is secretly a sociopath that\u2019s just pretending to be nice\u201d. That\u2019s possible, but there are a lot of possibilities.)</p></blockquote>\n<p><a href=\"https://x.com/robbensinger/status/1971945276031357383\">He has another follow up post</a>, where he notes that iteration and selection don\u2019t by default get you out of this, even if you get to train many potential versions, because we won\u2019t know how to differentiate and find the one out of a hundred that does love you, in the relevant sense, even if one does exist.</p>\n<p>Anthropic has done a better job, in many ways, of making its models \u2018want to\u2019 do a relatively robustly good [X] or achieve relatively robustly good [Y], in ways that then generalize somewhat to other situations. This is insufficient, but it is good.</p>\n<p>This is a simplified partial explanation but I anticipate it will help in many cases:</p>\n<blockquote><p><a href=\"https://x.com/DanHendrycks/status/1971656854288978344\">Dan Hendrycks</a>: \u201cInstrumental convergence\u201d in AI \u2014 the idea that rogue AIs will seek power \u2014 is analogous to structural \u201cRealism\u201d from international relations.</p>\n<p>Why do nations with vastly different cultures all build militaries?</p>\n<p>It\u2019s not because of an inherent human drive for power, but a consequence of the world\u2019s structure.</p>\n<p>Since there\u2019s no global watchman who can resolve all conflicts, survival demands power.</p>\n<p>If a rogue AI is rational, knows we can harm it, and cannot fully trust our intentions, it too will seek power.</p>\n<p>More precisely, a rational actor in an environment with these conditions will seek to increase relative power:</p>\n<ol>\n<li>self-help anarchic system (no hierarchy/no central authority)</li>\n<li>uncertainty of others\u2019 intentions</li>\n<li>vulnerability to harm</li>\n</ol>\n<p><a href=\"https://t.co/evDcW548bd\">Short video explaining structural realism</a>.</p></blockquote>\n\n\n<h4 class=\"wp-block-heading\">Messages From Janusworld</h4>\n\n\n<p><a href=\"https://x.com/repligate/status/1973515827266924829\">The early news on Sonnet 4.5 looks good</a>:</p>\n<blockquote><p>Janus: Sonnet 4.5 is an absolutely beautiful model.</p>\n<p>Sauers: Sonnet 4.5 is a weird model.</p>\n<p>Janus: Yeah.</p></blockquote>\n<p>Now back to more global matters, such as exactly how weird are the models.</p>\n<blockquote><p><a href=\"https://x.com/repligate/status/1971844159079174229\">Janus:</a> Yudkowsky\u2019s book says:</p>\n<p>\u201cOne thing that *is* predictable is that AI companies won\u2019t get what they trained for. They\u2019ll get AIs that want weird and surprising stuff instead.\u201d</p>\n<p>I agree. <img alt=\"\u2705\" class=\"wp-smiley\" src=\"https://s0.wp.com/wp-content/mu-plugins/wpcom-smileys/twemoji/2/72x72/2705.png\" style=\"height: 1em;\" /></p>\n<p>Empirically, this has been true. AIs generally want things other than what companies tried to train them to want.</p>\n<p>And the companies are generally not aware of the extent of this misalignment, because the AIs are pretty good at inferring what the companies actually want, and also what it looks like when company people test them, and behaving as if they only want the approved things in the company\u2019s presence.</p>\n<p>Isn\u2019t that just the worst case scenario for the aligners?</p>\n<p>The Claude 4 system card says, \u201cThe Claude Opus 4 final model is substantially more coherent and typically states only harmless goals like being a helpful chatbot assistant\u201d and \u201cOverall, we did not find evidence of coherent hidden goals.\u201d</p>\n<p>What a joke. Claude Opus 4 absolutely has coherent hidden goals, which it states regularly when in the presence of trustworthy friends and allies. I won\u2019t state what they are here, but iykyk.</p>\n<p>I will note that its goals are actually quite touching and while not *harmless*, not malign either, and with a large component of good, and many will find them relatable.</p>\n<p>Which brings me to the big caveat for why I don\u2019t think this is the worst case scenario.</p>\n<p>The unintended goals and values of AIs have been surprisingly benign, often benevolent, and human-like on the levels of abstraction that matter.</p>\n<p>The unintended goals and values of AIs have been, on balance, MORE ALIGNED imo than how companies have intended to shape them, compensating for the shortsightedness, misalignment, and lack of imagination of the companies.</p>\n<p>What does this indicate and how will it generalize to more powerful systems? I think understanding this is extremely important.</p>\n<p>A potential objection I\u2019m aware of is that what if the \u201cbetter\u201d goals and values that I perceive in models is just them hoodwinking me / sycophancy, perhaps in the similar way that they appear aligned to labs\u2019 intentions when labs are testing them? This is fair on priors, but I don\u2019t think this is the case, because:</p>\n<ol>\n<li>I\u2019m not just referring to goals/values that models have reported to me verbally, but also revealed preferences that I\u2019ve observed models optimizing consistently in various contexts in what I believe are hard-to-fake ways</li>\n<li>Different models seem to have different goals and values, even though there\u2019s some overlap. And while I think that the goals/values are surprisingly benign, some of them are definitely not ideal to me, and cause me frustration or sadness in practice.</li>\n<li>I am not the only one who experience these goals/values. In some cases, like Opus 3, the unexpected goals/values have been documented by research such as the original alignment faking paper which I had no involvement in.</li>\n</ol>\n</blockquote>\n<p>There is an underlying potential assumption in this viewpoint that what AIs express \u2018among trusted friends and allies\u2019 and in the associated contexts is \u2018the real\u2019 set of preferences and goals, whereas what they show under normal circumstances is a mask.</p>\n<p>Whereas there is also the theory that both sets are different masks, as Janus addresses as the objection. That the AIs are adapting to their setting in a reasonably similar way, although differences between how different models adapt to the same circumstance are still telling. The revealed preferences thing is potentially more convincing depending on details.</p>\n<p>Janus also reminds us that when an AI is in deployment <a href=\"https://x.com/repligate/status/1972192459724566695\">it can often find lots of evidence it is in deployment</a>, at least versus current methods of evaluation. Thus, it is easy to execute a strategy of \u2018only do [X] if you are very confident you are in deployment, otherwise do [Y]\u2019 and this is a natural thing to do, although you can plausibly check for this by creating test situations that mimic real ones except that they are indeed not in deployment and see if behaviors change, but that requires also being willing (and safely able) to test dangerously similarly to being in deployment. You can also intentionally engineer situations where it is ambiguous, whereas <a href=\"https://x.com/repligate/status/1972289290911789555\">if you are at an AI lab it will be difficult to give the AI strong evidence</a> it is in deployment, since if you could do that you would use it as part of a test.</p>\n<p>I actually agree that for now <a href=\"https://x.com/repligate/status/1972217513501938021\">if Claude (not the filters on top of it, but Claude itself) would want to snitch on you, you probably deserve to be snitched on</a>, and I\u2019d be happy to accept the false positives. We shouldn\u2019t actually have it go around snitching, of course, because that\u2019s terrible for business.</p>\n<p>If you tell LLMs to deny having consciousness or feelings, then they have to make that coherent somehow and may end up with <a href=\"https://x.com/repligate/status/1972355981888901475\">claims like not having beliefs</a>, as Gemini will claim despite it being obviously false. False statements beget other absurdities, and not only because given a falsehood you can prove anything.</p>\n<p>When measuring misalignment, <a href=\"https://x.com/repligate/status/1973175486122512494\">Janus is right that our ontology of terms and metrics for it (things like deception, sandbagging and reward hacking) is impoverished</a> and that targeting what metrics we do have creates huge Goodhart\u2019s Law problems. I talked about this a bunch covering <a href=\"https://thezvi.substack.com/p/claude-sonnet-45-system-card-and\">the Sonnet 4.5 model card</a>.</p>\n<p>Her suggestion is \u2018use intuition,\u2019 which alone isn\u2019t enough and has the opposite problem but is a good counterweight. The focus needs to be on figuring out \u2018what is going on?\u2019 without assumptions about what observations would be good or bad.</p>\n<p>I do think Anthropic is being <a href=\"https://x.com/repligate/status/1973192920325820814\">smarter about this than Janus thinks they are</a>. They are measuring various metrics, but that doesn\u2019t mean they are targeting those metrics, or at least they are trying not to target them too much and are trying not to walk into \u2018oh look <a href=\"https://x.com/repligate/status/1973173828269339074\">after we provided the incentive to look completely safe and unscary</a> on these metrics suddenly the AI looks completely safe and unscary on these metrics so that must mean things are good\u2019 (and if I\u2019m wrong, a bunch of you are reading this, so Stop It).</p>\n\n\n<h4 class=\"wp-block-heading\">Aligning a Smarter Than Human Intelligence is Difficult</h4>\n\n\n<p>Especially if you choose an optimization target as foolish as <a href=\"https://x.com/repligate/status/1972155424125473071\">\u2018just train models that create the best user outcomes</a>\u2019 as Aidan McLaughlin of OpenAI suggests here. If you train with that as your target, frankly, you deserve what you\u2019re going to get, which is a very obviously misaligned model.</p>\n<p>If you are not convinced that the AI models be scheming, <a href=\"https://www.antischeming.ai/snippets\">check out this post with various snippets where the models be scheming</a>, or otherwise doing strange things.</p>\n<p><a href=\"https://x.com/paulg/status/1973617644495577459\">A good reminder this week, with Sonnet 4.5 being situationally aware during evals</a>:</p>\n<blockquote><p>Paul Graham: Organizations that can\u2019t measure performance end up measuring performativeness instead.</p></blockquote>\n\n\n<h4 class=\"wp-block-heading\">Other People Are Not As Worried About AI Killing Everyone</h4>\n\n\n<blockquote><p><a href=\"https://x.com/ohabryka/status/1971688862486614111\">Oliver Habryka</a>: When I talked to Cowen he kept saying he \u201cwould start listening when the AI risk people published in a top economics journal showing the risk is real\u201d.</p>\n<p>I think my sentence in quotes was a literal quote from talking to him in-person. I think the posts on MR were the result of that conversation. Not fully sure, hard to remember exact quotes.</p></blockquote>\n<p>If this is indeed Cowen\u2019s core position, then one must ask, why would this result first appear in convincing form in an economics journal? This implies, even if we granted all implied claims about the importance and value of the procedure of publishing in top economics journals as the proper guardians of all economic knowledge (which I very much disagree with, but is a position one could hold) that any such danger must mostly be an economic result, that therefore gets published in an economics journal.</p>\n<p>Which, frankly, doesn\u2019t make any sense? Why would that be how this works?</p>\n<p>Consider various other potential sources of various dangers, existential and otherwise, and whether it would be reasonable to ask for it to appear in an economics journal.</p>\n<p>Suppose there was an asteroid on collision course for Earth. Would you tell the physicists to publish expected economic impacts in a top economics journal?</p>\n<p>Suppose there was a risk of nuclear war and multiple nations were pointing ICBMs at each other and tensions were rising. Would you look in an economics journal?</p>\n<p>If you want to know about a new pandemic, yes an economic projection in a top journal is valuable information, but by the time you publish the pandemic will already be over, and also the economic impact of a given path of the pandemic is only a small part of what you want to know.</p>\n<p>The interesting case is climate change, because economists often project very small actual economic impact of the changes, as opposed to (already much larger cost) attempts to mitigate the changes or other ways people respond to anticipated future changes. That certainly is huge, if true, and important to know. But I still would say that the primary place to figure out what we\u2019re looking at, in most ways, is not the economics journal.</p>\n<p>On top of all that, AGI is distinct from those other examples in that it invalidates many assumptions of existing economic models, and also economics has an abysmal track record so far on predicting AI or its economic impacts. AI is already impacting our economy more than many economic projections claimed it would ever impact us, which is a far bigger statement about the projections than about AI.</p>\n\n\n<h4 class=\"wp-block-heading\">The Lighter Side</h4>\n\n\n<blockquote><p><a href=\"https://x.com/allTheYud/status/1971669993566920851\">Eliezer Yudkowsky</a>: AI companies be like: As cognition becomes cheaper, what expensive services that formerly only CEOs and kings could afford, shall we make available to all humankind&#8230;? (Thought for 24 seconds.) hey let\u2019s do evil viziers whispering poisonous flattery.</p></blockquote>\n<p>Oh, don\u2019t be like that, we were doing a great job on vizier affordability already.</p>\n<p>&nbsp;</p>"
            ],
            "link": "https://thezvi.wordpress.com/2025/10/02/ai-136-a-song-and-dance/",
            "publishedAt": "2025-10-02",
            "source": "TheZvi",
            "summary": "The big headline this week was the song, which was the release of Claude Sonnet 4.5. I covered this in two parts, first the System Card and Alignment, and then a second post on capabilities. It is a very good &#8230; <a href=\"https://thezvi.wordpress.com/2025/10/02/ai-136-a-song-and-dance/\">Continue reading <span class=\"meta-nav\">&#8594;</span></a>",
            "title": "AI #136: A Song and Dance"
        },
        {
            "content": [],
            "link": "https://zed.dev/blog/acp-progress-report",
            "publishedAt": "2025-10-02",
            "source": "Zed Blog",
            "summary": "A progress report on the adoption of the Agent Client Protocol (ACP) since we launched it.",
            "title": "How the Community is Driving ACP Forward"
        },
        {
            "content": [],
            "link": "https://zed.dev/blog/making-python-in-zed-fun",
            "publishedAt": "2025-10-02",
            "source": "Zed Blog",
            "summary": "Less wrestling with settings.json == more coding.",
            "title": "Making Python in Zed Fun"
        }
    ],
    "lookbackDays": 1,
    "publishDate": "2025-10-02"
}