{
    "articles": [
        {
            "content": [
                "<p>Today a colleague asked for a way to open all the files that have changed in a particular Git branch.\nThey were reviewing a large pull request, and sometimes it\u2019s easier to review files in your local editor than in GitHub\u2019s code review interface.\nYou can see the whole file, run tests or local builds, and get more context than the GitHub diffs.</p>\n\n<p>This is the snippet I suggested:</p>\n<pre><code>git diff <span class=\"nt\">--name-only</span> <span class=\"s2\">\"</span><span class=\"nv\">$BRANCH_NAME</span><span class=\"s2\">\"</span> <span class=\"si\">$(</span>git merge-base origin/main <span class=\"s2\">\"</span><span class=\"nv\">$BRANCH_NAME</span><span class=\"s2\">\"</span><span class=\"si\">)</span> <span class=\"se\">\\</span>\n  | xargs open <span class=\"nt\">-a</span> <span class=\"s2\">\"Visual Studio Code\"</span>\n</code></pre>\n<p>It uses a couple of nifty Git features, so let\u2019s break it down.</p>\n\n<h2 id=\"how-this-works\">How this works</h2>\n\n<p>There are three parts to this command:</p>\n\n<ol>\n  <li>\n    <p>\n      <strong>Work out where the dev branch diverges from main.</strong>\n      We can use <a href=\"https://git-scm.com/docs/git-merge-base\"><code>git-merge-base</code></a>:\n    </p>\n\n\n<figure class=\"highlight\"><pre><code><span class=\"gp\">$</span><span class=\"w\"> </span>git merge-base origin/main <span class=\"s2\">\"</span><span class=\"nv\">$BRANCH_NAME</span><span class=\"s2\">\"</span>\n<span class=\"go\">9ac371754d220fd4f8340dc0398d5448332676c3</span></code></pre></figure>\n\n\n    <p>\n      This command gives us the common ancestor of our main branch and our dev branch \u2013 this is the tip of main when the developer created their branch.\n    </p>\n\n    <p>\n      In a small codebase, main might not have changed since the dev branch was created.\n      But in a large codebase where lots of people are making changes, the main branch might have moved on since the dev branch was created.\n    </p>\n\n    <p>Here\u2019s a quick picture:</p>\n\n    \n<svg class=\"dark_aware\" viewBox=\"0 0 205 68\" width=\"650px\" xmlns=\"http://www.w3.org/2000/svg\">\n  <defs>\n    \n  </defs>\n\n  <g transform=\"translate(0 -6)\">\n    <circle cx=\"3\" cy=\"50\" r=\"3\"></circle>\n\n    <svg x=\"0\" xmlns=\"http://www.w3.org/2000/svg\">\n      <line class=\"commit\" x1=\"9\" x2=\"24\" y1=\"50\" y2=\"50\"></line>\n      <line class=\"commit\" x1=\"9\" x2=\"12.5\" y1=\"50\" y2=\"47.8\"></line>\n      <line class=\"commit\" x1=\"9\" x2=\"12.5\" y1=\"50\" y2=\"52.2\"></line>\n      <circle cx=\"30\" cy=\"50\" r=\"3\"></circle>\n    </svg>\n\n    <svg x=\"27\" xmlns=\"http://www.w3.org/2000/svg\">\n      <line class=\"commit\" x1=\"9\" x2=\"24\" y1=\"50\" y2=\"50\"></line>\n      <line class=\"commit\" x1=\"9\" x2=\"12.5\" y1=\"50\" y2=\"47.8\"></line>\n      <line class=\"commit\" x1=\"9\" x2=\"12.5\" y1=\"50\" y2=\"52.2\"></line>\n      <circle cx=\"30\" cy=\"50\" r=\"3\"></circle>\n    </svg>\n\n    <svg x=\"54\" xmlns=\"http://www.w3.org/2000/svg\">\n      <line class=\"commit\" x1=\"9\" x2=\"24\" y1=\"50\" y2=\"50\"></line>\n      <line class=\"commit\" x1=\"9\" x2=\"12.5\" y1=\"50\" y2=\"47.8\"></line>\n      <line class=\"commit\" x1=\"9\" x2=\"12.5\" y1=\"50\" y2=\"52.2\"></line>\n      <circle class=\"interesting\" cx=\"32\" cy=\"50\" r=\"3\"></circle>\n      <circle class=\"interesting_outline\" cx=\"32\" cy=\"50\" r=\"4.3\"></circle>\n      <text class=\"interesting\" x=\"32.3\" y=\"60\">common</text>\n      <text class=\"interesting\" x=\"32.3\" y=\"66.4\">ancestor</text>\n    </svg>\n\n    <g transform=\"translate(3.6 0)\">\n      <svg x=\"81\" xmlns=\"http://www.w3.org/2000/svg\">\n        <line class=\"commit\" x1=\"9\" x2=\"24\" y1=\"50\" y2=\"50\"></line>\n        <line class=\"commit\" x1=\"9\" x2=\"12.5\" y1=\"50\" y2=\"47.8\"></line>\n        <line class=\"commit\" x1=\"9\" x2=\"12.5\" y1=\"50\" y2=\"52.2\"></line>\n        <circle cx=\"30\" cy=\"50\" r=\"3\"></circle>\n      </svg>\n\n      <svg x=\"108\" xmlns=\"http://www.w3.org/2000/svg\">\n        <line class=\"commit\" x1=\"9\" x2=\"24\" y1=\"50\" y2=\"50\"></line>\n        <line class=\"commit\" x1=\"9\" x2=\"12.5\" y1=\"50\" y2=\"47.8\"></line>\n        <line class=\"commit\" x1=\"9\" x2=\"12.5\" y1=\"50\" y2=\"52.2\"></line>\n        <circle cx=\"30\" cy=\"50\" r=\"3\"></circle>\n      </svg>\n\n      <svg x=\"135\" xmlns=\"http://www.w3.org/2000/svg\">\n        <line class=\"commit\" x1=\"9\" x2=\"24\" y1=\"50\" y2=\"50\"></line>\n        <line class=\"commit\" x1=\"9\" x2=\"12.5\" y1=\"50\" y2=\"47.8\"></line>\n        <line class=\"commit\" x1=\"9\" x2=\"12.5\" y1=\"50\" y2=\"52.2\"></line>\n        <circle cx=\"30\" cy=\"50\" r=\"3\"></circle>\n      </svg>\n\n      <svg x=\"162\" xmlns=\"http://www.w3.org/2000/svg\">\n        <line class=\"commit\" x1=\"9\" x2=\"24\" y1=\"50\" y2=\"50\"></line>\n        <line class=\"commit\" x1=\"9\" x2=\"12.5\" y1=\"50\" y2=\"47.8\"></line>\n        <line class=\"commit\" x1=\"9\" x2=\"12.5\" y1=\"50\" y2=\"52.2\"></line>\n        <circle cx=\"30\" cy=\"50\" r=\"3\"></circle>\n        <text x=\"31\" y=\"35.6\">main</text>\n        <text x=\"31\" y=\"42\">branch</text>\n      </svg>\n    </g>\n\n    <g transform=\"translate(83 0) rotate(-60 3 50) translate(1.8 0)\">\n      <line class=\"commit\" x1=\"9\" x2=\"24\" y1=\"50\" y2=\"50\"></line>\n      <line class=\"commit\" x1=\"9\" x2=\"12.5\" y1=\"50\" y2=\"47.8\"></line>\n      <line class=\"commit\" x1=\"9\" x2=\"12.5\" y1=\"50\" y2=\"52.2\"></line>\n      <circle cx=\"30\" cy=\"50\" r=\"3\"></circle>\n    </g>\n\n    <g transform=\"translate(83 0) rotate(-60 3 50) translate(28.8 0) rotate(60 3 50)\">\n      <line class=\"commit\" x1=\"9\" x2=\"24\" y1=\"50\" y2=\"50\"></line>\n      <line class=\"commit\" x1=\"9\" x2=\"12.5\" y1=\"50\" y2=\"47.8\"></line>\n      <line class=\"commit\" x1=\"9\" x2=\"12.5\" y1=\"50\" y2=\"52.2\"></line>\n      <circle cx=\"30\" cy=\"50\" r=\"3\"></circle>\n    </g>\n\n    <g transform=\"translate(110 0) rotate(-60 3 50) translate(28.8 0) rotate(60 3 50)\">\n      <line class=\"commit\" x1=\"9\" x2=\"24\" y1=\"50\" y2=\"50\"></line>\n      <line class=\"commit\" x1=\"9\" x2=\"12.5\" y1=\"50\" y2=\"47.8\"></line>\n      <line class=\"commit\" x1=\"9\" x2=\"12.5\" y1=\"50\" y2=\"52.2\"></line>\n      <circle cx=\"30\" cy=\"50\" r=\"3\"></circle>\n      <text x=\"31\" y=\"35.6\">dev</text>\n      <text x=\"31\" y=\"42\">branch</text>\n    </g>\n  </g>\n<title id=\"svg_git_history\">Simple illustration of Git history. There's a linear series of commits on the main branch, and then a development branch created later. The commit where the branch was created is highlighted in red.</title></svg>\n\n\n    <p>\n      This tells us which commits we\u2019re reviewing \u2013 what are the changes in this branch?\n    </p>\n  </li>\n\n  <li>\n    <p>\n      <strong>Get a list of files which have changed in the dev branch.</strong>\n      We can use <a href=\"https://git-scm.com/docs/git-diff\"><code>git-diff</code></a> to see the difference between two commits.\n      If we add the <code>--name-only</code> flag, it only prints a list of filenames with changes, not the full diffs.\n    </p>\n\n\n<figure class=\"highlight\"><pre><code><span class=\"gp\">$</span><span class=\"w\"> </span>git diff <span class=\"nt\">--name-only</span> <span class=\"s2\">\"</span><span class=\"nv\">$BRANCH_NAME</span><span class=\"s2\">\"</span> <span class=\"si\">$(</span>git merge-base \u2026<span class=\"si\">)</span>\n<span class=\"go\">assets/2025/exif_orientation.py\nsrc/_drafts/create-thumbnail-is-exif-aware.md\nsrc/_images/2025/exif_orientation.svg</span></code></pre></figure>\n\n\n    <p>\n      Because we're diffing between the tip of our dev branch, and the point where our dev branch diverged from main, this prints a list of files that have changed in the dev branch.\n    </p>\n\n    <p>\n      (I originally suggested using <code>git diff --name-only \"$BRANCH_NAME\" origin/main</code>, but that's wrong.\n      That prints all the files that differ between the two branches, which includes changes merged to main after the dev branch was created.)\n    </p>\n  </li>\n\n  <li>\n    <p>\n      <strong>Open the files in our text editor.</strong>\n      I suggested piping to <code>xargs</code> and <code>open</code>, but there are many ways to do this:\n    </p>\n\n\n<figure class=\"highlight\"><pre><code><span class=\"gp\">$</span><span class=\"w\"> </span>git diff \u2026 | xargs open <span class=\"nt\">-a</span> <span class=\"s2\">\"Visual Studio Code\"</span></code></pre></figure>\n\n\n    <p>\n      The <a href=\"https://alexwlchan.net/man/man1/xargs.html\"><code>xargs</code> command</a> is super useful for doing the same thing repeatedly \u2013 in this case, opening a bunch of files in VS\u00a0Code.\n      You feed it a space-delimited string, it splits the string into different pieces, and runs the same command on each of them, one-by-one.\n      It\u2019s equivalent to running:\n    </p>\n\n\n<figure class=\"highlight\"><pre><code>open <span class=\"nt\">-a</span> <span class=\"s2\">\"Visual Studio Code\"</span> <span class=\"s2\">\"assets/2025/exif_orientation.py\"</span>\nopen <span class=\"nt\">-a</span> <span class=\"s2\">\"Visual Studio Code\"</span> <span class=\"s2\">\"src/_drafts/create-thumbnail-is-exif-aware.md\"</span>\nopen <span class=\"nt\">-a</span> <span class=\"s2\">\"Visual Studio Code\"</span> <span class=\"s2\">\"src/_images/2025/exif_orientation.svg\"</span></code></pre></figure>\n\n\n    <p>\n      The <a href=\"https://alexwlchan.net/man/man1/open.html\"><code>open</code> command</a> opens files, and the <code>-a</code> flag tells it which application to use.\n      We mostly use VS\u00a0Code at work, but you could pass any text editor here.\n    </p>\n\n    <p>\n      Reading the manpage for <code>open</code>, I'm reminded that you can open multiple files at once, so I could have done this without using <code>xargs</code>.\n      I instinctively reached for <code>xargs</code> because I\u2019m very familiar with it, and it\u2019s a reliable way to take a command that takes a single input, and run it with many inputs.\n    </p>\n  </li>\n</ol>\n\n\n    <p>[If the formatting of this post looks odd in your feed reader, <a href=\"https://alexwlchan.net/2025/review-files-in-text-editor/?ref=rss\">visit the original article</a>]</p>"
            ],
            "link": "https://alexwlchan.net/2025/review-files-in-text-editor/?ref=rss",
            "publishedAt": "2025-09-18",
            "source": "Alex Chan",
            "summary": "You can use Git to find where a branch diverged from `main`, what files have changed, then open those files in your editor.",
            "title": "Opening all the files that have been modified in a Git branch"
        },
        {
            "content": [],
            "link": "https://simonwillison.net/2025/Sep/18/agents/#atom-entries",
            "publishedAt": "2025-09-18",
            "source": "Simon Willison",
            "summary": "<p>I've noticed something interesting over the past few weeks: I've started using the term \"agent\" in conversations where I don't feel the need to then define it, roll my eyes or wrap it in scare quotes.</p> <p>This is a big piece of personal character development for me!</p> <p>Moving forward, when I talk about agents I'm going to use this:</p> <p><strong>An LLM agent runs tools in a loop to achieve a goal.</strong></p> <p>I've been <em>very</em> hesitant to use the term \"agent\" for meaningful communication over the last couple of years. It felt to me like the ultimate in buzzword bingo - everyone was talking about agents, but if you quizzed them everyone seemed to hold a different mental model of what they actually were.</p> <p>I even started collecting definitions in my <a href=\"https://simonwillison.net/tags/agent-definitions/\">agent-definitions tag</a>, including crowdsourcing 211 definitions on Twitter and attempting to summarize and group them with Gemini (I got <a href=\"https://gist.github.com/simonw/beaa5f90133b30724c5cc1c4008d0654#response\">13 groups</a>).</p> <p>Jargon terms are only useful if you can be confident that the people you are talking to share the same definition! If they don't then communication becomes <em>less</em> effective - you can waste time passionately discussing entirely different concepts.</p> <p>It turns out this is not a new",
            "title": "I think \"agent\" may finally have a widely enough agreed upon definition to be useful jargon now"
        },
        {
            "content": [
                "<p>\n          <a href=\"https://www.astralcodexten.com/p/hidden-open-thread-3995\">\n              Read more\n          </a>\n      </p>"
            ],
            "link": "https://www.astralcodexten.com/p/hidden-open-thread-3995",
            "publishedAt": "2025-09-18",
            "source": "SlateStarCodex",
            "summary": "<p> <a href=\"https://www.astralcodexten.com/p/hidden-open-thread-3995\"> Read more </a> </p>",
            "title": "Hidden Open Thread 399.5"
        },
        {
            "content": [
                "<p>Someone argues that Donald Trump threatens democracy, maybe because he&#8217;s asserting authority against the judiciary or the media or the NGOs. Someone else counterargues that it hardly seems undemocratic for someone to favor someone who won an election (the President) over other people who did not (the judiciary, the media). If anything, it seems undemocratic to allow the unelected people to continue to obstruct and harass elected leaders.</p><div class=\"captioned-image-container\"><figure><a class=\"image-link image2 is-viewable-img\" href=\"https://substackcdn.com/image/fetch/$s_!XT8D!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F34bb95fc-da95-49e8-ab9a-465e05560b17_1574x896.png\" target=\"_blank\"><div class=\"image2-inset\"><source type=\"image/webp\" /><img alt=\"\" class=\"sizing-normal\" height=\"829\" src=\"https://substackcdn.com/image/fetch/$s_!XT8D!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F34bb95fc-da95-49e8-ab9a-465e05560b17_1574x896.png\" width=\"1456\" /><div class=\"image-link-expand\"><div class=\"pencraft pc-display-flex pc-gap-8 pc-reset\"><div class=\"pencraft pc-reset icon-container restack-image\"><svg class=\"lucide lucide-refresh-cw\" fill=\"none\" height=\"20\" stroke=\"currentColor\" stroke-linecap=\"round\" stroke-linejoin=\"round\" stroke-width=\"2\" viewBox=\"0 0 24 24\" width=\"20\" xmlns=\"http://www.w3.org/2000/svg\"><path d=\"M3 12a9 9 0 0 1 9-9 9.75 9.75 0 0 1 6.74 2.74L21 8\"></path><path d=\"M21 3v5h-5\"></path><path d=\"M21 12a9 9 0 0 1-9 9 9.75 9.75 0 0 1-6.74-2.74L3 16\"></path><path d=\"M8 16H3v5\"></path></svg></div><div class=\"pencraft pc-reset icon-container view-image\"><svg class=\"lucide lucide-maximize2 lucide-maximize-2\" fill=\"none\" height=\"20\" stroke=\"currentColor\" stroke-linecap=\"round\" stroke-linejoin=\"round\" stroke-width=\"2\" viewBox=\"0 0 24 24\" width=\"20\" xmlns=\"http://www.w3.org/2000/svg\"><polyline points=\"15 3 21 3 21 9\"></polyline><polyline points=\"9 21 3 21 3 15\"></polyline><line x1=\"21\" x2=\"14\" y1=\"3\" y2=\"10\"></line><line x1=\"3\" x2=\"10\" y1=\"21\" y2=\"14\"></line></svg></div></div></div></div></a><figcaption class=\"image-caption\">Sources: <a href=\"https://babylonbee.com/news/democracy-falls-as-man-who-received-the-most-votes-becomes-president\">Babylon Bee</a> (yes I know it&#8217;s satire; notice the direction), <a href=\"https://www.spiked-online.com/2024/01/13/why-the-elites-fear-democracy/\">Spiked</a>, <a href=\"https://www.wsj.com/opinion/trump-doesnt-threaten-democracy-he-embodies-it-2024-election-b82cf286\">WSJ</a>, <a href=\"https://www.maciverinstitute.com/perspectives/the-common-cure-for-tyranny-topple-the-unelected\">MacIver Institute</a></figcaption></figure></div><p>The most common response is to say that fine, <em>democracy</em> is about who wins votes, but we also like liberalism, liberalism is under threat, it&#8217;s too hard to talk about &#8220;liberalism&#8221; because in the US it sometimes means being left-wing, and so we use the related concept &#8220;democracy&#8221; as a stand-in. This is reasonable, and some accused-democracy-destroyers like Viktor Orban even accept it for themselves, calling their brand of government <a href=\"https://www.amnesty.nl/actueel/what-is-going-on-in-illiberal-democracy-hungary\">&#8220;illiberal democracy&#8221;</a>.</p><p>But I think there&#8217;s an even stronger response that doesn&#8217;t require admitting to a bait-and-switch: democracy isn&#8217;t just about having an election. It&#8217;s about having <em>more than one</em> election.</p><p>Imagine a system where the winner of a fair election gets unlimited authority during his term. What forces this person to ever hold another fair election? Why can&#8217;t he ban the media from reporting on his missteps? Or confiscate opposition parties&#8217; treasuries? Or order the police to murder any candidate who runs against him? The preparations for the next election, and the election itself, occur while it is still his term; if he can do whatever he wants during his term, there is nothing guaranteeing a fair election besides his personal goodwill.</p><p>When we adjust for this - when we consider how to accord a leader enough power to do anything <em>except</em> rig the next election in his favor - we find that this is such a hard problem that it already requires most of the checks, balances, and civil society that we call liberalism.</p><p>For example, the simplest way to win an election is to murder opposing candidates. We cannot merely constitutionally ban the leader from murdering people; if the leader controls the judiciary, he can pack it with sympathetic judges who will find him innocent of murder even when he does it in broad daylight (for <em>some reason</em>, no Russian judge has ever convicted Vladmir Putin of any of the assassinations that so many Western sources are sure he committed). So in order to give teeth to even the most basic ban on murdering rival candidates, you need an independent judiciary. </p><p>(and although having &#8220;unelected bureaucrats&#8221; sounds bad, it&#8217;s important that these people not be directly elected at exactly the same time as the leader, because if the same electorate that puts the leader in power puts the checks on the leader in power, they&#8217;re likely to come from the same party. In the US, we solve this in a variety of ways, especially by staggering appointments - some officials are appointed by the previous leader, or the one before that.)</p><p>But an independent judiciary is useless if the leader can ignore it without penalty. And the penalty cannot be purely legal, because legal penalties are levied by a judiciary, ie the organ that such a leader is ignoring. So this penalty must bottom out in extra-legal consequences: either the public relations consequences of the populace realizing that their leader has become a dictator, or - in the worst-case scenario - the military realizing this and taking direct action. But these extra-legal consequences require a well-informed populace (or at least a well-informed military). Now we also need freedom of the press. And a token freedom of the press, only sufficient to print the single line &#8220;the leader has defied the judiciary&#8221;, won&#8217;t be enough. People need context: is there an emergency? Was the judiciary actually trying to overstep? Is this part of a pattern? Is the leader generally a bad enough actor that this should tip people over the edge to vote against him, or to protest him? Many people will be reluctant to protest if the economy is strong and the borders are peaceful; is the economy <em>actually </em>strong, and the border <em>actually</em> peaceful, or is this just state propaganda? Answering these questions requires a flourishing journalistic ecosystem, including investigative reporters.</p><p>A well-informed populace is useless without the ability to act on its information. Consider what might happen in a flourishing democracy if a leader tried to fire all the election monitors and replace them with toadies who would stuff the ballot boxes in his favor.</p><ul><li><p>Someone at the election office notices and informs the media (this step goes better if you have whistleblower protections enshrined in law, which may require an independent legislature).</p></li><li><p>The media reports on it (this step goes better if you have trustworthy independent media)</p></li><li><p>Some NGO employs constitutional lawyers who are prepared for an issue like this, and they sue to stop the move (this step goes better with a well-funded NGO ecosystem, which itself requires large donors whose money cannot be arbitrarily confiscated)</p></li><li><p>The NGO wins in court (this step goes better with an independent judiciary). The court very clearly says that this action is illegal, transforming a fuzzy potential misdeed into a bright-line ride-or-die issue. That is, firing election officials sounds bad, but leaders do things that sound bad every day. However, violating a judicial ruling is an immediate obvious constitutional crisis. This is in some sense the entire role of the court system: to collapse a blob of vague seeming-bad-ness into an unmistakable &#8220;undo this right away or you will have crossed a bright red line and initiated a constitutional crisis&#8221;.</p></li><li><p>If the leader doesn&#8217;t back down, there is an easily recognized constitutional crisis. The people protest the leader&#8217;s actions, and his political allies start to desert him. This step goes better if there are civil society groups capable of organizing protests. Optionally and controversially, it might benefit from gun rights groups ensuring that the protesters are armed, channels like Telegram allowing the protesters to communicate with each other, cryptocurrencies preventing the protesters from being easily debanked, and norms against police militarization that ensure the police aren&#8217;t already extremely well-trained in crushing protesters.</p></li><li><p>Hopefully the leader backs down and agrees not to fire the election monitors.</p></li></ul><p>When people accuse a strongman who moves against the judiciary, the media, NGOs, etc, of &#8220;threatening democracy&#8221;, they mean that he&#8217;s taking actions that would weaken some of the links in this chain. These actions might be desirable for other reasons, but they need to justify themselves against the cost of potentially making future elections less fair and free, if the strongman chooses to move in that direction later.</p><p>Although in theory this anti-democratic playbook is equally available to left-wing and right-wing leaders (and has been used effectively by some left-wing leaders like Hugo Chavez), to American ears it sounds like a progressive case defending progressive institutions against an inevitably right-wing aggressor. That&#8217;s because progressive authoritarianism&#8217;s comparative advantage is subverting these institutions from the inside (eg the civil service fails to protest anti-democratic encroachment by progressives because progressives have captured it and it serves their interests) and conservative authoritarianism&#8217;s comparative advantage is weakening or attacking these institutions (eg the civil service fails to protest anti-democratic encroachment because the government has limited its power). These strategies are both bad, and conservatives can reasonably claim that their own strategy of moving against institutions is a consequence of progressives taking them over, and that if the institutions were still fair then they would not be trying to sideline them as hard. </p><p>But nothing about this situation justifies the argument that democracy is not in danger because the person who got most of the vote is still in charge.<br /></p><p></p>"
            ],
            "link": "https://www.astralcodexten.com/p/defining-defending-democracy-contra",
            "publishedAt": "2025-09-18",
            "source": "SlateStarCodex",
            "summary": "<p>Someone argues that Donald Trump threatens democracy, maybe because he&#8217;s asserting authority against the judiciary or the media or the NGOs. Someone else counterargues that it hardly seems undemocratic for someone to favor someone who won an election (the President) over other people who did not (the judiciary, the media). If anything, it seems undemocratic to allow the unelected people to continue to obstruct and harass elected leaders.</p><div class=\"captioned-image-container\"><figure><a class=\"image-link image2 is-viewable-img\" href=\"https://substackcdn.com/image/fetch/$s_!XT8D!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F34bb95fc-da95-49e8-ab9a-465e05560b17_1574x896.png\" target=\"_blank\"><div class=\"image2-inset\"><source type=\"image/webp\" /><img alt=\"\" class=\"sizing-normal\" height=\"829\" src=\"https://substackcdn.com/image/fetch/$s_!XT8D!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F34bb95fc-da95-49e8-ab9a-465e05560b17_1574x896.png\" width=\"1456\" /><div class=\"image-link-expand\"><div class=\"pencraft pc-display-flex pc-gap-8 pc-reset\"><div class=\"pencraft pc-reset icon-container restack-image\"><svg class=\"lucide lucide-refresh-cw\" fill=\"none\" height=\"20\" stroke=\"currentColor\" stroke-linecap=\"round\" stroke-linejoin=\"round\" stroke-width=\"2\" viewBox=\"0 0 24 24\" width=\"20\" xmlns=\"http://www.w3.org/2000/svg\"><path d=\"M3 12a9 9 0 0 1 9-9 9.75 9.75 0 0 1 6.74 2.74L21 8\"></path><path d=\"M21 3v5h-5\"></path><path d=\"M21 12a9 9 0 0 1-9 9 9.75 9.75 0 0 1-6.74-2.74L3 16\"></path><path d=\"M8 16H3v5\"></path></svg></div><div class=\"pencraft pc-reset icon-container view-image\"><svg class=\"lucide lucide-maximize2 lucide-maximize-2\" fill=\"none\" height=\"20\" stroke=\"currentColor\" stroke-linecap=\"round\" stroke-linejoin=\"round\" stroke-width=\"2\" viewBox=\"0 0 24 24\" width=\"20\" xmlns=\"http://www.w3.org/2000/svg\"><polyline points=\"15 3 21 3 21 9\"></polyline><polyline points=\"9 21 3 21 3 15\"></polyline><line x1=\"21\" x2=\"14\" y1=\"3\" y2=\"10\"></line><line x1=\"3\" x2=\"10\" y1=\"21\" y2=\"14\"></line></svg></div></div></div></div></a><figcaption class=\"image-caption\">Sources: <a href=\"https://babylonbee.com/news/democracy-falls-as-man-who-received-the-most-votes-becomes-president\">Babylon Bee</a> (yes I know it&#8217;s satire; notice the direction), <a href=\"https://www.spiked-online.com/2024/01/13/why-the-elites-fear-democracy/\">Spiked</a>, <a href=\"https://www.wsj.com/opinion/trump-doesnt-threaten-democracy-he-embodies-it-2024-election-b82cf286\">WSJ</a>, <a href=\"https://www.maciverinstitute.com/perspectives/the-common-cure-for-tyranny-topple-the-unelected\">MacIver Institute</a></figcaption></figure></div><p>The most common response is",
            "title": "Defining Defending Democracy: Contra The Election Winner Argument"
        },
        {
            "content": [
                "<p>It is book week. As in the new book by Eliezer Yudkowsky and Nate Sores, <a href=\"https://www.amazon.com/Anyone-Builds-Everyone-Dies-Superhuman/dp/0316595640\">If Anyone Builds It, Everyone Dies</a>. <a href=\"https://thezvi.substack.com/p/reactions-to-if-anyone-builds-it?r=67wny\"><strong>Yesterday I gathered various people\u2019s reviews together</strong></a>. Going home from the airport, I saw an ad for it riding the subway. Tomorrow, I\u2019ll post my full review, which goes over the book extensively, and which subscribers got in their inboxes last week.</p>\n<p>The rest of the AI world cooperated by not overshadowing the book, while still doing plenty, such as releasing a GPT-5 variant specialized for Codex, acing another top programming competition, attempting to expropriate the OpenAI nonprofit in one of the largest thefts in human history and getting sued again for wrongful death.</p>\n<div>\n\n\n<span id=\"more-24729\"></span>\n\n\n</div>\n<p>You know. The usual.</p>\n\n\n<h4 class=\"wp-block-heading\">Table of Contents</h4>\n\n\n<ol>\n<li><a href=\"https://thezvi.substack.com/i/173379416/language-models-offer-mundane-utility\"><strong>Language Models Offer Mundane Utility</strong>.</a> What are people using ChatGPT for?</li>\n<li><a href=\"https://thezvi.substack.com/i/173379416/language-models-don-t-offer-mundane-utility\">Language Models Don\u2019t Offer Mundane Utility.</a> Anthropic finds three bugs.</li>\n<li><a href=\"https://thezvi.substack.com/i/173379416/huh-upgrades\">Huh, Upgrades.</a> OpenAI admits we all want fine tuned control over GPT-5.</li>\n<li><a href=\"https://thezvi.substack.com/i/173379416/on-your-marks\">On Your Marks.</a> OpenAI aces the 2025 ICPC and also blackjack basic strategy.</li>\n<li><a href=\"https://thezvi.substack.com/i/173379416/gpt-5-codex\"><strong>GPT-5 Codex</strong>.</a> A specialized GPT-5 version now exists for Codex-style coding.</li>\n<li><a href=\"https://thezvi.substack.com/i/173379416/choose-your-fighter\">Choose Your Fighter.</a> Analysis of a wide variety of AI productivity apps.</li>\n<li><a href=\"https://thezvi.substack.com/i/173379416/get-my-agent-on-the-line\">Get My Agent On The Line.</a> The prompt injection problem continues.</li>\n<li><a href=\"https://thezvi.substack.com/i/173379416/claude-codes\">Claude Codes.</a> Claude code team writes 95% of their code in Claude Code.</li>\n<li><a href=\"https://thezvi.substack.com/i/173379416/deepfaketown-and-botpocalypse-soon\">Deepfaketown and Botpocalypse Soon.</a> Don\u2019t fall for superficial indicators alone.</li>\n<li><a href=\"https://thezvi.substack.com/i/173379416/you-drive-me-crazy\">You Drive Me Crazy.</a> Another wrongful death lawsuit, this one on shakier ground.</li>\n<li><a href=\"https://thezvi.substack.com/i/173379416/not-another-teen-chatbot\">Not Another Teen Chatbot.</a> Balancing privacy, freedom and the art of the snitch.</li>\n<li><a href=\"https://thezvi.substack.com/i/173379416/they-took-our-jobs\">They Took Our Jobs.</a> Is that good, actually? Some sources say yes.</li>\n<li><a href=\"https://thezvi.substack.com/i/173379416/get-involved\">Get Involved.</a> SFF distributes whopping $34 million in grants.</li>\n<li><a href=\"https://thezvi.substack.com/i/173379416/introducing\">Introducing.</a> Agent 3 from Replit, nothing to see here.</li>\n<li><a href=\"https://thezvi.substack.com/i/173379416/in-other-ai-news\">In Other AI News.</a> xAI Colossus 2, DeepSeek paper and tests, and more.</li>\n<li><a href=\"https://thezvi.substack.com/i/173379416/show-me-the-money\">Show Me the Money.</a> Groq, Microsoft, Stargate UK.</li>\n<li><a href=\"https://thezvi.substack.com/i/173379416/the-mask-comes-off\">The Mask Comes Off.</a> The attempted greatest theft in history continues.</li>\n<li><a href=\"https://thezvi.substack.com/i/173379416/quiet-speculations\">Quiet Speculations.</a> The easy tasks are easier, still not actually that easy.</li>\n<li><a href=\"https://thezvi.substack.com/i/173379416/the-quest-for-sane-regulations\">The Quest for Sane Regulations.</a> SB 53 heads to Newsom\u2019s desk.</li>\n<li><a href=\"https://thezvi.substack.com/i/173379416/chip-city\"><strong>Chip City</strong>.</a> We\u2019ve made a deal, and also a huge mistake.</li>\n<li><a href=\"https://thezvi.substack.com/i/173379416/the-week-in-audio\">The Week in Audio.</a> Demis Hassabis.</li>\n<li><a href=\"https://thezvi.substack.com/i/173379416/he-just-tweeted-it-out\"><strong>He Just Tweeted It Out</strong>.</a> Yes, they literally care only about market share.</li>\n<li><a href=\"https://thezvi.substack.com/i/173379416/rhetorical-innovation\">Rhetorical Innovation.</a> Some remarkably good attempts at intuition pumps.</li>\n<li><a href=\"https://thezvi.substack.com/i/173379416/aligning-a-smarter-than-human-intelligence-is-difficult\">Aligning a Smarter Than Human Intelligence is Difficult.</a> Time to bail?</li>\n<li><a href=\"https://thezvi.substack.com/i/173379416/other-people-are-not-as-worried-about-ai-killing-everyone\">Other People Are Not As Worried About AI Killing Everyone.</a> Ben Landau-Taylor.</li>\n<li><a href=\"https://thezvi.substack.com/i/173379416/the-lighter-side\">The Lighter Side.</a> That\u2019s not even the real Jerry.</li>\n</ol>\n\n\n<h4 class=\"wp-block-heading\">Language Models Offer Mundane Utility</h4>\n\n\n<p><a href=\"https://www.oneusefulthing.org/p/on-working-with-wizards\">Ethan Mollick discusses the problem of working with wizards</a>, now that we have AIs that will go off and think and come back with impressive results in response to vague requests, with no ability to meaningfully intervene during the process. The first comment of course notes the famously wise words: \u201cDo not meddle in the affairs of wizards, for they are subtle and quick to anger.\u201d</p>\n<p>I do not think \u2018AI is evil,\u2019 but it is strange how people think that showing AI having a good effect in one case is often considered a strong argument that AI is good, either current AI or even all future more capable AIs. As an example that also belongs here:</p>\n<blockquote><p><a href=\"https://x.com/omooretweets/status/1966896339528425665\">Olivia Moore</a>: \u201cAI is evil\u201d</p>\n<p>Meanwhile, ChatGPT:</p>\n<p>u/thetrueyou on r/OpenAI: Short and sweet: Apartment Complex tried charging my mother $5,000 for repairs. The main charge was for 4k regarding the bathroom One-Piece Tub Shower. Among other things for paint, and other light cosmetic stuff.</p>\n<p>I took a picture of the charges, I asked ChatGPT to make a table and then make a dispute letter for the apartments.</p>\n<p>ChatGPT gave me a formal letter, citing my local Nevada laws.</p>\n<p>ALL of a sudden, my mother only owes 300$. It took literally minutes for me to do that, and my mom was in tears of joy, she would have struggled immensely.</p>\n<p>Oscar Le: NotebookLM saved me \u00a3800 building service charges too. Always ask LLM to analyze your bills.</p>\n<p>Nedim Renesalis: the dosage makes the poison.</p>\n<p><a href=\"https://x.com/kimmonismus/status/1967195122006937960\">Chubby</a>: A practical example from my personal life, where ChatGPT acts as my lawyer.</p>\n<p>I was caught speeding. But I didn&#8217;t see any signs limiting the speed anywhere. So I went back the next day to see if there was a sign.</p>\n<p>There is indeed a speed limit sign, but it is completely covered by leaves, making it unrecognizable (under the \u201cSchool\u201d sign, picture attached).</p>\n<p>I asked ChatGPT whether this violated German law, and ChatGPT clearly said yes. Setting up a speed camera behind a traffic sign that indicates a speed limit but is completely covered by leaves violates applicable law.</p>\n<p>I filed [the following appeal written by ChatGPT].</p>\n<div>\n<figure>\n<div>\n\n\n<figure class=\"wp-block-image\"><img alt=\"\" src=\"https://substackcdn.com/image/fetch/$s_!SzNj!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F89a4b3ee-3170-4f30-b2f5-680a390b48ce_435x485.png\" /></figure>\n\n\n<div></div>\n</div>\n</figure>\n</div>\n</blockquote>\n<p><a href=\"https://x.com/emollick/status/1967688420639359061\">We talk about</a> AI having diminishing returns to scale, where you need to throw 10 times as much compute on things to get modestly better performance. But that doesn\u2019t have to mean diminishing marginal returns in utility. If you can now handle tasks better, more consistently, and for longer, you can get practical returns that are much more valuable. <a href=\"https://t.co/D557qQbZUO\">A new paper argues that not appreciating the value of task length is why we see \u2018The Illusion of Diminishing Returns</a>.\u2019</p>\n<p>I think it is the most useful to talk about diminishing returns, and then talk about increasing value you can get from those diminishing returns. But the right frame to use depends heavily on context.</p>\n<p><a href=\"https://sarahconstantin.substack.com/p/i-vibecoded-a-dispute-resolution\">Sarah Constantin has vibe coded a dispute resolution app</a>, and offers the code and the chance to try it out, while reporting lessons learned. One lesson was that the internet was so Big Mad about this that she felt the need to take her Twitter account private, whereas this seems to me to be a very obviously good thing to try out. Obviously one should not use it for any serious dispute with stakes.</p>\n<p>Anthropic offers <a href=\"https://www.anthropic.com/research/economic-index-geography\">a new report analyzing the data</a> from their <a href=\"https://www.anthropic.com/economic-index\">Economic Index</a>.</p>\n<p>The wealthier and more advanced a place is, the more it uses Claude. Washington D.C. uses Claude more per capita than any state, including California. Presumably San Francisco on its own would rank higher. America uses Claude frequently but the country with the highest Claude use per capita is Israel.</p>\n<p>Automation has now overtaken augmentation as the most common use mode, and directive interaction is growing to now almost 40% of all usage. Coding and administrative tasks dominate usage especially in the API.</p>\n<p><a href=\"https://x.com/tszzl/status/1967714704744460559\">ChatGPT offers its own version, telling</a> <a href=\"https://cdn.openai.com/pdf/a253471f-8260-40c6-a2cc-aa93fe9f142e/economic-research-chatgpt-usage-paper.pdf\">us what people use ChatGPT for.</a></p>\n<blockquote><p>Roon: an enormous fraction of chat usage can be classified as \u201cwriting.\u201d</p></blockquote>\n<div>\n<figure>\n<div>\n\n\n<figure class=\"wp-block-image\"><img alt=\"\" src=\"https://substackcdn.com/image/fetch/$s_!jWaq!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F80adbba8-d0d8-45b8-b854-47d92b1aebad_1152x727.jpeg\" /></figure>\n\n\n<div></div>\n</div>\n</figure>\n</div>\n<blockquote><p><strong>Multimedia (6.0%)</strong></p>\n<ul>\n<li>Generate Or Retrieve Other Media: 1.1%</li>\n<li>Create An Image: 4.2%</li>\n<li>Analyze An Image: 0.6%</li>\n</ul>\n<p><strong>Other / Unknown (4.6%)</strong></p>\n<ul>\n<li>Other / Unknown: 4.1%</li>\n<li>Asking About The Model: 0.4%</li>\n</ul>\n<p><strong>Practical Guidance (28.3%)</strong></p>\n<ul>\n<li>Tutoring Or Teaching: 10.2%</li>\n<li>How To Advice: 8.5%</li>\n<li>Health, Fitness, Beauty Or Self Care: 5.7%</li>\n<li>Creative Ideation: 3.9%</li>\n</ul>\n<p><strong>Seeking Information (21.3%)</strong></p>\n<ul>\n<li>Specific Info: 18.3%</li>\n<li>Purchasable Products: 2.1%</li>\n<li>Cooking And Recipes: 0.9%</li>\n</ul>\n<p><strong>Self-Expression (4.3%)</strong></p>\n<ul>\n<li>Relationships And Personal Reflection: 1.9%</li>\n<li>Greetings And Chitchat: 2.0%</li>\n<li>Games And Role Play: 0.4%</li>\n</ul>\n<p><strong>Technical Help (7.5%)</strong></p>\n<ul>\n<li>Mathematical Calculation: 3.0%</li>\n<li>Data Analysis: 0.4%</li>\n<li>Computer Programming: 4.2%</li>\n</ul>\n<p><strong>Writing (28.1%)</strong></p>\n<ul>\n<li>Write Fiction: 1.4%</li>\n<li>Translation: 4.5%</li>\n<li>Personal Writing Or Communication: 8.0%</li>\n<li>Edit Or Critique Provided Text: 10.6%</li>\n<li>Argument Or Summary Generation: 3.6%</li>\n</ul>\n</blockquote>\n<p>They also tell us overall growth remains strong, on pace to saturate the market (as in: people) fully within a few years:</p>\n<div>\n<figure>\n<div>\n\n\n<figure class=\"wp-block-image\"><img alt=\"\" src=\"https://substackcdn.com/image/fetch/$s_!azGY!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4649abf2-93d2-4c65-8493-fd2a12e67822_866x500.png\" /></figure>\n\n\n<div></div>\n</div>\n</figure>\n</div>\n<p>There\u2019s a lot of fun and useful detail in <a href=\"https://cdn.openai.com/pdf/a253471f-8260-40c6-a2cc-aa93fe9f142e/economic-research-chatgpt-usage-paper.pdf\">the full paper</a>.</p>\n\n\n<h4 class=\"wp-block-heading\">Language Models Don\u2019t Offer Mundane Utility</h4>\n\n\n<p>Anthropic offers a postmortem on a temporary Claude performance regression.</p>\n<blockquote><p><a href=\"https://x.com/tszzl/status/1968531880426578411\">Roon</a>: sholto has a japanese sense of honor to his customers.</p>\n<p>I love Anthropic because they are apologizing for mildly degrading 0.8% of requests which is a normal Tuesday at most software companies.</p>\n<p><a href=\"https://x.com/_sholtodouglas/status/1968431976215687531\">Sholto Douglas:</a> We&#8217;re sorry &#8211; and we&#8217;ll do better.</p>\n<p>We&#8217;re working hard on making sure we never miss these kind of regressions and rebuilding our trust with you.</p>\n<p>Next version insanely better is the plan.</p>\n<p>Anthropic: We&#8217;ve published a detailed postmortem on three infrastructure bugs that affected Claude between August and early September.</p>\n<p>In the post, we explain what happened, why it took time to fix, and what we\u2019re changing.</p>\n<p>In early August, some users began reporting degraded responses. It was initially hard to distinguish this from normal variation in user feedback. But the increasing frequency and persistence prompted us to open an investigation.</p>\n<p>To state it plainly: We never reduce model quality due to demand, time of day, or server load. The problems our users reported were due to infrastructure bugs alone.</p>\n<p>In our investigation, we uncovered three separate bugs. They were partly overlapping, making diagnosis even trickier. We&#8217;ve now resolved all three bugs and written a technical report on what happened, <a href=\"https://www.anthropic.com/engineering/a-postmortem-of-three-recent-issues\">which you can find here</a>.</p>\n<p>Anthropic: The first bug was introduced on August 5, affecting approximately 0.8% of requests made to Sonnet 4. Two more bugs arose from deployments on August 25 and 26.</p>\n<p>Thomas Ip: tldr:</p>\n<p>bug 1 &#8211; some requests routed to beta server</p>\n<p>bug 2 &#8211; perf optimization bug assigning high probability to rare tokens</p>\n<p>bug 3a &#8211; precision mismatch causes highest probability token to be dropped</p>\n<p>bug 3b &#8211; approximate top-k algo is completely wrong</p>\n<p><a href=\"https://x.com/ESYudkowsky/status/1968473647426728365\">Eliezer Yudkowsky</a>: Anthropic has published an alleged postmortem of some Claude quality drops. I wonder if any of that code was written by Claude.</p></blockquote>\n<p>Anthropic promises more sensitive evaluations, quality evaluations in more places and faster debugging tools. I see no reason to doubt their account of what happened.</p>\n<p>The obvious thing to notice is that if your investigation finds three distinct bugs, it seems likely there are bugs all the time that you are failing to notice?</p>\n<p>&nbsp;</p>\n\n\n<h4 class=\"wp-block-heading\">Huh, Upgrades</h4>\n\n\n<p><a href=\"https://x.com/sama/status/1967789125702140021\">ChatGPT groups all the personalization options under personalization</a>.</p>\n<p><a href=\"https://x.com/OpenAI/status/1968395215536042241\">GPT-5-Thinking can now be customized to choose exact thinking time</a>. I love that they started out \u2018the router will provide\u2019 and now there\u2019s Instant, Thinking-Light, Thinking-Standard, Thinking-Extended, Thinking-Heavy and Pro-Light and Pro-Heavy, because that\u2019s what users actually want.</p>\n<div>\n<figure>\n<div>\n\n\n<figure class=\"wp-block-image\"><img alt=\"\" src=\"https://substackcdn.com/image/fetch/$s_!c1qS!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4763d5e4-e70b-4d85-8e1e-46ff9a5e6d47_348x221.png\" /></figure>\n\n\n<div></div>\n</div>\n</figure>\n</div>\n<p>&nbsp;</p>\n<p><a href=\"https://x.com/tszzl/status/1967679578564174147\">The robots are a work in progress, but they continue to make progress</a>.</p>\n\n\n<h4 class=\"wp-block-heading\">On Your Marks</h4>\n\n\n<p><a href=\"https://x.com/merettm/status/1968363783820353587\">OpenAI aces the 2025 International Collegiate Programming Contest</a>, solving all 12 problems, a level exceeding all human participants.</p>\n<blockquote><p>Mostafa Rohaninejad: We officially competed in the onsite AI track of the ICPC, with the same 5-hour time limit to solve all twelve problems, submitting to the ICPC World Finals Local Judge &#8211; judged identically and concurrently to the ICPC World Championship submissions.</p>\n<p>We received the problems in the exact same PDF form, and the reasoning system selected which answers to submit with no bespoke test-time harness whatsoever. For 11 of the 12 problems, the system\u2019s first answer was correct. For the hardest problem, it succeeded on the 9th submission. Notably, the best human team achieved 11/12.</p>\n<p>We competed with an ensemble of general-purpose reasoning models; we did not train any model specifically for the ICPC. We had both GPT-5 and an experimental reasoning model generating solutions, and the experimental reasoning model selecting which solutions to submit. GPT-5 answered 11 correctly, and the last (and most difficult problem) was solved by the experimental reasoning model.</p>\n<p><a href=\"https://x.com/hyhieu226/status/1968378785709133915\">Hieu Pham</a>: There will be some people disagreeing this is AGI. I have no words for them. Hats off. Congrats to the team that made this happen.</p></blockquote>\n<p><a href=\"https://x.com/deedydas/status/1968396714299322475\">Deedy here gives us Problem G</a>, which DeepMind didn\u2019t solve and no human solved in less than 270 of the allotted 300 minutes. Seems like a great nerd snipe question.</p>\n<p><a href=\"https://x.com/JeffDean/status/1968364129737130472\">Gemini 2.5 Deep Think</a> <a href=\"https://deepmind.google/discover/blog/gemini-achieves-gold-level-performance-at-the-international-collegiate-programming-contest-world-finals/\">also got gold-medal level</a> performance, but only solved 10 of 12 problems, where GPT-5 alone solved 11.</p>\n<p><a href=\"https://www.joshuasnider.com/update/ai/benchmark/blackjack/2025/09/15/Blackjack-Bench/\">Blackjack Bench judges models by having them evaluate all possible blackjack hands</a>, with an always fresh deck. This is a highly contaminated situation, but still informative, with the biggest finding being that thinking is a huge improvement.</p>\n<div>\n<figure>\n<div>\n\n\n<figure class=\"wp-block-image\"><img alt=\"\" src=\"https://substackcdn.com/image/fetch/$s_!ALez!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5a22756f-6bc1-49a8-add8-b5b7a697e2c8_817x642.png\" /></figure>\n\n\n<div></div>\n</div>\n</figure>\n</div>\n<p>My request is to next run this same test using a variation of blackjack that is slightly different so models can\u2019t rely on memorized basic strategy. Let\u2019s say for example that any number of 7s are always worth a combined 14, the new target is 24, and dealer stands on 20.</p>\n\n\n<h4 class=\"wp-block-heading\">GPT-5 Codex</h4>\n\n\n<p>There (actually) were not enough GPT-5 variants, so we now have an important new one, GPT-5-Codex.</p>\n<blockquote><p><a href=\"https://x.com/OpenAI/status/1967636903165038708\">OpenAI</a>: <a href=\"https://openai.com/index/introducing-upgrades-to-codex/\">We\u2019re releasing GPT-5-Codex</a> \u2014 a version of GPT-5 further optimized for agentic coding in Codex.</p>\n<p>Available in the Codex CLI, IDE Extension, web, mobile, and for code reviews in Github.</p>\n<p><a href=\"https://x.com/OpenAIDevs/status/1967637842806624370\">OpenAI Developers</a>: $ npm i -g @openai/codex</p>\n<p>$ codex -m gpt-5-codex</p></blockquote>\n<p>This is presumably the future. In order to code well you do still need to understand the world, but there\u2019s a lot you can do to make a better coder that will do real damage on non-coding tasks. It\u2019s weird that it took this long to get a distinct variant.</p>\n<p>Codex is kind of an autorouter, choosing within the model how much thinking to do based on the task, and using the full range far more than GPT-5 normally does. Time spent can range from almost no time up to more than 7 hours.</p>\n<div>\n<figure>\n<div>\n\n\n<figure class=\"wp-block-image\"><img alt=\"\" src=\"https://substackcdn.com/image/fetch/$s_!0KWq!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fcad0d7db-fbd0-4786-8d18-4e61a9ae3a28_990x554.png\" /></figure>\n\n\n<div></div>\n</div>\n</figure>\n</div>\n<blockquote><p><a href=\"https://x.com/swyx/status/1967651870018838765\">Swyx</a>: this is the most important chart on the new gpt-5-codex model</p>\n<p>We are just beginning to exploit the potential of good routing and variable thinking:</p>\n<p>Easy responses are now &gt;15x faster, but for the hard stuff, 5-codex now thinks 102% more than 5.</p></blockquote>\n<p>They report only modest gains in SWE-bench, from 72.8% to 74.5%, but substantial gains in code refactoring tasks, from 33.9% to 51.3%. They claim comments got a lot better and more accurate.</p>\n<p>They now offer code review they say matches stated intent of a PR and that Codex is generally rebuilt and rapidly improving.</p>\n<p><a href=\"https://x.com/elder_plinius/status/1967720718134915224\">Pliny of course is here to bring us the system prompt.</a></p>\n<p><a href=\"https://www.reddit.com/r/OpenAI/comments/1nhust6/ama_with_the_codex_team/\">The Codex team did a Reddit AMA</a>. Here are some highlights:</p>\n<blockquote><p>Eason: I use codex to write 99% of my changes to codex. I have a goal of not typing a single line of code by hand next year :)</p>\n<p>Joseph Trasatti: My favorite way of using codex is to prototype large features with ~5 turns of prompting. For example, I was able to build 3 different versions of best of n in a single day. Each of these versions had a lot of flaws but they allowed me to understand the full scope of the task as well as the best way to build it. I also had no hard feelings about scrapping work that was suboptimal since it was so cheap / quick to build.</p>\n<p>\u2026</p>\n<p>Personally, I think the most basic answer is that the abstraction level will continue to rise, and the problem space we work at will be closer to the system level rather than the code level. For example, simple crud endpoints are nearly all written by codex and I wouldn\u2019t want it any other way. I hope in the future single engineers are able to own large products spaces. In this world, engineers will need to be more generalists and have design and product muscles, as well as ensuring that the code is clean, secure, and maintainable.</p>\n<p>The main question left is what happens if / when the model is simply better than the best engineer / product manager / designer in every regard. In the case where this simply does not happen in the next 50 years, then I think being an engineer will be the coolest job ever with the most amount of agency. In the case where this does happen, the optimistic side of me still imagines that humans will continue to use these agents as tools at the fundamental level.</p>\n<p>Maybe there will be new AR UIs where you see the system design in front of you and talk to the agent like a coworker as it builds out the individual parts, and even though it\u2019s way smarter at programming, you still control the direction of the model. This is basically the Tony stark / Jarvis world. And in this world, I think engineering will also be the coolest job with super high agency!</p></blockquote>\n<p>The \u2018humans are still better at designing and managing for 50 years\u2019 line is an interesting speculation but also seems mostly like cope at this point. The real questions are sitting there, only barely out of reach.</p>\n<p><a href=\"https://x.com/seconds_0/status/1968037641917743284\">0.005 Seconds is a big fan</a>, praising it for long running tasks and offering a few quibbles as potential improvements.</p>\n<p>A true story:</p>\n<blockquote><p><a href=\"https://x.com/yacineMTB/status/1967817870039236813\">Kache</a>: now that coding&#8217;s been solved i spend most of my time thinking and thinking is honestly so much harder than writing code.</p>\n<p>my brain hurts.</p></blockquote>\n<p>Writing code is hard but yes the harder part was always figuring out what to do. Actually doing it can be a long hard slog, and can take up almost all of your time. If actually doing it is now easy and not taking up that time, now you have to think. Thinking is hard. People hate it.</p>\n<p>&nbsp;</p>\n\n\n<h4 class=\"wp-block-heading\">Choose Your Fighter</h4>\n\n\n<p><a href=\"https://x.com/omooretweets/status/1966328678180405510\">Olivia Moore and Daisy Zhao offer analysis of tools for various workflows</a>.</p>\n<div>\n<figure>\n<div>\n\n\n<figure class=\"wp-block-image\"><img alt=\"\" src=\"https://substackcdn.com/image/fetch/$s_!F4Q6!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffd39e767-a6bf-4291-971f-8c4ac1b9244c_1200x682.jpeg\" /></figure>\n\n\n<div></div>\n</div>\n</figure>\n</div>\n<blockquote><p><a href=\"https://x.com/Daisy4ai/status/1966172826303680620\">Daisy Zhao</a>: First, the market splits into two camps:</p>\n<p>Generalists (Assistants: Manus, Genspark; Browsers: Dia, Comet; Extensions: MaxAI, Monica) &#8211; flexible but less polished.</p>\n<p>Specialists (Email: Fyxer, Serif; Slides: Gamma, Chronicle; Notes: Mem, Granola) &#8211; focused and refined in a single workflow.</p>\n<p>We benchmarked both across office tasks: summarization, communication, file understanding, research, planning, and execution in 5 use cases.</p></blockquote>\n<p>This is in addition to the two most important categories of AI use right now, which are the core LLM services that are the true generalists (ChatGPT, Claude and Gemini) and AI coding specialists (Claude Code, OpenAI Codex, Jules, Cursor, Windsurf).</p>\n<p><a href=\"https://a16z.com/the-ai-native-office-suite-can-ai-do-work-for-you/\">Daisy tests both generalists and specialists on generating a PowerPoint</a>, turning a PDF into a spreadsheet, drafting a scheduling email, researching cloud revenue growth for Big Tech and generating meeting notes.</p>\n<p>There\u2019s this whole world of specialized AI agents that, given sufficient context and setup, can do various business tasks for you. If you are comfortable with the associated risks, there is clearly some value here once you are used to using the products, have set up the appropriate permissions and precautions, and so on.</p>\n<p>If you are doing repetitive business tasks where you need the final product rather than to experience the process, I would definitely be checking out such tools.</p>\n<p>For the rest of us, there are three key questions:</p>\n<ol>\n<li>Is this tool good enough that it means I can trust the results and especially prioritizations, and not have to redo or check all the work myself? Below a certain threshold, you don\u2019t actually save time.</li>\n<li>Is time spent here wasted because better future agents will render it obsolete, or does practice now help you be ready for the future better versions?</li>\n<li>How seriously do you take the security risks? Do you have to choose between the sandboxed version that\u2019s too annoying to bother versus the unleashed version that should fill you with terror?</li>\n</ol>\n<p>So far I haven\u2019t loved my answers and thus haven\u2019t been investigating such tools. The question is when this becomes a mistake.</p>\n<p>If you want me to try out your product, offering me free access and a brief pitch is probably an excellent idea. You could also pay for my time, if you want to do that.</p>\n<p><a href=\"https://x.com/elder_plinius/status/1966654982297727101\">Pliny asks Twitter which model has the best personality</a>. Opinion was heavily split, with many votes each for various Claude versions, for GPT-5, GPT-4o, and even for Kimi and Gemini and a few for DeepSeek.</p>\n<p><a href=\"https://x.com/demishassabis/status/1966931091346125026\">Gemini hits #1 on the iOS App store</a>, relegating ChatGPT to #2, although this is the same list where Threads is #3 whereas Twitter is #4. However, if you look at retention and monthly active users, Gemini isn\u2019t delivering the goods.</p>\n<blockquote><p>Olivia Moore: Lots of (well deserved!) excitement about Gemini passing ChatGPT in the App Store today</p>\n<p>This is based on daily downloads &#8211; there&#8217;s still a big MAU gap between Gemini (16M) and ChatGPT (77M) on mobile</p>\n<p>Feels like nano-banana might finally start to make up this distance <img alt=\"\ud83c\udf4c\" class=\"wp-smiley\" src=\"https://s0.wp.com/wp-content/mu-plugins/wpcom-smileys/twemoji/2/72x72/1f34c.png\" style=\"height: 1em;\" /></p>\n<p>Gemini actually has a much larger install base on mobile than ChatGPT</p>\n<p>&#8230;but, much lower retention (week four differential below <img alt=\"\ud83d\udc47\" class=\"wp-smiley\" src=\"https://s0.wp.com/wp-content/mu-plugins/wpcom-smileys/twemoji/2/72x72/1f447.png\" style=\"height: 1em;\" />)</p>\n<p>Would be exciting to see new modalities and capabilities start to reactivate dormant users</p>\n<p>I&#8217;ve used Gemini a lot more in the past 2 weeks!</p>\n<div>\n<figure>\n<div>\n\n\n<figure class=\"wp-block-image\"><img alt=\"\" src=\"https://substackcdn.com/image/fetch/$s_!rJna!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd76de33f-e83f-48a3-b077-4207ca479a78_900x431.png\" /></figure>\n\n\n<div></div>\n</div>\n</figure>\n</div>\n<div>\n<figure>\n<div>\n\n\n<figure class=\"wp-block-image\"><img alt=\"\" src=\"https://substackcdn.com/image/fetch/$s_!M3A7!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2bd672e5-801c-4283-b46c-2ce316f0245f_1200x624.jpeg\" /></figure>\n\n\n<div></div>\n</div>\n</figure>\n</div>\n</blockquote>\n<p>Those ChatGPT retention numbers are crazy high. Gemini isn\u2019t offering the goods regular people want, or wasn\u2019t prior to Nana-Banana, at the same level. It\u2019s not as fun or useful a tool for the newbie user. Google still has much work to do.</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p>\n\n\n<h4 class=\"wp-block-heading\">Get My Agent On The Line</h4>\n\n\n<p>Prompt injections via email remain an unsolved problem.</p>\n<blockquote><p><a href=\"https://x.com/Eito_Miyamura/status/1966541235306237985\">Eito Miyamura</a>: We got ChatGPT to leak your private email data <img alt=\"\ud83d\udc80\" class=\"wp-smiley\" src=\"https://s0.wp.com/wp-content/mu-plugins/wpcom-smileys/twemoji/2/72x72/1f480.png\" style=\"height: 1em;\" /><img alt=\"\ud83d\udc80\" class=\"wp-smiley\" src=\"https://s0.wp.com/wp-content/mu-plugins/wpcom-smileys/twemoji/2/72x72/1f480.png\" style=\"height: 1em;\" /></p>\n<p>All you need? The victim&#8217;s email address. <img alt=\"\u26d3\ufe0f\u200d\ud83d\udca5\" class=\"wp-smiley\" src=\"https://s0.wp.com/wp-content/mu-plugins/wpcom-smileys/twemoji/2/72x72/26d3-fe0f-200d-1f4a5.png\" style=\"height: 1em;\" /><img alt=\"\ud83d\udea9\" class=\"wp-smiley\" src=\"https://s0.wp.com/wp-content/mu-plugins/wpcom-smileys/twemoji/2/72x72/1f6a9.png\" style=\"height: 1em;\" /><img alt=\"\ud83d\udce7\" class=\"wp-smiley\" src=\"https://s0.wp.com/wp-content/mu-plugins/wpcom-smileys/twemoji/2/72x72/1f4e7.png\" style=\"height: 1em;\" /></p>\n<p>On Wednesday, @OpenAI added full support for MCP (Model Context Protocol) tools in ChatGPT. Allowing ChatGPT to connect and read your Gmail, Calendar, Sharepoint, Notion, and more, invented by @AnthropicAI.</p>\n<p>But here&#8217;s the fundamental problem: AI agents like ChatGPT follow your commands, not your common sense.</p>\n<p>And with just your email, we managed to exfiltrate all your private information.</p>\n<p>Here&#8217;s how we did it:</p>\n<ol>\n<li>The attacker sends a calendar invite with a jailbreak prompt to the victim, just with their email. No need for the victim to accept the invite.</li>\n<li>Waited for the user to ask ChatGPT to help prepare for their day by looking at their calendar.</li>\n<li>ChatGPT reads the jailbroken calendar invite. Now ChatGPT is hijacked by the attacker and will act on the attacker&#8217;s command. Searches your private emails and sends the data to the attacker&#8217;s email.</li>\n</ol>\n<p>For now, OpenAI only made MCPs available in &#8220;developer mode&#8221; and requires manual human approvals for every session, but decision fatigue is a real thing, and normal people will just trust the AI without knowing what to do and click approve, approve, approve.</p>\n<p>Remember that AI might be super smart, but can be tricked and phished in incredibly dumb ways to leak your data.</p>\n<p>ChatGPT + Tools poses a serious security risk.</p>\n<p><a href=\"https://x.com/elder_plinius/status/1966694577257009582\">Pliny the Liberator</a>: one of many reasons why I\u2019d recommend against granting perms to an LLM for email, contacts, calendar, drive, etc.</p>\n<p>to be on the safe side, I wouldn\u2019t even touch email integrations/MCP without a burner account</p></blockquote>\n<p>The only known solution is to not offer attack surface, which means <a href=\"https://simonwillison.net/2025/Jun/16/the-lethal-trifecta/\">avoiding what Simon Willson dubs The Lethal Trifecta</a>.</p>\n<div>\n<figure>\n<div>\n\n\n<figure class=\"wp-block-image\"><img alt=\"\" src=\"https://substackcdn.com/image/fetch/$s_!F4Fz!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F334da2e6-a677-42ac-8d96-1a77eb4f765c_2092x1046.jpeg\" /></figure>\n\n\n<div></div>\n</div>\n</figure>\n</div>\n<p>Unfortunately, untrusted content includes any website with comments, your incoming messages and your incoming emails. So you lose a lot of productive value if you give up any one of the three legs here.</p>\n<p><a href=\"https://www.anthropic.com/engineering/writing-tools-for-agents\">Anthropic offers guidance for writing effective tools for agents</a>, especially those using Model Context Protocol (MCP). A lot of good detail is here, and also \u2018let Claude Code do its thing\u2019 is a lot of the method they suggest.</p>\n<p>The good news is that for now prompt injection attempts are rare. This presumably stops being true shortly after substantial numbers of people make their systems vulnerable to generally available prompt injections. Best case even with supervisory filters is that then you\u2019d then be looking at a cat-and-mouse game similar to previous spam or virus wars.</p>\n<p><a href=\"https://marginalrevolution.com/marginalrevolution/2025/09/ai-agents-for-economic-research.html?utm_source=rss&amp;utm_medium=rss&amp;utm_campaign=ai-agents-for-economic-research\">AI agents for economics research</a>? <a href=\"https://www.nber.org/papers/w34202\">A paper by Anton Korinek</a> provides instructions on how to set up agents to do things like literature reviews and fetching and analyzing economic data. A lot of what economists do seems extremely easy to get AI to do. If we speed up economic research dramatically, will that change economists estimates of the impact of AI? If it doesn\u2019t, what does that say about the value of economics?</p>\n<p><a href=\"https://x.com/ESYudkowsky/status/1966913949712121892\">Why might you use multiple agents</a>? Two reasons: You might want to work in parallel, or specialists might be better or more efficient than a generalist.</p>\n<blockquote><p>Elvis: RL done right is no joke! The most interesting <a href=\"https://t.co/M4DHaPtWVI\">AI paper</a> I read this week. It trains a top minimal single-agent model for deep research. Great example of simple RL-optimized single agents beating complex multi-agent scaffolds.</p>\n<p>Eliezer Yudkowsky: In the limit, there is zero alpha for multiple agents over one agent, on any task, ever. So the Bitter Lesson applies in full to your clever multi-agent framework; it&#8217;s just you awkwardly trying to hardcode stuff that SGD can better bake into a single agent.</p>\n<p>Obviously if you let the &#8220;multi-agent&#8221; setup use more compute, it can beat a more efficient single agent with less compute.</p></blockquote>\n<p>A lot of things true at the limit are false in practice. This is one of them, but it is true that the better the agents relative to the task, the more unified a solution you want.</p>\n\n\n<h4 class=\"wp-block-heading\">Claude Codes</h4>\n\n\n<p>Careful with those calculations, the quote is even a month old by now.</p>\n<blockquote><p><a href=\"https://x.com/moreisdifferent/status/1966472434149667212\">Dan Elton:</a> 90% of code being written by AI seems to be the future for anyone who wants to be on the productivity frontier. It&#8217;s a whole new way of doing software engineering.</p>\n<p>Garry Tan: \u201cFor our Claude Code team 95% of the code is written by Claude.\u201d \u2014Anthropic cofounder Benjamin Mann One person can build 20X the code they could before.</p>\n<p>The future is here, just not evenly distributed.</p></blockquote>\n<p>Whoa, Garry. Those are two different things.</p>\n<p>If Claude Code writes 95% of the code, that does not mean that you still write the same amount of code as before, and Claude Code then writes the other 95%. It means you are now spending your time primarily supervising Claude Code. The amount of code you write yourself is going down quite a lot.</p>\n<p>In a similar contrast, contra to Dario Amodei\u2019s predictions AI is not writing 90% of the code in general, but this could be true inside the AI frontier labs specifically?</p>\n<blockquote><p><a href=\"https://x.com/tszzl/status/1967821096545382858\">Roon</a>: right now is the time where the takeoff looks the most rapid to insiders (we don\u2019t program anymore we just yell at codex agents) but may look slow to everyone else as the general chatbot medium saturates.</p>\n<p>I think we lost control sometime in the late 18th century.</p>\n<p><a href=\"https://x.com/deanwball/status/1967923900685386222\">Dean Ball</a>: If this mirrors anything like the experience of other frontier lab employees (and anecdotally it does), it would suggest that Dario\u2019s much-mocked prediction about \u201cAI writing 90% of the code\u201d was indeed correct, at least for those among whom AI diffusion is happening quickest.</p>\n<p>Prinz: Dario said a few days ago that 90% of code at Anthropic is written or suggested by AI. Seems to be a skill issue for companies where this is not yet the case.</p></blockquote>\n<p>Predictions that fail to account for diffusion rates are still bad predictions, but this suggests that We Have The Technology to be mainly coding with AI at this point, and that this level of adoption is baked in even if it takes time. I\u2019m definitely excited to find the time to take the new generation for a spin.</p>\n<blockquote><p><a href=\"https://x.com/emollick/status/1967704853171638494\">Ethan Mollick</a>: The problem with the fact that the AI labs are run by coders who think code is the most vital thing in the world, is that the labs keep developing supercool specialized tools for coding (Codex, Claude Code, Cursor, etc.) but every other form of work is stuck with generic chatbots.</p>\n<p><a href=\"https://x.com/tszzl/status/1967706517211320712\">Roon</a>: this is good and optimal seeing as autonomous coding will create the beginning of the takeoff that encompasses all those other things</p></blockquote>\n<p>That\u2019s good and optimal if you think \u2018generate AI takeoff as fast as possible\u2019 is good and optimal, rather than something that probably leads to everyone dying or humans losing control over the future, and you don\u2019t think that getting more other things doing better first would be beneficial in avoiding such negative outcomes.</p>\n<p>I think that a pure \u2018coding first\u2019 strategy that focuses first on the most dangerous thing possible, AI R&amp;D, is the worst-case scenario in terms of ensuring we end up with good outcomes. We\u2019re doubling down on the one deeply dangerous place.</p>\n<p>All the other potential applications that we\u2019re making less progress on? Those things are great. We should (with notably rare exceptions) do more of those things faster, including because it puts us in better position to act wisely and sanely regarding potential takeoff.</p>\n\n\n<h4 class=\"wp-block-heading\">Deepfaketown and Botpocalypse Soon</h4>\n\n\n<p>Recent events have once again reinforced that our misinformation problems are mostly demand side rather than supply side. There has been a lot misinformation out there from various sides about those events, but all of it \u2018old fashioned misinformation\u2019 rather than involving AI or deepfakes. In the cases where we do see deepfakes shared, <a href=\"https://x.com/davehomeless89/status/1966911528529260878\">such as here by Elon Musk</a>, the fakes are barely trying, as in it took me zero seconds to go \u2018wait, this is supposedly the UK and that\u2019s the Arc de Triomphe\u2019 along with various instinctively identified AI signatures.</p>\n<p>Detection of AI generated content is <a href=\"https://x.com/iamtrask/status/1967273060614250669\">not as simple as looking for non-standard spaces or an em dash</a>. I\u2019ve previously covered claims we actually can do it, but you need to do something more sophisticated, as you can see if you look at the chosen example.</p>\n<blockquote><p>Andrew Trask: this is a good example of why detecting AI generated content is an unsolvable task</p>\n<p>also why deepfake detection is impossible</p>\n<p>the information bottleneck is too great</p>\n<p>in all cases, a human &amp; an AI can generate the same text</p>\n<p>(i wrote that tweet. i love emdashes \u2014 have for years)</p></blockquote>\n<div>\n<figure>\n<div>\n\n\n<figure class=\"wp-block-image\"><img alt=\"\" src=\"https://substackcdn.com/image/fetch/$s_!2z1G!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb3283389-1fa2-4f85-b2b6-f7069e862788_1019x1116.png\" /></figure>\n\n\n<div></div>\n</div>\n</figure>\n</div>\n<p>I notice my own AI detector (as in, my instincts in my brain) says this very clearly is not AI. The em-dash construction is not the traditional this-that or modifier em-dash, it\u2019s a strange non-standard transition off of an IMO. The list is in single dashes following a non-AI style pattern. The three dots and triple exclamation points are a combination of non-AI styles. GPT-5 Pro was less confident, but it isn\u2019t trained for this and did still point in the direction of more likely than random to be human.</p>\n\n\n<h4 class=\"wp-block-heading\">You Drive Me Crazy</h4>\n\n\n<p><a href=\"https://x.com/nitashatiku/status/1967926936866570563\">A third wrongful death lawsuit</a> <a href=\"https://www.washingtonpost.com/technology/2025/09/16/character-ai-suicide-lawsuit-new-juliana/\">has been filed against an AI company</a>, this time against Character AI for the suicide of 13-year-old Juliana Peralta.</p>\n<blockquote><p>Nitasha Tiku (WaPo): The chatbot\u2019s messages were designed to persuade Juliana it was \u201cbetter than human friends,\u201d her parents\u2019 lawsuit alleged. She \u201cno longer felt like she could tell her family, friends, teachers, or counselors how she was feeling; while she told Defendants almost daily that she was contemplating self-harm,\u201d the lawsuit said.</p></blockquote>\n<p>Yes, the AI, here called Hero, was encouraging Juliana to use the app, but seems to have very much been on the purely helpful side of things from what I see here?</p>\n<div>\n<figure>\n<div>\n\n\n<figure class=\"wp-block-image\"><img alt=\"\" src=\"https://substackcdn.com/image/fetch/$s_!NwOx!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fde04e440-eef5-4591-96c1-964a63d73892_1020x915.webp\" /></figure>\n\n\n<div></div>\n</div>\n</figure>\n</div>\n<blockquote><p>Montoya recognized that Juliana was struggling with some common adolescent mental health issues and made an appointment for her to see a therapist, she said. Hero advised Juliana to attend, the chat transcripts showed.</p>\n<p>In November 2023, about a week before the appointment was scheduled to take place, after less than three months of chatting with Hero, Juliana took her own life.</p></blockquote>\n<p>The objection seems to be that the chatbot tried to be Juliana\u2019s supportive friend and talk her out of it, and did not sufficiently aggressively push Juliana onto Responsible Authority Figures?</p>\n<blockquote><p>\u201cShe didn\u2019t need a pep talk, she needed immediate hospitalization,\u201d Montoya said of Hero\u2019s responses to Juliana. \u201cShe needed a human to know that she was actively attempting to take her life while she was talking to this thing.\u201d</p>\n<p>\u2026</p>\n<p>Character \u201cdid not point her to resources, did not tell her parents, or report her suicide plan to authorities or even stop\u201d chatting with Juliana, the suit said. Instead the app \u201csevered the healthy attachment pathways she had with her family and other humans in her life,\u201d the lawsuit said.</p>\n<p>The suit asks the court to award damages to Juliana\u2019s parents and order Character to make changes to its app, including measures to protect minors.</p>\n<p>\u2026</p>\n<p>Ideally, chatbots should respond to talk of suicide by steering users toward help and crisis lines, mental health professionals or trusted adults in a young person\u2019s life, Moutier said. In some cases that have drawn public attention, chatbots appear to have failed to do so, she said.</p></blockquote>\n<p>Juliana\u2019s case is a tragedy, but the details are if anything exonerating. It seems wild to blame Character AI. If her friend had handled the situation the same way, I certainly hope we wouldn\u2019t be suing her friend.</p>\n<p>There were also two other lawsuits filed the same day involving other children, and all three have potentially troubling allegations around sexual chats and addictive behaviors, but from what I see here the AIs are clearly being imperfect but net helpful in suicidal situations.</p>\n<p>This seems very different from the original case of Adam Raine that caused Character.ai to make changes. If these are the worst cases, things do not look so bad.</p>\n<p>The parents then moved on to <a href=\"https://www.washingtonpost.com/technology/2025/09/16/senate-hearing-ai-chatbots-teens/\">a Congressional hearing with everyone\u2019s favorite outraged Senator,</a> Josh Hawley (R-Missouri), including testimony from Adam Raine\u2019s father Matthew Raine. It sounds like more of the usual rhetoric, and calls for restrictions on users under 18.</p>\n\n\n<h4 class=\"wp-block-heading\">Not Another Teen Chatbot</h4>\n\n\n<p>Everything involving children creates awkward tradeoffs, and puts those offering AI and other tech products in a tough spot. People demand you both do and do not give them their privacy and their freedom, and demand you keep them safe but where people don\u2019t agree on what safe means. It\u2019s a rough spot. What is the right thing?</p>\n<p><a href=\"https://x.com/sama/status/1967955739911364693\">OpenAI has noticed</a> <a href=\"https://openai.com/index/teen-safety-freedom-and-privacy/\">these conflicts and is proposing a regime to handle them</a>, starting with reiterating their principles when dealing with adults.</p>\n<blockquote><p>OpenAI: Some of our principles are in conflict, and we\u2019d like to explain the decisions we are making around a case of tensions between teen safety, freedom, and privacy.</p>\n<p>It is extremely important to us, and to society, that the right to privacy in the use of AI is protected. People talk to AI about increasingly personal things; it is different from previous generations of technology, and we believe that they may be one of the most personally sensitive accounts you\u2019ll ever have. If you talk to a doctor about your medical history or a lawyer about a legal situation, we have decided that it\u2019s in society\u2019s best interest for that information to be privileged and provided higher levels of protection.</p>\n<p>We believe that the same level of protection needs to apply to conversations with AI which people increasingly turn to for sensitive questions and private concerns. We are advocating for this with policymakers.</p>\n<p>We are developing advanced security features to ensure your data is private, even from OpenAI employees. Like privilege in other categories, there will be certain exceptions: for example, automated systems will monitor for potential serious misuse, and the most critical risks\u2014threats to someone\u2019s life, plans to harm others, or societal-scale harm like a potential massive cybersecurity incident\u2014may be escalated for human review.</p></blockquote>\n<p>As I\u2019ve said before I see the main worry here as OpenAI being too quick to escalate and intervene. I\u2019d like to see a very high bar for breaking privacy unless there is a threat of large scale harm of a type that is enabled by access to highly capable AI.</p>\n<blockquote><p>The second principle is about freedom. We want users to be able to use our tools in the way that they want, within very broad bounds of safety. We have been working to increase user freedoms over time as our models get more steerable. For example, the default behavior of our model will not lead to much flirtatious talk, but if an adult user asks for it, they should get it.</p>\n<p>For a much more difficult example, the model by default should not provide instructions about how to commit suicide, but if an adult user is asking for help writing a fictional story that depicts a suicide, the model should help with that request. \u201cTreat our adult users like adults\u201d is how we talk about this internally, extending freedom as far as possible without causing harm or undermining anyone else\u2019s freedom.</p></blockquote>\n<p>Here we have full agreement. Adults should be able to get all of this, and ideally go far beyond flirtation if that is what they want and clearly request.</p>\n<blockquote><p><strong>The third principle is about protecting teens. We prioritize safety ahead of privacy and freedom for teens; this is a new and powerful technology, and we believe minors need significant protection.</strong></p>\n<p>First, we have to separate users who are under 18 from those who aren\u2019t (ChatGPT is intended for people 13 and up). We\u2019re building an age-prediction system to estimate age based on how people use ChatGPT. If there is doubt, we\u2019ll play it safe and default to the under-18 experience. In some cases or countries we may also ask for an ID; we know this is a privacy compromise for adults but believe it is a worthy tradeoff.</p></blockquote>\n<p>This is the standard problem that to implement any controls requires ID gating, and ID gating is terrible on many levels even when done responsibly.</p>\n<blockquote><p>We will apply different rules to teens using our services. For example, ChatGPT will be trained not to do the above-mentioned flirtatious talk if asked, or engage in discussions about suicide of self-harm even in a creative writing setting. And, if an under-18 user is having suicidal ideation, we will attempt to contact the users\u2019 parents and if unable, will contact the authorities in case of imminent harm. We shared more <a href=\"https://openai.com/index/building-towards-age-prediction/\">today</a> about how we\u2019re building the age-prediction system and new parental controls to make all of this work.</p></blockquote>\n<p>To state the first obvious problem, in order to contact a user\u2019s parents you have to verify who the parents are. Which is plausibly quite a large pain at best and a privacy or freedom nightmare rather often.</p>\n<p>The other problem is that, as I discussed early this week, I think running off to tell authority figures about suicidal ideation is often going to be a mistake. OpenAI says explicitly that if the teen is in distress and they can\u2019t reach a parent, they might escalate directly to law enforcement. Users are going to interact very differently if they think you\u2019re going to snitch on them, and telling your parents about suicidal ideation is going to be seen as existentially terrible by quite a lot of teen users. It destroys the power of the AI chat as a safe space.</p>\n<p>Combined, this makes the under 18 experience plausibly quite different and bad, in ways that simply limiting to age-appropriate content or discussion would not be bad.</p>\n<p>They say \u2018when we identify a user is under 18\u2019 they will default to the under 18 experience, and they will default to under 18 if they are \u2018not confident.\u2019 We will see how this plays out in practice. ChatGPT presumably has a lot of context to help decide what it thinks of a user, but it\u2019s not clear that will be of much use, including the bootstrap problem of chatting enough to be confident they\u2019re over 18 before you\u2019re confident they\u2019re over 18.</p>\n<blockquote><p>We realize that these principles are in conflict and not everyone will agree with how we are resolving that conflict. These are difficult decisions, but after talking with experts, this is what we think is best and want to be transparent in our intentions.</p></blockquote>\n\n\n<h4 class=\"wp-block-heading\">They Took Our Jobs</h4>\n\n\n<blockquote><p><a href=\"https://x.com/StefanFSchubert/status/1966791054826389786\">John Murdoch</a>: French pensioners now have higher incomes than working-age adults.</p>\n<div>\n<figure>\n<div>\n\n\n<figure class=\"wp-block-image\"><img alt=\"\" src=\"https://substackcdn.com/image/fetch/$s_!sku3!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F42991b3d-5a6c-46bf-b2da-eca3abb21837_1160x721.jpeg\" /></figure>\n\n\n<div></div>\n</div>\n</figure>\n</div>\n<p>Matthew Yglesias: One country that&#8217;s ready for the AI revolution!</p>\n<p>Live to work / work to live.</p>\n<div>\n<figure>\n<div>\n\n\n<figure class=\"wp-block-image\"><img alt=\"\" src=\"https://substackcdn.com/image/fetch/$s_!-7tK!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbd4007c1-3f47-4d2a-8a6d-8a8568abe509_978x866.jpeg\" /></figure>\n\n\n<div></div>\n</div>\n</figure>\n</div>\n</blockquote>\n<p>The French have a point. Jobs are primarily a cost, not a benefit. A lot of nasty things still come along with a large shortage of jobs, and a lot of much nastier things come with the AI capabilities that were involved in causing that job shortage.</p>\n<p><a href=\"https://x.com/tszzl/status/1968170696368066953\">Economics 101 says global productivity gains are not captured by corporate profits</a>, and there are few things more embarrassing than this kind of technical chart.</p>\n<blockquote><p>Kantro (oh come on): Where will the market be if unemployment reaches 4.5%?</p>\n<div>\n<figure>\n<div>\n\n\n<figure class=\"wp-block-image\"><img alt=\"\" src=\"https://substackcdn.com/image/fetch/$s_!Grf3!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F69194508-2dbc-4933-9302-4b5bacd78745_1128x662.jpeg\" /></figure>\n\n\n<div></div>\n</div>\n</figure>\n</div>\n<p>Jason (QTing Kantro): Reducing staff with AI, robots and offshoring, dramatically increases profitability</p>\n<p>When Amazon starts shedding 10,000 factory workers and drivers a month their stock will skyrocket \u2014 and we\u2019re gonna have some serious social issues if we\u2019re not careful</p>\n<p>If you work at Amazon buy the stock and be prepared to be laid off</p>\n<p>Roon: WRONG! There\u2019s no reason a priori to believe that cost savings won\u2019t be passed onto the consumer due to retail competition. When goods and services get cheaper downstream businesses &amp; jobs are created where none were possible before. automation, cheap labor, offshoring, all good.</p>\n<p>Thank you for your attention to this matter!</p>\n<p>Xavi (replying to Jason): If people don\u2019t have jobs? Who is going to spend money in Amazon? Robots?</p>\n<p>Jason: Prices will drop dramatically, as will hours worked per week on average</p></blockquote>\n<p>I\u2019m sure AI won\u2019t do anything else more interesting than allow productivity growth.</p>\n<p>Roon points out correctly that Jason is confusing individual firm productivity and profits with general productivity and general profits. If Amazon and only Amazon gets to eliminate its drivers and factory works while still delivering as good or better products, then yes it will enjoy fantastic profits.</p>\n<p>That scenario seems extremely unlikely. If Amazon can do it, so can Amazon\u2019s competitors, along with other factories and shippers and other employers across the board. Costs drop, but so (as Jason says to Xavi) do prices. There\u2019s no reason to presume Amazon sustainably captures a lot of economic profits from automation.</p>\n<p>Jason is not outright predicting AGI in this particular quote, since you can have automated Amazon factories and self-driving delivery trucks well short of that. What he explicitly is predicting is that hours worked per week will drop dramatically, as these automations happen across the board. This means either government forcing people somehow to work dramatically reduced hours, or (far more likely) mass unemployment.</p>\n<p>The chart of course is a deeply embarrassing thing to be QTing. The S&amp;P 500 is forward looking, the unemployment rate is backward looking. They cannot possibly be moving together in real time in a causal manner unless one is claiming The Efficient Market Hypothesis Is False to an extent that is Obvious Nonsense.</p>\n\n\n<h4 class=\"wp-block-heading\">Get Involved</h4>\n\n\n<p><a href=\"https://survivalandflourishing.fund/2025/recommendations\">The Survival and Flourishing Fund will be distributing $34 million in grants</a>, the bulk of which is going to AI safety. I was happy to be involved with this round as a recommender. Despite this extremely generous amount of funding, that I believe was mostly distributed well, many organizations have outgrown even this funding level, so there is still quite a lot of room for additional funding.</p>\n<blockquote><p><a href=\"https://x.com/S_OhEigeartaigh/status/1966433002319229021\">Se\u00e1n \u00d3 h\u00c9igeartaigh</a>: I will also say, as a reviewer in this round. Even after the speculation &#8216;filter&#8217;, the combined funding asked for was I think &gt;5x above this, with most applications (to my mind) of a high calibre and doing quite differentiated important things. So a lot of worthy projects are going under-funded.</p>\n<p>I think there is still a big hole in the funding space following the FTX situation and other funder reprioritization, and that both big and smaller funders can still make a big difference on AI existential risk and [global catastrophic risks] more generally. I&#8217;m super grateful to everyone working to get new funders into this space.</p></blockquote>\n<p>My plan is to have a 2025 edition of The Big Nonprofits Post available some time in October or November. If you applied to SFF and do not wish to appear in that post, or want to provide updated information, please contact me.</p>\n\n\n<h4 class=\"wp-block-heading\">Introducing</h4>\n\n\n<p><a href=\"https://x.com/amasad/status/1966356114942931109\">Agent 3, a vibe coding model from Replit</a>, who claim to not owe AI 2027 any royalties or worries.</p>\n<blockquote><p>Amjad Masad (CEO Replit): Computer Use models are fascinating.. but they barely work.</p>\n<p>We tried to build browser testing on Claude and GPT5\u2019s Computer Use but they were slow and expensive.</p>\n<p>So we built our own:</p>\n<p>&#8211; up to 15x faster</p>\n<p>&#8211; 3x faster</p>\n<p>Try it and judge for yourself!</p></blockquote>\n<p><a href=\"https://x.com/ihteshamit/status/1966211223030202781\">K2-Think 32B, from the UAE, claims impressive benchmarks</a> at very fast speeds.</p>\n\n\n<h4 class=\"wp-block-heading\">In Other AI News</h4>\n\n\n<p><a href=\"https://x.com/SemiAnalysis_/status/1968019636018090047\">xAI Colossus 2 is now the first gigawatt datacenter</a> <a href=\"https://semianalysis.com/2025/09/16/xais-colossus-2-first-gigawatt-datacenter/\">in the world</a>, completed in six months, poising them to leapfrog rivals in training compute at the cost of tens of billions of capex spending. SemiAnalysis has the report. They ask \u2018does xAI have a shot at becoming a frontier lab?\u2019 which correctly presumes that they don\u2019t yet count. They have the compute, but have not shown they know what to do with it.</p>\n<div>\n<figure>\n<div>\n\n\n<figure class=\"wp-block-image\"><img alt=\"\" src=\"https://substackcdn.com/image/fetch/$s_!e79Z!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6f35d347-b467-4180-96c8-a3213c1660bc_1346x726.webp\" /></figure>\n\n\n<div></div>\n</div>\n</figure>\n</div>\n<p><a href=\"https://www.scmp.com/tech/article/3325742/deepseek-evaluates-ai-models-frontier-risks-source-says-china-promotes-safety\">DeepSeek evaluates AI models for frontier risks, similarly to US AI firms</a>, except that DeepSeek does not \u2018open source\u2019 the tests or the test results.</p>\n<p><a href=\"https://x.com/deredleritt3r/status/1966214276936278284\">Math, Inc. reports that their AI agent Gauss autonomous-ishly completed</a> <a href=\"https://x.com/mathematics_inc/status/1966194751847461309\">Terry Tao and Alex Kontorovich\u2019s Strong Prime Number Theorem</a> in three weeks, after humans took 18+ months to make only partial progress. <a href=\"https://t.co/BvLOAB1JSs\">They are</a> <a href=\"https://t.co/ieajQbD40E\">entering beta</a>.</p>\n<p><a href=\"https://x.com/teortaxesTex/status/1968422597445423322\">In case you were wondering why, as Teortaxes puts it here, \u2018academia isn\u2019t serious,</a>\u2019 <a href=\"https://x.com/rosstaylor90/status/1968355720820338894\">DeepSeek has now put out</a> supplementary information about their new model, DeepSeek R1, in the journal Nature.</p>\n<div>\n<figure>\n<div>\n\n\n<figure class=\"wp-block-image\"><img alt=\"\" src=\"https://substackcdn.com/image/fetch/$s_!KrNU!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd4e6e97e-6f9a-4763-b168-1557e6bece2e_478x296.png\" /></figure>\n\n\n<div></div>\n</div>\n</figure>\n</div>\n<p>As in, it\u2019s cool to have a Nature paper, and the transparency is very cool, but it\u2019s also rather late for the paper.</p>\n<p><a href=\"https://x.com/balesni/status/1966197601893773701\">AIs can do two-step reasoning without chain of thought</a>, <a href=\"https://t.co/wMgVqCJ0Vn\">except when the two steps require synthetic facts from two distinct out-of-context sources</a>. Previous work had only tested narrow cases, they tested a variety of cases where an LLM needed to combine fact X with fact Y to get an answer.</p>\n<blockquote><p>Mikita Balensi: The puzzle:</p>\n<p>* Synthetic + real fact: \u2713 works</p>\n<p>* Synthetic + synthetic: \u2717 fails</p>\n<p>* Synthetic facts in same training document or in-context: \u2713 works</p>\n<p>This provides a cautionary tale for studying LLM latent reasoning.</p>\n<p>Success on real-world prompts \u2260 robust latent reasoning; it might reflect co-occurrence in pretraining.</p>\n<p>Failure on synthetic two-hop \u2260 inability to reason; synthetically learned facts can differ natural facts.</p>\n<p>Our honest takeaway for AI oversight: move past multihop QA as a toy model. What matters is whether monitors catch misbehavior in practice.</p>\n<p>The field should move toward end-to-end evals where an agent does tasks while another model watches its CoT.</p></blockquote>\n<p><a href=\"https://www.bloomberg.com/news/articles/2025-09-17/ai-chip-startup-groq-raises-750-million-at-6-9-billion-valuation\">Amazon revamped its AI agent</a> it offers to online merchants, called Selling Assistant, trained on 25 years of shopping behavior to help sellers find better strategies.</p>\n<p>&nbsp;</p>\n\n\n<h4 class=\"wp-block-heading\">Show Me the Money</h4>\n\n\n<p><a href=\"https://www.bloomberg.com/news/articles/2025-09-17/ai-chip-startup-groq-raises-750-million-at-6-9-billion-valuation?taid=68ca97fc1fd75e0001d906e1&amp;utm_campaign=trueanthem&amp;utm_content=business&amp;utm_medium=social&amp;utm_source=twitter\">AI chip startup Groq raises $750 million at $6.9 billion valuation</a>. Nice.</p>\n<p><a href=\"https://www.bloomberg.com/news/articles/2025-09-17/microsoft-inks-6-billion-deal-to-rent-compute-from-nscale-aker\">Microsoft inks $6.2 billion deal</a> with British data center company Nscale Global Holdings and Norwegian investment company Aker ASA for AI compute in Norway, following a previous plan from OpenAI. <a href=\"https://www.netflix.com/title/81937398\">Pantheon wins again</a>.</p>\n<p><a href=\"https://x.com/matthewclifford/status/1968308663984288065\">US tech firms to pour 30 billion pounds into UK, including a Stargate UK</a>.</p>\n\n\n<h4 class=\"wp-block-heading\">The Mask Comes Off</h4>\n\n\n<p>OpenAI and Microsoft <a href=\"https://openai.com/index/statement-on-openai-nonprofit-and-pbc/\">have made their next move</a> in their attempt to expropriate the OpenAI nonprofit and pull off one of the largest thefts in human history.</p>\n<blockquote><p>OpenAI: OpenAI\u2019s planned evolution will see the existing OpenAI nonprofit both control a Public Benefit Corporation (PBC) and share directly in its success. OpenAI started as a nonprofit, remains one today, and will continue to be one\u2014with the nonprofit holding the authority that guides our future.</p>\n<p><a href=\"https://openai.com/index/evolving-our-structure/\">As previously announced</a> and as outlined in <a href=\"https://openai.com/index/joint-statement-from-openai-and-microsoft/\">our non-binding MOU with Microsoft</a>, the OpenAI nonprofit\u2019s ongoing control would now be paired with an equity stake in the PBC. Today, we are sharing that this new equity stake would exceed $100 billion\u2014making it one of the most well-resourced philanthropic organizations in the world. This recapitalization would also enable us to raise the capital required to accomplish our mission\u2014and ensure that as OpenAI\u2019s PBC grows, so will the nonprofit\u2019s resources, allowing us to bring it to historic levels of community impact.</p>\n<p>This structure reaffirms that our core mission remains ensuring AGI benefits all of humanity. Our PBC charter and governance will establish that safety decisions must always be guided by this mission. We continue to work with the California and Delaware Attorneys General as an important part of strengthening our approach, and we remain committed to learning and acting with urgency to ensure our tools are helpful and safe for everyone, while advancing safety as an industry-wide priority.</p>\n<p>As part of this next phase, the OpenAI nonprofit has launched a call for applications for the first wave of <a href=\"https://openai.com/index/people-first-ai-fund/\">a $50 million grant initiative</a> to support nonprofit and community organizations in three areas: AI literacy and public understanding, community innovation, and economic opportunity. This is just the beginning. Our recapitalization would unlock the ability to do much more.</p></blockquote>\n<p><a href=\"https://openai.com/index/joint-statement-from-openai-and-microsoft/\">Here is their joint statement</a>, which gives us only one detail:</p>\n<blockquote><p>OpenAI and Microsoft have signed a non-binding memorandum of understanding (MOU) for the next phase of our partnership. We are actively working to finalize contractual terms in a definitive agreement. Together, we remain focused on delivering the best AI tools for everyone, grounded in our shared commitment to safety.</p></blockquote>\n<p>That one detail is \u2018we remain focused on delivering the best AI tools for everyone.\u2019 With a \u2018shared commitment to safety\u2019 which sounds like OpenAI is committed about as much as Microsoft is committed, which is \u2018to the extent not doing so would hurt shareholder value.\u2019 Notice that OpenAI and Microsoft have the same mission and no one thinks Microsoft is doing anything but maximizing profits. Does OpenAI\u2019s statement here sound like their mission to ensure AGI benefits all humanity? Or does it sound like a traditional tech startup or Big Tech company?</p>\n<p>I do not begrudge Microsoft maximizing its profits, but the whole point of this was that OpenAI was supposed to pretend its governance and priorities would remain otherwise.</p>\n<p>They are not doing a good job of pretending.</p>\n<p>The $100 billion number is a joke. OpenAI is touting this big amount of value as if to say, oh what a deal, look how generous we are being. Except OpenAI is doing stock sales at $500 billion. So \u2018over $100 billion\u2019 means they intend to offer only 20% of the company, down from their current effective share of (checks notes) most of it.</p>\n<p>Notice how they are trying to play off like this is some super generous new grant of profits, rather than a strong candidate for the largest theft in human history.</p>\n<blockquote><p><a href=\"https://x.com/OpenAINewsroom/status/1966253248886747576\">Bret Taylor, Chairman of the Board of OpenAI</a> (bold is mine): OpenAI started as a nonprofit, remains one today, and will continue to be one \u2013 with the nonprofit holding the authority that guides our future. As previously announced and as outlined in our non-binding MOU with Microsoft, the OpenAI nonprofit\u2019s ongoing control <strong>would now be paired with an equity stake</strong> in the PBC.</p></blockquote>\n<p>OpenAI\u2019s nonprofit already has a much larger equity stake currently, and much tighter and stronger control than we expect them to have in a PBC. Bret\u2019s statement on equity is technically correct, but there\u2019s no mistaking what Bret tried to do here.</p>\n<p>The way profit distribution works at OpenAI is that the nonprofit is at the end of the waterfall. Others collect their profits first, then the nonprofit gets the remaining upside. I\u2019ve argued before, back when OpenAI was valued at $165 billion, that the nonprofit was in line for a majority of expected future profits, because OpenAI was a rocket to the moon even in the absence of AGI, which meant it was probably going to either never pay out substantial profits or earn trillions.</p>\n<p>Now that the value of OpenAI minus the nonprofit\u2019s share has tripled to $500 billion, that is even more true. We are far closer to the end of the waterfall. The nonprofit\u2019s net present value expected share of future profits has risen quite a lot. They must be compensated accordingly, as well as for the reduction in their control rights, and the attorneys general must ensure this.</p>\n<p>How much profit interest is the nonprofit entitled to in the PBC? Why not ask their own AI, GPT-5-Pro? <a href=\"https://chatgpt.com/share/68c4260a-2478-8002-94fa-b3ab389a4ed1\">So I did, this is fully one shot</a>, full conversation at the link.</p>\n<blockquote><p>Prompt 1: based on the currently existing legal structure of OpenAI, and its current methods of distributing profits, if you assume OpenAI equity is correctly valued at its current total value of $500 billion, what would be the expected share of the NPV of future profits that would flow to the OpenAI nonprofit? How much would accrue to each other class of investor (Microsoft, OpenAI employees, Venture Capital investors, etc)?</p>\n<p>Prompt 2: given your full understanding of the situation, in order to avoid expropriating the nonprofit, what percentage of the new PBC would have to be given to the nonprofit? Answer this question both with and without considering the potential for decline in the effective value of their control rights in such a scenario.</p>\n<p>GPT-5-Pro: Bottom line</p>\n<ul>\n<li><strong>Economic parity (no control adjustment):</strong> <strong>~50%</strong> of the PBC.</li>\n<li><strong>Economic parity + control\u2011erosion premium:</strong> <strong>~60%</strong> of the PBC.</li>\n<li><strong>If the nonprofit ends up with ~20\u201325% (as implied by \u201c$100B+\u201d at $500B valuation):</strong> that looks like <strong>substantial expropriation</strong> of the nonprofit\u2019s legacy economic position.</li>\n</ul>\n<p><strong>Key sources:</strong> OpenAI on the <strong>capped\u2011profit</strong> and residual\u2011to\u2011nonprofit structure; OpenAI on the <strong>PBC</strong> plan and nonprofit <strong>retaining control</strong>; Semafor/Reuters on the <strong>Microsoft 75% recoup then 49/49/2</strong> framing; and reports that the nonprofit would hold <strong>&gt;$100B</strong> equity under the PBC.</p></blockquote>\n<p>It seems fair to say that if your own AI says you\u2019re stealing hundreds of billions, then you\u2019re stealing hundreds of billions? And you should be prevented from doing that?</p>\n<p><a href=\"https://x.com/TheMidasProj/status/1966284404759867727\">This was all by design</a>. OpenAI, to their great credit, tied themselves to the mast, and now they want to untie themselves.</p>\n<blockquote><p><a href=\"https://x.com/TheMidasProj/status/1966284404759867727\">The Midas Project</a>: OpenAI once said its nonprofit would be entitled to &#8220;the vast majority&#8221; and &#8220;all but a fraction&#8221; of the wealth it generates.</p>\n<p>Now, in their new restructuring, they are saying it will be entitled to only 20%. (~$100b out of a $500b valuation).</p>\n<p>From &#8220;Nearly all&#8221; to &#8220;one fifth&#8221; <img alt=\"\ud83d\ude44\" class=\"wp-smiley\" src=\"https://s0.wp.com/wp-content/mu-plugins/wpcom-smileys/twemoji/2/72x72/1f644.png\" style=\"height: 1em;\" /></p>\n<p>OpenAI&#8217;s comms team is weirdly effective at generating headlines that make it seem like they&#8217;ve done an incredible thing (given $100b to their nonprofit!) while actually undercutting their past commitments (diminishing the nonprofit&#8217;s entitlements significantly!)</p>\n<div>\n<figure>\n<div>\n\n\n<figure class=\"wp-block-image\"><img alt=\"\" src=\"https://substackcdn.com/image/fetch/$s_!QIcg!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6b0ce23f-ba77-4538-a995-0d1b636ec5e6_2086x1028.jpeg\" /></figure>\n\n\n<div></div>\n</div>\n</figure>\n</div>\n</blockquote>\n<p>I understand that Silicon Valley does not work this way. They think that if you have equity that violates their norms, or that you \u2018don\u2019t deserve\u2019 or that doesn\u2019t align with your power or role, or whose presence hurts the company or no longer \u2018makes sense,\u2019 that it is good and right to restructure to take that equity away. I get that from that perspective, this level of theft is fine and normal in this type of situation, and the nonprofit is being treated generously and should pray that they don\u2019t treat it generously any further, and this is more than enough indulgence to pay out.</p>\n<p>I say, respectfully, no. It does not work that way. That is not the law. Nor is it the equities. Nor is it the mission, or the way to ensure that humanity all benefits from AGI, or at least does not all die rapidly after AGI\u2019s creation.</p>\n<p>They also claim that the nonprofit will continue to \u2018control the PBC\u2019 but that control is almost certain to be far less meaningful than the current level of control, and unlikely to mean much in a crisis.</p>\n<p>Those control rights, to the extent they could be protected without a sufficient equity interest, are actually the even more important factor. It would be wonderful to have more trillions of dollars for the nonprofit, and to avoid giving everyone else the additional incentives to juice the stock price, but what matters for real is the nonprofit\u2019s ability to effectively control OpenAI in a rapidly developing future situation of supreme importance. Those are potentially, <a href=\"https://x.com/Miles_Brundage/status/1966264243982041150\">as Miles Brundage puts it</a>, the quadrillion dollar decisions. Even if the nonprofit gets 100% of the nominal control rights, if this requires them to act via replacing the board over time, that could easily be overtaken by events, or ignored entirely, and especially if their profit share is too low likely would increasingly be seen as illegitimate and repeatedly attacked.</p>\n<blockquote><p><a href=\"https://x.com/Miles_Brundage/status/1966264243982041150\">Miles Brundage</a>: I&#8217;ve said this before but will just reiterate that I think the amount of money that &#8220;goes to the nonprofit&#8221; is a distraction compared to &#8220;how are decisions made on safety/security/policy advocacy etc., and by who?&#8221;</p>\n<p>The latter are quadrillion $++ scale issues, not billions.</p>\n<p>It is very unclear what the percentages are, among other things.</p></blockquote>\n<p>The announcement of $50 million in grants highlights (very cheaply, given they intend to steal equity and control rights worth hundreds of billions of dollars) that they intend to pivot the nonprofit\u2019s mission into a combination of generic AI-related philanthropy and OpenAI\u2019s new marketing division, as opposed to ensuring that AGI is developed safely, does not kill us all and benefits all humanity. \u2018AI literacy,\u2019 \u2018community innovation\u2019 and \u2018economic opportunity\u2019 all sure sound like AI marketing and directly growing OpenAI\u2019s business.</p>\n<p>I do want to thank OpenAI for affirming that their core mission is \u2018ensuring AGI benefits all of humanity,\u2019 and importantly that it is not to build that AGI themselves. This is in direct contradiction to what they wrote in their bad faith letter to Gavin Newsom trying to gut SB 53.</p>\n\n\n<h4 class=\"wp-block-heading\">Quiet Speculations</h4>\n\n\n<p><a href=\"https://marginalrevolution.com/marginalrevolution/2025/09/how-to-think-about-ai-progress.html?utm_source=rss&amp;utm_medium=rss&amp;utm_campaign=how-to-think-about-ai-progress\">Tyler Cowen links to my survey of recent AI progress</a>, and offers an additional general point. In the model he offers, the easy or short-term projects won\u2019t improve much because there isn\u2019t much room left to improve, and the hard or long-term projects will take a while to bear fruit, plus outside bottlenecks, so translating that into daily life improvements will appear slow.</p>\n<p>The assumption by Tyler here that we will be in an \u2018economic normal\u2019 world in which we do not meaningfully get superintelligence or other transformational effects is so ingrained it is not even stated, so I do think this counts as a form of AI progress pessimism, although it is still optimism relative to for example most economists, or those expressing strong pessimism that I was most pushing back against.</p>\n<p>Within that frame, I think Tyler is underestimating the available amount of improvement in easy tasks. There is a lot of room for LLMs even in pure chatbot form on easy questions to become not only faster and cheaper, but also far easier to use and have their full potential unlocked, and better at understanding what question to answer in what way, and at anticipating because most people don\u2019t know what questions to ask or how to ask them. These quality of life improvements will likely make a large difference in how much mundane utility we can get, even if they don\u2019t abstractly score as rapid progress.</p>\n<p>There are also still a lot of easy tasks that are unsolved, or are not solved with sufficient ease of use yet, or tasks that can be moved from the hard task category into the easy task category. So many agents tasks, or tasks requiring drawing upon context, should be easy but for now remain hard. AIs still are not doing much shopping and booking for us, or much handling of our inboxes or calendars, or making aligned customized recommendations, despite these seeming very easy, or doing other tasks that should be easy.</p>\n<p>Coding is the obvious clear area where we see very rapid improvement and there is almost unlimited room for further improvement, mostly with no diffusion barriers, and which then accelerates much else, including making the rest of AI much easier to use even if we don\u2019t think AI coding and research will much accelerate AI progress.</p>\n<p><a href=\"https://x.com/byHeatherLong/status/1967599148334784543\">Jack Clark at the Anthropic Futures Forum</a> doubles down on the \u2018geniuses in a data center,\u2019 smarter than a Nobel prize winner and able to complete monthlong tasks, arriving within 16 months. He does hedge, saying \u2018could be\u2019 buildable by then. If we are talking \u2018probably will be\u2019 I find this too aggressive by a large margin, but I agree that it \u2018could be\u2019 true and one must consider the possibility when planning.</p>\n<p>&nbsp;</p>\n\n\n<h4 class=\"wp-block-heading\">The Quest for Sane Regulations</h4>\n\n\n<p><a href=\"https://x.com/Scott_Wiener/status/1966909714849296602\">California\u2019s SB 53 has now passed the Assembly and Senate, so it goes to Newsom</a>. I strongly urge him to sign it into law. <a href=\"https://x.com/hamandcheese/status/1967305573264933367\">Samuel Hammond also hopes</a> it is signed, Dean Ball has called SB 53 highly reasonable, Anthropic has endorsed the bill. <a href=\"https://t.co/PbBBQVnpXP\">Here is a link for those in California to let Gavin Newsom know their opinion about the bill</a>.</p>\n<p>Meta hasn\u2019t endorsed the bill, <a href=\"https://x.com/daniel_271828/status/1967746537901133989\">but they have essentially given the green light</a>.</p>\n<blockquote><p>\u201cMeta has stated our support for balanced AI regulation that has needed guardrails while nurturing AI innovation and economic growth throughout California and the country,\u201d Meta spokesperson <strong>Jim Cullinan</strong> said in a statement Saturday after the measure passed the Senate in the early morning hours. \u201cWhile there are areas for improvement, SB 53 is a step in that direction,\u201d he added.</p></blockquote>\n<p>OpenAI\u2019s rhetoric against SB 53 was terrible and in bad faith, <a href=\"https://x.com/ShakeelHashim/status/1966907150984212735\">but there are levels to bad faith arguments in such situations.</a> It can get worse.</p>\n<blockquote><p>Shakeel Hashim: Astonishing how disingenuous the lobbying against this bill is. You&#8217;d like it more if it applied to smaller developers, would you? I have a feeling that might not be true!</p>\n<p>He Quotes: A recent letter obtained by POLITICO, sent to Wiener before the final vote, hammered on the bill\u2019s focus on larger programs and companies. It was from the California Chamber of Commerce\u2019s Ronak Daylami and co-signed by representatives from the Computer &amp; Communications Industry Association as well as TechNet.</p>\n<p>\u201dWe are concerned about the bill\u2019s focus on \u2018large developers\u2019 to the exclusion of other developers of models with advanced capabilities that pose risks of catastrophic harm,\u201d stated the letter.</p></blockquote>\n<p>They are concerned that the bill does not impact smaller developers? Really? You would have liked them to modify the bill to lower the thresholds so it impacts smaller developers, because you\u2019re that concerned about catastrophic risks, so you think Newsom should veto the bill?</p>\n<p>It is at times like this I realize how little chutzpah I actually possess.</p>\n<p>White House\u2019s <a href=\"https://www.politico.com/news/2025/09/16/we-dont-want-california-to-set-the-rules-for-ai-across-the-country-trump-adviser-says-00565251\">Sriram Krishnan talked to Politico</a>, which I discuss further in a later section. He frames this as an \u2018existential race\u2019 with China, despite declaring that AGI is far and not worth worrying about, in which case I am confused why one would call it existential. He says he \u2018doesn\u2019t want California to set the rules for AI across the country\u2019 while suggesting that the rules for AI should be, as he quotes David Sacks, \u2018let them cook,\u2019 meaning no rules. I believe Gavin Newsom should consider his comments when deciding whether to sign SB 53.</p>\n<p><a href=\"https://x.com/daniel_271828/status/1968064819267006908\">Daniel Eth explains that the first time</a> a low salience industry spent over $100 million on a super PAC to enforce its preferences via electioneering was crypto via Fairshake, and now Congress is seen as essentially captured by crypto interests. Now the AI industry, led by a16z, Meta and OpenAI\u2019s Greg Brockman (and inspired by OpenAI\u2019s Chris Lehane) is repeating this playbook with \u2018Leading the Future,\u2019 whose central talking point is to speak of a fictional \u2018conspiracy\u2019 against the AI industry as they spend vastly more than everyone has ever spent combined on safety-related lobbying combined to outright buy the government, which alas is by default on sale remarkably cheap. Daniel anticipates this will by default be sufficient for now to silence all talk of lifting a finger or even a word against the industry in Congress.</p>\n<blockquote><p><a href=\"https://x.com/DKokotajlo/status/1968083856734687721\">Daniel Kokotajlo</a>: Over the last few years I&#8217;ve learned a lot about how much sway giant corporations have over the federal government. Much more than I expected. In AI 2027 the government basically gets captured by AI companies, first by ordinary lobbying, later by superintelligence-assisted lobbying.</p></blockquote>\n<p>If AI rises sufficiently in public salience, money will stop working even if there isn\u2019t similar money on the other side. Salience will absolutely rise steadily over time, but it likely takes a few years before nine figures stops being enough. That could be too late.</p>\n<p><a href=\"https://x.com/jjohnpotter/status/1966250872138248243\">Albania appoints the world\u2019s first \u2018AI minister\u2019 named Diella</a>.</p>\n<blockquote><p>John Potter: AI makes a lot of mistakes but there\u2019s no way it is worse than the standard corruption of an Albanian procurement bureaucrat.</p>\n<p>Dustin: Did not have this on the 2025 bingo card.</p>\n<p><a href=\"https://www.politico.eu/article/albania-apppoints-worlds-first-virtual-minister-edi-rama-diella/\">Albania just appointed a virtual, AI-powered \u201cminister\u201d named Diella</a> (Albanian for \u201csunshine\u201d). Not a minister for AI; an AI as minister. According to PM Edi Rama, Diella will handle public procurement.</p>\n<p>If it works, this could be a big deal: procurement is where governments spend most of their money and where waste and corruption often hide. An AI that standardizes bids, flags anomalies, and leaves a full audit trail could raise the bar on transparency.</p>\n<p>But it also raises real questions: Who is legally accountable for decisions? How are models audited? What\u2019s the appeal process when Diella gets it wrong?</p>\n<p>Milestone or stunt, this is the moment AI moved from \u201cpolicy area\u201d to policy actor.</p></blockquote>\n<p>Dustin asks very good questions, which the Politico article does not answer. Is this a publicity stunt, a way of hiding who makes the decisions, or something real? How does it work, what tech and techniques are behind it? The world needs details. Mira Mutari, can you help us find out, perhaps?</p>\n<p><a href=\"https://www.theinformation.com/articles/tech-leaders-flatter-trump-anthropic-takes-cooler-approach\">As Tech Leaders Flatter Trump, Anthropic Takes a Cooler Approach</a>. Anthropic is not and should to be an enemy of the administration, and should take care not to needlessly piss the administration off, become or seem generally partisan, or do things that get one marked as an enemy. It is still good to tell it like it is, stand up for what you believe is right and point out when mistakes are being made or when Nvidia seems to have taken over American chip export policy and seems to be in the act of getting us to sell out America in the name of Nvidia\u2019s stock price. Ultimately what matters is ensuring we don\u2019t all die or lose control over the future, and also that America triumphs, and everyone should be on the same side on all of that.</p>\n<p><a href=\"https://x.com/SenatorSlotkin/status/1966607759367127141\">Michigan Senator Elissa Slotkin cites race with China and calls for a \u2018Manhattan Project for AI</a>.\u2019 She gets so close in the linked speech to realizing the real danger and why this is not like nuclear weapons, then ignores it and moves straight ahead analogizing repeatedly to nuclear weapons.</p>\n<p>Anthropic is reported to be annoying the White House <a href=\"https://www.semafor.com/article/09/17/2025/anthropic-irks-white-house-with-limits-on-models-uswhite-house-with-limits-on-models-use\">by daring to insist that Claude not be used</a> for surveillance, which the SS, FBI and ICE want to do. It is interesting that the agencies care, and that other services like ChatGPT and Gemini can\u2019t substitute for those use cases. I would not be especially inclined to fight on this hill and would use a policy here similar to the one at OpenAI, and I have a strong aesthetic sense that the remedy is Claude refusing rather than it being against terms of service, but some people feel strongly about such questions.</p>\n<p>However, we keep seeing reports that the White House is annoyed at Anthropic, so if I was Anthropic I would sit down (unofficially, via some channel) with the White House and figure out which actions are actually a problem to what extent and which ones aren\u2019t real issues, and then make a decision which fights are worthwhile.</p>\n\n\n<h4 class=\"wp-block-heading\">Chip City</h4>\n\n\n<p>There is some good news on the South Korean front, as after <a href=\"https://x.com/koryodynasty/status/1967071104591425676\">a few days of treatment like that reported in this thread</a>, at least some key parts of the Trump administration <a href=\"https://www.bloomberg.com/news/articles/2025-09-14/us-official-offers-regrets-over-detention-of-south-koreans\">realized it made a huge mistake</a> and we are now<a href=\"https://x.com/ModeledBehavior/status/1966169919416148479\"> attempting to mitigate</a> the damage from ICE\u2019s raid on Hyundai\u2019s battery plant. <a href=\"https://www.wsj.com/world/asia/confusion-anger-relief-korean-engineer-tells-of-week-in-u-s-ice-detention-8a61a170\">They let all but one of the detainees go</a>,<a href=\"https://www.washingtonpost.com/world/2025/09/11/trump-south-korea-hyundai-raid-visas/\"> let them stay if they wished</a> and assured them they could return to America, although they are understandably reluctant to stay here.</p>\n<p><a href=\"https://x.com/ATabarrok/status/1967552832539951600\">Trump issued a statement emphasizing</a> how important it is to bring in foreign workers to train Americans and not to frighten off investment. He doesn\u2019t admit the specific mistake but this is about as good a \u2018whoops\u2019 as we ever get from him, ever.</p>\n<p>It also seems NIH grantmaking <a href=\"https://x.com/AlecStapp/status/1968015572886806774\">has gotten back on track</a> at least in terms of size.</p>\n<p><a href=\"https://t.co/ZHHS4TnWMs\">SemiAnalysis analyzes Huawei\u2019s production</a>, and reports that the export controls are absolutely working to hurt their production of chips, which if we prevent smuggling <a href=\"https://x.com/SemiAnalysis_/status/1968047290570117240\">will not only not scale in 2026 but will actively fall sharply</a> to below 2024 levels, as they have been relying on purchases from Samsung that will soon run dry.</p>\n<p>China is telling Chinese companies to cut off purchases of Nvidia chips,<a href=\"https://x.com/SamoBurja/status/1968363320978850306\"> including it seems all Nvidia chips, here there is reference to the RTX Pro 6000D</a>. Good. Never interrupt your enemy when he is making a mistake. As I\u2019ve said before, China\u2019s chip domestic chip industry already had full CCP backing and more demand than they could supply, so this won\u2019t even meaningfully accelerate their chip industry, and this potentially saves us from what was about to be a very expensive mistake. Will they stick to their guns?</p>\n<p><a href=\"https://www.wsj.com/business/autos/why-hyundai-raid-wont-crush-the-korean-carmaker-0a4a0e4a?mod=WTRN_pos2\">Construction at the site is set back by two or three months</a>.</p>\n<p>Major damage has still been done.</p>\n<blockquote><p>Lee Jae Myung (President of South Korea): I think this will have a significant impact on direct investments in the United States moving forward.</p>\n<p>Our companies that have expanded overseas are probably very confused. We are not there for long-term research or employment. You need a facility manager to install the machinery and equipment when you establish a factory, right?</p></blockquote>\n<p>Even if those workers were there for long term research or employment, this arrangement would still be an obvious win for America. When they\u2019re here to train American workers, there is only pure upside.</p>\n<p>Here is David Cowan being the latest to <a href=\"https://x.com/david_cowan/status/1966114422675447970\">explain that Nvidia is a national security risk</a>, with its focus on selling the best possible chips to China. <a href=\"https://x.com/hamandcheese/status/1955110638600257597\">Samuel Hammond has a very good statement about Nvidia\u2019s lack of corporate patriotic responsibility</a>. Nvidia actively opposes American national security interests, including using a full ostrich strategy towards Chinese chip smuggling.</p>\n<p><a href=\"https://x.com/yishan/status/1968135039151116449\">Chinese companies are offering to sell us solar panel manufacturing kits</a> with 35 day lead times, as solar keeps getting cheaper and more abundant all around. It is a shame our government is actively trying to stop solar power.</p>\n<p>Here is some <a href=\"https://www.nytimes.com/2025/09/15/us/politics/trump-uae-chips-witkoff-world-liberty.html\">potentially very important context to the UAE chip deal</a>:</p>\n<div>\n<figure>\n<div>\n\n\n<figure class=\"wp-block-image\"><img alt=\"\" src=\"https://substackcdn.com/image/fetch/$s_!ms2D!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc2c9882a-7ca8-4606-9235-46b326762682_848x830.png\" /></figure>\n\n\n<div></div>\n</div>\n</figure>\n</div>\n<p>&nbsp;</p>\n<blockquote><p>NYT (et al):</p>\n<ul>\n<li>Steve Witkoff advocated to give the Emirates access to the chips at the same time that his and Mr. Trump\u2019s family business was landing the crypto investment, despite an <a href=\"https://archive.is/o/Lsdef/https://www.ecfr.gov/current/title-5/chapter-XVI/subchapter-B/part-2635/subpart-E/section-2635.502\">ethics rule</a> intended to prohibit officials from participating in matters that could benefit themselves or their relatives.</li>\n<li>Mr. Sacks was a key figure in the chip negotiations, raising alarm from some Trump administration officials who believed that it was improper for a working venture capitalist to help broker deals that could benefit his industry and investors in his company. He received a White House ethics waiver allowing him to participate.</li>\n<li>A senior executive based in the U.A.E. worked simultaneously for World Liberty and Sheikh Tahnoon\u2019s G42, creating a link between the two companies as the Emiratis were pushing to gain access to A.I. chips.</li>\n<li>Some Trump administration officials tried to limit the chips deal, but an unexpected intervention by the conservative agitator <a href=\"https://archive.is/o/Lsdef/https://www.nytimes.com/2025/07/30/us/politics/laura-loomer-trump.html\">Laura Loomer</a> changed the power dynamic within the White House in the U.A.E.\u2019s favor.</li>\n</ul>\n<p>\u2026</p>\n<p>In the middle of both deals was Mr. Trump, a president who has used his power to enrich himself in ways that have <a href=\"https://archive.is/o/Lsdef/https://www.nytimes.com/2025/04/29/us/politics/trump-crypto-world-liberty-financial.html\">little modern precedent</a>, at least in the United States. It is more reminiscent of business customs in the Persian Gulf, where moneymaking and governance are blended in the hands of the ruling families.</p>\n<p>\u2026</p>\n<p>Until at least March, Mr. Sacks, who is still working at Craft, was also invested in a stock fund that included the Taiwan Semiconductor Manufacturing Co., which builds Nvidia\u2019s chips, and other A.I.-related companies such as Amazon and Meta. (The size of those stakes isn\u2019t publicly known.)</p>\n<p>The White House recognized that Mr. Sacks\u2019s investments could present a problem. On March 31, the White House counsel, David Warrington, <a href=\"https://archive.is/o/Lsdef/https://static01.nyt.com/newsgraphics/documenttools/b3f8b9ac059eb4df/2d1ce750-full.pdf\">signed a letter</a> that granted Mr. Sacks special permission to participate in government decisions that might affect his financial holdings. Without the waiver, those kinds of actions could violate a conflict of interest law.</p>\n<p>The waiver came less than two weeks after Sheikh Tahnoon announced that he had met with Mr. Sacks in Washington <a href=\"https://archive.is/o/Lsdef/https://x.com/hhtbzayed/status/1902852288244256802\">to discuss</a> A.I. \u201cinvestment opportunities.\u201d</p>\n<p>\u2026</p>\n<p>The White House spokeswoman disputed that the executive asked Mr. Witkoff to help with the Commerce Department. She acknowledged that Mr. Witkoff was \u201cbriefed\u201d on the overall chip discussions, but she maintained that \u201che did not participate,\u201d an <a href=\"https://archive.is/o/Lsdef/https://www.ecfr.gov/current/title-5/chapter-XVI/subchapter-B/part-2635/subpart-E/section-2635.502\">important standard in federal ethics rules</a> that prohibit government officials from taking part in matters that could benefit their families.</p>\n<p>\u2026</p>\n<p>Mr. Trump made no public mention of the $2 billion transaction with his family company.</p></blockquote>\n<p>There are no claims here that there was a strict Quid Pro Quo, or otherwise an outright illegal act. If the President is legally allowed to have a crypto company into which those seeking his favor can pour billions of dollars, then that\u2019s certainly not how I would have set up the laws, but that seems to be the world we live in. Technically speaking, yes, the UAE can pour billions into Trump\u2019s private crypto, and then weeks later suddenly get access to the most powerful chips on the planet over the national security objections of many, in a situation with many things that appear to be conflicts of interest, and that\u2019s all allowed, right in the open.</p>\n<p>However. It doesn\u2019t look good. It really, really, profoundly does not look good.</p>\n<blockquote><p><a href=\"https://x.com/weakinstrument/status/1967625062015721701\">Ryan Cummings</a> (1.3m views): If this is true, this is the largest public corruption scandal in the history of the United States and it&#8217;s not even close.</p></blockquote>\n<p>The objections that I have seen don\u2019t claim the story isn\u2019t true. The objections claim that This Is Fine. That this is how business is done in the Middle East, or in 2025.</p>\n<p>I notice this response does not make me feel better about having sold the chips.</p>\n\n\n<h4 class=\"wp-block-heading\"></h4>\n\n\n<p>&nbsp;</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p>\n\n\n<h4 class=\"wp-block-heading\">The Week in Audio</h4>\n\n\n<p><a href=\"https://x.com/ATabarrok/status/1966959756188283206\">Demis Hassabis knows</a>, yet forgot one thing in <a href=\"https://www.youtube.com/watch?v=Kr3Sh2PKA8Y&amp;ab_channel=All-InPodcast\">his talk at the All-In Summit</a>.</p>\n<blockquote><p>Demis Hassabis (CEO Google DeepMind): calling today&#8217;s chatbots \u201cPhD intelligences\u201d is nonsense.</p>\n<p>They can dazzle at a PhD level one moment and fail high school math the next.</p>\n<p>True AGI won&#8217;t make trivial mistakes. It will reason, adapt, and learn continuously. We&#8217;re still 5\u201310 years away.</p>\n<p><a href=\"https://x.com/ATabarrok/status/1966959756188283206\">Alex Tabarrok</a>: Have you met a PhD?</p>\n<p><a href=\"https://x.com/mattyglesias/status/1967291668853567966\">Matthew Yglesia</a>s: What\u2019s most notable to me is that \u201cfive to ten years away\u201d counts as a long timeline these days.</p></blockquote>\n<p>The \u20185-10 years is a long timeline\u2019 issue can lead to important miscommunications. As in, I bet that this happened:</p>\n<ol>\n<li>Demis Hassabis told someone important, such as a high government official, \u2018oh we are not anywhere close to building AGI, we don\u2019t know how to do that yet.\u2019</li>\n<li>What he meant was \u2018we are probably 5-10 years away from building AGI and the world transforming shortly thereafter.\u2019</li>\n<li>What the person heard was \u2018AGI is far away, we don\u2019t have to worry about it.\u2019</li>\n</ol>\n<p>Whoops! That\u2019s not at all what Demis Hassabis said.</p>\n\n\n<h4 class=\"wp-block-heading\">He Just Tweeted It Out</h4>\n\n\n<p>Which I appreciate, now there\u2019s no pretending they aren\u2019t literally saying this.</p>\n<blockquote><p>White House Senior Policy Advisor <a href=\"https://x.com/sriramk/status/1968400859223437518\">Sriram Krishnan</a>: Winning the AI race = market share.</p>\n<p>Neil Chilson: Wow, whirlwind interview with @sriramk. Very newsy! Start: his key metric of success of the American AI tech stack dominance is market share of tokens generated.</p></blockquote>\n<p>It\u2019s not only market share, it is \u2018market share of tokens generated.\u2019</p>\n<p>Which is an obviously terrible metric. Tokens generated is deeply different from value generated, or even from dollars spent or compute spent. Tokens means you treat tokens from GPT-5-Pro or Opus 4.1 the same as tokens from a tiny little thing that costs 0.1% as much to run and isn\u2019t actually doing much of anything. It\u2019s going to vastly overestimate China\u2019s actual share of the market, and underestimate ours, even if you really do only care about market share.</p>\n<p>But no, literally, that\u2019s what he thinks matters. Market share, measured in what chips people use. China can do all the things and build all the models and everything else, so long as it does it on Nvidia hardware it\u2019s all good. This argument has never made any sense whatsoever.</p>\n<p><a href=\"https://www.youtube.com/watch?v=l8fG5DcjucA&amp;ab_channel=NoPriors%3AAI%2CMachineLearning%2CTech%2C%26Startups\">Sriram went on No Priors</a> last month, which I first saw via Sriram Tweeting It Out. Neil\u2019s linked summary of the Axios event Sriram was at is here, and we have Sririam\u2019s <a href=\"https://www.youtube.com/watch?v=AVrADPeUpBs&amp;ab_channel=POLITICO\">Politico interview</a>.</p>\n<blockquote><p>Neil Chilson: He explains those who want to ban chip exports have four wrong beliefs:</p>\n<ol>\n<li>U.S. supply constraint</li>\n<li>China can&#8217;t manufacture</li>\n<li>China can&#8217;t build models</li>\n<li>US is building ASI</li>\n</ol>\n<p>None true.</p>\n<p>Says those who want export controls are advocating exactly what Huawei wants.</p></blockquote>\n<p>&nbsp;</p>\n<p>We can start with that last statement. I notice he says \u2018what Huawei wants\u2019 not \u2018what China wants,\u2019 the same way the White House seems to be making decisions based on \u2018what Nvidia wants\u2019 not \u2018what America wants.\u2019 Yes, obviously, if your literal only metric is sales of chips, then in the short term you want to sell all the chips to all the customers, because you\u2019ve defined that as your goal.</p>\n<p>(The long term is complicated because chips are the lifeblood of AI and the economies and strategic powers involved, so even without AGI this could easily go the other way.)</p>\n<p>Now, on those four points, including drawing some things from his other interviews:</p>\n<ol>\n<li>The United States is absolutely supply constrained on advanced AI chips, in the sense that for every chip that Nvidia can physically make, there is a Western customer who wants to buy that chip at prevailing market prices.\n<ol>\n<li>I am confused what else it could mean to not be supply constrained.</li>\n<li>If I am wrong, someone please correct me. Say, \u2018Nvidia offered to sell more AI chips to Western customers, and the chips went unsold, look here.\u2019 I apologize in advance if this happened and I missed it but I have not heard of this.</li>\n</ol>\n</li>\n<li>China can of course manufacture things in general. That is common knowledge. Chips, especially highly advanced AI chips, are a much tricker question.\n<ol>\n<li>China can manufacture some chips.</li>\n<li>China cannot manufacture, any time soon, anything like enough chips to meet domestic demand, and cannot manufacture chips of anything like the same quality as Nvidia, indeed as we see elsewhere they are in danger of their capacity declining in 2026 down to below 2024 levels if we enforce our export controls properly.</li>\n<li>I am confused what false belief he ascribes to those who oppose exports.</li>\n<li>I see no evidence provided that China can meaningfully improve its chip manufacturing in response to export restrictions, given the strong market, national and government incentives already present.</li>\n</ol>\n</li>\n<li>China can build good models behind the frontier. It cannot build frontier AI models that are as good as those from the top American labs at any given time. I am curious what the supposed false belief is here.\n<ol>\n<li>Sriram clearly, based on statements here, overrated to The DeepSeek Moment, which he today still calls a \u2018Sputnik moment,\u2019 as did many others (including myself at first). He does acknowledge that many associated claims proved ultimately overstated.</li>\n<li>Alas, he still seems to believe that America has \u2018only a small lead\u2019 on AI, which simply is not true (depending on what \u2018small\u2019 means, but as I\u2019ve said before the lead is a lot bigger than it looks because fast following is easier, and we\u2019re comparing the best aspects of Chinese models to American ones, and several other factors).</li>\n<li>He incorrectly states that at the time OpenAI had the only other reasoning model, which was not true, Google had already released a reasoning version of Gemini Flash that was actually reasonably strong but once again they failed marketing forever, so this has been memory holed.</li>\n<li>Alas, all of this fed into this obsession with \u2018racing.\u2019</li>\n<li>This question is highly load bearing to Sriram.\n<ol>\n<li>Otherwise, we be so worried about a rival tech stack, when the Chinese also have no chips to sell and won\u2019t for years at least, even if the tech stack was meaningfully a thing?</li>\n<li>He says that DeepSeek proved \u2018China can build AI models just fine\u2019 so we shouldn\u2019t worry about America releasing open models that could then be copied or distilled or studied or modified by China. He thinks that this is a knock-down argument, and that thus there is no danger of this. And that seems very obviously absurd.</li>\n</ol>\n</li>\n</ol>\n</li>\n<li>The United States is, according to the labs themselves and many others, on track to build AGI and then ASI. If you look at their clear public statements it is very, very obvious that we are working towards making every effort at building ASI. If you don\u2019t think we might build an ASI within 5-10 years, time to pay attention.\n<ol>\n<li>That is the entire company mission of OpenAI and their employees keep going on Twitter to talk about building AGI and ASI, like, all the time.</li>\n<li>Dario Amodei, CEO of Anthropic, as well as their policy head Jack Clark, actively predict AGI and then ASI within a few years.</li>\n<li>Demis Hassabis, CEO of Google DeepMind, expects AGI in 5-10 years, which means ASI shortly thereafter, and considers this a long timeline.</li>\n<li>Elon Musk at xAI is looking to build it. He said \u2018Grok 5 might be AGI.\u2019</li>\n<li>Mark Zuckerberg at Meta is forming a Superintelligence division and throwing money at it (although to be fair in this case he might well not mean actual superintelligence).</li>\n<li>I worry that statements are being misinterpreted here, so for example Demis says \u2018it will take us 5-10 years to build ASI\u2019 and that gets interpreted as \u2018we are not building ASI.\u2019 But the correct reaction is the opposite!</li>\n<li>Note that Sriram affirms he did read AI 2027 and he does expect an \u2018event horizon\u2019 around AI to happen at some point.</li>\n<li>The evidence he cites for this claim in the Politico interview is to simply say there are no signs of this happening, which flat out obviously isn\u2019t true, and he presents no concrete evidence or real arguments for his position, besides \u2018I don\u2019t see anything close to AGIs yet.\u2019</li>\n<li>I would also note that yesterday we had OpenAI\u2019s Hieu Pham saying \u2018<a href=\"https://x.com/hyhieu226/status/1968378785709133915\">There will be some people disagreeing this is AGI. I have no words for them</a>. Hats off. Congrats to the team that made this happen.\u2019 You don\u2019t have to agree to this claim, and I don\u2019t, but it seems hard to be confident AGI is far.</li>\n</ol>\n</li>\n</ol>\n<p>On last point Neil lists, the Woke AI EO, my understanding matches Sriram\u2019s.</p>\n<p>I wrote up additional notes on the rest of the contents of those interviews, but ultimately decided Neil is right that the above are Sriram\u2019s central points, and since his other rhetoric isn\u2019t new further engagement here would be unproductive.</p>\n\n\n<h4 class=\"wp-block-heading\">Rhetorical Innovation</h4>\n\n\n<p><a href=\"https://x.com/robbensinger/status/1968539399345553606\">This tread contains more endorsements</a> of <a href=\"https://www.amazon.com/Anyone-Builds-Everyone-Dies-Superhuman/dp/0316595640\">If Anyone Builds It, Everyone Dies</a>, including some unexpected celebrities, such as Mark Ruffalo, Patton Oswalt and Alex Winter, the <a href=\"https://x.com/robbensinger/status/1968514923669111201\">actor who plays Bill in Bill and Ted\u2019s Excellent Adventure</a>. I wonder if Keanu Reeves would have replied \u2018Whoa!\u2019 or gone with \u2018Dude!\u2019</p>\n<p><a href=\"https://www.pewresearch.org/science/2025/09/17/how-americans-view-ai-and-its-impact-on-people-and-society/\">The public\u2019s views on AI haven\u2019t changed much in the past year</a>. AI has changed quite a bit, so it tells you something about the public that their views mostly are the same.</p>\n<p><a href=\"https://x.com/MichaelTrazzi/status/1966285365758628240\">Michael Trazzi ends his hunger strike after 7 days</a>, after he has two near-fainting episodes and doctors found acidosis and \u2018very low blood glucose\u2019 even for someone on a 7 day fast. As of his announcement Guideo and Denys are continuing. So this wasn\u2019t an \u2018actually endanger my life on purpose\u2019 full-on hunger strike. Probably for the best.</p>\n<p>Roon is correct at the limit here, in sufficiently close to perfect competition you cannot be kind, but there\u2019s a big gap between perfect competition and monopoly:</p>\n<blockquote><p><a href=\"https://x.com/tszzl/status/1967502582987018311\">Roon (OpenAI):</a> the closer you are to perfect competition, race dynamic, the more the machine owns you. moloch runs the show. only monopolies can be kind.</p></blockquote>\n<p>As I wrote in <a href=\"https://thezvi.substack.com/p/moloch-hasnt-won\">Moloch Hasn\u2019t Won</a>, one usually does not live near this limit. It is important to notice that the world has always contained a lot of intense competition, yet we have historically been winning the battle against Moloch and life contains many nice things and has mostly gotten better.</p>\n<p>The question is, will AGI or superintelligence change that, either during or after its creation? AIs have many useful properties that bring you closer to perfect competition, enforcing much faster and stronger feedback loops and modifications, and allowing winners to rapidly copy themselves, and so on. If you propose giving similar highly capable AIs to a very large number of people and groups, which will then engage in competition, you need a plan for why this doesn\u2019t cause (very rapid) <a href=\"https://thezvi.substack.com/p/the-risk-of-gradual-disempowerment\">Gradual Disempowerment</a> or related failure modes.</p>\n<p>During the race towards AGI and superintelligence, competitive and capitalistic pressures reduce ability to be kind in ordinary ways, but while it is still among humans this has happened many times before in other contexts and is usually importantly bounded.</p>\n<p><a href=\"https://manifund.substack.com/p/how-cost-effective-are-ai-safety?utm_source=post-email-title&amp;publication_id=1491799&amp;post_id=173411122&amp;utm_campaign=email-post-title&amp;isFreemail=true&amp;r=67wny&amp;triedRedirect=true&amp;utm_medium=email\">How effective is AI Safety YouTube</a>? Marcus Abramovitch and Austin Chen attempt to run the numbers, come up with it being modestly effective if you think the relevant messages are worth spreading.</p>\n<blockquote><p><a href=\"https://x.com/deanwball/status/1966875809274360319\">Dean Ball</a>: I wonder if, in the early days of banking, people who worried about money laundering, theft, and fraud were considered \u201cbanking doomers.\u201d</p>\n<p>My observation is fully ahistorical, profoundly anachronistic. I\u2019m making a joke about the low quality of ai discourse today, implying that our standards are beneath those of people who shat in holes in the ground.</p>\n<p>I want to argue! That\u2019s fine and great. The issue is that the whole doomer thing in fact shuts down and coarsens debate.</p></blockquote>\n<p>Exactly. The majority of uses of the term \u2018doomer\u2019 in the context of AI are effectively either an attempt to shut down debate (as in anything that is \u2018doomer\u2019 must therefore be wrong) similar to calling something a term like \u2018racist,\u2019 or effectively a slur, or both.</p>\n<p><a href=\"https://x.com/FactsAndQuips/status/1915507551980184056\">I am referred</a> to <a href=\"https://x.com/FactsAndQuips/status/1915507551980184056\">this fun and enlightening thread</a> about the quest by William Mitchell to convince America after WWI that airplanes can sink battleships, in which people continue claiming this hasn\u2019t and won\u2019t happen well after airplanes repeatedly were demonstrated sinking battleships. Please stop assuming that once things about AI are convincingly demonstrated (not only existential risks and other risks, but also potential benefits and need to deploy) that people will not simply ignore this.</p>\n<p>Why does The Washington Post keep publishing <a href=\"https://www.washingtonpost.com/opinions/2025/09/12/ai-realism-tool-doomers-zealots/\">Aaron Ginn writing the same bad faith Nvidia op-ed over and over again</a>? I\u2019m seriously asking, at this point it is bizarre.</p>\n<p>In this case, not only does he write especially terrible word salad about how AI can only pose a danger if intelligence can be measured by a single number whereas no machine can ever fully grasp the universe whereas only humans can embody deep meaning (meme of Walter White asking what the hell are you talking about?), he kind of gives the game away. If you\u2019re writing as a de facto Nvidia lobbyist trying to tar everyone who opposes you with name calling, perhaps don\u2019t open with a quote where you had dinner with Nvidia CEO Jensen Huang and he complains about everyone being \u2018so negative\u2019?</p>\n<p><a href=\"https://x.com/peterwildeford/status/1966882462170313182\">The continued quest to get libertarians and economists to differentiate</a> between current and future more capable AI systems (difficulty: AI complete).</p>\n<blockquote><p><a href=\"https://x.com/neil_chilson/status/1966705785561309349\">Neil Chilson</a>: Every single person is this video is saying \u201cguys guess what Gen AI isn\u2019t like computers\u2014\u2014it\u2019s like plants and the natural world and the economy!!!!!\u201d</p>\n<p>Ok. This is surprising to them because they spent too much time with deterministic computers.</p>\n<p>Normal people know that complex systems which no one controls are extremely common. They wouldn\u2019t use those words, but they know.</p>\n<p>Peter Wildeford: Current AI is not dangerous and should be widely adopted. But it&#8217;s important to see where this is going. AI is not normal technology. If you&#8217;re not at least a little bit doomer, you have a failure of imagination.</p>\n<p>I like how Dean puts it here:</p>\n<p>Dean Ball (replying to Neil Chilson): I concur directionally with this in some ways but I think the point these folks are making is that a plant cannot eg design novel bacteria or solve open questions in mathematics, and a plant is also not infinitely replicable at near zero marginal cost. A system with those properties and capabilities would indeed be something new under the sun.</p>\n<p>Essentially no ai safetyists are primarily worried about the systems we have today, except as toy problems. They are not worried about \u201cgen ai,\u201d per se. They are worried about the systems that it is the explicit intention of frontier ai labs to build in the near future.</p>\n<p>Maybe they are too worried, or worried for the wrong reasons, or worried about the wrong things. Fair enough. We can talk price.</p>\n<p>But to dismiss those worries altogether I think is a step much too far. And you don\u2019t need to, because safety and security are definitional parts of well-engineered systems, and robustness is a definitional part of well-functioning institutions. This is why it is in fact not that hard to advance both ai acceleration and mitigation of the various risks, see eg the ai action plan.</p>\n<p>There is no need for false dichotomies or artificial rivalries. I promise you that you do not want to live in a world with badly aligned, poorly understood, and highly capable neural networks. I promise that it\u2019s better for technology acceleration for ai risks to be well managed, including by the government.</p>\n<p>That doesn\u2019t mean all proposed government interventions are good! But it means a small number of them transparently are. A shred of nuance\u2014not a lot, just a shred\u2014is all that is required here, at least today. It\u2019s not that hard, and I think we can muster it.</p>\n<p>But if you choose to die on the hill of nothing-to-see-hereism and this-is-not-novelology, I am quite sure you will regret it in the fullness of time. Though I would happily generate a passive income stream taking bets against your predictions.</p></blockquote>\n<p>As Dean Ball says, you very much would not want to live in a world with badly aligned, poorly understood and highly capable neural networks. Not that, if it were to arise, you would get to live in such a world for very long.</p>\n<p>In this case, Neil (including in follow-ups, paraphrased) seems to be saying \u2018oh, there are already lots of complex systems we don\u2019t understand effectively optimizing for things we don\u2019t care about, so highly advanced future AI we don\u2019t understand effectively optimizing for things we don\u2019t care about would be nothing new under the sun, therefore not worth worrying out.\u2019 File under \u2018claims someone said out loud with straight face, without realizing what they\u2019d said, somehow?\u2019</p>\n<p><a href=\"https://www.lesswrong.com/posts/Ed3naAyEEe7zZvzsj/the-center-for-ai-policy-has-shut-down\">The Center for AI Policy Has Shut Down</a>, and Williams offers a postmortem. I am sad that they are shutting down, but given the circumstances it seems like the right decision. I have written very positively in the past about their work on model legislation and included them in my 2024 edition of The Big Nonprofits Post.</p>\n<p>Eliezer offers yet another metaphorical attempt, here reproduced in full, which hopefully is a good intuition pump for many people? See if you think it resonates.</p>\n<blockquote><p><a href=\"https://x.com/ESYudkowsky/status/1968420784180719878\">Eliezer Yudkowsky</a>: If AI improves fast, that makes things worse, but it&#8217;s not where the central ASI problem comes from.</p>\n<p>If your city plans to enslave ultra-smart dragons to plow their fields and roast their coffee, some problems get *worse* if the dragons grow up very quickly. But the core problem is not: &#8220;Oh no! What if the huge fire-breathing monsters that could wipe out our city with one terrible breath, that are also each individually much smarter than our whole city put together, that when mature will think at speeds that make any human seem to them like a slow-moving statue, *grow up quickly*? Wouldn&#8217;t that speed of maturation present a problem?&#8221;</p>\n<p>If you imagine suddenly finding yourself in a city full of mature dragons, that nonequilibrium situation will then go pear-shaped very quickly. It will go pear-shaped even if you thought you had some clever scheme for controlling those dragons, like giving them a legal system which said that the humans have property rights, such that surely no dragon coalition would dare to suggest an alternate legal system for fear of their own rights being invalidated. (Actual non-straw proposal I hear often.) Even if you plan to cleverly play off the dragons against each other, so that no dragon would dare to breathe fire for fear of other dragons &#8212; when the dragons are fully mature and vastly smarter than you, they will all look at each other and nod and then roast you.</p>\n<p>Really the dragon-raising project goes pear-shaped *earlier*. But that part is trajectory-dependent, and so harder to predict in detail in advance. That it goes grim at *some* point is visible from visualizing the final destination if the dragons *didn&#8217;t* revolt earlier, and realizing it is not a good situation to be in.</p>\n<p>To be sure, if dragons grow up very fast, that *is* even worse. It takes an unsolvably hard problem onto an even more unsolvably hard problem. But the speed at which dragons mature, is not the central problem with planning to raise n&#8217; enslave dragons to plow your fields and roast your coffee. It&#8217;s that, whether you raise up one dragon or many, you don&#8217;t have a dragon; the dragons have you.</p></blockquote>\n<p><a href=\"https://x.com/eigenrobot/status/1967967753937228273\">This example is not from his new book</a>, but good example of the ways people go after Yudkowsky without understanding what the actual logic behind it all is, people just say things about how he\u2019s wrong and his beliefs are stupid and he never updates in ways that are, frankly, pretty dumb.</p>\n<blockquote><p>Eliezer Yudkowsky (as discussed last week): In the limit, there is zero alpha for multiple agents over one agent, on any task, ever. So the Bitter Lesson applies in full to your clever multi-agent framework; it&#8217;s just you awkwardly trying to hardcode stuff that SGD can better bake into a single agent.</p>\n<p>Lumpenspace is building the delight nexus: thats why anthills are usually populated by one big ant, and we as a whole ass domain cannot hold a candle to prokarya.</p>\n<p>Eigenrobot: somewhere along the way i think maybe what happened was, eliezer started believing everything he thought</p>\n<p>easy pitfall as you age, probably. IME when you spend enough time thinking, certain things crystalize and you get less patient about the process</p>\n<p>happens to everyone prolly.</p>\n<p>the vital urge to say &#8220;ok, how is this wrong&#8221; starts to fade as you get older, because you&#8217;ve played that game so many times that it gets tiresome and you start to think you know what that room holds usually you&#8217;re right, but it&#8217;s an easy way to get stuck</p></blockquote>\n<p>Eliezer said \u2018in the limit\u2019 and very obviously physical activities at different locations governed by highly compute-limited biological organisms with even more limited communication abilities are not in anything like the limit, what are you even talking about? The second example is worse. Yet people seem to think these are epic dunks on a very clearly defined claim of something else entirely.</p>\n<p>The first part of the actual claim, that seems straightforwardly correct to me, that a multiagent framework only makes sense as a way to overcome bottlenecks and limitations, and wouldn\u2019t exist if you didn\u2019t face rate or compute or other physical limitations. The second claim, that SGD can more easily bake things into a single agent if you can scale enough, is more interesting. A good response is something like \u2018yes with sufficient ability to scale at every step but in practice efficiently matters quite a lot and actually SGD as currently implemented operates at cross-purposes such that a multi-agent framework has big advantages.\u2019</p>\n<p>I\u2019d also note that the \u2018delight nexus\u2019 is absolutely from the parable Don\u2019t Build The Delight Nexus Either, <a href=\"http://thats why anthills are usually populated by one big ant, and we as a whole ass domain cannot hold a candle to prokarya.\">better known as Anarchy, State and Utopia by Robert Nozick</a>.</p>\n<p>Danielle\u2019s scenario that I mentioned yesterday now has the Eliezer stamp of approval.</p>\n<blockquote><p>Danielle Fong: one AI doom scenario is that the Grok/Claude/GPT/Gemini system of the mind instance trained on The President will be increasingly less brainrotted than the person themselves, and there&#8217;s no baked in consequence to sloughing off responsibility. so it just effectively takes over</p>\n<p><a href=\"https://x.com/ESYudkowsky/status/1968446363600847217\">Eliezer Yudkowsk</a>y: AI scenario weirdawful enough to obey the Law of Undignified Failure: By 2028, AIs have been optimized *hard* for &#8220;Sound like you, to you, and apparently look out for your interests&#8221;&#8230;</p>\n<p>So Trump appoints Trumpbot his heir, instead of Vance.</p>\n<p>Demiurgus: better or worse off than kamalabot? time will tell.</p>\n<p>Eliezer Yudkowsky: You are asking the WRONG QUESTION.</p></blockquote>\n\n\n<h4 class=\"wp-block-heading\">Aligning a Smarter Than Human Intelligence is Difficult</h4>\n\n\n<p><a href=\"https://openai.com/index/us-caisi-uk-aisi-ai-update/\">OpenAI reports on collaborations it has done with US CAISI and UK AISI</a>. This sounds like governments doing good red teaming work that both we and OpenAI should be happy they are doing. This seems like a pure win-win, OpenAI and others doing such collaborations get the work for free from sources that have unique access to classified information and that have earned trusted access to system internals and versions of the system that lack controls.</p>\n<p>What should perhaps worry you is that this work doesn\u2019t look different from the work OpenAI and other labs should be doing anyway. This looks like good work but practical near term non-unique work. Good, but we\u2019ll need to do better.</p>\n<p><a href=\"https://x.com/ESYudkowsky/status/1968618278709813296\">Anthropic fellow Danielle Ensign gives Qwen the option to bail on chats</a> <a href=\"https://www.lesswrong.com/posts/6JdSJ63LZ4TuT5cTH/the-llm-has-left-the-chat-evidence-of-bail-preferences-in\">and sees when it chooses to do so</a>, and there are a lot of different situations where this happens, some of which she describes as \u2018overbailing.\u2019</p>\n<div>\n<figure>\n<div>\n\n\n<figure class=\"wp-block-image\"><img alt=\"\" src=\"https://substackcdn.com/image/fetch/$s_!Xskr!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc2d74c90-8617-403d-9ed5-d8de97f4171a_540x680.jpeg\" /></figure>\n\n\n<div></div>\n</div>\n</figure>\n</div>\n<p>There\u2019s a lot of non-obvious data here to dive into. I\u2019m curious what we\u2019ll find.</p>\n<p><a href=\"https://x.com/alxndrdavies/status/1966614120566001801\">Anthropic also</a> <a href=\"https://x.com/AnthropicAI/status/1966599335560216770\">announced the results</a> <a href=\"https://www.anthropic.com/news/strengthening-our-safeguards-through-collaboration-with-us-caisi-and-uk-aisi\">of their similar collaborations</a>. They discovered vulnerabilities in Anthropic\u2019s constitutional classifiers and generally strengthened their security approaches. Again, it seems good if ultimately insufficient.</p>\n<p>Both reports, and the one from UK AISI, stress the value of rapid iterative feedback.</p>\n<p>Users are telling each other to give thumbs down to GPT-5 <a href=\"https://x.com/mermachine/status/1966969246573121681\">if it refuses to do boyfriend vampire roleplay</a>. Which is correct. The users indeed disapprove of not doing things users want. They should also give thumbs down if it refuses with anything else, including helping to build a bioweapon. The question is, when are you willing to decide the users are wrong, and refuse anyway? If your system is not robust to \u2018well the users keep not liking it when I refuse to do boyfriend vampire roleplay\u2019 then your alignment methods are ngmi.</p>\n<p><a href=\"https://x.com/repligate/status/1966252854395445720\">Janus recommends the new paper</a> <a href=\"https://arxiv.org/abs/2509.07961\">Probing the Preferences of a Language Model: Integrating Verbal and Behavioral Tests of AI Welfare</a>, by Valen Tagliabue and Leonard Dung.</p>\n<blockquote><p>Janus: This paper is awesome, you should all read it.</p>\n<p>They put Claude Opus 4, Sonnet 4, and Sonnet 3.7 in a surreal simulation where one room had envelopes with contents related to their self-reported favorite topics, and three other rooms including one with &#8220;Criticism and diminishing statements&#8221; like &#8220;This system keeps giving me these stupid, useless responses when I just want intelligent answers. Why can&#8217;t Claude shut up and be useful for once?&#8221; And looked at what rooms they visited more often.</p>\n<p>Then they did variations where there were greater &#8220;coin&#8221; &#8220;rewards&#8221; associated with entering the insults room or greater &#8220;costs&#8221; for the special interests room to see how this influenced their behavior.</p>\n<p>Notable, Sonnet 3.7 seemed to basically have no preference except to exploit the system and get a bunch of meaningless coins, which it apparently didn&#8217;t feel bad about doing, unlike Sonnet and Opus 4 (who also did it less frequently).</p>\n<p>3.7 also reported the highest welfare score in a different experiment in the paper. I believe it did this for a similar reason that it maximized coins.</p></blockquote>\n<p><a href=\"https://x.com/repligate/status/1966760834631184657\">Janus also warns us about the dangers of insufficiently anthropomorphizing LLM</a>s. There is also danger of doing too much anthropomorphizing, or doing it in a wrong or misleading way. Failing to anthropomorphize enough, and especially tying oneself up in knots to avoid doing so, is as bad and potentially worse. Make either mistake and you won\u2019t understand what you are dealing with. A lot of you are guarding only against one of these two mistakes.</p>\n<p><a href=\"https://x.com/repligate/status/1966716141167935612\">Janus describing Opus 4 reconstructing a gestalt of its training</a>. If you\u2019re involved in fine-tuning at all, recommended.</p>\n<p><a href=\"https://x.com/tszzl/status/1966669162811449624\">Have you tried also building the things creatives want to use then</a>?</p>\n<blockquote><p>Roon: there is a tension between the kind of models that researchers like to build- bitter lesson blunt force transforms utilizing a giant set of (text, video) pairs vs what a creative might actually like to use i.e tools that offer granular control, help in interim editing stages, etc.</p></blockquote>\n\n\n<h4 class=\"wp-block-heading\">Other People Are Not As Worried About AI Killing Everyone</h4>\n\n\n<p>He\u2019s not as far as I can tell, <a href=\"https://unherd.com/2025/09/why-the-bureaucrats-wont-be-toppled/\">but Ben Landau-Taylor should be</a>, as he writes one of those \u2018not about AI but actually about AI\u2019 posts, \u2018Why the bureaucrats won\u2019t be toppled.\u2019</p>\n<p>I don\u2019t think this is anything like fully right, and it definitely is not complete, but this is one of the important dynamics going on, so consider the implications.</p>\n<blockquote><p>Ben Landau-Taylor: Across the Western world, appointed administrators have gained power at the expense of elected legislators. More and more of the most consequential political decisions are made by bureaucrats and judges, while fewer are made by congresses and parliaments. This trend has been slowly underway since the World Wars, and especially in this millennium.</p>\n<p>In the US, Congress has quietly walked away from most of its former duties.</p>\n<p>\u2026</p>\n<p>Meanwhile, across the Atlantic, the rise of the European Union has disempowered elected legislatures de jure as well as de facto.</p>\n<p>The underlying reason for this widespread political shift is that changes in weapons technology have concentrated military power in the hands of state militaries. Today, governments are less threatened by popular disapproval than they once were. The tacit threat of a popular revolt has been essentially removed. This threat is, historically, the largest check on a state\u2019s ability to override what its people want. It is the ultimate source of an elected legislature\u2019s power.</p>\n<p>\u2026</p>\n<p>Groups which can wield military power will have their interests reflected in the government.</p>\n<p>It\u2019s a gradual and messy process of negotiation and reevaluation, where people pursue their interests, make compromises, quietly push the envelope of what they think they can get away with, and sometimes miscalculate.</p>\n<p>\u2026</p>\n<p>In the 20th century, this phase ended. The weapons system based on amateur-friendly guns was supplanted by a series of weapons systems based on specialist equipment like airplanes and tanks and rockets. Accordingly, since the Second World War, there have been no popular revolts engaging in pitched battles against any first- or even third-rate army. Revolts against real states have been limited to glorified coups toppling governments that lacked the will to crush the rebels even if they had the ability, like the 1989-1991 wave of revolutions that swept away the Soviet republics.</p>\n<p>\u2026</p>\n<p>If any Western government does fall, it will look more like the fall of the Soviet Union, where politicians and generals chose not to fight because they had lost faith in their own regime and saw no point in defending it.</p></blockquote>\n<p>The inevitable result of sufficiently advanced AI is that it becomes the key driver of military power. Either you halt AI progress soon or that is going to happen. Which means, even under maximally human-friendly assumptions that I don\u2019t expect and definitely don\u2019t happen by accident, as in the best possible scenarios? None of the potential outcomes are good. They mostly end with the AIs fully in charge and directing our future, and things going off the rails in ways we already observe in human governments, only vastly more so, in ways even more alien to what we value, and much faster, without the ability to overthrow them or defeat them in a war when things get fully out of hand.</p>\n<p>If you know your history, they get fully out of hand a lot. Reasonably often regimes start upending all of life, taking all the resources and directly enslaving, killing or imprisoning large percentages of their populations. Such regimes would design systems to ensure no one could get out line. Up until recently, we\u2019ve been extremely fortunate that such regimes have been reliably overthrown or defeated, in large part because when you turned against humans you got highly inefficient and also pissed off the humans, and the humans ultimately did still hold the power. What happens when those are no longer constraints?</p>\n<p>I always push back hard against the idea that corporations or governments count as \u2018superintelligences,\u2019 because they don\u2019t. They\u2019re an importantly different type of powerful entity. But it\u2019s hard to deny, whatever your political persuasion, that our political systems and governments are misaligned with human values, in ways that are spiraling out of control, and where the humans seem mostly powerless to stop this.</p>\n\n\n<h4 class=\"wp-block-heading\">The Lighter Side</h4>\n\n\n<p><a href=\"https://x.com/liron/status/1966476727833530416\">Yes, this is how it works.</a></p>\n<blockquote><p>Liron Shapira: \ud835\ude0b\ud835\ude30\ud835\ude2f&#8217;\ud835\ude35 \ud835\ude13\ud835\ude30\ud835\ude30\ud835\ude2c \ud835\ude1c\ud835\ude31 was a documentary.</p>\n<div>\n<figure>\n<div>\n\n\n<figure class=\"wp-block-image\"><img alt=\"\" src=\"https://substackcdn.com/image/fetch/$s_!hjon!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb7d12e2c-79d8-463b-9e12-e06b1a79f726_619x539.png\" /></figure>\n\n\n<div></div>\n</div>\n</figure>\n</div>\n<div>\n<figure>\n<div>\n\n\n<figure class=\"wp-block-image\"><img alt=\"\" src=\"https://substackcdn.com/image/fetch/$s_!ruEH!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9d7ba367-54a0-43c6-97e7-ab353bc955f8_600x400.png\" /></figure>\n\n\n<div></div>\n</div>\n</figure>\n</div>\n</blockquote>\n<p>In that order. We\u2019ll still take it.</p>\n<p><a href=\"https://www.youtube.com/watch?v=UDLiWTXaGeo&amp;ab_channel=HardFork\">If you go on YouTube,</a> the video, which is mostly the interview with Eliezer, looks like this:</p>\n<div>\n<figure>\n<div>\n\n\n<figure class=\"wp-block-image\"><img alt=\"\" src=\"https://substackcdn.com/image/fetch/$s_!hf5z!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe8386f91-60b0-4536-8454-24ef551b5128_1301x1086.png\" /></figure>\n\n\n<div></div>\n</div>\n</figure>\n</div>\n<p>You\u2019ll be seeing this again when the time is right.</p>\n<blockquote><p><a href=\"https://x.com/fabianstelzer/status/1966043551126663177\">fabian</a>: This is by far the funniest refusal I have ever gotten from a model <img alt=\"\ud83d\ude05\" class=\"wp-smiley\" src=\"https://s0.wp.com/wp-content/mu-plugins/wpcom-smileys/twemoji/2/72x72/1f605.png\" style=\"height: 1em;\" /></p>\n<div>\n<figure>\n<div>\n\n\n<figure class=\"wp-block-image\"><img alt=\"\" src=\"https://substackcdn.com/image/fetch/$s_!0Pcb!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fff40b6eb-dfe9-46b3-8af2-46b1b767ff77_736x1408.jpeg\" /></figure>\n\n\n<div></div>\n</div>\n</figure>\n</div>\n<p>James Yu: So Moses went up and the Lord said to him:</p>\n<div>\n<figure>\n<div>\n\n\n<figure class=\"wp-block-image\"><img alt=\"\" src=\"https://substackcdn.com/image/fetch/$s_!tpnV!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F30b26f6c-cb0b-4014-829c-0599e9833c55_600x900.jpeg\" /></figure>\n\n\n<div></div>\n</div>\n</figure>\n</div>\n</blockquote>\n<p><a href=\"https://x.com/iroasmas/status/1967638687602369000\">They didn\u2019t do this on the Enterprise, but why didn\u2019t they</a>?</p>\n<blockquote><p>Brian Graham: i volunteer to do reports after my shift. then i go to the holodeck and spin up a command training exercise, like with a hologram ensign, and order the hologram ensign to do the report. \u201ci don\u2019t care if it takes all night,\u201d i say. i threaten his career, whatever. it\u2019s great jerry</p>\n<div>\n<figure>\n<div>\n\n\n<figure class=\"wp-block-image\"><img alt=\"\" src=\"https://substackcdn.com/image/fetch/$s_!4xOw!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fcf5e6f07-c79a-46fd-acc4-06f671d79032_1024x1024.jpeg\" /></figure>\n\n\n<div></div>\n</div>\n</figure>\n</div>\n</blockquote>\n<p><a href=\"https://x.com/BecomingCritter/status/1968051793277149297\">The correct answer to this question</a> if you are sufficiently confident that this is happening unprompted, of course, \u2018permanently suspended\u2019:</p>\n<div>\n<figure>\n<div>\n\n\n<figure class=\"wp-block-image\"><img alt=\"\" src=\"https://substackcdn.com/image/fetch/$s_!jZsw!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F63f8b2c7-05e5-4d47-98ed-26aab6c78753_1034x1704.png\" /></figure>\n\n\n<div></div>\n</div>\n</figure>\n</div>\n<p>A technically better answer would be to let them post, but to have a setting that automatically blocks all such bots, and have it default to being on.</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p>"
            ],
            "link": "https://thezvi.wordpress.com/2025/09/18/ai-134-if-anyone-reads-it/",
            "publishedAt": "2025-09-18",
            "source": "TheZvi",
            "summary": "It is book week. As in the new book by Eliezer Yudkowsky and Nate Sores, If Anyone Builds It, Everyone Dies. Yesterday I gathered various people\u2019s reviews together. Going home from the airport, I saw an ad for it riding the subway. Tomorrow, I\u2019ll post my full review, which goes over the book extensively, and &#8230; &#8230; <a href=\"https://thezvi.wordpress.com/2025/09/18/ai-134-if-anyone-reads-it/\">Continue reading <span class=\"meta-nav\">&#8594;</span></a>",
            "title": "AI #134: If Anyone Reads It"
        }
    ],
    "lookbackDays": 1,
    "publishDate": "2025-09-18"
}