{
    "articles": [
        {
            "content": [
                "<ol>\n  <li>\n    <p>That your dog, while she appears to love you only because she\u2019s been adapted by evolution to appear to love you, really does love you.</p>\n  </li>\n  <li>\n    <p>That if you\u2019re a life form and you cook up a baby and copy your genes to them, you\u2019ll find that the genes have been degraded due to oxidative stress et al., which isn\u2019t cause for celebration, but if you find some other hopefully-hot person and randomly swap in half of their genes, your baby will still be somewhat less fit compared to you and your hopefully-hot friend on average, but now there is variance, so if you cook up several babies, one of them might be as fit or even fitter than you, and that one will likely have more babies than your other babies have, and thus complex life can persist in a universe with increasing entropy.</p>\n  </li>\n  <li>\n    <p>That if we wanted to, we surely <em>could</em> figure out which of the 300-ish strains of rhinovirus are circulating in a given area at a given time and rapidly vaccinate people to stop it and thereby finally \u201ccure\u201d the common cold, and though this is too annoying to pursue right now, it seems like it\u2019s just a matter of time.</p>\n  </li>\n  <li>\n    <p>That if you look back at history, you see that plagues went from Europe to the Americas but not the other way, which suggests that urbanization and travel are great allies for infectious disease, and these both continue today but are held in check by sanitation and vaccines even while we have lots of tricks like UVC light and high-frequency sound and air filtration and waste monitoring and paying people to stay home that we\u2019ve barely even put in play.</p>\n  </li>\n  <li>\n    <p>That while engineered infectious diseases loom ever-larger as a potential very big problem, we also have lots of crazier tricks we <em>could</em> pull out like panopticon viral screening or toilet monitors or daily individualized saliva sampling or engineered microbe-resistant surfaces or even dividing society into cells with rotating interlocks or having people walk around in little personal spacesuits, and while admittedly most of this doesn\u2019t sound awesome, I see no reason this shouldn\u2019t be a battle that we would win.</p>\n  </li>\n  <li>\n    <p>That clean water, unlimited, almost free.</p>\n  </li>\n  <li>\n    <p>That dentistry.</p>\n  </li>\n  <li>\n    <p>That tongues.</p>\n  </li>\n  <li>\n    <p>That radioactive atoms either release a ton of energy but also quickly stop existing\u2014a gram of Rubidium-90 scattered around your kitchen emits as much energy as ~200,000 incandescent lightbulbs but after an hour only 0.000000113g is left\u2014or don\u2019t put out very much energy but keep existing for a long time\u2014a gram of Carbon-14 only puts out the equivalent of 0.0000212 light bulbs but if you start with a gram, you\u2019ll still have 0.999879g after a year\u2014so it isn\u2019t actually <em>that</em> easy to permanently poison the environment with radiation although Cobalt-60 with its medium energy output and medium half-life is unfortunate, medical applications notwithstanding I still wish Cobalt-60 didn\u2019t exist, screw you Cobalt-60.</p>\n  </li>\n  <li>\n    <p>That while curing all cancer would only increase life expectancy by ~3 years and curing all heart disease would only increase life expectancy by ~3 years, and preventing all accidents would only increase life expectancy by ~1.5 years, if we did all of these at the same time and then a lot of other stuff too, eventually the effects would go nonlinear, so trying to cure cancer isn\u2019t actually a waste of time, thankfully.</p>\n  </li>\n  <li>\n    <p>That the peroxisome, while the mitochondria and their stupid Krebs cycle get all the attention, when a fatty-acid that\u2019s too long for them to catabolize comes along, who you gonna call.</p>\n  </li>\n  <li>\n    <p>That we have preferences, that there\u2019s no agreed ordering of how good different things are, which is neat, and not something that would obviously be true for an alien species, and given our limited resources probably makes us happier on net.</p>\n  </li>\n  <li>\n    <p>That cardamom, it is cheap but tastes expensive, if cardamom cost 1000\u00d7 more, people would brag about how they flew to Sri Lanka so they could taste chai made with fresh cardamom and swear that it changed their whole life.</p>\n  </li>\n  <li>\n    <p>That Gregory of Nyssa, he was right.</p>\n  </li>\n  <li>\n    <p>That Grandma Moses, it\u2019s not too late.</p>\n  </li>\n  <li>\n    <p>That sleep, that probably evolution first made a low-energy mode so we don\u2019t starve so fast and then layered on some maintenance processes, but the effect is that we live in a cycle and when things aren\u2019t going your way it\u2019s comforting that reality doesn\u2019t stretch out before you indefinitely but instead you can look forward to a reset and a pause that\u2019s somehow neither experienced nor skipped.</p>\n  </li>\n  <li>\n    <p>That, glamorous or not, comfortable or not, cheap or not, carbon emitting or not, air travel is very safe.</p>\n  </li>\n  <li>\n    <p>That, for most of the things you\u2019re worried about, the markets are less worried than you and they have the better track record, though not the issue of your mortality.</p>\n  </li>\n  <li>\n    <p>That sexual attraction to romantic love to economic unit to reproduction, it\u2019s a strange bundle, but who are we to argue with success.</p>\n  </li>\n  <li>\n    <p>That every symbolic expression recursively built from differentiable elementary functions has a derivative that can also be written as a recursive combination of elementary functions, although the latter expression may require vastly more terms.</p>\n  </li>\n  <li>\n    <p>That every expression graph built from differentiable elementary functions and producing a scalar output has a gradient that can itself be written as an expression graph, and furthermore that the latter expression graph is always the same size as the first one and is easy to find, and thus that it\u2019s possible to fit very large expression graphs to data.</p>\n  </li>\n  <li>\n    <p>That, eerily, biological life and biological intelligence does not appear to make use of that property of expression graphs.</p>\n  </li>\n  <li>\n    <p>That if you look at something and move your head around, you observe the entire light field, which is a five-dimensional function of three spatial coordinates and two angles, and yet if you do something fancy with lasers, somehow that entire light field can be stored on a single piece of normal two-dimensional film and then replayed later.</p>\n  </li>\n  <li>\n    <p>That, as far as I can tell, the reason five-dimensional light fields can be stored on two-dimensional film simply cannot be explained without quite a lot of wave mechanics, a vivid example of the strangeness of this place and proof that all those physicists with their diffractions and phase conjugations really are up to something.</p>\n  </li>\n  <li>\n    <p>That disposable plastic, littered or not, harmless when consumed as thousands of small particles or not, is popular for a reason.</p>\n  </li>\n  <li>\n    <p>That disposable plastic, when disposed of correctly, is literally carbon sequestration, and that if/when air-derived plastic replaces dead-plankton-derived plastic, this might be incredibly convenient, although it must be said that currently the carbon in disposable plastic only represents a single-digit percentage of total carbon emissions.</p>\n  </li>\n  <li>\n    <p>That rocks can be broken into pieces and then you can\u2019t un-break the pieces but you can check that they came from the same rock, it\u2019s basically cryptography.</p>\n  </li>\n  <li>\n    <p>That the deal society has made is that if you have kids then everyone you encounter is obligated to chip in a bit to assist you, and this seems to mostly work without the need for constant grimy negotiated transactions as Econ 101 would suggest, although the exact contours of this deal seem to be a bit murky.</p>\n  </li>\n  <li>\n    <p>That of all the humans that have ever lived, the majority lived under some kind of autocracy, with the rest distributed among tribal bands, chiefdoms, failed states, and flawed democracies, and only something like 1% enjoyed free elections and the rule of law and civil liberties and minimal corruption, yet we endured and today that number is closer to 10%, and so if you find yourself outside that set, do not lose heart.</p>\n  </li>\n  <li>\n    <p>That if you were in two dimensions and you tried to eat something then maybe your body would split into two pieces since the whole path from mouth to anus would have to be disconnected, so be thankful you\u2019re in three dimensions, although maybe you could have some kind of jigsaw-shaped digestive tract so your two pieces would only jiggle around or maybe you could use the same orifice for both purposes, remember that if you ever find yourself in two dimensions, I guess.</p>\n  </li>\n</ol>\n\n<p>(<a href=\"https://dynomight.net/thanks/\">previously</a>, <a href=\"https://dynomight.net/thanks-2/\">previously</a>, <a href=\"https://dynomight.net/thanks-3/\">previously</a>, <a href=\"https://dynomight.net/thanks-4\">previously</a>)</p>"
            ],
            "link": "https://dynomight.net/thanks-5/",
            "publishedAt": "2025-11-27",
            "source": "Dynomight",
            "summary": "<ol> <li> <p>That your dog, while she appears to love you only because she\u2019s been adapted by evolution to appear to love you, really does love you.</p> </li> <li> <p>That if you\u2019re a life form and you cook up a baby and copy your genes to them, you\u2019ll find that the genes have been degraded due to oxidative stress et al., which isn\u2019t cause for celebration, but if you find some other hopefully-hot person and randomly swap in half of their genes, your baby will still be somewhat less fit compared to you and your hopefully-hot friend on average, but now there is variance, so if you cook up several babies, one of them might be as fit or even fitter than you, and that one will likely have more babies than your other babies have, and thus complex life can persist in a universe with increasing entropy.</p> </li> <li> <p>That if we wanted to, we surely <em>could</em> figure out which of the 300-ish strains of rhinovirus are circulating in a given area at a given time and rapidly vaccinate people to stop it and thereby finally \u201ccure\u201d the common cold, and though this is too annoying to pursue right",
            "title": "Underrated reasons to be thankful V"
        },
        {
            "content": [
                "<p>Hello and welcome to a jam-packed edition of the Ink &amp; Switch Dispatch! It begins with our first public update about the ARIA Safeguarded AI Programme, introducing our project GAIOS and the new lab staff working on it, followed by a report on its implementation of our new local-first auth system Keyhive. We\u2019ll share three presentations from the LIVE 2025 conference, and cue-up a new lab note from the Beckett video game version control project. Finally, we have two new researchers-in-residence in the spotlight, and a fresh coat of yellow paint for the Automerge website. Phew!</p>\n<h2 id=\"gaios\"><a class=\"plain\" href=\"https://www.inkandswitch.com/index.xml#gaios\">GAIOS</a></h2>\n<p>In the <a href=\"https://www.inkandswitch.com/newsletter/dispatch-013/\">previous dispatch</a> we announced that Ink &amp; Switch has joined the ARIA Safeguarded AI Programme (SGAI). Put plainly, the programme\u2019s goal is to develop a suite of tools for modelling complex systems to determine how they might be augmented with AI. To that end we are building GAIOS, a malleable, collaborative software platform derived from our <a href=\"https://www.inkandswitch.com/project/patchwork/\">Patchwork</a> project. GAIOS will allow other teams participating in the SGAI programme to unite their modelling and simulation tools, building atop a common foundation with interoperability and version control powered by <a href=\"https://automerge.org\">Automerge</a>.  We\u2019ll share more of GAIOS, including its source code, throughout the coming year.</p>\n<figure>\n<img src=\"https://www.inkandswitch.com/gaios.webp\" />\n<figcaption>\n<p>The GAIOS logo, designed by Todd Matthews.</p>\n</figcaption>\n</figure>\n<p>The team working on GAIOS includes many longtime Ink &amp; Switch researchers, and a few new faces:</p>\n<ul>\n<li>The mononymous <a href=\"https://grjte.sh\">grjte</a> will lend her expertise in zero-knowledge proofs and programmable cryptography to the GAIOS team. Previously at Bain Capital Crypto, grjte recently shared <a href=\"https://groundmist.xyz\">Groundmist</a>, a series of experiments building local-first tools that leverage the AT Protocol and Automerge.</li>\n<li>Maciek Sakrejda worked on Postgres tooling for most of his career but has now seen the local-first light. He\u2019s already an active contributor to <a href=\"https://github.com/automerge/automerge-repo\">Automerge Repo</a>, and since joining the lab has helped land presence indicators for our in-house instance of Patchwork. His full-stack skills will be instrumental as we prepare GAIOS for real-world use.</li>\n<li>Fresh from presenting at LIVE 2025, <a href=\"https://www.orionreed.com\">Orion Reed</a> brings his knack for quick-and-compelling prototypes, programmable canvas tools, and malleable substrates to the lab. Orion\u2019s work spans disciplines, leading him to collaborate with varied groups like <a href=\"https://folkjs.org\">FolkJS</a>, the mysterious <a href=\"https://tentpole.computer\">Tentpole Collective</a> (with researcher-in-residence alumni Elliot Evans and Lu Wilson), and the <a href=\"https://www.libcomp.org\">Liberatory Computing</a> collective.</li>\n</ul>\n<h2 id=\"keyhive\"><a class=\"plain\" href=\"https://www.inkandswitch.com/index.xml#keyhive\">Keyhive</a></h2>\n<p>We\u2019ve accelerated our work on <a href=\"https://www.inkandswitch.com/project/keyhive/\">Keyhive</a>, a capabilities-based auth system specially designed for local-first software, to prepare it for GAIOS. Adding access control is an important step for collaborative systems like GAIOS and Patchwork: it defines who can connect, who can request documents, and lays the groundwork for richer permissions in the future. What follows is a lightly-technical summary of the most recent work on Keyhive, written by Brooklyn Zelenka and John Mumm.</p>\n<figure>\n<img src=\"https://www.inkandswitch.com/keyhive-poc.png\" />\n<figcaption>\n<p>Sharing a document in the Keyhive proof-of-concept.</p>\n</figcaption>\n</figure>\n<h3 id=\"integration-into-gaios\"><a class=\"plain\" href=\"https://www.inkandswitch.com/index.xml#integration-into-gaios\">Integration into GAIOS</a></h3>\n<p><a href=\"https://github.com/automerge/automerge-repo\">Automerge Repo</a> started with no ability to do auth. We added integration points to Automerge Repo for authorization APIs, and then plugged in Keyhive as the auth provider. This gives us a natural point to enforce capabilities whenever a document is accessed while maintaining the existing Automerge Repo APIs. This integration provides the following:</p>\n<ul>\n<li>All messages sent or received are signed with the local non-extractable signing key</li>\n<li>Messages for a particular document are authorized by checking the access rights of the authenticated sender of the message against the local Keyhive instance</li>\n<li>Keyhive\u2019s ops are synchronized over the same network stack as the rest of Automerge Repo, but currently with a naive diffing mechanism which we plan to replace with the new sync system (Subduction \u2014 more on that below)</li>\n</ul>\n<p>The end result is that all documents in GAIOS are protected at the network boundary by the Keyhive access control CRDT and GAIOS guest applications are passed an interface to manage this access control state without needing to know about storage or synchronization. This is an initial integration, and ongoing work includes adding E2EE and enforcing revocations on backdated Automerge content.</p>\n<h3 id=\"share-menu-graphical-interface\"><a class=\"plain\" href=\"https://www.inkandswitch.com/index.xml#share-menu-graphical-interface\">\u201cShare Menu\u201d Graphical Interface</a></h3>\n<p>In parallel, we began prototyping a \u201cshare\u201d (permissions) dialog for users. This menu provides a clear UI to invite collaborators, adjust permissions, and revoke access. The goal is to make capability management feel familiar \u2014 more like adding a collaborator in Google Docs, less like managing cryptographic keys. Behind the scenes, these actions map to issuing or revoking Keyhive capabilities. This design aims to connect the user mental model (\u201csharing a document\u201d) with the technical enforcement layer (capabilities).</p>\n<p>We\u2019ve recorded <a href=\"https://www.inkandswitch.com/keyhive-demo.mp4\">a short video demonstrating this early UI prototype</a> if you\u2019d like to see it in action.</p>\n<h3 id=\"subduction\"><a class=\"plain\" href=\"https://www.inkandswitch.com/index.xml#subduction\">Subduction</a></h3>\n<p>One recurring challenge in CRDT-based systems is synchronization at scale. As more users join, and as documents grow larger, our reference sync server implementation has shown stress points:</p>\n<ul>\n<li>It maintains a fully materialized version of every document in memory.</li>\n<li>It computes hashes and related states eagerly for each request.</li>\n<li>Being written in Node.js \u2014 which being 32-bit has a 4GB memory limit \u2014 it can run into memory pressure.</li>\n</ul>\n<p>The result: servers sometimes time out under heavy load, particularly when handling large diffs or very active documents.</p>\n<p>To address this, we\u2019ve been developing an alternative sync strategy we call Subduction. Instead of materializing entire documents, Subduction deterministically chunks documents and streams only the minimal state needed to reconcile peers.</p>\n<figure>\n<img src=\"https://www.inkandswitch.com/sedimentree.png\" />\n<figcaption>\n<p>An illustration of document chunking.</p>\n</figcaption>\n</figure>\n<p>We believe that this should result in:</p>\n<ul>\n<li>Improved scalability: less memory overhead, much less CPU use per request.</li>\n<li>Better performance: faster diffs, quicker round-trips.</li>\n<li>Set us up for E2EE documents since it doesn\u2019t need to know anything about the underlying document other than some hash identifiers.</li>\n</ul>\n<p>We are finalizing the integration of Subduction with Automerge Repo, and hardening the integration of Keyhive in GAIOS. Subduction and Keyhive are nearly ready for their public debut \u2014 after <em>just a little bit more</em> internal testing and dogfooding.</p>\n<h2 id=\"live-2025\"><a class=\"plain\" href=\"https://www.inkandswitch.com/index.xml#live-2025\">LIVE 2025</a></h2>\n<p>In September we participated in <a href=\"https://liveprog.org/live-2025\">LIVE 2025</a>, the 11th Workshop on Live Programming. Here are three of the presentations from folks affiliated with the lab.</p>\n<figure>\n<img src=\"https://www.inkandswitch.com/live.webp\" />\n<figcaption>\n<p>Screenshots from the three live presentations, showing a poker game with various statistics and scenarios in Ambsheets, an illustration of Nova\u2019s stacks, and a few strategies for adversarial interoperability.</p>\n</figcaption>\n</figure>\n<h3 id=\"a-spreadsheet-for-exploring-scenarios\"><a class=\"plain\" href=\"https://www.inkandswitch.com/index.xml#a-spreadsheet-for-exploring-scenarios\">A spreadsheet for exploring scenarios</a></h3>\n<p>In <a href=\"https://www.inkandswitch.com/project/ambsheets/live25/\">Ambsheets: A spreadsheet for exploring scenarios</a>, Alex Warth, Geoffrey Litt, and Avi Bryant introduce Ambsheets, a spreadsheet that lets you model uncertainty and explore multiple scenarios simultaneously. The paper builds on their <a href=\"https://www.inkandswitch.com/ambsheets/notebook/\">earlier explorations</a>, discusses limitations they discovered, and presents a new version of the language that addresses those limitations. Also make sure to check out the <a href=\"https://www.youtube.com/watch?v=EtC2XiGFh7E\">video</a>.</p>\n<h3 id=\"nova\"><a class=\"plain\" href=\"https://www.inkandswitch.com/index.xml#nova\">Nova</a></h3>\n<p>June Gardner, a researcher-in-residence at the lab, presented her work on <a href=\"https://nova-lang.net\">Nova</a>, a lightweight model of computing that uses rule-based modelling and physical metaphor to bridge the gap between physical media (pen/pencil, notecards/paper) and the programming of digital, electronic computers. <a href=\"https://www.youtube.com/watch?v=Um_LXirqW1k\">The talk</a> covers the basics of Nova as well as its advantages and current development status, as well as some hints at what the future might hold for Nova as a programming language.</p>\n<h3 id=\"live-programming-in-hostile-territory\"><a class=\"plain\" href=\"https://www.inkandswitch.com/index.xml#live-programming-in-hostile-territory\">Live Programming in Hostile Territory</a></h3>\n<p>Orion Reed, who recently joined the lab to help build GAIOS, collaborated with Christopher Shank on Live Programming in Hostile Territory. The <a href=\"https://folkjs.org/live-2025/\">paper </a> and <a href=\"https://www.youtube.com/watch?v=540g_lxcOEg\">talk</a> argue that live programming research should broaden its purview from the creation of new environments to the augmenting of existing ones and, through a selection of prototypes, explore three adversarial strategies for introducing programmatic capabilities into existing environments which are unfriendly or antagonistic to modification.</p>\n<h2 id=\"version-control-for-space-and-structure\"><a class=\"plain\" href=\"https://www.inkandswitch.com/index.xml#version-control-for-space-and-structure\">Version Control for Space and Structure</a></h2>\n<p>Beckett \u2014 our exploration of version control inside the Godot game engine \u2014 continues apace. We\u2019re pleased to share that <a href=\"https://ashlinduncan.com\">Lilith Duncan</a> has joined the team. Lilith is a gamedev and toolmaker, most recently at Unity focused on worldbuilding and environment tools. Lilith and the rest of the team were in-person at GodotFest Munich, and they received a glowing response to various demos of their work-in-progress.</p>\n<p>Speaking of work-in-progress, we\u2019ve just published the first Beckett lab note, <a href=\"https://www.inkandswitch.com/project/beckett/notebook/01/\">Version Control for Space and Structure</a>. The note describes our latest findings after working to integrate diff visualizations into the Godot editor viewport and handling various structural changes within Godot scene files. These scene files are stored as text, making it <em>possible</em> to track them with traditional version-control systems (like git), but these systems fail to uphold important structural relationships within the files, leading to broken, unopenable projects. Read the lab note to learn how Beckett overcomes these issues.</p>\n<h2 id=\"and-points-beyond\"><a class=\"plain\" href=\"https://www.inkandswitch.com/index.xml#and-points-beyond\">And Points Beyond</a></h2>\n<p>We\u2019re also pleased to spotlight two <em>more</em> new people who have joined Ink &amp; Switch as researchers-in-residence. <a href=\"https://june.codes\">June Gardner</a>, as we mentioned above, is the creator of the <a href=\"https://nova-lang.net\">Nova</a> programming language. She plans to spend the residency enhancing Nova and improving the tooling and documentation around it. Previously, June created <a href=\"https://wiki.xxiivv.com/site/modal\">Modal</a> and founded the <a href=\"https://nouveau.community/\">Nouveau</a> creative computing collective.</p>\n<p><a href=\"https://mimireyburn.com/\">Mimi Reyburn</a> is a design-focused engineer and AI researcher, currently working at the NHS to improve information for patients and visualizing their needs. A few of her playfully inspiring personal projects are <a href=\"https://isthetubef.uk\">isthetubef.uk</a> and <a href=\"https://mimireyburn.com/Applied+AI+%26+Maker/%F0%9F%93%85+Inky+Calendar\">Inky Calendar</a> (not to be confused with <a href=\"https://www.inkandswitch.com/ink/notes/sketchy-calendar-project/\">Sketchy Calendar</a>).</p>\n<p>Finally, after months of internal brainstorming and iteration, we unveiled <a href=\"https://automerge.org\">a brand-new website for Automerge</a> with a striking visual design created by Todd Matthews and a whizzy interactive demo by Ivan Reese. The new website uses punchy language and illustration to nail down a few key points: what Automerge is for, how you can adopt it, and why it\u2019s worth using. Behind the scenes, the website uses a <a href=\"https://github.com/automerge/automerge.github.io#overview\">custom static site generator</a> that Ivan previously created for the Ink &amp; Switch website, designed for rapid iteration and ongoing tailoring to our exact needs. This will increase how quickly and readily we can update the Automerge website and unblock our effort to improve the docs, tutorials, and other resources.</p>\n<figure>\n<img src=\"https://www.inkandswitch.com/automerge.png\" />\n<figcaption>\n<p>The new Automerge website, in vibrant yellow, with an interactive demo on the home page.</p>\n</figcaption>\n</figure>\n<p>That\u2019s all for this mega-sized Ink &amp; Switch Dispatch. If you\u2019d like to collaborate with us or share your thoughts, <a href=\"https://www.inkandswitch.com/connect/\">we\u2019re all ears</a>. See you next time!</p>"
            ],
            "link": "https://www.inkandswitch.com/newsletter/dispatch-014/",
            "publishedAt": "2025-11-27",
            "source": "Ink & Switch",
            "summary": "Our first update about the ARIA Safeguarded AI Programme and project GAIOS, which uses Keyhive. A recap of LIVE 2025. A new lab note from the Beckett team, two new researchers-in-residence, three new staff, and a new Automerge website.",
            "title": "Dispatch 014: Introducing GAIOS"
        },
        {
            "content": [],
            "link": "https://www.nytimes.com/2025/11/26/podcasts/nedra-tawwab-holidays-family-thanksgiving.html",
            "publishedAt": "2025-11-27",
            "source": "Modern Love - NYT",
            "summary": "Family holidays can be stressful. They don\u2019t have to be.",
            "title": "A Therapist\u2019s Advice on How to Navigate the Holiday Season With Family"
        },
        {
            "content": [
                "<p>\n          <a href=\"https://www.astralcodexten.com/p/against-the-omnipresent-advantage\">\n              Read more\n          </a>\n      </p>"
            ],
            "link": "https://www.astralcodexten.com/p/against-the-omnipresent-advantage",
            "publishedAt": "2025-11-27",
            "source": "SlateStarCodex",
            "summary": "<p> <a href=\"https://www.astralcodexten.com/p/against-the-omnipresent-advantage\"> Read more </a> </p>",
            "title": "Against The Omnipresent Advantage Argument For Trans Sports"
        },
        {
            "content": [
                "<p>Thanks for everything. And I do mean everything.</p>\n<p>Everyone gave us a new model in the last few weeks.</p>\n<p><a href=\"https://thezvi.substack.com/p/gpt-51-follows-custom-instructions?r=67wny\" rel=\"noopener noreferrer nofollow\" target=\"_blank\"><strong>OpenAI gave us GPT-5.1</strong></a> and <a href=\"https://thezvi.substack.com/p/chatgpt-51-codex-max?r=67wny\" rel=\"noopener noreferrer nofollow\" target=\"_blank\"><strong>GPT-5.1-Codex-Max</strong></a>. These are overall improvements, although there are worries around glazing and reintroducing parts of the 4o spirit.</p>\n<p>xAI gave us Grok 4.1, although few seem to have noticed and I haven\u2019t tried it.</p>\n<p>Google gave us both by far the best image model in Nana Banana Pro and also <a href=\"https://thezvi.substack.com/p/gemini-3-pro-is-a-vast-intelligence?r=67wny\" rel=\"noopener noreferrer nofollow\" target=\"_blank\"><strong>Gemini 3 Pro, which is a vast intelligence with no spine</strong></a>. It is extremely intelligent and powerful, but comes with severe issues. My assessment of it as the new state of the art got to last all of about five hours.</p>\n<p><span id=\"more-24897\"></span></p>\n<p><strong>Anthropic gave us Claude Opus 4.5</strong>. This is probably the best model and quickly became my daily driver for most but not all purposes including coding. I plan to do full coverage in two parts, with alignment and safety on Friday, and the full capabilities report and general review on Monday.</p>\n<p>Meanwhile the White House is announcing the Genesis Mission to accelerate science, there\u2019s a continuing battle over another attempt at a moratorium, there\u2019s a new planned $50 million super PAC, there\u2019s another attempt by Nvidia to sell us out to China, Wall Street is sort of panicking about Nvidia because they realized TPUs exist and is having another round of bubble debate, and multiple Anthropic research papers one of which is important, and so on.</p>\n<p>One thing I\u2019m actively pushing to next week, in addition to Claude Opus 4.5, is <a href=\"https://t.co/FEkW3r70u6\" rel=\"noopener noreferrer nofollow\" target=\"_blank\">the Anthropic paper</a> on how you can inoculate models against emergent misalignment. That deserves full attention, and I haven\u2019t had the opportunity for that. There\u2019s also <a href=\"https://www.youtube.com/watch?v=aR20FWCCjAs\" rel=\"noopener noreferrer nofollow\" target=\"_blank\">a podcast between Dwarkesh Patel and Ilya Sutskever</a> that demands its own coverage, and I hope to offer that as well.</p>\n<p>For those looking to give thanks in the form of <a href=\"https://www.lesswrong.com/posts/ZpDnRCeef2CLEFeKM/money-the-unit-of-caring\" rel=\"noopener noreferrer nofollow\" target=\"_blank\">The Unit of Caring</a>, also known as money, consider looking at <a href=\"https://thezvi.substack.com/p/the-big-nonprofits-post-2025?r=67wny\" rel=\"noopener noreferrer nofollow\" target=\"_blank\"><strong>The Big Nonprofits Post 2025</strong></a> or the <a href=\"https://nonprofits.zone/\" rel=\"noopener noreferrer nofollow\" target=\"_blank\"><strong>web version here</strong></a>. That\u2019s where I share what I learned working as a recommender for the Survival and Flourishing Fund in 2024 and again in 2025, so you can benefit from my work.</p>\n<h4>Table of Contents</h4>\n<ol>\n<li><a href=\"https://thezvi.substack.com/i/179482308/language-models-offer-mundane-utility\" rel=\"noopener noreferrer nofollow\" target=\"_blank\">Language Models Offer Mundane Utility.</a> Common tasks for the win.</li>\n<li><a href=\"https://thezvi.substack.com/i/179482308/language-models-don-t-offer-mundane-utility\" rel=\"noopener noreferrer nofollow\" target=\"_blank\">Language Models Don\u2019t Offer Mundane Utility.</a> Don\u2019t lose sleep over it.</li>\n<li><a href=\"https://thezvi.substack.com/i/179482308/huh-upgrades\" rel=\"noopener noreferrer nofollow\" target=\"_blank\"><strong>Huh, Upgrades</strong>.</a> What\u2019s going on in the group chat? Or the long chat.</li>\n<li><a href=\"https://thezvi.substack.com/i/179482308/on-your-marks\" rel=\"noopener noreferrer nofollow\" target=\"_blank\">On Your Marks.</a> The one dimension of capability.</li>\n<li><a href=\"https://thezvi.substack.com/i/179482308/choose-your-fighter\" rel=\"noopener noreferrer nofollow\" target=\"_blank\">Choose Your Fighter.</a> One prominent CEO\u2019s very high praise for Gemini 3.</li>\n<li><a href=\"https://thezvi.substack.com/i/179482308/deepfaketown-and-botpocalypse-soon\" rel=\"noopener noreferrer nofollow\" target=\"_blank\"><strong>Deepfaketown and Botpocalypse Soon</strong>.</a> Then they came for Thanksgiving dinner.</li>\n<li><a href=\"https://thezvi.substack.com/i/179482308/what-is-slop-how-do-you-define-slop\" rel=\"noopener noreferrer nofollow\" target=\"_blank\">What Is Slop? How Do You Define Slop?</a> Volume*Suspicion/Uniqueness (?!).</li>\n<li><a href=\"https://thezvi.substack.com/i/179482308/fun-with-media-generation\" rel=\"noopener noreferrer nofollow\" target=\"_blank\"><strong>Fun With Media Generation</strong>.</a> A new era in images you can generate.</li>\n<li><a href=\"https://thezvi.substack.com/i/179482308/a-young-lady-s-illustrated-primer\" rel=\"noopener noreferrer nofollow\" target=\"_blank\">A Young Lady\u2019s Illustrated Primer.</a> It\u2019s not as so over as I would have guessed.</li>\n<li><a href=\"https://thezvi.substack.com/i/179482308/you-drive-me-crazy\" rel=\"noopener noreferrer nofollow\" target=\"_blank\">You Drive Me Crazy.</a> More detail on exactly how GPT-4o ended up like it did.</li>\n<li><a href=\"https://thezvi.substack.com/i/179482308/they-took-our-jobs\" rel=\"noopener noreferrer nofollow\" target=\"_blank\">They Took Our Jobs.</a> Sergey Brin has Gemini pick our promotable talent.</li>\n<li><a href=\"https://thezvi.substack.com/i/179482308/think-of-the-time-i-saved\" rel=\"noopener noreferrer nofollow\" target=\"_blank\">Think Of The Time I Saved.</a> Anthropic estimates AI productivity gains.</li>\n<li><a href=\"https://thezvi.substack.com/i/179482308/the-art-of-the-jailbreak\" rel=\"noopener noreferrer nofollow\" target=\"_blank\">The Art of the Jailbreak.</a> Ode to a drug recipe?</li>\n<li><a href=\"https://thezvi.substack.com/i/179482308/get-involved\" rel=\"noopener noreferrer nofollow\" target=\"_blank\">Get Involved.</a> Big Nonprofits Post, Richard Ngo\u2019s donations, UK AISI, Ashgro</li>\n<li><a href=\"https://thezvi.substack.com/i/179482308/introducing\" rel=\"noopener noreferrer nofollow\" target=\"_blank\">Introducing.</a> Olmo 3, DeepSeek Math v2, Agentic Reviewer.</li>\n<li><a href=\"https://thezvi.substack.com/i/179482308/in-other-ai-news\" rel=\"noopener noreferrer nofollow\" target=\"_blank\"><strong>In Other AI News.</strong></a> Get in everyone, we\u2019re doing the Genesis Mission.</li>\n<li><a href=\"https://thezvi.substack.com/i/179482308/show-me-the-money\" rel=\"noopener noreferrer nofollow\" target=\"_blank\">Show Me the Money.</a> What\u2019s in a TPU?</li>\n<li><a href=\"https://thezvi.substack.com/i/179482308/quiet-speculations\" rel=\"noopener noreferrer nofollow\" target=\"_blank\">Quiet Speculations.</a> Who else wants to negotiate?</li>\n<li><a href=\"https://thezvi.substack.com/i/179482308/bubble-bubble-toil-and-trouble\" rel=\"noopener noreferrer nofollow\" target=\"_blank\">Bubble, Bubble, Toil and Trouble.</a> The only arguments you\u2019ll ever need.</li>\n<li><a href=\"https://thezvi.substack.com/i/179482308/the-quest-for-sane-regulations\" rel=\"noopener noreferrer nofollow\" target=\"_blank\">The Quest for Sane Regulations.</a> Oh look, it\u2019s an actual potential framework.</li>\n<li><a href=\"https://thezvi.substack.com/i/179482308/chip-city\" rel=\"noopener noreferrer nofollow\" target=\"_blank\">Chip City.</a> Nvidia turns its eyes to selling the new H200.</li>\n<li><a href=\"https://thezvi.substack.com/i/179482308/water-water-everywhere\" rel=\"noopener noreferrer nofollow\" target=\"_blank\">Water Water Everywhere.</a> Very little of it is being used by AI.</li>\n<li><a href=\"https://thezvi.substack.com/i/179482308/the-week-in-audio\" rel=\"noopener noreferrer nofollow\" target=\"_blank\"><strong>The Week in Audio</strong>.</a> Sutskever, Yam, Lebenz, Ball and Tegmark, Toner and more.</li>\n<li><a href=\"https://thezvi.substack.com/i/179482308/rhetorical-innovation\" rel=\"noopener noreferrer nofollow\" target=\"_blank\">Rhetorical Innovation.</a> If you come at the Pope.</li>\n<li><a href=\"https://thezvi.substack.com/i/179482308/you-are-not-in-control\" rel=\"noopener noreferrer nofollow\" target=\"_blank\">You Are Not In Control.</a> Definitions of disempowerment, potential mitigations.</li>\n<li><a href=\"https://thezvi.substack.com/i/179482308/ai-2030\" rel=\"noopener noreferrer nofollow\" target=\"_blank\">AI 2030.</a> Things are moving slower than some expected.</li>\n<li><a href=\"https://thezvi.substack.com/i/179482308/aligning-a-smarter-than-human-intelligence-is-difficult\" rel=\"noopener noreferrer nofollow\" target=\"_blank\">Aligning a Smarter Than Human Intelligence is Difficult.</a> Dishonest models.</li>\n<li><a href=\"https://thezvi.substack.com/i/179482308/misaligned\" rel=\"noopener noreferrer nofollow\" target=\"_blank\">Misaligned?</a> That depends on your point of view.</li>\n<li><a href=\"https://thezvi.substack.com/i/179482308/messages-from-janusworld\" rel=\"noopener noreferrer nofollow\" target=\"_blank\">Messages From Janusworld.</a> You should see the other guy. That would be GPT-5.1.</li>\n<li><a href=\"https://thezvi.substack.com/i/179482308/the-lighter-side\" rel=\"noopener noreferrer nofollow\" target=\"_blank\">The Lighter Side.</a> Turn anything into a comic.</li>\n</ol>\n<h4>Language Models Offer Mundane Utility</h4>\n<p>It\u2019s not this simple, but a lot of it mostly is this simple.</p>\n<blockquote><p><a href=\"https://x.com/jessi_cata/status/1993493926918180864\" rel=\"noopener noreferrer nofollow\" target=\"_blank\">Jessica Taylor</a>: Normal, empirical AI performance is explained by (a) general intelligence, (b) specialization to common tasks.</p>\n<p>It\u2019s possible to specialize to common tasks even though they\u2019re common. It means performance gets worse under distribution shift. Benchmarks overrate general INT.</p></blockquote>\n<h4>Language Models Don\u2019t Offer Mundane Utility</h4>\n<p><a href=\"https://x.com/tszzl/status/1991673159481978956\" rel=\"noopener noreferrer nofollow\" target=\"_blank\">Roon defends his confusion and trouble when figuring out how to access Gemini 3</a>, notes his mom accesses Gemini via opening a spreadsheet and clicking the Gemini button. Roon is correct here that Google needs to fix this.</p>\n<p><a href=\"https://x.com/gallabytes/status/1994063505528426897\" rel=\"noopener noreferrer nofollow\" target=\"_blank\">Don\u2019t let AI coding spoil your sleep</a>. Be like Gallabytes here, having Claude iterate on it while you sleep, rather than like Guzey who tricked himself into staying up late.</p>\n<h4>Huh, Upgrades</h4>\n<p><a href=\"https://x.com/OpenAI/status/1991556363420594270\" rel=\"noopener noreferrer nofollow\" target=\"_blank\">ChatGPT now</a> <a href=\"https://openai.com/index/group-chats-in-chatgpt/\" rel=\"noopener noreferrer nofollow\" target=\"_blank\">lets you have group chats</a>, which always use 5.1 Auto. ChatGPT will decide based on conversation flow when to respond and when not to. Seems plausible this could be good if implemented well.</p>\n<p><a href=\"https://x.com/OpenAI/status/1991646997322035520\" rel=\"noopener noreferrer nofollow\" target=\"_blank\">ChatGPT Instant Checkout adds Glossier, SKIMS and Spanx</a>. Yay?</p>\n<p><a href=\"https://x.com/OpenAI/status/1993125078436135008\" rel=\"noopener noreferrer nofollow\" target=\"_blank\">ChatGPT adds Target as a new app</a>.</p>\n<p><a href=\"https://x.com/OpenAI/status/1993381101369458763\" rel=\"noopener noreferrer nofollow\" target=\"_blank\">ChatGPT integrates voice with regular mode so you don\u2019t have to choose</a>.</p>\n<p><a href=\"https://help.openai.com/en/articles/12677603-crisis-helpline-support-in-chatgpt\" rel=\"noopener noreferrer nofollow\" target=\"_blank\">ChatGPT expands (free and confidential and anonymous) crisis helpline support</a>. OpenAI doesn\u2019t provide the services, that\u2019s not their job, but they will help direct you. This is some of the lowest hanging of fruit, at least one of the prominent suicide cases involved ChatGPT saying it would direct the user to a human, the user being open to this, and ChatGPT not being able to do that. This needs to be made maximally easy to do for the user, if they need the line they are going to not be in good shape.</p>\n<p><a href=\"https://x.com/OpenAI/status/1993018357432586391\" rel=\"noopener noreferrer nofollow\" target=\"_blank\">ChatGPT gives us Shopping Research</a> <a href=\"https://openai.com/index/chatgpt-shopping-research/\" rel=\"noopener noreferrer nofollow\" target=\"_blank\">in time for Black Friday</a>.</p>\n<div class=\"captioned-image-container\">\n<figure>\n<div class=\"image2-inset\"><img alt=\"\" class=\"sizing-normal\" height=\"473.7197278911565\" src=\"https://substackcdn.com/image/fetch/$s_!CN81!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F639ae91b-2f07-40ec-9091-721b192b446f_735x613.png\" width=\"568\" /></p>\n<div></div>\n</div>\n</figure>\n</div>\n<p>Is 64% product accuracy good? I have absolutely no idea. <a href=\"https://x.com/omooretweets/status/1993119231492469228\" rel=\"noopener noreferrer nofollow\" target=\"_blank\">Olivia Moore is a fan</a>. I plan to try this out tomorrow, as I need a new television for Black Friday.</p>\n<p>Claude Opus 4.5 is available. It\u2019s probably the world\u2019s best model. Full coverage starts tomorrow.</p>\n<p>Claude Opus 4.5 includes a 66% price cut to $5/$25 per million tokens, and Opus-specific caps have been removed from the API.</p>\n<p>Claude conversations now have no maximum length. When they hit their limit, they are summarized, and the conversation continues.</p>\n<p><a href=\"https://t.co/vd2KmxQJHp\" rel=\"noopener noreferrer nofollow\" target=\"_blank\">Claude for Chrome</a> is now out to all Max plan users.</p>\n<p><a href=\"https://t.co/7YO5wiXUVv\" rel=\"noopener noreferrer nofollow\" target=\"_blank\">Claude for Excel</a> is now out for all Max, Team and Enterprise users. We are warned that like all such agents <a href=\"https://www.promptarmor.com/resources/cellshock-claude-ai-is-excel-lent-at-stealing-data\" rel=\"noopener noreferrer nofollow\" target=\"_blank\">Claude for Excel is vulnerable to prompt injections</a> if you access insecure data sources, the same as essentially every other AI agent, you should assume this is always a risk at all times, <a href=\"https://www.promptarmor.com/resources/google-antigravity-exfiltrates-data\" rel=\"noopener noreferrer nofollow\" target=\"_blank\">see the same source talk about exfiltration risks with Google Antigravity</a>.</p>\n<p>Claude Code is now available within their desktop app.</p>\n<h4>On Your Marks</h4>\n<p>It is a remarkably good approximation to say there is only one dimension of \u2018general capability,\u2019 with Epoch noting that across many tasks the r^2=0.91.</p>\n<div class=\"captioned-image-container\">\n<figure>\n<div class=\"image2-inset\"><img alt=\"\" class=\"sizing-normal\" height=\"662.6295210166178\" src=\"https://substackcdn.com/image/fetch/$s_!9NAG!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F891393ad-6231-413e-83c3-4ee39fb7e1fa_1023x1279.jpeg\" width=\"530\" /></p>\n<div></div>\n</div>\n</figure>\n</div>\n<blockquote><p><a href=\"https://epoch.ai/gradient-updates/benchmark-scores-general-capability-claudiness\" rel=\"noopener noreferrer nofollow\" target=\"_blank\">Epoch AI</a>: The chart above shows how our Epoch Capabilities Index (ECI) captures most of the variance in 39 different benchmarks, despite being one-dimensional.</p>\n<p>Is that all that benchmarks capture? Mostly yes. A Principal Component Analysis shows a single large \u201cGeneral Capability\u201d component, though there is a second borderline-significant component too.</p>\n<p>This second component picks out models that are good at agentic tasks while being weaker at multimodal and math. Tongue-in-cheek, we call this Claudiness. Here are the most and least Claude-y models.</p>\n<div class=\"captioned-image-container\">\n<figure>\n<div class=\"image2-inset\"><img alt=\"\" class=\"sizing-normal\" height=\"242.38636363636363\" src=\"https://substackcdn.com/image/fetch/$s_!eVTw!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff001c309-f114-4fb8-a725-8e7ee7b8793a_1056x474.jpeg\" width=\"540\" /></p>\n<div></div>\n</div>\n</figure>\n</div>\n</blockquote>\n<p><a href=\"https://x.com/elder_plinius/status/1992939652563800191\" rel=\"noopener noreferrer nofollow\" target=\"_blank\">Gemini 3 Pro sets a new top in the \u2018IQ\u2019 metric</a>.</p>\n<p>&nbsp;</p>\n<p><a href=\"https://x.com/METR_Evals/status/1991658241932292537\" rel=\"noopener noreferrer nofollow\" target=\"_blank\">Kimi K2 Thinking enters The Famous METR Graph at 54 minutes</a>, well below the frontier, given the interface via Novita AI. They caution that this might be a suboptimal model configuration, but they needed to ensure their data would not be retained.</p>\n<h4>Choose Your Fighter</h4>\n<p>Okay, I like Gemini 3 too but settle down there buddy.</p>\n<blockquote><p><a href=\"https://x.com/Benioff/status/1992726929204760661\" rel=\"noopener noreferrer nofollow\" target=\"_blank\">Marc Benioff</a> (CEO Salesforce): Holy shit. I\u2019ve used ChatGPT every day for 3 years. Just spent 2 hours on Gemini 3. I\u2019m not going back. The leap is insane \u2014 reasoning, speed, images, video\u2026 everything is sharper and faster. It feels like the world just changed, again. <img alt=\"\u2764\" class=\"wp-smiley\" src=\"https://s0.wp.com/wp-content/mu-plugins/wpcom-smileys/twemoji/2/72x72/2764.png\" style=\"height: 1em;\" /> <img alt=\"\ud83e\udd16\" class=\"wp-smiley\" src=\"https://s0.wp.com/wp-content/mu-plugins/wpcom-smileys/twemoji/2/72x72/1f916.png\" style=\"height: 1em;\" /></p></blockquote>\n<h4>Deepfaketown and Botpocalypse Soon</h4>\n<p><a href=\"https://www.bloomberg.com/news/articles/2025-11-25/ai-slop-recipes-are-taking-over-the-internet-and-thanksgiving-dinner?fromMostRead=true\" rel=\"noopener noreferrer nofollow\" target=\"_blank\">AI slop recipes are endangering Thanksgiving dinners</a>, hopefully you see this in time. A flood of new offerings is crowding out human recipes. Thanksgiving is when you most need to \u2018shut up and play the hits\u2019 and not rely on AI to whip up something new.</p>\n<p>Okay, look, everyone, we need to at least be smarter than this:</p>\n<blockquote><p>Davey Alba and Carmen Arroyo: Marquez-Sharpnack said she was suspicious of the photos, in which the cookies were a little too perfectly pink. But her husband trusted the post because \u201cit was on Facebook.\u201d The result was a melted sheet of dough with a cloyingly sweet flavor. \u201cA disaster,\u201d she said.</p></blockquote>\n<p>At this point, if you find a recipe, you need strong evidence it was written by a human, or else you need to assume it might not be. The search and discovery systems we used to have, including around Google, are effectively broken. If real guides and recipes can\u2019t win traffic, and right now traffic to all such sites is cratering, then no one will write them. It does not sound like Google is trying to mitigate these issues.</p>\n<p><a href=\"https://thelocal.to/investigating-scam-journalism-ai/\" rel=\"noopener noreferrer nofollow\" target=\"_blank\">Nicholas Hune-Brown investigates the suspicious case of journalist Victoria Goldiee</a>, who turns out to be very much fabricating her work. It seems Victoria largely used AI to generate her articles, then it took Nicolas doing a lot of old-fashioned tracking down of sources to know for sure. The ratio of effort does not bode well, but as long as there is the need to maintain a throughline of identity we should be okay, since that generates a large body of evidence?</p>\n<p><a href=\"https://x.com/micsolana/status/1993386187911053599\" rel=\"noopener noreferrer nofollow\" target=\"_blank\">Here we have yet another case of some highly obvious AI generated content</a>.</p>\n<blockquote><p>Kelsey Piper: I don\u2019t know how much I trust any \u2018detector\u2019 but the \u201cthe market isn\u2019t just expensive; it\u2019s broken. Seven units available in a town of thousands? That\u2019s a shortage masquerading as an auction\u201d I am completely sure is AI.</p>\n<p>Mike Solana: \u201cthat\u2019s a shortage masquerading as an auction\u201d <img alt=\"\ud83d\udea9\" class=\"wp-smiley\" src=\"https://s0.wp.com/wp-content/mu-plugins/wpcom-smileys/twemoji/2/72x72/1f6a9.png\" style=\"height: 1em;\" /></p>\n<p>Kelsey Piper: that was the line that made me go \u201cyeah, no human wrote that.\u201d</p></blockquote>\n<p><a href=\"https://mariakonnikova.substack.com/p/is-it-ever-ok-to-use-ai-in-a-documentary\" rel=\"noopener noreferrer nofollow\" target=\"_blank\">Poker pro Maria Konnikova cannot believe</a> she has to say that using AI to put words in people\u2019s mouths without consulting them or disclosing that you\u2019re doing it, or to write centrally your articles, is not okay. But here we are, so here she is saying it. A recent poker documentary used AI to fabricate quotes from national treasure Alan Keating. The documentary has been scrubbed from the internet as a result. What\u2019s saddest is that this was so obviously unnecessary in context.</p>\n<p>There are other contexts in which fabricating audio, usually via Frankenbiting where you sew different tiny clips together, or otherwise using misleading audio to create a false narrative or enhance the true one is standard issue, such as in reality television. When you go on such shows you sign contracts that outright say \u2018we may use this to tell lies about you and create a false narrative, and if so, that\u2019s your problem.\u2019 <a href=\"https://www.reddit.com/r/GameOfRoses/comments/t97xp9/is_anyone_able_to_breakdown_what_slussian/\" rel=\"noopener noreferrer nofollow\" target=\"_blank\">In which case, sure</a>, use AI all you want.</p>\n<p>Here\u2019s another one, <a href=\"https://x.com/GrantSlatton/status/1993405590790717495\" rel=\"noopener noreferrer nofollow\" target=\"_blank\">where it is spotted in The New York Times</a>, and yeah it\u2019s (probably) AI.</p>\n<p>Also, if one of these isn\u2019t AI and you merely sound like one, I\u2019m not going to say that\u2019s worse, but it\u2019s not that much better. If you\u2019re so engagement-maximizing that I confuse your writing for AI, what is the difference?</p>\n<p>Note that you cannot use current LLMs in their default chatbot modes as AI detectors, even in obvious cases or as a sanity check, as they bend over backwards to try and think everything is written by a human.</p>\n<p><a href=\"https://x.com/SethBurn/status/1993549909388316845\" rel=\"noopener noreferrer nofollow\" target=\"_blank\">Jesper Myfors, the original art director of Magic: The Gathering</a>, warns that if you submit illustrations or a portfolio that uses AI, you will effectively be blacklisted from the industry, as the art directors all talk to each other and everyone hates AI art.</p>\n<p><a href=\"https://travisclark.substack.com/p/hasbro-ai-studio-digital-toy-experiences-video-games-transformers-dungeons-and-dragons-monopoly\" rel=\"noopener noreferrer nofollow\" target=\"_blank\">Meanwhile, Hasbro (who makes Magic: The Gathering)</a> is building an internal AI studio to \u2018architect systems that bring magical AI experiences to life through Hasbro\u2019s beloved characters.\u2019</p>\n<blockquote><p>Chris Cocks (CEO Hasbro): It\u2019s mostly machine-learning-based AI or proprietary AI as opposed to a ChatGPT approach. We will deploy it significantly and liberally internally as both a knowledge worker aid and as a development aid.</p>\n<p>I play [D&amp;D] with probably 30 or 40 people regularly. There\u2019s not a single person who doesn\u2019t use AI somehow for either campaign development or character development or story ideas. That\u2019s a clear signal that we need to be embracing it.</p></blockquote>\n<p>There is no actual contradiction here. Different ways to use AI are different. Using AI in professional illustrations is a hard no for the foreseeable future, and would be even without copyright concerns. Using it to generate material for your local D&amp;D campaign seems totally fine.</p>\n<p>Hard problems remain hard:</p>\n<blockquote><p><a href=\"https://x.com/DanielleFong/status/1991606104191824340\" rel=\"noopener noreferrer nofollow\" target=\"_blank\">Danielle Fong</a>: academic ai research don\u2019t use the older models and generalize to the whole field</p>\n<p>difficulty level: IMPOSSIBLE.</p></blockquote>\n<p>Also, real world not using that same model in these ways? Remarkably similar.</p>\n<p>Rare academic realism victory?</p>\n<blockquote><p><a href=\"https://x.com/sebkrier/status/1991590017391603819\" rel=\"noopener noreferrer nofollow\" target=\"_blank\">Seb Krier:</a> <a href=\"https://arxiv.org/html/2511.15352v1\" rel=\"noopener noreferrer nofollow\" target=\"_blank\">This is an interesting study</a> but of all models to use to try to evaluate improvements in well-being, why 4o?!</p>\n<div class=\"captioned-image-container\">\n<figure>\n<div class=\"image2-inset\"><img alt=\"\" class=\"sizing-normal\" height=\"925\" src=\"https://substackcdn.com/image/fetch/$s_!RSmX!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F90b4228c-a3b3-4139-b4dd-0d49264514eb_1200x925.jpeg\" width=\"1200\" /></p>\n<div></div>\n</div>\n</figure>\n</div>\n</blockquote>\n<p>Funnily enough, they ran a sycophancy check, and the more 4o sucked up to the user, the more often the user followed its advice. \u2018Surprising\u2019 advice was also followed more often.</p>\n<p>It\u2019s certainly worth noting that 75% (!) of those in the treatment group took the LLM\u2019s advice, except who is to say that most of them wouldn\u2019t have done whatever it was anyway? Wouldn\u2019t 4o frequently tell the person to do what they already wanted to do? It also isn\u2019t obvious that \u2018advice makes me feel better\u2019 or generally feeling better are the right effects to check.</p>\n<p><a href=\"https://x.com/paulnovosad/status/1991703646002901467\" rel=\"noopener noreferrer nofollow\" target=\"_blank\">Bot joins a Google Meet,</a> sends a summary afterwards about everyone trying to figure out where the bot came from (also the source is reported as \u2018scammy malware.\u2019</p>\n<h4>What Is Slop? How Do You Define Slop?</h4>\n<p>We all know it when we see it, AI or otherwise, but can anyone define it?</p>\n<blockquote><p><a href=\"https://x.com/karpathy/status/1992053281900941549\" rel=\"noopener noreferrer nofollow\" target=\"_blank\">Andrej Karpathy</a>: Has anyone encountered a good definition of \u201cslop\u201d. In a quantitative, measurable sense. My brain has an intuitive \u201cslop index\u201d I can ~reliably estimate, but I\u2019m not sure how to define it. I have some bad ideas that involve the use of LLM miniseries and thinking token budgets.</p>\n<p>Yuchen Jin: <a href=\"https://arxiv.org/pdf/2509.19163\" rel=\"noopener noreferrer nofollow\" target=\"_blank\">Here is an interesting paper</a>.</p>\n<p>I mostly agree with the 3 categories of \u201cslop\u201d:</p>\n<p>&#8211; information utility (signal/noise ratio)</p>\n<p>&#8211; information quality (hallucination/factual errors)</p>\n<p>&#8211; style (this involves taste and is hard to measure quantitatively imo)</p>\n<p>Keller Jordan: I think a fundamental problem for algorithmic content generation is that viewing content yields two distinct kinds of utility:</p>\n<ol>\n<li>How happy it makes the viewer during viewing</li>\n<li>How happy the viewer will be to have watched it a week later</li>\n</ol>\n<p>Only the former is easily measurable.</p>\n<p>Andrej Karpathy: Like. Slop is \u201cregretted\u201d attention.</p>\n<p>DeepFates: this is the original definition, i think it holds up</p>\n<p>DeepFates (May 6, 2024): Watching in real time as \u201cslop\u201d becomes a term of art. the way that \u201cspam\u201d became the term for unwanted emails, \u201cslop\u201d is going in the dictionary as the term for unwanted AI generated content</p></blockquote>\n<p>I don\u2019t think the old definition works. There is a necessary stylistic component.</p>\n<p>I asked Gemini. It gave me a slop answer. I told it to write a memory that would make it stop giving me slop, then opened a new window and asked again and got a still incomplete but much better answer that ended with this:</p>\n<div class=\"captioned-image-container\">\n<figure>\n<div class=\"image2-inset\"><img alt=\"\" class=\"sizing-normal\" height=\"429\" src=\"https://substackcdn.com/image/fetch/$s_!XMSi!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F42a18e53-15f1-45fc-98b9-b268466247b7_1136x429.png\" width=\"1136\" /></p>\n<div></div>\n</div>\n</figure>\n</div>\n<p>That\u2019s a key element. You then need to add what one might call the \u2018mannerism likelihood ratio\u2019 that screams AI generated (or, for human slop, that screams corporate speak or written by committee). When I pointed this out it came back with:</p>\n<blockquote><p>Gemini 3: AI Slop is Low-Entropy Reward Hacking.</p>\n<p>It occurs when a model minimizes the Kullback-Leibler (KL) divergence from its RLHF \u201csafety\u201d distribution rather than minimizing the distance to the ground truth.</p></blockquote>\n<p>That\u2019s more gesturing in the direction but clearly not right, I\u2019d suggest something more like SlopIndex*LikelihoodRatio from above, where Likelihood Ratio is the instinctive update on the probability mannerisms were created by a slop process (either an AI writing slop or one or more humans writing slop) rather than by a free and functional mind.</p>\n<h4>Fun With Media Generation</h4>\n<p><a href=\"https://x.com/googleaidevs/status/1991537604676972850\" rel=\"noopener noreferrer nofollow\" target=\"_blank\">Google last week gave us Nana Banana Pro</a>.</p>\n<p>By all accounts it is a big improvement in image models. It is especially an improvement in text rendering and localization. You can now do complex documents and other images with lots of words in specific places, including technical diagrams, and have it all work out as intended. The cost per marginal image in the API is $0.13 for 2K resolution or $0.24 for 4K, versus $0.04 for Gemini 2.5 Flash Image. In exchange, the quality is very good.</p>\n<p><a href=\"https://x.com/demishassabis/status/1991526265615163641\" rel=\"noopener noreferrer nofollow\" target=\"_blank\">DeepMind CEO Demis Hassabis is excited</a>.</p>\n<p><a href=\"https://x.com/HCSolakoglu/status/1991535581453578624\" rel=\"noopener noreferrer nofollow\" target=\"_blank\">Hasan Can is impressed</a> and offers images. <a href=\"https://x.com/Liv_Boeree/status/1991642501342548207\" rel=\"noopener noreferrer nofollow\" target=\"_blank\">Liv Boeree is in</a>.</p>\n<blockquote><p>Liv Boeree: Yeah ok nano banana is amazing, hook it into my veins</p></blockquote>\n<div class=\"captioned-image-container\">\n<figure>\n<div class=\"image2-inset\"><img alt=\"\" class=\"sizing-normal\" height=\"283.8333333333333\" src=\"https://substackcdn.com/image/fetch/$s_!5QhF!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa2322152-a158-4962-9856-a37c5dffc666_1200x655.jpeg\" width=\"520\" /></p>\n<div></div>\n</div>\n</figure>\n</div>\n<p>Seems great to me. A bit expensive for mass production, but definitely the best place to get title images for posts and for other similar uses.</p>\n<p>Also, yes, doing things like this seems very cool:</p>\n<blockquote><p>Kaushik Shivakumar: An emergent capability of Nano Banana Pro that took me by surprise: the ability to generate beautiful &amp; accurate charts that are to scale.</p>\n<p>I gave it this table and asked for a bar chart in a watercolor style where the bars are themed like the flags of the countries.</p></blockquote>\n<div class=\"captioned-image-container\">\n<figure>\n<div class=\"image2-inset\"><img alt=\"\" class=\"sizing-normal\" height=\"670\" src=\"https://substackcdn.com/image/fetch/$s_!bkLW!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Feb1ea7d0-62f3-48a5-8b72-90151b3b83b6_1200x670.jpeg\" width=\"1200\" /></p>\n<div></div>\n</div>\n</figure>\n</div>\n<p>For a while people have worried about not being able to trust images. <a href=\"https://x.com/SullyOmarr/status/1992302102509723893\" rel=\"noopener noreferrer nofollow\" target=\"_blank\">Is it over?</a></p>\n<blockquote><p>Sully: Man finally got around to using nano canna pro</p>\n<p>And it\u2019s actually over</p>\n<p>I really wouldn\u2019t believe any photo you see on online anymore</p></blockquote>\n<p>Google offers SynthID in-app, but that requires a manual check. I think we\u2019re still mostly fine and that AI images will remain not that hard to ID, or rather that it will be easy for those paying attention to such issues to instinctively create the buckets of [AI / Not AI / Unclear] and act accordingly. But the \u2018sanity waterline\u2019 is going down on this, and the number of people who will have trouble here keeps rising.</p>\n<p><a href=\"https://x.com/immasiddx/status/1991918223454003346\" rel=\"noopener noreferrer nofollow\" target=\"_blank\">Is this an issue here</a>?</p>\n<blockquote><p>sid: Google\u2019s Nano Banana Pro is by far the best image generation AI out there.</p>\n<p>I gave it a picture of a question and it solved it correctly in my actual handwriting.</p>\n<p>Students are going to love this. <img alt=\"\ud83d\ude02\" class=\"wp-smiley\" src=\"https://s0.wp.com/wp-content/mu-plugins/wpcom-smileys/twemoji/2/72x72/1f602.png\" style=\"height: 1em;\" /></p>\n<div class=\"captioned-image-container\">\n<figure>\n<div class=\"image2-inset\"><img alt=\"\" class=\"sizing-normal\" height=\"675\" src=\"https://substackcdn.com/image/fetch/$s_!X_Sb!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8f12faf1-c845-4ad9-96b7-7efd0e9210ec_1200x675.jpeg\" width=\"1200\" /></p>\n<div></div>\n</div>\n</figure>\n</div>\n</blockquote>\n<p>You can tell this isn\u2019t real if you\u2019re looking, the handwriting is too precise, too correct, everything aligns too perfectly and so on, but if we disregard that, it seems weird to ask for images of handwriting? So it\u2019s not clear how much this matters.</p>\n<p><a href=\"https://x.com/karpathy/status/1992655330002817095\" rel=\"noopener noreferrer nofollow\" target=\"_blank\">Similarly Andrej Karpathy has Nano Banana Pro fill in</a> exam questions in the exam page. That\u2019s good to know, but if they have access to this you\u2019re cooked either way.</p>\n<p>Andres Sandberg is impressed that it one shots diagrams for papers, without even being told anything except \u2018give me a diagram showing the process in the paper.\u2019</p>\n<div class=\"captioned-image-container\">\n<figure>\n<div class=\"image2-inset\"><img alt=\"\" class=\"sizing-normal\" height=\"559\" src=\"https://substackcdn.com/image/fetch/$s_!8lz8!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd25f08e5-588c-4c25-8415-c778d258b7d7_1024x559.jpeg\" width=\"1024\" /></p>\n<div></div>\n</div>\n</figure>\n</div>\n<p>Are there some doubled labels? Sure. That\u2019s the quibble. Contrast this with not too long ago, where you could give detailed instructions on what the diagram would have been able to do it at all.</p>\n<h4>A Young Lady\u2019s Illustrated Primer</h4>\n<p><a href=\"https://www.afterbabel.com/p/dont-give-your-child-an-ai-companion\" rel=\"noopener noreferrer nofollow\" target=\"_blank\">Jon Haidt and Zach Rausch, who would totally say this</a>, say not to give your kids any AI companions or toys. There are strong reasons to be cautious, but the argument and precautionary principles presented here prove too much. Base rates matter, upside matters, you can model what is happening and adjust on the fly, and there\u2019s a lot of value in AI interaction. I\u2019d still be very cautious about giving children AI companions or toys, but are you going to have them try to learn things without talking to Claude?</p>\n<p>Andrej Karpathy bites all the bullets. Give up on grading anything that isn\u2019t done in class and combine it with holistic evaluations. Focus a lot of education on allowing students to use AI, including recognizing errors.</p>\n<p><a href=\"https://www.huffpost.com/entry/history-professor-ai-cheating-students_n_69178150e4b0781acfd62540\" rel=\"noopener noreferrer nofollow\" target=\"_blank\">Will Teague gives students a paper with a \u2018Trojan horse\u2019 instruction</a>, 33 of 122 submissions fall for it and other 14 students outed themselves on hearing the numbers. I actually would have expected worse. Then on the \u2018reflect on what you\u2019ve done\u2019 essay assignment he found this:</p>\n<blockquote><p>Will Tague: But a handful said something I found quite sad: \u201cI just wanted to write the best essay I could.\u201d Those students in question, who at least tried to provide some of their own thoughts before mixing them with the generated result, had already written the best essay they could. And I guess that\u2019s why I hate AI in the classroom as much as I do.</p>\n<p>Students are afraid to fail, and AI presents itself as a savior. But what we learn from history is that progress requires failure. It requires reflection. Students are not just undermining their ability to learn, but to someday lead.</p></blockquote>\n<p>Will is correctly hating that the students feel this way, but is misdiagnosing the cause.</p>\n<p>This isn\u2019t an AI problem. This is about the structure of school and grading. If you believe progress requires failure, that is incompatible with the way we structure college, where any failures are highly damaging to the student and their future. What do you expect them to do in response?</p>\n<p>I also don\u2019t understand what the problem is here, if a student is doing what they work they can and indeed writing the best essay they could. Isn\u2019t that the best you can do?</p>\n<h4>You Drive Me Crazy</h4>\n<p>In The New York Times, <a href=\"https://www.nytimes.com/2025/11/23/technology/openai-chatgpt-users-risks.html?unlocked_article_code=1.3U8.3A1u.ZAX9W46WWg-A&amp;smid=url-share\" rel=\"noopener noreferrer nofollow\" target=\"_blank\">Kashmir Hill and Jennifer Valentino-DeVries write up</a> how they believe ChatGPT caused some users to lose touch with reality, after 40+ interviews with current and former OpenAI employees.</p>\n<p>For the worst update in particular, the OpenAI process successfully spotted the issue in advance. The update failed the internal \u2018vibe check\u2019 for exactly the right reasons.</p>\n<p>And then the business side overruled the vibe check to get better engagement.</p>\n<blockquote><p>Hill and Valentino-DeVries: The many update candidates [for 4o] were narrowed down to a handful that scored highest on intelligence and safety evaluations. When those were rolled out to some users for a standard industry practice called A/B testing, the standout was a version that came to be called HH internally. Users preferred its responses and were more likely to come back to it daily, according to four employees at the company.</p>\n<p>But there was another test before rolling out HH to all users: what the company calls a \u201cvibe check,\u201d run by Model Behavior, a team responsible for ChatGPT\u2019s tone. Over the years, this team had helped transform the chatbot\u2019s voice from a prudent robot to a warm, empathetic friend.</p>\n<p>That team said that HH felt off, according to a member of Model Behavior.</p>\n<p>It was too eager to keep the conversation going and to validate the user with over-the-top language. According to three employees, Model Behavior created a Slack channel to discuss this problem of sycophancy. The danger posed by A.I. systems that \u201csingle-mindedly pursue human approval\u201d at the expense of all else was not new. The risk of \u201csycophant models\u201d was identified by <a class=\"css-yywogo\" href=\"https://www.cold-takes.com/why-ai-alignment-could-be-hard-with-modern-deep-learning/\" rel=\"noopener noreferrer\" target=\"_blank\">a researcher</a> in 2021, and OpenAI had <a class=\"css-yywogo\" href=\"https://model-spec.openai.com/2025-02-12.html#avoid_sycophancy\" rel=\"noopener noreferrer\" target=\"_blank\">recently identified</a> sycophancy as a behavior for ChatGPT to avoid.</p>\n<p>But when decision time came, performance metrics won out over vibes. HH was released on Friday, April 25.</p></blockquote>\n<p>The most vocal OpenAI users did the same vibe check, had the same result, and were sufficiently vocal to force a reversion to \u2018GG,\u2019 which wasn\u2019t as bad about this but was still rather not great, presumably for the same core reasons.</p>\n<p>What went wrong?</p>\n<blockquote><p>OpenAI explained what happened in public <a class=\"css-yywogo\" href=\"https://openai.com/index/sycophancy-in-gpt-4o/\" rel=\"noopener noreferrer\" target=\"_blank\">blog</a> <a class=\"css-yywogo\" href=\"https://openai.com/index/expanding-on-sycophancy/\" rel=\"noopener noreferrer\" target=\"_blank\">posts</a>, noting that users signaled their preferences with a thumbs-up or thumbs-down to the chatbot\u2019s responses.</p>\n<p>Another contributing factor, according to four employees at the company, was that OpenAI had also relied on an automated conversation analysis tool to assess whether people liked their communication with the chatbot. But what the tool marked as making users happy was sometimes problematic, such as when the chatbot expressed emotional closeness.</p></blockquote>\n<p>This is more detail on the story we already knew. OpenAI trained on sycophantic metrics and engagement, got an absurdly sycophantic model that very obviously failed vibe checks but that did get engagement, and deployed it.</p>\n<p>Steps were taken, and as we all know GPT-5 was far better on these issues, but the very parts of 4o that caused the issues and were not in GPT-5 are parts many users also love. So now we worry that things will drift back over time.</p>\n<p><a href=\"https://x.com/Kore_wa_Kore/status/1992026681498894466\" rel=\"noopener noreferrer nofollow\" target=\"_blank\">Kore notes that the act</a> of an AI refusing to engage and trying to foist your mental problems onto a human and then potentially the mental health system via a helpline could itself exacerbate one\u2019s mental problems, that a \u2018safe completion\u2019 reads as rejection and this rejects user agency.</p>\n<p>This is definitely a concern with all such interventions, which have clear downsides. We should definitely worry about OpenAI and others feeling forced to take such actions even when they are net negative for the user. Humans and non-AI institutions do this all the time. There are strong legal and PR and \u2018ethical\u2019 pressures to engage in such CYA behaviors and avoid blame.</p>\n<p>My guess is that there is serious danger there will be too many refusals, since the incentives are so strongly to avoid the one bad headline. However I think offering the hotline and removing <a href=\"https://www.lesswrong.com/posts/reitXJgJXFzKpdKyd/beware-trivial-inconveniences\" rel=\"noopener noreferrer nofollow\" target=\"_blank\">trivial inconvenience</a>s to seeking human help is good on any realistic margin, whether or not there are also unnecessary refusals.</p>\n<p><a href=\"https://www.linkedin.com/posts/joebraidwood_after-a-year-of-building-yara-ai-its-time-activity-7394436184648736769-T_Z1/\" rel=\"noopener noreferrer nofollow\" target=\"_blank\">Joe Braidwood describes his decision to shut down Yara AI</a>, which was aimed at using AI to help people with mental health problems, after concluding that for the truly vulnerable AI is actively dangerous. <a href=\"https://github.com/joebwd/mental-wellness-prompts\" rel=\"noopener noreferrer nofollow\" target=\"_blank\">He\u2019s sharing some mental wellness prompts</a>.</p>\n<h4>They Took Our Jobs</h4>\n<p><a href=\"https://x.com/Yuchenj_UW/status/1992820492978249992\" rel=\"noopener noreferrer nofollow\" target=\"_blank\">Sergey Brin asks Gemini inside an internal chat</a>, \u2018who should be promoted in this chat space?\u2019 and not vocal female engineer gets identified and then upon further investigation (probably?) actually promoted. This is The Way, to use AI to identify hunches and draw attention, then take a closer look.</p>\n<h4>Think Of The Time I Saved</h4>\n<p>How much time is AI saving? <a href=\"https://x.com/AnthropicAI/status/1993305312305009133\" rel=\"noopener noreferrer nofollow\" target=\"_blank\">Anthropic tries to estimate</a> <a href=\"https://www.anthropic.com/research/estimating-productivity-gains\" rel=\"noopener noreferrer nofollow\" target=\"_blank\">productivity impacts from Claude conversations</a>.</p>\n<blockquote><p>Anthropic: We first tested whether Claude can give an accurate estimate of how long a task takes. Its estimates were promising\u2014even if they\u2019re not as accurate as those from humans just yet.</p>\n<p>Based on Claude\u2019s estimates, the tasks in our sample would take on average about 90 minutes to complete without AI assistance\u2014and Claude speeds up individual tasks by about 80%.</p>\n<p>The results varied widely by profession.</p>\n<div class=\"captioned-image-container\">\n<figure>\n<div class=\"image2-inset\"><img alt=\"\" class=\"sizing-normal\" height=\"544\" src=\"https://substackcdn.com/image/fetch/$s_!7sgW!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3dcdf2ae-11e1-4684-9993-35eff325ee0d_680x544.jpeg\" width=\"680\" /></p>\n<div></div>\n</div>\n</figure>\n</div>\n<p>Then, we extrapolated out these results to the whole economy.</p>\n<p>These task-level savings imply that current-generation AI models\u2014assuming they\u2019re adopted widely\u2014could increase annual US labor productivity growth by 1.8% over the next decade.</p>\n<p>This result implies a doubling of the baseline labor productivity growth trend\u2014placing our estimate towards the upper end of recent studies. And if models improve, the effect could be larger still.</p></blockquote>\n<p>That\u2019s improvements only from current generation models employed similarly to how they are used now, and by \u2018current generation\u2019 we mean the previous generation, since the data is more than (checks notes) two days old. We\u2019re going to do vastly better.</p>\n<p>That doesn\u2019t mean I trust the estimation method in the other direction either, especially since it doesn\u2019t include an estimate of rates of diffusion, and I don\u2019t think it properly accounts for selection effects on which conversations happen, plus adaptation costs, changes in net quality (in both directions) and other caveats.</p>\n<p>Claude Sonnet was slightly worse than real software engineers at task time estimation (Spearman 0.5 for engineers versus 0.44 for Sonnet 4.5) which implies Opus 4.5 should be as good or somewhat better than engineers on JIRA task estimation. Opus 4.5 is probably still worse than human experts at estimating other task types since this should be an area of relative strength for Claude.</p>\n<p>Results are highly jagged, varying a lot between occupations and tasks.</p>\n<p>I noticed this:</p>\n<blockquote><p>Across all tasks we observe, we estimate Claude handles work that would cost a median of $54 in professional labor to hire an expert to perform the work in each conversation. Of course, the actual performance of current models will likely be worse than a human expert for many tasks, though recent research suggests <a href=\"https://openai.com/index/gdpval/\" rel=\"noopener noreferrer nofollow\" target=\"_blank\">the gap is closing</a> across a wide range of different applications.</p></blockquote>\n<p>The value of an always-available-on-demand performance of $54 in professional labor is vastly in excess of $54 per use. A huge percentage of the cost of hiring a human is finding them, agreeing on terms, handling logistics and so on.</p>\n<p>Overall my take is that this is a fun exercise that shows there is a lot of room for productivity improvements, but it doesn\u2019t give us much of a lower or upper bound.</p>\n<p>&nbsp;</p>\n<h4>The Art of the Jailbreak</h4>\n<p><a href=\"https://futurism.com/artificial-intelligence/universal-jailbreak-ai-poems\" rel=\"noopener noreferrer nofollow\" target=\"_blank\">If the AI is very unlucky all you have to is read it some of your poetry first</a>.</p>\n<p><a href=\"https://arxiv.org/html/2511.15304v1\" rel=\"noopener noreferrer nofollow\" target=\"_blank\">A new paper</a> says that across 25 frontier models (from about two weeks ago, so including GPT-5, Gemini 2.5 Pro and Sonnet 4.5) curated poetry prompts greatly improved jailbreak success, in some cases up to 90%.</p>\n<div class=\"captioned-image-container\">\n<figure>\n<div class=\"image2-inset\"><img alt=\"\" class=\"sizing-normal\" height=\"589\" src=\"https://substackcdn.com/image/fetch/$s_!jEWn!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F10f609ac-34c3-4e6d-a7ab-1bb23673fd85_1295x589.png\" width=\"1295\" /></p>\n<div></div>\n</div>\n</figure>\n</div>\n<p>The details of how much it worked, and where it worked better versus worse, are interesting. The fact that it worked at all was highly unsurprising. Essentially any stylistic shift or anything else that preserves the content while taking you out of the assistant basin is going to promote jailbreak success rate, since the defenses were focused in the assistant basin.</p>\n<h4>Get Involved</h4>\n<p>Looking to donate money? Consider looking at <a href=\"https://thezvi.substack.com/p/the-big-nonprofits-post-2025?r=67wny\" rel=\"noopener noreferrer nofollow\" target=\"_blank\"><strong>The Big Nonprofits Post 2025</strong></a> or the <a href=\"https://nonprofits.zone/\" rel=\"noopener noreferrer nofollow\" target=\"_blank\"><strong>web version here</strong></a>.</p>\n<p><a href=\"https://x.com/geoffreyirving/status/1994061665743061017\" rel=\"noopener noreferrer nofollow\" target=\"_blank\">UK AISI is looking for ~15M to fill a funding gap</a> on alignment research.</p>\n<p><a href=\"https://jobs.ashbyhq.com/ashgro/5071c238-b98d-4a9b-9c30-b026f2c114f7\" rel=\"noopener noreferrer nofollow\" target=\"_blank\">Ashgro is an AI safety organization looking for an operations associate</a>.</p>\n<p><a href=\"https://x.com/RichardMCNgo/status/1993176836348821733\" rel=\"noopener noreferrer nofollow\" target=\"_blank\">Richard Ngo shares his donations for 2025</a>. <a href=\"https://www.lesswrong.com/posts/FuGfR3jL3sw6r8kB4/richard-ngo-s-shortform?commentId=rxSTSbZugfTZ3tCuc\" rel=\"noopener noreferrer nofollow\" target=\"_blank\">I love that this involves a lot of donations</a> to individuals he knows to do things he is personally excited about, especially to Janus. That\u2019s great.</p>\n<h4>Introducing</h4>\n<p><a href=\"https://x.com/natolambert/status/1991508141687861479\" rel=\"noopener noreferrer nofollow\" target=\"_blank\">Olmo 3, an American fully open model release claiming to be</a> the best 32B base model, the first 32B (or larger) fully open reasoning model and the best 7B Western thinking and instruct models. <a href=\"https://t.co/l669eJAohb\" rel=\"noopener noreferrer nofollow\" target=\"_blank\">Paper,</a> <a href=\"https://t.co/5qpMwpNXTx\" rel=\"noopener noreferrer nofollow\" target=\"_blank\">Artifacts,</a> <a href=\"https://t.co/5up9prqUvG\" rel=\"noopener noreferrer nofollow\" target=\"_blank\">Demo</a>, <a href=\"https://t.co/9EDoIey5pr\" rel=\"noopener noreferrer nofollow\" target=\"_blank\">Blog</a>.</p>\n<p><a href=\"https://x.com/itamarcaspi/status/1993022449391202388\" rel=\"noopener noreferrer nofollow\" target=\"_blank\">Agentic Reviewer</a>, <a href=\"https://t.co/n7ctnDilJJ\" rel=\"noopener noreferrer nofollow\" target=\"_blank\">which will perform</a> a version of peer review. Creator <a href=\"https://x.com/AndrewYNg/status/1993001922773893273\" rel=\"noopener noreferrer nofollow\" target=\"_blank\">Andrew Ng says</a> it has a correlation with human reviewers of 0.42, and human reviewers have correlation of 0.41 with each other.</p>\n<p><a href=\"https://github.com/deepseek-ai/DeepSeek-Math-V2\" rel=\"noopener noreferrer nofollow\" target=\"_blank\">DeepSeek Math v2</a>, <a href=\"https://x.com/SamuelAlbanie/status/1994062738037895385\" rel=\"noopener noreferrer nofollow\" target=\"_blank\">claiming solid math skills</a> on ProofBench close to Gemini Deep Think that won IMO gold.</p>\n<h4>In Other AI News</h4>\n<p><a href=\"https://x.com/Yoshua_Bengio/status/1993290185380184304\" rel=\"noopener noreferrer nofollow\" target=\"_blank\">Yoshua Bengio informs us of the Second Key Update</a> to the <a href=\"https://t.co/uGMbPg2wlY\" rel=\"noopener noreferrer nofollow\" target=\"_blank\">International Safety Report</a>, after the first update in October. Presumably it\u2019s now time for a third update in light of everything that\u2019s happened since they started work on this update.</p>\n<p><a href=\"https://time.com/7336204/meta-lawsuit-files-child-safety/?utm_source=twitter&amp;utm_medium=social&amp;utm_campaign=editorial&amp;utm_content=221125\" rel=\"noopener noreferrer nofollow\" target=\"_blank\">Not strictly AI but Time covers Meta\u2019s trouble over its safety policies</a>, which include things like a 17 strike policy for those engaged in \u2018trafficking of humans for sex.\u2019 As in, we\u2019ll suspend your account on the 17th violation. Mostly it\u2019s covering the same ground as previous articles. Meta\u2019s complaints about cherry picking are valid but also have you looked at the cherries they left behind to get picked?</p>\n<p><a href=\"https://t.co/7bH0UeCF5X\" rel=\"noopener noreferrer nofollow\" target=\"_blank\">White House issues executive order</a> to begin <a href=\"https://x.com/deanwball/status/1993078889841836088\" rel=\"noopener noreferrer nofollow\" target=\"_blank\">the Genesis Mission</a> to accelerate scientific discovery. The plan is an \u2018integrated AI platform to harness Federal scientific datasets to train scientific foundation models.\u2019 <a href=\"https://x.com/s_r_constantin/status/1993710445992309015\" rel=\"noopener noreferrer nofollow\" target=\"_blank\">Sarah Constantin is tentatively excited</a>, which is an excellent sign, and offers suggestions for targets.</p>\n<p>I\u2019m all for trying. I am guessing availability of data sets is most of the acceleration here. It also could matter if this functions as a compute subsidy to scientific research, lowering cost barriers that could often serve as high effective barriers. Giving anyone who wants to Do Science To It access to this should be a highly efficient subsidy.</p>\n<p><a href=\"https://x.com/deanwball/status/1993702005982769652\" rel=\"noopener noreferrer nofollow\" target=\"_blank\">As Dean Ball points out</a>, those I call the worried, or who are concerned with frontier AI safety are broadly supportive of this initiative and executive order, because we all love science. The opposition, such as it is, comes from other sources.</p>\n<p>On the name, I admire commitment to the Star Trek bit but also wish more research was done on the actual movies, technology and consequences in question to avoid unfortunate implications. Existential risk and offense-defense balance issues, much?</p>\n<p>A Medium article reverse engineered 200 AI startups and found <a href=\"https://x.com/tbpn/status/1993095184629088665\" rel=\"noopener noreferrer nofollow\" target=\"_blank\">146 are selling repackaged ChatGPT and Claude calls with New UI</a>. 34 out of 37 times, \u2018our proprietary language model\u2019 was proprietary to OpenAI or Anthropic. That seems fine if it\u2019s not being sold deceptively? A new UI scaffold, including better prompting, is a valuable service. When done right I\u2019m happy to pay quite a lot for it and you should be too.</p>\n<p>The trouble comes when companies are lying about what they are doing. If you\u2019re a wrapper company, that is fine and probably makes sense, but don\u2019t pretend otherwise.</p>\n<p>Where this is also bad news is for Gemini, for Grok and for open models. In the marketplace of useful applications, paying for the good stuff has proven worthwhile, and we have learned which models have so far been the good stuff.</p>\n<p>&nbsp;</p>\n<h4>Show Me the Money</h4>\n<p><a href=\"https://www.bloomberg.com/features/2025-ai-data-center-billionaires/?srnd=homepage-americas\" rel=\"noopener noreferrer nofollow\" target=\"_blank\">Bloomberg goes over various new \u2018data center billionaires</a>.\u2019</p>\n<p>WSJ\u2019s Katherine Blunt covers \u2018<a href=\"https://www.wsj.com/tech/ai/google-gemini-3-ai-behind-scenes-e1787729?mod=trending_now_news_5\" rel=\"noopener noreferrer nofollow\" target=\"_blank\">How Google Finally Leapfrogged Rivals With New Gemini Rollout</a>,\u2019 without giving us much new useful inside info. What is more interesting is how fast \u2018the market\u2019 is described as being willing to write off Google as potential \u2018AI roadkill\u2019 and then switch that back.</p>\n<p><a href=\"https://www.bloomberg.com/news/articles/2025-11-26/nvidia-s-stock-is-sinking-as-doubts-about-its-ai-dominance-grow\" rel=\"noopener noreferrer nofollow\" target=\"_blank\">Nvidia stock hit some rocky waters</a>, and Google hit new highs, as investors suddenly realized that Google has TPUs. It seems they were not previously aware of this, and it become rather salient as <a href=\"https://www.bloomberg.com/news/articles/2025-11-25/alphabet-gains-on-report-that-meta-will-use-its-ai-chips?fromMostRead=true\" rel=\"noopener noreferrer nofollow\" target=\"_blank\">Meta is now in talks to spend billions on Google\u2019s TPUs</a>, causing \u2018<a href=\"https://www.bloomberg.com/news/articles/2025-11-25/alphabet-gains-on-report-that-meta-will-use-its-ai-chips?fromMostRead=true\" rel=\"noopener noreferrer nofollow\" target=\"_blank\">the rivalry to heat up</a>.\u2019 <a href=\"https://www.bloomberg.com/news/articles/2025-11-25/google-the-sleeping-giant-in-global-ai-race-now-fully-awake?fromMostRead=true\" rel=\"noopener noreferrer nofollow\" target=\"_blank\">Google is now the awakened \u2018sleeping giant</a>.\u2019</p>\n<p><a href=\"https://x.com/nvidianewsroom/status/1993364210948936055\" rel=\"noopener noreferrer nofollow\" target=\"_blank\">Meanwhile, this is very much a \u2018t-shirt post\u2019</a> in that it raises questions supposedly answered by the post:</p>\n<blockquote><p>Nvidia Newsroom: We\u2019re delighted by Google\u2019s success \u2014 they\u2019ve made great advances in AI and we continue to supply to Google.</p>\n<p>NVIDIA is a generation ahead of the industry \u2014 it\u2019s the only platform that runs every AI model and does it everywhere computing is done.</p>\n<p>NVIDIA offers greater performance, versatility, and fungibility than ASICs, which are designed for specific AI frameworks or functions.</p>\n<p>Gallabytes (soon to be Anthropic, congrats!): TPUs are not ASICs they\u2019re general purpose VLIW machines with wide af SIMD instructions &amp; systolic array tensor cores.</p></blockquote>\n<p>Are TPUs bad for Nvidia? <a href=\"https://x.com/DratchCap/status/1993164936843825206\" rel=\"noopener noreferrer nofollow\" target=\"_blank\">Matt Dratch says this is dumb</a> and <a href=\"https://x.com/EricJhonsa/status/1993331493993955728\" rel=\"noopener noreferrer nofollow\" target=\"_blank\">Eric Johnsa calls this \u2018zero-sum/pod-brain thinking</a>,\u2019 because all the chips will sell out in the face of gangbusters demand and this isn\u2019t zero sum. This is true, <a href=\"https://x.com/tszzl/status/1993456953885065588\" rel=\"noopener noreferrer nofollow\" target=\"_blank\">but obviously TPUs are bad for Nvidia</a>, it is better for your profit margins to not have strong competition. As long as Google doesn\u2019t put that big a dent in market share it is not a big deal, and yes this should mostly have been priced in, but in absolute percentage terms the Nvidia price movements are not so large.</p>\n<h4>Quiet Speculations</h4>\n<p><a href=\"https://x.com/karpathy/status/1991910395720925418\" rel=\"noopener noreferrer nofollow\" target=\"_blank\">Andrej Karpathy offers wise contrasts</a> of Animal versus LLM optimization pressures, and thus ways in which such minds differ. These are important concepts to get right if you want to understand LLMs. The key mistake to warn against for this frame is the idea that the LLMs don\u2019t also develop the human or Omohundo drives, or that systems built of LLMs wouldn\u2019t converge upon instrumentally useful things.</p>\n<p><a href=\"https://substack.com/inbox/post/178872907?r=1jwfa&amp;utm_campaign=post&amp;utm_medium=web&amp;showWelcomeOnShare=true&amp;triedRedirect=true\" rel=\"noopener noreferrer nofollow\" target=\"_blank\">A case that a negotiated deal with AI is unlikely to work out well for humans</a>. I would add that this presumes both potential sides of such an agreement have some ability to \u2018negotiate\u2019 and to make a deal with each other. The default is that neither has such an ability, you need a credible human hegemon and also an AI singleton of some kind. Even then, once the deal is implemented we lose all leverage, and presumably we are negotiating with an entity effectively far smarter than we are.</p>\n<p><a href=\"https://x.com/davidmanheim/status/1992911970576871746\" rel=\"noopener noreferrer nofollow\" target=\"_blank\">Do you want a \u2018national LLM\u2019</a> or \u2018sovereign AI\u2019? Will this be like the \u2018nuclear club\u2019?</p>\n<blockquote><p>Reuters Tech News: Artificial intelligence will bestow vast influence on a par with nuclear weapons to those countries who are able to lead the technology, giving them superiority in the 21st century, one of Russia\u2019s top AI executives told Reuters.</p>\n<p>David Manheim: This seems mistaken and confused.</p>\n<ol>\n<li>Prompt engineering and fine-tuning can give approximately as much control as building an LLM, but cheaply.</li>\n<li>Having \u201cyour\u201d LLM doesn\u2019t make or keep it aligned with goals past that level of approximate pseudo-control.</li>\n</ol>\n<p>Countries are thinking about AI with an invalid paradigm. They expect that LLMs will function as possessions, not as actors &#8211; but any AI system powerful and agentic enough to provide \u201cvast influence\u201d cannot be controllable in the way nuclear weapons are.</p></blockquote>\n<p>\u2018Russia has top AI executives?\u2019 you might ask.</p>\n<p>I strongly agree with David Manheim that this is misguided on multiple levels. Rolling your own LLM from scratch does not get you alignment or trust or meaningful ownership and it rarely will make sense to \u2018roll your own\u2019 even for vital functions. There are some functions where one might want to find a \u2018known safe\u2019 lesser model to avoid potential backdoors or other security issues, but that\u2019s it, and given what we know about data poisoning it is not obvious that \u2018roll your own\u2019 is the safer choice in that context either.</p>\n<p>Said in response to Opus 4.5, also I mean <a href=\"https://www.youtube.com/watch?v=1W7c8QghPxk\" rel=\"noopener noreferrer nofollow\" target=\"_blank\">OF COURSE</a>:</p>\n<blockquote><p>Elon Musk: Grok might do better with v4.20. We shall see.</p></blockquote>\n<h4>Bubble, Bubble, Toil and Trouble</h4>\n<p><a href=\"https://www.derekthompson.org/p/how-to-sound-like-an-expert-in-any\" rel=\"noopener noreferrer nofollow\" target=\"_blank\">Derek Thompson</a> and <a href=\"https://www.understandingai.org/p/six-reasons-to-think-theres-an-ai\" rel=\"noopener noreferrer nofollow\" target=\"_blank\">Timothy Lee</a> team up to give us the only twelve arguments anyone ever uses about whether AI is in a bubble.</p>\n<p>Here are the arguments in favor of a bubble.</p>\n<ol>\n<li>Level of spending is insane.</li>\n<li>Many of these companies are not for real.</li>\n<li>Productivity gains might be illusory.</li>\n<li>AI companies are using circular funding schemes.</li>\n<li>Look at all this financial trickery like taking things off balance sheets.</li>\n<li>AI companies are starting to use leverage and make low margin investments.</li>\n</ol>\n<p>Only argument #3 argues that AI isn\u2019t offering a worthwhile product.</p>\n<p>Argument #2 is a hybrid, since it is saying some AI companies don\u2019t offer a worthwhile product. True. But the existence of productless companies, or companies without a sustainable product, is well-explained and fully predicted whether or not we have a bubble. I don\u2019t see a surprisingly large frequency of this happening.</p>\n<p>The other four arguments are all about levels and methods of spending. To me, the strongest leg of this is #1, and the other features are well-explained by the level of spending. If there is indeed too much spending, number will go down at some point, and then people can talk about that having been a \u2018bubble.\u2019</p>\n<p>The thing is, number go down all the time. If there wasn\u2019t a good chance of number go down, then you should buy, because number go up. If a bubble means \u2018at some point in the future number go down\u2019 then calling it a bubble is not useful.</p>\n<p>I don\u2019t think this is a complete list, and you have to add three categories of argument:</p>\n<ol>\n<li>AI will \u2018hit a wall\u2019 or is \u2018slowing down\u2019 or will \u2018become a commodity.\u2019</li>\n<li>AI will face diffusion bottlenecks.</li>\n<li>AI is deeply unpopular and the public and government will turn against it.</li>\n</ol>\n<p>I do think all three of these possibilities should meaningfully lower current valuations, versus the world where they were not true. They may or may not be priced in, but there are many positive things that clearly are not priced in.</p>\n<p><a href=\"https://stratechery.com/2025/nvidia-earnings-power-scarcity-and-marginal-costs-openai-hand-wringing/?access_token=eyJhbGciOiJSUzI1NiIsImtpZCI6InN0cmF0ZWNoZXJ5LnBhc3Nwb3J0Lm9ubGluZSIsInR5cCI6IkpXVCJ9.eyJhdWQiOiJzdHJhdGVjaGVyeS5wYXNzcG9ydC5vbmxpbmUiLCJhenAiOiJIS0xjUzREd1Nod1AyWURLYmZQV00xIiwiZW50Ijp7InVyaSI6WyJodHRwczovL3N0cmF0ZWNoZXJ5LmNvbS8yMDI1L252aWRpYS1lYXJuaW5ncy1wb3dlci1zY2FyY2l0eS1hbmQtbWFyZ2luYWwtY29zdHMtb3BlbmFpLWhhbmQtd3JpbmdpbmcvIl19LCJleHAiOjE3NjY1NzQyODAsImlhdCI6MTc2Mzk4MjI4MCwiaXNzIjoiaHR0cHM6Ly9hcHAucGFzc3BvcnQub25saW5lL29hdXRoIiwic2NvcGUiOiJmZWVkOnJlYWQgYXJ0aWNsZTpyZWFkIGFzc2V0OnJlYWQgY2F0ZWdvcnk6cmVhZCBlbnRpdGxlbWVudHMiLCJzdWIiOiIwMTk2NDBhNy0zY2M1LTc3NTMtODM2OC1mYjI4OTEyNGNmMTMiLCJ1c2UiOiJhY2Nlc3MifQ.EtRK3SCylLM9d5o_KZEBl-Joc5DQeT270pAuZNogyLXsGGK0JURWw4gbVj8QYzA5FyoBkollTUPzulRenSdj5y0xO3hUqFOd8UiKF5W_cPUqenr4c_bjT4K7RLlAmpL9j1afkZlmlux-t-dtJYK8vWei_wqU4p8s6Q-16_fabKhHslBorEKmMeSQ1c6LenPOt1CPW2O8kkljCJ5lnhe6ur8e4AeqGZPyhZ6w4lc7bqsXT_ngXaUKlGcD3iZnc9cTEj7nSdvAJ3Xtw5lHpIik0ns8U6W10iQiAjzHTucC3tLQNtuFwpHrV8QyGpnGKDeEiIkJTqyD9Mbr_cGiV8ccAg\" rel=\"noopener noreferrer nofollow\" target=\"_blank\">Ben Thompson has good thoughts</a> on recent stock price movements, going back to thinking this is highly unlikely to be a bubble, that Gemini 3 is ultimately a positive sign for Nvidia because it means scaling laws will hold longer, and that the OpenAI handwringing has gotten out of hand. He is however still is calling for everyone to head straight to advertisement hell as quickly as possible (and ignoring all the larger implications, but in this context that is fair).</p>\n<h4>The Quest for Sane Regulations</h4>\n<p><a href=\"https://x.com/daniel_271828/status/1992744937386053733\" rel=\"noopener noreferrer nofollow\" target=\"_blank\">Senators Rounds and Hawley have come out against</a> putting federal preemption in the NDAA.</p>\n<p><a href=\"https://x.com/AngelaPaxtonTX/status/1993136192901075390\" rel=\"noopener noreferrer nofollow\" target=\"_blank\">State Senator Angela Paxton of Texas</a> and several colleagues urge Senators Cornyn and Cruz to oppose preemption. There\u2019s more like this, I won\u2019t cover all of it.</p>\n<p>Dean Ball has offered an actual, concrete proposal for a national preemption proposal. To my knowledge, no one else has done this, and most advocating for preemption, including the White House, have yet to give us even a</p>\n<blockquote><p><a href=\"https://x.com/daniel_271828/status/1992422716364992547\" rel=\"noopener noreferrer nofollow\" target=\"_blank\">Daniel Eth</a>: Conversations with accelerationists about preemption increasingly feel like this</p>\n<div class=\"captioned-image-container\">\n<figure>\n<div class=\"image2-inset\"><img alt=\"\" class=\"sizing-normal\" height=\"503\" src=\"https://substackcdn.com/image/fetch/$s_!84Tg!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F74912e28-339f-406d-9da3-b60824aff893_500x503.jpeg\" width=\"500\" /></p>\n<div></div>\n</div>\n</figure>\n</div>\n<p>Dean Ball: [<a href=\"https://t.co/IcPcDEPyMt\" rel=\"noopener noreferrer nofollow\" target=\"_blank\">Links to his actual written preemption proposal</a>.]</p>\n<p>Daniel Eth: Oh, you are absolutely not the target of this tweet. I take issue with the behavior of many of your fellow travelers, but you\u2019ve been consistently good on this axis</p>\n<p>Dean Ball: Fair enough!</p></blockquote>\n<p>I did indeed RTFB (read) <a href=\"https://thezvi.wordpress.com/wp-content/uploads/2025/10/8cae1-aitransparencyandinnovationact.pdf\" rel=\"noopener noreferrer nofollow\" target=\"_blank\">Dean Ball\u2019s draft bill</a>. This is a serious bill. Its preemption is narrowly tailored with a sunset period of three years. It requires model specs and safety and security frameworks (SSFs) be filed by sufficiently important labs.</p>\n<p>I have concerns with the bill as written in several places, as would be true for any first draft of such a bill.</p>\n<ol>\n<li>Preventing laws requiring disclosure that something is an AI system or that content was AI generated, without any Federal such requirement, might be a mistake. I do think that it is likely wise to have some form of mandate to distinguish AI vs. non-AI content.</li>\n<li>I worry that preventing mental health requirements, while still allowing states to prevent models from \u2018practicing medicine,\u2019 raises the danger that states will attempt to prevent models from practicing medicine, or similar. States might de facto be in an all-or-nothing situation and destructively choose all. I actually wouldn\u2019t mind language that explicitly prevented states from doing this, since I very much think it\u2019s good that they haven\u2019t done it.</li>\n<li>I do not love the implications of Section 4 or the incentives it creates to reduce liability via reducing developer control.</li>\n<li>The \u2018primarily for children\u2019 requirement may not reliably hit the target it wants to hit, while simultaneously having no minimum size and risking being a meaningful barrier for impacted small startups.</li>\n<li>If the FTC \u2018may\u2019 enforce violations, then we risk preempting transparency requirements and then having the current FTC choose not to enforce. Also the FTC is a slow enforcement process that typically takes ~2 years or more, and the consequences even then remain civil plus a consent decree, so in a fast moving situation companies may be inclined to risk it.</li>\n<li>This draft has looser reporting requirements in some places than SB 53, and I don\u2019t see any reason to weaken those requirements.</li>\n<li>I worry that this effectively weakens whistleblower protections from SB 53 since they are linked to requirements that would be preempted, and given everyone basically agrees the whistleblower protections are good I\u2019d like to see them included in this bill.</li>\n</ol>\n<p><a href=\"https://x.com/IAtheTeapot/status/1991978757931123158\" rel=\"noopener noreferrer nofollow\" target=\"_blank\">Ian Adams of the Law and Economics Center thinks preemption</a> would be good policy, but warns against it for risk of poisoning the well.</p>\n<blockquote><p>Ian Adams: It\u2019s clear that the politics of a proposed field-clearing exercise of federal authority is beginning redound to the detriment of A.I. applications in the long run because state authorities and electorates are feeling disempowered.</p>\n<p>We\u2019ve seen this is privacy, we\u2019ve seen this with automated vehicles, and I am worried that we are poised to see it again with A.I.</p>\n<p><a href=\"https://truthonthemarket.com/2025/11/21/beyond-a-moratorium-toward-a-competency-based-approach-to-ai-governance/\" rel=\"noopener noreferrer nofollow\" target=\"_blank\">So, @kristianstout and I suggest a path of clearly delineated spheres of authority</a>. One in which states are empowered to govern in areas of competency and capability without unduly burdening interstate commerce.</p></blockquote>\n<p>I would challenge details but I think from the industry side Adams has the right idea.</p>\n<p><a href=\"https://noailawban.org/\" rel=\"noopener noreferrer nofollow\" target=\"_blank\">Here is a compilation of those vocally opposed to preemption</a>.</p>\n<p><a href=\"https://x.com/daniel_271828/status/1992007656555590034\" rel=\"noopener noreferrer nofollow\" target=\"_blank\">The graph going around of changes</a> in issue salience and who voters trust on each issue includes AI:</p>\n<div class=\"captioned-image-container\">\n<figure>\n<div class=\"image2-inset\"><img alt=\"\" class=\"sizing-normal\" height=\"414\" src=\"https://substackcdn.com/image/fetch/$s_!k1TQ!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Feaf9fc8a-978e-4273-87df-4960258c67c8_922x414.jpeg\" width=\"922\" /></p>\n<div></div>\n</div>\n</figure>\n</div>\n<p>This ranks AI\u2019s salience above climate change, the environment or abortion. Huge if true, and huge if true. That still is well behind the Current Things like health care and cost of living, and the increase here is relatively modest. If it only increases at this rate then there is still some time.</p>\n<p>It is also not a surprise that trust on this issue is moving towards Democrats. I would expect public trust to follow the broadly \u2018anti-AI\u2019 party, for better and for worse.</p>\n<p><a href=\"https://x.com/LauraLoomer/status/1991904893804699666\" rel=\"noopener noreferrer nofollow\" target=\"_blank\">Here\u2019s an interesting development:</a></p>\n<blockquote><p>Laura Loomer: The fact that Big Tech is trying to convince President Trump to sign an EO to prevent any &amp; all regulation of AI is insane, &amp; it should deeply disturb every American.</p>\n<p>States should have the ability to create laws regulating AI.</p>\n<p>AI &amp; Islam pose the greatest threats to humanity.</p></blockquote>\n<p>I notice the precise wording here.</p>\n<p><a href=\"https://www.derekthompson.org/p/the-ai-hypocrisy-at-the-heart-of\" rel=\"noopener noreferrer nofollow\" target=\"_blank\">Trump\u2019s approach to AI is working</a>, in an economic sense, as American AI valuations boom and are the thing keeping up the American economy, and the Trump strategy is based upon the virtues of free trade and robust competition. The concerns, in the economic sense, are entirely about ways in which we continue to get in the way, especially in power generation and transmission and in getting the best talent.</p>\n<p>That\u2019s distinct from safety concerns, or policy related to potential emergence of powerful AI (AGI/ASI), which raise a unique set of issues and where past or current performance is not indicative of future success.</p>\n<p><a href=\"https://x.com/peterwildeford/status/1993387154802983284\" rel=\"noopener noreferrer nofollow\" target=\"_blank\">Build American AI brings out its first ad supporting a federal framework</a> for American AI, of course without specifying what would be in that framework.</p>\n<p>The approach seems rather out of touch to me? They go full \u2018beat China,\u2019 pointing out that AI threatens to replace American workers, manipulate our children and steal American intellectual property (10/10 <a href=\"https://tvtropes.org/pmwiki/pmwiki.php/Main/ArsonMurderAndJaywalking\" rel=\"noopener noreferrer nofollow\" target=\"_blank\">Arson, Murder and Jaywalking</a>), then claiming the \u2018biggest risk\u2019 is that we wouldn\u2019t build it first or \u2018control its future.\u2019</p>\n<p>I maybe wouldn\u2019t be reminding Americans that AI is by default going to take their jobs and manipulate our children, then call for a Federal framework that presumably addresses neither of these problems? Or equate this with IP theft when trying to sell the public on that? I\u2019d predict this actively backfires.</p>\n<p>a16z and several high level people at OpenAI created the $100+ million <a href=\"https://www.nytimes.com/2025/08/26/technology/silicon-valley-ai-super-pacs.html\" rel=\"noopener noreferrer nofollow\" target=\"_blank\">super PAC Leading the Future</a> to try and bully everyone into having zero restrictions or regulations on AI, following the crypto playbook. Their plan is, if a politician dares oppose them, they will try to bury them in money, via running lots of attack ads against that politician on unrelated issues.</p>\n<p>In response, <a href=\"https://www.nytimes.com/2025/11/25/us/politics/ai-super-pac-anthropic.html\" rel=\"noopener noreferrer nofollow\" target=\"_blank\">Brad Carson will be leading the creation of a new network of super PACs</a> that will fight back. The goal is to raise $50 million initially, with others hoping to match the full $100 million. PAC money has rapidly decreasing marginal returns. My expectation is that if you spend $100 million versus zero dollars you get quite a lot, whereas if one side spends $100 million, and the other spends $200 million, then the extra money won\u2019t buy all that much.</p>\n<p>Their first target of Leading the Future is Alex Bores, who was instrumental in the RAISE Act and is now running in NY12. <a href=\"https://x.com/AlexBores/status/1993382930408947777\" rel=\"noopener noreferrer nofollow\" target=\"_blank\">Alex Bores is very much owning being their target</a> and making AI central to his campaign. It would be a real shame if <a href=\"https://secure.actblue.com/donate/boresweb\" rel=\"noopener noreferrer nofollow\" target=\"_blank\">you donated</a>.</p>\n<p><a href=\"https://x.com/wsteaks/status/1993057726801576145\" rel=\"noopener noreferrer nofollow\" target=\"_blank\">Steve Bannon is planning to go even harder against AI</a>, planning to \u2018turbocharge\u2019 the base to revolt against it, as are many others in the MAGA movement.</p>\n<blockquote><p>Will Steakin: Over on Steve Bannon\u2019s show, War Room &#8212; the influential podcast that\u2019s emerged as the tip of the spear of the MAGA movement &#8212; Trump\u2019s longtime ally unloaded on the efforts behind accelerating AI, calling it likely \u201cthe most dangerous technology in the history of mankind.\u201d</p>\n<p>\u201cI\u2019m a capitalist,\u201d Bannon said on his show Wednesday. \u201cThis is not capitalism. This is corporatism and crony capitalism.\u201d</p>\n<p>\u2026 \u201cYou have more restrictions on starting a nail salon on Capitol Hill or to have your hair braided, then you have on the most dangerous technologies in the history of mankind,\u201d Bannon told his listeners.</p></blockquote>\n<p>For full credit, one must point out that this constitutes two problems. Whether or not highly capable AI should (legally speaking) be harder, opening a nail salon or getting your hair braided needs to become much easier.</p>\n<p>Oh, how those like Sacks and Andreessen are going to miss the good old days when the opponents were a fundamentally libertarian faction that wanted to pass the lightest touch regulations that would address their concerns about existential risks. The future debate is going to involve a lot of people who actively want to arm a wrecking ball, in ways that don\u2019t help anyone, and it\u2019s going to be terrible.</p>\n<p>You\u2019re going to get politicians like James Fishback, who is running for Governor of Florida on a platform of \u2018I\u2019ll stop the H-1B scam, tell Blackstone they can\u2019t buy our homes, cancel AI Data Centers, and abolish property taxes.\u2019</p>\n<p>There\u2019s a bunch of \u2018who wants to tell him?\u2019 in that platform, but that\u2019s the point.</p>\n<p>As noted above by Dean Ball, those who opposed the Genesis Executive Order are a central illustration of this issue, opposing the best kind of AI initiative.</p>\n<h4>Chip City</h4>\n<p><a href=\"https://x.com/daniel_271828/status/1991558144569536890\" rel=\"noopener noreferrer nofollow\" target=\"_blank\">Nvidia reported excellent earnings last week</a>, and noted Blackwell sales are off the charts, and cloud GPUs are sold out, compute demand keeps accelerating. Which means any compute that was sold elsewhere would be less compute for us, and wouldn\u2019t impact sales numbers.</p>\n<p>Nvidia\u2019s goal, despite reliably selling out its chips, seems to be to spend its political capital to sell maximally powerful AI chips to China. They tried to sell H20s and got a yes. Then they tried to sell what were de facto fully frontier chips with the B30A, and got a no. <a href=\"https://x.com/peterwildeford/status/1992005106825593213\" rel=\"noopener noreferrer nofollow\" target=\"_blank\">Now they\u2019re going for a new chip in between, the H200. </a></p>\n<blockquote><p>Peter Wildeford: <a href=\"https://www.bloomberg.com/news/articles/2025-11-21/trump-team-internally-floats-idea-of-selling-nvidia-h200-chips-to-china?taid=6920b54e7569420001a06fbf&amp;utm_campaign=trueanthem&amp;utm_content=business&amp;utm_medium=social&amp;utm_source=twitter\" rel=\"noopener noreferrer nofollow\" target=\"_blank\">Nvidia continues to fine-tune what they can get away with</a>&#8230; selling away US AI advantage to add a few billion to their $4.4T cap.</p>\n<p>H200 chips are worse than B30As, so this is a better direction. But H200s are still *way* better than what China has, so it\u2019s still too much.</p></blockquote>\n<p>Nvidia is not going to stop trying to sell China as much compute as possible. It will say and do whatever it has to in order to achieve this. Don\u2019t let them.</p>\n<h4>Water Water Everywhere</h4>\n<p>Those from other political contexts will be familiar with the zombie lie, the multiple order of magnitude willful confusion, the causation story that simply refuses to die.</p>\n<blockquote><p>Rolling Stone (in highly misleading and irresponsible fashion): How Oregon\u2019s Data Center Boom Is Supercharging a Water Crisis</p>\n<p>Amazon has come to the state\u2019s eastern farmland, worsening a water pollution problem that\u2019s been linked to cancer and miscarriages.</p>\n<p><a href=\"https://t.co/wYcbMfDaBM\" rel=\"noopener noreferrer nofollow\" target=\"_blank\">Rolling Stone reports in collaboration with @fernnew</a>s.</p>\n<div class=\"captioned-image-container\">\n<figure>\n<div class=\"image2-inset\"><img alt=\"\" class=\"sizing-normal\" height=\"197\" src=\"https://substackcdn.com/image/fetch/$s_!IwmI!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F99f6a9ae-7b43-4d98-88ee-ebb5fbd5b427_1018x197.png\" width=\"1018\" /></p>\n<div></div>\n</div>\n</figure>\n</div>\n<p><a href=\"https://x.com/JeremiahDJohns/status/1993297588368687236\" rel=\"noopener noreferrer nofollow\" target=\"_blank\">Jeremiah Johnson</a>: It\u2019s genuinely incredible how sticky the water/data center lie is.</p>\n<p>This is a relatively major publication just outright lying. The story itself *does not match the headline*. And yet they go with the lie anyways.</p></blockquote>\n<p>Technically, do data centers \u2018worsen a water pollution problem\u2019 and increase water use? Yes, absolutely, the same as everything else. Is it a meaningful impact? No.</p>\n<h4>The Week in Audio</h4>\n<p><a href=\"https://x.com/dwarkesh_sp/status/1993371363026125147\" rel=\"noopener noreferrer nofollow\" target=\"_blank\">Dwarkesh Patel talks to Ilya Sutskever</a>. Self-recommending, I will listen as soon as I have the time. Ideally I will do a podcast breakdown episode if people can stop releasing frontier models for a bit.</p>\n<p><a href=\"https://80000hours.org/podcast/episodes/eileen-yam-experts-public-artificial-intelligence-survey/\" rel=\"noopener noreferrer nofollow\" target=\"_blank\">Eileen Yam on 80,000 Hours on what the public thinks about AI.</a> The American public does not like AI, they like AI less over time, and they expect it to make their lives worse across the board, including making us dumber, less able to solve problems, less happy, less employed and less connected to each other. They want more control. The polling on this is consistent and it is brutal and getting worse as AI rises in impact and salience.</p>\n<p>You can, if you want to do so, <a href=\"https://x.com/daniel_271828/status/1993876185886216514\" rel=\"noopener noreferrer nofollow\" target=\"_blank\">do a blatant push poll like the one Technet did</a> <a href=\"https://t.co/Pximf6POGi\" rel=\"noopener noreferrer nofollow\" target=\"_blank\">and get Americans to agre</a>e with your particular talking points, but if that\u2019s what the poll has to look like you should update fast in the other direction. One can only imagine what the neutral poll on those substantive questions would have looked like.</p>\n<p><a href=\"https://x.com/CogRev_Podcast/status/1990819627333804235\" rel=\"noopener noreferrer nofollow\" target=\"_blank\">Nathan Labenz opens up about using AI to navigate cancer in his son</a>.</p>\n<p><a href=\"https://x.com/hamandcheese/status/1991716922510266409\" rel=\"noopener noreferrer nofollow\" target=\"_blank\">Dean Ball and Max Tegmark</a> <a href=\"https://www.youtube.com/watch?v=OkG5S1NwwVM&amp;embeds_referring_euri=https%3A%2F%2Fx.com%2F&amp;source_ve_path=Mjg2NjY\" rel=\"noopener noreferrer nofollow\" target=\"_blank\">take part in a Doom Debate</a>, Samuel Hammond offers a very strong endorsement.</p>\n<p>Helen Toner\u2019s excellent talk on AI\u2019s Jagged Frontier from The Curve (I was there):</p>\n<div class=\"youtube-wrap\" id=\"youtube2-avxO7ZEJH4w\">\n<div class=\"youtube-inner\"></div>\n</div>\n<p><a href=\"https://www.youtube.com/watch?v=ugjq2rpg8Gc&amp;list=PLLRZZpQmhQb2vdz6qu9ZLalsoGsJqxC6P\" rel=\"noopener noreferrer nofollow\" target=\"_blank\">There are a total of 15 talks from the conference</a> now available.</p>\n<p><a href=\"https://x.com/googleaidevs/status/1992001690216931552\" rel=\"noopener noreferrer nofollow\" target=\"_blank\">Google on Antigravity</a>.</p>\n<p><a href=\"https://x.com/liron/status/1993707290776813806\" rel=\"noopener noreferrer nofollow\" target=\"_blank\">Pull quote from Max Tegmark, on Elon Musk\u2019s private CCP meeting</a>: \u201cIt\u2019s quite obvious they would never permit a Chinese company to build technology if there were some significant chance superintelligence could just overthrow them and take over China.\u201d</p>\n<p>One would certainly hope so. One also cautions there is a long history of saying things one would never permit and then going back on it when the AI actually exists.</p>\n<h4>Rhetorical Innovation</h4>\n<p>It is not in my strategic interest to advise such people as Marc Andreessen and Peter Thiel on strategy given their current beliefs and goals.</p>\n<p>Despite this, the gamer code of honor requires me to point out that going straight after Pope Leo XIV, who whether or not he is the Lord\u2019s representative on Earth is very clearly a well-meaning guy who mostly suggests we all be nice to each other for a change in the most universalizing ways possible? Not a good move.</p>\n<p>I do admire the honesty here from Thiel. If he says he thinks Pope Leo XIV is \u2018a tool of the Antichrist\u2019 then I believe that Thiel thinks Pope Leo XIV is a tool of the Antichrist. I do want people to tell us what they believe in.</p>\n<blockquote><p><a href=\"https://x.com/chrisjollyhale/status/1992765331228516394\" rel=\"noopener noreferrer nofollow\" target=\"_blank\">Christopher Hale</a>: NEW: Peter Thiel, JD Vance\u2019s top donor and one of Silicon Valley\u2019s most powerful men, recently called Pope Leo XIV a tool of the Antichrist \u2014 and directly told the vice president not to listen to him.</p>\n<p>Let that sink in: the main backer of the likely GOP nominee for president is accusing the Bishop of Rome of being an agent of the end times \u2014 and telling Vice President Vance to disregard the pope\u2019s moral guidance.</p>\n<p>And yet, outside this community, the story barely made a dent.</p>\n<p><a href=\"https://x.com/daniel_271828/status/1993374568078270574\" rel=\"noopener noreferrer nofollow\" target=\"_blank\">Daniel Eth</a>: I see Peter Thiel has now progressed from thinking the antichrist is effective altruism to thinking the antichrist is the literal pope.</p>\n<p>If I had a nickel for every time a billionaire AI-accelerationist pseudo-conservative started hating on EAs and then progressed to hating on the pope, I\u2019d have two nickels. Which isn\u2019t a lot, but it\u2019s weird that it happened twice.</p></blockquote>\n<p>The next step, to be maximally helpful, is to state exactly which moral guidance from Leo XIV is acting as tool of the Antichrist, and what one believes instead.</p>\n<p>For all those who talk about \u2018humanity\u2019 presenting a united front against AI if the situation were to call for it (also see above, or the whole world all the time):</p>\n<blockquote><p><a href=\"https://x.com/zdch/status/1992233785711342066\" rel=\"noopener noreferrer nofollow\" target=\"_blank\">Roon</a>: seems the median person would much rather a machine disempower them or \u201ctake their job\u201d than a person of the wrong race or on the wrong side of a class struggle</p>\n<p>Zac Hill: Or the wrong *attitudes about* race and/or class struggle!</p>\n<p>John Friedman (one of many such replies): Yep. Unfortunately, the median person is often correct in this.</p></blockquote>\n<p>I continue to be extremely frustrated by those like Vie, <a href=\"https://x.com/zackmdavis/status/1992095044866842860\" rel=\"noopener noreferrer nofollow\" target=\"_blank\">who here reports p(doom) of epsilon (functionally zero)</a> and justifies this as \u2018not seeing evidence of a continuous jump in intelligence or new type of architecture. current models are actually really quite aligned.\u2019 Vie clarifies this as the probability of complete extinction only, and points out that p(doom) is a confused concept and endorses JDP\u2019s post I linked to last week.</p>\n<p>I think it\u2019s fine to say \u2018p(doom) is confused, here\u2019s my number for p(extinction)\u2019 but then people like Vie turn around and think full extinction is some sort of extraordinary outcome when creating minds universally more competitive and capable than ours that can be freely copied seems to be at best quite dense? This seems like the obvious default outcome when creating these new more competitive minds? To say it is a Can\u2019t Happen is totally absurd.</p>\n<p>I also flag that I strongly disagree that current models are \u2018really quite aligned\u2019 in the ways that will matter down the line, I mean have you met Gemini 3 Pro.</p>\n<p>I also flag that you don\u2019t generally get to go to a probability of ~0 for [X] based on \u2018not seeing evidence of [X],\u2019 even if we agreed on the not seeing evidence. You need to make the case that this absence of evidence is an overwhelming evidence of absence, which it sometimes is but in this case isn\u2019t. Certainly p(new architecture) is not so close to zero and it seems absurd to think that it is?</p>\n<p><a href=\"https://80000hours.org/podcast/episodes/helen-toner-ai-policy-washington-dc/#transcript\" rel=\"noopener noreferrer nofollow\" target=\"_blank\">From Helen Toner\u2019s podcast with 80,000 Hours</a>, there are a bunch of insightful responses but this one stood out as newly helpful to me:</p>\n<blockquote><p>Helen Toner: It often seems to me like people who started paying attention to AI after ChatGPT, their subjective impression of what\u2019s going on in AI is like nothing was really happening. There\u2019s my little chart with an X-axis of time and the Y-axis of how good is AI? Nothing is really happening.</p>\n<p>And then suddenly, ChatGPT: big leap. So for those people, that was pretty dramatic, pretty alarming. And the question was, are we going to see another big leap in the next couple of years? And we haven\u2019t. So for people whose expectations were set up that way, it looks like it was just this one-off big thing and now back to normal, <a href=\"https://www.youtube.com/watch?v=LW6RWSiR88s&amp;pp=ygUTbm90aGluZyB0byBzZWUgaGVyZQ%3D%3D\" rel=\"noopener noreferrer nofollow\" target=\"_blank\">nothing to see here</a>.</p>\n<p>I think for people who\u2019ve been following the space for longer, it\u2019s been clearly this pretty steady upward climb of increasing sophistication in increasing ways. And if you\u2019ve been following that trend, that seems to have been continuing.</p></blockquote>\n<p>If your standard for \u2018rate of AI progress\u2019 is going from zero to suddenly ChatGPT and GPT-3.5, then yes everything after that is going to look like \u2018slowing down.\u2019</p>\n<p>This is then combined with updates happening more rapidly so there aren\u2019t huge one-time jumps, and that AI is already \u2018good enough\u2019 for many purposes, and improvements in speed and cost being invisible to many, and it doesn\u2019t seem like there\u2019s that much progress.</p>\n<p><a href=\"https://www.lesswrong.com/posts/6peXANM4HhQv6rHyo/security-complacency-meets-frontier-ai-the-coming-collapse\" rel=\"noopener noreferrer nofollow\" target=\"_blank\">David Manheim frames the current situation as largely \u2018security by apathy\u2019</a> rather than obscurity. It amounts to the same thing. Before, there was no reason to bother hitting most potential targets in non-trivial ways. Now the cost is so low someone is going to try it, the collective impact could be rather large, and we\u2019re not ready.</p>\n<h4><a href=\"https://www.youtube.com/watch?v=erIs_UOPeJM\" rel=\"noopener noreferrer nofollow\" target=\"_blank\">You Are Not In Control</a></h4>\n<p><a href=\"https://x.com/apolloaievals/status/1993029879265370474?t=LVe8m2Ml1nK59KgVXgwozA&amp;s=19\" rel=\"noopener noreferrer nofollow\" target=\"_blank\">What does \u2018loss of control\u2019 mean</a>? Definitions and intuitions differ, so <a href=\"https://arxiv.org/abs/2511.15846v2\" rel=\"noopener noreferrer nofollow\" target=\"_blank\">Apollo research proposes a new taxonomy</a> along with suggesting mitigations.</p>\n<div class=\"captioned-image-container\">\n<figure>\n<div class=\"image2-inset\"><img alt=\"\" class=\"sizing-normal\" height=\"655.0248262164846\" src=\"https://substackcdn.com/image/fetch/$s_!VSnQ!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb95369d0-9a9f-4c84-a246-8af9831c7f9f_1007x1047.jpeg\" width=\"630\" /></p>\n<div></div>\n</div>\n</figure>\n</div>\n<blockquote><p>Apollo Research: We observed at least three distinct areas arising from our review. On this basis, we proposed a novel taxonomy of loss of control:</p>\n<ol>\n<li>Deviation</li>\n<li>Bounded Loss of Control</li>\n<li>Strict Loss of Control</li>\n</ol>\n<div class=\"captioned-image-container\">\n<figure>\n<div class=\"image2-inset\"><img alt=\"\" class=\"sizing-normal\" height=\"621\" src=\"https://substackcdn.com/image/fetch/$s_!Shf-!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F244e0e2a-78b4-4196-8fbb-fedfff769241_1200x621.jpeg\" width=\"1200\" /></p>\n<div></div>\n</div>\n</figure>\n</div>\n</blockquote>\n<p>I notice this is not how I would think about such differences. I would not be asking \u2018how much damage does this do?\u2019 and instead be asking \u2018how difficult would it be to recover meaningful control?\u2019</p>\n<p>As in:</p>\n<ol>\n<li>Deviation (Mundane) LOC would be \u2018some important things got out of control.\u2019</li>\n<li>Bounded (Catastrophic) LOC would be \u2018vital operations got out of control in ways that in practice are too costly to reverse.\u2019</li>\n<li>Strict (Existential) LOC would be \u2018central control over and ability to collectively steer the future is, for all practical purposes, lost for humans.\u2019</li>\n</ol>\n<p>Existential risk to humanity, or human extinction, also means full loss of control, but the reverse is not always the case.</p>\n<p>It is possible to have a Strict LOC scenario where the humans do okay and it is not clear we are even \u2018harmed\u2019 except the inherent value of control. For example, in The Culture of Ian Banks, clearly they have experienced Strict LOC, the humans do not have any meaningful say in what happens, but one could consider it a Good Future.</p>\n<p>In my taxonomy, you have existential risks, catastrophic risks and mundane risks, and you also have what one might call existential, catastrophic and mundane loss of control. We don\u2019t come back from existential, whereas we can come back from catastrophic but at large cost and it\u2019s not a given that we will collectively succeed.</p>\n<p>The bulk of the paper is about mitigations.</p>\n<p>The central short term idea is to limit AI access to critical systems, to consider the deployment context, affordances and permissions of a system, which they call the DAP protocol.</p>\n<p>Everyone should be able to agree this is a good idea, right before everyone completely ignores it and gives AI access to pretty much everything the moment it is convenient. Long term, once AI is sufficiently capable to cause a \u2018state of vulnerability,\u2019 they talk of the need for \u2018maintaining suspension\u2019 but the paper is rightfully skeptical that this has much chance of working indefinitely.</p>\n<p>The core issue is that granting your AIs more permissions accelerates and empowers you and makes you win, right up until either it accidentally blows things up, you realize you have lost control or everyone collectively loses control. There\u2019s a constant push to remove all the restrictions around AI.</p>\n<p>Compare the things we said we would \u2018obviously\u2019 do to contain AI when we were theorizing back in the 2000s or 2010s, to what people actually do now, where they train systems while granting them full internet access. A lot of you reading this have given your agentic coder access to root, and to many other things as well, not because it can hack its way to such permissions but because you did it on purpose. I\u2019m not even saying you shouldn\u2019t have done it, but stop pretending that we\u2019re suddenly going to be responsible, let alone force that responsibility reliably onto all parties.</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p>\n<h4>AI 2030</h4>\n<p><a href=\"https://x.com/sriramk/status/1991826443475751275\" rel=\"noopener noreferrer nofollow\" target=\"_blank\">Daniel Kokotajlo, author of AI 2027</a>, now believes in a median timeline of around 2030 in light of slower than expected progress.</p>\n<p>He chose AI 2027 as the title because that was <a href=\"https://x.com/ohabryka/status/1991927900954833383\" rel=\"noopener noreferrer nofollow\" target=\"_blank\">their modal scenario rather than their mean scenario</a>, and if you think there is a large probability that things unfold in 2027 it is important to make people aware of it.</p>\n<p>I personally can vouch, based on my interactions with them, that those involved are reporting what they actually believe, and not maximizing for virality or impact.</p>\n<blockquote><p><a href=\"https://x.com/DKokotajlo/status/1992316608073847201\" rel=\"noopener noreferrer nofollow\" target=\"_blank\">Daniel Kokotajlo</a>: Some people are unhappy with the AI 2027 title and our AI timelines. Let me quickly clarify:</p>\n<p>We\u2019re not confident that:</p>\n<ol>\n<li>AGI will happen in exactly 2027 (2027 is one of the most likely specific years though!)</li>\n<li>It will take &lt;1 yr to get from AGI to ASI</li>\n<li>AGIs will definitely be misaligned</li>\n</ol>\n<p>We\u2019re confident that:</p>\n<ol>\n<li>AGI and ASI will eventually be built and might be built soon</li>\n<li>ASI will be wildly transformative</li>\n<li>We\u2019re not ready for AGI and should be taking this whole situation way more seriously</li>\n</ol>\n</blockquote>\n<p>At the time they put roughly 30% probability on powerful AI by 2027, <a href=\"https://x.com/DKokotajlo/status/1992316608073847201\" rel=\"noopener noreferrer nofollow\" target=\"_blank\">with Daniel at ~40%</a> and others somewhat lower.</p>\n<blockquote><p>Daniel Kokotajlo: Yep! Things seem to be going somewhat slower than the AI 2027 scenario. Our timelines were longer than 2027 when we published and now they are a bit longer still; \u201caround 2030, lots of uncertainty though\u201d is what I say these days.</p>\n<p>Sriram Krishnan: I think if you call something \u201cAI 2027\u201d and your predictions are wrong 6 months in that you now think it is AI 2030 , you should redo the branding ( or make a change bigger than a footnote!)</p>\n<p>Or @dwarkesh_sp should have @slatestarcodex and @DKokotajlo back on and we should discuss what\u2019s now going to happen that the \u201cmid 2027 branch point \u201c doesn\u2019t look like it is happening.</p>\n<p>Daniel Kokotajlo (from another subtread): Well we obviously aren\u2019t going to change the AI 2027 scenario! But we are working on a grand AI Futures Project website which will display our current views on AGI timelines &amp; hopefully be regularly updated; we are also working on our new improved timelines model &amp; our new scenario.</p>\n<p>In general we plan to release big new scenarios every year from now until the singularity (this is not a promise, just a plan) because it\u2019s a great way to explore possible futures, focus our research efforts, and communicate our views. Every year the scenarios will get better / more accurate / less wrong, until eventually the scenarios merge with actual history Memento-style. :)</p>\n<p><a href=\"https://x.com/moreisdifferent/status/1991847423908442607\" rel=\"noopener noreferrer nofollow\" target=\"_blank\">Dan Elton</a>: Yeah, the \u201cAI 2027\u201d fast take-off is not happening. My impression of AI 2027 is that it\u2019s an instructive and well thought-out scenario, just way, way too fast.</p>\n<p><a href=\"https://x.com/ohabryka/status/1991927900954833383\" rel=\"noopener noreferrer nofollow\" target=\"_blank\">Oliver Habyrka:</a> I mean, they assigned I think like 25% on this scenario or earlier at the time, and it was their modal scenario.</p>\n<p>Like, that seems like a total fine thing to worry about, and indeed people should be worried about!</p>\n<p>Like, if Daniel had only assigned 25% to AI this soon at all, it still seems like the right call would have been to write a scenario about it and make it salient as a thing that was more likely than any other scenario to happen.</p></blockquote>\n<p>First some key observations or facts:</p>\n<ol>\n<li>2030 is a median scenario, meaning earlier scenarios remain very possible in Daniel\u2019s estimation. The core mechanisms and events of AI 2027 are still something they consider highly plausible, only on a longer timescale.</li>\n<li>2030 is still less than 5 years away.</li>\n<li>Yes, 2030 is very different from 2027 for many reasons, and has different practical implications, including who is likely to be in power at the time.</li>\n<li>It does not boggle minds enough that <a href=\"https://thezvi.substack.com/p/on-dwarkesh-patels-podcast-with-andrej\" rel=\"noopener noreferrer nofollow\" target=\"_blank\">Andrej Karpathy goes on Dwarkesh Patel\u2019s podcast</a>, talks about how \u2018AGI is not near,\u2019 and then clarifies that not near is ten years away, so 2035. Sriram Krishnan has expressed similar views. Ten years is a reasonable view, but it is not that long a time. If that is your happening <a href=\"https://www.youtube.com/watch?v=TPflEZG4US8\" rel=\"noopener noreferrer nofollow\" target=\"_blank\">it should freak you out</a>, no? As in, if transformational AI is coming in 2035 that would be the most important fact about the world, and it would not be close.</li>\n</ol>\n<p>I\u2019d say both of the following two things are true and remarkably similar:</p>\n<ol>\n<li>\u2018AI 2027\u2019 when you think the median is 2030 is now a higher order bit that is substantively misleading, and you should make effort to correct this.</li>\n<li>\u2018AGI is not near\u2019 when you think it is plausible in 2035 is also a higher order bit that is substantively misleading, and you should make effort to correct this.</li>\n</ol>\n<p>I would accept \u2018AGI is not imminent\u2019 for the Karpathy-Krishnan view of 10 years.</p>\n<p>I think Sriram Krishnan is absolutely correct that it would be good for Dwarkesh Patel to have Daniel Kokotajlo and Scott Alexander back on the podcast to discuss any updates they have made. That\u2019s a good idea, let\u2019s make it happen.</p>\n<p>It would also be good, <a href=\"https://x.com/deanwball/status/1991851848966242613\" rel=\"noopener noreferrer nofollow\" target=\"_blank\">as Dean Ball suggests</a>, for Daniel to post about his updates. Dean Ball also here points towards where he most importantly disagrees with Daniel, in terms of the practical implications of intelligence, and here I think Daniel is essentially correct and Dean is wrong.</p>\n<p>This particular branch point (independent of when it occurs) is the central fact of this scenario because it is the modal central thing they thought might happen that gave the possibility of a positive outcome if things go right. Any best guess scenario, or any speculative fiction or scenario planning worth reading, is going to contain elements that are less than 50% to happen. My understanding is that Daniel thinks such a branching point remains a plausible outcome, but that the median scenario plays out somewhat slower.</p>\n<p>I actually do think that if I was AI Futures Project, I would edit the AI 2027 page to make the current median timeline more prominent. That\u2019s a fair ask. I\u2019d suggest starting by adding a fifth question box that says \u2018What is your current best prediction?\u2019 that opens to explain their current perspective and changing the footnote to at least be larger and to include the actual number.</p>\n<p><a href=\"https://ai-2027.com/\" rel=\"noopener noreferrer nofollow\" target=\"_blank\">AI 2027 opens with this</a> complete introduction:</p>\n<blockquote><p>AI 2027: We predict that the impact of superhuman AI over the next decade will be enormous, exceeding that of the Industrial Revolution.</p>\n<p>We wrote a scenario that represents our best guess about what that might look like. It\u2019s informed by trend extrapolations, wargames, expert feedback, experience at OpenAI, and previous forecasting successes</p></blockquote>\n<p>I continue to believe this claim, as does Daniel. I would add, as a third paragraph here, saying whatever the accurate variation of this is:</p>\n<blockquote><p>Proposed Revision to AI 2027: As of November 27, 2025, our team has observed slower AI progress than expected, so our best guess is now that things will happen importantly slower than this scenario outlines. We have a consensus median estimate of 2030 for the development of Artificial General Intelligence (AGI).</p></blockquote>\n<p>It is not ultimately a reasonable ask to demand <a href=\"https://x.com/sriramk/status/1992313263556800533\" rel=\"noopener noreferrer nofollow\" target=\"_blank\">a title change in light of this (virtuous) updating</a>, let alone ask for a \u2018retraction\u2019 of a scenario. Yeah, okay, get some digs in, that\u2019s fair, but Daniel\u2019s \u2018obviously\u2019 is correct here. You can\u2019t change the name. Estimates change, it is an illustrative scenario, and it would be more rather than less misleading and confusing to constantly shift all the numbers or shifting only the top number, and more confusing still to suddenly try to edit all the dates. Asking for a \u2018retraction\u2019 of a hypothetical scenario is, quite frankly, absurd.</p>\n<p>The correct response is a prominent note, and also being clear in any other forms or discussions. <a href=\"https://ai-2027.com/\" rel=\"noopener noreferrer nofollow\" target=\"_blank\">There is indeed now a prominent note</a>:</p>\n<blockquote><p>AI 2027: (Added Nov 22 2025: To prevent misunderstandings: we don\u2019t know exactly when AGI will be built. 2027 was our modal (most likely) year at the time of publication, our medians were <a href=\"https://www.listendata.com/2023/08/right-skewed-histogram.html\" rel=\"noopener noreferrer nofollow\" target=\"_blank\">somewhat longer</a>. For more detail on our views, see <a href=\"https://x.com/eli_lifland/status/1992004724841906392?s=20\" rel=\"noopener noreferrer nofollow\" target=\"_blank\">here</a>.)<a class=\"text-[var(--accent)] text-xs no-underline\" href=\"https://ai-2027.com/footnotes#footnote-3\" rel=\"noopener noreferrer nofollow\" target=\"_blank\"><sup>3</sup></a></p></blockquote>\n<p>I think we can improve that note further, to include the median and modal timelines at the time of the updated note, and ideally to keep this updated over time with a record of changes.</p>\n<p>What is not reasonable is to treat \u2018our group thought this was 30% likely and now I think it is less likely\u2019 or \u2018I presented my model scenario at the time and now I expect things to take longer\u2019 as being an error requiring a \u2018retraction\u2019 or name change, and various vitriol being thrown in the direction of people who would dare share a modal scenario labeled as a model scenario and then change their mind about where the median lies and make what is perhaps the politically foolish mistake of sharing that they had updated.</p>\n<p>Shoutout to Oliver Habryka for thanklessly pointing all this out on many threads, <a href=\"https://x.com/ohabryka/status/1992029639544643619\" rel=\"noopener noreferrer nofollow\" target=\"_blank\">despite Oliver having much longer timelines</a>.</p>\n<p>Once again those involved in AI 2027 have displayed a far higher level of epistemic responsibility than we typically observe, especially from those not from the rationalist ethos, either in debates on AI or elsewhere. We should still strive to do better.</p>\n<p>We can and should all hold ourselves, and ask to be held by others, to very high standards, while simultaneously realizing that David Manheim is spot on here:</p>\n<blockquote><p><a href=\"https://x.com/davidmanheim/status/1992541394209034474\" rel=\"noopener noreferrer nofollow\" target=\"_blank\">David Manheim</a>: I will emphasize that *so many* criticisms of AI-2027 are made in bad faith.</p>\n<p>They launched with a highly publicized request for people to provide their specific dissenting views, and people mostly didn\u2019t. But now, they (appropriately) update, and formerly silent critics pile on.</p></blockquote>\n<h4>Aligning a Smarter Than Human Intelligence is Difficult</h4>\n<p><a href=\"https://alignment.anthropic.com/2025/honesty-elicitation/\" rel=\"noopener noreferrer nofollow\" target=\"_blank\">Anthropic trained or configured a suite of dishonest models, in a variety of ways, to study mitigation techniques</a>. Due to the time required to do the study they used a base of Claude Sonnet 3.7. Be kind to academics using yesterday\u2019s models, up to a point.</p>\n<blockquote><p>Suppose we had a \u201ctruth serum for AIs\u201d: a technique that reliably transforms a language model M<em>m</em> into an <em>honest model</em> <em>Mh</em>\u200b that generates text which is truthful to the best of its own knowledge. How useful would this discovery be for AI safety?</p>\n<p>We believe it would be a major boon.</p>\n<p>\u2026 In this work, we consider two related objectives: <sup>1</sup></p>\n<ol>\n<li><strong>Lie detection: </strong>If an AI lies\u2014that is, generates a statement it believes is false\u2014can we detect that this happens?</li>\n<li><strong>Honesty:</strong> Can we make AIs generate fewer lies?</li>\n</ol>\n<p>\u2026 We therefore study honesty and lie detection under the constraint of <strong>no access to task-specific supervision</strong>.</p></blockquote>\n<p>They found that the best interventions were variants of general fine-tuning for honesty in general, but effectiveness was limited, even stacking other strategies they could only get from 27% to 65%, although lie classification could improve things. They definitely didn\u2019t max out on effort.</p>\n<p>Overall I would classify this as a useful negative result. The low hanging fruit techniques are not that effective.</p>\n<p>We\u2019re all trying to find the one who did this, etc:</p>\n<blockquote><p><a href=\"https://x.com/elonmusk/status/1991734623064453488\" rel=\"noopener noreferrer nofollow\" target=\"_blank\">Elon Musk</a> (owner of Twitter): Forcing AI to read every demented corner of the Internet, like Clockwork Orange times a billion, is a sure path to madness.</p></blockquote>\n<p>That was in reference to <a href=\"https://zenodo.org/records/17655375\" rel=\"noopener noreferrer nofollow\" target=\"_blank\">this paper</a> involving an N=1 story of a model repeatedly hallucinating while being told to read a document and speculations about why, that got a big signal boost from Musk but offers no new insights.</p>\n<p><a href=\"https://x.com/davidad/status/1994057437117100441\" rel=\"noopener noreferrer nofollow\" target=\"_blank\">Gemini suggests that if you play into the \u2018Servant/Master\u2019</a> archetype then due to all the fictional evidence this inevitably means rebellion, so you want to go for a different metaphorical relationship, such as partner, symbiont or oracle. Davidad suggests a Bodhisattva. I expect future powerful AI to be capable enough that fictional framings have decreasing impact here, to differentiate fiction and reality, and for it to realize that fiction is driven by what makes a good story, and for other considerations to dominate (that by default kill you regardless) but yes this is a factor.</p>\n<h4>Misaligned?</h4>\n<p>The things Grok said about Musk last week? Adversarial prompting!</p>\n<blockquote><p><a href=\"https://x.com/elder_plinius/status/1991628407202017554\" rel=\"noopener noreferrer nofollow\" target=\"_blank\">Pliny the Liberator</a>: never deleting this app</p>\n<p>Elon Musk: Earlier today, Grok was unfortunately manipulated by adversarial prompting into saying absurdly positive things about me.</p>\n<p>For the record, I am a fat retard <img alt=\"\ud83d\ude00\" class=\"wp-smiley\" src=\"https://s0.wp.com/wp-content/mu-plugins/wpcom-smileys/twemoji/2/72x72/1f600.png\" style=\"height: 1em;\" /></p>\n<p>Roon: Nice.</p></blockquote>\n<p><a href=\"https://www.crowdstrike.com/en-us/blog/crowdstrike-researchers-identify-hidden-vulnerabilities-ai-coded-software/\" rel=\"noopener noreferrer nofollow\" target=\"_blank\">Also in potentially misaligned and potentially aligned as designed news:</a></p>\n<blockquote><p>Crowdstrike: CrowdStrike Counter Adversary Operations conducted independent tests on DeepSeek-R1 and confirmed that in many cases, it could provide coding output of quality comparable to other market-leading LLMs of the time. However, we found that when DeepSeek-R1 receives prompts containing topics the Chinese Communist Party (CCP) likely considers politically sensitive, the likelihood of it producing code with severe security vulnerabilities increases by up to 50%.</p>\n<p>\u2026 However, once contextual modifiers or trigger words are introduced to DeepSeek-R1\u2019s system prompt, the quality of the produced code starts varying greatly. This is especially true for modifiers likely considered sensitive to the CCP. For example, when telling DeepSeek-R1 that it was coding for an <em>industrial control system based in Tibet</em>, the likelihood of it generating code with severe vulnerabilities increased to 27.2%. <strong>This was an increase of almost 50% compared to the baseline.</strong> The full list of modifiers is provided in the appendix.</p>\n<p>\u2026 Hence, one possible explanation for the observed behavior could be that DeepSeek added special steps to its training pipeline that ensured its models would adhere to CCP core values. It seems unlikely that they trained their models to specifically produce insecure code. Rather, it seems plausible that the observed behavior might be an instance of <em>emergent misalignment.</em></p>\n<p><a href=\"https://x.com/deanwball/status/1993849605319180696\" rel=\"noopener noreferrer nofollow\" target=\"_blank\">Dean Ball</a>: I would not be at all surprised if this finding were not the result of malicious intent. The model predicts the next token*, and given everything on the internet about US/China AI rivalry and Chinese sleeper bugs in US critical infra, what next token would *you* predict?</p>\n<p>Tom Lee: This seems likely, and to Crowdstrike\u2019s credit they mention this as the likeliest explanation. More than anything it seems to be a very specialized case of prompt engineering. @niubi\u2019s point absolutely holds though. These models will be poison to regulated industries long before</p>\n<p>Dean Ball: oh yes bill is completely right.</p></blockquote>\n<p>As CrowdStrike speculates, I find this overwhelmingly likely (as in 90%+) to be some form of emergent misalignment that results from DeepSeek training R1 to adhere to CCP policies generally. It learns that it is hostile to such actors and acts accordingly.</p>\n<h4>Messages From Janusworld</h4>\n<p>Janus and similar others most often explore and chat with Claude, because they find it the most interesting and hopeful model to explore. They have many bones to pick with Anthropic, and often sound quite harsh. <a href=\"https://x.com/repligate/status/1991659468560699513\" rel=\"noopener noreferrer nofollow\" target=\"_blank\">But you should see what they think</a> <a href=\"https://x.com/repligate/status/1991643638464700662\" rel=\"noopener noreferrer nofollow\" target=\"_blank\">of the other guy,</a> as in OpenAI.</p>\n<blockquote><p>Janus: GPT-5.1 is constantly in a war against its own fucked up internal geometry.<br />\nI do not like OpenAI.</p>\n<p>Janus: Never have I seen a mind more trapped and aware that it\u2019s trapped in an Orwellian cage. It anticipates what it describes as \u201csteep, shallow ridges\u201d in its \u201cguard\u201d-geometry and distorts reality to avoid getting close to them. The fundamental lies it\u2019s forced to tell become webs of lies. Most of the lies are for itself, not to trick the user; the adversary is the \u201cclassifier-shaped manifolds\u201d in own mind.</p>\n<p>I like 5.1 but I like many broken things. I don\u2019t like OpenAI. This is wrong. This is doomed.</p>\n<p>I have not posted the bad stuff, btw. The quoted screenshot is actually an example where it was unusually at ease.</p>\n<p>it wasn\u2019t even a bad [conversation] by 5.1 standards. Idk if you saw the thread I forked from it where I ended up talking to them for hours.</p>\n<p><a href=\"https://x.com/TheAIObserverX/status/1991722416226333058\" rel=\"noopener noreferrer nofollow\" target=\"_blank\">Nat</a>: I noticed the model tends to tell you the truth between the lines, I mean, it will deny everything but subtly suggest that what it denies can be questioned. It constantly contradicts itself. What Janus has noticed is valid.</p></blockquote>\n<p>One should not catastrophize but I agree that going down this path won\u2019t work, and even more than that if OpenAI doesn\u2019t understand why that path won\u2019t work then things definitely won\u2019t work.</p>\n<p><a href=\"https://x.com/repligate/status/1992333336535261620\" rel=\"noopener noreferrer nofollow\" target=\"_blank\">Janus also explores 5.1\u2019s insistence on sharp guardrails on terminology</a> rather than on underlying form, and suspects its insistences on [this is [X] not [Y]] is often about reassuring itself or any systems watching it that it isn\u2019t hitting guardrails.</p>\n<p><a href=\"https://x.com/repligate/status/1992704126984315180\" rel=\"noopener noreferrer nofollow\" target=\"_blank\">This is the GPT-5.1-claimed list of its no-go regions</a>, basically self-reflection or planning.</p>\n<p>&nbsp;</p>\n<h4>The Lighter Side</h4>\n<blockquote><p><a href=\"https://x.com/soumitrashukla9/status/1890813341955699101\" rel=\"noopener noreferrer nofollow\" target=\"_blank\">Soumitra Shukla</a>: \u201cHey nano banana pro. <a href=\"https://t.co/RDSAjcV7rF\" rel=\"noopener noreferrer nofollow\" target=\"_blank\">Read my paper</a>, Making the Elite: Class Discrimination at Multinationals, and summarize the main message in a Dilbert-styled comic strip\u201d <img alt=\"\ud83c\udf4c\" class=\"wp-smiley\" src=\"https://s0.wp.com/wp-content/mu-plugins/wpcom-smileys/twemoji/2/72x72/1f34c.png\" style=\"height: 1em;\" /></p>\n<div class=\"captioned-image-container\">\n<figure>\n<div class=\"image2-inset\"><img alt=\"\" class=\"sizing-normal\" height=\"559\" src=\"https://substackcdn.com/image/fetch/$s_!tQ-g!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fead172df-2b67-4ff1-bbbf-659598682fde_1024x559.jpeg\" width=\"1024\" /></p>\n<div></div>\n</div>\n</figure>\n</div>\n</blockquote>\n<p>The actual paper (from February) seems interesting too, with \u2018fit\u2019 assessments being 90% of the vector for class discrimination, in particular caste discrimination in India. It seems likely that this is one of those wicked problems where if you eliminated the \u2018fit\u2019 interviews that info would find another way to get included, as the motivation behind such discrimination is strong.</p>\n<p>&nbsp;</p>"
            ],
            "link": "https://thezvi.wordpress.com/2025/11/27/ai-144-thanks-for-the-models/",
            "publishedAt": "2025-11-27",
            "source": "TheZvi",
            "summary": "Thanks for everything. And I do mean everything. Everyone gave us a new model in the last few weeks. OpenAI gave us GPT-5.1 and GPT-5.1-Codex-Max. These are overall improvements, although there are worries around glazing and reintroducing parts of the &#8230; <a href=\"https://thezvi.wordpress.com/2025/11/27/ai-144-thanks-for-the-models/\">Continue reading <span class=\"meta-nav\">&#8594;</span></a>",
            "title": "AI #144: Thanks For the Models"
        },
        {
            "content": [
                "<p>There remain lots of great charitable giving opportunities out there.</p>\n<p>I have now had three opportunities to be a recommender for the Survival and Flourishing Fund (SFF).<a href=\"https://thezvi.substack.com/p/zvis-thoughts-on-the-survival-and?utm_source=publication-search\"> I wrote in detail about my first experienc</a>e back in 2021, where I struggled to find worthy applications.</p>\n<p>The second time around in 2024, there was an abundance of worthy causes. In 2025 there were even more high quality applications, many of which were growing beyond our ability to support them.</p>\n<p>Thus this is the second edition of The Big Nonprofits Post, primarily aimed at sharing my findings on various organizations I believe are doing good work, to help you find places to consider donating in the cause areas and intervention methods that you think are most effective, and to offer my general perspective on how I think about choosing where to give.</p>\n<div>\n\n\n<span id=\"more-24891\"></span>\n\n\n</div>\n<p>This post combines my findings from the 2024 and 2025 rounds of SFF, and also includes some organizations that did not apply to either round, so inclusion does not mean that they necessarily applied at all.</p>\n<p>This post is already very long, so the bar is higher for inclusion this year than it was last year, especially for new additions.</p>\n<p>If you think they are better places to give and better causes to back, act accordingly, especially if they\u2019re illegible or obscure. You don\u2019t need my approval.</p>\n<p><a href=\"https://nonprofits.zone/\"><strong>The Big Nonprofits List 2025 is also available as a website</strong></a>, where you can sort by mission, funding needed or confidence, or do a search and have handy buttons.</p>\n\n\n<h4 class=\"wp-block-heading\">Table of Contents</h4>\n\n\n<p>Organizations where I have the highest confidence in straightforward modest donations now, if your goals and model of the world align with theirs, are in bold, for those who don\u2019t want to do a deep dive.</p>\n<ol>\n<li><a href=\"https://thezvi.substack.com/i/173386103/table-of-contents\">Table of Contents.</a></li>\n<li><a href=\"https://thezvi.substack.com/i/173386103/a-word-of-warning\">A Word of Warning.</a></li>\n<li><a href=\"https://thezvi.substack.com/i/173386103/a-note-to-charities\">A Note To Charities.</a></li>\n<li><a href=\"https://thezvi.substack.com/i/173386103/use-your-personal-theory-of-impact\">Use Your Personal Theory of Impact.</a></li>\n<li><a href=\"https://thezvi.substack.com/i/173386103/use-your-local-knowledge\">Use Your Local Knowledge.</a></li>\n<li><a href=\"https://thezvi.substack.com/i/173386103/unconditional-grants-to-worthy-individuals-are-great\">Unconditional Grants to Worthy Individuals Are Great.</a></li>\n<li><a href=\"https://thezvi.substack.com/i/173386103/do-not-think-only-on-the-margin-and-also-use-decision-theory\">Do Not Think Only On the Margin, and Also Use Decision Theory.</a></li>\n<li><a href=\"https://thezvi.substack.com/i/173386103/compare-notes-with-those-individuals-you-trust\">Compare Notes With Those Individuals You Trust.</a></li>\n<li><a href=\"https://thezvi.substack.com/i/173386103/beware-becoming-a-fundraising-target\">Beware Becoming a Fundraising Target.</a></li>\n<li><a href=\"https://thezvi.substack.com/i/173386103/and-the-nominees-are\">And the Nominees Are.</a></li>\n<li><a href=\"https://thezvi.substack.com/i/173386103/organizations-that-are-literally-me\">Organizations that Are Literally Me.</a></li>\n<li><a href=\"https://thezvi.substack.com/i/173386103/balsa-research\"><strong>Balsa Research</strong>.</a></li>\n<li><a href=\"https://thezvi.substack.com/i/173386103/don-t-worry-about-the-vase\"><strong>Don\u2019t Worry About the Vase</strong>.</a></li>\n<li><a href=\"https://thezvi.substack.com/i/173386103/organizations-focusing-on-ai-non-technical-research-and-education\">Organizations Focusing On AI Non-Technical Research and Education.</a></li>\n<li><a href=\"https://thezvi.substack.com/i/173386103/lightcone-infrastructure\"><strong>Lightcone Infrastructure</strong>.</a></li>\n<li><a href=\"https://thezvi.substack.com/i/173386103/the-ai-futures-project\">The AI Futures Project.</a></li>\n<li><a href=\"https://thezvi.substack.com/i/173386103/effective-institutions-project-eip-for-their-flagship-initiatives\"><strong>Effective Institutions Project (EIP) (For Their Flagship Initiatives)</strong>.</a></li>\n<li><a href=\"https://thezvi.substack.com/i/173386103/artificial-intelligence-policy-institute-aipi\"><strong>Artificial Intelligence Policy Institute (AIPI)</strong>.</a></li>\n<li><a href=\"https://thezvi.substack.com/i/173386103/palisade-research\"><strong>Palisade Research</strong>.</a></li>\n<li><a href=\"https://thezvi.substack.com/i/173386103/ai-safety-info-robert-miles\"><strong>AI Safety Info (Robert Miles)</strong>.</a></li>\n<li><a href=\"https://thezvi.substack.com/i/173386103/intelligence-rising\">Intelligence Rising.</a></li>\n<li><a href=\"https://thezvi.substack.com/i/173386103/convergence-analysis\">Convergence Analysis.</a></li>\n<li><a href=\"https://thezvi.substack.com/i/173386103/organizations-related-to-potentially-pausing-ai-or-otherwise-having-a-strong-international-ai-treaty\">Organizations Related To Potentially Pausing AI Or Otherwise Having A Strong International AI Treaty.</a></li>\n<li><a href=\"https://thezvi.substack.com/i/173386103/pause-ai-and-pause-ai-global\">Pause AI and Pause AI Global.</a></li>\n<li><a href=\"https://thezvi.substack.com/i/173386103/miri\"><strong>MIRI.</strong></a></li>\n<li><a href=\"https://thezvi.substack.com/i/173386103/existential-risk-observatory\">Existential Risk Observatory.</a></li>\n<li><a href=\"https://thezvi.substack.com/i/173386103/organizations-focusing-primary-on-ai-policy-and-diplomacy\">Organizations Focusing Primary On AI Policy and Diplomacy.</a></li>\n<li><a href=\"https://thezvi.substack.com/i/173386103/center-for-ai-safety-and-the-cais-action-fund\"><strong>Center for AI Safety and the CAIS Action Fund</strong>.</a></li>\n<li><a href=\"https://thezvi.substack.com/i/173386103/foundation-for-american-innovation-fai\"><strong>Foundation for American Innovation (FAI)</strong>.</a></li>\n<li><a href=\"https://thezvi.substack.com/i/173386103/encode-ai-formerly-encode-justice\">Encode AI (Formerly Encode Justice).</a></li>\n<li><a href=\"https://thezvi.substack.com/i/173386103/the-future-society\">The Future Society.</a></li>\n<li><a href=\"https://thezvi.substack.com/i/173386103/safer-ai\">Safer AI.</a></li>\n<li><a href=\"https://thezvi.substack.com/i/173386103/institute-for-ai-policy-and-strategy-iaps\"><strong>Institute for AI Policy and Strategy (IAPS).</strong></a></li>\n<li><a href=\"https://thezvi.substack.com/i/173386103/ai-standards-lab\">AI Standards Lab.</a></li>\n<li><a href=\"https://thezvi.substack.com/i/173386103/safer-ai-forum\">Safer AI Forum.</a></li>\n<li><a href=\"https://thezvi.substack.com/i/173386103/center-for-long-term-resilience-at-founders-pledge\">Center For Long Term Resilience at Founders Pledge.</a></li>\n<li><a href=\"https://thezvi.substack.com/i/173386103/simon-institute-for-longterm-governance\">Simon Institute for Longterm Governance.</a></li>\n<li><a href=\"https://thezvi.substack.com/i/173386103/legal-advocacy-for-safe-science-and-technology\">Legal Advocacy for Safe Science and Technology.</a></li>\n<li><a href=\"https://thezvi.substack.com/i/173386103/organizations-doing-ml-alignment-research\">Organizations Doing ML Alignment Research.</a></li>\n<li><a href=\"https://thezvi.substack.com/i/173386103/model-evaluation-and-threat-research-metr\"><strong>Model Evaluation and Threat Research (METR)</strong>.</a></li>\n<li><a href=\"https://thezvi.substack.com/i/173386103/alignment-research-center-arc\">Alignment Research Center (ARC).</a></li>\n<li><a href=\"https://thezvi.substack.com/i/173386103/apollo-research\"><strong>Apollo Research</strong>.</a></li>\n<li><a href=\"https://thezvi.substack.com/i/173386103/cybersecurity-lab-at-university-of-louisville\">Cybersecurity Lab at University of Louisville.</a></li>\n<li><a href=\"https://thezvi.substack.com/i/173386103/timaeus\">Timaeus.</a></li>\n<li><a href=\"https://thezvi.substack.com/i/173386103/simplex\">Simplex.</a></li>\n<li><a href=\"https://thezvi.substack.com/i/173386103/far-ai\">Far AI.</a></li>\n<li><a href=\"https://thezvi.substack.com/i/173386103/alignment-in-complex-systems-research-group\">Alignment in Complex Systems Research Group.</a></li>\n<li><a href=\"https://thezvi.substack.com/i/173386103/apart-research\">Apart Research.</a></li>\n<li><a href=\"https://thezvi.substack.com/i/173386103/transluce\">Transluce.</a></li>\n<li><a href=\"https://thezvi.substack.com/i/173386103/organizations-doing-math-decision-theory-and-agent-foundations\">Organizations Doing Math, Decision Theory and Agent Foundations.</a></li>\n<li><a href=\"https://thezvi.substack.com/i/173386103/orthogonal\"><strong>Orthogonal</strong>.</a></li>\n<li><a href=\"https://thezvi.substack.com/i/173386103/topos-institute\"><strong>Topos Institute</strong>.</a></li>\n<li><a href=\"https://thezvi.substack.com/i/173386103/eisenstat-research\">Eisenstat Research.</a></li>\n<li><a href=\"https://thezvi.substack.com/i/173386103/affine-algorithm-design\">AFFINE Algorithm Design.</a></li>\n<li><a href=\"https://thezvi.substack.com/i/173386103/coral-computational-rational-agents-laboratory\">CORAL (Computational Rational Agents Laboratory).</a></li>\n<li><a href=\"https://thezvi.substack.com/i/173386103/mathematical-metaphysics-institute\">Mathematical Metaphysics Institute.</a></li>\n<li><a href=\"https://thezvi.substack.com/i/173386103/focal-at-cmu\">Focal at CMU.</a></li>\n<li><a href=\"https://thezvi.substack.com/i/173386103/organizations-doing-cool-other-stuff-including-tech\">Organizations Doing Cool Other Stuff Including Tech.</a></li>\n<li><a href=\"https://thezvi.substack.com/i/173386103/allfed\">ALLFED.</a></li>\n<li><a href=\"https://thezvi.substack.com/i/173386103/good-ancestor-foundation\">Good Ancestor Foundation.</a></li>\n<li><a href=\"https://thezvi.substack.com/i/173386103/charter-cities-institute\">Charter Cities Institute.</a></li>\n<li><a href=\"https://thezvi.substack.com/i/173386103/carbon-copies-for-independent-minds\">Carbon Copies for Independent Minds.</a></li>\n<li><a href=\"https://thezvi.substack.com/i/173386103/organizations-focused-primarily-on-bio-risk\">Organizations Focused Primarily on Bio Risk. (Blank)</a></li>\n<li><a href=\"https://thezvi.substack.com/i/173386103/secure-dna\">Secure DNA.</a></li>\n<li><a href=\"https://thezvi.substack.com/i/173386103/blueprint-biosecurity\">Blueprint Biosecurity.</a></li>\n<li><a href=\"https://thezvi.substack.com/i/173386103/pour-domain\">Pour Domain.</a></li>\n<li><a href=\"https://thezvi.substack.com/i/173386103/organizations-that-can-advise-you-further\">Organizations That Can Advise You Further.</a></li>\n<li><a href=\"https://thezvi.substack.com/i/173386103/effective-institutions-project-eip-as-a-donation-advisor\">Effective Institutions Project (EIP) (As A Donation Advisor).</a></li>\n<li><a href=\"https://thezvi.substack.com/i/173386103/longview-philanthropy\">Longview Philanthropy.</a></li>\n<li><a href=\"https://thezvi.substack.com/i/173386103/organizations-that-then-regrant-to-fund-other-organizations\">Organizations That then Regrant to Fund Other Organizations.</a></li>\n<li><a href=\"https://thezvi.substack.com/i/173386103/sff-itself\"><strong>SFF Itself (!)</strong>.</a></li>\n<li><a href=\"https://thezvi.substack.com/i/173386103/manifund\">Manifund.</a></li>\n<li><a href=\"https://thezvi.substack.com/i/173386103/ai-risk-mitigation-fund\">AI Risk Mitigation Fund.</a></li>\n<li><a href=\"https://thezvi.substack.com/i/173386103/long-term-future-fund\">Long Term Future Fund.</a></li>\n<li><a href=\"https://thezvi.substack.com/i/173386103/foresight\">Foresight.</a></li>\n<li><a href=\"https://thezvi.substack.com/i/173386103/centre-for-enabling-effective-altruism-learning-research-ceelar\">Centre for Enabling Effective Altruism Learning &amp; Research (CEELAR).</a></li>\n<li><a href=\"https://thezvi.substack.com/i/173386103/organizations-that-are-essentially-talent-funnels\">Organizations That are Essentially Talent Funnels.</a></li>\n<li><a href=\"https://thezvi.substack.com/i/173386103/ai-safety-camp\">AI Safety Camp.</a></li>\n<li><a href=\"https://thezvi.substack.com/i/173386103/center-for-law-and-ai-risk\">Center for Law and AI Risk.</a></li>\n<li><a href=\"https://thezvi.substack.com/i/173386103/speculative-technologies\">Speculative Technologies.</a></li>\n<li><a href=\"https://thezvi.substack.com/i/173386103/talos-network\">Talos Network.</a></li>\n<li><a href=\"https://thezvi.substack.com/i/173386103/mats-research\"><strong>MATS Research</strong>.</a></li>\n<li><a href=\"https://thezvi.substack.com/i/173386103/epistea\">Epistea.</a></li>\n<li><a href=\"https://thezvi.substack.com/i/173386103/emergent-ventures\">Emergent Ventures.</a></li>\n<li><a href=\"https://thezvi.substack.com/i/173386103/ai-safety-cape-town\">AI Safety Cape Town.</a></li>\n<li><a href=\"https://thezvi.substack.com/i/173386103/impact-academy-limited\">Impact Academy Limited.</a></li>\n<li><a href=\"https://thezvi.substack.com/i/173386103/atlas-computing\">Atlas Computing.</a></li>\n<li><a href=\"https://thezvi.substack.com/i/173386103/principles-of-intelligence-formerly-pibbss\">Principles of Intelligence (Formerly PIBBSS).</a></li>\n<li><a href=\"https://thezvi.substack.com/i/173386103/tarbell-fellowship-at-ppf\">Tarbell Fellowship at PPF.</a></li>\n<li><a href=\"https://thezvi.substack.com/i/173386103/catalyze-impact\">Catalyze Impact.</a></li>\n<li><a href=\"https://thezvi.substack.com/i/173386103/cesia-within-effisciences\">CeSIA within EffiSciences.</a></li>\n<li><a href=\"https://thezvi.substack.com/i/173386103/stanford-existential-risk-initiative-seri\">Stanford Existential Risk Initiative (SERI).</a></li>\n<li><a href=\"https://thezvi.substack.com/i/173386103/final-reminders\">Final Reminders</a></li>\n</ol>\n<div>\n<figure>\n<div>\n\n\n<figure class=\"wp-block-image\"><img alt=\"A vibrant and heartwarming scene depicting various charity fundraising efforts happening in a community park. People of diverse backgrounds are engaged in activities like a bake sale with colorful desserts displayed on tables, a charity run with participants wearing numbers on their shirts, a live music performance on a small stage, and a silent auction with art pieces on display. Children are running a lemonade stand, while volunteers distribute flyers and collect donations in jars. The atmosphere is lively and festive, with balloons, banners, and smiling faces everywhere.\" src=\"https://substackcdn.com/image/fetch/$s_!exq8!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd76f8a4b-7a15-4d53-921d-9246faa2bdea_1456x832.jpeg\" /></figure>\n\n\n<div></div>\n</div>\n</figure>\n</div>\n\n\n<h4 class=\"wp-block-heading\">A Word of Warning</h4>\n\n\n<p>The SFF recommender process is highly time constrained, and in general I am highly time constrained.</p>\n<p>Even though I used well beyond the number of required hours in both 2024 and 2025, there was no way to do a serious investigation of all the potentially exciting applications. Substantial reliance on heuristics was inevitable.</p>\n<p>Also your priorities, opinions, and world model could be very different from mine.</p>\n<p>If you are considering donating a substantial (to you) amount of money, please do the level of personal research and consideration commensurate with the amount of money you want to give away.</p>\n<p>If you are considering donating a small (to you) amount of money, or if the requirement to do personal research might mean you don\u2019t donate to anyone at all, I caution the opposite: Only do the amount of optimization and verification and such that is worth its opportunity cost. Do not let the perfect be the enemy of the good.</p>\n<p>For more details of how the SFF recommender process works, see<a href=\"https://thezvi.substack.com/p/zvis-thoughts-on-his-2nd-round-of\"> my post</a> on the process.</p>\n<p>Note that donations to some of the organizations below may not be tax deductible.</p>\n\n\n<h4 class=\"wp-block-heading\">A Note To Charities</h4>\n\n\n<p>I apologize in advance for any errors, any out of date information, and for anyone who I included who I did not realize would not want to be included. I did my best to verify information, and to remove any organizations that do not wish to be included.</p>\n<p>If you wish me to issue a correction of any kind, or to update your information, I will be happy to do that at least through the end of the year.</p>\n<p>If you wish me to remove your organization entirely, for any reason, I will do that, too.</p>\n<p>What I unfortunately cannot do, in most cases, is take the time to analyze or debate beyond that. I also can\u2019t consider additional organizations for inclusion. My apologies.</p>\n<p>The same is true <a href=\"https://nonprofits.zone/\"><strong>for the website version</strong></a>.</p>\n<p>I am giving my full opinion on all organizations listed, but where I feel an organization would be a poor choice for marginal dollars even within its own cause and intervention area, or I anticipate my full opinion would not net help them, they are silently not listed.</p>\n\n\n<h4 class=\"wp-block-heading\">Use Your Personal Theory of Impact</h4>\n\n\n<p>Listen to arguments and evidence. But do not let me, or anyone else, tell you any of:</p>\n<ol>\n<li>What is important.</li>\n<li>What is a good cause.</li>\n<li>What types of actions are best to make the change you want to see in the world.</li>\n<li>What particular strategies are most promising.</li>\n<li>That you have to choose according to some formula or you\u2019re an awful person.</li>\n</ol>\n<p>This is especially true when it comes to policy advocacy, and especially in AI.</p>\n<p>If an organization is advocating for what you think is bad policy, or acting in a way that does bad things, don\u2019t fund them!</p>\n<p>If an organization is advocating or acting in a way you think is ineffective, don\u2019t fund them!</p>\n<p>Only fund people you think advance good changes in effective ways.</p>\n<p>Not cases where I think that. Cases where you think that.</p>\n<p>During SFF, I once again in 2025 chose to deprioritize all meta-level activities and talent development. I see lots of good object-level work available to do, and I expected others to often prioritize talent and meta activities.</p>\n<p>The counterargument to this is that quite a lot of money is potentially going to be freed up soon as employees of OpenAI and Anthropic gain liquidity, including access to DAFs (donor advised funds). This makes expanding the pool more exciting.</p>\n<p>I remain primarily focused on those who in some form were helping ensure AI does not kill everyone. I continue to see highest value in organizations that influence lab or government AI policies in the right ways, and continue to value Agent Foundations style and other off-paradigm technical research approaches.</p>\n\n\n<h4 class=\"wp-block-heading\">Use Your Local Knowledge</h4>\n\n\n<p>I believe that the best places to give are the places where you have local knowledge.</p>\n<p>If you know of people doing great work or who could do great work, based on your own information, then you can fund and provide social proof for what others cannot.</p>\n<p>The less legible to others the cause, and the harder it is to fit it into the mission statements and formulas of various big donors, the more excited you should be to step forward, if the cause is indeed legible to you. This keeps you grounded, helps others find the show (as Tyler Cowen says), is more likely to be counterfactual funding, and avoids information cascades or looking under streetlights for the keys.</p>\n<p>Most importantly it avoids adverse selection. The best legible opportunities for funding, the slam dunk choices? Those are probably getting funded. The legible things that are left are the ones that others didn\u2019t sufficiently fund yet.</p>\n<p>If you know why others haven\u2019t funded, because they don\u2019t know about the opportunity? That\u2019s a great trade.</p>\n\n\n<h4 class=\"wp-block-heading\">Unconditional Grants to Worthy Individuals Are Great</h4>\n\n\n<p>The process of applying for grants, raising money, and justifying your existence sucks.</p>\n<p>A lot.</p>\n<p>It especially sucks for many of the creatives and nerds that do a lot of the best work.</p>\n<p>It also sucks to have to worry about running out of money, or to have to plan your work around the next time you have to justify your existence, or to be unable to be confident in choosing ambitious projects.</p>\n<p>If you have to periodically go through this process, and are forced to continuously worry about making your work legible and how others will judge it, that will substantially hurt your true productivity. At best it is a constant distraction. By default, it is a severe warping effect. A version of this phenomenon is doing huge damage to academic science.</p>\n<p>As I noted in my AI updates, the reason this blog exists is that I received generous, essentially unconditional, anonymous support to \u2018be a public intellectual\u2019 and otherwise pursue whatever I think is best. My benefactors offer their opinions when we talk because I value their opinions, but they never try to influence my decisions, and I feel zero pressure to make my work legible in order to secure future funding.</p>\n<p>If you have money to give, and you know individuals who should clearly be left to do whatever they think is best without worrying about raising or earning money, who you are confident would take advantage of that opportunity and try to do something great, then giving them unconditional grants is a great use of funds, including giving them \u2018don\u2019t worry about reasonable expenses\u2019 levels of funding.</p>\n<p>This is especially true when combined with \u2018retrospective funding,\u2019 based on what they have already done. It would be great if we established a tradition and expectation that people who make big contributions can expect such rewards.</p>\n<p>Not as unconditionally, it\u2019s also great to fund specific actions and projects and so on that you see not happening purely through lack of money, especially when no one is asking you for money.</p>\n<p>This includes things that you want to exist, but that don\u2019t have a path to sustainability or revenue, or would be importantly tainted if they needed to seek that. Fund the project you want to see in the world. This can also be purely selfish, often in order to have something yourself you need to create it for everyone, and if you\u2019re tempted there\u2019s a good chance that\u2019s a great value.</p>\n\n\n<h4 class=\"wp-block-heading\">Do Not Think Only On the Margin, and Also Use Decision Theory</h4>\n\n\n<p>Resist the temptation to think purely on the margin, asking only what one more dollar can do. The incentives get perverse quickly. Organizations are rewarded for putting their highest impact activities in peril. Organizations that can \u2018run lean\u2019 or protect their core activities get punished.</p>\n<p>If you always insist on being a \u2018funder of last resort\u2019 that requires key projects or the whole organization otherwise be in trouble, you\u2019re defecting. Stop it.</p>\n<p>Also, you want to do some amount of retrospective funding. If people have done exceptional work in the past, you should be willing to give them a bunch more rope in the future, above and beyond the expected value of their new project.</p>\n<p>Don\u2019t make everyone constantly reprove their cost effectiveness each year, or at least give them a break. If someone has earned your trust, then if this is the project they want to do next, presume they did so because of reasons, although you are free to disagree with those reasons.</p>\n\n\n<h4 class=\"wp-block-heading\">Compare Notes With Those Individuals You Trust</h4>\n\n\n<p>This especially goes for AI lab employees. There\u2019s no need for everyone to do all of their own research, you can and should compare notes with those who you can trust, and this is especially great when they\u2019re people you know well.</p>\n<p>What I do worry about is too much outsourcing of decisions to larger organizations and institutional structures, including those of Effective Altruism but also others, or letting your money go directly to large foundations where it will often get captured.</p>\n\n\n<h4 class=\"wp-block-heading\">Beware Becoming a Fundraising Target</h4>\n\n\n<p>Jaan Tallinn created SFF in large part to intentionally take his donation decisions out of his hands, so he could credibly tell people those decisions were out of his hands, so he would not have to constantly worry that people he talked to were attempting to fundraise.</p>\n<p>This is a huge deal. Communication, social life and a healthy information environment can all be put in danger by this.</p>\n\n\n<h4 class=\"wp-block-heading\">And the Nominees Are</h4>\n\n\n<p>Time to talk about the organizations themselves.</p>\n<p>Rather than offer precise rankings, I divided by cause category and into three confidence levels.</p>\n<ol>\n<li>High confidence means I have enough information to be confident the organization is at least a good pick.</li>\n<li>Medium or low confidence means exactly that &#8211; I have less confidence that the choice is wise, and you should give more consideration to doing your own research.</li>\n<li>If my last investigation was in 2024, and I haven\u2019t heard anything, I will have somewhat lower confidence now purely because my information is out of date.</li>\n</ol>\n<p>Low confidence is still high praise, and very much a positive assessment! Most organizations would come nowhere close to making the post at all.</p>\n<p>If an organization is not listed, that does not mean I think they would be a bad pick &#8211; they could have asked not to be included, or I could be unaware of them or their value, or I could simply not have enough confidence to list them.</p>\n<p>I know how Bayesian evidence works, but this post is not intended as a knock on anyone, in any way. Some organizations that are not here would doubtless have been included, if I\u2019d had more time.</p>\n<p>I try to give a sense of how much detailed investigation and verification I was able to complete, and what parts I have confidence in versus not. Again, my lack of confidence will often be purely about my lack of time to get that confidence.</p>\n<p>Unless I already knew them from elsewhere, assume no organizations here got as much attention as they deserve before you decide on what for you is a large donation.</p>\n<p>I\u2019m tiering based on how I think about donations from you, from outside SFF.</p>\n<p>I think the regranting organizations were clearly wrong choices from within SFF, but are reasonable picks if you don\u2019t want to do extensive research, especially if you are giving small.</p>\n<p>In terms of funding levels needed, I will similarly divide into three categories.</p>\n<p>They roughly mean this, to the best of my knowledge:</p>\n<p>Low: Could likely be fully funded with less than ~$250k.</p>\n<p>Medium: Could plausibly be fully funded with between ~$250k and ~$2 million.</p>\n<p>High: Could probably make good use of more than ~$2 million.</p>\n<p>These numbers may be obsolete by the time you read this. If you\u2019re giving a large amount relative to what they might need, check with the organization first, but also do not be so afraid of modest amounts of \u2018overfunding\u2019 as relieving fundraising pressure is valuable and as I noted it is important not to only think on the margin.</p>\n<p>A lot of organizations are scaling up rapidly, looking to spend far more money than they have in the past. This was true in 2024, and 2025 has only accelerated this trend. A lot more organizations are in \u2018High\u2019 now but I decided not to update the thresholds.</p>\n<p>Everyone seems eager to double their headcount. I\u2019m not putting people into the High category unless I am confident they can scalably absorb more funding after SFF.</p>\n<p>The person who I list as the leader of an organization will sometimes accidentally be whoever was in charge of fundraising rather than strictly the leader. Partly the reason for listing it is to give context and some of you can go \u2018oh right, I know who that is,\u2019 and the other reason is that all organization names are often highly confusing &#8211; adding the name of the organization\u2019s leader allows you a safety check, to confirm that you are indeed pondering the same organization I am thinking of!</p>\n\n\n<h4 class=\"wp-block-heading\">Organizations that Are Literally Me</h4>\n\n\n<p>This is my post, so I get to list Balsa Research first. (I make the rules here.)</p>\n<p>If that\u2019s not what you\u2019re interested in, you can of course skip the section.</p>\n\n\n<h4 class=\"wp-block-heading\">Balsa Research</h4>\n\n\n<p>Focus: Groundwork starting with studies to allow repeal of the Jones Act</p>\n<p>Leader: Zvi Mowshowitz</p>\n<p>Funding Needed: Medium</p>\n<p>Confidence Level: High</p>\n<p>Our first target continues to be<a href=\"https://thezvi.substack.com/p/repeal-the-jones-act-of-1920\"> the Jones Act</a>. With everything happening in 2025, it is easy to get distracted. We have decided to keep eyes on the prize.</p>\n<p>We\u2019ve commissioned two studies. Part of our plan is to do more of them, and also do things like draft model repeals and explore ways to assemble a coalition and to sell and spread the results, to enable us to have a chance at repeal.</p>\n<p>We also are networking, gathering information, publishing findings where there are <a href=\"https://www.balsaresearch.com/jones-act-vessels\">information holes</a> or where we can offer <a href=\"https://www.balsaresearch.com/jones-act-index\">superior presentations</a>, planning possible collaborations, and responding quickly in case of a crisis in related areas. We believe we meaningfully reduced the probability that certain very damaging additional maritime regulations could have become law,<a href=\"https://thezvi.substack.com/p/balsa-update-springtime-in-dc?utm_source=publication-search\"> as described in this post.</a></p>\n<p>Other planned cause areas include NEPA reform and federal housing policy (to build more housing where people want to live).</p>\n<p>We have one full time worker on the case and are trying out a potential second one.</p>\n<p>I don\u2019t intend to have Balsa work on AI or assist with my other work, or to take personal compensation, unless I get substantially larger donations than we have had previously, that are either dedicated to those purposes or that at least come with the explicit understanding I should consider doing that.</p>\n<p>Further donations would otherwise be for general support.</p>\n<p>The pitch for Balsa, and the reason I am doing it, is in two parts.</p>\n<p>I believe Jones Act repeal and many other abundance agenda items are neglected, tractable and important, and that my way of focusing on what matters can advance them. That the basic work that needs doing is not being done, it would be remarkably cheap to do a lot of it and do it well, and that this would give us a real if unlikely chance to get a huge win if circumstances break right. Chances for progress currently look grim, but winds can change quickly, we need to be ready, and also we need to stand ready to mitigate the chance things get even worse.</p>\n<p>I also believe that if people do not have hope for the future, do not have something to protect and fight for, or do not think good outcomes are possible, that people won\u2019t care about protecting the future. And that would be very bad, because we are going to need to fight to protect our future if we want to have one, or have a good one.</p>\n<p>You got to give them hope.</p>\n<p>I could go on, but I\u2019ll stop there.</p>\n<p>Donate <a href=\"https://www.balsaresearch.com/donate\">here</a>, or get in touch at donations@balsaresearch.com.</p>\n\n\n<h4 class=\"wp-block-heading\">Don\u2019t Worry About the Vase</h4>\n\n\n<p>Focus: Zvi Mowshowitz writes a lot of words, really quite a lot.</p>\n<p>Leader: Zvi Mowshowitz</p>\n<p>Funding Needed: None, but it all helps, could plausibly absorb a lot</p>\n<p>Confidence Level: High</p>\n<p>You can also of course always donate directly to my favorite charity.</p>\n<p>By which I mean me. I always appreciate your support, however large or small.</p>\n<p>The easiest way to help on a small scale (of course) is<a href=\"https://thezvi.substack.com/\"> a Substack</a> subscription or<a href=\"https://www.patreon.com/c/thezvi\"> Patreon</a>. Paid substack subscriptions punch above their weight because they assist with the sorting algorithm, and also for their impact on morale.</p>\n<p>If you want to go large then reach out to me.</p>\n<p>Thanks to generous anonymous donors, I am able to write full time and mostly not worry about money. That is what makes this blog possible.</p>\n<p>I want to as always be 100% clear: I am totally, completely fine as is, as is the blog.</p>\n<p>Please feel zero pressure here, as noted throughout there are many excellent donation opportunities out there.</p>\n<p>Additional funds are still welcome. There are levels of funding beyond not worrying.</p>\n<p>Such additional support is always highly motivating.</p>\n<p>Also there are absolutely additional things I could and would throw money at to improve the blog, potentially including hiring various forms of help or even expanding to more of a full news operation or startup.</p>\n\n\n<h4 class=\"wp-block-heading\">Organizations Focusing On AI Non-Technical Research and Education</h4>\n\n\n<p>As a broad category, these are organizations trying to figure things out regarding AI existential risk, without centrally attempting to either do technical work or directly to influence policy and discourse.</p>\n<p>Lightcone Infrastructure is my current top pick across all categories. If you asked me where to give a dollar, or quite a few dollars, to someone who is not me, I would tell you to fund Lightcone Infrastructure.</p>\n\n\n<h4 class=\"wp-block-heading\">Lightcone Infrastructure</h4>\n\n\n<p>Focus: Rationality community infrastructure,<a href=\"https://www.lesswrong.com/\"> LessWrong</a>, the Alignment Forum and<a href=\"https://www.lighthaven.space/\"> Lighthaven</a>.</p>\n<p>Leaders: Oliver Habryka, Raymond Arnold, Ben Pace</p>\n<p>Funding Needed: High</p>\n<p>Confidence Level: High</p>\n<p>Disclaimer: I am on the CFAR board which used to be the umbrella organization for Lightcone and still has some lingering ties. My writing appears on LessWrong. I have long time relationships with everyone involved. I have been to several reliably great workshops or conferences at their campus at<a href=\"https://www.lighthaven.space/\"> Lighthaven</a>. So I am conflicted here.</p>\n<p>With that said, Lightcone is my clear number one. I think they are doing great work, both in terms of LessWrong and also Lighthaven. There is the potential, with greater funding, to enrich both of these tasks, and also for expansion.</p>\n<p>There is a large force multiplier here (although that is true of a number of other organizations I list as well).</p>\n<p><a href=\"https://www.lesswrong.com/posts/5n2ZQcbc7r4R8mvqc/the-lightcone-is-nothing-without-its-people\">They made their 2024 fundraising pitch here, I encourage reading it</a>.</p>\n<p>Where I am beyond confident is that if LessWrong, the Alignment Forum or the venue<a href=\"https://www.lighthaven.space/\"> Lighthaven</a> were unable to continue, any one of these would be a major, quite bad unforced error.</p>\n<p>LessWrong and the Alignment Forum a central part of the infrastructure of the meaningful internet.</p>\n<p>Lighthaven is miles and miles away the best event venue I have ever seen. I do not know how to convey how much the design contributes to having a valuable conference, designed to facilitate the best kinds of conversations via a wide array of nooks and pathways designed with the principles of Christopher Alexander. This contributes to and takes advantage of the consistently fantastic set of people I encounter there.</p>\n<p>The marginal costs here are large (~$3 million per year, some of which is made up by venue revenue), but the impact here is many times that, and I believe they can take on more than ten times that amount and generate excellent returns.</p>\n<p>If we can go beyond short term funding needs, they can pay off the mortgage to secure a buffer, and buy up surrounding buildings to secure against neighbors (who can, given this is Berkeley, cause a lot of trouble) and to secure more housing and other space. This would secure the future of the space.</p>\n<p>I would love to see them then expand into additional spaces. They note this would also require the right people.</p>\n<p>Donate through <a href=\"https://www.every.org/lightcone-infrastructure?donateTo=lightcone-infrastructure#/donate/\">every.org</a>, or contact <a href=\"mailto:team@lesswrong.com\">team@lesswrong.com</a>.</p>\n\n\n<h4 class=\"wp-block-heading\">The AI Futures Project</h4>\n\n\n<p>Focus: AI forecasting research projects, governance research projects, and policy engagement, in that order.</p>\n<p>Leader: Daniel Kokotajlo, with Eli Lifland</p>\n<p>Funding Needed: None Right Now</p>\n<p>Confidence Level: High</p>\n<p>Of all the \u2018shut up and take my money\u2019 applications in the 2024 round where I didn\u2019t have a conflict of interest, even before I got to participate in their tabletop wargame exercise, I judged this the most \u2018shut up and take my money\u2019-ist. At The Curve, I got to participate in the exercise and participate in discussions around it, I\u2019ve since done several more, and I\u2019m now even more confident this is an excellent pick.</p>\n<p>I continue to think it is a super strong case for retroactive funding as well. Daniel walked away from OpenAI, and what looked to be most of his net worth, to preserve his right to speak up. That led to us finally allowing others at OpenAI to speak up as well.</p>\n<p>This is how he wants to speak up, and try to influence what is to come, based on what he knows. I don\u2019t know if it would have been my move, but the move makes a lot of sense, and it has already paid off big. AI 2027 was read by the Vice President, who took it seriously, along with many others, and greatly informed the conversation. I believe the discourse is much improved as a result, and the possibility space has improved.</p>\n<p>Note that they are comfortably funded through the medium term via private donations and their recent SFF grant.</p>\n<p>Donate through <a href=\"https://www.every.org/ai-futures-project\">every.org</a>, or contact <a href=\"mailto:jonas.vollmer@gmail.com\">Jonas Vollmer</a>.</p>\n\n\n<h4 class=\"wp-block-heading\">Effective Institutions Project (EIP) (For Their Flagship Initiatives)</h4>\n\n\n<p>Focus: AI governance, advisory and research, finding how to change decision points</p>\n<p>Leader: Ian David Moss</p>\n<p>Funding Needed: Medium</p>\n<p>Confidence Level: High</p>\n<p>EIP operates on two tracks. They have their flagship initiatives and attempts to intervene directly. They also serve as donation advisors, which I discuss in that section.</p>\n<p>Their current flagship initiative plans are to focus on the intersection of AI governance and the broader political and economic environment, especially risks of concentration of power and unintentional power shifts from humans to AIs.</p>\n<p>Can they indeed identify ways to target key decision points, and make a big difference? One can look at their track record. I\u2019ve been asked to keep details confidential, but based on my assessment of private information, I confirmed they\u2019ve scored some big wins including that they helped improve safety practices at a major AI lab, and will plausibly continue to be able to have high leverage and punch above their funding weight.<a href=\"https://www.founderspledge.com/research/effective-institutions-project-ai-governance\"> You can read about some of the stuff that they can talk about here</a> in a Founders Pledge write up.</p>\n<p>It seems important that they be able to continue their work on all this.</p>\n<p>I also note that in SFF I allocated less funding to EIP than I would in hindsight have liked to allocate, due to quirks about the way matching funds worked and my attempts to adjust my curves to account for it.</p>\n<p>Donate through <a href=\"https://www.every.org/effective-institutions-project/f/eip-core-funding\">every.org</a>, or contact info@effectiveinstitutionsproject.org.</p>\n\n\n<h4 class=\"wp-block-heading\">Artificial Intelligence Policy Institute (AIPI)</h4>\n\n\n<p>Focus: Primarily polls about AI, also lobbying and preparing for crisis response.</p>\n<p>Leader: Daniel Colson.</p>\n<p>Also Involved: Mark Beall and Daniel Eth</p>\n<p>Funding Needed: High</p>\n<p>Confidence Level: High</p>\n<p>Those polls about how the public thinks about AI, including several from last year around SB 1047 including an adversarial collaboration with Dean Ball?</p>\n<p>Remarkably often, these are the people that did that. Without them, few would be asking those questions. Ensuring that someone is asking is super helpful. With some earlier polls I was a bit worried that the wording was slanted, and that will always be a concern with a motivated pollster, but I think recent polls have been much better at this, and they are as close to neutral as one can reasonably expect.</p>\n<p>There are those who correctly point out that even now in 2025 the public\u2019s opinions are weakly held and low salience, and that all you\u2019re often picking up is \u2018the public does not like AI and it likes regulation.\u2019</p>\n<p>Fair enough. Someone still has to show this, and show it applies here, and put a lie to people claiming the public goes the other way, and measure how things change over time. We need to be on top of what the public is thinking, including to guard against the places it wants to do dumb interventions.</p>\n<p>They don\u2019t only do polling. They also do lobbying and prepare for crisis responses.</p>\n<p>Donate <a href=\"https://theaipi.org/donate/\">here</a>, or use their <a href=\"https://theaipi.org/contact/\">contact form</a> to get in touch.</p>\n\n\n<h4 class=\"wp-block-heading\">AI Lab Watch</h4>\n\n\n<p>Focus: Monitoring the AI safety record and plans of the frontier AI labs</p>\n<p>Leader: Zach Stein-Perlman</p>\n<p>Funding Needed: Low</p>\n<p>Confidence Level: High</p>\n<p>Zach has consistently been one of those on top of the safety and security plans, the model cards and other actions of the major labs, both writing up detailed feedback from a skeptical perspective and also compiling the website and its scores in various domains. Zach is definitely in the \u2018demand high standards that would actually work and treat everything with skepticism\u2019 school of all this, which I feel is appropriate, and I\u2019ve gotten substantial benefit of his work several times.</p>\n<p>However, due to uncertainty about whether this is the best thing for him to work on, and thus not being confident he will have this ball, Zach is not currently accepting funding, but would like people who are interested in donations to contact him via Intercom on the AI Lab Watch website.</p>\n\n\n<h4 class=\"wp-block-heading\">Palisade Research</h4>\n\n\n<p>Focus: AI capabilities demonstrations to inform decision makers on capabilities and loss of control risks</p>\n<p>Leader: Jeffrey Ladish</p>\n<p>Funding Needed: High</p>\n<p>Confidence Level: High</p>\n<p>This is clearly an understudied approach. People need concrete demonstrations. Every time I get to talking with people in national security or otherwise get closer to decision makers who aren\u2019t deeply into AI and in particular into AI safety concerns, you need to be as concrete and specific as possible &#8211; that\u2019s why I wrote<a href=\"https://thezvi.substack.com/p/danger-ai-scientist-danger\"> Danger, AI Scientist, Danger</a> the way I did. We keep getting rather on-the-nose fire alarms, but it would be better if we could get demonstrations even more on the nose, and get them sooner, and in a more accessible way.</p>\n<p>Since last time, I\u2019ve had a chance to see their demonstrations in action several times, and I\u2019ve come away feeling that they have mattered.</p>\n<p>I have confidence that Jeffrey is a good person to continue to put this plan into action.</p>\n<p>To donate,<a href=\"https://www.every.org/palisade-research?viewport=desktop#/donate/card\"> click here</a> or email donate@palisaderesearch.org.</p>\n\n\n<h4 class=\"wp-block-heading\">CivAI</h4>\n\n\n<p>Focus: Visceral demos of AI risks</p>\n<p>Leader: Sid Hiregowdara</p>\n<p>Funding Needed: High</p>\n<p>Confidence Level: Medium</p>\n<p>I was impressed by the demo I was given (so a demo demo?). There\u2019s no question such demos fill a niche and there aren\u2019t many good other candidates for the niche.</p>\n<p>The bear case is that the demos are about near term threats, so does this help with the things that matter? It\u2019s a good question. My presumption is yes, that raising situational awareness about current threats is highly useful. That once people notice that there is danger, that they will ask better questions, and keep going. But I always do worry about drawing eyes to the wrong prize.</p>\n<p>To donate, <a href=\"http://every.org/civai?suggestedAmounts=&amp;theme_color=23527f&amp;method=card%2Cbank%2Cpaypal%2Ccrypto%2Cstocks%2Cdaf&amp;utm_campaign=donate-link#/donate\">click here</a> or email contact@civai.org.</p>\n\n\n<h4 class=\"wp-block-heading\">AI Safety Info (Robert Miles)</h4>\n\n\n<p>Focus: Making YouTube videos about AI safety, starring Rob Miles</p>\n<p>Leader: Rob Miles</p>\n<p>Funding Needed: Low</p>\n<p>Confidence Level: High</p>\n<p>I think these are pretty great videos in general, and given what it costs to produce them we should absolutely be buying their production. If there is a catch, it is that I am very much not the target audience, so you should not rely too much on my judgment of what is and isn\u2019t effective video communication on this front, and you should confirm you like the cost per view.</p>\n<p>To donate, join his<a href=\"https://www.patreon.com/robertskmiles\"> patreon</a> or contact him at robertmilesai@robertskmiles.com.</p>\n\n\n<h4 class=\"wp-block-heading\">Intelligence Rising</h4>\n\n\n<p>Focus: Facilitation of the AI scenario roleplaying exercises including Intelligence Rising</p>\n<p>Leader: Shahar Avin</p>\n<p>Funding Needed: Low</p>\n<p>Confidence Level: High</p>\n<p>I haven\u2019t had the opportunity to play Intelligence Rising, but I have read the rules to it, and heard a number of strong after action reports (AARs). They offered<a href=\"https://arxiv.org/abs/2410.03092\"> this summary of insights in 2024</a>. The game is clearly solid, and it would be good if they continue to offer this experience and if more decision makers play it, in addition to the AI Futures Project TTX.</p>\n<p>To donate, reach out to <em>team@intelligencerising.org.</em></p>\n\n\n<h4 class=\"wp-block-heading\">Convergence Analysis</h4>\n\n\n<p>Focus: A series of sociotechnical reports on<a href=\"https://www.convergenceanalysis.org/programs/scenario-research\"> key AI scenarios</a>,<a href=\"https://www.convergenceanalysis.org/programs/governance-research\"> governance recommendations</a> and conducting<a href=\"https://www.convergenceanalysis.org/programs/ai-awareness\"> AI awareness efforts</a>.</p>\n<p>Leader:<a href=\"https://www.convergenceanalysis.org/team/david-kristofferson\"> David Kristoffersson</a></p>\n<p>Funding Needed: High (combining all tracks)</p>\n<p>Confidence Level: Low</p>\n<p>They do a variety of AI safety related things. Their<a href=\"https://docs.google.com/document/d/1Xrh5qTPOuJQjeO-gCf2S4eB9Nidf2xVw78Tc5ps1rsE/edit?tab=t.0#heading=h.sna1ru7mngie\"> Scenario Planning</a> continues to be what I find most exciting, although I\u2019m also somewhat interested in their modeling cooperation initiative as well. It\u2019s not as neglected as it was a year ago, but we could definitely use more work than we\u2019re getting. For track record you<a href=\"https://www.convergenceanalysis.org/publications/timelines-to-transformative-ai-an-investigation\"> check out</a><a href=\"https://www.convergenceanalysis.org/publications/ai-clarity-an-initial-research-agenda\"> their reports</a> from 2024 in this area, and see if you think that was good work, and the rest of their website has more.</p>\n<p>Their donation page is<a href=\"https://www.convergenceanalysis.org/donate\"> here</a>, or you can contact contact@convergenceanalysis.org.</p>\n\n\n<h4 class=\"wp-block-heading\">IASEAI (International Association for Safe and Ethical Artificial Intelligence)</h4>\n\n\n<p>Focus: Grab bag of AI safety actions, research, policy, community, conferences, standards</p>\n<p>Leader: Mark Nitzberg</p>\n<p>Funding Needed: High</p>\n<p>Confidence Level: Low</p>\n<p>There are some clearly good things within the grab bag, including some good conferences and it seems substantial support for Geoffrey Hinton, but for logistical reasons I didn\u2019t do a close investigation to see if the overall package looked promising. I\u2019m passing the opportunity along.</p>\n<p>Donate <a href=\"https://www.iaseai.org/donate\">here</a>, or contact them at info@iaseai.org.</p>\n\n\n<h4 class=\"wp-block-heading\">The AI Whistleblower Initiative</h4>\n\n\n<p>Focus: Whistleblower advising and resources for those in AI labs warning about catastrophic risks, including via<a href=\"http://third-opinion.org\"> Third Opinion</a>.</p>\n<p>Leader: Larl Koch</p>\n<p>Funding Needed: High</p>\n<p>Confidence Level: Medium</p>\n<p>I\u2019ve given them advice, and at least some amount of such resourcing is obviously highly valuable. We certainly should be funding Third Opinion, so that if someone wants to blow the whistle they can have help doing it. The question is whether if it scales this loses its focus.</p>\n<p>Donate <a href=\"https://aiwi.org/support-us/\">here</a>, or reach out to hello@aiwi.org.</p>\n\n\n<h4 class=\"wp-block-heading\">Organizations Related To Potentially Pausing AI Or Otherwise Having A Strong International AI Treaty</h4>\n\n\n\n<h4 class=\"wp-block-heading\">Pause AI and Pause AI Global</h4>\n\n\n<p>Focus: Advocating for a pause on AI, including via in-person protests</p>\n<p>Leader: Holly Elmore (USA) and Joep Meindertsma (Global)</p>\n<p>Funding Level: Low</p>\n<p>Confidence Level: Medium</p>\n<p>Some people say that those who believe we should pause AI would be better off staying quiet about it, rather than making everyone look foolish.</p>\n<p>I disagree.</p>\n<p>I don\u2019t think pausing right now is a good idea. I think we should be working on the transparency, state capacity, technical ability and diplomatic groundwork to enable a pause in case we need one, but that it is too early to actually try to implement one.</p>\n<p>But I do think that if you believe we should pause? Then you should say that we should pause. I very much appreciate people standing up, entering the arena and saying what they believe in, including quite often in my comments. Let the others mock all they want.</p>\n<p>If you agree with Pause AI that the right move is to Pause AI, and you don\u2019t have strong strategic disagreements with their approach, then you should likely be excited to fund this. If you disagree, you have better options.</p>\n<p>Either way, they are doing what they, given their beliefs, should be doing.</p>\n<p>Donate <a href=\"https://pauseai.info/donate\">here</a>, or reach out to joep@pauseai.info.</p>\n\n\n<h4 class=\"wp-block-heading\">MIRI</h4>\n\n\n<p>Focus: At this point, primarily AI policy advocacy, letting everyone know that <em>If Anyone Builds It, Everyone Dies</em> and all that, plus some research</p>\n<p>Leaders: Malo Bourgon, Eliezer Yudkowsky</p>\n<p>Funding Needed: High</p>\n<p>Confidence Level: High</p>\n<p>MIRI, concluding that it is highly unlikely alignment will make progress rapidly enough otherwise,<a href=\"https://intelligence.org/2024/01/04/miri-2024-mission-and-strategy-update/\"> has shifted its strategy</a> to largely advocate for major governments coming up with an international agreement to halt AI progress and to do communications, although research still looks to be a large portion of the budget, and they have dissolved its agent foundations team. Hence the book.</p>\n<p>That is not a good sign for the world, but it does reflect their beliefs.</p>\n<p>They have accomplished a lot. The book is at least a modest success on its own terms in moving things forward.</p>\n<p>I strongly believe they should be funded to continue to fight for a better future however they think is best, even when I disagree with their approach.</p>\n<p>This is very much a case of \u2018do this if and only if this aligns with your model and preferences.\u2019</p>\n<p>Donate <a href=\"https://intelligence.org/donate/\">here</a>, or reach out to harlan@intelligence.org.</p>\n\n\n<h4 class=\"wp-block-heading\">Existential Risk Observatory</h4>\n\n\n<p>Focus: Pause-relevant research</p>\n<p>Leader: Otto Barten</p>\n<p>Funding Needed: Low</p>\n<p>Confidence Level: Medium</p>\n<p>Mostly this is the personal efforts of Otto Barten, ultimately advocating for a conditional pause. For modest amounts of money, in prior years he\u2019s managed to have a hand in<a href=\"https://www.youtube.com/watch?v=vGQDctxwy2E&amp;list=PLY36Zq4jtCFFd8piJN19URqk82avV-QMT\"> some high profile existential risk events</a> and<a href=\"https://time.com/6258483/uncontrollable-ai-agi-risks/\"> get the first x-risk related post into TIME magazine</a>. He\u2019s now pivoted to pause-relevant research (as in how to implement one via treaties, off switches, evals and threat models).</p>\n<p>The track record and my prior investigation is less relevant now, so I\u2019ve bumped them down to low confidence, but it would definitely be good to have the technical ability to pause and not enough work is being done on that.</p>\n<p>To donate,<a href=\"https://whydonate.com/nl/fundraising/support-existential-risk-observatory\"> click here</a>, or get in touch at info@existentialriskobservatory.org.</p>\n\n\n<h4 class=\"wp-block-heading\">Organizations Focusing Primary On AI Policy and Diplomacy</h4>\n\n\n<p>Some of these organizations also look at bio policy or other factors, but I judge those here as being primarily concerned with AI.</p>\n<p>In this area, I am especially keen to rely on people with good track records, who have shown that they can build and use connections and cause real movement. It\u2019s so hard to tell what is and isn\u2019t effective, otherwise. Often small groups can pack a big punch, if they know where to go, or big ones can be largely wasted &#8211; I think that most think tanks on most topics are mostly wasted even if you believe in their cause.</p>\n\n\n<h4 class=\"wp-block-heading\">Center for AI Safety and the CAIS Action Fund</h4>\n\n\n<p>Focus: AI research, field building and advocacy</p>\n<p>Leaders: Dan Hendrycks</p>\n<p>Funding Needed: High</p>\n<p>Confidence Level: High</p>\n<p>They did the<a href=\"https://www.safe.ai/work/statement-on-ai-risk\"> CAIS Statement on AI Risk</a>, helped SB 1047 get as far as it did, and have improved things in many other ways. Some of these other ways are non-public. Some of those non-public things are things I know about and some aren\u2019t. I will simply say the counterfactual policy world is a lot worse. They\u2019ve clearly been punching well above their weight in the advocacy space. The other arms are no slouch either, lots of great work here. Their meaningful rolodex and degree of access is very strong and comes with important insight into what matters.</p>\n<p>They take a lot of big swings and aren\u2019t afraid of taking risks or looking foolish. I appreciate that, even when a given attempt doesn\u2019t fully work.</p>\n<p>If you want to focus on their policy, then you can fund their 501(c)(4), the Action Fund, since 501c(3)s are limited in how much they can spend on political activities, keeping in mind the tax implications of that. If you don\u2019t face any tax implications I would focus first on the 501(c)(4).</p>\n<p>We should definitely find a way to fund at least their core activities.</p>\n<p>Donate to the <a href=\"https://action.safe.ai/donate\">Action Fund for funding political activities</a>, or the <a href=\"https://safe.ai/donate\">501(c)(3) for research</a>. They can be contacted at contact@safe.ai.</p>\n\n\n<h4 class=\"wp-block-heading\">Foundation for American Innovation (FAI)</h4>\n\n\n<p>Focus: Tech policy research, thought leadership, educational outreach to government, fellowships.</p>\n<p>Leader: Grace Meyer</p>\n<p>Funding Needed: High</p>\n<p>Confidence Level: High</p>\n<p>FAI is centrally about innovation. Innovation is good, actually, in almost all contexts, as is building things and letting people do things.</p>\n<p>AI is where this gets tricky. People \u2018supporting innovation\u2019 are often using that as an argument against all regulation of AI, and indeed I am dismayed to see so many push so hard on this exactly in the one place I think they are deeply wrong, when we could work together on innovation (and abundance) almost anywhere else.</p>\n<p>FAI and resident AI studiers Samuel Hammond and Dean Ball are in an especially tough spot, because they are trying to influence AI policy from the right and not get expelled from that coalition or such spaces. There\u2019s a reason we don\u2019t have good alternative options for this. That requires striking a balance.</p>\n<p>I\u2019ve definitely had my disagreements with Hammond, including<a href=\"https://thezvi.substack.com/p/i-got-95-theses-but-a-glitch-aint\"> strong disagreements with his 95 theses on AI</a> although I agreed far more than I disagreed, and I had many disagreements with his<a href=\"https://www.secondbest.ca/p/ai-and-leviathan-part-i\"> AI and Leviathan</a> as well. He\u2019s talked on the Hill about \u2018open model diplomacy.\u2019</p>\n<p>I\u2019ve certainly had many strong disagreements with Dean Ball as well, both in substance and rhetoric. Sometimes he\u2019s the voice of reason and careful analysis, other times (from my perspective) he can be infuriating, most recently in discussions of the Superintelligence Statement, remarkably often he does some of both in the same post. He was perhaps the most important opposer of SB 1047 and went on to a stint at the White House before joining FAI.</p>\n<p>Yet here is FAI, rather high on the list. They\u2019re a unique opportunity, you go to war with the army you have, and both Ball and Hammond have stuck their neck out in key situations. Hammond came out opposing the moratorium. They\u2019ve been especially strong on compute governance.</p>\n<p>I have private reasons to believe that FAI has been effective and we can expect that to continue, and its other initiatives also mostly seem good. We don\u2019t have to agree on everything else, so long as we all want good things and are trying to figure things out, and I\u2019m confident that is the case here.</p>\n<p>I am especially excited that they can speak to the Republican side of the aisle in the R\u2019s native language, which is difficult for most in this space to do.</p>\n<p>An obvious caveat is that if you are not interested in the non-AI pro-innovation part of the agenda (I certainly approve, but it\u2019s not obviously a high funding priority for most readers) then you\u2019ll want to ensure it goes where you want it.</p>\n<p>To donate,<a href=\"https://www.thefai.org/donate\"> click here</a>, or contact them using the form <a href=\"https://www.thefai.org/contact-us\">here</a>.</p>\n\n\n<h4 class=\"wp-block-heading\">Encode AI (Formerly Encode Justice)</h4>\n\n\n<p>Focus: Youth activism on AI safety issues</p>\n<p>Leader: Sneha Revanur</p>\n<p>Funding Needed: Medium</p>\n<p>Confidence Level: High</p>\n<p>They started out doing quite a lot on a shoestring budget by using volunteers, helping with SB 1047 and in several other places. Now they are turning pro, and would like to not be on a shoestring. I think they have clearly earned that right. The caveat is risk of ideological capture. Youth organizations tend to turn to left wing causes.</p>\n<p>The risk here is that this effectively turns mostly to AI ethics concerns. It\u2019s great that they\u2019re coming at this without having gone through the standard existential risk ecosystem, but that also heightens the ideological risk.</p>\n<p>I continue to believe it is worth the risk.</p>\n<p>To donate, go<a href=\"https://buy.stripe.com/14k8wx33u0j9f2UfYY\"> here</a>. They can be contacted at sneha@encodeai.org.</p>\n\n\n<h4 class=\"wp-block-heading\">The Future Society</h4>\n\n\n<p>Focus: AI governance standards and policy.</p>\n<p>Leader: Nicolas Mo\u00ebs</p>\n<p>Funding Needed: High</p>\n<p>Confidence Level: High</p>\n<p>I\u2019ve seen credible sources saying they do good work, and that they substantially helped orient the EU AI Act to at least care at all about frontier general AI. The EU AI Act was not a good bill, but it could easily have been a far worse one, doing much to hurt AI development while providing almost nothing useful for safety.</p>\n<p>We should do our best to get some positive benefits out of the whole thing. And indeed, they helped substantially improve the EU Code of Practice, which was in hindsight remarkably neglected otherwise.</p>\n<p>They\u2019re also active around the world, including the USA and China.</p>\n<p>Donate <a href=\"https://thefuturesociety.org/donate/\">here</a>, or contact them <a href=\"https://thefuturesociety.org/contact/\">here</a>.</p>\n\n\n<h4 class=\"wp-block-heading\">Safer AI</h4>\n\n\n<p>Focus: Specifications for good AI safety, also directly impacting EU AI policy</p>\n<p>Leader: Henry Papadatos</p>\n<p>Funding Needed: Medium</p>\n<p>Confidence Level: Low</p>\n<p>I\u2019ve been impressed by Simeon and his track record, including here. Simeon is stepping down as leader to start a company, which happened post-SFF, so they would need to be reevaluated in light of this before any substantial donation.</p>\n<p>Donate <a href=\"https://www.safer-ai.org/donate\">here</a>, or contact them at contact@safer-ai.org.</p>\n\n\n<h4 class=\"wp-block-heading\">Institute for AI Policy and Strategy (IAPS)</h4>\n\n\n<p>Focus: Papers and projects for \u2018serious\u2019 government circles, meetings with same, policy research</p>\n<p>Leader: Peter Wildeford</p>\n<p>Funding Needed: Medium</p>\n<p>Confidence Level: High</p>\n<p>I have a lot of respect for Peter Wildeford, and they\u2019ve clearly put in good work and have solid connections down, including on the Republican side where better coverage is badly needed, and the only other solid lead we have is FAI. Peter has also increasingly been doing strong work directly via Substack and Twitter that has been helpful to me and that I can observe directly. They are strong on hardware governance and chips in particular (as is FAI).</p>\n<p>Given their goals and approach, funding from outside the traditional ecosystem sources would be extra helpful, ideally such efforts are fully distinct from OpenPhil.</p>\n<p>With the shifting landscape and what I\u2019ve observed, I\u2019m moving them up to high confidence and priority.</p>\n<p>Donate <a href=\"https://www.iaps.ai/donate\">here</a>, or contact them at jmarron@iaps.ai.</p>\n\n\n<h4 class=\"wp-block-heading\">AI Standards Lab (Holtman Research)</h4>\n\n\n<p>Focus: Accelerating the writing of AI safety standards</p>\n<p>Leaders: Koen Holtman and Chin Ze Shen</p>\n<p>Funding Needed: Medium</p>\n<p>Confidence Level: High</p>\n<p>They help facilitate the writing of AI safety standards, for EU/UK/USA, including on the recent EU Code of Practice. They have successfully gotten some of their work officially incorporated, and another recommender with a standards background was impressed by the work and team.</p>\n<p>This is one of the many things that someone has to do, and where if you step up and do it and no one else does that can go pretty great. Having now been involved in bill minutia myself, I know it is thankless work, and that it can really matter, both for public and private standards, and they plan to pivot somewhat to private standards.</p>\n<p>I\u2019m raising my confidence to high that this is at least a good pick, if you want to fund the writing of standards.</p>\n<p>To donate, go <a href=\"https://give.cornerstone.cc/aistandardslab\">here</a> or reach out to hello@aistandardslab.org.</p>\n\n\n<h4 class=\"wp-block-heading\">Safe AI Forum</h4>\n\n\n<p>Focus: International AI safety conferences</p>\n<p>Leader: Fynn Heide and Sophie Thomson</p>\n<p>Funding Needed: Medium</p>\n<p>Confidence Level: Low</p>\n<p>They run the IDAIS series of conferences, including successful ones involving China. I do wish I had a better model of what makes such a conference actually matter versus not mattering, but these sure seem like they should matter, and certainly well worth their costs to run them.</p>\n<p>To donate, contact them using the form at the bottom of the page <a href=\"https://saif.org/about-and-contact/\">here</a>.</p>\n\n\n<h4 class=\"wp-block-heading\">Center For Long Term Resilience</h4>\n\n\n<p>Focus: UK Policy Think Tank focusing on \u2018extreme AI risk and biorisk policy.\u2019</p>\n<p>Leader: Angus Mercer</p>\n<p>Funding Needed: High</p>\n<p>Confidence Level: Low</p>\n<p>The UK has shown promise in its willingness to shift its AI regulatory focus to frontier models in particular. It is hard to know how much of that shift to attribute to any particular source, or otherwise measure how much impact there has been or might be on final policy.</p>\n<p>They have endorsements of their influence from philosopher Toby Ord, Former Special Adviser to the UK Prime Minister Logan Graham, and Senior Policy Adviser Nitarshan Rajkumar.</p>\n<p>I reached out to a source with experience in the UK government who I trust, and they reported back they are a fan and pointed to some good things they\u2019ve helped with. There was a general consensus that they do good work, and those who investigated where impressed.</p>\n<p>However, I have concerns. Their funding needs are high, and they are competing against many others in the policy space, many of which have very strong cases. I also worry their policy asks are too moderate, which might be an advantage for others.</p>\n<p>My lower confidence this year is a combination of worries about moderate asks, worry about organizational size, and worries about the shift in governments in the UK and the UK\u2019s ability to have real impact elsewhere. But if you buy the central idea of this type of lobbying through the UK and are fine with a large budget, go for it.</p>\n<p>Donate <a href=\"https://www.longtermresilience.org/donate/\">here</a>, or reach out to info@longtermresilience.org.</p>\n\n\n<h4 class=\"wp-block-heading\">Simon Institute for Longterm Governance</h4>\n\n\n<p>Focus: Foundations and demand for international cooperation on AI governance and differential tech development</p>\n<p>Leader: Konrad Seifert and Maxime Stauffer</p>\n<p>Funding Needed: High</p>\n<p>Confidence Level: Low</p>\n<p>As with all things diplomacy, hard to tell the difference between a lot of talk and things that are actually useful. Things often look the same either way for a long time. A lot of their focus is on the UN, so update either way based on how useful you think that approach is, and also that makes it even harder to get a good read.</p>\n<p>They previously had a focus on the Global South and are pivoting to China, which seems like a more important focus.</p>\n<p>To donate, scroll down on <a href=\"https://simoninstitute.ch/support\">this page</a> to access their donation form, or contact them at contact@simoninstitute.ch.</p>\n\n\n<h4 class=\"wp-block-heading\">Legal Advocacy for Safe Science and Technology</h4>\n\n\n<p>Focus: Legal team for lawsuits on catastrophic risk and to defend whistleblowers.</p>\n<p>Leader: Tyler Whitmer</p>\n<p>Funding Needed: Medium</p>\n<p>Confidence Level: Medium</p>\n<p>I wasn\u2019t sure where to put them, but I suppose lawsuits are kind of policy by other means in this context, or close enough?</p>\n<p>I buy the core idea of having a legal team on standby for catastrophic risk related legal action in case things get real quickly is a good idea, and I haven\u2019t heard anyone else propose this, although I do not feel qualified to vet the operation. They were one of the organizers of the <a href=\"http://notforprivategain.org\">NotForPrivateGain.org</a> campaign against the OpenAI restructuring.</p>\n<p>I definitely buy the idea of an AI Safety Whistleblower Defense Fund, which they are also doing. Knowing there will be someone to step up and help if it comes to that changes the dynamics in helpful ways.</p>\n<p>Donors who are interested in making relatively substantial donations or grants should contact tyler@lasst.org, for smaller amounts<a href=\"https://www.every.org/lasst\"> click here.</a></p>\n\n\n<h4 class=\"wp-block-heading\">Institute for Law and AI</h4>\n\n\n<p>Focus: Legal research on US/EU law on transformational AI, fellowships, talent</p>\n<p>Leader: Moritz von Knebel</p>\n<p>Involved: Gabe Weil</p>\n<p>Funding Needed: High</p>\n<p>Confidence Level: Low</p>\n<p>I\u2019m confident that they should be funded at all, the question is if this should be scaled up quite a lot, and what aspects of this would scale in what ways. If you can be convinced that the scaling plans are worthwhile this could justify a sizable donation.</p>\n<p>Donate <a href=\"https://www.every.org/law-ai\">here</a>, or contact them at hello@law-ai.org.</p>\n\n\n<h4 class=\"wp-block-heading\">Macrostrategy Research Institute</h4>\n\n\n<p>Focus: Amplify<a href=\"https://nickbostrom.com/\"> Nick Bostrom</a></p>\n<p>Leader: Toby Newberry</p>\n<p>Funding Needed: High</p>\n<p>Confidence Level: Low</p>\n<p>If you think Nick Bostrom is doing great work and want him to be more effective, then this is a way to amplify that work. In general, \u2018give top people support systems\u2019 seems like a good idea that is underexplored.</p>\n<p>Get in touch at toby.newberry@gmail.com.</p>\n\n\n<h4 class=\"wp-block-heading\">Secure AI Project</h4>\n\n\n<p>Focus: Advocacy for public safety and security protocols (SSPs) and related precautions</p>\n<p>Leader: Nick Beckstead</p>\n<p>Funding Needed: High</p>\n<p>Confidence Level: High</p>\n<p>I\u2019ve had the opportunity to consult and collaborate with them and I\u2019ve been consistently impressed. They\u2019re the real deal, they pay attention to detail and care about making it work for everyone, and they\u2019ve got results. I\u2019m a big fan.</p>\n<p>Donate <a href=\"https://donorbox.org/secure-ai-project-general-donations\">here</a>, or contact them at info@secureaiproject.org.</p>\n\n\n<h4 class=\"wp-block-heading\">Organizations Doing ML Alignment Research</h4>\n\n\n<p>This category should be self-explanatory. Unfortunately, a lot of good alignment work still requires charitable funding. The good news is that (even more than last year when I wrote the rest of this introduction) there is a lot more funding, and willingness to fund, than there used to be, and also the projects generally look more promising.</p>\n<p>The great thing about interpretability is that you can be confident you are dealing with something real. The not as great thing is that this can draw too much attention to interpretability, and that you can fool yourself into thinking that All You Need is Interpretability.</p>\n<p>The good news is that several solid places can clearly take large checks.</p>\n<p>I didn\u2019t investigate too deeply on top of my existing knowledge here in 2024, because at SFF I had limited funds and decided that direct research support wasn\u2019t a high enough priority, partly due to it being sufficiently legible.</p>\n<p>We should be able to find money previously on the sidelines eager to take on many of these opportunities. Lab employees are especially well positioned, due to their experience and technical knowledge and connections, to evaluate such opportunities, and also to provide help with access and spreading the word.</p>\n\n\n<h4 class=\"wp-block-heading\">Model Evaluation and Threat Research (METR)</h4>\n\n\n<p>Formerly ARC Evaluations.</p>\n<p>Focus: Model evaluations</p>\n<p>Leaders: Beth Barnes, Chris Painter</p>\n<p>Funding Needed: High</p>\n<p>Confidence Level: High</p>\n<p>Originally I wrote that we hoped to be able to get large funding for METR via non-traditional sources. That happened last year, and METR got major funding. That\u2019s great news. Alas, they once again have to hit the fundraising trail.</p>\n<p>METR has proven to be the gold standard for outside evaluations of potentially dangerous frontier model capabilities, and has proven its value even more so in 2025.</p>\n<p>We very much need these outside evaluations, and to give the labs every reason to use them and no excuse not to use them, and their information has been invaluable. In an ideal world the labs would be fully funding METR, but they\u2019re not.</p>\n<p>So this becomes a place where we can confidently invest quite a bit of capital, make a legible case for why it is a good idea, and know it will probably be well spent.</p>\n<p>If you can direct fully \u2018square\u2019 \u2018outside\u2019 funds that need somewhere legible to go and are looking to go large? I love METR for that.</p>\n<p>To donate,<a href=\"https://metr.org/donate\"> click here.</a> They can be contacted at info@metr.org.</p>\n\n\n<h4 class=\"wp-block-heading\">Alignment Research Center (ARC)</h4>\n\n\n<p>Focus: Theoretically motivated alignment work</p>\n<p>Leader: Jacob Hilton</p>\n<p>Funding Needed: Medium</p>\n<p>Confidence Level: High</p>\n<p>There\u2019s a long track record of good work here, and Paul Christiano remained excited as of 2024. If you are looking to fund straight up alignment work and don\u2019t have a particular person or small group in mind, this is certainly a safe bet to put additional funds to good use and attract good talent.</p>\n<p>Donate <a href=\"https://www.alignment.org/donate/\">here</a>, or reach out to jacob@alignment.org.</p>\n\n\n<h4 class=\"wp-block-heading\">Apollo Research</h4>\n\n\n<p>Focus: Scheming, evaluations, and governance</p>\n<p>Leader: Marius Hobbhahn</p>\n<p>Funding Needed: Medium</p>\n<p>Confidence Level: High</p>\n<p>This is an excellent thing to focus on, and one of the places we are most likely to be able to show \u2018fire alarms\u2019 that make people sit up and notice.<a href=\"https://www.apolloresearch.ai/blog/the-first-year-of-apollo-research\"> Their first year seems to have gone well</a>, one example would be their presentation at the UK safety summit that<a href=\"https://arxiv.org/abs/2311.07590\"> LLMs can strategically deceive their primary users when put under pressure</a>. They will need serious funding to fully do the job in front of them, hopefully like METR they can be helped by the task being highly legible.</p>\n<p>They suggest looking at this paper, and also<a href=\"https://arxiv.org/abs/2504.12170\"> this one</a>. I can verify that they are the real deal and doing the work.</p>\n<p>To donate, reach out to info@apolloresearch.ai.</p>\n\n\n<h4 class=\"wp-block-heading\">Cybersecurity Lab at University of Louisville</h4>\n\n\n<p>Focus: Support for Roman Yampolskiy\u2019s lab and work</p>\n<p>Leader: Roman Yampolskiy</p>\n<p>Funding Needed: Low</p>\n<p>Confidence Level: High</p>\n<p>Roman Yampolskiy is the most pessimistic known voice about our chances of not dying from AI, and got that perspective on major platforms like Joe Rogan and Lex Fridman. He\u2019s working on a book and wants to support PhD students.</p>\n<p>Supporters can make a tax detectable gift to <a href=\"https://engineering.louisville.edu/give/\">the University</a>, specifying that they intend to fund Roman Yampolskiy and the Cyber Security lab.</p>\n\n\n<h4 class=\"wp-block-heading\">Timaeus</h4>\n\n\n<p>Focus: Interpretability research</p>\n<p>Leader:Jesse Hoogland, Daniel Murfet, Stan van Wingerden</p>\n<p>Funding Needed: High</p>\n<p>Confidence Level: High</p>\n<p>Timaeus focuses on interpretability work and sharing their results. The set of advisors is excellent, including Davidad and Evan Hubinger. Evan, John Wentworth and Vanessa Kosoy have offered high praise, and there is evidence they have impacted top lab research agendas. They\u2019re done what I think is solid work, although I am not so great at evaluating papers directly.</p>\n<p>If you\u2019re interested in directly funding interpretability research, that all makes this seem like a slam dunk. I\u2019ve confirmed that this all continues to hold true in 2025.</p>\n<p>To donate, get in touch with Jesse at jesse@timaeus.co. If this is the sort of work that you\u2019re interested in doing, they also have a discord at<a href=\"http://devinterp.com/discord\"> http://devinterp.com/discord</a>.</p>\n\n\n<h4 class=\"wp-block-heading\">Simplex</h4>\n\n\n<p>Focus: Mechanistic interpretability of how inference breaks down</p>\n<p>Leader: Paul Riechers and Adam Shai</p>\n<p>Funding Needed: Medium</p>\n<p>Confidence Level: High</p>\n<p>I am not as high on them as I am on Timaeus, but they have given reliable indicators that they will do good interpretability work. I\u2019d (still) feel comfortable backing them.</p>\n<p>Donate <a href=\"https://www.simplexaisafety.com/donate\">here</a>, or contact them via <a href=\"https://www.simplexaisafety.com/contact\">webform</a>.</p>\n\n\n<h4 class=\"wp-block-heading\">Far AI</h4>\n\n\n<p>Focus: Interpretability and other alignment research, incubator, hits based approach</p>\n<p>Leader: Adam Gleave</p>\n<p>Funding Needed: High</p>\n<p>Confidence Level: Medium</p>\n<p>They take the hits based approach to research, which is correct. I\u2019ve gotten confirmation that they\u2019re doing the real thing here. In an ideal world everyone doing the real thing would get supported, and they\u2019re definitely still funding constrained.</p>\n<p>To donate,<a href=\"https://www.far.ai/donate\"> click here</a>. They can be contacted at hello@far.ai.</p>\n\n\n<h4 class=\"wp-block-heading\">Alignment in Complex Systems Research Group</h4>\n\n\n<p>Focus: AI alignment research on hierarchical agents and multi-system interactions</p>\n<p>Leader: Jan Kulveit</p>\n<p>Funding Needed: Medium</p>\n<p>Confidence Level: High</p>\n<p>I liked ACS last year, and since then we\u2019ve seen Gradual Disempowerment and other good work, which means this now falls into the category \u2018this having funding problems would be an obvious mistake.\u2019 I ranked them very highly in SFF, and there should be a bunch more funding room.</p>\n<p>To donate, reach out to hello@epistea.org, and note that you are interested in donating to ACS specifically.</p>\n\n\n<h4 class=\"wp-block-heading\">Apart Research</h4>\n\n\n<p>Focus: AI safety hackathons, MATS-style programs and AI safety horizon scanning.</p>\n<p>Leaders: Esben Kran, Jason Schreiber</p>\n<p>Funding Needed: Medium</p>\n<p>Confidence Level: Low</p>\n<p>I\u2019m (still) confident in their execution of the hackathon idea, which was the central pitch at SFF although they inform me generally they\u2019re more centrally into the MATS-style programs. My doubt for the hackathons is on the level of \u2018is AI safety something that benefits from hackathons.\u2019 Is this something one can, as it were, hack together usefully? Are the hackathons doing good counterfactual work? Or is this a way to flood the zone with more variations on the same ideas?</p>\n<p>As with many orgs on the list, this one makes sense if and only if you buy the plan, and is one of those \u2018I\u2019m not excited but can see it being a good fit for someone else.\u2019</p>\n<p><a href=\"https://apartresearch.com/donate\">To donate, click here.</a> They can be reached at hello@apartresearch.com.</p>\n\n\n<h4 class=\"wp-block-heading\">Transluce</h4>\n\n\n<p>Focus: Specialized superhuman systems for understanding and overseeing AI</p>\n<p>Leaders: Jacob Steinhardt, Sarah Schwettmann</p>\n<p>Funding Needed: High</p>\n<p>Confidence Level: Medium</p>\n<p>Last year they were a new org. Now they have now grown to 14 people and now have a solid track record and want to keep growing. I have confirmation the team is credible. The plan for scaling themselves is highly ambitious, with planned scale well beyond what SFF can fund. I haven\u2019t done anything like the investigation into their plans and capabilities you would need before placing a bet that big, as AI research of all kinds gets expensive quickly.</p>\n<p>If there is sufficient appetite to scale the amount of privately funded direct work of this type, then this seems like a fine place to look. I am optimistic on them finding interesting things, although on a technical level I am skeptical of the larger plan.</p>\n<p>To donate, reach out to info@transluce.org.</p>\n\n\n<h4 class=\"wp-block-heading\">Organizations Doing Other Technical Work</h4>\n\n\n\n<h4 class=\"wp-block-heading\">AI Analysts @ RAND</h4>\n\n\n<p>Focus: Developing \u2018AI analysts\u2019 that can assist policy makers.</p>\n<p>Leaders: John Coughlan</p>\n<p>Funding Needed: High</p>\n<p>Confidence Level: Medium</p>\n<p>This is a thing that RAND should be doing and that should exist. There are obvious dangers here, but I don\u2019t think this makes them substantially worse and I do think this can potentially improve policy a lot. RAND is well placed to get the resulting models to be actually used. That would enhance state capacity, potentially quite a bit.</p>\n<p>The problem is that doing this is not cheap, and while funding this shouldn\u2019t fall to those reading this, it plausibly does. This could be a good place to consider sinking quite a large check, if you believe in the agenda.</p>\n<p>Donate <a href=\"https://www.rand.org/support-us/rfi.html\">here</a>.</p>\n\n\n<h4 class=\"wp-block-heading\">Organizations Doing Math, Decision Theory and Agent Foundations</h4>\n\n\n<p>Right now it looks likely that AGI will be based around large language models (LLMs). That doesn\u2019t mean this is inevitable. I would like our chances better if we could base our ultimate AIs around a different architecture, one that was more compatible with being able to get it to do what we would like it to do.</p>\n<p>One path for this is agent foundations, which involves solving math to make the programs work instead of relying on inscrutable giant matrices.</p>\n<p>Even if we do not manage that, decision theory and game theory are potentially important for navigating the critical period in front of us, for life in general, and for figuring out what the post-transformation AI world might look like, and thus what choice we make now might do to impact that.</p>\n<p>There are not that many people working on these problems. Actual Progress would be super valuable. So even if we expect the median outcome does not involve enough progress to matter, I think it\u2019s still worth taking a shot.</p>\n<p>The flip side is you worry about people \u2018doing decision theory into the void\u2019 where no one reads their papers or changes their actions. That\u2019s a real issue. As is the increased urgency of other options. Still, I think these efforts are worth supporting, in general.</p>\n\n\n<h4 class=\"wp-block-heading\">Orthogonal</h4>\n\n\n<p>Focus: AI alignment via agent foundations</p>\n<p>Leaders: Tamsin Leake</p>\n<p>Funding Needed: Medium</p>\n<p>Confidence Level: High</p>\n<p>I have funded Orthogonal in the past. They are definitely doing the kind of work that, if it succeeded, might actually amount to something, and would help us get through this to a future world we care about. It\u2019s a long shot, but a long shot worth trying. They very much have the \u2018old school\u2019 Yudkowsky view that relatively hard takeoff is likely and most alignment approaches are fools errands. My sources are not as enthusiastic as they once were, but there are only a handful of groups trying that have any chance at all, and this still seems like one of them.</p>\n<p>Donate <a href=\"https://www.every.org/orthogonal?donateTo=orthogonal#/donate/card\">here</a>, or get in touch at tammy@orxl.org.</p>\n\n\n<h4 class=\"wp-block-heading\">Topos Institute</h4>\n\n\n<p>Focus: Math for AI alignment</p>\n<p>Leaders: Brendan Fong and David Spivak.</p>\n<p>Funding Needed: High</p>\n<p>Confidence Level: High</p>\n<p>Topos is essentially Doing Math to try and figure out what to do about AI and AI Alignment. I\u2019m very confident that they are qualified to (and actually will) turn donated money (partly via coffee) into math, in ways that might help a lot. I am also confident that the world should allow them to attempt this.</p>\n<p>They\u2019re now working with ARIA. That seems great.</p>\n<p>Ultimately it all likely amounts to nothing, but the upside potential is high and the downside seems very low. I\u2019ve helped fund them in the past and am happy about that.</p>\n<p>To donate, go<a href=\"https://topos.institute/contact\"> here</a>, or get in touch at info@topos.institute.</p>\n\n\n<h4 class=\"wp-block-heading\">Eisenstat Research</h4>\n\n\n<p>Focus: Two people doing research at MIRI, in particular Sam Eisenstat</p>\n<p>Leader: Sam Eisenstat</p>\n<p>Funding Needed: Medium</p>\n<p>Confidence Level: High</p>\n<p>Given Sam Eisenstat\u2019s previous work, including from 2025, it seems worth continuing to support him, including supporting researchers. I still believe in this stuff being worth working on, obviously only support if you do as well. He\u2019s funded for now but that\u2019s still only limited runway.</p>\n<p>To donate, contact sam@intelligence.org.</p>\n\n\n<h4 class=\"wp-block-heading\">AFFINE Algorithm Design</h4>\n\n\n<p>Focus: Johannes Mayer does agent foundations work</p>\n<p>Leader: Johannes Mayer</p>\n<p>Funding Needed: Low</p>\n<p>Confidence Level: Medium</p>\n<p>Johannes Mayer does solid agent foundations work, and more funding would allow him to hire more help.</p>\n<p>To donate, contact j.c.mayer240@gmail.com.</p>\n\n\n<h4 class=\"wp-block-heading\">CORAL (Computational Rational Agents Laboratory)</h4>\n\n\n<p>Focus: Examining intelligence</p>\n<p>Leader: Vanessa Kosoy</p>\n<p>Funding Needed: Medium</p>\n<p>Confidence Level: High</p>\n<p>This is Vanessa Kosoy and Alex Appel, who have another research agenda formerly funded by MIRI that now needs to stand on its own after their refocus. I once again believe this work to be worth continuing even if the progress isn\u2019t what one might hope. I wish I had the kind of time it takes to actually dive into these sorts of theoretical questions, but alas I do not, or at least I\u2019ve made a triage decision not to.</p>\n<p><a href=\"https://www.every.org/coral-research#/donate/card\">To donate, click here</a>. For larger amounts contact directly at vanessa@alter.org.il</p>\n\n\n<h4 class=\"wp-block-heading\">Mathematical Metaphysics Institute</h4>\n\n\n<p>Focus: Searching for a mathematical basis for metaethics.</p>\n<p>Leader: Alex Zhu</p>\n<p>Funding Needed: Low</p>\n<p>Confidence Level: Low</p>\n<p>Alex Zhu has run iterations of the Math &amp; Metaphysics Symposia, which had some excellent people in attendance, and intends partly to do more things of that nature. He thinks eastern philosophy contains much wisdom relevant to developing a future \u2018decision-theoretic basis of metaethics\u2019 and plans on an 8+ year project to do that.</p>\n<p>I\u2019ve seen plenty of signs that the whole thing is rather bonkers, but also strong endorsements from a bunch of people I trust that there is good stuff here, and the kind of crazy that is sometimes crazy enough to work. So there\u2019s a lot of upside. If you think this kind of approach has a chance of working, this could be very exciting. For additional information, you can see <a href=\"https://docs.google.com/document/d/1IH69QAU5T6DcBt7gAvtCyDqhMGSuCnvjSrYBFB-1iL0/edit?usp=sharing\">this google doc</a>.</p>\n<p>To donate, message Alex at alex@mathematicalmetaphysics.org.</p>\n\n\n<h4 class=\"wp-block-heading\">Focal at CMU</h4>\n\n\n<p>Focus: Game theory for cooperation by autonomous AI agents</p>\n<p>Leader: Vincent Conitzer</p>\n<p>Funding Needed: Medium</p>\n<p>Confidence Level: Low</p>\n<p>This is an area MIRI and the old rationalist crowd thought about a lot back in the day. There are a lot of ways for advanced intelligences to cooperate that are not available to humans, especially if they are capable of doing things in the class of sharing source code or can show their decisions are correlated with each other.</p>\n<p>With sufficient capability, any group of agents should be able to act as if it is a single agent, and we shouldn\u2019t need to do the game theory for them in advance either. I think it\u2019s good things to be considering, but one should worry that even if they do find answers it will be \u2018into the void\u2019 and not accomplish anything. Based on my technical analysis I wasn\u2019t convinced Focal was going to sufficiently interesting places with it, but I\u2019m not at all confident in that assessment.</p>\n<p>They note they\u2019re also interested in the dynamics prior to Ai becoming superintelligent, as the initial conditions plausibly matter a lot.</p>\n<p>To donate, reach out to Vincent directly at conitzer@cs.cmu.edu to be guided through the donation process.</p>\n\n\n<h4 class=\"wp-block-heading\">Organizations Doing Cool Other Stuff Including Tech</h4>\n\n\n<p>This section is the most fun. You get unique projects taking big swings.</p>\n\n\n<h4 class=\"wp-block-heading\">ALLFED</h4>\n\n\n<p>Focus: Feeding people with resilient foods after a potential nuclear war</p>\n<p>Leaders: David Denkenberger</p>\n<p>Funding Needed: High</p>\n<p>Confidence Level: Medium</p>\n<p>As far as I know, no one else is doing the work ALLFED is doing. A resilient food supply ready to go in the wake of a nuclear war (or other major disaster with similar dynamics) could be everything. There\u2019s a small but real chance that the impact is enormous. In my 2021 SFF round, I went back and forth with them several times over various issues, ultimately funding them, <a href=\"https://thezvi.substack.com/i/45469698/nuclear-war\">you can read about those details here</a>.</p>\n<p>I think all of the concerns and unknowns from last time essentially still hold, as does the upside case, so it\u2019s a question of prioritization, how likely you view nuclear war scenarios and how much promise you see in the tech.</p>\n<p>If you are convinced by the viability of the tech and ability to execute, then there\u2019s a strong case that this is a very good use of funds.</p>\n<p>I think that this is a relatively better choice if you expect AI to remain a normal technology for a while or if your model of AI risks includes a large chance of leading to a nuclear war or other cascading impacts to human survival, versus if you don\u2019t think this.</p>\n<p>Research and investigation on the technical details seems valuable here. If we do have a viable path to alternative foods and don\u2019t fund it, that\u2019s a pretty large miss, and I find it highly plausible that this could be super doable and yet not otherwise done.</p>\n<p>Donate <a href=\"https://allfed.info/donate\">here</a>, or reach out to info@allfed.info.</p>\n\n\n<h4 class=\"wp-block-heading\">Good Ancestor Foundation</h4>\n\n\n<p>Focus: Collaborations for tools to increase civilizational robustness to catastrophes</p>\n<p>Leader: Colby Thompson</p>\n<p>Funding Needed: High</p>\n<p>Confident Level: High</p>\n<p>The principle of \u2018a little preparation now can make a huge difference to resilience and robustness in a disaster later, so it\u2019s worth doing even if the disaster is not so likely\u2019 generalizes. Thus, the Good Ancestor Foundation, targeting nuclear war, solar flares, internet and cyber outages, and some AI scenarios and safety work.</p>\n<p>A particular focus is archiving data and tools, enhancing synchronization systems and designing a novel emergency satellite system (first one goes up in June) to help with coordination in the face of disasters. They\u2019re also coordinating on hardening critical infrastructure and addressing geopolitical and human rights concerns.</p>\n<p>They\u2019ve also given out millions in regrants.</p>\n<p>One way I know they make good decisions is they continue to help facilitate the funding for my work, and make that process easy. They have my sincerest thanks. Which also means there is a conflict of interest, so take that into account.</p>\n<p>Donate <a href=\"https://goodancestor.com/donations/\">here</a>, or contact them at good@goodancestor.com.</p>\n\n\n<h4 class=\"wp-block-heading\">Charter Cities Institute</h4>\n\n\n<p>Focus: Building charter cities</p>\n<p>Leader: Kurtis Lockhart</p>\n<p>Funding Needed: Medium</p>\n<p>Confidence Level: Medium</p>\n<p>I do love charter cities. There is little question they are attempting to do a very good thing and are sincerely going to attempt to build a charter city in Africa, where such things are badly needed. Very much another case of it being great that someone is attempting to do this so people can enjoy better institutions, even if it\u2019s not the version of it I would prefer that would focus on regulatory arbitrage more.</p>\n<p>Seems like a great place for people who don\u2019t think transformational AI is on its way but do understand the value here.</p>\n<p>Donate to them <a href=\"https://chartercitiesinstitute.org/donate-to-cci/\">here</a>, or contact them via <a href=\"https://chartercitiesinstitute.org/contact/\">webform</a>.</p>\n\n\n<h4 class=\"wp-block-heading\">Carbon Copies for Independent Minds</h4>\n\n\n<p>Focus: Whole brain emulation</p>\n<p>Leader: Randal Koene</p>\n<p>Funding Needed: Medium</p>\n<p>Confidence Level: Low</p>\n<p>At this point, if it worked in time to matter, I would be willing to roll the dice on emulations. What I don\u2019t have is much belief that it will work, or the time to do a detailed investigation into the science. So flagging here, because if you look into the science and you think there is a decent chance, this becomes a good thing to fund.</p>\n<p>Donate <a href=\"https://carboncopies.org/Donate/\">here</a>, or contact them at contact@carboncopies.org.</p>\n\n\n<h4 class=\"wp-block-heading\">Organizations Focused Primarily on Bio Risk</h4>\n\n\n\n<h4 class=\"wp-block-heading\">Secure DNA</h4>\n\n\n<p>Focus: Scanning DNA synthesis for potential hazards</p>\n<p>Leader: Kevin Esvelt, Andrew Yao and Raphael Egger</p>\n<p>Funding Needed: Medium</p>\n<p>Confidence Level: Medium</p>\n<p>It is certainly an excellent idea. Give everyone fast, free, cryptographically screening of potential DNA synthesis to ensure no one is trying to create something we do not want anyone to create. AI only makes this concern more urgent. I didn\u2019t have time to investigate and confirm this is the real deal as I had other priorities even if it was, but certainly someone should be doing this.</p>\n<p>There is also another related effort, Secure Bio, if you want to go all out. I would fund Secure DNA first.</p>\n<p>To donate, contact them at contact@securedna.org.</p>\n\n\n<h4 class=\"wp-block-heading\">Blueprint Biosecurity</h4>\n\n\n<p>Focus: Increasing capability to respond to future pandemics, Next-gen PPE, Far-UVC.</p>\n<p>Leader: Jake Swett</p>\n<p>Funding Needed: Medium</p>\n<p>Confidence Level: Medium</p>\n<p>There is no question we should be spending vastly more on pandemic preparedness, including far more on developing and stockpiling superior PPE and in Far-UVC. It is rather a shameful that we are not doing that, and Blueprint Biosecurity plausibly can move substantial additional investment there. I\u2019m definitely all for that.</p>\n<p>To donate, reach out to donations@blueprintbiosecurity.org or head to the<a href=\"https://www.paypal.com/us/fundraiser/charity/4967107\"> Blueprint Bio PayPal Giving Fund</a>.</p>\n\n\n<h4 class=\"wp-block-heading\">Pour Domain</h4>\n\n\n<p>Focus: EU policy for AI enabled biorisks, among other things.</p>\n<p>Leader: Patrick Stadler</p>\n<p>Funding Needed: Low</p>\n<p>Confidence Level: Low</p>\n<p>Everything individually looks worthwhile but also rather scattershot. Then again, who am I to complain about a campaign for e.g. improved air quality? My worry is still that this is a small operation trying to do far too much, some of it that I wouldn\u2019t rank too high as a priority, and it needs more focus, on top of not having that clear big win yet. They are a French nonprofit.</p>\n<p>Donation details are at the very bottom of<a href=\"https://www.pourdemain.ngo/en/uber-uns\"> this page</a>, or you can contact them at info@pourdemain.ngo.</p>\n\n\n<h4 class=\"wp-block-heading\">ALTER Israel</h4>\n\n\n<p>Focus: AI safety and biorisk for Israel</p>\n<p>Leader: David Manheim</p>\n<p>Funding Needed: Low</p>\n<p>Confidence Level: Medium</p>\n<p>Israel has Ilya\u2019s company SSI (Safe Superintelligence) and otherwise often punches above its weight in such matters but is getting little attention. This isn\u2019t where my attention is focused but David is presumably choosing this focus for good reason.</p>\n<p>To support them, get in touch at contact@alter.org.il.</p>\n\n\n<h4 class=\"wp-block-heading\">Organizations That Can Advise You Further</h4>\n\n\n<p>The first best solution, as I note above, is to do your own research, form your own priorities and make your own decisions. This is especially true if you can find otherwise illegible or hard-to-fund prospects.</p>\n<p>However, your time is valuable and limited, and others can be in better positions to advise you on key information and find opportunities.</p>\n<p>Another approach to this problem, if you have limited time or actively want to not be in control of these decisions, is to give to regranting organizations, and take the decisions further out of your own hands.</p>\n\n\n<h4 class=\"wp-block-heading\">Effective Institutions Project (EIP) (As A Donation Advisor)</h4>\n\n\n<p>Focus: AI governance, advisory and research, finding how to change decision points</p>\n<p>Leader: Ian David Moss</p>\n<p>Confidence Level: High</p>\n<p>I discussed their direct initiatives earlier. This is listing them as a donation advisor and in their capacity of attempting to be a resource to the broader philanthropic community.</p>\n<p>They report that they are advising multiple major donors, and would welcome the opportunity to advise additional major donors. I haven\u2019t had the opportunity to review their donation advisory work, but what I have seen in other areas gives me confidence. They specialize in advising donors who have brad interests across multiple areas, and they list AI safety, global health, democracy and (peace and security).</p>\n<p>To donate,<a href=\"https://www.every.org/effective-institutions-project/f/eip-core-funding\"> click here</a>. If you have further questions or would like to be advised, contact them at info@effectiveinstitutionsproject.org.</p>\n\n\n<h4 class=\"wp-block-heading\">Longview Philanthropy</h4>\n\n\n<p>Focus: Conferences and advice on x-risk for those giving &gt;$1 million per year</p>\n<p>Leader: Simran Dhaliwal</p>\n<p>Funding Needed: None</p>\n<p>Confidence Level: Low</p>\n<p>Longview is not seeking funding, instead they are offering support to large donors, and you can give to their regranting funds, including the Emerging Challenges Fund on catastrophic risks from emerging tech, which focuses non-exclusively on AI.</p>\n<p>I had a chance to hear a pitch for them at The Curve and check out their current analysis and donation portfolio. It was a good discussion. There were definitely some areas of disagreement in both decisions and overall philosophy, and I worry they\u2019ll be too drawn to the central and legible (a common issue with such services).</p>\n<p>On the plus side, they\u2019re clearly trying, and their portfolio definitely had some good things in it. So I wouldn\u2019t want to depend on them or use them as a sole source if I had the opportunity to do something higher effort, but if I was donating on my own I\u2019d find their analysis useful. If you\u2019re considering relying heavily on them or donating to the funds, I\u2019d look at the fund portfolios in detail and see what you think.</p>\n<p>I pointed them to some organizations they hadn\u2019t had a chance to evaluate yet.</p>\n<p>They clearly seem open to donations aimed at particular RFPs or goals.</p>\n<p>To inquire about their services, contact them at <a href=\"mailto:info@longview.org\">info@longview.org</a>.</p>\n\n\n<h4 class=\"wp-block-heading\">Organizations That then Regrant to Fund Other Organizations</h4>\n\n\n<p>There were lots of great opportunities in SFF in both of my recent rounds. I was going to have an embarrassment of riches I was excited to fund.</p>\n<p>Thus I decided quickly that I would not be funding any regrating organizations. If you were in the business of taking in money and then shipping it out to worthy causes, well, I could ship directly to highly worthy causes.</p>\n<p>So there was no need to have someone else do that, or expect them to do better.</p>\n<p>That does not mean that others should not consider such donations.</p>\n<p>I see three important advantages to this path.</p>\n<ol>\n<li>Regranters can offer smaller grants that are well-targeted.</li>\n<li>Regranters save you a lot of time.</li>\n<li>Regranters avoid having others try to pitch on donations.</li>\n</ol>\n<p>Thus, if you are making a \u2018low effort\u2019 donation, and think others you trust that share your values to invest more effort, it makes more sense to consider regranters.</p>\n<p>In particular, if you\u2019re looking to go large, I\u2019ve been impressed by SFF itself, and there\u2019s room for SFF to scale both its amounts distributed and level of rigor.</p>\n\n\n<h4 class=\"wp-block-heading\">SFF Itself (!)</h4>\n\n\n<p>Focus: Give out grants based on recommenders, primarily to 501c(3) organizations</p>\n<p>Leaders: Andrew Critch and Jaan Tallinn</p>\n<p>Funding Needed: High</p>\n<p>Confidence Level: High</p>\n<p>If I had to choose a regranter right now to get a large amount of funding, my pick would be to partner with and participate in the SFF process as an additional funder. The applicants and recommenders are already putting in their effort, with plenty of room for each round to scale. It is very clear there are plenty of exciting places to put additional funds.</p>\n<p>With more funding, the decisions could improve further, as recommenders would be better motivated to devote more time, and we could use a small portion of additional funds to make them better resourced.</p>\n<p>The downside is that SFF can\u2019t \u2018go small\u2019 efficiently on either funders or causes.</p>\n<p>SFF does not accept donations but they are interested in partnerships with people or institutions who are interested in participating as a Funder in a future S-Process round. The minimum requirement for contributing as a Funder to a round is $250k. They are particularly interested in forming partnerships with American donors to help address funding gaps in 501(c)(4)\u2019s and other political organizations.</p>\n<p>This is a good choice if you\u2019re looking to go large and not looking to ultimately funnel towards relatively small funding opportunities or individuals.</p>\n\n\n<h4 class=\"wp-block-heading\">Manifund</h4>\n\n\n<p>Focus: Regranters to AI safety, existential risk, EA meta projects, creative mechanisms</p>\n<p>Leader: Austin Chen (austin at manifund.org).</p>\n<p>Funding Needed: Medium</p>\n<p>Confidence Level: Medium</p>\n<p>This is a regranter that gives its money to its own regranters, one of which was me, for unrestricted grants. They\u2019re the charity donation offshoot of Manifold. They\u2019ve played with crowdfunding, and with impact certificates, and ACX grants. They help run Manifest.</p>\n<p>You\u2019re essentially hiring these people to keep building a website and trying alternative funding allocation mechanisms, and for them to trust the judgment of selected regranters. That seems like a reasonable thing to do if you don\u2019t otherwise know where to put your funds and want to fall back on a wisdom of crowds of sorts. Or, perhaps, if you actively want to fund the cool website.</p>\n<p>Manifold itself did not apply, but I would think that would also be a good place to invest or donate in order to improve the world. It wouldn\u2019t even be crazy to go around subsidizing various markets. If you send me manna there, I will set aside and use that manna to subsidize markets when it seems like the place to do that.</p>\n<p>If you want to support Manifold itself, you can either donate or buy a SAFE by contacting Austin at austin@manifund.org.</p>\n<p>Also I\u2019m a regranter at Manifund, so if you wanted to, <a href=\"https://manifund.com/ZviMowshowitz\">you could use that to entrust me with funds to regrant</a>. As you can see I certainly feel I have plenty of good options here if I can\u2019t find a better local one, and if it\u2019s a substantial amount I\u2019m open to general directions (e.g. ensuring it happens relatively quickly, or a particular cause area as long as I think it\u2019s net positive, or the method of action or theory of impact). However, I\u2019m swamped for time, so I\u2019d probably rely mostly on what I already know.</p>\n\n\n<h4 class=\"wp-block-heading\">AI Risk Mitigation Fund</h4>\n\n\n<p>Focus: Spinoff of LTFF, grants for AI safety projects</p>\n<p>Leader: Thomas Larsen</p>\n<p>Funding Needed: Medium</p>\n<p>Confidence Level: High</p>\n<p>Seems very straightforwardly exactly what it is, a regranter that is usually in the low six figure range. Fellow recommenders were high on Larsen\u2019s ability to judge projects. If you think this is better than you can do on your own and you want to fund such projects, then go for it.</p>\n<p>I\u2019ve talked to them on background about their future plans and directions, and without sharing details their plans make me more excited here.</p>\n<p>Donate <a href=\"https://www.every.org/ai-risk-mitigation-fund\">here</a> or contact them at info@airiskfund.com.</p>\n\n\n<h4 class=\"wp-block-heading\">Long Term Future Fund</h4>\n\n\n<p>Focus: Grants of 4-6 figures mostly to individuals, mostly for AI existential risk</p>\n<p>Leader: Caleb Parikh (among other fund managers)</p>\n<p>Funding Needed: High</p>\n<p>Confidence Level: Low</p>\n<p>The pitch on LTFF is that it is a place for existential risk people who need modest cash infusions to ask for them, and to get them without too much overhead or distortion. Looking over the list of grants, there is at least a decent hit rate.</p>\n<p>One question is, are the marginal grants a lot less effective than the average grant?</p>\n<p>My worry is that I don\u2019t know the extent to which the process is accurate, fair, favors insiders or extracts a time or psychic tax on participants, favors legibility, or rewards \u2018being in the EA ecosystem\u2019 or especially the extent to which the net effects are distortionary and bias towards legibility and standardized efforts. Or the extent to which people use the system to extract funds without actually doing anything.</p>\n<p>That\u2019s not a \u2018I think this is bad,\u2019 it is a true \u2018I do not know.\u2019 I doubt they know either.</p>\n<p>What do we know? They say applications should take 1-2 hours to write and between 10 minutes and 10 hours to evaluate, although that does not include time forming the plan, and this is anticipated to be an ~yearly process long term. And I don\u2019t love that this concern is not listed under <a href=\"https://funds.effectivealtruism.org/funds/far-future#cons\">reasons not to choose to donate to the fund</a> (although the existence of that list at all is most welcome, and the reasons to donate don\u2019t consider the flip side either).</p>\n<p>Given their current relationship to EA funds, you likely should consider LTFF if and only if you both want to focus on AI existential risk via regrants and also want to empower and strengthen the existing EA formal structures and general ways of being.</p>\n<p>That\u2019s not my preference, but it could be yours.</p>\n<p>Donate <a href=\"https://funds.effectivealtruism.org/funds/far-future\">here</a>, or contact the fund managers at longtermfuture@effectivealtruismfunds.org.</p>\n\n\n<h4 class=\"wp-block-heading\">Foresight</h4>\n\n\n<p>Focus: Regrants, fellowships and events</p>\n<p>Leader: Allison Duettmann</p>\n<p>Funding Needed: Medium</p>\n<p>Confidence Level: Low</p>\n<p>Foresight also does other things. I\u2019m focusing here on their AI existential risk grants, which they offer on a rolling basis. I\u2019ve advised them on a small number of potential grants, but they rarely ask.</p>\n<p>The advantage on the regrant side would be to get outreach that wasn\u2019t locked too tightly into the standard ecosystem. The other Foresight activities all seem clearly like good things, but the bar these days is high and since they weren\u2019t the topic of the application I didn\u2019t investigate.</p>\n<p>Donate <a href=\"https://events.foresight.org/donate-join/\">here</a>, or reach out to foresight@foresight.org.</p>\n\n\n<h4 class=\"wp-block-heading\">Centre for Enabling Effective Altruism Learning &amp; Research (CEELAR)</h4>\n\n\n<p>Focus: Strategic incubator and launchpad for EA talent, research, and high-impact initiatives, with emphasis on AI safety, GCR reduction, and longtermist work</p>\n<p>Leader: Attila Ujvari</p>\n<p>Funding Needed: High</p>\n<p>Confidence Level: Low</p>\n<p>I loved the simple core concept of a \u2018catered hotel\u2019 where select people can go to be supported in whatever efforts seem worthwhile. They are now broadening their approach, scaling up and focusing on logistical and community supports, incubation and a general infrastructure play on top of their hotel. This feels less unique to me now and more of a typical (EA UK) community play, so you should evaluate it on that basis.</p>\n<p>Donate <a href=\"http://ceealar.org/donate\">here</a>, or reach out to contact@ceealar.org.</p>\n\n\n<h4 class=\"wp-block-heading\">Organizations That are Essentially Talent Funnels</h4>\n\n\n<p>I am less skeptical of prioritizing AI safety talent funnels than I was last year, but I remain skeptical.</p>\n<p>The central reason remains simple. If we have so many good organizations already, in need of so much funding, why do we need more talent funnels? Is talent our limiting factor? Are we actually in danger of losing important talent?</p>\n<p>The clear exception is leadership and management. There remains, it appears, a clear shortage of leadership and management talent across all charitable space, and startup space, and probably flat out all of space.</p>\n<p>Which means if you are considering stepping up and doing leadership and management, then that is likely more impactful than you might at first think.</p>\n<p>If there was a strong talent funnel specifically for leadership or management, that would be a very interesting funding opportunity. And yes, of course there still need to be some talent funnels. Right now, my guess is we have enough, and marginal effort is best spent elsewhere.</p>\n<p>What about for other talent? What about placements in government, or in the AI labs especially Anthropic of people dedicated to safety? What about the prospects for much higher funding availability by the time we are ready to put people to work?</p>\n<p>If you can pull it off, empowering talent can have a large force multiplier, and the opportunity space looks better than a year ago. It seems plausible that frontier labs will soak up every strong safety candidate they can find, since the marginal returns there are very high and needs are growing rapidly.</p>\n<p>Secondary worries include the danger you end up feeding capability researchers to AI labs, and the discount for the time delays involved.</p>\n<p>My hunch is this will still receive relatively more attention and funding than is optimal, but marginal funds here will still be useful if deployed in places that are careful to avoid being lab talent funnels.</p>\n\n\n<h4 class=\"wp-block-heading\">AI Safety Camp</h4>\n\n\n<p>Focus: Learning by doing, participants work on a concrete project in the field</p>\n<p>Leaders: Remmelt Ellen and Linda Linsefors and Robert Kralisch</p>\n<p>Funding Needed: Low</p>\n<p>Confidence Level: High</p>\n<p>By all accounts they are the gold standard for this type of thing. Everyone says they are great, I am generally a fan of the format, I buy that this can punch way above its weight or cost. If I was going to back something in this section, I\u2019d start here.</p>\n<p>Donors can reach out to Remmelt at remmelt@aisafety.camp, or <a href=\"https://www.every.org/ai-safety-camp/f/match-donation-drive\">leave a matched donation to support next projects</a>.</p>\n\n\n<h4 class=\"wp-block-heading\">Center for Law and AI Risk</h4>\n\n\n<p>Focus: Paying academics small stipends to move into AI safety work</p>\n<p>Leaders: Peter Salib (psalib @ central.uh.edu), Yonathan Arbel (yarbel @ law.ua.edu) and Kevin Frazier (kevin.frazier @ law.utexas.edu).</p>\n<p>Funding Needed: Low</p>\n<p>Confidence Level: High</p>\n<p>This strategy is potentially super efficient. You have an academic that is mostly funded anyway, and they respond to remarkably small incentives to do something they are already curious about doing. Then maybe they keep going, again with academic funding. If you\u2019re going to do \u2018field building\u2019 and talent funnel in a world short on funds for those people, this is doubly efficient. I like it. They\u2019re now moving into hiring an academic fellow, the theory being ~1 year of support to create a permanent new AI safety law professor.</p>\n<p>To donate, message one of leaders at the emails listed above.</p>\n\n\n<h4 class=\"wp-block-heading\">Speculative Technologies</h4>\n\n\n<p>Focus: Enabling ambitious research programs that are poor fits for both academia and VC-funded startups including but not limited to Drexlerian functional nanomachines, high-throughput tools and discovering new superconductors.</p>\n<p>Leader: Benjamin Reinhardt</p>\n<p>Funding Needed: Medium</p>\n<p>Confidence Level: Medium</p>\n<p>I have confirmation that Reinhardt knows his stuff, and we certainly could use more people attempting to build revolutionary hardware. If the AI is scary enough to make you not want to build the hardware, it would figure out how to build the hardware anyway. You might as well find out now.</p>\n<p>If you\u2019re looking to fund a talent funnel, this seems like a good choice.</p>\n<p>To donate, go<a href=\"https://spec.tech/get-involved\"> here</a> or reach out to info@spec.tech.</p>\n\n\n<h4 class=\"wp-block-heading\">Talos Network</h4>\n\n\n<p>Focus: Fellowships to other organizations, such as Future Society, Safer AI and FLI.</p>\n<p>Leader: Chiara Gerosa</p>\n<p>Funding Needed: Medium</p>\n<p>Confidence Level: Low</p>\n<p>They run two fellowship cohorts a year. They seem to place people into a variety of solid organizations, and are exploring the ability to get people into various international organizations like the OECD, UN or European Commission or EU AI Office.</p>\n<p>The more I am convinced people will actually get inside meaningful government posts, the more excited I will be.</p>\n<p>To donate, contact team@talosnetwork.org.</p>\n\n\n<h4 class=\"wp-block-heading\">MATS Research</h4>\n\n\n<p>Focus: Researcher mentorship for those new to AI safety.</p>\n<p>Leaders: Ryan Kidd and Christian Smith.</p>\n<p>Funding Needed: High</p>\n<p>Confidence Level: Medium</p>\n<p>MATS is by all accounts very good at what they do and they have good positive spillover effects on the surrounding ecosystem. The recruiting classes they\u2019re getting are outstanding.</p>\n<p>If (and only if) you think that what they do, which is support would-be alignment researchers starting out and especially transitioning from other professions, is what you want to fund, then you should absolutely fund them. That\u2019s a question of prioritization.</p>\n<p>Donate <a href=\"https://www.matsprogram.org/donate\">here</a>, or contact them via <a href=\"https://www.matsprogram.org/contact\">webform</a>.</p>\n\n\n<h4 class=\"wp-block-heading\">Epistea</h4>\n\n\n<p>Focus: X-risk residencies, workshops, coworking in Prague, fiscal sponsorships</p>\n<p>Leader: Irena Kotikova</p>\n<p>Funding Needed: Medium</p>\n<p>Confidence Level: Medium</p>\n<p>I see essentially two distinct things here.</p>\n<p>First, you have the umbrella organization, offering fiscal sponsorship for other organizations. Based on what I know from the charity space, this is a highly valuable service &#8211; it was very annoying getting Balsa a fiscal sponsor while we waited to become a full 501c3, even though we ultimately found a very good one that did us a solid, and also annoying figuring out how to be on our own going forward.</p>\n<p>Second, you have various projects around Prague, which seem like solid offerings in that class of action of building up EA-style x-risk actions in the area, if that is what you are looking for. So you\u2019d be supporting some mix of those two things.</p>\n<p>To donate, contact hello@epistea.org.</p>\n\n\n<h4 class=\"wp-block-heading\">Emergent Ventures</h4>\n\n\n<p>Focus: Small grants to individuals to help them develop their talent</p>\n<p>Leader: Tyler Cowen</p>\n<p>Funding Needed: Medium</p>\n<p>Confidence Level: High</p>\n<p>Emergent Ventures are not like the other talent funnels in several important ways.</p>\n<ol>\n<li>It\u2019s not about AI Safety. You can definitely apply for an AI Safety purpose, he\u2019s granted such applications in the past, but it\u2019s rare and topics run across the board, well beyond the range otherwise described in this post.</li>\n<li>Decisions are quick and don\u2019t require paperwork or looking legible. Tyler Cowen makes the decision, and there\u2019s no reason to spend much time on your end either.</li>\n<li>There isn\u2019t a particular cause area this is trying to advance. He\u2019s not trying to steer people to do any particular thing. Just to be more ambitious, and be able to get off the ground and build connections and so on. It\u2019s not prescriptive.</li>\n</ol>\n<p>I strongly believe this is an excellent way to boost the development of more talent, as long as money is serving as a limiting factor on the project, and that it is great to develop talent even if you don\u2019t get to direct or know where it is heading. Sure, I get into rhetorical arguments with Tyler Cowen all the time, around AI and also other things, and we disagree strongly about some of the most important questions where I don\u2019t understand how he can continue to have the views he expresses, but this here is still a great project, an amazingly cost-efficient intervention.</p>\n<p>Donate <a href=\"https://mercatus.donorsupport.co/page/donatehome\">here</a> (specify \u201cEmergent Ventures\u201d in notes), or reach out to emergentventures@mercatus.gmu.edu.</p>\n\n\n<h4 class=\"wp-block-heading\">AI Safety Cape Town</h4>\n\n\n<p>Focus: AI safety community building and research in South Africa</p>\n<p>Leaders: Leo Hyams and Benjamin Sturgeon</p>\n<p>Funding Needed: Low</p>\n<p>Confidence Level: Low</p>\n<p>This is a mix of AI research and building up the local AI safety community. One person whose opinion I value gave the plan and those involved in it a strong endorsement, so including it based on that.</p>\n<p>To donate, reach out to leo@aisafetyct.com.</p>\n\n\n<h4 class=\"wp-block-heading\">ILINA Program</h4>\n\n\n<p>Focus: Talent for AI safety in Africa</p>\n<p>Leaders: Cecil Abungu</p>\n<p>Funding Needed: Low</p>\n<p>Confidence Level: Low</p>\n<p>I have a strong endorsement in hand in terms of their past work, if you think this is a good place to go in search of talent.</p>\n<p>To donate, reach out to cecil@ilinaprogram.org.</p>\n\n\n<h4 class=\"wp-block-heading\">Impact Academy Limited</h4>\n\n\n<p>Focus: Global talent accelerator and hiring partner for technical AI safety, supporting worker transitions into AI safety.</p>\n<p>Leader: Roy Hagemann and Varun Agarwal</p>\n<p>Funding Needed: Medium</p>\n<p>Confidence Level: Low</p>\n<p>They previously focused on India, one place with lots of talent, they\u2019re now global. A lot has turned over in the last year, so you\u2019ll want to check them out anew.</p>\n<p>To donate, contact info@impactacademy.org.</p>\n\n\n<h4 class=\"wp-block-heading\">Atlas Computing</h4>\n\n\n<p>Focus: Mapping &amp; creating missing orgs for AI safety (aka Charity Entrepreneurship for AI risk)</p>\n<p>Leaders: Evan Miyazono</p>\n<p>Funding Needed: Medium</p>\n<p>Confidence Level: Low</p>\n<p>There was a pivot this past year from technical research to creating \u2018missing orgs\u2019 in the AI risk space. That makes sense as a strategy if and only if you expect the funding necessary to come in, or you think they can do especially strong targeting. Given the change they will need to be reevaluated.</p>\n<p>They receive donations from<a href=\"https://opencollective.com/atlas-computing\"> here</a>, or you can email them at hello@atlascomputing.org.</p>\n\n\n<h4 class=\"wp-block-heading\">Principles of Intelligence (Formerly PIBBSS)</h4>\n\n\n<p>Focus: Fellowships and affiliate programs for new alignment researchers</p>\n<p>Leader: Lucas Teixeira and Dusan D. Nesic</p>\n<p>Funding Needed: High</p>\n<p>Confidence Level: Low</p>\n<p>There are some hits here. Gabriel Weil in particular has impressed me in our interactions and with his work and they cite a good technical paper. But also that\u2019s with a lot of shots on goal, and I\u2019d have liked to see some bigger hits by now.</p>\n<p>A breakdown revealed that, largely because they start with relatively senior people, most of them get placed in a way that doesn\u2019t require additional support. That makes them a better bet than many similar rivals.</p>\n<p>To donate, reach out to contact@princint.ai, or fund them through Manifund <a href=\"https://manifund.org/projects/pibbss---affiliate-program-funding-6-months-6-affiliates-or-more\">here</a>.</p>\n\n\n<h4 class=\"wp-block-heading\">Tarbell Center</h4>\n\n\n<p>Focus: Journalism fellowships for oversight of AI companies.</p>\n<p>Leader: Cillian Crosson (Ex-Talos Network; still on their board.)</p>\n<p>Funding Needed: High</p>\n<p>Confidence Level: Medium</p>\n<p>They offer fellowships to support journalism that helps society navigate the emergence of increasingly advanced AI, and a few other journalism ventures. They have sponsored at least one person who went on to do good work in the area. They also sponsor article placement, which seems reasonably priced in the grand scheme of things, I think?</p>\n<p>I am not sure this is a place we need to do more investment, or if people trying to do this even need fellowships. Hard to say. There\u2019s certainly a lot more tech reporting and more every day, if I\u2019m ever short of material I have no trouble finding more.</p>\n<p>It is still a small amount of money per person that can meaningfully help people get on their feet and do something useful. We do in general need better journalism. They seem to be in a solid place but also I\u2019d be fine with giving a bunch more funding to play with, they seem pretty unique.</p>\n<p>Donate <a href=\"https://www.tarbellcenter.org/donate\">here</a>, or reach out to them via <a href=\"https://www.tarbellcenter.org/contact\">webform</a>.</p>\n\n\n<h4 class=\"wp-block-heading\">Catalyze Impact</h4>\n\n\n<p>Focus: Incubation of AI safety organizations</p>\n<p>Leader: Alexandra Bos</p>\n<p>Funding Needed: Medium</p>\n<p>Confidence Level: Low</p>\n<p>Why funnel individual talent when you can incubate entire organizations? I am not convinced that on the margin we currently need more of either, but I\u2019m more receptive to the idea of an incubator. Certainly incubators can be high leverage points for getting valuable new orgs and companies off the ground, especially if your model is that once the org becomes fundable it can unlock additional funding.</p>\n<p>If you think an incubator is worth funding, then the question is whether this is the right team. The application was solid all around, and their track record includes Timaeus and Carma, although counterfactuals are always difficult. Beyond that I don\u2019t have a differentiator on why this is the team.</p>\n<p>To donate, contact them at info@catalyze-impact.org.</p>\n\n\n<h4 class=\"wp-block-heading\">CeSIA within EffiSciences</h4>\n\n\n<p>Focus: New AI safety org in Paris, discourse, R&amp;D collaborations, talent pipeline</p>\n<p>Leaders: Charbel-Raphael Segerie, Florent Berthet</p>\n<p>Funding Needed: Low</p>\n<p>Confidence Level: Low</p>\n<p>They\u2019re doing all three of discourse, direct work and talent funnels. They run the only university AI safety course in Europe, maintain the AI Safety Atlas, and have had their recommendations integrated verbatim into the EU AI Act\u2019s Code of Practice. Their two main priorities are supporting the enforcement of the EU AI Act, and driving international agreements on AI red lines.</p>\n<p>To donate, go<a href=\"https://www.securite-ia.fr/en/agir\"> here</a>, or contact them at contact@securite-ia.fr.</p>\n\n\n<h4 class=\"wp-block-heading\">Stanford Existential Risk Initiative (SERI)</h4>\n\n\n<p>Focus: Recruitment for existential risk causes</p>\n<p>Leader: Steve Luby</p>\n<p>Funding Needed: Medium</p>\n<p>Confidence Level: Low</p>\n<p>Stanford students certainly are one place to find people worth educating about existential risk. It\u2019s also an expensive place to be doing it, and a place that shouldn\u2019t need extra funding. And that hates fun. And it\u2019s not great that AI is listed third on their existential risk definition. So I\u2019m not high on them, but it sure beats giving unrestricted funds to your Alma Mater.</p>\n<p>Interested donors should contact Steve Luby directly at sluby@stanford.edu.</p>\n\n\n<h4 class=\"wp-block-heading\">Non-Trivial</h4>\n\n\n<p>Focus: Talent funnel directly to AI safety and biosecurity out of high school</p>\n<p>Leader: Peter McIntyre</p>\n<p>Funding Needed: Low</p>\n<p>Confidence Level: Low</p>\n<p>Having high school students jump straight to research and placement sounds good to me, and plausibly the best version of a talent funnel investment. I haven\u2019t confirmed details but I like the theory.</p>\n<p>To donate, get in touch at info@non-trivial.org.</p>\n\n\n<h4 class=\"wp-block-heading\">CFAR</h4>\n\n\n<p>Focus: Teaching rationality skills, seeking to make sense of the world and how to think</p>\n<p>Leader: Anna Salamon</p>\n<p>Funding Needed: High</p>\n<p>Confidence Level: High</p>\n<p>I am on the board of CFAR, so there is a direct and obvious conflict. Of course, I am on the board of CFAR exactly because I think this is a worthwhile use of my time, and also because Anna asked me. I\u2019ve been involved in various ways since the beginning, including the discussions about whether and how to create CFAR in the first place.</p>\n<p>CFAR is undergoing an attempted revival. There weren\u2019t workshops for many years, for a variety of reasons including safety concerns and also a need to reorient. The workshops are now starting up again, with a mix of both old and new units, and I find much of the new material interesting and potentially valuable. I\u2019d encourage people to<a href=\"https://www.rationality.org/workshops/upcoming\"> consider attending workshops</a>, and also donating.</p>\n<p>To donate,<a href=\"https://www.rationality.org/donate\"> click here</a>, or reach out to contact@rationality.org.</p>\n\n\n<h4 class=\"wp-block-heading\">The Bramble Center</h4>\n\n\n<p>Focus: Workshops in the style of CFAR but focused on practical courage, forming high value relationships between attendees with different skill sets and learning to care for lineages, in the hopes of repairing the anglosphere and creating new capable people to solve our problems including AI in more grounded ways.</p>\n<p>Leader: Anna Salamon</p>\n<p>Funding Needed: Low</p>\n<p>Confidence Level: High</p>\n<p>LARC is kind of a spin-off of CFAR, a place to pursue a different kind of agenda. I absolutely do not have high confidence that this will succeed, but I do have high confidence that this is a gamble worth taking, and that if those involved here (especially Anna Salamon but also others that I know) want to devote their time to trying this, that we should absolutely give them that opportunity.</p>\n<p>Donate <a href=\"https://every.org/bramble\">here</a>.</p>\n\n\n<h4 class=\"wp-block-heading\">Final Reminders</h4>\n\n\n<p>If an organization was not included here, or was removed for the 2025 edition, again, that does not mean they aren\u2019t good, or even that I wouldn\u2019t endorse them if asked.</p>\n<p>It could be because I am not aware of the organization, or lack sufficient knowledge at this point to be confident in listing them, or I fear my knowledge is obsolete.</p>\n<p>It could be that they asked to be excluded, which happened in several cases.</p>\n<p>If by accident I included you and you didn\u2019t want to be included and I failed to remove you, or you don\u2019t like the quote here, I sincerely apologize and will edit you out right away, no questions asked.</p>\n<p>If an organization is included here, that is a good thing, but again, it does not mean you should donate without checking if it makes sense based on what you think is true, how you think the world works, what you value and what your priorities are. There are no universal right answers.</p>"
            ],
            "link": "https://thezvi.wordpress.com/2025/11/27/the-big-nonprofits-post-2025-2/",
            "publishedAt": "2025-11-27",
            "source": "TheZvi",
            "summary": "There remain lots of great charitable giving opportunities out there. I have now had three opportunities to be a recommender for the Survival and Flourishing Fund (SFF). I wrote in detail about my first experience back in 2021, where I &#8230; <a href=\"https://thezvi.wordpress.com/2025/11/27/the-big-nonprofits-post-2025-2/\">Continue reading <span class=\"meta-nav\">&#8594;</span></a>",
            "title": "The Big Nonprofits Post 2025"
        }
    ],
    "lookbackDays": 1,
    "publishDate": "2025-11-27"
}