{
    "articles": [
        {
            "content": [
                "<p>Resizing an image is one of those programming tasks that seems simple, but has some rough edges.\nOne common mistake is forgetting to handle the EXIF orientation, which can make resized images look very different from the original.</p>\n\n<p>Last year I wrote <a href=\"https://alexwlchan.net/2024/create-thumbnail/\">a\u00a0<code>create_thumbnail</code> tool</a> to resize images, and today I released <a href=\"https://github.com/alexwlchan/create_thumbnail/releases/tag/v1.0.2\">a\u00a0small update</a>.\nNow it\u2019s aware of EXIF orientation, and it no longer mangles these images.\nThis is possible thanks to a new version of the Rust <code>image</code> crate, which just improved its EXIF support.</p>\n\n<h2 id=\"whats-exif-orientation\">What\u2019s EXIF orientation?</h2>\n\n<p>Images can specify an orientation in their EXIF metadata, which can describe both rotation and reflection.\nThis metadata is usually added by cameras and phones, which can detect how you\u2019re holding them, and tell viewing software how to display the picture later.</p>\n\n<p>For example, if you take a photo while holding your camera on its side, the camera can record that the image should be rotated 90 degrees when viewed.\nIf you use a front-facing selfie camera, the camera could note that the picture needs to be mirrored.</p>\n\n<p>There are eight different values for EXIF orientation \u2013 rotating in increments of 90\u00b0, and mirrored or not.\nThe default value is \u201c1\u201d (display as-is), and here are the other seven values:</p>\n\n<svg class=\"dark_aware\" viewBox=\"0 0 540 240\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n  <defs>\n    \n\n    <g id=\"baseImage\">\n      <rect height=\"34.375\" width=\"68.75\"></rect>\n\n      <path d=\"M 10 7.638888888888889                l 12.5 0 m -12.5 0                l 0 19.09722222222222 m 0 -9.54861111111111                l 10 0\"></path>\n      <path d=\"M 31.25 7.638888888888889                l 0 19.09722222222222                m 12.5 0                l -12.5 0\"></path>\n      <path d=\"M 47.5 7.638888888888889                l 6.25 9.54861111111111                l 6.25 -9.54861111111111                m -6.25 9.54861111111111                l 0 9.54861111111111\"></path>\n    </g>\n  </defs>\n\n  <circle cx=\"120\" cy=\"120\" r=\"75\" stroke-dasharray=\"3,3\"></circle>\n\n  <text x=\"120\" y=\"15.535714285714285\">1</text>\n  <text x=\"15.535714285714285\" y=\"120\">6</text>\n  <text x=\"224.46428571428572\" y=\"120\">8</text>\n  <text x=\"120\" y=\"229.375\">3</text>\n\n  <use transform=\"translate(85.625 27.8125)\" xlink:href=\"#baseImage\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"></use>\n  <use transform=\"translate(160.625 102.8125) rotate(90 34.375 17.1875) \" xlink:href=\"#baseImage\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"></use>\n  <use transform=\"translate(85.625 177.8125) rotate(180 34.375 17.1875)\" xlink:href=\"#baseImage\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"></use>\n  <use transform=\"translate(10.625 102.8125) rotate(270 34.375 17.1875)\" xlink:href=\"#baseImage\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"></use>\n\n  <text x=\"420\" y=\"15.535714285714288\">2</text>\n  <text x=\"315.5357142857143\" y=\"120\">5</text>\n  <text x=\"524.4642857142857\" y=\"120\">7</text>\n  <text x=\"420\" y=\"229.375\">4</text>\n\n  <circle cx=\"420\" cy=\"120\" r=\"75\" stroke-dasharray=\"3,3\"></circle>\n\n  <use transform=\"translate(454.375 27.8125) scale(-1 1)\" xlink:href=\"#baseImage\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"></use>\n  <use transform=\"translate(460.625 171.5625) rotate(90 34.375 17.1875)  scale(-1 1)\" xlink:href=\"#baseImage\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"></use>\n  <use transform=\"translate(316.875 177.8125) rotate(180 34.375 17.1875) scale(-1 1)\" xlink:href=\"#baseImage\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"></use>\n  <use transform=\"translate(310.625 34.0625) rotate(270 34.375 17.1875) scale(-1 1)\" xlink:href=\"#baseImage\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"></use>\n\n\n<title id=\"svg_exif_orientation\">A diagram showing the eight different orientations of the word \u2018FLY\u2019: four rotations, four rotations with a mirror reflection.</title></svg>\n\n<p>You can see the EXIF orientation value with programs like <a href=\"https://exiftool.org/\">Phil Harvey\u2019s exiftool</a>, which helpfully converts the numeric value into a human-readable description:</p>\n\n<pre class=\"language-console\"><code><span class=\"gp\">$</span><span class=\"w\"> </span><span class=\"c\"># exiftool's default output is human-readable</span>\n<span class=\"gp\">$</span><span class=\"w\"> </span>exiftool <span class=\"nt\">-orientation</span> my_picture.jpg\n<span class=\"go\">Orientation                     : Rotate 270 CW\n\n</span><span class=\"gp\">$</span><span class=\"w\"> </span><span class=\"c\"># or we can get the raw numeric value</span>\n<span class=\"gp\">$</span><span class=\"w\"> </span>exiftool <span class=\"nt\">-n</span> <span class=\"nt\">-orientation</span> my_picture.jpg\n<span class=\"go\">Orientation                     : 8\n</span></code></pre>\n<h2 id=\"resizing-images-in-rust\">Resizing images in Rust</h2>\n\n<p>I use the <a href=\"https://crates.io/crates/image\"><code>image</code> crate</a> to resize images in Rust.</p>\n\n<p>My old code for resizing images would open the image, resize it, then save it back to disk.\nHere\u2019s a short example:</p>\n<pre><code><span class=\"k\">use</span> <span class=\"nn\">image</span><span class=\"p\">::</span><span class=\"nn\">imageops</span><span class=\"p\">::</span><span class=\"n\">FilterType</span><span class=\"p\">;</span>\n<span class=\"k\">use</span> <span class=\"nn\">std</span><span class=\"p\">::</span><span class=\"nn\">error</span><span class=\"p\">::</span><span class=\"n\">Error</span><span class=\"p\">;</span>\n\n<span class=\"k\">fn</span> <span class=\"nf\">main</span><span class=\"p\">()</span> <span class=\"k\">-&gt;</span> <span class=\"nb\">Result</span><span class=\"o\">&lt;</span><span class=\"p\">(),</span> <span class=\"nb\">Box</span><span class=\"o\">&lt;</span><span class=\"k\">dyn</span> <span class=\"n\">Error</span><span class=\"o\">&gt;&gt;</span> <span class=\"p\">{</span>\n    <span class=\"c1\">// Old method: doesn't know about EXIF orientation</span>\n    <span class=\"k\">let</span> <span class=\"n\">img</span> <span class=\"o\">=</span> <span class=\"nn\">image</span><span class=\"p\">::</span><span class=\"nf\">open</span><span class=\"p\">(</span><span class=\"s\">\"original.jpg\"</span><span class=\"p\">)</span><span class=\"o\">?</span><span class=\"p\">;</span>\n    <span class=\"n\">img</span><span class=\"nf\">.resize</span><span class=\"p\">(</span><span class=\"mi\">180</span><span class=\"p\">,</span> <span class=\"mi\">120</span><span class=\"p\">,</span> <span class=\"nn\">FilterType</span><span class=\"p\">::</span><span class=\"n\">Lanczos3</span><span class=\"p\">)</span>\n        <span class=\"nf\">.save</span><span class=\"p\">(</span><span class=\"s\">\"thumbnail.jpg\"</span><span class=\"p\">)</span><span class=\"o\">?</span><span class=\"p\">;</span>\n\n    <span class=\"nf\">Ok</span><span class=\"p\">(())</span>\n<span class=\"p\">}</span>\n</code></pre>\n<p>The thumbnail will keep the resized pixels in the same order as the original image, but the thumbnail doesn\u2019t have the EXIF orientation metadata.\nThis means that if the original image had an EXIF orientation, the thumbnail could look different, because it\u2019s no longer being rotated/reflected properly.</p>\n\n<p>When I wrote <code>create_thumbnail</code>, the <code>image</code> crate didn\u2019t know anything about EXIF orientation \u2013 but last week\u2019s <a href=\"https://github.com/image-rs/image/releases/tag/v0.25.8\">v0.25.8 release</a> added several functions related to EXIF orientation.\nIn particular, I can now read the orientation and apply it to an image:</p>\n<pre><code><span class=\"k\">use</span> <span class=\"nn\">image</span><span class=\"p\">::</span><span class=\"nn\">imageops</span><span class=\"p\">::</span><span class=\"n\">FilterType</span><span class=\"p\">;</span>\n<span class=\"k\">use</span> <span class=\"nn\">image</span><span class=\"p\">::{</span><span class=\"n\">DynamicImage</span><span class=\"p\">,</span> <span class=\"n\">ImageDecoder</span><span class=\"p\">,</span> <span class=\"n\">ImageReader</span><span class=\"p\">};</span>\n<span class=\"k\">use</span> <span class=\"nn\">std</span><span class=\"p\">::</span><span class=\"nn\">error</span><span class=\"p\">::</span><span class=\"n\">Error</span><span class=\"p\">;</span>\n\n<span class=\"k\">fn</span> <span class=\"nf\">main</span><span class=\"p\">()</span> <span class=\"k\">-&gt;</span> <span class=\"nb\">Result</span><span class=\"o\">&lt;</span><span class=\"p\">(),</span> <span class=\"nb\">Box</span><span class=\"o\">&lt;</span><span class=\"k\">dyn</span> <span class=\"n\">Error</span><span class=\"o\">&gt;&gt;</span> <span class=\"p\">{</span>\n    <span class=\"c1\">// New methods in image v0.25.8 know about EXIF orientation,</span>\n    <span class=\"c1\">// and allow us to apply it to the image before resizing.</span>\n    <span class=\"k\">let</span> <span class=\"k\">mut</span> <span class=\"n\">decoder</span> <span class=\"o\">=</span> <span class=\"nn\">ImageReader</span><span class=\"p\">::</span><span class=\"nf\">open</span><span class=\"p\">(</span><span class=\"s\">\"original.jpg\"</span><span class=\"p\">)</span><span class=\"o\">?</span><span class=\"nf\">.into_decoder</span><span class=\"p\">()</span><span class=\"o\">?</span><span class=\"p\">;</span>\n    <span class=\"k\">let</span> <span class=\"n\">orientation</span> <span class=\"o\">=</span> <span class=\"n\">decoder</span><span class=\"nf\">.orientation</span><span class=\"p\">()</span><span class=\"o\">?</span><span class=\"p\">;</span>\n    <span class=\"k\">let</span> <span class=\"k\">mut</span> <span class=\"n\">img</span> <span class=\"o\">=</span> <span class=\"nn\">DynamicImage</span><span class=\"p\">::</span><span class=\"nf\">from_decoder</span><span class=\"p\">(</span><span class=\"n\">decoder</span><span class=\"p\">)</span><span class=\"o\">?</span><span class=\"p\">;</span>\n    <span class=\"n\">img</span><span class=\"nf\">.apply_orientation</span><span class=\"p\">(</span><span class=\"n\">orientation</span><span class=\"p\">);</span>\n\n    <span class=\"n\">img</span><span class=\"nf\">.resize</span><span class=\"p\">(</span><span class=\"mi\">180</span><span class=\"p\">,</span> <span class=\"mi\">120</span><span class=\"p\">,</span> <span class=\"nn\">FilterType</span><span class=\"p\">::</span><span class=\"n\">Lanczos3</span><span class=\"p\">)</span>\n        <span class=\"nf\">.save</span><span class=\"p\">(</span><span class=\"s\">\"thumbnail.jpg\"</span><span class=\"p\">)</span><span class=\"o\">?</span><span class=\"p\">;</span>\n\n    <span class=\"nf\">Ok</span><span class=\"p\">(())</span>\n<span class=\"p\">}</span>\n</code></pre>\n<p>The thumbnail still doesn\u2019t have any EXIF orientation data, but the pixels have been rearranged so the resized image looks similar to the original.\nThat\u2019s what I want.</p>\n\n<p>Here\u2019s a visual comparison of the three images.\nNotice how the thumbnail from the old code looks upside down:</p>\n\n\n\n<table id=\"examples\">\n  <tr>\n    <td>\n      <img alt=\"\" src=\"https://alexwlchan.net/images/2025/original_exif_image.jpg\" />\n      <span>original image</span>\n    </td>\n    <td>\n      <img alt=\"\" src=\"https://alexwlchan.net/images/2025/thumbnail_without_exif_aware.jpg\" />\n      <span>thumbnail from the old code</span>\n    </td>\n    <td>\n      <img alt=\"\" src=\"https://alexwlchan.net/images/2025/thumbnail_with_exif_aware.jpg\" />\n      <span>thumbnail from the new code</span>\n    </td>\n  </tr>\n</table>\n\n<p>This test image comes from Dave Perrett\u2019s <a href=\"https://github.com/recurser/exif-orientation-examples\">exif-orientation-examples repo</a>, which has a collection of images that were very helpful for testing this code.</p>\n\n<h2 id=\"is-this-important\">Is this important?</h2>\n\n<p>This is a small change, but it solves an annoyance I\u2019ve hit in every project that deals with images.\nI\u2019ve written this fix, but images with an EXIF orientation are rare enough that I always forget them when I start a new project \u2013 and I used to solve the same problem again and again.</p>\n\n<p>By handling EXIF orientation in <code>create_thumbnail</code>, I won\u2019t have to think about this again.\nThat\u2019s the beauty of a shared tool \u2013 I fix it once, and then it\u2019s fixed for all my current and future projects.</p>\n\n\n    <p>[If the formatting of this post looks odd in your feed reader, <a href=\"https://alexwlchan.net/2025/create-thumbnail-is-exif-aware/?ref=rss\">visit the original article</a>]</p>"
            ],
            "link": "https://alexwlchan.net/2025/create-thumbnail-is-exif-aware/?ref=rss",
            "publishedAt": "2025-09-08",
            "source": "Alex Chan",
            "summary": "A new version of Rust's image crate has support for EXIF orientation, which allows me to resize images without mangling their rotation.",
            "title": "Resizing images in Rust, now with EXIF orientation support"
        },
        {
            "content": [],
            "link": "https://harper.blog/notes/2025-09-08_b603a459a8e5_my-homepod-mini-just-randomly-/",
            "publishedAt": "2025-09-08",
            "source": "Harper Reed",
            "summary": "<p>my homepod mini just randomly started playing type o negative. not a bad start to the week.</p> <hr /> <p>Thank you for using RSS. I appreciate you. <a href=\"mailto:harper&#64;modest.com\">Email me</a></p>",
            "title": "Note #286"
        },
        {
            "content": [
                "<p>This is the weekly visible open thread. Post about anything you want, ask random questions, whatever. ACX has an unofficial <a href=\"https://www.reddit.com/r/slatestarcodex/\">subreddit</a>, <a href=\"https://discord.gg/RTKtdut\">Discord</a>, and <a href=\"https://www.datasecretslox.com/index.php\">bulletin board</a>, and <a href=\"https://www.lesswrong.com/community?filters%5B0%5D=SSC\">in-person meetups around the world</a>. Most content is free, some is subscriber only; you can subscribe <strong><a href=\"https://astralcodexten.substack.com/subscribe\">here</a></strong>. Also:</p><div><hr /></div><p><strong>1: </strong>Meetups this week include Abuja, Dublin, Ho Chi Minh City, London, Manchester, Montevideo, Montreal, Moscow, Munich, Nairobi, Ottawa, Rio, Santiago, Singapore, Stockholm, Tokyo, Baltimore, <a href=\"https://www.astralcodexten.com/p/berkeley-meetup-this-tuesday\">Berkeley</a>, Madison, Phoenix, Pittsburgh, Seattle, and many others. And late additions to the meetup list include Vilnius, Haifa, Vegas, and Durham. See <a href=\"https://www.astralcodexten.com/p/meetups-everywhere-2025-times-and\">the list</a> for more. </p><p><strong>2: </strong>There is a Boston preliminary municipal election this Tuesday; the Boston ACX meetup group has put together a voting guide, <a href=\"https://docs.google.com/document/d/18HRUaPVmkKyWyOLY9ojgP5N_5O_sqLVD2dQG3IMTWtc/edit?tab=t.0\">which you can find here</a>.</p><p><strong>3: </strong>Comments of the week, in both cases on <a href=\"https://www.astralcodexten.com/p/links-for-september-2025\">the links post</a>: Byrel Mitchell <a href=\"https://www.astralcodexten.com/p/links-for-september-2025/comment/152350648\">challenges the analogy </a>between the Skrmetti Supreme Court case on transgender and the Bostock case on homosexuality. And Ryan W. <a href=\"https://www.astralcodexten.com/p/links-for-september-2025/comment/152527419\">writes his own poem in the style of the yeti example</a>, with some help from AI.</p><p><strong>4: </strong>2024 ACX grantee Alexander Putilin has an update on his EEG entrainment replication project. He is looking for study participants and he is publishing the project&#8217;s source code:</p><blockquote><p>A quick recap. The study &#8220;Learning at your brain&#8217;s rhythm: individualized entrainment boosts learning for perceptual decisions&#8221; claims that entrainment (flashing a bright white light) at a person's individual peak alpha frequency (IAF) helps them learn to distinguish two types of patterns faster.</p><p>The project is to replicate one of the core claims of the paper: entrainment at IAF + timing the stimulus with a trough of the person&#8217;s alpha rhythm (T-match) is significantly better entrainment at IAF + timing the stimulus of a peak of the same rhythm (P-match).</p><p>I am seeing early signs of the effect on myself. My performance in the T-match condition is noticeably better than in the P-match condition (64% vs 58% on average). I also subjectively feel like I am &#8216;learning more&#8217; in the T-match condition. So I&#8217;m very optimistic about the study actually replicating.</p><p>I am now starting to collect the data. If you are in London, please consider <a href=\"https://forms.gle/X37zyTV3KhbSb3Ze9\">signing up</a>. Also, I&#8217;ll be doing a public demo and a Q&amp;A at <a href=\"https://luma.com/ACX-London-Sep-2025\">the London ACX meetup on the 13th of September</a> at 2PM.</p><p>The code is now published on <a href=\"https://github.com/eleweek/EEG_entrainment\">Github</a>. If you own an EEG headset and experiment with the code, your feedback will be greatly appreciated.</p><p>The full replication results will be published on my <a href=\"https://psychotechnology.substack.com/\">psychotechnology</a> substack.</p></blockquote>"
            ],
            "link": "https://www.astralcodexten.com/p/open-thread-398",
            "publishedAt": "2025-09-08",
            "source": "SlateStarCodex",
            "summary": "<p>This is the weekly visible open thread. Post about anything you want, ask random questions, whatever. ACX has an unofficial <a href=\"https://www.reddit.com/r/slatestarcodex/\">subreddit</a>, <a href=\"https://discord.gg/RTKtdut\">Discord</a>, and <a href=\"https://www.datasecretslox.com/index.php\">bulletin board</a>, and <a href=\"https://www.lesswrong.com/community?filters%5B0%5D=SSC\">in-person meetups around the world</a>. Most content is free, some is subscriber only; you can subscribe <strong><a href=\"https://astralcodexten.substack.com/subscribe\">here</a></strong>. Also:</p><div><hr /></div><p><strong>1: </strong>Meetups this week include Abuja, Dublin, Ho Chi Minh City, London, Manchester, Montevideo, Montreal, Moscow, Munich, Nairobi, Ottawa, Rio, Santiago, Singapore, Stockholm, Tokyo, Baltimore, <a href=\"https://www.astralcodexten.com/p/berkeley-meetup-this-tuesday\">Berkeley</a>, Madison, Phoenix, Pittsburgh, Seattle, and many others. And late additions to the meetup list include Vilnius, Haifa, Vegas, and Durham. See <a href=\"https://www.astralcodexten.com/p/meetups-everywhere-2025-times-and\">the list</a> for more. </p><p><strong>2: </strong>There is a Boston preliminary municipal election this Tuesday; the Boston ACX meetup group has put together a voting guide, <a href=\"https://docs.google.com/document/d/18HRUaPVmkKyWyOLY9ojgP5N_5O_sqLVD2dQG3IMTWtc/edit?tab=t.0\">which you can find here</a>.</p><p><strong>3: </strong>Comments of the week, in both cases on <a href=\"https://www.astralcodexten.com/p/links-for-september-2025\">the links post</a>: Byrel Mitchell <a href=\"https://www.astralcodexten.com/p/links-for-september-2025/comment/152350648\">challenges the analogy </a>between the Skrmetti Supreme Court case on transgender and the Bostock case on homosexuality. And Ryan W. <a href=\"https://www.astralcodexten.com/p/links-for-september-2025/comment/152527419\">writes his own poem in the style of the yeti example</a>, with some help from AI.</p><p><strong>4: </strong>2024 ACX grantee Alexander Putilin has an update on his EEG entrainment replication project. He is looking for study",
            "title": "Open Thread 398"
        },
        {
            "content": [
                "<p>I am a little late to the party on several key developments at OpenAI:</p>\n<ol>\n<li>OpenAI\u2019s Chief Global Affairs Officer Chris Lehane was central to the creation of the new $100 million PAC where they will partner with a16z to oppose any and all attempts of states to regulate AI in any way for any reason.</li>\n<li>Effectively as part of that effort, OpenAI sent a deeply bad faith letter to Governor Newsom opposing SB 53.</li>\n<li>OpenAI seemingly has embraced descending fully into paranoia around various nonprofit organizations and also Effective Altruism in general, or at least is engaging in rhetoric and legal action to that effect, joining the style of Obvious Nonsense rhetoric about this previously mostly used by a16z.\n<div>\n\n\n<span id=\"more-24704\"></span>\n\n\n</div>\n</li>\n</ol>\n<p>This is deeply troubling news. It is substantially worse than I was expecting of them. Which is presumably my mistake.</p>\n<p>This post covers those events, along with further developments around two recent tragic suicides where ChatGPT was plausibly at fault for what went down, including harsh words from multiple attorneys general who can veto OpenAI\u2019s conversion to a for-profit company.</p>\n\n\n<h4 class=\"wp-block-heading\">A Brief History of OpenAI Lobbying</h4>\n\n\n<p><a href=\"https://thezvi.substack.com/p/openai-11-america-action-plan?utm_source=chatgpt.com\">In OpenAI #11: America Action Plan</a>, I documented that OpenAI:</p>\n<ol>\n<li><a href=\"https://openai.com/global-affairs/openai-proposals-for-the-us-ai-action-plan/\">Submitted an American AI Action Plan</a> proposal that went full jingoist, framing AI as a race against the CCP in which we must prevail, with intentionally toxic vibes throughout.</li>\n<li>Requested immunity from all AI regulations.</li>\n<li>Attempted to ban DeepSeek using bad faith arguments.</li>\n<li>Demanded absolute fair use, for free, for all AI training, or else.</li>\n<li>Also included some reasonable technocratic proposals, such as a National Transmission Highway Act, AI Opportunity Zones, along with some I think are worse on the merits such as their \u2018national AI readiness strategy.\u2019</li>\n</ol>\n<p><a href=\"https://thezvi.substack.com/p/openai-the-battle-of-the-board?utm_source=chatgpt.com\">Also</a> worth remembering:</p>\n<ol>\n<li><a href=\"https://corporateeurope.org/en/2023/11/byte-byte\">This article claims</a> both OpenAI and Microsoft were central in lobbying to take any meaningful requirements for foundation models out of the EU\u2019s AI Act. If I was a board member, I would see this as incompatible with the OpenAI charter. This was then <a href=\"https://www.openaifiles.org/ceo-integrity\">fleshed out further in the OpenAI Files</a> and in <a href=\"https://corporateeurope.org/en/2023/11/byte-byte\">this article from Corporate Europe Observatory</a>.</li>\n<li>OpenAI lobbied against SB 1047, both reasonably and unreasonably.</li>\n<li>OpenAI\u2019s CEO Sam Altman has over time used increasingly jingoistic language throughout his talks, has used steadily less talk about</li>\n</ol>\n\n\n<h4 class=\"wp-block-heading\">OpenAI\u2019s Latest Bad Faith Lobbying Attempt</h4>\n\n\n<p>OpenAI\u2019s Chief Global Affairs Officer, Christopher Lehane, <a href=\"https://cdn.openai.com/pdf/oai_ca-safety-letter_8-11-25.pdf\">sent a letter to Governor Newsom urging him to gut SB 53</a> (<a href=\"https://docs.google.com/document/d/1ijSZem9Lvv3Tt_vrL_ebgfNYIg2RVE0DF187CcOLvAA/mobilebasic\">or see Miles\u2019s in-line responses included here</a>), which is already very much a compromise bill <a href=\"https://news.bgov.com/bloomberg-government-news/major-california-ai-bills-narrowed-by-panel-over-cost-pressures\">that got compromised further</a> by pushing its \u2018large AI companies\u2019 threshold and eliminating the third-party audit requirement. That already eliminated almost all of what little burden could be claimed was being imposed by the bill.</p>\n<p>OpenAI\u2019s previous lobbying efforts were in bad faith. This is substantially worse.</p>\n<p>Here is the key ask from OpenAI, bold in original:</p>\n<blockquote>\n<p>In order to make California a leader in global, national and state-level AI policy, <strong>we encourage the state to consider frontier model developers compliant with its state requirements when they sign onto a parallel regulatory framework like the CoP or enter into a safety-oriented agreement with a relevant US federal government agency.</strong></p>\n</blockquote>\n<p>As in, California should abdicate its responsibilities entirely, and treat giving lip service to the EU\u2019s Code of Practice (not even actually complying with it!) as sufficient to satisfy California on all fronts. It also says that if a company makes any voluntary agreement with the Federal Government on anything safety related, then that too should satisfy all requirements.</p>\n<p>This is very close to saying California should have no AI safety regulations at all.</p>\n<p>The rhetoric behind this request is what you would expect. You\u2019ve got:</p>\n<ol>\n<li>The jingoism.</li>\n<li>The talk about \u2018innovation.\u2019</li>\n<li>The Obvious Nonsense threats about this slowing down progress or causing people to withdraw from California.</li>\n<li>The talk about Federal leadership on regulation without any talk of what that would look like while the only Federal proposal that ever got traction was \u2018ban the states from acting and still don\u2019t do anything on the Federal level.\u2019</li>\n<li>The talk about burden on \u2018small developers\u2019 when to be covered by SB 53 at all you now have to spend a full $500 million in training compute, and the only substantive expense (the outside audits) are entirely gone.</li>\n<li>The false claim that California lacks state capacity to handle this, and the false assurance us the EU and Federal Government have totally have what they need.</li>\n<li>The talk of a \u2018California approach\u2019 which here means \u2018do nothing.\u2019</li>\n</ol>\n<p>They even try to equate SB 53 to CEQA, which is a non-sequitur.</p>\n<p>They equate OpenAI\u2019s \u2018commitment to work with\u2019 the US federal government in ways that likely amount to running some bespoke tests focused on national security concerns as equivalent to being under a comprehensive regulatory regime, and as a substitute for SB 53 including its transparency requirements.</p>\n<p>They emphasize that they are a non-profit, while trying to transform themselves into a for-profit and expropriate most of the non-profit\u2019s wealth for private gain.</p>\n<p>Plus we have again the important misstatement of OpenAI\u2019s mission.</p>\n<blockquote>\n<p>OpenAI\u2019s actual mission: Ensure that AGI benefits all of humanity.</p>\n<p>OpenAI says its mission is: Building AI that benefits all of humanity.</p>\n</blockquote>\n<p>That is very importantly not the same thing. The best way to ensure AGI benefits all of humanity could importantly be to not build it.</p>\n<p>Also as you would expect, the letter does not, anywhere, explain why the even fully complying with Code of Practice, let alone any future unspecified voluntary safety-oriented agreement, would satisfy the policy goals behind SB 53.</p>\n<p>Because very obviously, if you read the Code of Practice and SB 53, they wouldn\u2019t.</p>\n<p>Miles Brundage <a href=\"https://docs.google.com/document/d/1ijSZem9Lvv3Tt_vrL_ebgfNYIg2RVE0DF187CcOLvAA/mobilebasic\">responds to the letter in-line</a> (which I recommend if you want to go into the details at that level) and also offers this Twitter thread:</p>\n<blockquote>\n<p><a href=\"https://x.com/Miles_Brundage/status/1962775385005007111\">Miles Brundage</a> (September 1): TIL OpenAI sent a letter to Governor Newsom filled with misleading garbage about SB 53 and AI policy generally.</p>\n<p>Unsurprising if you follow this stuff, but worth noting for those who work there and don&#8217;t know what&#8217;s being done in their name.</p>\n<p>I don&#8217;t think it&#8217;s worth dignifying it with a line-by-line response but I&#8217;ll just say that it was clearly not written by people who know what they&#8217;re talking about (e.g., what&#8217;s in the Code of Practice + what&#8217;s in SB 53).</p>\n<p>It also boils my blood every time that team comes up with new and creative ways to misstate OpenAI&#8217;s mission.</p>\n<p>Today it&#8217;s &#8220;the AI Act is so strong, you should just assume that we&#8217;re following everything else&#8221; [even though the AI Act has a bunch of issues].</p>\n<p>Tomorrow it&#8217;s &#8220;the AI Act is being enforced too stringently &#8212; it needs to be relaxed in ways A, B, and C.&#8221;</p>\n<ol>\n<li>The context here is OpenAI trying to water down SB 53 (which is not that strict to begin with &#8211; e.g. initially third parties would verify companies&#8217; safety claims in *2030* and now there is just *no* such requirement)</li>\n<li>The letter treats the Code of Practice for the AI Act, one the one hand &#8211; imperfect but real regulation &#8211; and a voluntary agreement to do some tests sometimes with a friendly government agency, on the other &#8211; as if they&#8217;re the same. They&#8217;re not, and neither is SB 53&#8230;</li>\n<li>It&#8217;s very disingenuous to act as if OpenAI is super interested in harmonious US-EU integration + federal leadership over states when they have literally never laid out a set of coherent principles for US federal AI legislation.</li>\n<li>Vague implied threats to slow down shipping products or pull out of CA/the US etc. if SB 53 went through, as if it is super burdensome&#8230; that&#8217;s just nonsense. No one who knows anything about this stuff thinks any of that is even remotely plausible.</li>\n<li>The &#8220;California solution&#8221; is basically &#8220;pretend different things are the same,&#8221; which is funny because it&#8217;d take two braincells for OpenAI to articulate an actually-distinctively-Californian or actually-distinctively-American approach to AI policy. But there&#8217;s no such effort.</li>\n<li>For example, talk about how SB 53 is stronger on actual transparency (and how the Code of Practice has a &#8220;transparency&#8221; section that basically says &#8220;tell stuff to regulators/customers, and it&#8217;d sure be real nice if you sometimes published it&#8221;). Woulda been trivial. The fact that none of that comes up suggests the real strategy is &#8220;make number of bills go down.&#8221;</li>\n<li>OpenAI&#8217;s mission is to ensure that AGI benefits all of humanity. Seems like something you&#8217;d want to get right when you have court cases about mission creep.</li>\n</ol>\n</blockquote>\n<p><a href=\"https://nomadsvagabonds.substack.com/p/the-safe-harbor-trap\">We also have this essay response from Nomads Vagabonds</a>. He is if anything even less kind than Miles. He reminds us that OpenAI through Greg Brockman has teamed up with a16z to dedicate $100 million to <a href=\"https://nomadsvagabonds.substack.com/p/the-100-million-tell\">ensuring no regulation of AI, anywhere in any state, for any reason</a>, in<a href=\"https://www.wsj.com/politics/silicon-valley-launches-pro-ai-pacs-to-defend-industry-in-midterm-elections-287905b3\"> a PAC that was the brainchild of OpenAI vice president of global affairs Chris Lehane</a>.</p>\n<p>He also goes into detail about the various bad faith provisions.</p>\n\n\n<h4 class=\"wp-block-heading\">OpenAI Descends Into Paranoia and Legally Attacks Nonprofits</h4>\n\n\n<p>These four things can be true at once.</p>\n<ol>\n<li>OpenAI has several competitors that strongly dislike OpenAI and Sam Altman, for a combination of reasons with varying amounts of merit.</li>\n<li>Elon Musk\u2019s lawsuits against OpenAI are often without legal merit, although the objections to OpenAI\u2019s conversion to for-profit were ruled by the judge to absolutely have merit, with the question mainly being if Musk had standing.</li>\n<li>There are many other complaints about OpenAI that have a lot or merit.</li>\n<li>AI might kill everyone and you might want to work to prevent this without having it out for OpenAI in particular or being funded by OpenAI\u2019s competitors.</li>\n</ol>\n<p>OpenAI seems, <a href=\"https://sfstandard.com/2025/09/02/openai-sam-altman-elon-musk-ai-regulation/\">by Shugerman\u2019s reporting</a>, to have responded to this situation by becoming paranoid that there is some sort of vast conspiracy Out To Get Them, funded and motivated by commercial rivalry, as opposed to people who care about AI not killing everyone and also this Musk guy who is Big Mad.</p>\n<p>Of course a lot of us, as the primary example, are going to take issue with OpenAI\u2019s attempt to convert from a non-profit to a for-profit while engaging in one of the biggest thefts in human history by expropriating most of the nonprofit\u2019s financial assets, worth hundreds of billions, for private gain. That opposition has very little to do with Elon Musk.</p>\n<blockquote>\n<p><a href=\"https://x.com/EmilyDreyfuss/status/1962904352764670284\">Emily Dreyfuss</a>: Inside OpenAI, there&#8217;s a growing paranoia that some of its loudest critics are being funded by Elon Musk and other billionaire competitors. Now, they are going after these nonprofit groups, but their evidence of a vast conspiracy is often extremely thin.</p>\n<p><a href=\"https://sfstandard.com/2025/09/02/openai-sam-altman-elon-musk-ai-regulation/\">Emily Shugerman</a> (SF Standard): Nathan Calvin, who joined Encode in 2024, two years after graduating from Stanford Law School, was being subpoenaed by OpenAI. \u201cI was just thinking, \u2018Wow, they&#8217;re really doing this,\u2019\u201d he said. \u201c\u2018This is really happening.\u2019\u201d</p>\n<p>The subpoena was filed as part of the ongoing lawsuits between Elon Musk and OpenAI CEO Sam Altman, in which Encode had filed an amicus brief supporting some of Musk&#8217;s arguments. It asked for any documents relating to Musk\u2019s involvement in the founding of Encode, as well as any communications between Musk, Encode, and Meta CEO Mark Zuckerberg, whom Musk <a href=\"https://fortune.com/2025/08/22/elon-musk-mark-zuckerberg-openai-takeover/\">reportedly</a> tried to involve in his OpenAI takeover bid in February.</p>\n<p>Calvin said the answer to these questions was easy: The requested documents didn\u2019t exist.</p>\n<p>\u2026</p>\n<p>In media interviews, representatives for an OpenAI-affiliated super PAC have described a \u201cvast force\u201d working to slow down AI progress and steal American jobs.</p>\n</blockquote>\n<p>This has long been the Obvious Nonsense a16z line, but now OpenAI is joining them via being part of the \u2018Leading the Future\u2019 super PAC. If this was merely Brockman contributing it would be one thing, but no, it\u2019s far beyond that:</p>\n<blockquote>\n<p>According to the<a href=\"https://www.wsj.com/politics/silicon-valley-launches-pro-ai-pacs-to-defend-industry-in-midterm-elections-287905b3\"> Wall Street Journal</a>, the PAC is in part the brainchild of Chris Lehane, OpenAI\u2019s vice president of global affairs.</p>\n</blockquote>\n<p>Meanwhile, OpenAI is treating everyone who opposes their transition to a for-profit as if they have to be part of this kind of vast conspiracy.</p>\n<blockquote>\n<p>Around the time Musk mounted his legal fight [against OpenAI\u2019s conversion to a for-profit], advocacy groups began to voice their opposition to the transition plan, too. Earlier this year, groups like the San Francisco Foundation, Latino Prosperity, and Encode organized <a href=\"https://notforprivategain.org/#names\">open</a> <a href=\"https://www.sff.org/Offsite-Media/Charitable-coalition-letter-on-OpenAI-conversion-1-29-25.pdf\">letters</a> to the California attorney general, demanding further questioning about OpenAI\u2019s move to a for-profit. One group, the Coalition for AI Nonprofit Integrity (CANI), helped write a California bill introduced in March that would have blocked the transition. (The assemblymember who introduced the bill suddenly gutted it less than a month later, saying the issue required further study.)</p>\n<p>In the ensuing months, OpenAI leadership seems to have decided that these groups and Musk were working in concert.</p>\n<p>Catherine Bracy: Based on my interaction with the company, it seems they\u2019re very paranoid about Elon Musk and his role in all of this, and it\u2019s become clear to me that that\u2019s driving their strategy</p>\n</blockquote>\n<p>No, these groups were not (as far as I or anyone else can tell) funded by or working in concert with Musk.</p>\n<p>The suspicions that Meta was involved, including in Encode which is attempting to push forward SB 53, are not simply paranoid, they flat out don\u2019t make any sense. Nor does the claim about Musk, either, given how he handles opposition:</p>\n<blockquote>\n<p>Both LASST and Encode have spoken out against Musk and Meta \u2014 the entities OpenAI is accusing them of being aligned with \u2014 and advocated against their aims: Encode recently filed a complaint with the FTC about Musk\u2019s AI company producing nonconsensual nude images; LASST has criticized the company for abandoning its structure as a public benefit corporation. Both say they have not taken money from Musk nor talked to him. \u201cIf anything, I\u2019m more concerned about xAi from a safety perspective than OpenAI,\u201d Whitmer said, referring to Musk\u2019s AI product.</p>\n</blockquote>\n<p>I\u2019m more concerned about OpenAI because I think they matter far more than xAI, but pound for pound xAI is by far the bigger menace acting far less responsibly, and most safety organizations in this supposed conspiracy will tell you that if you ask them, and act accordingly when the questions come up.</p>\n<blockquote>\n<p><a href=\"https://x.com/Miles_Brundage/status/1962960242825896190\">Miles Brundage</a>: First it was the EAs out to get them, now it\u2019s Elon.</p>\n<p>The reality is just that most people think we should be careful about AI</p>\n<p>(Elon himself is ofc actually out to get them, but most people who sometimes disagree with OpenAI have nothing to do with Elon, including Encode, the org discussed at the beginning of the article. And ironically, many effective altruists are more worried about Elon than OAI now)</p>\n</blockquote>\n<p>OpenAI\u2019s paranoia started with CANI, and then extended to Encode, and then to LASST.</p>\n<blockquote>\n<p>Nathan Calvin: \u200bThey seem to have a hard time believing that we are an organization of people who just, like, actually care about this.</p>\n<p>\u2026</p>\n<p><a href=\"https://sfstandard.com/2025/09/02/openai-sam-altman-elon-musk-ai-regulation/\">Emily Shugerman</a>: Lehane, who joined the company last year, is perhaps best known for coining the term \u201cvast right-wing conspiracy\u201d to dismiss the allegations against Bill Clinton during the Monica Lewinsky scandal \u2014 a line that seems to have seeped into Leading the Future\u2019s messaging, too.</p>\n<p>\u2026</p>\n<p>In a statement to the Journal, representatives from the PAC decried a \u201cvast force out there that\u2019s looking to slow down AI deployment, prevent the American worker from benefiting from the U.S. leading in global innovation and job creation, and erect a patchwork of regulation.\u201d&#8221;</p>\n</blockquote>\n<p>The hits keep coming as the a16z-level paranoid about EA being a \u2018vast conspiracy\u2019 kicks into high gear , such as the idea that Dustin Moskovitz doesn\u2019t care about AI safety, he\u2019s going after them because of his stake in Anthropic, can you possibly be serious right now, why do you think he invested in Anthropic.</p>\n<blockquote>\n<p>Of particular interest to OpenAI is the fact that both Omidyar and Moskovitz are investors in Anthropic \u2014 an OpenAI competitor that claims to produce safer, more steerable AI technology.</p>\n<p>\u2026</p>\n<p>Groups backed by competitors often present themselves as disinterested public voices or \u2018advocates\u2019, when in reality their funders hold direct equity stakes in competitors in their sector &#8211; in this case worth billions of dollars,\u201d she said. \u201cRegardless of all the rhetoric, their patrons will undoubtedly benefit if competitors are weakened.\u201d</p>\n</blockquote>\n<p>Never mind that Anthropic has not supported Moskovitz on AI regulation, and that the regulatory interventions funded by Moskovitz would constantly (aside from any role in trying to stop OpenAI\u2019s for-profit conversion) be bad for Anthropic\u2019s commercial outlook.</p>\n<blockquote>\n<p>Open Philanthropy (funded by Dustin Moskovitz): Reasonable people can disagree about the best guardrails to set for emerging technologies, but right now we\u2019re seeing an unusually brazen effort by some of the biggest companies in the world to buy their way out of any regulation they don\u2019t like. They\u2019re putting their potential profits ahead of U.S. national security and the interests of everyday people.</p>\n</blockquote>\n<p>Companies do this sort of thing all the time. This case is still very brazen, and very obvious, and OpenAI has now jumped into a16z levels of paranoia and bad faith between the lawfare, the funding of the new PAC and their letter on SB 53.</p>\n<p>Suing and attacking nonprofits engaging in advocacy is a new low. Compare that to the situation with Daniel Kokotajlo, where OpenAI to its credit once confronted with its bad behavior backed down rather than going on a legal offensive.</p>\n<blockquote>\n<p><a href=\"https://x.com/DKokotajlo/status/1963040166744097116\">Daniel Kokotajlo</a>: Having a big corporation come after you legally, even if they are just harassing you and not trying to actually get you imprisoned, must be pretty stressful and scary. (I was terrified last year during the nondisparagement stuff, and that was just the fear of what *might* happen, whereas in fact OpenAI backed down instead of attacking) I&#8217;m glad these groups aren&#8217;t cowed.</p>\n</blockquote>\n\n\n<h4 class=\"wp-block-heading\">Is The OpenAI Paranoia Real?</h4>\n\n\n<p>As in, do OpenAI and Sam Altman believe these false paranoid conspiracy theories?</p>\n<p>I have long wondered the same thing about Marc Andreessen and a16z, and others who say there is a \u2018vast conspiracy\u2019 out there by which they mean Effective Altruism (EA), or when they claim it\u2019s all some plot to make money.</p>\n<p>I mean, these people are way too smart and knowledgeable to actually believe that, asks Padme, right? And certainly Sam Altman and OpenAI have to know better.</p>\n<p>Wouldn\u2019t the more plausible theory be that these people are simply lying? That Lehane doesn\u2019t believe in a \u2018vast EA conspiracy\u2019 any more than he believed in a \u2018vast right-wing conspiracy\u2019 when he coined the term \u2018vast right-wing conspiracy\u2019 about the (we now know very true) allegations around Monica Lewinsky. It\u2019s an op. It\u2019s rhetoric. It\u2019s people saying what they think will work to get them what they want. It\u2019s not hard to make that story make sense.</p>\n<p>Then again, maybe they do really believe it, or at least aren\u2019t sure? People often believe genuinely crazy things that do not in any way map to reality, especially once politics starts to get involved. And I can see how going up against Elon Musk and being engaged in one the biggest heists in human history in broad daylight, while trying to build superintelligence that poses existential risks to humanity that a lot of people are very worried about and that also will have more upside than anything ever, could combine to make anyone paranoid. Highly understandable and sympathetic.</p>\n<p>Or, of course, they could have been talking to their own AIs about these questions. I hear there are some major sycophancy issues there. One must be careful.</p>\n<p>I sincerely hope that those involved here are lying. It beats the alternatives.</p>\n\n\n<h4 class=\"wp-block-heading\">The District Attorneys Are Not Happy With OpenAI</h4>\n\n\n<p>It seems that OpenAI\u2019s failures on sycophancy and dealing with suicidality <a href=\"https://sfstandard.com/2025/09/05/openai-chatgpt-safety-concerns-state-ags/\">might endanger its relationship with those who must approve</a> its attempted restructuring into a for-profit, also known as one of the largest attempted thefts in human history?</p>\n<p>Maybe they will take OpenAI\u2019s charitable mission seriously after all, at least in this way, despite presumably not understanding the full stakes involved and having the wrong idea about what kind of safety matters?</p>\n<blockquote>\n<p><a href=\"https://x.com/GarrisonLovely/status/1964035522084687899\">Garrison Lovely</a>: Scorching new letter from CA and DE AGs to OpenAI, who each have the power to block the company&#8217;s restructuring to loosen nonprofit controls.</p>\n<p>They are NOT happy about the recent teen suicide and murder-suicide that followed prolonged and concerning interactions with ChatGPT.</p>\n<p>Rob Bonta (California Attorney General) and Kathleen Jennings (Delaware Attorney General) <a href=\"https://t.co/o1hBNd3UV4\">in a letter</a>: In our meeting, we conveyed in the strongest terms that safety is a non-negotiable priority, especially when it comes to children. Our teams made additional requests about OpenAI\u2019s current safety precautions and governance. We expect that your responses to these will be prioritized and that immediate remedial measures are being taken where appropriate.</p>\n<p>We recognize that OpenAI has sought to position itself as a leader in the AI industry on safety. Indeed, OpenAI has publicly committed itself to build safe AGI to benefit all humanity, including children. And before we get to benefiting, we need to ensure that adequate safety measures are in place to not harm.</p>\n<p><strong>It is our shared view that OpenAI and the industry at large are not where they need to be in ensuring safety in AI products\u2019 development and deployment. As Attorneys General, public safety is one of our core missions. As we continue our dialogue related to OpenAI\u2019s recapitalization plan, we must work to accelerate and amplify safety as a governing force in the future of this powerful technology.</strong></p>\n<p>The recent deaths are unacceptable. They have rightly shaken the American public\u2019s confidence in OpenAI and this industry. OpenAI \u2013 and the AI industry \u2013 must proactively and transparently ensure AI\u2019s safe deployment. <strong>Doing so is mandated by OpenAI\u2019s charitable mission, and will be required and enforced by our respective offices.</strong></p>\n<p>We look forward to hearing from you and working with your team on these important issues.</p>\n</blockquote>\n<p><a href=\"https://x.com/_NathanCalvin/status/1964757835734188409\">Some other things said by the AGs</a>:</p>\n<blockquote>\n<p><a href=\"https://sfstandard.com/2025/09/05/openai-chatgpt-safety-concerns-state-ags/\">Bonta</a>: We were looking for a rapid response. They\u2019ll know what that means, if that\u2019s days or weeks. I don\u2019t see how it can be months or years.</p>\n<p>All antitrust laws apply, all consumer protection laws apply, all criminal laws apply. We are not without many tools to regulate and prevent AI from hurting the public and the children.</p>\n</blockquote>\n\n\n<h4 class=\"wp-block-heading\">OpenAI Responds With Parental Controls</h4>\n\n\n<p>With a lawsuit filed that OpenAI might well lose and the the two attorney generals that can veto its restructuring breathing down OpenAI\u2019s neck, <a href=\"https://openai.com/index/helping-people-when-they-need-it-most/\">OpenAI is promising various fixes</a> and in particular <a href=\"https://time.com/7314210/openai-chatgpt-parental-controls/\">OpenAI has decided it is time for parental controls</a> <a href=\"https://openai.com/index/building-more-helpful-chatgpt-experiences-for-everyone/\">as soon as they </a>can, which should be within a month.</p>\n<p>Their first announcement on August 26 included these plans:</p>\n<blockquote>\n<p>OpenAI: While our initial mitigations prioritized acute self-harm, some people experience other forms of mental distress. For example, someone might enthusiastically tell the model they believe they can drive 24/7 because they realized they\u2019re invincible after not sleeping for two nights. Today, ChatGPT may not recognize this as dangerous or infer play and\u2014by curiously exploring\u2014could subtly reinforce it.</p>\n<p>We are working on an update to GPT\u20115 that will cause ChatGPT to de-escalate by grounding the person in reality. In this example, it would explain that sleep deprivation is dangerous and recommend rest before any action<strong>.</strong></p>\n</blockquote>\n<p>Better late than never on that one, I suppose. That is indeed why I am relatively not so worried about problems like this, we can adjust after things start to go wrong.</p>\n<blockquote>\n<p>OpenAI: In addition to emergency services, we\u2019re exploring ways to make it easier for people to reach out to those closest to them. This could include one-click messages or calls to saved emergency contacts, friends, or family members with suggested language to make starting the conversation less daunting.</p>\n<p>We\u2019re also considering features that would allow people to opt-in for ChatGPT to reach out to a designated contact on their behalf in severe cases.</p>\n<p>We will also soon introduce parental controls that give parents options to gain more insight into, and shape, how their teens use ChatGPT. We\u2019re also exploring making it possible for teens (with parental oversight) to designate a trusted emergency contact. That way, in moments of acute distress, ChatGPT can do more than point to resources: it can help connect teens directly to someone who can step in.</p>\n</blockquote>\n<p>On September 2 they followed up with additional information about how they are \u2018<a href=\"https://openai.com/index/building-more-helpful-chatgpt-experiences-for-everyone/\">partnering with experts</a>\u2019 and providing more details.</p>\n<blockquote>\n<p>OpenAI: Earlier this year, we began building more ways for families to use ChatGPT together and decide what works best in their home. <strong>Within the next month,</strong> parents will be able to:</p>\n<ul>\n<li>Link their account with their teen\u2019s account (minimum age of 13) through a simple email invitation.</li>\n<li>Control how ChatGPT responds to their teen with age-appropriate model behavior rules, which are on by default.</li>\n<li>Manage which features to disable, including memory and chat history.</li>\n<li>Receive notifications when the system detects their teen is in a moment of acute distress. Expert input will guide this feature to support trust between parents and teens.</li>\n</ul>\n<p>These controls add to features we have rolled out for all users including in-app reminders during long sessions to encourage breaks.</p>\n</blockquote>\n<p>Parental controls seem like an excellent idea.</p>\n<p>I would consider most of this to effectively be \u2018on by default\u2019 already, for everyone, in the sense that AI models have controls against things like NSFW content that largely treat us all like teens. You could certainly tighten them up more for an actual teen, and it seems fine to give parents the option, although mostly I think you\u2019re better off not doing that.</p>\n\n\n<h4 class=\"wp-block-heading\">I Spy With My AI</h4>\n\n\n<p>The big new thing is the notification feature. That is a double edged sword. As I\u2019ve discussed previously, an AI or other source of help that can \u2018rat you out\u2019 to authorities, even \u2018for your own good\u2019 or \u2018in moments of acute distress\u2019 is inherently very different from a place where your secrets are safe. There is a reason we have confidentiality for psychologists and lawyers and priests, and balancing when to break that is complicated.</p>\n<p>Given an AI\u2019s current level of reliability and its special role as a place free from human judgment or social consequence, I am actually in favor of it outright never altering others without an explicit user request to do so.</p>\n<p>Whereas things are moving in the other direction, with predictable results.</p>\n<p>As in, OpenAI is already scanning your chats as per their posts I discussed above.</p>\n<blockquote>\n<p><a href=\"https://x.com/gregisenberg/status/1964675896926765112\">Greg Isenberg</a>: ChatGPT is potentially leaking your private convos to the police.</p>\n<p>People use ChatGPT because it feels like talking to a smart friend who won&#8217;t judge you. <a href=\"https://futurism.com/openai-scanning-conversations-police\">Now, people are realizing it&#8217;s more like talking to a smart friend who might snitch.</a></p>\n<p>This is the same arc we saw in social media: early excitement, then paranoia, then demand for smaller, private spaces.</p>\n<p>OpenAI (including as <a href=\"https://futurism.com/openai-scanning-conversations-police\">quoted by Futurism</a>): When we detect users who are planning to harm others, we route their conversations to specialized pipelines where they are reviewed by a small team trained on our usage policies and who are authorized to take action, including banning accounts.</p>\n<p>If human reviewers determine that a case involves an imminent threat of serious physical harm to others, we may refer it to law enforcement.</p>\n<p>We are currently not referring self-harm cases to law enforcement to respect people\u2019s privacy given the uniquely private nature of ChatGPT interactions.</p>\n<p>Futurism: When describing its rule against &#8220;harm [to] yourself or others,&#8221; the company listed off some pretty standard examples of prohibited activity, including using ChatGPT &#8220;to promote suicide or self-harm, develop or use weapons, injure others or destroy property, or engage in unauthorized activities that violate the security of any service or system.&#8221;</p>\n</blockquote>\n<p>They are not directing self-harm cases to protect privacy, but harm to others is deemed different. That still destroys the privacy of the interaction. And \u2018harm to others\u2019 could rapidly morph into any number of places, both with false positives and also with changes in ideas about what constitutes \u2018harm.\u2019</p>\n<p>They\u2019re not even talking about felonies or imminent physical harm. They\u2019re talking about \u2018engage in unauthorized activities that violate the security of any service or system,\u2019 or \u2018destroy property,\u2019 so this could potentially extend quite far, and in places that seem far less justified than intervening in response a potentially suicidal user. These are circumstances in which typical privileged communication would hold.</p>\n<p>I very much do not like where that is going, and if I heard reports this was happening on the regular it would fundamentally alter my relationship to ChatGPT, even though I \u2018have nothing to hide.\u2019</p>\n<p>What\u2019s most weird about this is that OpenAI was recently advocating for \u2018AI privilege.\u2019</p>\n<blockquote>\n<p><a href=\"https://x.com/Rahll/status/1963309091419234313\">Reid Southern</a>: OpenAI went from warning users that there&#8217;s no confidentiality when using ChatGPT, and calling for &#8220;AI privilege&#8221;, to actively scanning your messages to send to law enforcement, seemingly to protect themselves in the aftermath of the ChatGPT induced murder-suicide</p>\n<div>\n<figure>\n<div>\n\n\n<figure class=\"wp-block-image\"><img alt=\"\" src=\"https://substackcdn.com/image/fetch/$s_!5t8y!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F627958ff-4d16-42e0-b0b4-173c88ae87ff_1200x819.jpeg\" /></figure>\n\n\n<div></div>\n</div>\n</figure>\n</div>\n</blockquote>\n<p>This is partially a case of \u2018if I\u2019m not legally forbidden to do [X] then I will get blamed for not doing [X] so please ban me from doing it\u2019 so it\u2019s not as hypocritical as it sounds. It is still rather hypocritical and confusing to escalate like this. Why respond to suicides by warning you will be scanning for harm to others and intent to impact the security of systems, but definitely not acting if someone is suicidal?</p>\n<p>If you think AI users deserve privilege, and I think this is a highly reasonable position, then act like it. Set a good example, set a very high bar for ratting, and confine alerting human reviewers let alone the authorities to when you catch someone on the level of trying to make a nuke or a bioweapon, or at minimum things that would force a psychologist to break privilege. It\u2019s even good for business.</p>\n<p><a href=\"https://www.reddit.com/r/singularity/comments/1n5mame/people_are_furious_that_openai_is_reporting/\">Otherwise</a> <a href=\"https://x.com/ai_for_success/status/1962679920263602243\">people</a> are indeed going to get furious, and there will be increasing demand to run models locally or in other ways that better preserve privacy. There\u2019s not zero of that already, but it would escalate quickly.</p>\n\n\n<h4 class=\"wp-block-heading\">Some People Still Think OpenAI Is Some Sort Of Safety Company?</h4>\n\n\n<p><a href=\"https://x.com/steve47285/status/1961115721129214314\">Steven Byrnes notes the weirdness of seeing Ben\u2019s essay describe OpenAI as an \u2018AI safety company</a>\u2019 rather than a company most AI safety folks hate with a passion.</p>\n<blockquote>\n<p>Steven Byrnes: I can\u2019t even describe how weird it is to hear OpenAI, as a whole, today in 2025, being described as an AI safety company. Actual AI safety people HATE OPENAI WITH A PASSION, almost universally. The EA people generally hate it. The Rationalists generally hate it even more.</p>\n<p>AI safety people have protested at the OpenAI offices with picket signs &amp; megaphones! When the board fired Sam Altman, everyone immediately blamed EA &amp; AI safety people! OpenAI has churned through AI safety staff b/c they keep quitting in protest! \u2026What universe is this?</p>\n<p>Yes, many AI safety people are angry about OpenAI being cavalier &amp; dishonest about harm they might cause in the future, whereas you are angry about OpenAI being cavalier &amp; dishonest about harm they are causing right now. That doesn\u2019t make us enemies. \u201cWhy not both?\u201d</p>\n</blockquote>\n<p>I think that\u2019s going too far. It\u2019s not good to hate with a passion.</p>\n<p>Even more than that, you could do so, so much worse than OpenAI on all of these questions (e.g. Meta, or xAI, or every major Chinese lab, basically everyone except Anthropic or Google is worse).</p>\n<p>Certainly we think OpenAI is on net not helping and deeply inadequate to the task, their political lobbying and rhetoric is harmful, and their efforts have generally made the world a lot less safe. They still are doing a lot of good work, making a lot of good decisions, and I believe that Altman is normative, that he is far more aware of what is coming and the problems we will face than most or than he currently lets on.</p>\n<p>I believe he is doing a much better job on these fronts than most (but not all) plausible CEOs of OpenAI would do in his place. For example, if OpenAI\u2019s CEO of Applications Fidji Simo were in charge, or Chairman of the Board Bret Taylor were in charge, or Greg Brockman was in charge, or the CEO of any of the magnificent seven were in charge, I would expect OpenAI to act far less responsibly.</p>\n<p>Thus I consider myself relatively well-inclined towards OpenAI among those worried about AI or advocating or AI safety.</p>\n<p>I still have an entire series of posts about how terrible things have been at OpenAI and a regular section about them called \u2018The Mask Comes Off.\u2019</p>\n<p>And I find myself forced to update my view importantly downward, towards being more concerned, in the wake of the recent events described in this post. OpenAI is steadily becoming more of a bad faith actor in the public sphere.</p>"
            ],
            "link": "https://thezvi.wordpress.com/2025/09/08/openai-14-openai-descends-into-paranoia-and-bad-faith-lobbying/",
            "publishedAt": "2025-09-08",
            "source": "TheZvi",
            "summary": "I am a little late to the party on several key developments at OpenAI: OpenAI\u2019s Chief Global Affairs Officer Chris Lehane was central to the creation of the new $100 million PAC where they will partner with a16z to oppose &#8230; <a href=\"https://thezvi.wordpress.com/2025/09/08/openai-14-openai-descends-into-paranoia-and-bad-faith-lobbying/\">Continue reading <span class=\"meta-nav\">&#8594;</span></a>",
            "title": "OpenAI #14: OpenAI Descends Into Paranoia and Bad Faith Lobbying"
        },
        {
            "content": [],
            "link": "https://xkcd.com/3139/",
            "publishedAt": "2025-09-08",
            "source": "XKCD",
            "summary": "<img alt=\"The draw-by-repetition rule does a good job of keeping players from sliding a tile back and forth repeatedly, but the tiles definitely introduce some weird en passant and castling edge cases.\" src=\"https://imgs.xkcd.com/comics/chess_variant.png\" title=\"The draw-by-repetition rule does a good job of keeping players from sliding a tile back and forth repeatedly, but the tiles definitely introduce some weird en passant and castling edge cases.\" />",
            "title": "Chess Variant"
        }
    ],
    "lookbackDays": 1,
    "publishDate": "2025-09-08"
}