{
    "articles": [
        {
            "content": [],
            "link": "https://olano.dev/blog/fallacies-of-code-generation",
            "publishedAt": "2025-04-13",
            "source": "Facundo Olano",
            "summary": "Writing code is the hardest part of software development.",
            "title": "Fallacies of Code Generation"
        },
        {
            "content": [
                "<img alt=\"I dream about AI subagents; they whisper to me while I&apos;m asleep\" src=\"https://ghuntley.com/content/images/2025/04/A-traditional-tattoo-art-print-of-AI-subagents--computer-programming-agents--a-butler-for-an-AI-agent--symbolic-imagery--vibrant-colors--retro-flair--complex-ornamental-details--white-background--diffused-shadows--bold.jpg\" /><p>In a <a href=\"https://ghuntley.com/redlining/\" rel=\"noreferrer\">previous post</a>, I shared about &quot;real context window&quot; sizes and &quot;advertised context window sizes&quot;</p><blockquote>Claude 3.7&#x2019;s advertised context window is 200k, but I&apos;ve noticed that the quality of output clips at the 147k-152k mark. Regardless of which agent is used, when clipping occurs, <a href=\"https://ghuntley.com/mcp\" rel=\"noreferrer\">tool call to tool call invocation</a> starts to fail</blockquote><p>The short version is that we are in another era of &quot;640kb should be enough for anyone,&quot; and folks need to start thinking about how the current generation of context windows is similar to RAM on a computer in the 1980s until such time that <code>DOS=HIGH,UMB</code> becomes a thing...</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img alt=\"I dream about AI subagents; they whisper to me while I&apos;m asleep\" class=\"kg-image\" height=\"1443\" src=\"https://ghuntley.com/content/images/2025/04/image-6.png\" width=\"1920\" /><figcaption><span style=\"white-space: pre-wrap;\">LLM context windows are like RAM in an IBM 8086 XT and are a precious resource, but engineers and developer tooling companies do not treat them as such. </span></figcaption></figure><p>The current generation of coding agents work via a tight <a href=\"https://ghuntley.com/mcp/\" rel=\"noreferrer\">evaluation loop of tool calls to tool calls</a> that operate within a single context window (ie. RAM). However, the problem with this design is that when an LLM provides a bad outcome, the coding assistants/agents&apos; death spiral and brute force on the main context window which consumes precious resources as it tries to figure out the next steps.</p><figure class=\"kg-card kg-image-card kg-width-full kg-card-hascaption\"><img alt=\"I dream about AI subagents; they whisper to me while I&apos;m asleep\" class=\"kg-image\" height=\"1730\" src=\"https://ghuntley.com/content/images/2025/04/Untitled-diagram-2025-04-13-010340.png\" width=\"2000\" /><figcaption><span style=\"white-space: pre-wrap;\">the current generation of software development agents works like this. it&apos;s not great (tm)</span></figcaption></figure><p> However, I&apos;ve been thinking: What if an agent could spawn a new agent and clone the context window? If such a thing were possible, it would enable an agent to spawn a sub-agent. The main agent would pause, wait for the sub-agent to burn through its own context window (ie. SWAP), and then provide concrete next steps for the primary agent.</p><figure class=\"kg-card kg-image-card kg-width-full kg-card-hascaption\"><img alt=\"I dream about AI subagents; they whisper to me while I&apos;m asleep\" class=\"kg-image\" height=\"1493\" src=\"https://ghuntley.com/content/images/2025/04/Untitled-diagram-2025-04-13-004708-1.png\" width=\"2000\" /><figcaption><span style=\"white-space: pre-wrap;\">i suspect next generation agents will look something like this under the hood</span></figcaption></figure><p></p><p>It&apos;s theoretical right now, and I haven&apos;t looked into it. Still, I dream of the possibility that in the future, software development agents will not waste precious context (RAM) and enter a death spiral on the main thread.</p><h2 id=\"ps-socials\">p.s. socials</h2><ul><li>LinkedIn: <a href=\"https://www.linkedin.com/posts/geoffreyhuntley_i-dream-about-ai-subagents-they-whisper-activity-7316994087725813762-YuJX?utm_source=share&amp;utm_medium=member_desktop&amp;rcm=ACoAAABQKuUB2AJ059keUcRUVLbtmoa6miLVlTI\">https://www.linkedin.com/posts/geoffreyhuntley_i-dream-about-ai-subagents-they-whisper-activity-7316994087725813762-YuJX</a></li><li>Twitter: <a href=\"https://x.com/GeoffreyHuntley/status/1911237235644809466?ref=ghuntley.com\">https://x.com/GeoffreyHuntley/status/1911237235644809466</a></li><li>BlueSky: <a href=\"https://bsky.app/profile/ghuntley.com/post/3lmnv43hmk32d?ref=ghuntley.com\">https://bsky.app/profile/ghuntley.com/post/3lmnv43hmk32d</a></li></ul><h3 id=\"pps-extra-reading\">pps. extra reading</h3><ul><li><a href=\"https://rahulpandita.github.io/files/riniICSE2019.pdf?ref=ghuntley.com\" rel=\"noreferrer\">Latent Patterns in Activities: A Field Study of How<br />Developers Manage Context</a></li></ul><figure class=\"kg-card kg-bookmark-card kg-card-hascaption\"><a class=\"kg-bookmark-container\" href=\"https://blog.sshh.io/p/building-multi-agent-systems?ref=ghuntley.com\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">Building Multi-Agent Systems</div><div class=\"kg-bookmark-description\">Scaling LLM-based agents to handle complex problems reliably.</div><div class=\"kg-bookmark-metadata\"><img alt=\"I dream about AI subagents; they whisper to me while I&apos;m asleep\" class=\"kg-bookmark-icon\" src=\"https://ghuntley.com/content/images/icon/icon.svg\" /><span class=\"kg-bookmark-author\">Shrivu&#x2019;s Substack</span><span class=\"kg-bookmark-publisher\">Shrivu Shankar</span></div></div><div class=\"kg-bookmark-thumbnail\"><img alt=\"I dream about AI subagents; they whisper to me while I&apos;m asleep\" src=\"https://ghuntley.com/content/images/thumbnail/https-3A-2F-2Fsubstack-post-media.s3.amazonaws.com-2Fpublic-2Fimages-2F659040b8-af1b-46c9-b035-7644b6a67f38_1024x1024.jpeg\" /></div></a><figcaption><p><span style=\"white-space: pre-wrap;\">&quot;You see this [breakdown] a lot even in non-coding agentic systems where a single agent just starts to break down at some point.&quot; - Shrivu Shankar</span></p></figcaption></figure>"
            ],
            "link": "https://ghuntley.com/subagents/",
            "publishedAt": "2025-04-13",
            "source": "Geoffrey Huntley",
            "summary": "<p>In a <a href=\"https://ghuntley.com/redlining/\" rel=\"noreferrer\">previous post</a>, I shared about &quot;real context window&quot; sizes and &quot;advertised context window sizes&quot;</p><blockquote>Claude 3.7&#x2019;s advertised context window is 200k, but I&apos;ve noticed that the quality of output clips at the 147k-152k mark. Regardless of which agent is</blockquote>",
            "title": "I dream about AI subagents; they whisper to me while I'm asleep"
        }
    ],
    "lookbackDays": 1,
    "publishDate": "2025-04-13"
}