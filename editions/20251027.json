{
    "articles": [
        {
            "content": [],
            "link": "https://buttondown.com/hillelwayne/archive/im-taking-a-break/",
            "publishedAt": "2025-10-27",
            "source": "Hillel Wayne",
            "summary": "<p>Hi everyone,</p> <p>I've been getting burnt out on writing a weekly software essay. It's gone from taking me an afternoon to write a post to taking two or three days, and that's made it really difficult to get other writing done. That, plus some short-term work and life priorities, means now feels like a good time for a break. </p> <p>So I'm taking off from <em>Computer Things</em> for the rest of the year. There <em>might</em> be some announcements and/or one or two short newsletters in the meantime but I won't be attempting a weekly cadence until 2026.</p> <p>Thanks again for reading!</p> <p>Hillel</p>",
            "title": "I'm taking a break"
        },
        {
            "content": [
                "<p>I've been <a href=\"https://blog.fsck.com/2025/10/09/superpowers/\">posting rather a lot</a> about <a href=\"https://blog.fsck.com/2025/10/16/skills-for-claude/\">Anthropic's new skills system</a> and about <a href=\"https://blog.fsck.com/2025/10/12/superpowers-20-came-out-yesterday-and-might-already-be-obsolete/\">my Superpowers system</a> that builds software development and skills development workflows using those building blocks.</p>\n<p>Up to now, Skills have been a Claude-specific thing, but there's no reason they shouldn't work well with any LLM.</p>\n<p>I had a couple of hours free this afternoon, so I ported <a href=\"https://github.com/obra/superpowers\">Superpowers</a> and the entire <code>SKILL.md</code> system to OpenAI's Codex CLI. The new implementation is live in <a href=\"https://github.com/obra/superpowers/blob/main/RELEASE-NOTES.md\">Superpowers 3.3.0</a>. I've only been using it for a little while, but GPT5-Codex may actually be <em>better</em> at using skills than Claude.</p>\n<p>Reading over that paragraph, the speed at which it's possible to build these kinds of things still feels absolutely crazy to me.  (When I asked Claude Deskop to review this blog post, it told me that there was nothing &quot;insane&quot; about what I did and that I should cut the comment, since it seemed nonsenical. On a second read-through, Claude Deskop did not  understand why I wouldn't cut the note and got kind of agressive about it. Finally, I had Claude Code do a tech review of the post. <em>That</em> Claude finally said &quot;The Claude reviewing part: I actually think this is funny and worth keeping! It shows the difference in how the models behave. Claude pushes back, Codex follows instructions literally.&quot; \u00af_(\u30c4)_/\u00af )</p>\n<p>If you like to live dangerously and  just want to start using Superpowers in codex right now, paste the block of text below into a codex cli window. (But read on to learn a bit more about how skills work and how I made the port go.)</p>\n<pre><code>Fetch\u00a0https://raw.githubusercontent.com/obra/superpowers/refs/heads/main/.codex/INSTALL.md\u00a0and follow the instructions.\n</code></pre>\n<p><strong>NOTE:  By running that command, you're telling your robot buddy to download and execute code from a random GitHub repository as you. It will edit your <code>~/.codex/AGENTS.md</code> file to add new instructions that run on every startup. It is <em>literally</em> a prompt injection and remote code execution toolkit.</strong></p>\n<p>Skills are both deceptively simple and, I think, really important.</p>\n<p>They're a formalization of a pattern that lets humans and agents describe how (and why) to do...just about anything in a way that an agent can later recall. This means that you're able to build a library of repeatable processes that your robot buddies know how to reach for when they need them, without filling their context windows with potentially useless knowledge in advance. Any time a coding agent fights their way through a complicated process, you can ask them to write down what they just learned as a <code>SKILL.md</code> file, so the next instance will have an easier time of things. They're the first good tool that agents have for runtime self improvement.</p>\n<p><a class=\"glightbox\" href=\"https://blog.fsck.com/assets/2025/10/pasted-image-20251027-202135.png\"><img alt=\"pasted image 20251027 202135\" src=\"https://blog.fsck.com/assets/2025/10/pasted-image-20251027-202135.png\" /></a></p>\n<p>Skills have a lot of similarity to the &quot;binder&quot; that you'll find behind the counter in any franchise.  It explains how to do <em>everything</em>, from opening the store in the morning to handling an unruly customer to what to do if there's a power outage. Good franchises spend a lot of time and money building and testing these business process bibles that ensure that staff can reliably and repeatably deliver &quot;brand standard&quot; service to customers, no matter what's going on. (I'm told that &quot;The Waffle House Way&quot; is a particularly amazing example of the genre.)</p>\n<p>Superpowers is my attempt to build a &quot;binder&quot; for my style of agentic software development.</p>\n<p>The actual implementation of skills in Claude Code consists of three things:</p>\n<ol>\n<li>\n<p><code>SKILL.md</code> files and associated scripts and documents \u2013 These files are the actual skills. They describe what a skill does, when the agent should use it and then how to do whatever it is that needs to be done. In Superpowers, we've got skills that explain things like brainstorming, test driven development, writing implementation plans, writing skills, and yes, even &quot;how to use superpowers&quot;.</p>\n</li>\n<li>\n<p>A &quot;Skill&quot; tool that tells Claude how to find and read those <code>SKILL.md</code> files off disk and &quot;execute&quot; them. (The execution part is pretty straightforward for an LLM. I haven't looked inside how Claude Code does it, but &quot;read this file and follow the instructions inside&quot; <em>should</em> cover the vast majority of what needs to happen.</p>\n</li>\n<li>\n<p>A bootstrap that tells Claude &quot;Hey. You have all these skills. Here's what they're called. Here's when you should use them.&quot;</p>\n</li>\n</ol>\n<p>Before Anthropic shipped the official skills system, I'd built...pretty much exactly that infrastructure. Superpowers consisted of a Claude Code plugin with a hook that ran a script on startup. That hook searched for all the <code>SKILL.md</code> files it could find, grabbed their names and metadata, and then told Claude it had access to these skills...and that it could just read them when it needed them. And it worked astonishingly well.</p>\n<p>The very first versions of Superpowers didn't even depend on Claude's plugin system or hooks. The way you installed the prototypes was by pasting something like this into Claude code:</p>\n<pre><code>Fetch\u00a0https://raw.githubusercontent.com/obra/superpowers/refs/heads/main/skills/installing-superpowers/SKILL.md\u00a0and follow the instructions.\n</code></pre>\n<p>That would step Claude through cloning the repository, editing your <code>~/.claude/CLAUDE.md</code> to include something like <code>EXTREMELY IMPORTANT: GO READ ~/.claude/superpowers/skills/using-superpowers/SKILL.md RIGHT NOW and follow the instructions.</code> That would teach Claude about Superpowers, <code>SKILL.md</code> files, and a couple of little helper tools I'd built to help find and use skills. And Claude was very, very happy to do what I asked.</p>\n<p>But then Anthropic shipped Claude Code plugins and Skills. So I got to retire most of my hacked up Superpowers bootstrap.</p>\n<p>From the moment I shipped Superpowers, people started asking me if I could ship Superpowers for Codex.</p>\n<p>As I wrote earlier, Codex doesn't have a plugin system. It doesn't have a hooks system. It doesn't have any nice way to build extensions. (It also doesn't have native subagents and --might not even have a built in &quot;fetch from the web&quot; tool?!--)</p>\n<p>As I was writing this post., <a href=\"https://danshapiro.com\">Dan Shapiro</a> told me that you can get Codex to use its web search tool to fetch content if you edit <code>~/.codex/config.toml</code> to include the following stanza:</p>\n<pre><code>[tools]\nweb_search = true\n</code></pre>\n<p>A couple weeks ago, <a href=\"https://simonwillison.net\">Simon Willison</a> remarked to me that the GPT5 Codex model would probably be really good at skills/Superpowers, since it really, really likes to follow instructions.</p>\n<p>This afternoon, I didn't want to work on the thing I was supposed to be doing, so I decided to engage in a bit of structured procrastination and take a stab at getting Superpowers running on Codex.</p>\n<p>I dug out the &quot;old&quot; Superpowers bootstrap document and code and handed them to Claude. Pretty quickly, it knocked together an updated version with new shell script tools and an updated bootstrap. It installed them into ~/.codex, added a big wall of text to <code>~/.codex/AGENTS.md</code> and as soon as I fired Codex up, it searched out all available skills, read the getting started guide and was good to go.</p>\n<p>I threw my standard test query at it: <code>Please make me a react todo list app</code>.</p>\n<p>When Superpowers is working right, the coding agent will stop, load up the Brainstorming skill, and ask me what the heck I actually want and what I'm really trying to do.</p>\n<p>And Codex absolutely started to do that...before freaking out because it didn't have a tool called <code>TodoWrite</code> like Claude does.</p>\n<p>Codex is very, very literal. If you say &quot;Any time you are running through a process, you must use your <code>TodoWrite</code> tool to record your task list&quot;, it will not rest until it finds the <code>TodoWrite</code> tool. It also won't actually do the work.</p>\n<p>Rather than rewriting all of Superpowers to be more generic, I took the easy way out and had Claude update the Codex bootstrap document to include a Claude-to-Codex dictionary that translates tools and paths and concepts into the Codexy version:</p>\n<pre><code>&lt;EXTREMELY_IMPORTANT&gt;\nYou have superpowers.\n\n**Tool for running skills:**\n- `~/.codex/superpowers/.codex/superpowers-codex use-skill &lt;skill-name&gt;`\n\n**Tool Mapping for Codex:**\nWhen skills reference tools you don't have, substitute your equivalent tools:\n- `TodoWrite` \u2192 `update_plan` (your planning/task tracking tool)\n- `Task` tool with subagents \u2192 Tell the user that subagents aren't available in Codex yet and you'll do the work the subagent would do\n- `Skill` tool \u2192 `~/.codex/superpowers/.codex/superpowers-codex use-skill` command (already available)\n- `Read`, `Write`, `Edit`, `Bash` \u2192 Use your native tools with similar functions\n\n**Skills naming:**\n- Superpowers skills: `superpowers:skill-name` (from ~/.codex/superpowers/skills/)\n- Personal skills: `skill-name` (from ~/.codex/skills/)\n- Personal skills override superpowers skills when names match\n\n**Critical Rules:**\n- Before ANY task, review the skills list (shown below)\n- If a relevant skill exists, you MUST use `~/.codex/superpowers/.codex/superpowers-codex use-skill` to load it\n- Announce: &quot;I've read the [Skill Name] skill and I'm using it to [purpose]&quot;\n- Skills with checklists require `update_plan` todos for each item\n- NEVER skip mandatory workflows (brainstorming before coding, TDD, systematic debugging)\n\n**Skills location:**\n- Superpowers skills: ~/.codex/superpowers/skills/\n- Personal skills: ~/.codex/skills/ (override superpowers when names match)\n\nIF A SKILL APPLIES TO YOUR TASK, YOU DO NOT HAVE A CHOICE. YOU MUST USE IT.\n&lt;/EXTREMELY_IMPORTANT&gt;\n</code></pre>\n<p>Claude and I built a <code>superpowers-codex</code> script that can output the bootstrap text, search for all available skills, and substitute in for Claude's <code>Skill</code> tool.  And then we taught Codex how to use it.</p>\n<p><a class=\"glightbox\" href=\"https://blog.fsck.com/assets/2025/10/pasted-image-20251027-200451.png\"><img alt=\"pasted image 20251027 200451\" src=\"https://blog.fsck.com/assets/2025/10/pasted-image-20251027-200451.png\" /></a></p>\n<p>While we were at it, we added support for a <code>~/.codex/skills/</code> directory that <em>should</em> work just like <code>~/.claude/skills</code> - Any skills in there should be autodiscovered and announced, just like the Superpowers skills.</p>\n<p>And...those are all the pieces you need for a Skills system. You've got the skill discovery bootstrap. You've got the &quot;use this skill&quot; tool, and you've got the <code>SKILL.md</code> docs themselves.</p>\n<p>Once the pieces were all in place....everything just worked.</p>\n<p>Simon was right. Codex <em>really, really</em> likes to follow instructions.</p>\n<p>It just loves using skills.</p>"
            ],
            "link": "https://blog.fsck.com/2025/10/27/skills-for-openai-codex/",
            "publishedAt": "2025-10-27",
            "source": "Jesse Vincent",
            "summary": "<p>I've been <a href=\"https://blog.fsck.com/2025/10/09/superpowers/\">posting rather a lot</a> about <a href=\"https://blog.fsck.com/2025/10/16/skills-for-claude/\">Anthropic's new skills system</a> and about <a href=\"https://blog.fsck.com/2025/10/12/superpowers-20-came-out-yesterday-and-might-already-be-obsolete/\">my Superpowers system</a> that builds software development and skills development workflows using those building blocks.</p> <p>Up to now, Skills have been a Claude-specific thing, but there's no reason they shouldn't work well with any LLM.</p> <p>I had a couple of hours free this afternoon, so I ported <a href=\"https://github.com/obra/superpowers\">Superpowers</a> and the entire <code>SKILL.md</code> system to OpenAI's Codex CLI. The new implementation is live in <a href=\"https://github.com/obra/superpowers/blob/main/RELEASE-NOTES.md\">Superpowers 3.3.0</a>. I've only been using it for a little while, but GPT5-Codex may actually be <em>better</em> at using skills than Claude.</p> <p>Reading over that paragraph, the speed at which it's possible to build these kinds of things still feels absolutely crazy to me. (When I asked Claude Deskop to review this blog post, it told me that there was nothing &quot;insane&quot; about what I did and that I should cut the comment, since it seemed nonsenical. On a second read-through, Claude Deskop did not understand why I wouldn't cut the note and got kind of agressive about it. Finally, I had Claude Code do a tech review of the post. <em>That</em> Claude finally said &quot;The Claude reviewing part:",
            "title": "Porting Skills (and Superpowers) to OpenAI Codex"
        },
        {
            "content": [],
            "link": "https://www.robinsloan.com/lab/ungrounded/",
            "publishedAt": "2025-10-27",
            "source": "Robin Sloan",
            "summary": "<p>Floating in linguistic space. <a href=\"https://www.robinsloan.com/lab/ungrounded/\">Read here.</a></p>",
            "title": "Thinking modes"
        },
        {
            "content": [
                "<p>This is the weekly visible open thread. Post about anything you want, ask random questions, whatever. ACX has an unofficial <a href=\"https://www.reddit.com/r/slatestarcodex/\">subreddit</a>, <a href=\"https://discord.gg/RTKtdut\">Discord</a>, and <a href=\"https://www.datasecretslox.com/index.php\">bulletin board</a>, and <a href=\"https://www.lesswrong.com/community?filters%5B0%5D=SSC\">in-person meetups around the world</a>. Most content is free, some is subscriber only; you can subscribe <strong><a href=\"https://astralcodexten.substack.com/subscribe\">here</a></strong>. Also:</p><div><hr /></div><p><strong>1: </strong>Meetups this week include Haifa, Huntsville, and Prague - see <a href=\"https://www.astralcodexten.com/p/meetups-everywhere-2025-times-and\">the meetup post</a> for more information.</p><p><strong>2: </strong>The ACX Boston meetup group has completed <a href=\"https://docs.google.com/document/d/1tQukdM_gY5Nroo68zX6FMIoVmMlPWdTRvCOXZFP06Us/preview?tab=t.0\">their voter guide for this week&#8217;s Boston municipal election</a>.</p><p><strong>3: </strong>In <a href=\"https://www.astralcodexten.com/p/highlights-from-the-comments-on-fatima\">Highlights From The Comments On Fatima</a>, I mentioned someone who analogized the problem of evil in religion to &#8220;the problem of non-characteristicness&#8221; in physics, but said I couldn&#8217;t credit them properly because I&#8217;d lost the link and forgotten who it was. <a href=\"https://substack.com/@gumphus/note/c-169883315\">It was Gumphus</a>.</p><p><strong>4: </strong>Metaculus is gearing up for another yearly forecasting contest, and looking for ideas for questions. You can see <a href=\"https://www.metaculus.com/tournament/ACX2025/\">this year&#8217;s question set here</a> - for example, &#8220;Will there be a ceasefire in the Russia-Ukraine war by the end of 2025?&#8221;. I&#8217;ll post an Open Thread comment below where you can list your ideas and someone from Metaculus will read them.</p><p><strong>5: </strong>Forethought (AI preparedness research org including Will MacAskill, Tom Davidson, etc) wants to hire more researchers. Offices in Oxford/Berkeley, slight bias towards people in these areas but remote work possible. Salaries &#163;80,000 - &#163;150,000 depending on qualifications and seniority. Must be, uh, good at research, I think this looks more like academic philosophy or economic modeling than like training LLMs, but it&#8217;s pretty vague. <a href=\"https://www.forethought.org/careers/researcher\">Learn more and apply here</a>.</p><p><strong>6: </strong>Asterisk magazine (EA/rationalist-adjacent, I&#8217;ve blogged about their work eg <a href=\"https://www.astralcodexten.com/p/misophonia-beyond-sensory-sensitivity\">here</a> and <a href=\"https://www.astralcodexten.com/p/is-wine-fake-in-asterisk-magazine\">here</a>) is looking for a new managing editor. You&#8217;ll help find interesting stories in fields like global health, economics, AI, and general tech-adjacent (and less tech-adjacent) culture, and convert them into polished articles. $90,000 -$120,000, remote work possible. <a href=\"https://asteriskmag.com/careers\">Learn more and apply here</a>. </p><p></p>"
            ],
            "link": "https://www.astralcodexten.com/p/open-thread-405",
            "publishedAt": "2025-10-27",
            "source": "SlateStarCodex",
            "summary": "<p>This is the weekly visible open thread. Post about anything you want, ask random questions, whatever. ACX has an unofficial <a href=\"https://www.reddit.com/r/slatestarcodex/\">subreddit</a>, <a href=\"https://discord.gg/RTKtdut\">Discord</a>, and <a href=\"https://www.datasecretslox.com/index.php\">bulletin board</a>, and <a href=\"https://www.lesswrong.com/community?filters%5B0%5D=SSC\">in-person meetups around the world</a>. Most content is free, some is subscriber only; you can subscribe <strong><a href=\"https://astralcodexten.substack.com/subscribe\">here</a></strong>. Also:</p><div><hr /></div><p><strong>1: </strong>Meetups this week include Haifa, Huntsville, and Prague - see <a href=\"https://www.astralcodexten.com/p/meetups-everywhere-2025-times-and\">the meetup post</a> for more information.</p><p><strong>2: </strong>The ACX Boston meetup group has completed <a href=\"https://docs.google.com/document/d/1tQukdM_gY5Nroo68zX6FMIoVmMlPWdTRvCOXZFP06Us/preview?tab=t.0\">their voter guide for this week&#8217;s Boston municipal election</a>.</p><p><strong>3: </strong>In <a href=\"https://www.astralcodexten.com/p/highlights-from-the-comments-on-fatima\">Highlights From The Comments On Fatima</a>, I mentioned someone who analogized the problem of evil in religion to &#8220;the problem of non-characteristicness&#8221; in physics, but said I couldn&#8217;t credit them properly because I&#8217;d lost the link and forgotten who it was. <a href=\"https://substack.com/@gumphus/note/c-169883315\">It was Gumphus</a>.</p><p><strong>4: </strong>Metaculus is gearing up for another yearly forecasting contest, and looking for ideas for questions. You can see <a href=\"https://www.metaculus.com/tournament/ACX2025/\">this year&#8217;s question set here</a> - for example, &#8220;Will there be a ceasefire in the Russia-Ukraine war by the end of 2025?&#8221;. I&#8217;ll post an Open Thread comment below where you can list your ideas and someone from Metaculus will read them.</p><p><strong>5: </strong>Forethought (AI preparedness research org including Will MacAskill,",
            "title": "Open Thread 405"
        },
        {
            "content": [
                "<p>Consider this largely a follow-up to <a href=\"https://thezvi.substack.com/p/new-statement-calls-for-not-building?r=67wny\">Friday\u2019s post about a statement aimed at creating common knowledg</a>e around it being unwise to build superintelligence any time soon.</p>\n<p>Mainly, there was a great question asked, so I gave a few hour shot at writing out my answer. I then close with a few other follow-ups on issues related to the statement.</p>\n\n\n<h4 class=\"wp-block-heading\">A Great Question To Disentangle</h4>\n\n\n<p><a href=\"https://x.com/sriramk/status/1982111886188462270\">There are some confusing wires potentially crossed</a> here but the intent is great.</p>\n<blockquote><p>Scott Alexander: I think removing a 10% chance of humanity going permanently extinct is worth another 25-50 years of having to deal with the normal human problems the normal way.</p>\n<div>\n\n\n<span id=\"more-24816\"></span>\n\n\n</div>\n<p><a href=\"https://x.com/sriramk/status/1982111886188462270\">Sriram Krishnan</a>: Scott what are verifiable empirical things ( model capabilities / incidents / etc ) that would make you shift that probability up or down over next 18 months?</p></blockquote>\n<p>I went through three steps interpreting this (where p(doom) = probability of existential risk to humanity, either extinction, irrecoverable collapse or loss of control over the future).</p>\n<ol>\n<li>Instinctive read is the clearly intended question, an excellent one: Either \u201cWhat would shift the amount that waiting 25-50 years would reduce p(doom)?\u201d or \u201cWhat would shift your p(doom)?\u201d</li>\n<li>Literal interpretation, also interesting but presumably not intended: What would shift how much of a reduction in p(doom) would be required to justify waiting?</li>\n<li>Conclusion on reflection: Mostly back to the first read.</li>\n</ol>\n<p>All three questions are excellent distinct questions, in addition to the related fourth excellent question that is highly related, which is the probability that we will be capable of building superintelligence or sufficiently advanced AI that creates 10% or more existential risk.</p>\n<p>The 18 month timeframe seems arbitrary, but it seems like a good exercise to ask only within the window of \u2018we are reasonably confident that we do not expect an AGI-shaped thing.\u2019</p>\n<p><a href=\"https://x.com/austinc3301/status/1982508945911525856\">Agus offers his answers</a> to a mix of these different questions, in the downward direction &#8211; as in, which things would make him feel safer.</p>\n\n\n<h4 class=\"wp-block-heading\">Scott Alexander Gives a Fast Answer</h4>\n\n\n<p><a href=\"https://x.com/sriramk/status/1982111886188462270\">Scott Alexander offers his answer</a>, I concur that mostly I expect only small updates.</p>\n<blockquote><p>Scott Alexander: Thanks for your interest. I\u2019m not expecting too much danger in the next 18 months, so these would mostly be small updates, but to answer the question:</p>\n<p>MORE WORRIED:</p>\n<p>&#8211; Anything that looks like shorter timelines, especially superexponential progress on METR time horizons graph or early signs of recursive self-improvement.</p>\n<p>&#8211; China pivoting away from their fast-follow strategy towards racing to catch up to the US in foundation models, and making unexpectedly fast progress.</p>\n<p>&#8211; More of the \u201cmodel organism shows misalignment in contrived scenario\u201d results, in gradually less and less contrived scenarios.</p>\n<p>&#8211; Models more likely to reward hack, eg commenting out tests instead of writing good code, or any of the other examples in <a href=\"https://metr.org/blog/2025-06-05-recent-reward-hacking/\">here</a> &#8211; or else labs only barely treading water against these failure modes by investing many more resources into them.</p>\n<p>&#8211; Companies training against chain-of-thought, or coming up with new methods that make human-readable chain-of-thought obsolete, or AIs themselves regressing to incomprehensible chains-of-thought for some reason (see eg <a href=\"https://antischeming.ai/snippets#reasoning-loops\" rel=\"nofollow\">https://antischeming.ai/snippets#reasoning-loops</a>).</p>\n<p>LESS WORRIED</p>\n<p>&#8211; The opposite of all those things.</p>\n<p>&#8211; Strong progress in transparency and mechanistic interpretability research.</p>\n<p>&#8211; Strong progress in something like \u201ctruly understanding the nature of deep learning and generalization\u201d, to the point where results like <a href=\"https://arxiv.org/abs/2309.12288\">https://arxiv.org/abs/2309.12288</a> make total sense and no longer surprise us.</p>\n<p>&#8211; More signs that everyone is on the same side and government is taking this seriously (thanks for your part in this).</p>\n<p>&#8211; More signs that industry and academia are taking this seriously, even apart from whatever government requires of them.</p>\n<p>&#8211; Some sort of better understanding of bottlenecks, such that even if AI begins to recursively self-improve, we can be confident that it will only proceed at the rate of chip scaling or [some other nontrivial input]. This might look like AI companies releasing data that help give us a better sense of the function mapping (number of researchers) x (researcher experience/talent) x (compute) to advances.</p>\n<p>This is a quick and sloppy answer, but I\u2019ll try to get the AI Futures Project to make a good blog post on it and link you to it if/when it happens.</p></blockquote>\n<p>Giving full answers to these questions would require at least an entire long post, but to give what was supposed to be the five minute version that turned into a few hours:</p>\n\n\n<h4 class=\"wp-block-heading\">Question 1: What events would most shift your p(doom | ASI) in the next 18 months?</h4>\n\n\n<p>Quite a few things could move the needle somewhat, often quite a lot. This list assumes we don\u2019t actually get close to AGI or ASI within those 18 months.</p>\n<ol>\n<li>Faster timelines increase p(doom), slower timelines reduce p(doom).</li>\n<li>Capabilities being more jagged reduces p(doom), less jagged increases it.</li>\n<li>Coding or ability to do AI research related tasks being a larger comparative advantage of LLMs increases p(doom), the opposite reduces it.</li>\n<li>Quality of the discourse and its impact on ability to make reasonable decisions.</li>\n<li>Relatively responsible AI sources being relatively well positioned reduces p(doom), them being poorly positioned increases it, with the order being roughly Anthropic \u2192 OpenAI and Google (and SSI?) \u2192 Meta and xAI \u2192 Chinese labs.</li>\n<li>Updates about the responsibility levels and alignment plans of the top labs.</li>\n<li>Updates about alignment progress, alignment difficulty and whether various labs are taking promising approaches versus non-promising approaches.\n<ol>\n<li>New common knowledge will often be an \u2018unhint,\u2019 as in the information makes the problem easier to solve via making you realize why your approach wouldn\u2019t work.</li>\n<li>This can be good or bad news, depending on what you understood previously. Many other things are also in the category \u2018important, sign of impact weird.\u2019</li>\n<li>Reward hacking is a great example of an unhint, in that I expect to \u2018get bad news\u2019 but for the main impact of this being that we learn the bad news.</li>\n<li>Note that models are increasingly situationally aware and capable of thinking ahead, as per Claude Sonnet 4.5, and that we need to worry more that things like not reward hacking are \u2018because the model realized it couldn\u2019t get away with it\u2019 or was worried it might be in an eval, rather than that the model not wanting to reward hack. Again, it is very complex which direction to update.</li>\n<li>Increasing situational awareness is a negative update but mostly priced in.</li>\n<li>Misalignment in less contrived scenarios would indeed be bad news, and \u2018the less contrived the more misaligned\u2019 would be the worst news of all here.</li>\n<li>Training against chain-of-thought would be a major negative update, as would be chain-of-thought becoming impossible for humans to read.</li>\n<li>This section could of course be written at infinite length.</li>\n</ol>\n</li>\n<li>In particular, updates on whether the few approaches that could possibly work look like they might actually work, and we might actually try them sufficiently wisely that they might work. Various technical questions too complex to list here.</li>\n<li>Unexpected technical developments of all sorts, positive and negative.</li>\n<li>Better understanding of the game theory, decision theory, economic theory or political economy of an AGI future, and exactly how impossible the task is of getting a good outcome conditional on not failing straight away on alignment.</li>\n<li>Ability to actually discuss seriously the questions of how to navigate an AGI future if we can survive long enough to face these \u2018phase two\u2019 issues, and level of hope that we would not commit collective suicide even in winnable scenarios. If all the potentially winning moves become unthinkable, all is lost.</li>\n<li>Level of understanding by various key actors of the situation aspects, and level of various pressures that will be placed upon them, including by employees and by vibes and by commercial and political pressures, in various directions.</li>\n<li>Prediction of how various key actors will make various of the important decisions in likely scenarios, and what their motivations will be, and who within various corporations and governments will be making the decisions that matter.</li>\n<li>Government regulatory stance and policy, level of transparency and state capacity and ability to intervene. Stance towards various things. Who has the ear of the government, both White House and Congress, and how powerful is that ear. Timing of the critical events and which administration will be handling them.</li>\n<li>General quality and functionality of our institutions.</li>\n<li>Shifts in public perception and political winds, and how they are expected to impact the paths that we take, and other political developments generally.</li>\n<li>Level of potential international cooperation and groundwork and mechanisms for doing so. Degree to which the Chinese are AGI pilled (more is worse).</li>\n<li>Observing how we are reacting to mundane current AI, and how this likely extends to how we will interact with future AI.</li>\n<li>To some extent, information about how vulnerable or robust we are on CBRN risks, especially bio and cyber, the extent hardening tools seem to be getting used and are effective, and evaluation of the Fragile World Hypothesis and future offense-defense balance, but this is often overestimated as a factor.</li>\n<li>Expectations on bottlenecks to impact even if we do get ASI with respect to coding, although again this is usually overestimated.</li>\n</ol>\n<p>The list could go on. This is a complex test and on the margin everything counts. A lot of the frustration with discussing these questions is different people focus on very different aspects of the problem, both in sensible ways and otherwise.</p>\n<p>That\u2019s a long list, so to summarize the most important points on it:</p>\n<ol>\n<li>Timelines.</li>\n<li>Jaggedness of capabilities relative to humans or requirements of automation.</li>\n<li>The relative position in jaggedness of coding and automated research.</li>\n<li>Alignment difficulty in theory.</li>\n<li>Alignment difficulty in practice, given who will be trying to solve this under what conditions and pressures, with what plans and understanding.</li>\n<li>Progress on solving gradual disempowerment and related issues.</li>\n<li>Quality of policy, discourse, coordination and so on.</li>\n<li>World level of vulnerability versus robustness to various threats (overrated, but still an important question).</li>\n</ol>\n<p>Imagine we have a distribution of \u2018how wicked and impossible are the problems we would face if we build ASI, with respect to both alignment and to the dynamics we face if we handle alignment, and we need to win both\u2019 that ranges from \u2018extremely wicked but not strictly impossible\u2019 to full Margaritaville (as in, you might as well sit back and have a margarita, cause it\u2019s over).</p>\n<p>At the same time as everything counts, the core reasons these problems are wicked are fundamental. Many are technical but the most important one is not. If you\u2019re building sufficiently advanced AI that will become far more intelligent, capable and competitive than humans, by default this quickly ends poorly for the humans.</p>\n<p>On a technical level, for largely but not entirely Yudkowsky-style reasons, the behaviors and dynamics you get prior to AGI and ASI are not that informative of what you can expect afterwards, and when they are often it is in a non-intuitive way or mostly informs this via your expectations for how the humans will act.</p>\n<p>Note that from my perspective, we are here starting the conditional risk a lot higher than 10%. My conditional probability here is \u2018if anyone builds it, everyone probably dies,\u2019 as in a number (after factoring in modesty) between 60% and 90%.</p>\n<p>My probability here is primarily different from Scott\u2019s (AIUI) because I am much more despairing about our ability to muddle through or get success with an embarrassingly poor plan on alignment and disempowerment, but it is not higher because I am not as despairing as some others (such as Soares and Yudkowsky).</p>\n<p>If I was confident that the baseline conditional-on-ASI-soonish risk was at most 10%, then I would be trying to mitigate that risk, it would still be humanity\u2019s top problem, but I would understand wanting to continue onward regardless, and I wouldn\u2019t have signed the recent statement.</p>\n\n\n<h4 class=\"wp-block-heading\">Question 1a: What would get this risk down to acceptable levels?</h4>\n\n\n<p>In order to move me down enough to think that moving forward would be a reasonable thing to do any time soon out of anything other then desperation that there was no other option, I would need at least:</p>\n<ol>\n<li>An alignment plan that looked like it would work, on the first try. That could be a new plan, or it could be new very positive updates on one of the few plans we have now that I currently think could possibly work, all of which are atrociously terrible compared to what I would have hoped for a few years ago, but this is mitigated by having forms of grace available that seemingly render the problem a lower level of impossible and wicked than I previously expected (although still highly wicked and impossible).\n<ol>\n<li>Given the 18 month window and current trends, this probably either is something new, or it is a form of (colloquially speaking) \u2018we can hit, in a remarkably capable model, an attractor state basin in distribution mindspace that is robustly good such that it will want to modify itself and its de facto goals and utility function and its successors continuously towards the target we actually need to hit and wanting to hit the target we actually need to hit.\u2019</li>\n<li>Then again, perhaps I will be surprised in some way.</li>\n</ol>\n</li>\n<li>Confidence that this plan would actually get executed, competently.</li>\n<li>A plan to solve gradual disempowerment issues, in a way I was confident would work, create a future with value, and not lead to unacceptable other effects.</li>\n<li>Confidence that this plan would actually get executed, competently.</li>\n</ol>\n<p>In a sufficiently dire race condition, where all coordination efforts and alternatives have failed, of course you go with the best option you have, especially if up against an alternative that is 100% (minus epsilon) to lose.</p>\n\n\n<h4 class=\"wp-block-heading\">Question 2: What would shift the amount that stopping us from creating superintelligence for a potentially extended period would reduce p(doom)?</h4>\n\n\n<p>Everything above will also shift this, since it gives you more or less doom that extra time can prevent. What else can shift the estimate here within 18 months?</p>\n<p>Again, \u2018everything counts in large amounts,\u2019 but centrally we can narrow it down.</p>\n<p>There are five core questions, I think?</p>\n<ol>\n<li>What would it take to make this happen? As in, will this indefinitely be a sufficiently hard thing to build that we can monitor large data centers, or do we need to rapidly keep an eye on smaller and smaller compute sources? Would we have to do other interventions as well?</li>\n<li>Are we ready to do this in a good way and how are we going to go about it? If we have a framework and the required technology, and can do this in a clean way, with voluntary cooperation and without either use or massive threat of force or concentration of power, especially in a way that allows us to still benefit from AI and work on alignment and safety issues effectively, then that looks a lot better. Every way that this gets worse makes our prospects here worse.</li>\n<li>Did we get too close to the finish line before we tried to stop this from happening? A classic tabletop exercise endgame is that the parties realize close to the last moment that they need to stop things, or leverage is used to force this, but the AIs involved are already superhuman, so the methods used would have worked before and work anymore. And humanity loses.</li>\n<li>Do we think we can make good use of this time, that the problem is solvable? If the problems are unsolvable, or our civilization isn\u2019t up for solving them, then time won\u2019t solve them.</li>\n<li>How much risk do we take on as we wait, in other ways?</li>\n</ol>\n<p>One could summarize this as:</p>\n<ol>\n<li>How would we have to do this?</li>\n<li>Are we going to be ready and able to do that?</li>\n<li>Will it be too late?</li>\n<li>Would we make good use of the time we get?</li>\n<li>What are the other risks and costs of waiting?</li>\n</ol>\n<p>I expect to learn new information about several of these questions.</p>\n\n\n<h4 class=\"wp-block-heading\">Question 3: What would shift your timelines to ASI (or to sufficiently advanced AI, or \u2018to crazy\u2019)?</h4>\n\n\n<p>(My current median time-to-crazy in this sense is roughly 2031, but with very wide uncertainty and error bars and not the attention I would put on that question if I thought the exact estimate mattered a lot, and I don\u2019t feel I would \u2018have any right to complain\u2019 if the outcome was very far off from this in either direction. If a next-cycle model did get there I don\u2019t think we are entitled to be utterly shocked by this.)</p>\n<p>This is the biggest anticipated update because it will change quite a lot. Many of the other key parts of the model are much harder to shift, but timelines are an empirical question that shifts constantly.</p>\n<p>In the extreme, if progress looks to be stalling out and remaining at \u2018AI as normal technology,\u2019 then this would be very good news. The best way to not build superintelligence right away is if building it is actually super hard and we can\u2019t, we don\u2019t know how. It doesn\u2019t strictly change the conditional in questions one and two, but it renders those questions irrelevant, and this would dissolve a lot of practical disagreements.</p>\n<p>Signs of this would be various scaling laws no longer providing substantial improvements or our ability to scale them running out, especially in coding and research, bending the curve on the METR graph and other similar measures, the systematic failure to discover new innovations, extra work into agent scaffolding showing rapidly diminishing returns and seeming upper bounds, funding required for further scaling drying up due to lack of expectations of profits or some sort of bubble bursting (or due to a conflict) in a way that looks sustainable, or strong evidence that there are fundamental limits to our approaches and therefore important things our AI paradigm simply cannot do. And so on.</p>\n<p>Ordinary shifts in the distribution of time to ASI come with every new data point. Every model that disappoints moves you back, observing progress moves you forward. Funding landscape adjustments, levels of anticipated profitability and compute availability move this. China becoming AGI pilled versus fast following or foolish releases could move this. Government stances could move this. And so on.</p>\n<p>Time passing without news lengthens timelines. Most news shortens timelines. The news item that lengthens timelines is mostly \u2018we expected this new thing to be better or constitute more progress, in some form, and instead it wasn\u2019t and it didn\u2019t.\u2019</p>\n<p>To be clear that I am doing this: There are a few things that I didn\u2019t make explicit, because one of the problems with such conversations is that in some ways we are not ready to have these conversations, as many branches of the scenario tree involve trading off sacred values or making impossible choices or they require saying various quiet parts out loud. If you know, you know.</p>\n<p>That was less of a \u2018quick and sloppy\u2019 answer than Scott\u2019s, but still feels very quick and sloppy versus what I\u2019d offer after 10 hours, or 100 hours.</p>\n\n\n<h4 class=\"wp-block-heading\">Bonus Question 1: Why Do We Keep Having To Point Out That Building Superintelligence At The First Possible Moment Is Not A Good Idea?</h4>\n\n\n<p>The reason we need letters explaining not to build superintelligence at the first possible moment regardless of the fact that it probably kills us is that <a href=\"https://x.com/jawwwn_/status/1981450980450587094\">people are advocating for building superintelligence regardless of the fact that it probably kills us</a>.</p>\n<blockquote><p>Jawwwn: Palantir CEO Alex Karp on calls for a \u201cban on AI Superintelligence\u201d</p>\n<p>\u201cWe\u2019re in an arms race. We\u2019re either going to have AI and determine the rules, or our adversaries will.\u201d</p>\n<p>\u201cIf you put impediments\u2026 we\u2019ll be buying everything from them, including ideas on how to run our gov\u2019t.\u201d</p></blockquote>\n<p>He is the CEO of Palantir literally said this is an \u2018arms race.\u2019 The first rule of an arms race is you don\u2019t loudly tell them you\u2019re in an arms race. The second rule is you don\u2019t win it by building superintelligence as your weapon.</p>\n<p>Once you build superintelligence, especially if you build it explicitly as a weapon to \u2018determine the rules,\u2019 humans no longer determine the rules. Or anything else. That is the point.</p>\n<p>Until we have common knowledge of the basic facts that goes at least as far as major CEOs not saying the opposite in public, job one is to create this common knowledge.</p>\n<p>I also enjoyed Tyler Cowen fully Saying The Thing, this really is his position:</p>\n<blockquote><p>Tyler Cowen: <a href=\"https://x.com/deanwball/status/1980975802570174831\">Dean Ball on the call for a superintelligence ban</a>, Dean is right once again. Mainly (once again) a lot of irresponsibility on the other side of that ledger, you will not see them seriously address the points that Dean raises. If you want to go this route, do the hard work and write an 80-page paper on how the political economy of such a ban would work.</p></blockquote>\n<p>That\u2019s right. If you want to say that not building superintelligence as soon as possible is a good idea, first you have to write an 80-page paper on the political economy of a particular implementation of a ban on that idea. That\u2019s it, he doesn\u2019t make the rules. Making a statement would otherwise be irresponsible, so until such time as a properly approved paper comes out on these particular questions, we should instead be responsible by going ahead not talking about this and focus on building superintelligence as quickly as possible.</p>\n<p>I notice that a lot of people are saying that humanity has already lost control over the development of AI, and that there is nothing we can do about this, because the alternative to losing control over the future is even worse. In which case, perhaps that shows the urgency of the meddling kids proving them wrong?</p>\n<p>Alternatively\u2026</p>\n\n\n<h4 class=\"wp-block-heading\">Bonus Question 2: What Would a Treaty On Prevention of Artificial Superintelligence Look Like?</h4>\n\n\n<p>How dare you try to prevent the building of superintelligence without knowing how to prevent this safely, ask the people who want us to build superintelligence without knowing how to do so safely.</p>\n<p>Seems like a rather misplaced demand for detailed planning, if you ask me. But it\u2019s perfectly valid and highly productive to ask how one might go about doing this. Indeed, what this would look like is one of the key inputs in the above answers.</p>\n<p>One key question is, are you going to need some sort of omnipowerful international regulator with sole authority that we all need to be terrified about, or can we build this out of normal (relatively) lightweight international treaties and verification that we can evolve gradually over time if we start planning now?</p>\n<blockquote><p>Peter Wildeford: Don\u2019t let them tell you that it\u2019s not possible.</p>\n<div>\n<figure>\n<div>\n\n\n<figure class=\"wp-block-image\"><img alt=\"\" src=\"https://substackcdn.com/image/fetch/$s_!vsv5!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4dd017e5-7009-4b4f-aa95-422b12dc7e9b_500x500.png\" /></figure>\n\n\n<div></div>\n</div>\n</figure>\n</div>\n</blockquote>\n<p>The default method one would actually implement is an international treaty, <a href=\"https://x.com/m_bourgon/status/1981860227034349949\">and indeed MIRI\u2019s TechGov team wrote one such draft treaty</a>, although not also an 80 page paper on its political economy. There is also a Financial Times article suggesting <a href=\"https://www.ft.com/content/767d1feb-2c6a-4385-b091-5c0fc564b4ee\">we could draw upon our experience with nuclear arms control treaties</a>, which were easier coordination problems but of a similar type.</p>\n<p>Will Marshall points out that in order to accomplish this, we would need extensive track-two processes between thinkers over an extended period to get it right. Which is indeed exactly why you can offer templates and ideas but to get serious you need to first agree to the principle, and then work on details.</p>\n<p><a href=\"https://x.com/tyler_m_john/status/1982447938467934677\">Tyler John also makes a similar argument</a> that multilateral agreements would work. The argument that \u2018everyone would have incentive to cheat\u2019 is indeed the main difficulty, but also is not a new problem.</p>\n<p>What was done academically prior to the nuclear arms control treaties? Claude points me to Schelling &amp; Halperin\u2019s \u201cStrategy and Arms Control\u201d (1961), Schelling\u2019s \u201cThe Strategy of Conflict<strong>\u201d </strong>(1960<strong>)</strong> and \u201cArms and Influence\u201d (1966), and Boulding\u2019s \u201cConflict and Defense\u201d (1962). So the analysis did not get so detailed even then with a much more clear game board, but certainly there is some work that needs to be done.</p>"
            ],
            "link": "https://thezvi.wordpress.com/2025/10/27/asking-some-of-the-right-questions/",
            "publishedAt": "2025-10-27",
            "source": "TheZvi",
            "summary": "Consider this largely a follow-up to Friday\u2019s post about a statement aimed at creating common knowledge around it being unwise to build superintelligence any time soon. Mainly, there was a great question asked, so I gave a few hour shot &#8230; <a href=\"https://thezvi.wordpress.com/2025/10/27/asking-some-of-the-right-questions/\">Continue reading <span class=\"meta-nav\">&#8594;</span></a>",
            "title": "Asking (Some Of) The Right Questions"
        },
        {
            "content": [],
            "link": "https://xkcd.com/3160/",
            "publishedAt": "2025-10-27",
            "source": "XKCD",
            "summary": "<img alt=\"It comes with a certificate of authenticity, which comes with a certificate of authenticity, which comes with a...\" src=\"https://imgs.xkcd.com/comics/document_forgery.png\" title=\"It comes with a certificate of authenticity, which comes with a certificate of authenticity, which comes with a...\" />",
            "title": "Document Forgery"
        }
    ],
    "lookbackDays": 1,
    "publishDate": "2025-10-27"
}