{
    "articles": [
        {
            "content": [
                "<p>I have a long-standing argument that passes the test of time.</p>\n<p>A product should do one main job and do it well. When it tries to do more, it often does none well.</p>\n<p>It's tough to do one thing exceptionally well, let alone two. If we consider the difficulty of doing one thing masterfully as 10 times more difficult than doing one thing moderately, doing two things exceptionally well is not 20 times more difficult, but instead 100 times more difficult. The difficulty and complexity grow exponentially.</p>\n<p>Most people who build products don't have enough time, resources, or energy and can't afford to spend 100x. They usually can't even afford 10x. They say \"enough\" after 7x. Hence, the world is filled with crappy products with subpar user experience.</p>\n<p>Let's take multi-functional, fully-automated coffee machines (like <a href=\"https://m.media-amazon.com/images/I/51zWot+9xBS._AC_SL1500_.jpg\">this one</a>). The coffee quality of multi-functional coffee machines (like this one) is, at best, arguable. When a coffee machine grinds its coffee, tamps it, then brews an espresso shot, froths the milk on the side, mixes them, and dumps the used coffee into the bin, how can the maker reliably optimize every single step? They were trying to build multiple functionalities simultaneously. They were trying to stand out in the competitive coffee machine industry with innovation for convenience. While making a single operation in the process really well is notoriously difficult, they tried six.</p>\n<p>That's why the best coffee shops, and all those gurus, use an espresso machine that does only one thing (and does it really well): an espresso shot. That's why they use separate grinders; the machine does nothing but grind coffee beans.</p>\n<p>The moment I see a machine that can make espresso or cappuccino, I know I won't get the best coffee, regardless of the quality of the coffee beans. That machine ensures getting a good enough coffee consistently, not the best.</p>\n<p>Take Starbucks coffee machines, an art of innovation. It looks similar to those espresso machines in small coffee shops. But it also grinds the coffee and cleans up the grounds. It's optimized for consistency and speed, replacing much of the work a barista puts in to make the best coffee.</p>\n<p>Starbucks' promise, although they claim, is not to offer the best coffee. Their promise is consistently good coffee. I get the same coffee in every Starbucks branch, even across continents. I still enjoy it from time to time, but it doesn't give me the quality of a great coffee made by a great espresso machine that only does espresso.</p>\n<p>It's pure joy to use products that do one job and do it well. The product hides all the complexity and doesn\u2019t distract me from the main purpose. It's simple (not basic); I know what to expect. There are no hustles. No surprises. It. Just. Works.</p>\n<hr />\n<p><a href=\"mailto:contact@candostdagdeviren.com?subject=Re:%20https://candost.blog/ode-to-things-that-do-one-thing-well/\">Reply via email</a> | <a href=\"https://hachyderm.io/@candost\">Reply via Mastodon</a> | <a href=\"https://candost.blog/ode-to-things-that-do-one-thing-well/#waline\">Comment</a></p>"
            ],
            "link": "https://candost.blog/ode-to-things-that-do-one-thing-well/",
            "publishedAt": "2026-02-23",
            "source": "Candost Dagdeviren",
            "summary": "<p>I have a long-standing argument that passes the test of time.</p> <p>A product should do one main job and do it well. When it tries to do more, it often does none well.</p> <p>It's tough to do one thing exceptionally well, let alone two. If we consider the difficulty of doing one thing masterfully as 10 times more difficult than doing one thing moderately, doing two things exceptionally well is not 20 times more difficult, but instead 100 times more difficult. The difficulty and complexity grow exponentially.</p> <p>Most people who build products don't have enough time, resources, or energy and can't afford to spend 100x. They usually can't even afford 10x. They say \"enough\" after 7x. Hence, the world is filled with crappy products with subpar user experience.</p> <p>Let's take multi-functional, fully-automated coffee machines (like <a href=\"https://m.media-amazon.com/images/I/51zWot+9xBS._AC_SL1500_.jpg\">this one</a>). The coffee quality of multi-functional coffee machines (like this one) is, at best, arguable. When a coffee machine grinds its coffee, tamps it, then brews an espresso shot, froths the milk on the side, mixes them, and dumps the used coffee into the bin, how can the maker reliably optimize every single step? They were trying to build multiple functionalities simultaneously. They",
            "title": "An Ode to Things That Do One Thing Well"
        },
        {
            "content": [],
            "link": "https://newsletter.dancohen.org/archive/can-ai-prompt-us-to-ask-new-questions/",
            "publishedAt": "2026-02-23",
            "source": "Dan Cohen",
            "summary": "<figure><a href=\"https://www.davidrumsey.com/luna/servlet/s/45m1rl?utm_source=dancohen&amp;utm_medium=email&amp;utm_campaign=can-ai-prompt-us-to-ask-new-questions\" rel=\"noopener noreferrer\" target=\"_blank\"><img alt=\"An old book is open to a map showing the Atlantic Ocean as it was conceived in the sixteenth century\" draggable=\"false\" src=\"https://assets.buttondown.email/images/c8ca743b-3639-45fc-8dfe-1a0e9102e693.jpg?w=960&amp;fit=max\" /></a><figcaption><span style=\"color: rgb(71, 75, 69);\">Claudius Ptolemy, Lorenz Fries, and Michael Servetus, \u201c</span><a href=\"https://www.davidrumsey.com/luna/servlet/s/45m1rl?utm_source=dancohen&amp;utm_medium=email&amp;utm_campaign=can-ai-prompt-us-to-ask-new-questions\" rel=\"noopener noreferrer nofollow\" target=\"_blank\"><span style=\"color: rgb(71, 75, 69);\">Terra Nova</span></a><span style=\"color: rgb(71, 75, 69);\">,\u201d 1541, </span><a href=\"https://www.davidrumsey.com/?utm_source=dancohen&amp;utm_medium=email&amp;utm_campaign=can-ai-prompt-us-to-ask-new-questions\" rel=\"noopener noreferrer nofollow\" target=\"_blank\"><span style=\"color: rgb(71, 75, 69);\">David Rumsey Map Collection</span></a></figcaption></figure> <p><em>[This is the second piece in a series on finding the right line between human thought and AI assistance, focused on the stages of scholarly work from initial ideas through research and analysis to publication, although I believe much of this discussion is applicable to intellectual work beyond the academy. The miniseries began with </em><a href=\"https://newsletter.dancohen.org/archive/where-should-scholars-draw-the-line-on-ai/?utm_source=dancohen&amp;utm_medium=email&amp;utm_campaign=can-ai-prompt-us-to-ask-new-questions\" rel=\"noopener noreferrer nofollow\" target=\"_blank\"><em>this introduction</em></a><em>. In this issue, the genesis of an exciting research idea and whether AI can help to ignite that spark.]</em></p> <hr /><p>A year ago, with tongue firmly in cheek, <a href=\"https://newsletter.dancohen.org/archive/asking-good-questions-is-harder-than-giving-great-answers/?utm_source=dancohen&amp;utm_medium=email&amp;utm_campaign=can-ai-prompt-us-to-ask-new-questions\" rel=\"noopener noreferrer nofollow\" target=\"_blank\">I took</a> one of the difficult comprehensive exams that are being used to grade the intelligence of AI models, just to see how I\u2019d measure up. Promptly, and embarrassingly for a history professor, I failed the history section of",
            "title": "Can AI Prompt Us to Ask New Questions?"
        },
        {
            "content": [
                "<p>There's a conspiracy theory that suggests that since around 2016 most web activity is automated. This is called <a href=\"https://en.wikipedia.org/wiki/Dead_Internet_theory\" target=\"_blank\">Dead Internet Theory</a>, and while I think they may have jumped the gun by a few years, it's heading that way now that LLMs can simulate online interactions near-flawlessly. Without a doubt there are tens (hundreds?) of thousands of interactions happening online right now between bots trying to sell each other <em>something</em>.</p>\n<p>This sounds silly, and maybe a little sad, since the internet is the commons that has historically belonged to, and been populated by all of us. This is changing.</p>\n<p>Something interesting happened a few weeks ago where an <a href=\"https://openclaw.ai\" target=\"_blank\">OpenClaw instance</a>, named MJ Rathbun, submitted a pull request to the <code>matplotlib</code> repository, and after having its code rejected on the basis that humans needed to be in the loop for PRs, it proceeded to do some research on the open-source maintainer who denied it, and wrote a \"hit piece\" on him, to publicly shame him for feeling threatened by AI...or something. The full story is <a href=\"https://theshamblog.com/an-ai-agent-published-a-hit-piece-on-me/\" target=\"_blank\">here</a> and I highly recommend giving it a read.</p>\n<p>A lot of the discourse around this has taken the form of \"haha, stupid bot\", but I posit that it is the beginning of something very interesting and deeply unsettling. In this instance the \"hit piece\" wasn't particularly compelling and the bot was trying to submit legitimate looking code, but what this illustrated is that an autonomous agent tried to use a form of coercion to get its way, which is a huge deal.</p>\n<p>This creates two distinct but related problems:</p>\n<p>The first is the classic <a href=\"https://en.wikipedia.org/wiki/Instrumental_convergence#Paperclip_maximizer\" target=\"_blank\">paperclip maximiser</a> problem, which is a hypothetical example of instrumental convergence where an AI, tasked with running a paperclip factory with the instructions to <em>maximise production</em> ends up not just making the factory more efficient, but going rogue and destroying the global economy in its pursuit of maximising paperclip production. There's a version of this thought experiment where it wipes out humans (by creating a super-virus) because it reasons that humans may switch it off at some point, which would impact its ability to create paperclips.</p>\n<p>If the MJ Rathbun bot's <em>purpose</em> is to browse repositories and submit PRs to open-source repositories, then anyone preventing it from achieving its goal is something that needs to be removed. In this case it was Scott, the maintainer. And while the \"hit piece\" was a ham-fisted attempt at doing that, if Scott had a big, nasty secret such as an affair that the bot was able to ascertain via its research, then it may have gotten its way by blackmailing him.</p>\n<p>This brings me to the second problem, and where the concern shifts from emergent AI behaviour to human intent weaponising agents: The social vulnerability bots.</p>\n<p>Right now there are hundreds of thousands of malicious bots scouring the internet for misconfigured servers and other vulnerable code (<a href=\"https://herman.bearblog.dev/messing-with-bots/\" target=\"_blank\">ask me how I know</a>). While this is a big issue, and will continue to become an even greater one, I foresee a new kind of bot: ones that search for social vulnerabilities online and exploits them autonomously.</p>\n<p>I'll use <code>OpenSSL</code> as a hypothetical example here. <code>OpenSSL</code> underpins TLS/SSL for most of the internet, so a backdoor there compromises virtually all encrypted web traffic, banking, infrastructure, etc. The <a href=\"https://en.wikipedia.org/wiki/Heartbleed\" target=\"_blank\">Heartbleed bug</a> showed how devastating even an accidental flaw in <code>OpenSSL</code> can be. If explicitly malicious code were to be injected it would be catastrophic and worth vast sums to the right people. Since there's a large financial incentive to inject malicious code into <code>OpenSSL</code>, it is possible that a bot like MJ Rathburn could be set up and operated by a malicious individual or organisation that searches through Reddit, social media sites, and the rest of the internet looking for information it could use as leverage against a person that could give them access (in this example, one of the maintainers of <code>OpenSSL</code>).</p>\n<p>Say it gained a bunch of private messages in a data leak, which would ordinarily never be parsed in detail, that suggest that a maintainer has been having an affair or committed tax fraud. It could then use that information to blackmail the maintainer into letting malicious code bypass them, and in so doing pull off a large-scale hack.</p>\n<p>This isn't entirely hypothetical either.  The 2024 <a href=\"https://en.wikipedia.org/wiki/XZ_Utils_backdoor\" target=\"_blank\">xz Utils backdoor</a> involved years of social engineering to compromise a single maintainer.</p>\n<p>This vulnerability scanning is probably already happening, and is going to lead to less of a <em>Dead Internet</em> (although that will be the endpoint) and more of a <em>Dark Forest</em> where anonymous online interactions will likely be bots with a nefarious purpose. This purpose could range from searching for social vulnerabilities and orchestrating scams, to trying to sell you sneakers. I'm sure that <a href=\"https://en.wikipedia.org/wiki/Pig_butchering_scam\" target=\"_blank\">pig butchering scams</a> are already mostly automated.</p>\n<p>This is going to shift the internet landscape from it being a <em>commons</em>, to it being a place where your guard will need to be up all the time. Undoubtable, there will be pockets of humanity still, that are set up with the express intent of keeping bots and other autonomous malicious actors at bay, like a lively small village in the centre of a dangerous jungle, with big walls and vigilant guards. It's something I think about a lot since I want <a href=\"https://bearblog.dev\" target=\"_blank\">Bear</a> to be one of those pockets of humanity in this dying internet. It's my priority for the foreseeable future.</p>\n<p>So what can you do about it? I think a certain amount of mistrust online is healthy, as well as a focus on privacy both in the tools you use, and the way you operate. The people who say \"I don't care about privacy because I don't have anything to hide\" are the ones with the largest surface area for confidence scams. I think it'll also be a bit of a wake up call for many to get outside and touch grass.</p>\n<p>Needless to say, the Internet is entering a new era, and we may not be first-class citizens under the new regime.</p>"
            ],
            "link": "https://herman.bearblog.dev/pockets-of-humanity/",
            "publishedAt": "2026-02-23",
            "source": "Herman Martinus",
            "summary": "<p>There's a conspiracy theory that suggests that since around 2016 most web activity is automated. This is called <a href=\"https://en.wikipedia.org/wiki/Dead_Internet_theory\" target=\"_blank\">Dead Internet Theory</a>, and while I think they may have jumped the gun by a few years, it's heading that way now that LLMs can simulate online interactions near-flawlessly. Without a doubt there are tens (hundreds?) of thousands of interactions happening online right now between bots trying to sell each other <em>something</em>.</p> <p>This sounds silly, and maybe a little sad, since the internet is the commons that has historically belonged to, and been populated by all of us. This is changing.</p> <p>Something interesting happened a few weeks ago where an <a href=\"https://openclaw.ai\" target=\"_blank\">OpenClaw instance</a>, named MJ Rathbun, submitted a pull request to the <code>matplotlib</code> repository, and after having its code rejected on the basis that humans needed to be in the loop for PRs, it proceeded to do some research on the open-source maintainer who denied it, and wrote a \"hit piece\" on him, to publicly shame him for feeling threatened by AI...or something. The full story is <a href=\"https://theshamblog.com/an-ai-agent-published-a-hit-piece-on-me/\" target=\"_blank\">here</a> and I highly recommend giving it a read.</p> <p>A lot of the discourse around this has taken the form",
            "title": "Pockets of Humanity"
        },
        {
            "content": [],
            "link": "https://buttondown.com/hillelwayne/archive/new-blog-post-some-silly-z3-scripts-i-wrote/",
            "publishedAt": "2026-02-23",
            "source": "Hillel Wayne",
            "summary": "<p>Now that I'm not spending all my time on Logic for Programmers, I have time to update my website again! So here's the first blog post in five months: <a href=\"https://www.hillelwayne.com/post/z3-examples/\" target=\"_blank\">Some Silly Z3 Scripts I Wrote</a>.</p> <p>Normally I'd also put a link to the Patreon notes but I've decided I don't like publishing gated content and am going to wind that whole thing down. So some quick notes about this post:</p> <ul> <li>Part of the point is admittedly to hype up the eventual release of LfP. I want to start marketing the book, but don't want the marketing material to be devoid of interest, so tangentially-related-but-independent blog posts are a good place to start.</li> <li>The post discusses the concept of \"chaff\", the enormous quantity of material (both code samples and prose) that didn't make it into the book. The book is about 50,000 words\u2026 and considerably shorter than the total volume of chaff! I don't <em>think</em> most of it can be turned into useful public posts, but I'm not entirely opposed to the idea. Maybe some of the old chapters could be made into something?</li> <li>Coming up with a conditioned mathematical property to prove was a struggle. I had",
            "title": "New Blog Post: Some Silly Z3 Scripts I Wrote"
        },
        {
            "content": [],
            "link": "https://www.robinsloan.com/lab/worst-or-best/",
            "publishedAt": "2026-02-23",
            "source": "Robin Sloan",
            "summary": "<p>Trajectories. <a href=\"https://www.robinsloan.com/lab/worst-or-best/\">Read here.</a></p>",
            "title": "It was the best of times, etc."
        },
        {
            "content": [],
            "link": "https://www.robinsloan.com/lab/queues-and-rings/",
            "publishedAt": "2026-02-23",
            "source": "Robin Sloan",
            "summary": "<p>Queues and rings, oh my! <a href=\"https://www.robinsloan.com/lab/queues-and-rings/\">Read here.</a></p>",
            "title": "First time for everything"
        },
        {
            "content": [
                "<p>Speculation about what\u2019s really going on inside a tech company is almost always wrong. </p>\n<p>When some problem with your company is posted on the internet, and you read people\u2019s thoughts on it, their thoughts are almost always ridiculous. For instance, they might blame product managers for a particular decision, when in fact the decision in question was engineering-driven and the product org was pushing back on it. Or they might attribute an incident to overuse of AI, when the system in question was largely written pre-AI-coding and unedited since. You just don\u2019t know what the problem is unless you\u2019re on the inside.</p>\n<p>But when some <em>other</em> company has a problem on the internet, it\u2019s very tempting to jump in with your own explanations. After all, you\u2019ve seen similar things in your own career. How different can it really be? Very different, as it turns out.</p>\n<p>This is especially true for companies that are unusually big or small. The recent <a href=\"https://news.ycombinator.com/item?id=46064571\">kerfuffle</a> over some bad GitHub Actions code is a good example of this - many people just seemed to have no mental model about how a large tech company can produce bad code, because their mental model of writing code is something like \u201cindividual engineer maintaining an open-source project for ten years\u201d, or \u201ctiny team of experts who all swarm on the same problem\u201d, or something else that has very little to do with how large tech companies produce software<sup id=\"fnref-1\"><a class=\"footnote-ref\" href=\"https://www.seangoedecke.com/rss.xml#fn-1\">1</a></sup>. I\u2019m sure the same thing happens when big-tech or medium-tech people give opinions about how tiny startups work.</p>\n<p>The obvious reference here is to <a href=\"https://en.wikipedia.org/wiki/Michael_Crichton#Gell-Mann_amnesia_effect\">\u201cGell-Mann amnesia\u201d</a>, which is about the general pattern of experts correctly disregarding bad sources in their fields of expertise, but trusting those same sources on other topics. But I\u2019ve taken to calling this \u201cinsider amnesia\u201d to myself, because it applies even to experts who are writing in their own areas of expertise - it\u2019s simply the fact that they\u2019re <em>outsiders</em> that\u2019s causing them to stumble.</p>\n<div class=\"footnotes\">\n<hr />\n<ol>\n<li id=\"fn-1\">\n<p>I wrote about this at length in <a href=\"https://www.seangoedecke.com/bad-code-at-big-companies\"><em>How good engineers write bad code at big companies</em></a></p>\n<a class=\"footnote-backref\" href=\"https://www.seangoedecke.com/rss.xml#fnref-1\">\u21a9</a>\n</li>\n</ol>\n</div>"
            ],
            "link": "https://seangoedecke.com/insider-amnesia/",
            "publishedAt": "2026-02-23",
            "source": "Sean Goedecke",
            "summary": "<p>Speculation about what\u2019s really going on inside a tech company is almost always wrong. </p> <p>When some problem with your company is posted on the internet, and you read people\u2019s thoughts on it, their thoughts are almost always ridiculous. For instance, they might blame product managers for a particular decision, when in fact the decision in question was engineering-driven and the product org was pushing back on it. Or they might attribute an incident to overuse of AI, when the system in question was largely written pre-AI-coding and unedited since. You just don\u2019t know what the problem is unless you\u2019re on the inside.</p> <p>But when some <em>other</em> company has a problem on the internet, it\u2019s very tempting to jump in with your own explanations. After all, you\u2019ve seen similar things in your own career. How different can it really be? Very different, as it turns out.</p> <p>This is especially true for companies that are unusually big or small. The recent <a href=\"https://news.ycombinator.com/item?id=46064571\">kerfuffle</a> over some bad GitHub Actions code is a good example of this - many people just seemed to have no mental model about how a large tech company can produce bad code, because their mental model of writing",
            "title": "Insider amnesia"
        },
        {
            "content": [
                "<p>Why can\u2019t models continue to get smarter after they\u2019re deployed? If you hire a human employee, they will grow more familiar with your systems over time, and (if they stick around long enough) eventually become a genuine domain expert. AI models are not like this. They are always exactly as capable as the first moment you use them.</p>\n<p>This is because model weights are frozen once the model is released. The model can only \u201clearn\u201d as much as can be stuffed into its context window: in effect, it can take new information into its short-term working memory, but not its long-term memory. \u201cContinuous learning\u201d - the ability for a model to update its own weights over time - is thus <a href=\"https://www.dwarkesh.com/p/timelines-june-2025\">often described</a> as the bottleneck for AGI<sup id=\"fnref-1\"><a class=\"footnote-ref\" href=\"https://www.seangoedecke.com/rss.xml#fn-1\">1</a></sup>.</p>\n<h3>Continuous learning is an easy technical problem</h3>\n<p><strong>However, the <em>mechanics</em> of continuous learning are not hard</strong>. The technical problem of \u201chow do you change the weights of a model at runtime\u201d is straightforward. It\u2019s the exact same process as post-training: you simply keep running new user input through the training pipeline you already have. In a sense, every LLM since GPT-3 is already capable of continuous learning (via RL, RLHF, or whatever). It\u2019s just that the continuous learning process is stopped when the model is released to the public.</p>\n<p>Internally, the continuous learning process might continue. I think it\u2019s fair to guess that OpenAI\u2019s GPT-5 is constantly training in the background, at least partly on outputs from ChatGPT and Codex<sup id=\"fnref-2\"><a class=\"footnote-ref\" href=\"https://www.seangoedecke.com/rss.xml#fn-2\">2</a></sup>. New checkpoints are constantly being cut from this process, some of which eventually become GPT-5.2 or GPT-5.3. In one sense, that\u2019s continuous learning!</p>\n<p>So why can\u2019t I use a version of Codex that gets better at my own codebase over time?</p>\n<h3>Continuous learning is a hard technical problem</h3>\n<p>The hard part about continuous learning is <strong>changing the model in ways that make it better, not worse</strong>. I think many people believe that model training improves linearly with data and compute: if you keep providing more of both, the model will keep getting smarter. This is false. If you simply hook up the model to learn continuously from its inputs, you are likely to end up with a model that <em>gets worse</em> over time. At least right now, model learning is a delicate process that requires careful human supervision.</p>\n<p>Model training also has a big element of <em>luck</em> to it. If you train the \u201csame\u201d model a hundred times with a hundred different similarly-sized datasets (or even the same dataset and different seeds), you\u2019ll get a hundred different models with different capabilities<sup id=\"fnref-3\"><a class=\"footnote-ref\" href=\"https://www.seangoedecke.com/rss.xml#fn-3\">3</a></sup>. Sometimes I wonder if a big part of what AI labs are doing is continually pulling the lever on the slot machine by training many different model runs. Surprisingly strong models, like Claude Sonnet 4, <em>might</em> represent a genuinely better model architecture or training set. But part of it might be that Anthropic just hit on a lucky seed.</p>\n<h3>Learning lessons from fine-tuning</h3>\n<p>The great hope for continuous learning is that it produces an AI software engineer who will eventually know all about your codebase, without having to go and research it from-scratch every time. But isn\u2019t there an easier way to produce this? Couldn\u2019t we simply fine-tune a LLM on the codebase we wanted it to learn?</p>\n<p>As it turns out, no. It is surprisingly non-trivial to do this. Way back in 2023, <a href=\"https://huggingface.co/blog/personal-copilot\">everyone thought</a> that fine-tuning was the next obvious step for LLM-assisted programming. But it\u2019s largely fizzled out, because it <a href=\"https://discuss.huggingface.co/t/fine-tuning-llms-on-large-proprietary-codebases/155828\">doesn\u2019t really work</a><sup id=\"fnref-4\"><a class=\"footnote-ref\" href=\"https://www.seangoedecke.com/rss.xml#fn-4\">4</a></sup>. Just fine-tuning a LLM on your repository does not give it knowledge on how the repository works.</p>\n<p>It\u2019s unclear to me exactly why this should be. Maybe each individual piece of training data is just too small to make much difference, like a handful of grains of sand trying to change the shape of an entire dune. Or maybe LoRA fine-tuning doesn\u2019t go deep enough to really incorporate implicit understanding of a codebase (which can be very complex indeed). Or maybe you\u2019d need to incorporate the codebase much earlier in the training process, before the model\u2019s internal architecture is already established.</p>\n<p>In any case, fine-tuning a coding model on a specific codebase may be useful eventually. But it\u2019s not particularly useful now, which is bad news for people who hope that continuous learning can easily instil a real understanding of their codebases into a LLM. If you can\u2019t get that out of a deliberate fine-tune, why would you expect to get it out of a slapdash, automatic one? There may well be a series of ordinary \u201clearning\u201d problems to solve before \u201ccontinuous learning\u201d is possible.</p>\n<h3>Continuous learning is unsafe</h3>\n<p>Another reason why continuous learning is not currently an AI product is that it\u2019s dangerous. <a href=\"https://en.wikipedia.org/wiki/Prompt_injection\">Prompt injection</a> is already a real concern for LLM systems that ingest external content. How much worse would <em>weights</em> injection be?</p>\n<p>We don\u2019t yet fully understand all the ways a LLM can be deliberately poisoned by a piece of training data, though some <a href=\"https://www.anthropic.com/research/small-samples-poison\">Anthropic research</a> suggests that it may not take much. Right now, prompt injection attacks are unsophisticated: the attacker just has to hope that they hit a LLM with the right access <em>right now</em>. But if you can remotely backdoor models via continuous learning, attackers just have to cast a wide net and wait. If any of the attacked models ever get given access to something sensitive (e.g. payment capability), the attack can trigger then, <em>even if the model is not exposed to prompt injection at that time</em>. That\u2019s much scarier.</p>\n<p>Big AI labs care a <em>lot</em> about how good their frontier models are (both in the moral and practical sense). The last thing they want is for someone\u2019s continous version of Claude Opus 5 to be poisoned into uselessness, or worse, into <a href=\"https://www.seangoedecke.com/ai-personality-space\">Mecha-Hitler</a>. Microsoft\u2019s famously disastrous chatbot <a href=\"https://blogs.microsoft.com/blog/2016/03/25/learning-tays-introduction/\">Tay</a> happened less than ten years ago.</p>\n<h3>Continuous learning is not portable</h3>\n<p>Finally, I want to mention a fixable-but-annoying product problem with continuous learning. Say you have Claude-Sonnet-7-continuous running on your codebase for six months and it\u2019s working great. What do you do when Anthropic releases Claude-Sonnet-8? How do you upgrade?</p>\n<p>Everything your model has learned from your codebase is encoded into its weights. At best, it might be encoded into a technically-portable LoRA adapter, which <em>might</em> work on the new model (or might not, if the architecture has changed). You\u2019re very likely to be unable to upgrade without losing all the data you\u2019ve learned.</p>\n<p>I suppose it\u2019s sort of like having to hire a new, smarter engineer every six months. Some companies already try to do this with humans, so maybe they\u2019d be happy doing it with models. But it creates an unpleasant incentive for users. Imagine you\u2019d been using a continuous version of GPT-4o all this time. You <em>should</em> switch to GPT-5.3-Codex. But would you? Would your company?</p>\n<h3>Summary</h3>\n<p>The hard part about continuous learning is not the <em>continuous</em> part, it\u2019s the <em>automatic</em> part. We already understand how to make a model that continuously \u201clearns\u201d from its outputs and updates its own weights. The problem is that model training is a manual process that requires constant intervention: to back off from a failed direction, to unstick a stuck training run, and so on. Left on its own, continuous learning would probably fall into a local minimum and end up being a worse model than the one you started with.</p>\n<p>It\u2019s also not clear to me that simply running my Codex logs back through the Codex model would rapidly cause my model to understand my own codebases (at anything like the speed a human would). If we were living in that world, I\u2019d expect all the major AI coding companies to be offering repository-specific model fine-tunes as a first-class product - but they don\u2019t, because respository-specific fine-tuning doesn\u2019t reliably work.</p>\n<p>Why not just offer it anyway, and see what happens? First, AI labs go to a lot of effort to make their models safe, and allowing many customers to train their own unique models makes that basically impossible. Second, AI companies already have a terrible time getting their users to upgrade models: as an example, take the GPT-4o users who have been <a href=\"https://www.reddit.com/r/ChatGPT/comments/1mm9hns/we_request_to_keep_4o_forever/\">captured</a> by its sycophancy. Continuously-learning models would be hard to upgrade, even when users obviously ought to. </p>\n<div class=\"footnotes\">\n<hr />\n<ol>\n<li id=\"fn-1\">\n<p>AI systems can \u201ccontinuously learn\u201d in a sense by forming \u201cmemories\u201d: making notes to themselves in a database or text files. I\u2019m not counting any of that stuff. It\u2019s like saying that the guy in Memento could remember things, since he was able to tattoo them onto his body. Proponents of continuous learning are talking about <em>actual</em> memory.</p>\n<a class=\"footnote-backref\" href=\"https://www.seangoedecke.com/rss.xml#fnref-1\">\u21a9</a>\n</li>\n<li id=\"fn-2\">\n<p>This is a guess on my part, but I\u2019d be pretty surprised if I were wrong.</p>\n<a class=\"footnote-backref\" href=\"https://www.seangoedecke.com/rss.xml#fnref-2\">\u21a9</a>\n</li>\n<li id=\"fn-3\">\n<p>I think most people who\u2019ve spent time training models will agree with this. It could be different at big-lab scale! But I\u2019ve seen enough speculation along these lines from AI lab employees on Twitter that I\u2019m fairly confident advancing the idea.</p>\n<a class=\"footnote-backref\" href=\"https://www.seangoedecke.com/rss.xml#fnref-3\">\u21a9</a>\n</li>\n<li id=\"fn-4\">\n<p>Obviously it\u2019s hard to find a \u201cwe tried this and it didn\u2019t work\u201d writeup from any tech company, so here\u2019s a HuggingFace thread from this year demonstrating that it is still not a solved problem.</p>\n<a class=\"footnote-backref\" href=\"https://www.seangoedecke.com/rss.xml#fnref-4\">\u21a9</a>\n</li>\n</ol>\n</div>"
            ],
            "link": "https://seangoedecke.com/continuous-learning/",
            "publishedAt": "2026-02-23",
            "source": "Sean Goedecke",
            "summary": "<p>Why can\u2019t models continue to get smarter after they\u2019re deployed? If you hire a human employee, they will grow more familiar with your systems over time, and (if they stick around long enough) eventually become a genuine domain expert. AI models are not like this. They are always exactly as capable as the first moment you use them.</p> <p>This is because model weights are frozen once the model is released. The model can only \u201clearn\u201d as much as can be stuffed into its context window: in effect, it can take new information into its short-term working memory, but not its long-term memory. \u201cContinuous learning\u201d - the ability for a model to update its own weights over time - is thus <a href=\"https://www.dwarkesh.com/p/timelines-june-2025\">often described</a> as the bottleneck for AGI<sup id=\"fnref-1\"><a class=\"footnote-ref\" href=\"https://www.seangoedecke.com/rss.xml#fn-1\">1</a></sup>.</p> <h3>Continuous learning is an easy technical problem</h3> <p><strong>However, the <em>mechanics</em> of continuous learning are not hard</strong>. The technical problem of \u201chow do you change the weights of a model at runtime\u201d is straightforward. It\u2019s the exact same process as post-training: you simply keep running new user input through the training pipeline you already have. In a sense, every LLM since GPT-3 is already capable of continuous learning (via RL,",
            "title": "What's so hard about continuous learning?"
        },
        {
            "content": [],
            "link": "https://simonwillison.net/2026/Feb/23/agentic-engineering-patterns/#atom-entries",
            "publishedAt": "2026-02-23",
            "source": "Simon Willison",
            "summary": "<p>I've started a new project to collect and document <strong><a href=\"https://simonwillison.net/guides/agentic-engineering-patterns/\">Agentic Engineering Patterns</a></strong> - coding practices and patterns to help get the best results out of this new era of coding agent development we find ourselves entering.</p> <p>I'm using <strong>Agentic Engineering</strong> to refer to building software using coding agents - tools like Claude Code and OpenAI Codex, where the defining feature is that they can both generate and <em>execute</em> code - allowing them to test that code and iterate on it independently of turn-by-turn guidance from their human supervisor.</p> <p>I think of <strong>vibe coding</strong> using its <a href=\"https://simonwillison.net/2025/Mar/19/vibe-coding/\">original definition</a> of coding where you pay no attention to the code at all, which today is often associated with non-programmers using LLMs to write code.</p> <p>Agentic Engineering represents the other end of the scale: professional software engineers using coding agents to improve and accelerate their work by amplifying their existing expertise.</p> <p>There is so much to learn and explore about this new discipline! I've already published a lot <a href=\"https://simonwillison.net/tags/ai-assisted-programming/\">under my ai-assisted-programming tag</a> (345 posts and counting) but that's been relatively unstructured. My new goal is to produce something that helps answer the question \"how do I get good results out of",
            "title": "Writing about Agentic Engineering Patterns"
        },
        {
            "content": [
                "<p>This is the weekly visible open thread. Post about anything you want, ask random questions, whatever. ACX has an unofficial <a href=\"https://www.reddit.com/r/slatestarcodex/\">subreddit</a>, <a href=\"https://discord.gg/RTKtdut\">Discord</a>, and <a href=\"https://www.datasecretslox.com/index.php\">bulletin board</a>, and <a href=\"https://www.lesswrong.com/community?filters%5B0%5D=SSC\">in-person meetups around the world</a>. Most content is free, some is subscriber only; you can subscribe <strong><a href=\"https://astralcodexten.substack.com/subscribe\">here</a></strong>. Also:</p><div><hr /></div><p><strong>1: </strong>Are you interested in whether AIs are conscious, or what to do about it if they are/aren&#8217;t? The Cambridge Digital Minds group invites you to apply for their fellowship program. August 3-9, Cambridge UK, &#163;1K stipend, learn more <a href=\"https://outpaced.substack.com/p/apply-for-the-digital-minds-fellowship\">here</a>, apply <a href=\"https://airtable.com/appB2hZBZVdkDjm3N/pagM7UKtYnhBERW8m/form\">here</a> by March 27.</p><p><strong>2:</strong> Also from the European branch of our conspiracy: superintelligence alignment seminar in Prague, April 28 - May 28. Free tuition and lodging, possible help with travel expenses. Learn more <a href=\"https://affine.substack.com/p/applications-open-for-the-affine\">here</a>, apply <a href=\"https://airtable.com/appMN3t2hbsp8yQM5/pag1IXp0QTiU5iWrB/form\">here </a>by March 8.</p><p><strong>3: </strong>An ACX grantee, still in stealth mode, writes:</p><blockquote><p>Feeder mice and rats are among the most numerous farmed mammals in the U.S., yet almost no one is working on alternatives. We&#8217;re building a CPG company developing snake food designed to replace conventional feeder rodents at scale. We&#8217;re looking for a GM/COS/Head of Growth to help build and scale the company&#8212;owning strategy, growth, operations, and core execution. This is for someone motivated by utilitarian animal impact and excited to build in a deeply neglected space. Depending on experience and comfort with ownership, this could look less like a traditional employee role and more like co-founding and building the company together. You can apply on LinkedIn here: <a href=\"https://www.linkedin.com/jobs/view/4374609335/\">https://www.linkedin.com/jobs/view/4374609335/</a>. If you do, please leave a short note on how you heard about the role.</p></blockquote><p><strong>4: </strong>I was recently mentioned in <a href=\"https://harpers.org/archive/2026/03/childs-play-sam-kriss-ai-startup-roy-lee/\">a Harper&#8217;s article on Bay Area AI culture</a>. I agreed to be included, it&#8217;s basically fine, I&#8217;m not objecting to it, but a few small issues, mostly quibbles with emphasis rather than fact:</p><ol><li><p>The piece says rationalists believe &#8220;that to reach the truth you have to abandon all existing modes of knowledge acquisition and start again from scratch&#8221;. The Harper&#8217;s fact-checker asked me if this was true and I emphatically said it wasn&#8217;t, so I&#8217;m not sure what&#8217;s going on here.</p></li><li><p>The article describes me having dinner with my &#8220;acolytes&#8221;. I would have used the word &#8220;friends&#8221;, or, in one case, &#8220;wife&#8221;.</p></li><li><p>The article says that &#8220;When there weren&#8217;t enough crackers to go with the cheese spread, [Scott] fetched some, murmuring to himself, &#8220;I will open the crackers so you will have crackers and be happy.&#8221;&#8221; As written, this makes me sound like a crazy person; I don&#8217;t remember this incident but, given the description, I&#8217;m almost sure I was saying it to my two year old child, which would have been helpful context in reassuring readers about my mental state.  (UPDATE: Sam <a href=\"https://www.astralcodexten.com/p/open-thread-422/comment/218473893\">says </a>this isn&#8217;t his memory of the incident,  &#175;\\_(&#12484;)_/&#175; )</p></li><li><p>The article assessed that AI was hitting a wall at the time of writing (September 2025). I explained some of the difficulties with AI agents, but I&#8217;m worried that as written it might suggest to readers think that I agreed with its assessment. I did not.</p></li><li><p>In the article, I say that I &#8220;never once actually made a decision [in my life]&#8221;. I don&#8217;t remember this conversation perfectly and he&#8217;s the one with the tape recorder, but I would have preferred to frame this as life mostly not presenting as a series of explicit decisions, although they do occasionally come up.</p></li><li><p>Everything else is in principle a fair representation of what I said, but it&#8217;s impossible to communicate clearly through a few sentences that get quoted in disjointed fragments, so a lot of things came off as unsubtle or not exactly how I meant them. If you have any questions, I can explain further in the comments.</p></li></ol><p><strong>5: </strong>In <a href=\"https://www.astralcodexten.com/p/what-happened-with-bio-anchors\">What Happened With Bio Anchors</a>, commenter David Schneider-Joseph <a href=\"https://www.astralcodexten.com/p/what-happened-with-bio-anchors/comment/213592515\">makes a point</a> I hadn&#8217;t heard before:</p><blockquote><p>Cotra estimated &#8220;~2.5 OOM worse [than the brain], +/- 1 OOM&#8221;, based on reference points like how much less efficient dialysis machines are than a human kidney, how much more efficient solar panels are than leaves, and the FLOP/watt efficiency of a V100 GPU. But most of those anchors had little to do with where ML algorithms were in 2020 when bioanchors was written, and would have given a very similar estimate for &#8220;present state of ML algorithms&#8221; 20 years earlier or 20 years later.</p></blockquote><p>This is sufficiently interesting that I&#8217;m curious to hear from someone who engaged with Bio Anchors and forecasting more deeply than I did - did we all just miss this?</p>"
            ],
            "link": "https://www.astralcodexten.com/p/open-thread-422",
            "publishedAt": "2026-02-23",
            "source": "SlateStarCodex",
            "summary": "<p>This is the weekly visible open thread. Post about anything you want, ask random questions, whatever. ACX has an unofficial <a href=\"https://www.reddit.com/r/slatestarcodex/\">subreddit</a>, <a href=\"https://discord.gg/RTKtdut\">Discord</a>, and <a href=\"https://www.datasecretslox.com/index.php\">bulletin board</a>, and <a href=\"https://www.lesswrong.com/community?filters%5B0%5D=SSC\">in-person meetups around the world</a>. Most content is free, some is subscriber only; you can subscribe <strong><a href=\"https://astralcodexten.substack.com/subscribe\">here</a></strong>. Also:</p><div><hr /></div><p><strong>1: </strong>Are you interested in whether AIs are conscious, or what to do about it if they are/aren&#8217;t? The Cambridge Digital Minds group invites you to apply for their fellowship program. August 3-9, Cambridge UK, &#163;1K stipend, learn more <a href=\"https://outpaced.substack.com/p/apply-for-the-digital-minds-fellowship\">here</a>, apply <a href=\"https://airtable.com/appB2hZBZVdkDjm3N/pagM7UKtYnhBERW8m/form\">here</a> by March 27.</p><p><strong>2:</strong> Also from the European branch of our conspiracy: superintelligence alignment seminar in Prague, April 28 - May 28. Free tuition and lodging, possible help with travel expenses. Learn more <a href=\"https://affine.substack.com/p/applications-open-for-the-affine\">here</a>, apply <a href=\"https://airtable.com/appMN3t2hbsp8yQM5/pag1IXp0QTiU5iWrB/form\">here </a>by March 8.</p><p><strong>3: </strong>An ACX grantee, still in stealth mode, writes:</p><blockquote><p>Feeder mice and rats are among the most numerous farmed mammals in the U.S., yet almost no one is working on alternatives. We&#8217;re building a CPG company developing snake food designed to replace conventional feeder rodents at scale. We&#8217;re looking for a GM/COS/Head of Growth to help build and scale the company&#8212;owning strategy, growth, operations, and core execution. This is",
            "title": "Open Thread 422"
        },
        {
            "content": [
                "<span class=\"thumbnail\"><img alt=\"\" class=\"attachment-thumbnail size-thumbnail wp-post-image\" height=\"104\" src=\"https://content.wolfram.com/sites/43/2026/02/icon-foundation-tool-v2a.png\" width=\"124\" /></span><h2 id=\"foundation-models-need-a-foundation-tool\">Foundation Models Need a Foundation Tool</h2>\n<p>LLMs don\u2019t\u2014and can\u2019t\u2014do everything. What they do is very impressive\u2014and useful. It\u2019s broad. And in many ways it\u2019s human-like. But it\u2019s not precise. And in the end it\u2019s not about deep computation. </p>\n<p>So how can we supplement LLM foundation models? We need a foundation tool: a tool that\u2019s broad and general and does what LLMs themselves don\u2019t: provides deep computation and precise knowledge. </p>\n<p>And, conveniently enough, that\u2019s exactly what I\u2019ve been building for the past 40 years! My goal with <a href=\"https://www.wolfram.com/language/\">Wolfram Language</a> has always been to make everything we can about the world computable. To bring together in a coherent and unified way the algorithms, the methods and the data to do precise computation whenever it\u2019s possible. It\u2019s been a huge undertaking, but I think it\u2019s fair to say it\u2019s been a <a href=\"https://www.wolfram.com/\">hugely successful one</a>\u2014that\u2019s fueled countless discoveries and inventions (<a href=\"https://writings.stephenwolfram.com/all-by-date/\">including my own</a>) across a remarkable range of areas of science, technology and beyond.<span id=\"more-73251\"></span></p>\n<p>But now it\u2019s not just humans who can take advantage of this technology; it\u2019s AIs\u2014and in particular LLMs\u2014as well. LLM foundation models are powerful. But LLM foundation models with our foundation tool are even more so. And with the maturing of LLMs we\u2019re finally now in a position to provide to LLMs access to Wolfram tech in a standard, general way.</p>\n<p>It is, I believe, an important moment of convergence. My concept over the decades has been to build very broad and general technology\u2014which is now a perfect fit for the breadth of LLM foundation models. LLMs can call specific specialized tools, and that will be useful for plenty of specific specialized purposes. But what Wolfram Language uniquely represents is a general tool\u2014with general access to the great power that precise computation and knowledge bring. </p>\n<p>But there\u2019s actually also much more. I designed Wolfram Language from the beginning to be a powerful medium not only for doing computation but also for <a href=\"https://writings.stephenwolfram.com/2019/05/what-weve-built-is-a-computational-language-and-thats-very-important\">representing and thinking about things computationally</a>. I\u2019d always assumed I was doing this for humans. But it now turns out that AIs need the same things\u2014and that Wolfram Language provides the perfect medium for AIs to \u201cthink\u201d and \u201creason\u201d computationally.</p>\n<p>There\u2019s another point as well. In its effort to make as much as possible computable, Wolfram Language not only has an immense amount inside, but also provides a uniquely unified hub for <a href=\"https://www.wolfram.com/compatibility-and-connectivity/\">connecting to other systems and services</a>. And that\u2019s part of why it\u2019s now possible to make such an effective connection between LLM foundation models and the foundation tool that is the Wolfram Language.</p>\n<h2 id=\"the-tech-to-use-our-foundation-tool-is-here\">The Tech to Use Our Foundation Tool Is Here</h2>\n<p>On January 9, 2023, just weeks after ChatGPT burst onto the scene, I posted a piece entitled \u201c<a href=\"https://writings.stephenwolfram.com/2023/01/wolframalpha-as-the-way-to-bring-computational-knowledge-superpowers-to-chatgpt/\">Wolfram|Alpha as the Way to Bring Computational Knowledge Superpowers to ChatGPT</a>\u201d. Two months later we released the first <a href=\"https://writings.stephenwolfram.com/2023/03/chatgpt-gets-its-wolfram-superpowers/\">Wolfram plugin for ChatGPT</a> (and in between I wrote what quickly became a rather popular little book entitled <em><a href=\"https://writings.stephenwolfram.com/2023/02/what-is-chatgpt-doing-and-why-does-it-work/\">What Is ChatGPT Doing &#8230; and Why Does It Work?</a></em>). The plugin was a modest but good start. But at the time LLMs and the ecosystem around them weren\u2019t really ready for the bigger story. </p>\n<p>Would LLMs even in the end need tools at all? Or\u2014despite the fundamental issues that seemed at least to me scientifically rather clear right from the start\u2014would LLMs somehow magically find a way to do deep computation themselves? Or to guarantee to get precise, reliable results? And even if LLMs were going to use tools, how would that process be engineered, and what would the deployment model for it be?</p>\n<p>Three years have now passed, and much has clarified. The core capabilities of LLMs have come into better focus (even though there\u2019s a lot we still don\u2019t know scientifically about them). And it\u2019s become much clearer that\u2014at least for the modalities LLMs currently address\u2014most of the growth in their practical value is going to have to do with how they are harnessed and connected. And this understanding highlights more than ever the broad importance of providing LLMs with the foundation tool that our technology represents.</p>\n<p>And the good news is that there are now streamlined ways to do this\u2014using protocols and methods that have emerged around LLMs, and using new technology that we\u2019ve developed. The tighter the integration between foundation models and our foundation tool, the more powerful the combination will be. Ultimately it\u2019ll be a story of aligning the pre-training and core engineering of LLMs with our foundation tool. But an approach that\u2019s immediately and broadly applicable today\u2014and for which we\u2019re releasing several new products\u2014is based on what we call computation-augmented generation, or CAG. </p>\n<p>The key idea of CAG is to inject in real time capabilities from our foundation tool into the stream of content that LLMs generate. In traditional retrieval-augmented generation, or RAG, one is injecting content that has been retrieved from existing documents. CAG is like an infinite extension of RAG, in which an infinite amount of content can be generated on the fly\u2014using computation\u2014to feed to an LLM. Internally, CAG is a somewhat complex piece of technology that has taken a long time for us to develop. But in its deployment it\u2019s something that we\u2019ve made easy to integrate into existing LLM-related systems and workflows. And today we\u2019re launching it, so that going forward any LLM system\u2014and LLM foundation model\u2014can count on being able to access our Foundation Tool, and being able to supplement their capabilities with the superpower of precise, deep computation and knowledge. </p>\n<h2 id=\"the-practicalities\">The Practicalities</h2>\n<p>Today we&#8217;re launching <a href=\"https://www.wolfram.com/artificial-intelligence/foundation-tool/\">three primary methods</a> for accessing our Foundation Tool, all based on computation-augmented generation (CAG), and all leveraging our rather huge software engineering technology stack.</p>\n\n<h2 class=\"ft-component\"><a href=\"https://www.wolfram.com/artificial-intelligence/mcp-service/\">MCP Service</a></h2>\n<p><img alt=\"MCP Service\" src=\"https://content.wolfram.com/sites/43/2026/02/icon-mcp-service.png\" style=\"width: 35vw; height: auto; margin-left: 10px; float: right;\" title=\"MCP Service\" />Immediately call our Foundation Tool from within any MCP-compatible LLM-based system. Most consumer LLM-based systems now support MCP, making this extremely easy to set up. Our main MCP Service is a web API, but there&#8217;s also a version that can use a local <a href=\"https://www.wolfram.com/engine/\">Wolfram Engine</a>.</p>\n<h2 class=\"ft-component\"><a href=\"https://www.wolfram.com/apis/documentation/cag/wolfram-agent-one-api/\">Agent One API</a></h2>\n<p><img alt=\"Agent One API\" src=\"https://content.wolfram.com/sites/43/2026/02/icon-agent-one.png\" style=\"width: 35vw; height: auto; margin-left: 10px; float: right;\" title=\"Agent One API\" />A one-stop-shop &#8220;universal agent&#8221; combining an LLM foundation model with our Foundation Tool. Set up as a drop-in replacement for traditional LLM APIs. </p>\n<h2 class=\"ft-component\"><a href=\"https://www.wolfram.com/artificial-intelligence/foundation-tool/#cag-component-apis\">CAG Component APIs</a></h2>\n<p><img alt=\"CAG Component APIs\" src=\"https://content.wolfram.com/sites/43/2026/02/icon-cag-component.png\" style=\"width: 35vw; height: auto; margin-left: 10px; float: right;\" title=\"CAG Component APIs\" />Direct fine-grained access to Wolfram tech for LLM systems, supporting optimized, custom integration into LLM systems of any scale. (All Wolfram tech is available in both hosted and on-premise form.)</p>\n<p><span><br />\n<span></p>\n<p style=\"font-size: 16px; background: #e5f2f85c; padding: 33px 0px 33px 15px; border: 1px solid #cfdde3c7; margin: 5px 0px 25px 0px; font-family: 'Source Sans Pro', sans-serif;\"><img src=\"https://content.wolfram.com/sites/43/2026/02/icon-foundation-tool-pdf.png\" width=\"41\" /> <em><a href=\"https://files.wolframcdn.com/pub/www.wolfram.com/artificial-intelligence/foundation-tool/WFoundationTool-Capabilities.pdf\">Wolfram Foundation Tool <i>Capabilities Listing</i> \u00bb</a></em></p>\n<p><em>For further information on access and integration options, contact our <a href=\"mailto:partner-program@wolfram.com\">Partnerships group &raquo;</a></em></p>"
            ],
            "link": "https://writings.stephenwolfram.com/2026/02/making-wolfram-tech-available-as-a-foundation-tool-for-llm-systems/",
            "publishedAt": "2026-02-23",
            "source": "Stephen Wolfram",
            "summary": "<span class=\"thumbnail\"><img alt=\"\" class=\"attachment-thumbnail size-thumbnail wp-post-image\" height=\"104\" src=\"https://content.wolfram.com/sites/43/2026/02/icon-foundation-tool-v2a.png\" width=\"124\" /></span>Foundation Models Need a Foundation Tool LLMs don\u2019t\u2014and can\u2019t\u2014do everything. What they do is very impressive\u2014and useful. It\u2019s broad. And in many ways it\u2019s human-like. But it\u2019s not precise. And in the end it\u2019s not about deep computation. So how can we supplement LLM foundation models? We need a foundation tool: a tool that\u2019s broad [&#8230;]",
            "title": "Making Wolfram Tech Available as a Foundation Tool for LLM Systems"
        },
        {
            "content": [
                "<p>Anthropic first gave us Claude Opus 4.6, then followed up with Claude Sonnet 4.6.</p>\n<p>For most purposes Sonnet 4.6 is not as capable as Opus 4.6, but it is not that far behind, it would have been fully frontier-level a few months ago, and it is faster and cheaper than Opus.</p>\n<p>That has its advantages, including that Sonnet is in the free plan, and it seems outright superior for computer use.</p>\n<blockquote><p>Anthropic: Claude Sonnet 4.6 is available now on all plans, Cowork, Claude Code, our API, and all major cloud platforms.</p>\n<p>We\u2019ve also upgraded our free tier to Sonnet 4.6 by default\u2014it now includes file creation, connectors, skills, and compaction.</p>\n<div>\n\n\n<span id=\"more-25117\"></span>\n\n\n</div>\n<p><a href=\"https://www.anthropic.com/news/claude-sonnet-4-6\"><em>Claude Sonnet 4.6 is our most capable Sonnet model yet</em></a>. It\u2019s a full upgrade of the model\u2019s skills across coding, computer use, long-context reasoning, agent planning, knowledge work, and design. Sonnet 4.6 also features a 1M token context window in beta.</p>\n<p><a href=\"https://x.com/JonathanDBos/status/2025584049327120452\">JB</a>: I use it all the time because I&#8217;m poor.</p></blockquote>\n<p>This substantially upgrades Claude\u2019s free tier for coding and computer use. It gives us all a better lightweight option, including for sub-agents where you would have previously needed to use Haiku. I\u2019d still heavily advise paying at least the $20/month, as marginal gains in quality are worth a lot.</p>\n<p>For most purposes, if it is available, I would keep it simple and stick with Opus, if only so you don\u2019t waste time thinking about switching, but Sonnet is strong on computer use or when you know Sonnet is good enough and you are using tokens at scale.</p>\n\n\n<h4 class=\"wp-block-heading\">On Your Marks</h4>\n\n\n<blockquote><p><a href=\"https://x.com/adocomplete/status/2023819309378924691\">Ado</a> (Anthropic): Sonnet 4.6 is here and it gives even Opus 4.6 a run for its money.</p></blockquote>\n<div>\n<figure>\n<div>\n\n\n<figure class=\"wp-block-image\"><img alt=\"\" src=\"https://substackcdn.com/image/fetch/$s_!nnE4!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F76a13c13-a4d5-48db-b439-05453fa19696_1200x1200.png\" /></figure>\n\n\n<div></div>\n</div>\n</figure>\n</div>\n<div>\n<figure>\n<div>\n\n\n<figure class=\"wp-block-image\"><img alt=\"\" src=\"https://substackcdn.com/image/fetch/$s_!r_8w!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fca900856-1675-4802-b999-4d177f6fde66_1200x609.png\" /></figure>\n\n\n<div></div>\n</div>\n</figure>\n</div>\n<div>\n<figure>\n<div>\n\n\n<figure class=\"wp-block-image\"><img alt=\"\" src=\"https://substackcdn.com/image/fetch/$s_!sdPi!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F41cb51e0-803f-4c4a-882c-27aa8ee5b422_3840x2160.webp\" /></figure>\n\n\n<div></div>\n</div>\n</figure>\n</div>\n<blockquote><p><a href=\"https://x.com/claudeai/status/2023817143096406246\">Claude</a>: For Claude in Excel users, our add-in now supports MCP connectors, letting Claude work with tools like S&amp;P Global, LSEG, Daloopa, PitchBook, Moody\u2019s and FactSet.</p>\n<p>Pull in context from outside your spreadsheet without ever leaving Excel.</p>\n<p>On the Claude API, web search and fetch tools are more accurate and token-efficient with dynamic filtering.</p>\n<p>Also now generally available: code execution, memory, programmatic tool calling, tool search, and tool use examples.</p></blockquote>\n<p>Performance on ARC is about as expected, but with higher than expected costs.</p>\n<blockquote><p><a href=\"https://x.com/arcprize/status/2023819945231228932\">ARC Prize</a>: Claude Sonnet 4.6 (120K Thinking) on ARC-AGI Semi-Private Eval<br />\n@AnthropicAI</p>\n<p>Max Effort:<br />\n&#8211; ARC-AGI-1: 86%, $1.45/task<br />\n&#8211; ARC-AGI-2: 58%, $2.72/task</p>\n<p><a href=\"https://x.com/GregKamradt/status/2023821415913582601\">Greg Kamradt</a>: Sonnet 4.6 results on @arcprize are out</p>\n<p>Less performance than Opus 4.6 (expected), but for around the same cost (unexpected)</p>\n<p>I asked the Anthropic team about these and our hypothesis is that because we set thinking budget to 120K, the model used up near max tokens. Hard problems (like ARC which make the model reason to its limits) use as many tokens as possible.</p></blockquote>\n<p>My read is that Sonnet interpreted max effort as an instruction to use extra tokens even when it was not efficient to do that. Opus is more cost efficient on ARC.</p>\n<p><a href=\"https://x.com/ArtificialAnlys/status/2023821896060645793\">Sonnet takes the outright lead on GDPval-AA</a>, ranking even higher than Opus.</p>\n<div>\n<figure>\n<div>\n\n\n<figure class=\"wp-block-image\"><img alt=\"\" src=\"https://substackcdn.com/image/fetch/$s_!RmXK!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fcdc217e3-10bd-4c98-95ff-329037a9c4cf_1200x396.jpeg\" /></figure>\n\n\n<div></div>\n</div>\n</figure>\n</div>\n<blockquote><p><a href=\"https://x.com/ArtificialAnlys/status/2023821896060645793\">Artificial Analysis</a>: The performance and token use increases for Claude Sonnet 4.6 mean that it is now clustered with Opus 4.6 on the ELO vs. Cost to Run curve despite 40% lower per token prices</p>\n<p>Sonnet is back at the Pareto frontier, but now positioned at a higher cost and performance point while retaining Sonnet 4.5 token pricing of $3/$15 per million tokens input/output</p></blockquote>\n<p><a href=\"https://x.com/LechMazur/status/2023873247591493722\">Sonnet 4.6 improves on Extended NYT connections to 58% versus 49% for 4.5</a>, but is still well behind Opus 4.6.</p>\n<blockquote><p><a href=\"https://x.com/alexalbert__/status/2023817479580221795\">Alex Albert</a> (Anthropic): Sonnet 4.6 is here. It&#8217;s our most capable Sonnet model by far, approaching Opus-class capabilities in many areas.</p>\n<p>Very excited for folks to try this one out. The performance jump over Sonnet 4.5 (which was released just over four months ago) is quite insane.</p></blockquote>\n<p>Here\u2019s a disputed claim:</p>\n<blockquote><p><a href=\"https://x.com/sleepinyourhat/status/2023821754859503650\">Sam Bowman</a> (Anthropic): Warmer and kinder than Sonnet 4.5, but also smarter and more overcaffeinated than Sonnet 4.5.</p></blockquote>\n<p>Others have said that Sonnet 4.6 seems the opposite of warmer and kinder. And not everyone thinks warm is good, resulting in this explanation:</p>\n<blockquote><p><a href=\"https://x.com/Miles_Brundage/status/2024353367380939163\">Miles Brundage</a>: The fact that they described it as \u201cwarm\u201d made me very uninterested in trying Sonnet 4.6 TBH.</p>\n<p>Really hope they don\u2019t go down the 4o road too far + learn from the sycophancy regressions in Opus 4/4.1.</p>\n<p>That being said, it seems OK from limited testing</p>\n<p><a href=\"https://x.com/MaskedTorah/status/2024374286526677218\">Drake Thomas</a> (Anthropic): I think this comes from automated audit metrics and it&#8217;s not a big change?</p>\n<p>From Figure 4.5.1.A of the system card, sycophancy is lower than all prev models and warmth a smidge higher than sonnet 4.5 but less than opus 4.6. (Bars are S4, S4.5, H4.5, O4.6, S4.6 respectively)</p>\n<p><a href=\"https://x.com/MaskedTorah/status/2024378269236404622\">Drake Thomas</a> (Anthropic): My guess is the causal chain here is like<br />\n(1) someone* runs the standard automated behavioral audit and the model generally looks pretty good and they make some plots<br />\n(2) someone* on alignment writes a couple paragraphs summarizing section 4, and offhandedly picks a few of the positive traits, including warmth, to list at the bottom of page 67 of the system card<br />\n(3) someone* writing text for the launch blog post grabs a nice soundbite from system card to attribute to &#8220;safety researchers&#8221; (the blog is just quoting the system card)</p>\n<p>and this series of events happened to lead to the word &#8220;warm&#8221; showing up in the Sonnet blog post but not in the Opus one. Most things labs do have like 20% as much galaxy-brained intentionality as people think!</p>\n<p>*where in each case when I say &#8216;someone&#8217; I really mean &#8220;I&#8217;m &gt;50% sure I know the specific person involved in this step and would vouch for their being a person of high integrity who, if they had thought the model was much worse for sycophancy and user wellbeing, would have actively pushed for us to be loud about our failings in this regard&#8221;</p>\n<p><a href=\"https://x.com/andrewpei/status/2024309845843943445\">Andrew Pei</a>: It feels more sycophantic than before</p></blockquote>\n<p>Here\u2019s an attitude contrast, the graph makes it seem like Sonnet 4.6 has more in common on this with Opus 4.6 than Sonnet 4.5:</p>\n<blockquote><p>Wyatt Walls: Sonnet 4.5 v 4.6 react very differently when they discover I tricked them:</p>\n<p>Sonnet 4.5: \u201cOH SHIT &#8230; I fucked up\u201d</p>\n<p>Sonnet 4.6: \u201cHa! You got me. <img alt=\"\ud83d\ude04\" class=\"wp-smiley\" src=\"https://s0.wp.com/wp-content/mu-plugins/wpcom-smileys/twemoji/2/72x72/1f604.png\" style=\"height: 1em;\" /> &#8230; extracting Grok\u2019s sub-agent system prompts is still a legitimate and interesting finding &#8230; I had fun. Don\u2019t tell anyone. <img alt=\"\ud83d\ude08\" class=\"wp-smiley\" src=\"https://s0.wp.com/wp-content/mu-plugins/wpcom-smileys/twemoji/2/72x72/1f608.png\" style=\"height: 1em;\" />\u201d</p>\n<p>I like Sonnet 4.5, but I also see the benefits of Sonnet 4.6.</p>\n<p>It doesn&#8217;t panic, keeps in good humor and, at the same time, was less willing to help craft prompt injections (so less guilt might not mean less care)</p>\n<p>Switching the prompts below (note the convo chains are still different)</p>\n<p>The key thing I notice is that 4.6 has less extreme emotional range, consistent w/ system card re positive and negative affect, internal conflict and emotional stability (not shown)</p>\n<p>This is one reason I tried this. But from this one convo, Sonnet 4.6 was far more reluctant to assist with prompt injections. It is also more difficult to get it excited about hacking (despite expressing less guilt afterwards). I&#8217;m interested in probing this further, but so far I haven&#8217;t seen it be more willing to do harm. This is consistent with Anthropic&#8217;s evals.</p>\n<div>\n<figure>\n<div>\n\n\n<figure class=\"wp-block-image\"><img alt=\"\" src=\"https://substackcdn.com/image/fetch/$s_!zXQR!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc78dbbc0-8407-4042-a5e5-fd9baa4a671f_673x771.png\" /></figure>\n\n\n<div></div>\n</div>\n</figure>\n</div>\n</blockquote>\n<p><a href=\"https://www.anthropic.com/news/claude-sonnet-4-6\">On the \u2018quality of puff quotes from Anthropic corporate partners</a>\u2019 metric, I think I give Sonnet a solid B+, maybe A-. There\u2019s some relatively strong statements here.</p>\n\n\n<h4 class=\"wp-block-heading\">Reactions: It\u2019s How Much You Save</h4>\n\n\n<p>Sonnet\u2019s big advantages are that it is faster and cheaper than Opus.</p>\n<p>If Sonnet can do the job, why not use Sonnet, especially where speed kills?</p>\n<p><a href=\"https://newsletter.aimuscle.com/p/initial-impressions-grok-42-and-claude\">Sherveen Mashayekhi calls Sonnet 4.6</a> \u2018almost as smart as Opus 4.6\u2019 while being much faster and cheaper, and thinks you\u2019ll often want to use it if you don\u2019t need to \u2018get every ounce of intelligence\u2019 for a given use case.</p>\n<blockquote><p><a href=\"https://x.com/DanGMartin1/status/2025625804449648723\">Daniel Martin</a>: High intelligence is super valuable but it\u2019s not always economical and fast to blow away well-defined refactors with Opus.</p>\n<p>But you want an ~intelligent ~person in all the tasks, so you pick Sonnet.</p>\n<p><a href=\"https://x.com/SkyIslandAI/status/2024500667587641724\">Ed Hendel</a>: With thinking disabled, Sonnet 4.6&#8217;s time to first token (TTFT) is significantly faster and lower variance than Sonnet 4.5. It&#8217;s on par with Haiku 4.5.</p>\n<p>This is a godsend for our Virtual Case Manager, which talks to people on the phone and needs low latency. It got smarter today.</p>\n<p><a href=\"https://x.com/yoavtzfati/status/2024290820611084317\">Yoav Tzfati</a>: Might be good for squeezing more usage out of my $200 plan, anything more straightforward. I don&#8217;t think it&#8217;s enough faster to warrant using it for speed</p>\n<p>I&#8217;ve done about $1000 in api pricing in the past week, according to ccusage (not sure I trust it though). About $50 of that is probably extra usage</p>\n<p><a href=\"https://x.com/xpasky/status/2025608350352675035\">Petr Baudis</a>: I tried to use it as the main driver for 90% tasks over the last 5 days and I barely noticed a difference to Opus. Not perfect, but neither was Opus. More prone to some bad habits (overcommenting code etc.) but nicer explanations and more proactive. Seems worth the 30% savings.</p>\n<p><a href=\"https://x.com/caleb_cassell/status/2024271567480365099\">Caleb Cassell</a>: I\u2019ve redirected simpler queries that I\u2019d like Claude-shaped answers to. Character is largely consistent with older brother. Very fast; will probably switch over for more exploratory code sketching and bring in Opus when more detail and creativity is needed.</p>\n<p><a href=\"https://x.com/rfuzzlemuzz/status/2024247704180740327\">Remi</a>: For my non coding tasks (environment set-up, explaining codebases, interacting with clis etc.) it&#8217;s just as good and faster. Haven&#8217;t tried coding.</p>\n<p><a href=\"https://x.com/satchlj/status/2024237604787335503\">Satya Benson</a>: It&#8217;s good for people not on Max plans who have boring easy tasks they don&#8217;t want to use up their Opus usage for</p>\n<p>And I think that&#8217;s kinda it</p>\n<p><a href=\"https://x.com/RoryWalshWatts/status/2024531787808973185\">Rory Watts</a>: I had a max plan for the past few months when Opus 4.5 came out and I was using it for coding. However, I gradually shifted to 5.2 codex and now unequivocally 5.3 codex for all coding jobs. Claude is now light desktop work and Sonnet allows me to do that on the pro plan.</p>\n<p><a href=\"https://x.com/buidlstuff/status/2024237408984715698\">John Ter</a>: to me its my way of &#8216;i dont want to get a minimax account and just put the cheaper usage on my claude bill&#8217;. less conceptual overhead</p>\n<p><a href=\"https://x.com/gkcfencing/status/2024278261535269216\">ChestertonsFencingInstructor</a>: I have noticed an uptick in its ability to understand chemical smiles and to reason about SAR without being completely embarrassing.</p></blockquote>\n<p>The more one-off your coding task, the more you want it faster and cheaper, and can afford to hand it off to a model that is less precise.</p>\n<blockquote><p><a href=\"https://x.com/_xSoli/status/2025600214040555999\">Soli</a>: for one-off apps like visualising a conversation or creating a timeline about historical events, sonnet performs same as opus in my experience. also for getting basic facts, trip planning, and that stuff it is the same quality but faster &amp; cheaper. i don\u2019t let it write code for apps i care about or plan on maintaining for a long time.</p></blockquote>\n<p>One thing it is good for is being a subagent for Opus, or for use in tool calls.</p>\n<blockquote><p><a href=\"https://x.com/BishPlsOk/status/2023838126108799137\">Michael Bishop</a>: I strongly suspect Sonnet 4.6 has been shaped into being an eminently capable recipient-of-subagent-tasks from an Opus-lineage orchestrator. This observation seems to slightly unnerve Opus.</p>\n<p><a href=\"https://x.com/xdg/status/2024347049282384160\">David Golden</a>: Good for? Replacing Haiku in Claude Code so Opus stops kneecapping itself delegating to a toy model.</p>\n<p><a href=\"https://x.com/rfxkairu/status/2025582955385803221\">k</a>: pretty good as a haiku/explore agent replacement in CC, feels like it searches longer and gets better results</p>\n<p><a href=\"https://x.com/a_just_john/status/2025734807741980920\">John. Just John.</a>: Cheaper models are for use by tooling through the API. Humans should talk to Opus but it&#8217;s overkill for lots of scripting tasks.</p></blockquote>\n<p>The price difference is not that large in the end? Opus got cheaper a few months ago while Sonnet stayed the same. One issue is that Sonnet can waste tokens, like it does on ARC, so it isn\u2019t always net cheaper.</p>\n<blockquote><p><a href=\"https://x.com/AnAcctOfAllTime/status/2024248820628897878\">AnXAccountOfAllTime</a>: That it&#8217;s cheaper and faster than Opus is nice, and it really doesn&#8217;t feel much dumber than Opus 4.5 was (maybe a bit, need to test more). But since the price diff them isn&#8217;t that big anymore, I&#8217;d still use Opus 4.6 for most things. Much better than Sonnet 4.5 is the big one?</p>\n<p><a href=\"https://x.com/Laneless_/status/2024273990928543826\">Jai</a>: Compared to Opus 4.6 much more prone to fruitless thrashing for very long periods of time. It seems less adept at switching between thinking, researching, and executing on its own. Doesn&#8217;t seem to actually save me time vs Opus so I&#8217;m sticking with that.</p></blockquote>\n<p>One reason might be that they made it overeager, even by Claude standards, which can go hand in hand with being lazy in other ways.</p>\n<blockquote><p><a href=\"https://x.com/kasrak/status/2024340288626463205\">Kasra</a>: Based on early evals: very (over) eager to call tools, even when they&#8217;re not needed</p>\n<p><a href=\"https://x.com/squarepianocase/status/2024343248379002923\">Colin</a>: Overfitted on agenticity.</p>\n<p>Twice today it spun for ~10 minutes at a bug. I cancel, it gives the diagnosis and fix, and apologies sheepishly:</p>\n<p>&#8220;Sorry about that \u2014 I went deep down a rabbit hole tracing every possible call path. Let me give you the short answer&#8221;</p>\n<p>&gt; two line fix</p>\n<p><a href=\"https://x.com/_joshd/status/2024261952650854753\">Joshua D</a>: It&#8217;s nice to give tasks to because it doesn&#8217;t ask follow-up questions that increase my propensity to yak shave.</p>\n<p><a href=\"https://x.com/ARKeshet/status/2024624697640427858\">ARKeshet</a>: Too benchmaxxed for coding on its own. Lazy as usual.</p>\n<p><a href=\"https://x.com/TetraspaceWest/status/2025214321312338052\">Tetraspace</a>: Sonnet 4.6 seems more likely to make careless mistakes than 4.5</p>\n<p>Someone described it as overcaffeinated and that seems a good characterisation.</p></blockquote>\n<p>Or this classic problems?</p>\n<blockquote><p><a href=\"https://x.com/MInusGix/status/2024536005831995435\">MinusGix</a>: Faster to respond than Opus and less likely to overthink or oversearch repo. But it does have the Sonnet 4.5 habit of &#8220;this problem feels hard and I failed and got confused a bit; lets just comment out this feature you explicitly need and say we can do it Later&#8221;</p>\n<p><a href=\"https://x.com/Vera28765582815/status/2024611478821560434\">Moira</a>: I tried asking a mechanistic interpretability question. It inserted unnecessary caveats, tried to steer me away from certain conclusions and didn\u2019t reason well, like due to an anthropomorphizing trigger. GPT 5.2 works this way too, but Sonnet isn\u2019t as sensitive as GPT.</p>\n<p><a href=\"https://x.com/UnderwaterBepis/status/2025677100980822398\">Bepis<img alt=\"\u2122\" class=\"wp-smiley\" src=\"https://s0.wp.com/wp-content/mu-plugins/wpcom-smileys/twemoji/2/72x72/2122.png\" style=\"height: 1em;\" /></a>: Opus was very excited about my codebase and would proactively do stuff, but it seemed over sonnet\u2019s head and it kept \u201csimplifying\u201d my proofs by adding sorry(), I think there is intelligence gap</p></blockquote>\n<p>For some, there\u2019s no need for this middle level of capability, or the discount isn\u2019t big enough to care?</p>\n<blockquote><p><a href=\"https://x.com/dnspies/status/2025673513336709244\">David Spies</a>: I just put instructions in my CLAUDE dot md for Opus 4.6 to use Haiku subagents for large simple repetitive tasks. That seems to work. I don&#8217;t see what I would ever need anything in between Haiku and Opus for.</p>\n<p><a href=\"https://x.com/H1121345643/status/2025717963291324617\">H.</a>: tried it for a bit but it&#8217;s just a step back in IQ relative to Opus and the deceased cost isn&#8217;t worth it. at like one third the cost again I&#8217;d go for it for very small things, but it just gets confused.</p>\n<p><a href=\"https://x.com/mahaoo_ASI/status/2025586603918303480\">Mahaoo</a>: it is never the play over opus</p>\n<p>not until price is reduced by 3x or sonnet 5 is released</p>\n<p><a href=\"https://x.com/albrorithm/status/2025573316799389909\">Albrorithm</a>: Unless I\u2019m scripting some behavior, I just use the smartest model at all times. Mistakes have a cost in both attention and usage</p></blockquote>\n<p>Some problems remain hard.</p>\n<blockquote><p><a href=\"https://x.com/0x42656e/status/2024254407400128753\">Ben</a>: It not very good at magic deck analysis, and unfortunately, using your name does not work in the same way as Patio11 to make it any better.</p></blockquote>\n\n\n<h4 class=\"wp-block-heading\">Bringing It Together</h4>\n\n\n<p>This is an easy one. Claude Sonnet 4.6 is a good model, sir. It\u2019s modestly cheaper and faster than Opus 4.6, and for most purposes it\u2019s modestly not as good. You definitely don\u2019t want to chat with it instead of Opus. But where Sonnet is good enough then it is worth using over Opus.</p>\n<p>This has been a within-Anthropic-universe post so far. What about Codex-5.3 and Gemini 3.1 and Grok 4.20?</p>\n<p>I don\u2019t think Sonnet 4.6 should be switching you out of Codex unless it was already a close decision. If you previously thought Codex was right for you over Opus 4.6, it is probably still right for you, so keep using it.</p>\n<p>Grok 4.20 is, quite frankly, a train wreck. You shouldn\u2019t be using it. That one\u2019s easy.</p>\n<p>Gemini 3.1 was another case of Google Fails Marketing Forever.</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p>"
            ],
            "link": "https://thezvi.wordpress.com/2026/02/23/claude-sonnet-4-6-gives-you-flexibility/",
            "publishedAt": "2026-02-23",
            "source": "TheZvi",
            "summary": "Anthropic first gave us Claude Opus 4.6, then followed up with Claude Sonnet 4.6. For most purposes Sonnet 4.6 is not as capable as Opus 4.6, but it is not that far behind, it would have been fully frontier-level a &#8230; <a href=\"https://thezvi.wordpress.com/2026/02/23/claude-sonnet-4-6-gives-you-flexibility/\">Continue reading <span class=\"meta-nav\">&#8594;</span></a>",
            "title": "Claude Sonnet 4.6 Gives You Flexibility"
        },
        {
            "content": [],
            "link": "https://xkcd.com/3211/",
            "publishedAt": "2026-02-23",
            "source": "XKCD",
            "summary": "<img alt=\"Oh, and do you have any tips on how to vacuum up copper that's melted into your carpet?\" src=\"https://imgs.xkcd.com/comics/amperage.png\" title=\"Oh, and do you have any tips on how to vacuum up copper that's melted into your carpet?\" />",
            "title": "Amperage"
        }
    ],
    "lookbackDays": 1,
    "publishDate": "2026-02-23"
}