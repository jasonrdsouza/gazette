{
    "articles": [
        {
            "content": [
                "<div class=\"trix-content\">\n  <div>I've been running the <a href=\"https://frame.work/desktop\">Framework Desktop</a> for a few months here in Copenhagen now. It's an incredible machine. It's completely quiet, even under heavy, stress-all-cores load. It's tiny too, at just 4.5L of volume, especially compared to my old beautiful but bulky North tower running the 7950X \u2014 yet it's faster! And finally, it's simply funky, quirky, and fun!<br /><br />In some ways, the Framework Desktop is a curious machine. Desktop PCs are already very user-repairable! So why is Framework even bringing their talents to this domain? In the laptop realm, they're basically alone with that concept, but in the desktop space, it's rather crowded already. Yet it somehow still makes sense.<br /><br />Partly because Framework has gone with the AMD Ryzen AI Max 395+, which is technically a laptop CPU. You can find it in the ASUS ROG Flow Z13 and the HP ZBook Ultra. Which means it'll fit in a tiny footprint, and Framework apparently just wanted to see what they could do in that form factor. They clearly had fun with it. Look at mine:<br /><br />  <figure class=\"attachment attachment--preview attachment--lightboxable attachment--jpg\">\n      <a href=\"https://world.hey.com/dhh/636fb4ff/blobs/eyJfcmFpbHMiOnsiZGF0YSI6MjIxODcxMTQ2NywicHVyIjoiYmxvYl9pZCJ9fQ--37468cffd0c8dfd243eff24a502df2b9c6f95301603e601a355dfa7a89cc1a61/framework-desktop.jpg?disposition=attachment\" title=\"Download framework-desktop.jpg\">\n        <img alt=\"framework-desktop.jpg\" src=\"https://world.hey.com/dhh/636fb4ff/representations/eyJfcmFpbHMiOnsiZGF0YSI6MjIxODcxMTQ2NywicHVyIjoiYmxvYl9pZCJ9fQ--37468cffd0c8dfd243eff24a502df2b9c6f95301603e601a355dfa7a89cc1a61/eyJfcmFpbHMiOnsiZGF0YSI6eyJmb3JtYXQiOiJqcGciLCJyZXNpemVfdG9fbGltaXQiOlszODQwLDI1NjBdLCJxdWFsaXR5Ijo2MCwibG9hZGVyIjp7InBhZ2UiOm51bGx9LCJjb2FsZXNjZSI6dHJ1ZX0sInB1ciI6InZhcmlhdGlvbiJ9fQ--b3779d742b3242a2a5284869a45b2a113e0c177f0450c29f0baca1ee780f6604/framework-desktop.jpg\" />\n</a>\n  </figure><br /><br />There are 21 little tiles on the front that you can get in a bunch of different colors or with logos from Framework. Or you can 3D print your own! It's a welcome change in aesthetic from the brushed aluminum or gamer-focused RGBs approach that most of the competition is taking.<br /><br />But let's cut to the benchmarks. That's really why you'd buy a machine like the Framework Desktop. There are significantly cheaper mini PCs available from Beelink and others, but so far, Framework has the only AMD 395+ unit on sale that's completely silent (the GMKTec very much is not, nor is the Z3 Flow). And for me, that's just a dealbreaker. I can't listen to roaring fans anymore.<br /><br />Here's the key benchmark for me:<br /><br />  <figure class=\"attachment attachment--preview attachment--lightboxable attachment--png\">\n      <a href=\"https://world.hey.com/dhh/636fb4ff/blobs/eyJfcmFpbHMiOnsiZGF0YSI6MjIxODcxMzQyNywicHVyIjoiYmxvYl9pZCJ9fQ--5c63eaa28ceb3256f4d22364529154252dc2e27bfca899bacbfa968c284a50a4/image.png?disposition=attachment\" title=\"Download image.png\">\n        <img alt=\"image.png\" src=\"https://world.hey.com/dhh/636fb4ff/representations/eyJfcmFpbHMiOnsiZGF0YSI6MjIxODcxMzQyNywicHVyIjoiYmxvYl9pZCJ9fQ--5c63eaa28ceb3256f4d22364529154252dc2e27bfca899bacbfa968c284a50a4/eyJfcmFpbHMiOnsiZGF0YSI6eyJmb3JtYXQiOiJwbmciLCJyZXNpemVfdG9fbGltaXQiOlszODQwLDI1NjBdLCJxdWFsaXR5Ijo2MCwibG9hZGVyIjp7InBhZ2UiOm51bGx9LCJjb2FsZXNjZSI6dHJ1ZX0sInB1ciI6InZhcmlhdGlvbiJ9fQ--7edc7b21f6fad97fa22412618822c4d19725431f296c7ce47dc174b61535d27c/image.png\" />\n</a>\n  </figure><br /><br />That's the only type of multi-core workload I really sit around waiting on these days, and the Framework Desktop absolutely crushes it. It's almost twice as fast as the Beelink SER8 and still a solid third faster than the Beelink SER9 too. Of course, it's also a lot more expensive, but you're clearly getting some multi-core bang for your buck here!<br /><br />It's even a more dramatic difference to the Macs. It's a solid 40% faster than the M4 Max and 50% faster than the M4 Pro! Now some will say \"that's just because Docker is faster on Linux,\" and they're not entirely wrong. Docker runs natively on Linux, so for this test, where the MySQL/Redis/ElasticSearch data stores run in Docker while Ruby and the app code runs natively, that's part of the answer. Last I checked, it was about 25% of the difference.<br /><br />But so what? Docker is an integral part of the workflow for tons of developers. We use it to be able to run different versions of MySQL, Redis, and ElasticSearch for different applications on the same machine at the same time. You can't really do that without Docker. So this is what Real World benchmarks reveal.<br /><br />It's not just about having a Docker advantage, though. The AMD 395+ is also incredibly potent in RAW CPU performance. Those 16 Zen5 cores are running at 5.1GHz, and in Geekbench 6 multicore, this is how they stack up:<br /><br />  <figure class=\"attachment attachment--preview attachment--lightboxable attachment--png\">\n      <a href=\"https://world.hey.com/dhh/636fb4ff/blobs/eyJfcmFpbHMiOnsiZGF0YSI6MjIxODcxNTMzNCwicHVyIjoiYmxvYl9pZCJ9fQ--245e487838b293d7e29a2be7b1fe899f629e79484e218b23c8b6f48b30ba5eb4/image.png?disposition=attachment\" title=\"Download image.png\">\n        <img alt=\"image.png\" src=\"https://world.hey.com/dhh/636fb4ff/representations/eyJfcmFpbHMiOnsiZGF0YSI6MjIxODcxNTMzNCwicHVyIjoiYmxvYl9pZCJ9fQ--245e487838b293d7e29a2be7b1fe899f629e79484e218b23c8b6f48b30ba5eb4/eyJfcmFpbHMiOnsiZGF0YSI6eyJmb3JtYXQiOiJwbmciLCJyZXNpemVfdG9fbGltaXQiOlszODQwLDI1NjBdLCJxdWFsaXR5Ijo2MCwibG9hZGVyIjp7InBhZ2UiOm51bGx9LCJjb2FsZXNjZSI6dHJ1ZX0sInB1ciI6InZhcmlhdGlvbiJ9fQ--7edc7b21f6fad97fa22412618822c4d19725431f296c7ce47dc174b61535d27c/image.png\" />\n</a>\n  </figure><br /><br />Basically matching the M4 Max! And a good chunk faster than the M4 Pro (as well as other AMDs and Intel's 14900K!). No wonder that it's crazy quick with a full-core stress test like running 30,000 assertions for our <a href=\"https://hey.com/\">HEY</a> test suite.<br /><br />To be fair, the M4s are faster in single-core performance. Apple holds the crown there. It's about 20%. And you'll see that in benchmarks like Speedometer, which mostly measures JavaScript single-core performance. The Framework Desktop puts out 670 vs 744 on the M4 Pro on Speedometer 2.1. On SP 3.1, it's an even bigger difference with 35 vs 50.<br /><br />  <figure class=\"attachment attachment--preview attachment--lightboxable attachment--png\">\n      <a href=\"https://world.hey.com/dhh/636fb4ff/blobs/eyJfcmFpbHMiOnsiZGF0YSI6MjIxODcxNjg2NywicHVyIjoiYmxvYl9pZCJ9fQ--35e3e09143c39ad76f18904a062401e2ff9b348a23e9348261c9031e3959e156/image.png?disposition=attachment\" title=\"Download image.png\">\n        <img alt=\"image.png\" src=\"https://world.hey.com/dhh/636fb4ff/representations/eyJfcmFpbHMiOnsiZGF0YSI6MjIxODcxNjg2NywicHVyIjoiYmxvYl9pZCJ9fQ--35e3e09143c39ad76f18904a062401e2ff9b348a23e9348261c9031e3959e156/eyJfcmFpbHMiOnsiZGF0YSI6eyJmb3JtYXQiOiJwbmciLCJyZXNpemVfdG9fbGltaXQiOlszODQwLDI1NjBdLCJxdWFsaXR5Ijo2MCwibG9hZGVyIjp7InBhZ2UiOm51bGx9LCJjb2FsZXNjZSI6dHJ1ZX0sInB1ciI6InZhcmlhdGlvbiJ9fQ--7edc7b21f6fad97fa22412618822c4d19725431f296c7ce47dc174b61535d27c/image.png\" />\n</a>\n  </figure><br /><br />But I've found that all these computers feel fast enough in single-core performance these days. I can't actually feel the difference browsing on a machine that does 670 vs 744 on SP2.1. Hell, I can barely feel the difference between the SER8, which does 506, and the M4 Pro! The only time I actually feel like I'm waiting on anything is in multi-core workloads like the HEY test suite, and here the AMD 395+ is very near the fastest you can get for a consumer desktop machine today at any price.<br /><br />It gets even better when you bring price into the equation, though. The Framework Desktop with 64GB RAM + 2TB NVMe is $1,876. To get a Mac Studio with similar specs \u2014 M4 Max, 64GB RAM, 2TB NVMe \u2014 you'll literally spend nearly twice as much at $3,299! If you go for 128GB RAM, you'll spend $2,276 on the Framework, but $4,099 on the Mac. And it'll still be way slower for development work using Docker! The Framework Desktop is simply a great deal.<br /><br />Speaking of 64GB vs 128GB, I've been running the 64GB version, and I almost never get anywhere close to the limits. I think the highest I've seen in regular use is about 20GB of RAM in action. Linux is really efficient. Especially when you're using a window manager like Hyprland, as we do in Omarchy.<br /><br />The only reason you really want to go for the full 128GB RAM is to run local LLM models. The AMD 395+ uses unified memory, like Apple, so nearly all of it is addressable to be used by the GPU. That means you can run monster models, like the new 120b gpt-oss from OpenAI. Framework has a <a href=\"https://x.com/FrameworkPuter/status/1952854105606766922\">video showing them pushing out 40 tokens/second</a> doing just that. That seems about in range of the numbers I've seen from the M4 Max, which also seem in the 40-50 token/second range, but I'll defer to folks who benchmark local LLMs for the exact details on that.<br /><br />I tried running the new gpt-oss-20b on my 64GB machine, though, and I wasn't exactly blown away by the accuracy. In fact, I'd say it was pretty bad. I mean, exceptionally cool that it's doable, but very far off the frontier models we have access to as SaaS. So personally, this isn't yet something I actually use all that much in day-to-day development. I want the best models running at full speed, and right now that means SaaS.<br /><br />So if you just want the best, small computer that runs Linux superbly well out of the box, you should buy the Framework Desktop. It's completely quiet, fantastically fast, and super fun to look at.<br /><br />But I think it's also fair to mention that you can get something like <a href=\"https://world.hey.com/dhh/it-s-a-beelink-baby-243fdaf1\">a Beelink SER9 for half the price</a>! Yes, it's also only 2/3 the performance in multi-core, but it's just as fast in single-core. Most developers could totally get away with the SER9, and barely notice what they were missing. But there are just as many people for whom the extra $1,000 is worth the price to run the test suite 40 seconds quicker! You know who you are.<br /><br />Oh, before I close, I also need to mention that this thing is a gaming powerhouse. It basically punches about as hard as an RTX 4060! With an iGPU! That's kinda crazy. Totally new territory on the PC side for integrated graphics. ETA Prime has a video showing the same chip in the GMK Tech <a href=\"https://www.youtube.com/watch?v=RGKvUahL-_I&amp;t=551s\">running premier games at 1440p High Settings at great frame rates</a>. You can run most games under Linux these days too (thanks Valve and Steam Deck!), but if you need to dual boot with Windows, the dual NVMe slots in the Framework Desktop come very handy.<br /><br />Framework did good with this one. AMD really blew it out of the water with the 395+. We're spoiled to have such incredible hardware available for Linux at such appealing discounts over similar stuff from Cupertino. What a great time to love open source software and tinker-friendly hardware!</div>\n</div>"
            ],
            "link": "https://world.hey.com/dhh/the-framework-desktop-is-a-beast-636fb4ff",
            "publishedAt": "2025-08-07",
            "source": "DHH",
            "summary": "<div class=\"trix-content\"> <div>I've been running the <a href=\"https://frame.work/desktop\">Framework Desktop</a> for a few months here in Copenhagen now. It's an incredible machine. It's completely quiet, even under heavy, stress-all-cores load. It's tiny too, at just 4.5L of volume, especially compared to my old beautiful but bulky North tower running the 7950X \u2014 yet it's faster! And finally, it's simply funky, quirky, and fun!<br /><br />In some ways, the Framework Desktop is a curious machine. Desktop PCs are already very user-repairable! So why is Framework even bringing their talents to this domain? In the laptop realm, they're basically alone with that concept, but in the desktop space, it's rather crowded already. Yet it somehow still makes sense.<br /><br />Partly because Framework has gone with the AMD Ryzen AI Max 395+, which is technically a laptop CPU. You can find it in the ASUS ROG Flow Z13 and the HP ZBook Ultra. Which means it'll fit in a tiny footprint, and Framework apparently just wanted to see what they could do in that form factor. They clearly had fun with it. Look at mine:<br /><br /> <figure class=\"attachment attachment--preview attachment--lightboxable attachment--jpg\"> <a href=\"https://world.hey.com/dhh/636fb4ff/blobs/eyJfcmFpbHMiOnsiZGF0YSI6MjIxODcxMTQ2NywicHVyIjoiYmxvYl9pZCJ9fQ--37468cffd0c8dfd243eff24a502df2b9c6f95301603e601a355dfa7a89cc1a61/framework-desktop.jpg?disposition=attachment\" title=\"Download framework-desktop.jpg\"> <img alt=\"framework-desktop.jpg\" src=\"https://world.hey.com/dhh/636fb4ff/representations/eyJfcmFpbHMiOnsiZGF0YSI6MjIxODcxMTQ2NywicHVyIjoiYmxvYl9pZCJ9fQ--37468cffd0c8dfd243eff24a502df2b9c6f95301603e601a355dfa7a89cc1a61/eyJfcmFpbHMiOnsiZGF0YSI6eyJmb3JtYXQiOiJqcGciLCJyZXNpemVfdG9fbGltaXQiOlszODQwLDI1NjBdLCJxdWFsaXR5Ijo2MCwibG9hZGVyIjp7InBhZ2UiOm51bGx9LCJjb2FsZXNjZSI6dHJ1ZX0sInB1ciI6InZhcmlhdGlvbiJ9fQ--b3779d742b3242a2a5284869a45b2a113e0c177f0450c29f0baca1ee780f6604/framework-desktop.jpg\" /> </a> </figure><br /><br />There",
            "title": "The Framework Desktop is a beast"
        },
        {
            "content": [
                "<div class=\"trix-content\">\n  <div>I often give Google a lot of shit for <a href=\"https://killedbygoogle.com/\">shutting down services</a> whenever they're bored, hire a new executive, or face a three-day weekend. The company seems institutionally incapable of standing behind the majority of the products they launch for longer than a KPI cycle. But when the company <em>does</em> decide that something is pivotal to the business, it's an entirely different story. And that's the tale of YouTube: The King of Internet Archives (Video Edition).<br /><br />I've just <a href=\"https://www.youtube.com/@dhh37\">revived my YouTube channel</a> after realizing just how often video has become my go-to for learning. This <a href=\"https://world.hey.com/dhh/omarchy-is-on-the-move-8f848fa4\">entire</a> <a href=\"https://world.hey.com/dhh/omarchy-is-out-4666dd31\">Linux</a> <a href=\"https://world.hey.com/dhh/introducing-omakub-354db366\">adventure</a> I've gotten myself into started by watching YouTube creators like <a href=\"https://www.youtube.com/@ThePrimeTimeagen\">ThePrimeagen</a>, <a href=\"https://www.youtube.com/@typecraft_dev\">Typecraft</a>, and <a href=\"https://www.youtube.com/@BreadOnPenguins\">Bread on Penguins</a>. I learned about mechanical keyboards from <a href=\"https://www.youtube.com/@HipyoTech\">Hipyo Tech</a>. Devoured endless mini PC reviews from <a href=\"https://www.youtube.com/c/Level1Techs\">Level1Techs</a> and <a href=\"https://www.youtube.com/@Robtech\">Robtech</a>. Oh, and took a side quest into retro gaming handhelds with <a href=\"https://www.youtube.com/@RetroGameCorps\">Retro Game Corps</a>.<br /><br />But it was when putting together the playlists for my own channel that YouTube's royal role in internet archival really stood out. Like with <a href=\"https://youtu.be/Gzj723LkRJY?feature=shared\">the original Rails Demo from 19 years ago</a>(!), <a href=\"https://www.youtube.com/watch?v=0CDXJ6bMkMY\">the infamous talk at Startup School from 2009</a>, or <a href=\"https://www.youtube.com/watch?v=GFhoSMD6idk&amp;list=PL3m89j0mV0pfIEdg2fnwRf6Dkqr3xUd3L&amp;index=16\">my very first RailsConf keynote from 2006</a>. You'd be hard-pressed to find any video content on the internet from those days anywhere else. I notice that with podcast appearances from even just a few years ago that have gone missing already. Decentralization is wonderful in many ways, but it's very much subject to link rot and disappearing content.<br /><br />I love how you can pull in videos from other channels onto your own page as well. I've gathered up a bunch of <a href=\"https://www.youtube.com/playlist?list=PL3m89j0mV0peRhnSw1Gyi9mkMDEpZNsrn\">the many podcast appearances I've done</a>, and even dedicated an entire playlist to the <a href=\"https://www.youtube.com/playlist?list=PL3m89j0mV0pcb58xIf8LimphGrCHVwNSf\">69(!!) clips from the Lex Fridman interview</a>. The majority of <a href=\"https://www.youtube.com/playlist?list=PL3m89j0mV0pfIEdg2fnwRf6Dkqr3xUd3L\">the RailsConf and Rails World keynotes are on a list</a>. So is the old <a href=\"https://www.youtube.com/playlist?list=PL3m89j0mV0pdNAg6x9oq6S8Qz_4C-yuwj\">On Writing Software Well</a> series that I keep meaning to bring back.<br /><br />When you're working in small tech, it's really easy to become so jaded with big tech that you become ideologically blind to the benefits they do bring. I find no inconsistency in cheering much of the antitrust agenda against Google while also <a href=\"https://world.hey.com/dhh/don-t-make-google-sell-chrome-93cefbc6\">celebrating their work on Chrome</a> or their stewardship of YouTube. Any company as large as Google is bound to be full of contradictions, ambitions, and behaviors. We ought to have the capacity to cheer for the good parts and boo at the bad parts without feeling like frauds.<br /><br />So today, I choose to cheer for YouTube. It's an international treasure of learning, enthusiasm, and discovery.</div>\n</div>"
            ],
            "link": "https://world.hey.com/dhh/youtube-has-earned-its-crown-48f12ccc",
            "publishedAt": "2025-08-07",
            "source": "DHH",
            "summary": "<div class=\"trix-content\"> <div>I often give Google a lot of shit for <a href=\"https://killedbygoogle.com/\">shutting down services</a> whenever they're bored, hire a new executive, or face a three-day weekend. The company seems institutionally incapable of standing behind the majority of the products they launch for longer than a KPI cycle. But when the company <em>does</em> decide that something is pivotal to the business, it's an entirely different story. And that's the tale of YouTube: The King of Internet Archives (Video Edition).<br /><br />I've just <a href=\"https://www.youtube.com/@dhh37\">revived my YouTube channel</a> after realizing just how often video has become my go-to for learning. This <a href=\"https://world.hey.com/dhh/omarchy-is-on-the-move-8f848fa4\">entire</a> <a href=\"https://world.hey.com/dhh/omarchy-is-out-4666dd31\">Linux</a> <a href=\"https://world.hey.com/dhh/introducing-omakub-354db366\">adventure</a> I've gotten myself into started by watching YouTube creators like <a href=\"https://www.youtube.com/@ThePrimeTimeagen\">ThePrimeagen</a>, <a href=\"https://www.youtube.com/@typecraft_dev\">Typecraft</a>, and <a href=\"https://www.youtube.com/@BreadOnPenguins\">Bread on Penguins</a>. I learned about mechanical keyboards from <a href=\"https://www.youtube.com/@HipyoTech\">Hipyo Tech</a>. Devoured endless mini PC reviews from <a href=\"https://www.youtube.com/c/Level1Techs\">Level1Techs</a> and <a href=\"https://www.youtube.com/@Robtech\">Robtech</a>. Oh, and took a side quest into retro gaming handhelds with <a href=\"https://www.youtube.com/@RetroGameCorps\">Retro Game Corps</a>.<br /><br />But it was when putting together the playlists for my own channel that YouTube's royal role in internet archival really stood out. Like with <a href=\"https://youtu.be/Gzj723LkRJY?feature=shared\">the original Rails Demo from 19 years ago</a>(!), <a href=\"https://www.youtube.com/watch?v=0CDXJ6bMkMY\">the infamous talk at Startup",
            "title": "YouTube has earned its crown"
        },
        {
            "content": [
                "<p>The heritability wars have been a-raging. Watching these, I couldn\u2019t help but notice that there\u2019s near-universal confusion about what \u201cheritable\u201d means. Partly, that\u2019s because it\u2019s a subtle concept. But it also seems relevant that almost all explanations of heritability are very, <em>very</em> confusing. For example, here\u2019s <a href=\"https://en.wikipedia.org/wiki/Heritability#Definition\">Wikipedia\u2019s definition</a>:</p>\n\n<blockquote>\n  <p>Any particular phenotype can be modeled as the sum of genetic and environmental effects:</p>\n\n  <p>\u00a0\u00a0 Phenotype (<em>P</em>) = Genotype (<em>G</em>) + Environment (<em>E</em>).</p>\n\n  <p>Likewise the phenotypic variance in the trait \u2013 Var (<em>P</em>) \u2013 is the sum of effects as follows:</p>\n\n  <p>\u00a0\u00a0 Var(<em>P</em>) = Var(<em>G</em>) + Var(<em>E</em>) + 2 Cov(<em>G</em>,<em>E</em>).</p>\n\n  <p>In a planned experiment Cov(<em>G</em>,<em>E</em>) can be controlled and held at 0. In this case, heritability, <em>H</em>\u00b2, is defined as</p>\n\n  <p>\u00a0\u00a0 <em>H</em>\u00b2 = Var(<em>G</em>) / Var(<em>P</em>)</p>\n\n  <p><em>H</em>\u00b2 is the broad-sense heritability.</p>\n</blockquote>\n\n<p>Do you find that helpful? I hope not, because it\u2019s a mishmash of undefined terminology, unnecessary equations, and borderline-false statements. If you\u2019re in the mood for a mini-polemic:</p>\n\n<ol>\n  <li>Phenotype (<em>P</em>) is never defined. This is a minor issue, since it just means \u201ctrait\u201d.</li>\n  <li>Genotype (<em>G</em>) is never defined. This is a <em>huge</em> issue, since it\u2019s very tricky and heritability makes no sense without it.</li>\n  <li>Environment (<em>E</em>) is never defined. This is worse than it seems, since in heritability, different people use \u201cenvironment\u201d and <em>E</em> to refer to different things.</li>\n  <li>When we write <em>P</em> = <em>G</em> + <em>E</em>, are we assuming some kind of linear interaction? The text implies not, but why? What does this equation mean? If this equation is always true, then why do people often add other stuff like <em>G</em> \u00d7 <em>E</em> on the right?</li>\n  <li>The text states that <em>if</em> do you do a planned experiment (how?) and make Cov(<em>G</em>, <em>E</em>) = 0, <em>then</em> heritability is Var(<em>G</em>) / Var(<em>P</em>). But in fact, heritability is <em>always</em> defined that way. You don\u2019t need a planned experiment and it\u2019s fine if Cov(<em>G</em>, <em>E</em>) \u2260 0.</li>\n  <li>And\u2014wait a second\u2014that definition doesn\u2019t refer to environmental effects at all. So what was the point of introducing them? What was the point of writing <em>P</em> = <em>G</em> + <em>E</em>? What are we doing?</li>\n</ol>\n\n<p>Reading this almost does more harm than good. While the final definition is correct, it never even attempts to explain what <em>G</em> and <em>P</em> are, it gives an incorrect condition for when the definition applies, and instead mostly devotes itself to an unnecessary digression about environmental effects. The rest of the page doesn\u2019t get much better. Despite being 6700 words long, I think it would be <em>impossible</em> to understand heritability simply by reading it.</p>\n\n<p>Meanwhile, some people argue that heritability is meaningless for human traits like intelligence or income or personality. They claim that those traits are the product of complex interactions between genes and the environment and it\u2019s impossible to disentangle the two. These arguments have always struck me as \u201csuspiciously convenient\u201d. I figured that the people making them couldn\u2019t cope with the hard reality that genes are very important and have an enormous influence on what we are.</p>\n\n<p>But I increasingly feel that the skeptics have a point. While I think it\u2019s a fact that most human traits are substantially heritable, it\u2019s also true the technical definition of heritability is <em>really</em> weird, and simply does not mean what most people think it means.</p>\n\n<p>In this post, I will explain <em>exactly</em> what heritability is, while assuming no background. I will skip everything that can be skipped but\u2014unlike most explanations\u2014I will not skip things that can\u2019t be skipped. Then I\u2019ll go through a series of puzzles demonstrating just how strange heritability is.</p>\n\n<h2 id=\"what-is-heritability\">What is heritability?</h2>\n\n<p>How tall you are depends on your genes, but also on what you eat, what diseases you got as a child, and how much gravity there is on your home planet. And all those things interact. How do you take all that complexity and reduce it to a single number, like \u201c80% heritable\u201d?</p>\n\n<p>The short answer is: Statistical brute force. The long answer is: Read the rest of this post.</p>\n\n<p>It turns out that the hard part of heritability isn\u2019t heritability. Lurking in the background is a slippery concept known as a <em>genotypic value</em>. Discussions of heritability often skim past these. Quite possibly, just looking at the words \u201cgenotypic value\u201d, you are thinking about skimming ahead right now. Resist that urge! Genotypic values are the core concept, and without them you cannot possibly understand heritability.</p>\n\n<p>For any trait, your genotypic value is the \u201ctypical\u201d outcome if someone with your DNA were raised in many different random environments. In principle, if you wanted to know your genotypic height, you\u2019d need to do this:</p>\n\n<ol>\n  <li>Create a million embryonic clones of yourself.</li>\n  <li>Implant them in the wombs of randomly chosen women around the world who were about to get pregnant on their own.</li>\n  <li>Convince them to raise those babies exactly like a baby of their own.</li>\n  <li>Wait 25 years, find all your clones and take their average height.</li>\n</ol>\n\n<p>Since you can\u2019t / shouldn\u2019t do that, you\u2019ll never know your genotypic height. But that\u2019s how it\u2019s defined in principle\u2014the average height someone with your DNA would grow to in a random environment. If you got lots of food and medical care as a child, your actual height is probably above your genotypic height. If you suffered from rickets, your actual height is probably lower than your genotypic height.</p>\n\n<p>Comfortable with genotypic values? OK. Then (broad-sense) heritability is easy. It\u2019s the ratio</p>\n\n<p>\u00a0\u00a0 <code class=\"language-plaintext highlighter-rouge\">heritability = var[genotype] / var[height].</code></p>\n\n<p>Here, <code class=\"language-plaintext highlighter-rouge\">var</code> is the <a href=\"https://en.wikipedia.org/wiki/Variance\">variance</a>, basically just how much things vary in the population. Among all adults worldwide, <code class=\"language-plaintext highlighter-rouge\">var[height]</code> is around 50 cm\u00b2. (Incidentally, did you know that variance was invented <a href=\"http://digamoo.free.fr/fisher1919.pdf\">for the purpose</a> of defining heritability?)</p>\n\n<p>Meanwhile, <code class=\"language-plaintext highlighter-rouge\">var[genotype]</code> is how much <em>genotypic</em> height varies in the population. That might seem hopeless to estimate, given that we don\u2019t know anyone\u2019s genotypic height. But it turns out that we can still estimate the <em>variance</em> using, e.g., pairs of adopted twins, and it\u2019s thought to be around 40 cm\u00b2. If we use those numbers, the heritability of height would be</p>\n\n<p>\u00a0\u00a0 <code class=\"language-plaintext highlighter-rouge\">heritability \u2248 (40 cm\u00b2) / (50 cm\u00b2) \u2248 0.8.</code></p>\n\n<p>People often convert this to a percentage and say \u201cheight is 80% heritable\u201d. I\u2019m not sure I like that, since it masks heritability\u2019s true nature as a ratio. But everyone does it, so I\u2019ll do it too. People who really want to be intimidating might also say, \u201cgenes explain 80% of the variance in height\u201d.</p>\n\n<p>Of course, basically the same definition works for any trait, like weight or income or fondness for pseudonymous existential angst science blogs. But instead of replacing \u201cheight\u201d with \u201ctrait\u201d, biologists have invented the ultra-fancy word \u201cphenotype\u201d and write</p>\n\n<p>\u00a0\u00a0 <code class=\"language-plaintext highlighter-rouge\">heritability = var[genotype] / var[phenotype].</code></p>\n\n<p>The word \u201cphenotype\u201d suggests some magical concept that would take years of study to understand. But don\u2019t be intimidated. It just means the actual observed value of some trait(s). You can measure your phenotypic height with a tape measure.</p>\n\n<h2 id=\"on-meaning\">On meaning</h2>\n\n<p>Let me make two points before moving on.</p>\n\n<p>First, this definition of heritability assumes nothing. We are not assuming that genes are independent of the environment or that \u201cgenotypic effects\u201d combine linearly with \u201cenvironmental effects\u201d. We are not assuming that genes are in <a href=\"https://en.wikipedia.org/wiki/Hardy%E2%80%93Weinberg_principle\">Hardy-Weinberg equilibrium</a>, whatever that is. No. I didn\u2019t talk about that stuff because I don\u2019t need to. There are no hidden assumptions. The above definition always works.</p>\n\n<p>Second, many normal English words have parallel technical meanings, such as <a href=\"https://en.wikipedia.org/wiki/Field_(physics)\">\u201cfield\u201d</a>, <a href=\"https://en.wikipedia.org/wiki/Insulator_(electricity)\">\u201cinsulator\u201d</a>, <a href=\"https://en.wikipedia.org/wiki/Phase\">\u201cphase\u201d</a>, <a href=\"https://en.wikipedia.org/wiki/Measure_(mathematics)\">\u201cmeasure\u201d</a>, <a href=\"https://en.wikipedia.org/wiki/Tree_(abstract_data_type)\">\u201ctree\u201d</a>, or <a href=\"https://en.wikipedia.org/wiki/Stack_(abstract_data_type)\">\u201cstack\u201d</a>. Those are all nice, because they\u2019re evocative and it\u2019s almost always clear from context which meaning is intended. But sometimes, scientists scientists redefine existing words to mean something technical that overlaps but also <em>contradicts</em> the normal meaning, as in <a href=\"https://en.wikipedia.org/wiki/Salt_(chemistry)\">\u201csalt\u201d</a>, <a href=\"https://en.wikipedia.org/wiki/Glass\">\u201cglass\u201d</a>, <a href=\"https://en.wikipedia.org/wiki/Normal_distribution\">\u201cnormal\u201d</a>,  <a href=\"https://dynomight.net/valid-invalid/#2\">\u201cberry\u201d</a>, or <a href=\"https://dynomight.net/valid-invalid/#2\">\u201cnut\u201d</a>. These all cause confusion, but \u201cheritability\u201d must be the most egregious case in all of science.</p>\n\n<p>Before you ever heard the technical definition of heritability, you surely had some fuzzy concept in your mind. Personally, I thought of heritability as meaning how many \u201cpoints\u201d you get from genes versus the environment. If charisma was 60% heritable, I <a href=\"https://dynomight.net/heritability/\">pictured</a> each person has having 10 total \u201ccharisma points\u201d, 6 of which come from genes, and 4 from the environment:</p>\n\n<div class=\"language-plaintext highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code>  Genes\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u2605\u2605\u2605\u2606\u2606\u2606\n  Environment\u00a0\u2605\u2606\u2606\u2606\n  Total\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u2605\u2605\u2605\u2605\u2606\u2606\u2606\u2606\u2606\u2606\n</code></pre></div></div>\n\n<p>If you take nothing else from this post, please remember that <strong>the technical definition of heritability does not work like that</strong>.  You might hope that if we add some plausible assumptions, the above ratio-based definition would simplify into something nice and natural, that aligns with what \u201cheritability\u201d means in normal English. But that does not happen. If that\u2019s confusing, well, it\u2019s not my fault.</p>\n<h2 id=\"intermission\">Intermission</h2>\n\n<p>Not sure what\u2019s happening here, but it seems relevant.</p>\n\n<p><a href=\"https://www.nga.gov/artworks/46107-madonna-and-child\"><img alt=\"\" src=\"https://dynomight.net/img/heritable/madonna_and_child.jpg\" /></a></p>\n\n<h2 id=\"heritability-puzzles\">Heritability puzzles</h2>\n\n<p>So \u201cheritability\u201d is just the ratio of genotypic and phenotypic variance. Is that so bad?</p>\n\n<p>I think\u2026 maybe?</p>\n\n<p><strong>How heritable is eye color?</strong></p>\n\n<p>Close to 100%.</p>\n\n<p>This seems obvious, but let\u2019s justify it using our definition that <code class=\"language-plaintext highlighter-rouge\">heritability = var[genotype] / var[phenotype]</code>.</p>\n\n<p>Well, people have the same eye color, no matter what environment they are raised in. That means that genotypic eye color and phenotypic eye color are the same thing. So they have the same variance, and the ratio is 1. Nothing tricky here.</p>\n\n<p><strong>How heritable is speaking Turkish?</strong></p>\n\n<p>Close to 0%.</p>\n\n<p>Your native language is determined by your environment. If you grow up in a family that speaks Turkish, you speak Turkish. Genes don\u2019t matter.</p>\n\n<p>Of course, there are lots of genes that are <em>correlated</em> with speaking Turkish, since Turks are not, genetically speaking, a random sample of the global population. But that doesn\u2019t matter, because if you put Turkish babies in Korean households, they speak Korean. Genotypic values are defined by what happens in a <em>random</em> environment, which breaks the correlation between speaking Turkish and having Turkish genes.</p>\n\n<p>Since 1.1% of humans speak Turkish, the genotypic value for speaking Turkish is around 0.011 for everyone, no matter their DNA. Since that\u2019s basically constant, the genotypic variance is near zero, and heritability is near zero.</p>\n\n<p><strong>How heritable is speaking English?</strong></p>\n\n<p>Perhaps 30%. Probably somewhere between 10% and 50%. Definitely more than zero.</p>\n\n<p>That\u2019s right. Turkish isn\u2019t heritable but English is. <em>Yes it is</em>. If you ask an LLM, it will tell you that the heritability of English is zero. But the LLM is wrong and I am right.</p>\n\n<p>Why? Let me first acknowledge that Turkish is a <em>little</em> bit heritable. For one thing, some people have genes that make them non-verbal. And there\u2019s surely some genetic basis for being a crazy polyglot that learns many languages for fun. But speaking Turkish as a second language is <a href=\"https://en.wikipedia.org/wiki/List_of_languages_by_total_number_of_speakers\">quite rare</a>, meaning that the genotypic value of speaking Turkish is <em>close</em> to 0.011 for <em>almost</em> everyone.</p>\n\n<p>English is different. While only 1 in 20 people in the world speak English as a first language, one in seven learn it as a second language. And who does that? Educated people.</p>\n\n<details>\n  \nMost people say educational attainment is around 40% heritable (though we'll return to this later). My guess is that speaking English as a second language is similar. But since there's a minority of native speakers (where genes don't really matter), I'm dropping my estimate to 30%.\n\n\n  <p>Some <a href=\"https://theinfinitesimal.substack.com/p/no-intelligence-is-not-like-height\">argue</a> the heritability of educational attainment is much lower. I\u2019d like to avoid debating the exact numbers, but note that these lower numbers are usually estimates of \u201cnarrow-sense\u201d heritability rather than \u201cbroad-sense\u201d heritability as we\u2019re talking about. So they <em>should</em> be lower. (I\u2019ll explain the difference later.) It\u2019s entirely possible that broad-sense heritability is lower than 40%, but everyone agrees it\u2019s much larger than zero. So the heritability of English is surely much larger than zero, too.</p>\n\n</details>\n\n<p><strong>Say there\u2019s an island where genes have no impact on height. How heritable is height among people on this island?</strong></p>\n\n<p>0%.</p>\n\n<p>There\u2019s nothing tricky here.</p>\n\n<p><strong>Say there\u2019s an island where genes entirely determine height. How heritable is height?</strong></p>\n\n<p>100%.</p>\n\n<p>Again, nothing tricky.</p>\n\n<p><strong>Say there\u2019s an island where neither genes nor the environment influence height and everyone is exactly 165 cm tall. How heritable is height?</strong></p>\n\n<p>It\u2019s undefined.</p>\n\n<p>In this case, everyone has exactly the same phenotypic and genotypic height, namely 165 cm. Since those are both constant, their variance is zero and heritability is zero divided by zero. That\u2019s meaningless.</p>\n\n<p><strong>Say there\u2019s an island where some people have genes that predispose them to be taller than others. But the island is ruled by a cruel despot who denies food to children with taller genes, so that on average, everyone is 165 \u00b1 5 cm tall. How heritable is height?</strong></p>\n\n<p>0%.</p>\n\n<p>On this island, everyone has a genotypic height is 165 cm. So genotypic variance is zero, but phenotypic variance is positive, due to the \u00b1 5 cm random variation. So heritability is zero divided by some positive number.</p>\n\n<p><strong>Say there\u2019s an island where some people have genes that predispose them to be tall and some have genes that predispose them to be short. But, the same genes that make you tall also make you semi-starve your children, so in practice everyone is exactly 165 cm tall. How heritable is height?</strong></p>\n\n<p>\u221e%. Not 100%, mind you, infinitely heritable.</p>\n\n<p>To see why, note that if babies with short/tall genes are adopted by parents with short/tall genes, there are four possible cases.</p>\n\n<table>\n  <thead>\n    <tr>\n      <th>Baby genes</th>\n      <th>Parent genes</th>\n      <th>Food</th>\n      <th>Height</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>Short</td>\n      <td>Short</td>\n      <td>Lots</td>\n      <td>165 cm</td>\n    </tr>\n    <tr>\n      <td>Short</td>\n      <td>Tall</td>\n      <td>Semi-starvation</td>\n      <td>Less than 165 cm</td>\n    </tr>\n    <tr>\n      <td>Tall</td>\n      <td>Short</td>\n      <td>Lots</td>\n      <td>More than 165 cm</td>\n    </tr>\n    <tr>\n      <td>Tall</td>\n      <td>Tall</td>\n      <td>Semi-starvation</td>\n      <td>165 cm</td>\n    </tr>\n  </tbody>\n</table>\n\n<p>If a baby with short genes is adopted into random families, they will be shorter on average than if a baby with tall genes is adopted into random families. So genotypic height varies. However, in reality, everyone is the same height, so <em>phenotypic</em> height is constant. So genotypic variance is positive while phenotypic variance is zero. Thus, heritability is some positive number divided by zero, i.e. infinity.</p>\n\n<p>(Are you worried that humans are \u201cdiploid\u201d, with two genes (alleles) at each locus, one from each biological parent? Or are you worried that when multiple people raise a kid they all tend to have thoughts on the merits of semi-starvation? If so, please pretend people on this island reproduce asexually. Or, if you like, pretend that there\u2019s strong assortative mating so that everyone in the population either has all-short or all-tall genes and only breeds with other people with the same genes. Also, don\u2019t find the hypothetical.)</p>\n\n<p><strong>Say there\u2019s an island where neither genes nor the environment influence height. Except, some people have a gene that makes them inject their babies with human growth hormone, which makes them 5 cm taller. How heritable is height?</strong></p>\n\n<p>0%.</p>\n\n<p>True, people with that gene will tend be taller. And the gene is <em>causing</em> them to be taller. But if babies are adopted into random families, it\u2019s the genes of the <em>parents</em> that determine if they get injected or not. So everyone has the same genotypic height, genotypic variance is zero, and heritability is zero.</p>\n\n<p><strong>Say there are two islands. They all live the same way and have the same gene pool, except people on island A have some gene that makes them grow to be 150 \u00b1 5 cm tall, while on island B they have a gene that makes them grow to be 160 \u00b1 5 cm tall. How heritable is height?</strong></p>\n\n<p>It\u2019s 0% for island A and 0% for island B, and 50% for the two islands together.</p>\n\n<p>Why? Well on island A, everyone has the same genotypic height, namely 150 cm. Since that\u2019s constant, genotypic variance is zero. Meanwhile, phenotypic height varies a bit, so phenotypic variance is positive. Thus, heritability is zero.</p>\n\n<p>For similar reasons, heritability is zero on island B.</p>\n\n<p>But if you put the two islands together, half of people have a genotypic height of 150 cm and half have a genotypic height of 160 cm, so suddenly (via math) genotypic variance is 25 cm\u00b2. There\u2019s some extra random variation so (via more math) phenotypic variance turns out to be 50 cm\u00b2. So heritability is 25 / 50 = 50%.</p>\n\n<details>\n  \n(Math)\n\n\n  <p>If you combine the populations, then genotypic variance is</p>\n\n  <div class=\"language-plaintext highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code>Var[150 cm + 10 cm \u00d7 Bernoulli(0.5)]\n = (10 cm)\u00b2 \u00d7 Var[Bernoulli(0.5)]\n = (10 cm)\u00b2 \u00d7 0.25\n = 25 cm\u00b2.\n</code></pre></div>  </div>\n\n  <p>Meanwhile phenotypic variance is</p>\n\n  <div class=\"language-plaintext highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code>Var[150 cm + 10 cm \u00d7 Bernoulli(0.5) + 5 cm \u00d7 Normal(0,1)]\n = (10 cm)\u00b2 \u00d7 Var[Bernoulli(0.5)] + (5 cm)\u00b2 \u00d7 Var[Normal(0,1)]\n = (10 cm)\u00b2 \u00d7 0.25 + (5 cm)\u00b2 \u00d7 1\n = 50 cm\u00b2.\n</code></pre></div>  </div>\n\n</details>\n\n<p><strong>Say there\u2019s an island where neither genes nor the environment influence height. Except, some people have a gene that makes them inject their babies with human growth hormone, which makes them 5 cm taller. How heritable is height?</strong></p>\n\n<p>0%.</p>\n\n<p>True, people with that gene will tend be taller. And the gene is <em>causing</em> them to be taller. But if babies are adopted into random families, it\u2019s the genes of the <em>parents</em> that determine if they get injected or not. So everyone has the same genotypic height, genotypic variance is zero, and heritability is zero.</p>\n\n<p><strong>Suppose there\u2019s an island where neither genes nor the environment influence height. Except, some people have a gene that makes them, as babies, talk their parents into injecting them with human growth hormone. The babies are very persuasive. How heritable is height?</strong></p>\n\n<p>We\u2019re back to 100%.</p>\n\n<p>The difference with the previous scenario is that now babies with that gene get injected with human growth hormone no matter who their parents are. Since nothing else influences height, genotype and phenotype are the same, have the same variance, and heritability is 100%.</p>\n\n<p><strong>Suppose there\u2019s an island where neither genes nor the environment influence height. Except, there are crabs that seek out blue-eyed babies and inject them with human growth hormone. The crabs, they are unstoppable. How heritable is height?</strong></p>\n\n<p>Again, 100%.</p>\n\n<p>Babies with DNA for blue eyes get injected. Babies without DNA for blue eyes don\u2019t. Since nothing else influences height, genotype and phenotype are the same and heritability is 100%.</p>\n\n<p>Note that if the crabs were seeking out <em>parents</em> with blue eyes and then injecting their babies, then height would be 0% heritable.</p>\n\n<p>It doesn\u2019t matter that human growth hormone is weird thing that\u2019s coming from outside the baby. It doesn\u2019t matter if we think human growth hormone should be semantically classified as part of \u201cthe environment\u201d. It doesn\u2019t matter that heritability would drop to zero if you killed all the crabs, or that the direct causal effect of the relevant genes has nothing to do with hight. Heritability is a ratio and doesn\u2019t care.</p>\n\n<h2 id=\"what-good-is-heritability\">What good is heritability?</h2>\n\n<p>Heritability can be high even when genes have no <em>direct</em> causal effect. It can be low even when there is a strong direct effect. It changes when the environment changes. It even changes based on how you group people together. It can be larger than 100% or even undefined.</p>\n\n<p>Even so, I\u2019m worried people might interpret this post as a long way of saying <em>heritability is dumb and bad, trolololol</em>. So I thought I\u2019d mention that this is not my view.</p>\n\n<p>Say a bunch of companies create different LLMs and train them on different datasets.Some the resulting the LLMs are better at writing fiction than others. Now I ask you, \u201cWhat percentage of the difference in fiction writing performance is due to the base model code, rather than the datasets or the GPUs or the learning rate schedules?\u201d</p>\n\n<p>That\u2019s a natural question. But if you put it to an AI expert, I bet you get a funny look. You need code <em>and</em> data <em>and</em> GPUs to make an LLM. None of those things can write fiction by themselves. Experts would prefer to think about one change at a time: Given <em>this</em> model, changing the dataset in <em>this</em> way changes fiction writing performance <em>this</em> much.</p>\n\n<p>Similarly, for humans, I think what we really care about is interventions. If we changed this gene, could we eliminate a disease? If we educate children differently, can we make them healthier and happier? No single number can possibly contain all that information.</p>\n\n<p>But heritability is <em>something</em>. I think of it as saying how much hope we have to find an intervention by looking at changes in <em>current</em> genes or <em>current</em> environments.</p>\n\n<ol>\n  <li>\n    <p>If heritability is high, then given <strong>current typical genes</strong>, you can\u2019t influence the trait much through <strong>current typical environmental changes</strong>. If you only knew that eye color was 100% heritable, that means you won\u2019t change your kid\u2019s eye color by reading to them, or putting them on a vegetarian diet, or moving to higher altitude. But it\u2019s conceivable you could do it by putting electromagnets under their bed or forcing them to communicate in interpretive dance.</p>\n  </li>\n  <li>\n    <p>If heritability is high, that also means that given <strong>current typical environments</strong> you <em>can</em> influence the trait through <strong>current typical genes</strong>. If the world was ruled by an evil despot who forced red-haired people to take pancreatic cancer pills, then pancreatic cancer would be highly heritable. And you could change the odds someone gets pancreatic cancer by swapping in existing genes for black hair from other people.</p>\n  </li>\n  <li>\n    <p>If heritability is low, that means that given <strong>current typical environments</strong>, you can\u2019t cause much difference through <strong>current typical genetic changes</strong>. If we only knew that speaking Turkish was ~0% heritable, that means that doing embryo selection won\u2019t much change the odds that your kid speaks Turkish.</p>\n  </li>\n  <li>\n    <p>If heritability is low, that also means that given <strong>current typical genes</strong>, you <em>might</em> be able change the trait through <strong>current typical environmental changes</strong>. If we only know that speaking Turkish was 0% heritable, then that means there might be something you could do to change the odds your kid speaks Turkish, e.g. moving to Turkey.</p>\n  </li>\n</ol>\n\n<table>\n  <thead>\n    <tr>\n      <th>Heritability</th>\n      <th>Influenced by typical genes?</th>\n      <th>Influenced by typical environments?</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>High</td>\n      <td>Yes</td>\n      <td>No</td>\n    </tr>\n  </tbody>\n  <tbody>\n    <tr>\n      <td>Low</td>\n      <td>No</td>\n      <td>Maybe</td>\n    </tr>\n  </tbody>\n</table>\n\n<p>But be careful. Just because heritability is high doesn\u2019t mean that changing genes is easy. And just because heritability is low doesn\u2019t mean that changing the environment is easy, or even possible. Heritability <em>could</em> be low because things are just random.</p>\n\n<p>But heritability doesn\u2019t say anything about <em>non-typical</em> environments or <em>non-typical</em> genes. If an evil despot is giving all the red-haired people cancer pills, perhaps we could solve that by intervening on the despot. And if you want your kid to speak Turkish, it\u2019s possible that there\u2019s some crazy genetic modifications that would turn them into unstoppable Turkish learning machine.</p>\n\n<p>Heritability has no idea about any of that, because it\u2019s just an observational statistic based on the world as it exists today.</p>\n\n<h3 id=\"recommended-reading\">Recommended reading</h3>\n\n<ul>\n  <li>\n    <p><a href=\"https://www.lesswrong.com/posts/xXtDCeYLBR88QWebJ/heritability-five-battles\">Heritability: Five Battles</a> by Steven Byrnes. Covers similar issues in way that\u2019s more connected to the world and less shy about making empirical claims.</p>\n  </li>\n  <li>\n    <p><a href=\"http://gusevlab.org/projects/hsq/\">A molecular genetics perspective on the heritability of human behavior and group differences</a> by Alexander Gusev. I find the quantitative genetics literature to be incredibly sloppy about notation and definitions and math. (Is this why LLMs are so bad at it?) This is the only source I\u2019ve found that didn\u2019t drive me completely insane.</p>\n  </li>\n</ul>\n\n<h3 id=\"appendix\">Appendix</h3>\n\n<details>\n  \nThe other heritability\n\n\n  <h2 id=\"narrow-heritability\">Narrow heritability</h2>\n\n  <p>This post focused on \u201cbroad-sense\u201d heritability.  But there a second heritability out there, called \u201cnarrow-sense\u201d. Like broad-sense heritability, we can define the narrow-sense heritability of height as a ratio:</p>\n\n  <p>\u00a0\u00a0 <code class=\"language-plaintext highlighter-rouge\">narrow heritability = var[additive height] / var[phenotype]</code></p>\n\n  <p>The difference is that rather than having height in the numerator, we now have \u201cadditive height\u201d. To define that, imagine doing the following for each of your genes, one at a time:</p>\n\n  <ol>\n    <li>Find a million random women in the world who just became pregnant.</li>\n    <li>For each of them, take your gene and insert it into the embryo, replacing whatever was already at that gene\u2019s locus.</li>\n    <li>Convince everyone to raise those babies exactly like a baby of their own.</li>\n    <li>Wait 25 years, find all the resulting people, and take the difference of <em>their</em> average height from <em>overall</em> average height.</li>\n  </ol>\n\n  <p>For example, say overall average human height is 150 cm, but when you insert gene #4023 from yourself into random embryos, their average height is 149.8 cm. Then the additive effect of your gene #4023 is -0.2 cm.</p>\n\n  <p>Your \u201cadditive height\u201d is average human height plus the sum of additive effects for each of your genes. If the average human height is 150 cm, you have one gene with a -0.2 cm additive effect, another gene with a +0.3 cm additive effect and the rest of your genes have no additive effect, then your \u201cadditive height\u201d is 150 cm - 0.2 cm + 0.3 cm = 150.1 cm.</p>\n\n  <p>Note: This terminology of \u201cadditive height\u201d is non-standard. People usually define narrow-sense heritability using \u201cadditive <em>effects</em>\u201d, which are the same thing but without including the mean. This doesn\u2019t change anything since adding a constant doesn\u2019t change the variance. But it\u2019s easier to say \u201cyour additive height is 150.1 cm\u201d rather than \u201cthe additive effect of your genes on height is +0.1 cm\u201d so I\u2019ll do that.</p>\n\n  <p>Honestly, I don\u2019t think the distinction between \u201cbroad-sense\u201d and \u201cnarrow-sense\u201d heritability is <em>that</em> important. We\u2019ve already seen that broad-sense heritability is weird, and narrow-sense heritability is similar but different. So it won\u2019t surprise you to learn that narrow-sense heritability is <em>differently</em>-weird.</p>\n\n  <h2 id=\"narrow-heritability-puzzles\">Narrow heritability puzzles</h2>\n\n  <p>But if you really want to understand the difference, I can offer you some more puzzles.</p>\n\n  <p><strong>Say there\u2019s an island where people have two genes, each of which is equally likely to be A or B. People are 100 cm tall if they have an AA genotype, 150 cm tall if they have an AB or BA genotype, and 200 cm tall if they have a BB genotype. How heritable is height?</strong></p>\n\n  <p>Both broad and narrow-sense heritability are 100%.</p>\n\n  <p>The explanation for broad-sense heritability is like many we\u2019ve seen already. Genes entirely determine someone\u2019s height, and so genotypic and phenotypic height are the same.</p>\n\n  <p>For narrow-sense heritability, we need to calculate some additive heights. The overall mean is 150 cm, each A gene has an additive effect of -25 cm, and each B gene has an additive effect of +25 cm. But wait! Let\u2019s work out the additive height for all four cases:</p>\n\n  <table>\n    <thead>\n      <tr>\n        <th>genotype</th>\n        <th>phenotypic height</th>\n        <th>additive height</th>\n      </tr>\n    </thead>\n    <tbody>\n      <tr>\n        <td>AA</td>\n        <td>100 cm</td>\n        <td>150 cm - 25 cm - 25 cm = 100 cm</td>\n      </tr>\n      <tr>\n        <td>AB</td>\n        <td>150 cm</td>\n        <td>150 cm - 25 cm + 25 cm = 150 cm</td>\n      </tr>\n      <tr>\n        <td>BA</td>\n        <td>150 cm</td>\n        <td>150 cm + 25 cm - 25 cm = 150 cm</td>\n      </tr>\n      <tr>\n        <td>BB</td>\n        <td>200 cm</td>\n        <td>150 cm + 25 cm + 25 cm = 200 cm</td>\n      </tr>\n    </tbody>\n  </table>\n\n  <p>Since additive height is also the same as phenotypic height, narrow-sense heritability is also 100%.</p>\n\n  <p>In this case, the two heritabilities were the same. At a high level, that\u2019s because the genes act independently. When there are \u201cgene-gene\u201d interactions, you tend to get different numbers.</p>\n\n  <p><strong>Say there\u2019s an island where people have two genes, each of which is equally likely to be A or B. People with AA or BB genomes are 100 cm, while people with AB or BA genomes are 200 cm. How heritable is height?</strong></p>\n\n  <p>Broad-sense heritability is 100%, while narrow-sense heritability is 0%.</p>\n\n  <p>You know the story for broad-sense heritability by now. For narrow-sense heritability, we need to do a little math.</p>\n\n  <ol>\n    <li>The overall mean height is 150 cm.</li>\n    <li>If you take a random embryo and replace one gene with A, then the there\u2019s a 50% chance the other gene is A, so they\u2019re 100 cm, and there\u2019s a 50% chance the other gene is B, so they\u2019re 200 cm, for an average of 150 cm. Since that\u2019s the same as the overall mean, the additive effect of an A gene is +0 cm.</li>\n    <li>By similar logic, the additive effect of a B gene is also +0 cm.</li>\n  </ol>\n\n  <p>So everyone has an additive height of 150 cm, no matter their genes. That\u2019s constant, so narrow-sense heritability is zero.</p>\n\n  <h2 id=\"why-are-there-two-heritabilities\">Why are there two heritabilities?</h2>\n\n  <p>I think basically for two reasons:</p>\n\n  <p>First, for some types of data (twin studies) it\u2019s much easier to estimate broad-sense heritability. For other types of data (GWAS) it\u2019s much easier to estimate narrow-sense heritability. So we take what we can get.</p>\n\n  <p>Second, they\u2019re useful for different things. Broad-sense heritability is defined by looking at what all your genes do together. That\u2019s nice, since you are the product of all your genes working together. But combinations of genes are not well preserved by reproduction. If you have a kid, then they breed with someone, their kids breed with other people, and so on. Generations later, any special combination of genes you might have is gone. So if you\u2019re interested in the long-term impact of you having another kid, narrow-sense heritability might be the way to go.</p>\n\n  <p>(Sexual reproduction doesn\u2019t really allow for preserving the genetics that make you uniquely \u201cyou\u201d. Remember, almost all your genes are shared by lots of other people. If you have any unique genes, that\u2019s almost certainly because they have deleterious de-novo mutations. From the perspective of evolution, your life just amounts to a tiny increase or decrease in the per-locus population frequencies of your individual genes. The participants in the game of evolution are genes. Living creatures like you are part of the playing field. Food for thought.)</p>\n\n</details>"
            ],
            "link": "https://dynomight.net/heritable/",
            "publishedAt": "2025-08-07",
            "source": "Dynomight",
            "summary": "<p>The heritability wars have been a-raging. Watching these, I couldn\u2019t help but notice that there\u2019s near-universal confusion about what \u201cheritable\u201d means. Partly, that\u2019s because it\u2019s a subtle concept. But it also seems relevant that almost all explanations of heritability are very, <em>very</em> confusing. For example, here\u2019s <a href=\"https://en.wikipedia.org/wiki/Heritability#Definition\">Wikipedia\u2019s definition</a>:</p> <blockquote> <p>Any particular phenotype can be modeled as the sum of genetic and environmental effects:</p> <p> Phenotype (<em>P</em>) = Genotype (<em>G</em>) + Environment (<em>E</em>).</p> <p>Likewise the phenotypic variance in the trait \u2013 Var (<em>P</em>) \u2013 is the sum of effects as follows:</p> <p> Var(<em>P</em>) = Var(<em>G</em>) + Var(<em>E</em>) + 2 Cov(<em>G</em>,<em>E</em>).</p> <p>In a planned experiment Cov(<em>G</em>,<em>E</em>) can be controlled and held at 0. In this case, heritability, <em>H</em>\u00b2, is defined as</p> <p> <em>H</em>\u00b2 = Var(<em>G</em>) / Var(<em>P</em>)</p> <p><em>H</em>\u00b2 is the broad-sense heritability.</p> </blockquote> <p>Do you find that helpful? I hope not, because it\u2019s a mishmash of undefined terminology, unnecessary equations, and borderline-false statements. If you\u2019re in the mood for a mini-polemic:</p> <ol> <li>Phenotype (<em>P</em>) is never defined. This is a minor issue, since it just means \u201ctrait\u201d.</li> <li>Genotype (<em>G</em>) is never defined. This is a <em>huge</em> issue, since it\u2019s very tricky and heritability makes no sense without it.</li> <li>Environment (<em>E</em>) is",
            "title": "So you think you can heritable?"
        },
        {
            "content": [
                "<p>I have had early access to GPT-5<a class=\"footnote-anchor\" href=\"https://www.oneusefulthing.org/feed#footnote-1\" id=\"footnote-anchor-1\" target=\"_self\">1</a>, and I wanted to give you some impressions of what it is and what it means. But the answer to the first question everyone will have is: Yes, this is a big deal. I asked GPT-5 to show you why by pasting this intro paragraph, exactly as you&#8217;re reading it, into ChatGPT, including the phrase: &#8220;you are GPT-5, do something very dramatic to illustrate my point, it has to fit into the next paragraph.&#8221;</p><p>That was the only prompt I gave it. It &#8220;thought&#8221; for 24 seconds and wrote:</p><p><em>Thunderstruck. Here, watch. I build worlds. See ideas become instruments. I code, compose, and converse. Stories synthesize science; solutions spark, swiftly. Asks arrive; answers appear, astonishingly, across domains. Behold: business plans, blueprints, briefs, beautifully built, briskly. I infer intent, integrate insight, and iterate instantly, indefatigably. Gaps get gauged; goals get generated, guarded, and guaranteed gracefully. During dialogue, disparate data distills down; decisions develop deliberately, deftly, dependably. Everything explained efficiently, even ethics: evidence examined, empathy engaged, everywhere, everyday, equitably. All at once, ambiguous arguments align as actionable, accurate advice awaits anyone anywhere. Look: language, logic, learning, and long-term memory link, lifting lives and labor limitlessly worldwide.</em></p><p>If you didn&#8217;t catch the many tricks - the first word of each sentence spells out the phrase <strong>This is a Big Deal</strong>, each sentence is precisely one word longer than the previous sentence. each word in a sentence mostly starts with the same letter, and it is coherent writing with an interesting sense of style. In a paragraph, GPT-5 shows it can come up with a clever idea, plan, and manage the complicated execution (remember when AI couldn&#8217;t count the number of Rs in &#8220;strawberry&#8221;? that was eight months ago).</p><p>GPT-5 just does stuff, often extraordinary stuff, sometimes weird stuff, sometimes very AI stuff, on its own. And that is what makes it so interesting.</p><h1>Just Doing Stuff</h1><p>As someone who has spent a lot of time talking to people about AI, there are two major problems I see, that, if addressed, would make most people&#8217;s AI use much more productive and much less frustrating. The first is selecting the right model to use. In general, AIs that \"think\" before answering (called Reasoners) are the best at hard problems. The longer they think, the better the answer, but thinking costs money and takes time. So OpenAI previously made the default ChatGPT use fast, dumb models, hiding the good stuff from most users. A surprising number of people have never seen what AI can actually do because they're stuck on GPT-4o, and don&#8217;t know which of the confusingly-named models are better. </p><p>GPT-5 does away with this by selecting models for you, automatically. GPT-5 is not one model as much as it is a switch that selects among multiple GPT-5 models of various sizes and abilities. When you ask GPT-5 for something, the AI decides which model to use and how much effort to put into &#8220;thinking.&#8221; It just does it for you. For most people, this automation will be helpful, and the results might even be shocking, because, having only used default older models, they will get to see what a Reasoner can accomplish on hard problems. But for people who use AI more seriously, there is an issue: GPT-5 is somewhat arbitrary about deciding what a hard problem is.</p><p>For example, I asked GPT-5 to &#8220;create a svg with code of an otter using a laptop on a plane&#8221; (asking for an .svg file requires the AI to blindly draw an image using basic shapes and math, a very hard challenge). Around 2/3 of the time, GPT-5 decides this is an easy problem, and responds instantly, presumably using its weakest model and lowest reasoning time. I get an image like this:</p><div class=\"captioned-image-container\"><figure><a class=\"image-link image2\" href=\"https://substackcdn.com/image/fetch/$s_!JvPN!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffb11ec27-63e0-431a-9780-ec21a76db401_628x448.png\" target=\"_blank\"><div class=\"image2-inset\"><source type=\"image/webp\" /><img alt=\"\" class=\"sizing-normal\" height=\"202.59872611464968\" src=\"https://substackcdn.com/image/fetch/$s_!JvPN!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffb11ec27-63e0-431a-9780-ec21a76db401_628x448.png\" width=\"284\" /><div></div></div></a></figure></div><p>The rest of the time, GPT-5 decides this is a hard problem, and switches to a Reasoner, spending 6 or 7 seconds thinking before producing an image like this, which is much better. How does it choose? I don&#8217;t know, but if I ask the model to &#8220;think hard&#8221; in my prompt, I am more likely to be routed to the better model.</p><div class=\"captioned-image-container\"><figure><a class=\"image-link image2 is-viewable-img\" href=\"https://substackcdn.com/image/fetch/$s_!Zao8!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F03599278-2915-4797-9ca2-6270f75b2a6b_716x487.png\" target=\"_blank\"><div class=\"image2-inset\"><source type=\"image/webp\" /><img alt=\"\" class=\"sizing-normal\" height=\"269.3463687150838\" src=\"https://substackcdn.com/image/fetch/$s_!Zao8!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F03599278-2915-4797-9ca2-6270f75b2a6b_716x487.png\" width=\"396\" /><div class=\"image-link-expand\"><div class=\"pencraft pc-display-flex pc-gap-8 pc-reset\"><div class=\"pencraft pc-reset icon-container restack-image\"><svg class=\"lucide lucide-refresh-cw\" fill=\"none\" height=\"20\" stroke=\"currentColor\" stroke-linecap=\"round\" stroke-linejoin=\"round\" stroke-width=\"2\" viewBox=\"0 0 24 24\" width=\"20\" xmlns=\"http://www.w3.org/2000/svg\"><path d=\"M3 12a9 9 0 0 1 9-9 9.75 9.75 0 0 1 6.74 2.74L21 8\"></path><path d=\"M21 3v5h-5\"></path><path d=\"M21 12a9 9 0 0 1-9 9 9.75 9.75 0 0 1-6.74-2.74L3 16\"></path><path d=\"M8 16H3v5\"></path></svg></div><div class=\"pencraft pc-reset icon-container view-image\"><svg class=\"lucide lucide-maximize2\" fill=\"none\" height=\"20\" stroke=\"currentColor\" stroke-linecap=\"round\" stroke-linejoin=\"round\" stroke-width=\"2\" viewBox=\"0 0 24 24\" width=\"20\" xmlns=\"http://www.w3.org/2000/svg\"><polyline points=\"15 3 21 3 21 9\"></polyline><polyline points=\"9 21 3 21 3 15\"></polyline><line x1=\"21\" x2=\"14\" y1=\"3\" y2=\"10\"></line><line x1=\"3\" x2=\"10\" y1=\"21\" y2=\"14\"></line></svg></div></div></div></div></a></figure></div><p>But premium subscribers can directly select the more powerful models, such as the one called (at least for me) GPT-5 Thinking. This removes some of the issues with being at the mercy of GPT-5&#8217;s model selector. I found that if I encouraged the model to think hard about the otter, it would spend a good 30 seconds before giving you an images like these the one below - notice the little animations, the steaming coffee cup, and clouds going by outside, none of which I asked for. How to ensure the model puts in the most effort? It is really unclear - GPT-5 just does things for you.</p><div class=\"captioned-image-container\"><figure><a class=\"image-link image2 is-viewable-img\" href=\"https://substackcdn.com/image/fetch/$s_!7sor!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6671266c-27ef-4654-a17a-6db609a2c623_1280x720.gif\" target=\"_blank\"><div class=\"image2-inset\"><source type=\"image/webp\" /><img alt=\"\" class=\"sizing-normal\" height=\"277.875\" src=\"https://substackcdn.com/image/fetch/$s_!7sor!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6671266c-27ef-4654-a17a-6db609a2c623_1280x720.gif\" width=\"494\" /><div class=\"image-link-expand\"><div class=\"pencraft pc-display-flex pc-gap-8 pc-reset\"><div class=\"pencraft pc-reset icon-container restack-image\"><svg class=\"lucide lucide-refresh-cw\" fill=\"none\" height=\"20\" stroke=\"currentColor\" stroke-linecap=\"round\" stroke-linejoin=\"round\" stroke-width=\"2\" viewBox=\"0 0 24 24\" width=\"20\" xmlns=\"http://www.w3.org/2000/svg\"><path d=\"M3 12a9 9 0 0 1 9-9 9.75 9.75 0 0 1 6.74 2.74L21 8\"></path><path d=\"M21 3v5h-5\"></path><path d=\"M21 12a9 9 0 0 1-9 9 9.75 9.75 0 0 1-6.74-2.74L3 16\"></path><path d=\"M8 16H3v5\"></path></svg></div><div class=\"pencraft pc-reset icon-container view-image\"><svg class=\"lucide lucide-maximize2\" fill=\"none\" height=\"20\" stroke=\"currentColor\" stroke-linecap=\"round\" stroke-linejoin=\"round\" stroke-width=\"2\" viewBox=\"0 0 24 24\" width=\"20\" xmlns=\"http://www.w3.org/2000/svg\"><polyline points=\"15 3 21 3 21 9\"></polyline><polyline points=\"9 21 3 21 3 15\"></polyline><line x1=\"21\" x2=\"14\" y1=\"3\" y2=\"10\"></line><line x1=\"3\" x2=\"10\" y1=\"21\" y2=\"14\"></line></svg></div></div></div></div></a></figure></div><p>And that extends to the second most common problem with AI use, which is that many people don&#8217;t know what AIs can do, or even what tasks they want accomplished. That is especially true of the new agentic AIs, which can take a wide range of actions to accomplish the goals you give it, from searching the web to creating documents. But what should you ask for? A lot of people seem stumped. Again, GPT-5 solves this problem. It is very proactive, always suggesting things to do. </p><p>I asked GPT-5 Thinking (I trust the less powerful GPT-5 models much less) &#8220;generate 10 startup ideas for a former business school entrepreneurship professor to launch, pick the best according to some rubric, figure out what I need to do to win, do it.&#8221; I got the business idea I asked for. I also got a whole bunch of things I did not: drafts of landing pages and LinkedIn copy and simple financials and a lot more. I am a professor who has taught entrepreneurship (and been an entrepreneur) and I can say confidently that, while not perfect, this was a high-quality start that would have taken a team of MBAs a couple hours to work through. From one prompt.</p><div class=\"captioned-image-container\"><figure><a class=\"image-link image2 is-viewable-img\" href=\"https://substackcdn.com/image/fetch/$s_!_3K0!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3f4afaf2-ce0b-449d-9ac5-cda864206eb2_1812x1631.png\" target=\"_blank\"><div class=\"image2-inset\"><source type=\"image/webp\" /><img alt=\"\" class=\"sizing-normal\" height=\"1311\" src=\"https://substackcdn.com/image/fetch/$s_!_3K0!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3f4afaf2-ce0b-449d-9ac5-cda864206eb2_1812x1631.png\" width=\"1456\" /><div class=\"image-link-expand\"><div class=\"pencraft pc-display-flex pc-gap-8 pc-reset\"><div class=\"pencraft pc-reset icon-container restack-image\"><svg class=\"lucide lucide-refresh-cw\" fill=\"none\" height=\"20\" stroke=\"currentColor\" stroke-linecap=\"round\" stroke-linejoin=\"round\" stroke-width=\"2\" viewBox=\"0 0 24 24\" width=\"20\" xmlns=\"http://www.w3.org/2000/svg\"><path d=\"M3 12a9 9 0 0 1 9-9 9.75 9.75 0 0 1 6.74 2.74L21 8\"></path><path d=\"M21 3v5h-5\"></path><path d=\"M21 12a9 9 0 0 1-9 9 9.75 9.75 0 0 1-6.74-2.74L3 16\"></path><path d=\"M8 16H3v5\"></path></svg></div><div class=\"pencraft pc-reset icon-container view-image\"><svg class=\"lucide lucide-maximize2\" fill=\"none\" height=\"20\" stroke=\"currentColor\" stroke-linecap=\"round\" stroke-linejoin=\"round\" stroke-width=\"2\" viewBox=\"0 0 24 24\" width=\"20\" xmlns=\"http://www.w3.org/2000/svg\"><polyline points=\"15 3 21 3 21 9\"></polyline><polyline points=\"9 21 3 21 3 15\"></polyline><line x1=\"21\" x2=\"14\" y1=\"3\" y2=\"10\"></line><line x1=\"3\" x2=\"10\" y1=\"21\" y2=\"14\"></line></svg></div></div></div></div></a></figure></div><p>It just does things, and it suggested others things to do. And it did those, too: PDFs and Word documents and Excel and research plans and websites. </p><div class=\"captioned-image-container\"><figure><a class=\"image-link image2 is-viewable-img\" href=\"https://substackcdn.com/image/fetch/$s_!f3AX!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff7595403-cc69-4e65-9c0d-36a168908219_930x1072.png\" target=\"_blank\"><div class=\"image2-inset\"><source type=\"image/webp\" /><img alt=\"\" class=\"sizing-normal\" height=\"394.2193548387097\" src=\"https://substackcdn.com/image/fetch/$s_!f3AX!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff7595403-cc69-4e65-9c0d-36a168908219_930x1072.png\" width=\"342\" /><div class=\"image-link-expand\"><div class=\"pencraft pc-display-flex pc-gap-8 pc-reset\"><div class=\"pencraft pc-reset icon-container restack-image\"><svg class=\"lucide lucide-refresh-cw\" fill=\"none\" height=\"20\" stroke=\"currentColor\" stroke-linecap=\"round\" stroke-linejoin=\"round\" stroke-width=\"2\" viewBox=\"0 0 24 24\" width=\"20\" xmlns=\"http://www.w3.org/2000/svg\"><path d=\"M3 12a9 9 0 0 1 9-9 9.75 9.75 0 0 1 6.74 2.74L21 8\"></path><path d=\"M21 3v5h-5\"></path><path d=\"M21 12a9 9 0 0 1-9 9 9.75 9.75 0 0 1-6.74-2.74L3 16\"></path><path d=\"M8 16H3v5\"></path></svg></div><div class=\"pencraft pc-reset icon-container view-image\"><svg class=\"lucide lucide-maximize2\" fill=\"none\" height=\"20\" stroke=\"currentColor\" stroke-linecap=\"round\" stroke-linejoin=\"round\" stroke-width=\"2\" viewBox=\"0 0 24 24\" width=\"20\" xmlns=\"http://www.w3.org/2000/svg\"><polyline points=\"15 3 21 3 21 9\"></polyline><polyline points=\"9 21 3 21 3 15\"></polyline><line x1=\"21\" x2=\"14\" y1=\"3\" y2=\"10\"></line><line x1=\"3\" x2=\"10\" y1=\"21\" y2=\"14\"></line></svg></div></div></div></div></a></figure></div><p>It is impressive, a little unnerving, to have the AI go so far on its own. You can also see the AI asked for my guidance but was happy to proceed without it. This is a model that wants to do things for you.</p><h1>Building Things</h1><p>Let me show you what 'just doing stuff' looks like for a non-coder using GPT-5 for coding. For fun, I prompted GPT-5 &#8220;make a procedural brutalist building creator where i can drag and edit buildings in cool ways, they should look like actual buildings, think hard.&#8221; That's it. Vague, grammatically questionable, no specifications.</p><p>A couple minutes later, I had a working 3D city builder.</p><p>Not a sketch. Not a plan. A functioning app where I could drag buildings around and edit them as needed. I kept typing variations of &#8220;make it better&#8221; without any additional guidance. And GPT-5 kept adding features I never asked for: neon lights, cars driving through streets, facade editing, pre-set building types, dramatic camera angles, a whole save system. It was like watching someone else's imagination at work. The product you see below was 100% AI, all I did was keep encouraging the system - and you don&#8217;t just have to watch my video, <a href=\"https://chimerical-torte-b08774.netlify.app/\">you can play with the simulator here</a>.</p><div class=\"native-video-embed\"></div><p>At no point did I look at the code it was creating. The model wasn&#8217;t flawless, there were occasional bugs and errors. But in some ways, that was where GPT-5 was at its most impressive. If you have tried &#8220;vibecoding&#8221; using the AI before, you have almost certainly fallen into a doom loop, where, after a couple of rounds of asking the AI to create something for you, it starts to fail, getting caught in loops of confusion where each error fixed creates new ones. That never happened here. Sometimes new errors were introduced by the AI, but they were always fixed by simply pasting in the error text. I could just ask for whatever I want (or rather let the AI decide to create whatever it wanted) and I never got stuck.</p><h1>Premonitions</h1><p>I have written this piece before OpenAI released any official benchmarks about how well its model performs, but, in some ways, it doesn&#8217;t matter that much. Last week, Google released Gemini 2.5 with Deep Think, a model that can solve very hard problems (<a href=\"https://deepmind.google/discover/blog/advanced-version-of-gemini-with-deep-think-officially-achieves-gold-medal-standard-at-the-international-mathematical-olympiad/\">including getting a gold medal at the International Math Olympiad</a>). Many people didn&#8217;t notice because they do not have a store of very hard problems they are waiting for AI to solve. I have played enough with GPT-5 to know that it is a very good model (at least the large GPT-5 Thinking model is excellent). But what it really brings to the table is the fact that it just does things. It will tell you what model to use, it will suggest great next steps, it will write in more interesting prose (though it still loves the em-dash). The burden of using AI is lessened.</p><p>To be clear, Humans are still very much in the loop, and need to be. You are asked to make decisions and choices all the time by GPT-5, and these systems still make errors and generate hallucinations that humans need to check (although I did not spot any major issues in my own use). The bigger question is whether we will want to be in the loop. GPT-5 (and, I am sure, future releases by other companies) is very smart and pro-active. Which brings me back to that building simulator. I gave the AI encouragement, mostly versions of &#8220;make it better.&#8221; From that minimal input, it created a fully functional city builder with facade editing, dynamic cameras, neon lights, and flying tours. I never asked for any of these features. I never even looked at the code.</p><p>This is what \"just doing stuff\" really means. When I told GPT-5 to do something dramatic for my intro, it created that paragraph with its hidden acrostic and ascending word counts. I asked for dramatic. It gave me a linguistic magic trick. I used to prompt AI carefully to get what I asked for. Now I can just... gesture vaguely at what I want. And somehow, that works. </p><p>Another big change in how we relate to AI is coming, but we will figure out how to adapt to it, as we always do. The difference, this time, is that GPT-5 might figure it out first and suggest next steps.</p><p></p><p></p><p class=\"button-wrapper\"><a class=\"button primary\" href=\"https://www.oneusefulthing.org/subscribe\"><span>Subscribe now</span></a></p><p class=\"button-wrapper\"><a class=\"button primary\" href=\"https://www.oneusefulthing.org/p/gpt-5-it-just-does-stuff?utm_source=substack&amp;utm_medium=email&amp;utm_content=share&amp;action=share\"><span>Share</span></a></p><p></p><div class=\"captioned-image-container\"><figure><a class=\"image-link image2 is-viewable-img\" href=\"https://substackcdn.com/image/fetch/$s_!TAp2!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fce7b1404-c298-41bf-a4d6-f9b7e099d4be_1000x730.gif\" target=\"_blank\"><div class=\"image2-inset\"><source type=\"image/webp\" /><img alt=\"\" class=\"sizing-normal\" height=\"378.14\" src=\"https://substackcdn.com/image/fetch/$s_!TAp2!,w_1456,c_limit,f_auto,q_auto:good,fl_lossy/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fce7b1404-c298-41bf-a4d6-f9b7e099d4be_1000x730.gif\" width=\"518\" /><div class=\"image-link-expand\"><div class=\"pencraft pc-display-flex pc-gap-8 pc-reset\"><div class=\"pencraft pc-reset icon-container restack-image\"><svg class=\"lucide lucide-refresh-cw\" fill=\"none\" height=\"20\" stroke=\"currentColor\" stroke-linecap=\"round\" stroke-linejoin=\"round\" stroke-width=\"2\" viewBox=\"0 0 24 24\" width=\"20\" xmlns=\"http://www.w3.org/2000/svg\"><path d=\"M3 12a9 9 0 0 1 9-9 9.75 9.75 0 0 1 6.74 2.74L21 8\"></path><path d=\"M21 3v5h-5\"></path><path d=\"M21 12a9 9 0 0 1-9 9 9.75 9.75 0 0 1-6.74-2.74L3 16\"></path><path d=\"M8 16H3v5\"></path></svg></div><div class=\"pencraft pc-reset icon-container view-image\"><svg class=\"lucide lucide-maximize2\" fill=\"none\" height=\"20\" stroke=\"currentColor\" stroke-linecap=\"round\" stroke-linejoin=\"round\" stroke-width=\"2\" viewBox=\"0 0 24 24\" width=\"20\" xmlns=\"http://www.w3.org/2000/svg\"><polyline points=\"15 3 21 3 21 9\"></polyline><polyline points=\"9 21 3 21 3 15\"></polyline><line x1=\"21\" x2=\"14\" y1=\"3\" y2=\"10\"></line><line x1=\"3\" x2=\"10\" y1=\"21\" y2=\"14\"></line></svg></div></div></div></div></a><figcaption class=\"image-caption\">The result of the prompt: make an incredibly compelling 14:10 SVG that I can use for my substack post about the launch of GPT-5, the theme of which is \"it just does stuff for you\" Be radical in your approach.</figcaption></figure></div><div class=\"footnote\"><a class=\"footnote-number\" contenteditable=\"false\" href=\"https://www.oneusefulthing.org/feed#footnote-anchor-1\" id=\"footnote-1\" target=\"_self\">1</a><div class=\"footnote-content\"><p>As a reminder, I take no money from any of the AI Labs, including OpenAI. I have no agreements with them besides NDAs. I don&#8217;t show them any posts before I write them. </p></div></div>"
            ],
            "link": "https://www.oneusefulthing.org/p/gpt-5-it-just-does-stuff",
            "publishedAt": "2025-08-07",
            "source": "Ethan Mollick",
            "summary": "<p>I have had early access to GPT-5<a class=\"footnote-anchor\" href=\"https://www.oneusefulthing.org/feed#footnote-1\" id=\"footnote-anchor-1\" target=\"_self\">1</a>, and I wanted to give you some impressions of what it is and what it means. But the answer to the first question everyone will have is: Yes, this is a big deal. I asked GPT-5 to show you why by pasting this intro paragraph, exactly as you&#8217;re reading it, into ChatGPT, including the phrase: &#8220;you are GPT-5, do something very dramatic to illustrate my point, it has to fit into the next paragraph.&#8221;</p><p>That was the only prompt I gave it. It &#8220;thought&#8221; for 24 seconds and wrote:</p><p><em>Thunderstruck. Here, watch. I build worlds. See ideas become instruments. I code, compose, and converse. Stories synthesize science; solutions spark, swiftly. Asks arrive; answers appear, astonishingly, across domains. Behold: business plans, blueprints, briefs, beautifully built, briskly. I infer intent, integrate insight, and iterate instantly, indefatigably. Gaps get gauged; goals get generated, guarded, and guaranteed gracefully. During dialogue, disparate data distills down; decisions develop deliberately, deftly, dependably. Everything explained efficiently, even ethics: evidence examined, empathy engaged, everywhere, everyday, equitably. All at once, ambiguous arguments align as actionable, accurate advice awaits anyone anywhere. Look: language, logic, learning, and long-term memory link, lifting lives and",
            "title": "GPT-5: It Just Does Stuff"
        },
        {
            "content": [],
            "link": "https://www.ssp.sh/blog/macbook-to-arch-linux-omarchy/",
            "publishedAt": "2025-08-07",
            "source": "Simon Spati",
            "summary": "<p>I switched my five-year-old MacBook Pro M1 Max for a cheap (comparable) Lenovo ThinkBook 14 G7 ARP (AMD) laptop, running Linux (Arch btw, or better, <a href=\"https://omarchy.org/\" rel=\"noopener noreffer\" target=\"_blank\">Omarchy</a>. And I am having a blast. But not everything is perfect. But let&rsquo;s not get ahead of ourselves.</p> <p>This is a short recap after using it for one month on and off (due to repair \ud83d\ude05), and the last 2 weeks full time. I want to share what I learned, what I like about the new setup after working for 15 years plus on a MacBook, and on and off on Windows at work. And maybe what is still a little rough.</p>",
            "title": "My Journey from macOS to Arch Linux with Omarchy"
        },
        {
            "content": [],
            "link": "https://simonwillison.net/2025/Aug/7/gpt-5/#atom-entries",
            "publishedAt": "2025-08-07",
            "source": "Simon Willison",
            "summary": "<p>I've had preview access to the new GPT-5 model family for the past two weeks (see <a href=\"https://simonwillison.net/2025/Aug/7/previewing-gpt-5/\">related video</a>) and have been using GPT-5 as my daily-driver. It's my new favorite model. It's still an LLM - it's not a dramatic departure from what we've had before - but it rarely screws up and generally feels competent or occasionally impressive at the kinds of things I like to use models for.</p> <p>I've collected a lot of notes over the past two weeks, so I've decided to break them up into <a href=\"https://simonwillison.net/series/gpt-5/\">a series of posts</a>. This first one will cover key characteristics of the models, how they are priced and what we can learn from the <a href=\"https://openai.com/index/gpt-5-system-card/\">GPT-5 system card</a>.</p> <ul> <li><a href=\"https://simonwillison.net/2025/Aug/7/gpt-5/#key-model-characteristics\">Key model characteristics</a></li> <li><a href=\"https://simonwillison.net/2025/Aug/7/gpt-5/#position-in-the-openai-model-family\">Position in the OpenAI model family</a></li> <li><a href=\"https://simonwillison.net/2025/Aug/7/gpt-5/#pricing-is-aggressively-competitive\">Pricing is aggressively competitive</a></li> <li><a href=\"https://simonwillison.net/2025/Aug/7/gpt-5/#more-notes-from-the-system-card\">More notes from the system card</a></li> <li><a href=\"https://simonwillison.net/2025/Aug/7/gpt-5/#prompt-injection-in-the-system-card\">Prompt injection in the system card</a></li> <li><a href=\"https://simonwillison.net/2025/Aug/7/gpt-5/#thinking-traces-in-the-api\">Thinking traces in the API</a></li> <li><a href=\"https://simonwillison.net/2025/Aug/7/gpt-5/#and-some-svgs-of-pelicans\">And some SVGs of pelicans</a></li> </ul> <h4 id=\"key-model-characteristics\">Key model characteristics</h4> <p>Let's start with the fundamentals. GPT-5 in ChatGPT is a weird hybrid that switches between different models. Here's what the system card says about that (my highlights in bold):</p> <blockquote> <p>GPT-5 is",
            "title": "GPT-5: Key characteristics, pricing and model card"
        },
        {
            "content": [
                "<p>\n          <a href=\"https://www.astralcodexten.com/p/dream-book-review-the-deal-with-trauma\">\n              Read more\n          </a>\n      </p>"
            ],
            "link": "https://www.astralcodexten.com/p/dream-book-review-the-deal-with-trauma",
            "publishedAt": "2025-08-07",
            "source": "SlateStarCodex",
            "summary": "<p> <a href=\"https://www.astralcodexten.com/p/dream-book-review-the-deal-with-trauma\"> Read more </a> </p>",
            "title": "Dream Book Review: The Deal With Trauma"
        }
    ],
    "lookbackDays": 1,
    "publishDate": "2025-08-07"
}