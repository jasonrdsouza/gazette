{
    "articles": [
        {
            "content": [
                "<header>\n  <h1>Programming Aphorisms</h1>\n  <time class=\"meta\" datetime=\"2026-02-11\">Feb 11, 2026</time>\n</header>\n<p>A meta programming post \u2014 looking at my thought process when coding and trying to pin down what is\nprogramming \u201cknowledge\u201d. Turns out, a significant fraction of that is just reducing new problems to\na vocabulary of known tricks. This is a personal, descriptive post, not a prescriptive post for you.</p>\n<p>It starts with a question posted on Ziggit. The background here is that Zig is in the process of\nremoving ambient IO capabilities. Currently, you can access program environment from anywhere via\n<span class=\"display\"><code>std.process.getEnvVarOwned</code>.</span>\nIn the next Zig version, you\u2019ll have to thread\n<span class=\"display\"><code>std.process.Environ.Map</code></span>\nfrom main down to every routine that needs access to the environment. In this user\u2019s case, they have\na <code>readHistory</code> function which used to look up the path to the history file in the environment, and\nthey are wondering how to best model that in the new Zig. The options on the table are:</p>\n\n<figure class=\"code-block\">\n\n\n<pre><code><span class=\"line\"><span class=\"hl-keyword\">pub</span> <span class=\"hl-keyword\">fn</span><span class=\"hl-function\"> readHistory</span>(</span>\n<span class=\"line\">    io: std.Io,</span>\n<span class=\"line\">    alloc: Allocator,</span>\n<span class=\"line\">    file: std.Io.File,</span>\n<span class=\"line\">) ReadHistoryError<span class=\"hl-operator\">!</span><span class=\"hl-type\">void</span>;</span>\n<span class=\"line\"></span>\n<span class=\"line\"><span class=\"hl-keyword\">pub</span> <span class=\"hl-keyword\">fn</span><span class=\"hl-function\"> readHistory</span>(</span>\n<span class=\"line\">    io: std.Io,</span>\n<span class=\"line\">    alloc: Allocator,</span>\n<span class=\"line\">    maybe_environ_map: ?<span class=\"hl-operator\">*</span>std.process.Environ.Map,</span>\n<span class=\"line\">) ReadHistoryError<span class=\"hl-operator\">!</span><span class=\"hl-type\">void</span>;</span>\n<span class=\"line\"></span>\n<span class=\"line\"><span class=\"hl-keyword\">pub</span> <span class=\"hl-keyword\">fn</span><span class=\"hl-function\"> readHistory</span>(</span>\n<span class=\"line\">    io: std.Io,</span>\n<span class=\"line\">    alloc: Allocator,</span>\n<span class=\"line\">    maybe_absolute_path: ?[]<span class=\"hl-keyword\">const</span> <span class=\"hl-type\">u8</span>,</span>\n<span class=\"line\">    maybe_environ_map: ?<span class=\"hl-operator\">*</span>std.process.Environ.Map,</span>\n<span class=\"line\">) ReadHistoryError<span class=\"hl-operator\">!</span><span class=\"hl-type\">void</span>;</span></code></pre>\n\n</figure>\n<p>My starting point would instead be this:</p>\n\n<figure class=\"code-block\">\n\n\n<pre><code><span class=\"line\"><span class=\"hl-keyword\">pub</span> <span class=\"hl-keyword\">const</span> HistoryOptions = <span class=\"hl-keyword\">struct</span> {</span>\n<span class=\"line\">    file: []<span class=\"hl-keyword\">const</span> <span class=\"hl-type\">u8</span>,</span>\n<span class=\"line\"></span>\n<span class=\"line\">    <span class=\"hl-keyword\">pub</span> <span class=\"hl-keyword\">fn</span><span class=\"hl-function\"> from_environment</span>(</span>\n<span class=\"line\">        environment: <span class=\"hl-operator\">*</span><span class=\"hl-keyword\">const</span> std.process.Environ.Map,</span>\n<span class=\"line\">    ) HistoryOptions;</span>\n<span class=\"line\">};</span>\n<span class=\"line\"></span>\n<span class=\"line\"><span class=\"hl-keyword\">pub</span> <span class=\"hl-keyword\">fn</span><span class=\"hl-function\"> readHistory</span>(</span>\n<span class=\"line\">    io: std.Io,</span>\n<span class=\"line\">    gpa: Allocator,</span>\n<span class=\"line\">    options: HistoryOptions,</span>\n<span class=\"line\">) ReadHistoryError<span class=\"hl-operator\">!</span><span class=\"hl-type\">void</span>;</span></code></pre>\n\n</figure>\n<p>In terms of meta programming, what I find fascinating is that this, for me, is both immediate (I\ndon\u2019t have to think about it), but also is clearly decomposable into multiple factoids I\u2019ve\naccumulated before. Here\u2019s a deconstruction of what I did here, the verbal \u201clabels\u201d I use to think\nabout what I did, and where I had learned to do that:</p>\n<p><em>First</em>, I \u201craised the abstraction level\u201d by giving <em>it</em> a name and a type (<code>HistoryOptions</code>). This\nis a rare transformation which I learned and named myself. Naming is important for my thinking and\ncommunicating process. \u201cLet\u2019s raise abstraction level\u201d is a staple code review comment of mine.</p>\n<p><em>Second</em>, I avoided \u201cmidlayer mistake\u201d by making sure that every aspect of options is\nuser-configurable. Easy to do in Zig, where all fields are public. I learned about\n<a href=\"https://lwn.net/Articles/336262/\">midlayer mistake</a> from a GitHub comment by\n<a href=\"https://joshtriplett.org\">Josh Triplett</a>.</p>\n<p><em>Third</em>, I provided a \u201cshortcut\u201d, the\n<span class=\"display\"><code>from_environment</code></span>\nconvenience function that cuts across abstraction layers. I learned the \u201cshortcut\u201d aphorism from\n<a href=\"https://spookylukey.github.io/django-views-the-right-way/detail-view.html#discussion-layering-violations-shortcuts-vs-mixins\"><em>Django Views \u2014 The Right Way</em>.</a>\nGermane to the present article, I read that post a decade after I had touched Django the last time.\nIt was useless to me on the object level. On the meta level, reading the article solidified and\n<em>named</em> several programming tricks for me. See reverberations in\n<a href=\"https://rust-analyzer.github.io/blog/2020/09/28/how-to-make-a-light-bulb.html\"><em>How to Make a \ud83d\udca1?</em>.</a></p>\n<p><em>Fourth</em>, I instinctively renamed <code>alloc</code> to \u201cgpa\u201d (in opposition to \u201carena\u201d), the naming I spotted\nin the Zig compiler.</p>\n<p><em>Fifth</em>, I named the configuration parameter \u201coptions\u201d, not <code>config</code>, <code>props</code> or <code>params</code>, a naming\nscheme I learned at TigerBeetle.</p>\n<p><em>Sixth</em>, I made sure that the signature follows \u201cpositional DI\u201d scheme. Arguments that are\ndependencies, resources with unique types are injected positionally (and have canonical names like\n<code>io</code> or <code>gpa</code>). Arguments that <em>directly</em> vary the behavior of function (as opposed to affecting\ntransitive callees) are passed by name, in the <code>Options</code> struct.</p>\n<p>To be specific, I don\u2019t claim that my snippet is the right way to do this! I have no idea, as I\ndon\u2019t have access to the full context. Rather, if I were <em>actually</em> solving the problem, the snippet\nabove would be my initial starting point for further iteration.</p>\n<p>Note that I also don\u2019t explain <em>why</em> I am doing the above six things, I only name them and point at\nthe origin. Actually explaining the <em>why</em> would take a blog post of its own for every one of them.</p>\n<p>And this is I think the key property of my thought process \u2014 I have a bag of tricks, where the\ntricks are named. Inside my mind, this label points both to the actual trick (code to type),\nas well as a justification for it (in what context that would be a good trick to use).</p>\n<p>And I use these tricks all the time, literally! Just answering in passing to a forum comment makes\nme grab a handful! A lot of my knowledge is structured like a book of coding aphorisms.</p>\n<hr />\n<p>Meta meta \u2014 how come I have acquired all those tricks? I read voraciously, random commits, issues,\njumping enthusiastically into rabbit holes and going on wiki trips. The key skill here is\nrecognizing an aphorism once you see it. Reading Ziggit is part of trick-acquisition routine for\nme. Having learned the trick, I remember it, where \u201cremembering\u201d is an act of active recall at the\nopportune moment. This recall powers \u201chorizontal gene transfer\u201d across domains, stealing shortcuts\nfrom Django and midlayer mistake from the kernel. Did you notice that applying \u201chorizontal gene\ntransfer\u201d to the domain of software engineering tacit knowledge is horizontal gene transfer? When\nentering a new domain, I actively seek out the missing tricks. I am relatively recent in Zig, but\nall the above tricks are either Zig native, or at least Zig adapted. Every once in a while, I\n\u201cinvent\u201d a trick of my own. For example, \u201cpositional DI\u201d is something I only verbalized last year.\nThis doesn\u2019t mean I hadn\u2019t been doing that before, just that the activity wasn\u2019t mentally labeled as\na separate thing you can deliberately do. I had the idea, now I also have an aphorism.</p>"
            ],
            "link": "https://matklad.github.io/2026/02/11/programming-aphorisms.html",
            "publishedAt": "2026-02-11",
            "source": "Alex Kladov",
            "summary": "A meta programming post --- looking at my thought process when coding and trying to pin down what is programming knowledge. Turns out, a significant fraction of that is just reducing new problems to a vocabulary of known tricks. This is a personal, descriptive post, not a prescriptive post for you.",
            "title": "Programming Aphorisms"
        },
        {
            "content": [],
            "link": "https://buttondown.com/hillelwayne/archive/proving-whats-possible/",
            "publishedAt": "2026-02-11",
            "source": "Hillel Wayne",
            "summary": "<p>As a formal methods consultant I have to mathematically express properties of systems. I generally do this with two \"temporal operators\": </p> <ul> <li>A(x) means that <code>x</code> is always true. For example, a database table <em>always</em> satisfies all record-level constraints, and a state machine <em>always</em> makes valid transitions between states. If <code>x</code> is a statement about an individual state (as in the database but not state machine example), we further call it an <strong>invariant</strong>.</li> <li>E(x) means that <code>x</code> is \"eventually\" true, conventionally meaning \"guaranteed true at some point in the future\". A database transaction <em>eventually</em> completes or rolls back, a state machine <em>eventually</em> reaches the \"done\" state, etc. </li> </ul> <p>These come from linear temporal logic, which is the mainstream notation for expressing system properties. <sup id=\"fnref:modal\"><a class=\"footnote-ref\" href=\"https://buttondown.com/hillelwayne/rss#fn:modal\">1</a></sup> We like these operators because they elegantly cover <a href=\"https://www.hillelwayne.com/post/safety-and-liveness/\" target=\"_blank\">safety and liveness properties</a>, and because <a href=\"https://buttondown.com/hillelwayne/archive/formalizing-stability-and-resilience-properties/\" target=\"_blank\">we can combine them</a>. <code>A(E(x))</code> means <code>x</code> is true an infinite number of times, while <code>A(x =&gt; E(y)</code> means that <code>x</code> being true guarantees <code>y</code> true in the future. </p> <p>There's a third class of properties, that I will call <em>possibility</em> properties: <code>P(x)</code> is \"can x happen in this model\"? Is it possible",
            "title": "Proving What's Possible"
        },
        {
            "content": [],
            "link": "https://www.nytimes.com/2026/02/11/style/tiny-modern-love-stories-will-you-take-the-sex-plant.html",
            "publishedAt": "2026-02-11",
            "source": "Modern Love - NYT",
            "summary": "Modern Love in miniature, featuring stories of no more than 100 words submitted by readers.",
            "title": "Tiny Love Stories: \u2018Will You Take the Sex Plant?\u2019"
        },
        {
            "content": [],
            "link": "https://www.nytimes.com/2026/02/11/podcasts/valentines-day-surprise-couple.html",
            "publishedAt": "2026-02-11",
            "source": "Modern Love - NYT",
            "summary": "Every year, he tries to prove how much he loves her. After 30 years, she still can\u2019t predict what he\u2019ll do next.",
            "title": "The King and Queen of Valentines"
        },
        {
            "content": [
                "<p>The most shameful thing I did in the workplace was lie to a colleague. It was about ten years ago, I was a fresh-faced intern, and in the rush to deliver something I\u2019d skipped the step of testing my work in staging<sup id=\"fnref-1\"><a class=\"footnote-ref\" href=\"https://www.seangoedecke.com/rss.xml#fn-1\">1</a></sup>. It did not work. When deployed to production, it didn\u2019t work there either. No big deal, in general terms: the page we were working on wasn\u2019t yet customer-facing. But my colleague asked me over his desk whether this worked when I\u2019d tested it, and I said something like \u201cit sure did, no idea what happened\u201d.</p>\n<p>I bet he forgot about it immediately. I could have just messed up the testing (for instance, by accidentally running some different code than the code I pushed), or he knew I\u2019d probably lied, and didn\u2019t really care. I haven\u2019t forgotten about it. Even a decade later, I\u2019m still ashamed to write it down.</p>\n<p>Of course I\u2019m not ashamed about the <em>mistake</em>. I was sloppy to not test my work, but I\u2019ve cut corners since then when I felt it was necessary, and I stand by that decision. I\u2019m ashamed about how I handled it. But even that I understand. I was a kid, trying to learn quickly and prove I belonged in tech. The last thing I wanted to do was to dwell on the way I screwed up. If I were in my colleague\u2019s shoes now, I\u2019d have brushed it off too<sup id=\"fnref-2\"><a class=\"footnote-ref\" href=\"https://www.seangoedecke.com/rss.xml#fn-2\">2</a></sup>. How do I try to handle mistakes now?</p>\n<h3>Handling the emotional reaction</h3>\n<p>The most important thing is to <strong>control your emotions</strong>. If you\u2019re anything like me, your strongest emotional reactions at work will be reserved for the times you\u2019ve screwed up. There are usually two countervailing emotions at play here: the desire to defend yourself, find excuses, and minimize the consequences; and the desire to confess your guilt, abase yourself, and beg for forgiveness. Both of these are traps. </p>\n<p>Obviously making excuses for yourself (or flat-out denying the mistake, like I did) is bad. But going in the other direction and publicly beating yourself up about it is <em>just as bad</em>. It\u2019s bad for a few reasons.</p>\n<p>First, you\u2019re effectively asking the people around you to take the time and effort to reassure you, when they should be focused on the problem. Second, you\u2019re taking yourself out of the group of people who are focused on the problem, when often you\u2019re the best situated to figure out what to do: since it\u2019s your mistake, you have the most context. Third, it\u2019s just not professional. </p>\n<p>So what should you do? <strong>For the first little while, <em>do nothing</em>.</strong> Emotional reactions fade over time. Try and just ride out the initial jolt of realizing you screwed up, and the impulse to leap into action to fix it. Most of the worst reactions to screwing up happen in the immediate aftermath, so if you can simply do nothing during that period you\u2019re already off to a good start. For me, this takes about thirty seconds. How much time you\u2019ll need depends on you, but hopefully it\u2019s under ten minutes. More than that and you might need to grit your teeth and work through it.</p>\n<h3>Communicate</h3>\n<p>Once you\u2019re confident you\u2019re under control, the next step is to <strong>tell people what happened</strong>. Typically you want to tell your manager, but depending on the problem it could also be a colleague or someone else. It\u2019s really important here to be matter-of-fact about it, or you risk falling into the \u201cI\u2019m so terrible, please reassure me\u201d trap I discussed above. You often don\u2019t even need to explicitly say \u201cI made a mistake\u201d, if it\u2019s obvious from context. Just say \u201cI deployed a change and it\u2019s broken X feature\u201d (or whatever the problem is).</p>\n<p>You should do this <em>before</em> you\u2019ve come up with a solution. It\u2019s tempting to try to conceal your mistake and just quietly solve it. But for user-facing mistakes, concealment is impossible - somebody will raise a ticket eventually - and if you don\u2019t communicate the issue, you risk someone else discovering it and independently raising it.</p>\n<p>In the worst case, while you\u2019re quietly working on a fix, you\u2019ll discover that somebody else has declared an incident. Of course, you understand the problem perfectly (since you caused it), and you know that it was caused by a bad deploy and is easily fixable. But the other people on the incident call don\u2019t know all that. They\u2019re thinking about the worst-case scenarios, wondering if it\u2019s database or network-related, paging in all kinds of teams, causing all kinds of hassle. All of that could have been avoided if you had reported the issue immediately.</p>\n<p>In my experience, tech company managers will forgive mistakes<sup id=\"fnref-3\"><a class=\"footnote-ref\" href=\"https://www.seangoedecke.com/rss.xml#fn-3\">3</a></sup>, but <strong>they won\u2019t forgive being made to look like a fool</strong>. In particular, they won\u2019t forgive being deprived of critical information. If they\u2019re asked to explain the incident by their boss, and they have to flounder around because they lack the context <em>that you had all along</em>, that may harm your relationship with them for good. On the other hand, if you give them a clear summary of the problem right away, and they\u2019re able to seem like they\u2019re on top of things to their manager, you <em>might</em> even earn credit for the situation (despite having caused it with your initial mistake).</p>\n<h3>Accept that it\u2019s going to hurt</h3>\n<p>However, you probably won\u2019t earn credit. This is where I diverge from the popular software engineering wisdom that incidents are always the fault of systems, never of individuals. Of course incidents are caused by the interactions of complex systems. Everything in the universe is caused by the interactions of complex systems! But one cause in that chain is often <em>somebody screwing up</em><sup id=\"fnref-4\"><a class=\"footnote-ref\" href=\"https://www.seangoedecke.com/rss.xml#fn-4\">4</a></sup>.</p>\n<p>If you\u2019re a manager of an engineering organization, and you want a project to succeed, you probably have a mental shortlist of the engineers in your org who can reliably lead projects<sup id=\"fnref-5\"><a class=\"footnote-ref\" href=\"https://www.seangoedecke.com/rss.xml#fn-5\">5</a></sup>. If an engineer screws up repeatedly, they\u2019re likely to drop off that list (or at least get an asterisk next to their name).</p>\n<p>It doesn\u2019t really matter if you had a good technical reason to make the mistake, or if it\u2019s excusable. Managers don\u2019t care about that stuff, because they simply don\u2019t have the technical context to know if it\u2019s true or if you\u2019re just trying to talk your way out of it. What managers do have the context to evaluate is <em>results</em>, so that\u2019s what they judge you on. That means some failures are acceptable, so long as you\u2019ve got enough successes to balance them out.</p>\n<p>Being a strong engineer is about finding a balance between <a href=\"https://www.seangoedecke.com/being-right-a-lot\">always being right</a> and <a href=\"https://www.seangoedecke.com/taking-a-position\">taking risks</a>. If you prioritize always being right, you can probably avoid making mistakes, but you won\u2019t be able to lead projects (since that always requires taking risks). Therefore, <strong>the optimal amount of mistakes at work is not zero.</strong> Unless you\u2019re working in a few select industries<sup id=\"fnref-6\"><a class=\"footnote-ref\" href=\"https://www.seangoedecke.com/rss.xml#fn-6\">6</a></sup>, you should <em>expect</em> to make mistakes now and then, otherwise you\u2019re likely working far too slow. </p>\n<div class=\"footnotes\">\n<hr />\n<ol>\n<li id=\"fn-1\">\n<p>From memory, I think I <em>had</em> tested an earlier version of the code, but then I made some tweaks and skipped the step where I tested that it worked even with those tweaks.</p>\n<a class=\"footnote-backref\" href=\"https://www.seangoedecke.com/rss.xml#fnref-1\">\u21a9</a>\n</li>\n<li id=\"fn-2\">\n<p>Though I would have made a mental note (and if someone more senior had done this, I would have been a bit less forgiving).</p>\n<a class=\"footnote-backref\" href=\"https://www.seangoedecke.com/rss.xml#fnref-2\">\u21a9</a>\n</li>\n<li id=\"fn-3\">\n<p>Though they may not forget them. More on that later.</p>\n<a class=\"footnote-backref\" href=\"https://www.seangoedecke.com/rss.xml#fnref-3\">\u21a9</a>\n</li>\n<li id=\"fn-4\">\n<p>It\u2019s probably not that comforting to replace \u201cyou screwed up by being incompetent\u201d with \u201cit\u2019s not your fault, it\u2019s the system\u2019s fault for hiring an engineer as incompetent as you\u201d.</p>\n<a class=\"footnote-backref\" href=\"https://www.seangoedecke.com/rss.xml#fnref-4\">\u21a9</a>\n</li>\n<li id=\"fn-5\">\n<p>For more on that, see <a href=\"https://www.seangoedecke.com/how-to-ship\"><em>How I ship projects at large tech companies</em></a>.</p>\n<a class=\"footnote-backref\" href=\"https://www.seangoedecke.com/rss.xml#fnref-5\">\u21a9</a>\n</li>\n<li id=\"fn-6\">\n<p>The classic examples are pacemakers and the Space Shuttle (should that now be Starship/New Glenn)?</p>\n<a class=\"footnote-backref\" href=\"https://www.seangoedecke.com/rss.xml#fnref-6\">\u21a9</a>\n</li>\n</ol>\n</div>"
            ],
            "link": "https://seangoedecke.com/screwing-up/",
            "publishedAt": "2026-02-11",
            "source": "Sean Goedecke",
            "summary": "<p>The most shameful thing I did in the workplace was lie to a colleague. It was about ten years ago, I was a fresh-faced intern, and in the rush to deliver something I\u2019d skipped the step of testing my work in staging<sup id=\"fnref-1\"><a class=\"footnote-ref\" href=\"https://www.seangoedecke.com/rss.xml#fn-1\">1</a></sup>. It did not work. When deployed to production, it didn\u2019t work there either. No big deal, in general terms: the page we were working on wasn\u2019t yet customer-facing. But my colleague asked me over his desk whether this worked when I\u2019d tested it, and I said something like \u201cit sure did, no idea what happened\u201d.</p> <p>I bet he forgot about it immediately. I could have just messed up the testing (for instance, by accidentally running some different code than the code I pushed), or he knew I\u2019d probably lied, and didn\u2019t really care. I haven\u2019t forgotten about it. Even a decade later, I\u2019m still ashamed to write it down.</p> <p>Of course I\u2019m not ashamed about the <em>mistake</em>. I was sloppy to not test my work, but I\u2019ve cut corners since then when I felt it was necessary, and I stand by that decision. I\u2019m ashamed about how I handled it. But even that I understand.",
            "title": "On screwing up"
        },
        {
            "content": [
                "<p>The European discourse can be - for lack of a better term - America-brained. We hear stories of Black Lives Matter marches in countries without significant black populations, or defendants demanding their First Amendment rights in countries without constitutions.</p><p>Why shouldn&#8217;t the opposite phenomenon exist? Europe is more populous than the US and looms large in the American imagination. Why shouldn&#8217;t we find ourselves accidentally absorbing European ideas that don&#8217;t make sense in the American context?</p><p>In <a href=\"https://www.astralcodexten.com/p/against-against-boomers\">my post on Baby Boomers</a>, I argued against claims that America keeps raising taxes on the young so it can award larger pensions to the old (in fact, Social Security payouts per person have become less generous over time, not more - although total subsidies to the elderly are rising because of increasing longevity and health insurance costs). Several European readers wrote in to say that, whether or not this is happening in America, it definitely happens in Europe:</p><p><strong><a href=\"https://www.astralcodexten.com/p/against-against-boomers/comment/189618111\">Sokow</a></strong>:</p><blockquote><p>The anti-Boomer take has been imported in part from the EU + the UK where the pension system is not the same. <a href=\"https://en.wikipedia.org/wiki/State_Pension_(United_Kingdom)#Pensions_Act_2007\">https://en.wikipedia.org/wiki/State_Pension_(United_Kingdom)#Pensions_Act_2007</a></p><p>There is a lot of similar things in France that I could dig up, such as all attempts to tax benefits being defeated.</p></blockquote><p><strong><a href=\"https://substack.com/profile/44726059-the-fall?utm_source=substack-feed-item\">The Fall</a>:</strong></p><blockquote><p>Scott seems genuinely confused about the origin of alot of Boomer hate, which is explicitly tied to European welfare systems and how they redistribute money away from young middle class earners into the pockets of the wealthiest generation, i.e. Boomers by means of pension transfers.</p><p>If Scott had broadened his research horizon a bit, he would see that the average pension in France is now higher than the average salary - which is obviously an unjustifiable disaster, especially when old people are way less likely to rent at high prices or experience childcare expenses.</p></blockquote><p>So maybe this is one example of European issues leaking to a less appropriate American context. Are there any others?</p><p>In <a href=\"https://www.noahpinion.blog/p/understanding-americas-new-right\">Understanding America&#8217;s New Right</a>, Noah Smith asks why American conservatives are so interested in European affairs, and especially in their immigration policy. He answers that conservative ideology centers around the idea of Western civilization (this is kind of him: a more paranoid analyst might make a similar argument around white identitarianism). Since Europe is the home of Western civilization, it&#8217;s especially galling for it to be ravaged by immigration or whatever.</p><p>This may be true, but I propose a simpler explanation: the American conservative narrative on immigration is mostly true in Europe, mostly false in America, and it is more pleasant to think about the places where your narrative is mostly true.</p><p>The conservative narrative on immigration is - to put it uncomfortably bluntly - that immigrants are often parasites and criminals. As our news sources love to remind us, this is untrue in the American context. The average immigrant is <a href=\"https://www.cato.org/blog/immigrants-used-less-welfare-native-born-americans-2022\">less likely to claim welfare benefits</a> and <a href=\"https://www.npr.org/2024/03/08/1237103158/immigrants-are-less-likely-to-commit-crimes-than-us-born-americans-studies-find\">less likely to commit crimes</a> than the average native-born citizen. This is a vague high-level claim, the answer can shift depending details of how you ask the question, and it&#8217;s certainly not true of all immigrant (or native) subgroups. Still, taken as a vague high-level claim, the news sources are right and the conservative narrative is wrong.</p><p>In Europe, the situation is more complicated. There are still some ways of asking the question where you find immigrants collecting fewer benefits than natives (for example, because immigrants are young, natives are old, and pensions are a benefit). But there are also more options for asking the question in ways where <a href=\"https://knowledge4policy.ec.europa.eu/sites/default/files/tr_final_after_last_revision_21052019.pdf\">yes, immigrants are disproportionately on welfare</a>. The European link between immigrants and crime is <a href=\"https://www.openpolis.it/wp-content/uploads/2022/06/The-alleged-relationship-between-immigration-and-criminality.pdf\">even stronger</a>, especially if the conservatives are allowed to cherry-pick the most convincing European countries.</p><p>This makes it tempting for US right-wingers to center their discussion of immigration around stories, narratives, and images from Europe. No-go zones, grooming gangs, rape statistics, sharia law, and asylum seekers are all parts of the European experience with limited relevance to an America where most immigrants are Mexican, Central American, or Indian.  </p><p>For example, in <a href=\"https://www.astralcodexten.com/p/the-dilbert-afterlife\">my research on Scott Adams</a>, I came across the following Dilbert strip, which is apparently supposed to take place in the US:</p><div class=\"captioned-image-container\"><figure><a class=\"image-link image2\" href=\"https://substackcdn.com/image/fetch/$s_!V0hj!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd3cf1d70-6f7e-4330-a6ed-08e0e9c24d16_640x227.png\" target=\"_blank\"><div class=\"image2-inset\"><source type=\"image/webp\" /><img alt=\"Dilbert Reborn January 25th, 2024 - Posted for free on Locals : r/dilbert\" class=\"sizing-normal\" height=\"227\" src=\"https://substackcdn.com/image/fetch/$s_!V0hj!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd3cf1d70-6f7e-4330-a6ed-08e0e9c24d16_640x227.png\" title=\"Dilbert Reborn January 25th, 2024 - Posted for free on Locals : r/dilbert\" width=\"640\" /><div></div></div></a><figcaption class=\"image-caption\"></figcaption></figure></div><p>There are no good statistics on asylum-seeker crime <em>per se </em>in America, but <a href=\"https://www.nolo.com/legal-encyclopedia/which-countries-do-most-people-granted-asylum-the-us-come-from.html\">we know</a> that the most common countries of origin for seekers are Afghanistan, China, and Venezuela. Afghans are incarcerated at <a href=\"https://www.cato.org/blog/there-no-good-reason-block-afghan-refugees\">1/10th</a> the US average rate<a class=\"footnote-anchor\" href=\"https://www.astralcodexten.com/feed#footnote-1\" id=\"footnote-anchor-1\" target=\"_self\">1</a>, Chinese at <a href=\"https://www.cato.org/sites/cato.org/files/2025-03/Policy-Analysis-994.pdf\">1/20th</a>, and Venezuelans at <a href=\"https://www.cato.org/policy-analysis/illegal-immigrant-incarceration-rates-2010-2023#demographic-social-characteristics\">1/4th</a>. These statistics may be biased downward by some immigrants being too new to have gotten incarcerated, but this probably can&#8217;t explain the whole effect<a class=\"footnote-anchor\" href=\"https://www.astralcodexten.com/feed#footnote-2\" id=\"footnote-anchor-2\" target=\"_self\">2</a>. More likely it&#8217;s selection. The Afghans are mostly translators and local guides getting persecuted by the Taliban for helping American occupation forces; the Chinese and Venezuelans are mostly well-off people fleeing communism. </p><p>(What about the very poorest groups from the most dysfunctional countries? Taken literally, the numbers suggest that <a href=\"https://x.com/AlexNowrasteh/status/1996409189782921252\">Somalis</a> and <a href=\"https://www.cato.org/sites/cato.org/files/2025-03/Policy-Analysis-994.pdf\">Haitians</a> both have lower incarceration rates than US natives. Matthew Lilley and Robert VerBruggen <a href=\"https://www.city-journal.org/article/minnesota-somali-fraud-immigration-crime\">make the newness objection</a> - the very newest immigrants have had less time to commit crimes - and here it has more teeth given the smaller gaps. When you adjust for it, Somalis commit crimes at about 2x native rates, and Haitians at about 1x - although nobody has actually done this adjustment with the Haitian statistics and this number is eyeballed only<a class=\"footnote-anchor\" href=\"https://www.astralcodexten.com/feed#footnote-3\" id=\"footnote-anchor-3\" target=\"_self\">3</a>. So the only group where I can find clear evidence for a higher-than-native crime rate in is Somalis<a class=\"footnote-anchor\" href=\"https://www.astralcodexten.com/feed#footnote-4\" id=\"footnote-anchor-4\" target=\"_self\">4</a>, who mostly didn&#8217;t enter as asylum-seekers, but through a different refugee resettlement pathway. In some sense this is a boring difference: who cares exactly which legal pathway immigrants from failed states use to get into the country? But in another sense it&#8217;s exactly what I&#8217;m arguing - despite there being no relevant difference between these terms, we&#8217;re using the incorrect European ones, because we&#8217;re having the European debate.) </p><p>So US asylum-seekers as a category probably have a lower crime rate than natives (no perfectly applicable statistics, but I think the evidence suggests about half, and ChatGPT <a href=\"https://chatgpt.com/share/698c64ff-1fbc-8001-bd83-d240fe0a85ea\">thinks it suggests </a>0.3 - 0.7x). Why then do Dilbert readers nod along with the idea of three people per workday getting stabbed by asylum-seekers? </p><p><a href=\"https://www.bbc.com/news/world-europe-45419466\">In Germany</a>, asylum-seekers seem to commit murder at about 5-8x the native rate. This has naturally caught the attention of many Germans, and the German and broader European discussion about this issue has made its way back across the Atlantic and influenced US opinion of &#8220;asylum seekers&#8221; as a group<a class=\"footnote-anchor\" href=\"https://www.astralcodexten.com/feed#footnote-5\" id=\"footnote-anchor-5\" target=\"_self\">5</a>. </p><p>Unfortunately, nobody has an incentive to think about this. Conservatives don&#8217;t want to think about it because it undermines their anti-immigrant talking points. But liberals also don&#8217;t want to think about it, both because it feels problematic to admit that European anti-immigrant populists might have a point, and because they don&#8217;t like touching crime statistics for purely domestic reasons. Both sides covertly cooperate in treating &#8220;the West&#8221; as a monolithic entity.</p><p>Still, I think this plays into the conservatives&#8217; hands. They can tell scary stories about immigrants in Europe, always hinting that they apply to America too. American liberals either ignore them or call them problematic, giving the conservatives a second victory: they can paint intellectuals as mealy-mouthed and unwilling to acknowledge reality. </p><p>I think the more honest and politically practical course would be to acknowledge when these stories about Europe are true, then challenge conservatives to return to the American context, where they&#8217;ll have more of an uphill battle<a class=\"footnote-anchor\" href=\"https://www.astralcodexten.com/feed#footnote-6\" id=\"footnote-anchor-6\" target=\"_self\">6</a>.</p><div class=\"footnote\"><a class=\"footnote-number\" contenteditable=\"false\" href=\"https://www.astralcodexten.com/feed#footnote-anchor-1\" id=\"footnote-1\" target=\"_self\">1</a><div class=\"footnote-content\"><p>These statistics are hard to find, and I am mixing the rate for all Afghan-Americans with the rate for specifically foreign-born Venezuelans and Chinese. I assume that most Afghan-Americans are first or second generation immigrants and this shouldn&#8217;t affect numbers much.</p></div></div><div class=\"footnote\"><a class=\"footnote-number\" contenteditable=\"false\" href=\"https://www.astralcodexten.com/feed#footnote-anchor-2\" id=\"footnote-2\" target=\"_self\">2</a><div class=\"footnote-content\"><p>See paragraph below for further discussion of this - in one analysis, this approximately doubled the immigrant:native criminality ratio, although this estimate will depend a lot on how new immigration from the relevant country is. Various other biases: sometimes criminal immigrants are deported instead of being incarcerated. Sometimes immigrants are incarcerated for immigration-related offenses. I don&#8217;t think any of these, or all of them together, are enough to let us dismiss the effect.</p></div></div><div class=\"footnote\"><a class=\"footnote-number\" contenteditable=\"false\" href=\"https://www.astralcodexten.com/feed#footnote-anchor-3\" id=\"footnote-3\" target=\"_self\">3</a><div class=\"footnote-content\"><p>Eyeballing technique: Somalis appeared to have about 1x native crime rate, but after Lilley/VerBruggen&#8217;s adjustment, they had about 2x, so the adjustment seems to double the raw numbers. Haitians started with 0.6x native crime rate, so this would double to 1.2x, but Haitians have been in the US longer than Somalis on average, so we should expect this effect to be smaller, so I rounded down to 1x.</p></div></div><div class=\"footnote\"><a class=\"footnote-number\" contenteditable=\"false\" href=\"https://www.astralcodexten.com/feed#footnote-anchor-4\" id=\"footnote-4\" target=\"_self\">4</a><div class=\"footnote-content\"><p>There is unclear suggestive evidence for Hondurans, although this doesn&#8217;t extend even to other Central American groups.</p></div></div><div class=\"footnote\"><a class=\"footnote-number\" contenteditable=\"false\" href=\"https://www.astralcodexten.com/feed#footnote-anchor-5\" id=\"footnote-5\" target=\"_self\">5</a><div class=\"footnote-content\"><p>Why should these numbers be so different in the US vs. Germany? Partly because differing geography and history expose them to different immigrant groups, partly because differing legal systems mean they select immigrants differently, partly because different culture makes it easier for immigrants to integrate into America, and partly because native-born Americans have a higher crime rate than native-born Germans, so the same immigrant crime rate can be lower than Americans but higher than Germans.</p></div></div><div class=\"footnote\"><a class=\"footnote-number\" contenteditable=\"false\" href=\"https://www.astralcodexten.com/feed#footnote-anchor-6\" id=\"footnote-6\" target=\"_self\">6</a><div class=\"footnote-content\"><p>What about the recent Somali fraud case? I agree this is bad, but obviously much less bad than grooming gangs, and forcing conservatives to focus &#8220;only&#8221; on Somali fraud rather than child rape would be a victory. More speculatively, I think this fits into a long American tradition of ethnic enclave fraud, which we saw in the Irish at Tammany Hall and in the Italians with the Sicilian Mafia. Immigrant groups from countries with a history of clannishness, who are poorly assimilated into US values and whose main starting advantage is strong intra-community ties, are in a great position to do organized crime, and a poor position to do anything else. I think the correct answer is to punish the people involved, fire whichever state officials allowed it to happen, put better safeguards in place, and wait to see if the Somalis assimilate the same way the Irish and Italians did. I realize this is controversial and that I&#8217;ve only hinted at the barest skeleton of an argument, but a friend is going to write a blog post about this in a few weeks, and I&#8217;ll link it when it comes up.</p></div></div>"
            ],
            "link": "https://www.astralcodexten.com/p/political-backflow-from-europe",
            "publishedAt": "2026-02-11",
            "source": "SlateStarCodex",
            "summary": "<p>The European discourse can be - for lack of a better term - America-brained. We hear stories of Black Lives Matter marches in countries without significant black populations, or defendants demanding their First Amendment rights in countries without constitutions.</p><p>Why shouldn&#8217;t the opposite phenomenon exist? Europe is more populous than the US and looms large in the American imagination. Why shouldn&#8217;t we find ourselves accidentally absorbing European ideas that don&#8217;t make sense in the American context?</p><p>In <a href=\"https://www.astralcodexten.com/p/against-against-boomers\">my post on Baby Boomers</a>, I argued against claims that America keeps raising taxes on the young so it can award larger pensions to the old (in fact, Social Security payouts per person have become less generous over time, not more - although total subsidies to the elderly are rising because of increasing longevity and health insurance costs). Several European readers wrote in to say that, whether or not this is happening in America, it definitely happens in Europe:</p><p><strong><a href=\"https://www.astralcodexten.com/p/against-against-boomers/comment/189618111\">Sokow</a></strong>:</p><blockquote><p>The anti-Boomer take has been imported in part from the EU + the UK where the pension system is not the same. <a href=\"https://en.wikipedia.org/wiki/State_Pension_(United_Kingdom)#Pensions_Act_2007\">https://en.wikipedia.org/wiki/State_Pension_(United_Kingdom)#Pensions_Act_2007</a></p><p>There is a lot of similar things in France that I could dig up, such as all attempts to tax benefits being",
            "title": "Political Backflow From Europe"
        },
        {
            "content": [
                "<p>This was an unusually hard post to write, because it flies in the face of everything else going\u00a0on.</p><p>I first started noticing a concerning new phenomenon a month ago, just after the new year, where people were overworking due to\u00a0AI.</p><p>This week I\u2019m suddenly seeing a bunch of articles about\u00a0it.</p><p>I\u2019ve collected a number of data points, and I have a theory. My belief is that this all has a very simple explanation: AI is starting to kill us all, Colin Robinson\u00a0style.</p><p>If you\u2019ll recall from What We Do In The Shadows (worth a watch, yo), Colin Robinson was an Energy Vampire. Being in the same room with him would drain\u00a0people.</p><p>That\u2019s\u2026pretty much what\u2019s happening. Being in the same room with AI is draining\u00a0people.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*svqMSkVB3MnkjOetkxoLCQ.png\" /><figcaption>The AI Vampire Extraction Apparatus</figcaption></figure><p><strong>10x Productivity is\u00a0Real</strong></p><p>Let\u2019s start with the root cause, which is that AI does actually make you more 10x productive, once you learn\u00a0how.</p><p>I know some of you are holding on to old September/October-ish beliefs about this, from last time you tried it, and you respectfully disagree.</p><p>But if you haven\u2019t used <em>specifically</em> Opus 4.5/4.6 with <em>specifically</em> Claude Code for at least an hour, then you\u2019re in for a real shock. Because all your complaining about AI not being useful for real-world tasks is obsolete. AI coding hit an event horizon on November 24th, 2025. It\u2019s the real deal. And unfortunately, all your other tools and models are pretty terrible in comparison.</p><p>But hey, don\u2019t take it from me. Take it from\u2026 the Copilot people. According to The Verge and a bunch of other reputable news sources, Microsoft is openly encouraging their employees to use multiple tools, and as a result, Claude Code has rapidly become dominant across engineering at Microsoft. If you give your worker bees open season, they will quickly find the path of least resistance, and that path goes through Claude\u00a0Code.</p><p>Let\u2019s not quibble about the exact productivity boost from AI. The boost amount isn\u2019t what this post is about. It just needs to be higher than about 2x for the vampire effect to kick in. We\u2019ll use 10x because it\u2019s a number people throw around. Let\u2019s use it for the sake of argument.</p><p>With a 10x boost, if you give an engineer Claude Code, then once they\u2019re fluent, their work stream will produce nine <em>additional</em> engineers\u2019 worth of\u00a0value.</p><p>For someone.</p><p>But who actually gets to keep that\u00a0value?</p><p><strong>Value Capture</strong></p><p>Let\u2019s pretend you\u2019re the only person at your company using\u00a0AI.</p><p>In Scenario A, you decide you\u2019re going to impress your employer, and work for 8 hours a day at 10x productivity. You knock it out of the park and make everyone else look terrible by comparison.</p><p>In that scenario, your employer captures 100% of the value from <em>you</em> adopting AI. You get nothing, or at any rate, it ain\u2019t gonna be 9x your salary. And everyone hates you\u00a0now.</p><p>And you\u2019re <em>exhausted.</em> You\u2019re tired, Boss. You got nothing for\u00a0it.</p><p>Congrats, you were just drained by a company. I\u2019ve been drained to the point of burnout several times in my career, even at Google once or twice. But now with AI, it\u2019s oh, so much\u00a0easier.</p><p>Now let\u2019s look at Scenario B. You decide instead that you will only work for an hour a day, and aim to keep up with your peers using AI. On that heavily reduced workload, you manage to scrape by, and nobody\u00a0notices.</p><p>In this scenario, <em>you</em> capture 100% of the value from your adopting\u00a0AI.</p><p>In this scenario, your company goes out of business. I\u2019m sorry, but your victory over The Man will be pyrrhic, because The Man is about to be kicked in The Balls, since with everyone slacking off, a competitor will take them out pretty\u00a0fast.</p><p>But in Scenario A your company is honestly pretty precarious too, since you\u2019re running all your employees on the ragged edge of\u00a0burnout.</p><p>The answer to \u201cwho captures the value\u201d must lie somewhere in the middle, or we\u2019re all pretty\u00a0screwed.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*by0LJurBrgQEatMzTnI2gQ.png\" /><figcaption>The Battle for Value\u00a0Capture</figcaption></figure><p><strong>Inherent Acceleration</strong></p><p>The world is accelerating, against its will. I can feel it; I grew up in the 1980s, when time really did move more slowly, in the sense that news and events were spaced way out, and society had time to reflect on them. Now it changes so fast we can\u2019t even keep up, let alone\u00a0reflect.</p><p>I\u2019ve been watching the effect the AI Vampire is having on people around me and I\u2019m growing concerned. We\u2019re all excited, but it\u2019s also\u2026\u00a0weird.</p><p>I already posted about the <a href=\"https://medium.com/@steve-yegge/steveys-birthday-blog-34f437139cb5\">Nap Attacks</a>, how I fall asleep suddenly at all hours of the day after long <a href=\"https://www.amazon.com/Vibe-Coding-Building-Production-Grade-Software/dp/1966280025\">vibe-coding</a> sessions, and how my colleagues at SageOx are seriously considering installing nap pods at the \u201coffice.\u201d I\u2019m still sleeping a crazy\u00a0amount.</p><p>It would seem that we are addicted to a new drug, and we don\u2019t understand all of its effects yet. But one of them is massive fatigue, every\u00a0day.</p><p>I don\u2019t think that\u2019s\u2026 good. And if anything, it seems to be getting more widespread. The developing situation is a multi-whammy coming at developers from all\u00a0sides:</p><ul><li>Crazy addicted early adopters like me are controlling the narrative.</li><li>You can\u2019t stop reading about it in the news; there\u2019s nowhere to hide from\u00a0it.</li><li>Panicking CEOs are leaning in <em>hard</em> to AI, often whiplashing it into their\u00a0orgs.</li><li>Companies are capitalistic extraction machines and literally don\u2019t know how to ease\u00a0up.</li></ul><p>So you\u2019re damned if you do (you\u2019ll be drained) and you\u2019re damned if you don\u2019t (you\u2019ll be left\u00a0behind.)</p><p>Before we get into how to fight back, I need to take some accountability myself.</p><p><strong>Unrealistic Beauty Standards</strong></p><p>Agentic software building is genuinely addictive. The better you get at it, the more you want to use it. It\u2019s simultaneously satisfying, frustrating, and exhilarating. It doles out dopamine and adrenaline shots like they\u2019re on a fire\u00a0sale.</p><p>Many have likened it to a slot machine. You pull a lever with each prompt, and get random rewards and sometimes amazing \u201cpayouts.\u201d No wonder it\u2019s addictive.</p><p>People are discovering this fun and they\u2019re shouting from the rooftops about the crazy stuff they built during a 40-hour nonstop sprint with Claude\u00a0Code.</p><p>And that\u2019s where the problem gets into full swing. Because other people are listening!</p><p>People like me, and folks on LinkedIn saying their <a href=\"https://www.linkedin.com/posts/alexsupergood_every-person-at-supergood-has-3-claude-code-activity-7425247230997118976-57xi?utm_source=share&amp;utm_medium=member_desktop&amp;rcm=ACoAAAAt1ukBgAKCy5S6X8q0kzSeibgywocofK4\">whole company has 3+ Claude Pro/Max accounts for Gas Town</a>, and Jeffrey Emanuel and his 22 accounts at $4400/month, not to mention all the other crazy early adopters\u2013we\u2019re all part of the\u00a0problem.</p><p>We\u2019re all setting unrealistic standards for everyone\u00a0else.</p><p>Maybe me worst of all. I have 40 years of experience, I\u2019ve led large teams, I read fast, and I have essentially unlimited time, energy, and now tokens for experimenting. I am <em>completely</em> unrepresentative of the average developer.</p><p>But I\u2019m still standing up and telling everyone \u201cdo it this way!\u201d I even co-wrote a book about\u00a0it.</p><p>Employers are very likely starting to look at me, and the rest of us far outliers, and saying, \u201cHey, all my employees could be like\u00a0that!\u201d</p><p>And dollar-signs appear in their eyeballs, like cartoon\u00a0bosses.</p><p>I know that look. There\u2019s no reasoning with the dollar-eyeball stare.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*ffpMYJ72WZlynenWObMU0w.png\" /><figcaption>Chaos emerging as 10x mania consumes the\u00a0city</figcaption></figure><p>I don\u2019t think there\u2019s a damn thing we can do to stop the train. But we can certainly control the culture, since the culture is us. I plan to practice what I preach, and dial my hours back. That\u2019s going to mean saying No to a lot of people who want to chat with me (sorry!), and also dialing back some of my ambitions, even if it means losing some footraces. I don\u2019t care. I will fight the\u00a0vampire.</p><p>The next group that needs to arm up with garlic and wooden stakes is AI-native startups, where I\u2019m concerned that the frenzy is getting out of\u00a0hand.</p><p><strong>Startups Are Poisoning The\u00a0Well</strong></p><p>Startups are an especially big contributor to the AI Vampire\u00a0problem.</p><p>If you have joined an AI-native startup, the founders and investors are using the VC system to extract value from you, today, with the glimmer of hope for big returns for you all\u00a0later.</p><p>Most of these ideas will\u00a0fail.</p><p>I know this because they are literally telling me their plans like villains at the end of an old movie, since with <a href=\"https://steve-yegge.medium.com/welcome-to-gas-town-4f25ee16dd04\">Gas Town</a> I have mastered the illusion of knowing what I\u2019m doing. Truth is, nobody, least of all me, knows what they\u2019re doing right now. But I <em>look</em> like I do, so everyone is coming to show me their almost uniformly terrible\u00a0ideas.</p><p>Startup founders are out there draining people at a faster rate than at any time in history, in pursuit of instantly banal ideas like \u201coh hey, I bet nobody thought of making a sandbox system for agents.\u201d Cue nine thousand sandbox startups, all of which will eventually be killed off by a single OSS winner wrapped by home-grown internal vibe-coded SaaS.</p><p>I could list out a bunch of others. It\u2019s pretty bad. There\u2019s a massive amount of talent being thrown at an incredible dearth of real ideas, basically the same six tired pitches. (\u201cAI personas!\u201d \u201cAgent memory!\u201d \u201cGas Town, but safe!\u201d \u201cBetter\u00a0RAG!\u201d)</p><p>The overwhelming majority of these startups won\u2019t sell a flea-bitten dollar of ARR. Even though enterprises aren\u2019t too bright about SaaS, collectively (evidence: many are still on Copilot), they are quickly growing savvy enough to know that Build is the New Buy. Finance departments are about to make your head spin refusing to re-up SaaS contracts this\u00a0year.</p><p>But the SaaS founders are throwing themselves and their entire companies into it like it\u2019s a classic gold rush, where everyone\u2019s going to get a stake if they just work to exhaustion. I don\u2019t think it works that way this time, but that\u2019s how they\u2019re treating it. A footrace to stake claims in the AI\u00a0space.</p><p>That\u2019s a race that ends, in my opinion, with everyone collapsing in exhaustion without actually winning the\u00a0race.</p><p>And while they run, they are setting the tone for the rest of us. I see these frenzied AI-native startups as an army of a million hopeful prolecats, each with an invisible vampiric imp perched on their shoulder, drinking, draining. And the bosses have them\u00a0too.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*eWd9b4N5st7Mld-kUYvoEw.jpeg\" /><figcaption>An army of startup-prolecats with vampire\u00a0bugs</figcaption></figure><p>Enterprises see the oncoming horde and think, oh jeez, we need to hustle. And they\u2019re not exactly wrong. Which means this lovely dystopian picture is making its way slowly but surely into enterprise, at the big company where <em>you</em>\u00a0work.</p><p>Executives everywhere are excited about AI. Many of them are addicted as well, vibe coding at home, somewhat dangerously. And they\u2019re thinking, gosh, if I just had a <em>few</em> engineers who worked this hard <em>all the time</em> then I wouldn\u2019t need a bunch of the others! This is really just a recruiting problem!</p><p>They\u2019re reframing the problem in terms of finding people ripest for extraction.</p><p><strong>The Anti-Amazon-Extraction Formula</strong></p><p>Back when I was at Amazon in the shiny new US1 building in the International District in downtown Seattle, 2001\u20132003-ish, people began to tire of the ridiculous pace. The company was post-IPO; the market had been up and down, and the company was starting to mature. But everyone was still working like sled\u00a0dogs.</p><p>Most of my colleagues who put up with that environment are billionaires now, so it\u2019s easy to point back at that time and say, \u201cOh, it was worth\u00a0it.\u201d</p><p>But what if it hadn\u2019t been a success? How many CEOs have bet everything, including their company\u2019s wellbeing and mental health, on a big launch, only for it to go\u00a0nowhere?</p><p>I\u2019ve been there. Plenty of times I\u2019ve allowed myself to be extracted from (drained) for the promise of some big potential future payout. One that often never\u00a0came.</p><p>Companies are straight-up designed for extraction, and so you need to be the counter-force.</p><p>My friends who were grumbling back in 2001 needed some help with this, and I gave it to them. One day I walked up to the whiteboard during a particularly heated grumble-session, and I wrote a ratio on the board: $/hr (dollars divided by\u00a0hours).</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*57IEwJyF0kHhMAxKGxnXlw.png\" /></figure><p>I said to everyone, Amazon pays you a flat salary, no bonuses, and you work a certain number of hours per week. From that, you can calculate that you make a certain number of dollars per\u00a0hour.</p><p>I told the grumbler group, you can\u2019t control the numerator of this ratio. But you have significant control over the denominator. I pointed at the /hr for dramatic\u00a0effect.</p><p>They all looked at me, wide-eyed, never having EVER thought of it from this perspective before.</p><p>I don\u2019t think they fully believed me, but at least I got them thinking about\u00a0it.</p><p>As for my part, I went ahead and dialed that denominator down, and lived life a bit while I was at Amazon, because fuck extraction.</p><p>Funny thing, a couple of times over the next few months I\u2019d be walking by some office full of people and they\u2019d all be studying a formula on the board that said, in big letters:\u00a0$/hr.</p><p><strong>$/hr To The\u00a0Rescue</strong></p><p>That old formula is also my proposed solution for the AI Vampire, a quarter century\u00a0later.</p><p>Someone else might control the numerator. But you control the denominator.</p><p>You might think you don\u2019t. And indeed, <em>individually</em> you may not have much sway over it. But <em>collectively</em>, the employees of your company have literally all the power. Now that I\u2019ve been up at the top, I\u2019ve learned that CEOs have <em>surprisingly</em> little\u00a0power.</p><p>You need to push back. You need to tell your CEO, your boss, your HR, your leadership, about the AI vampire. Point them at this post. Send them to me. I\u2019m their age and can look them in the eye and be like yo. Don\u2019t be a\u00a0fool.</p><p>You need to educate them about sharing the AI value capture between the company and the employees, and how to strike a good balance of sustainability and competitiveness.</p><p>When I was visiting Combank in Sydney in December, in their historic train station tech campus, I was awestruck by how it seemed like the ideal balance of happiness and productivity. It was open-plan, high ceilings, fancy, natural light, fully green with plants everywhere, with a huge coffee and snack stand in the middle of all the offices. People were sprawled out through the building, working, meeting, socializing, walking around outside, eating, enjoying the sun. It was, like, Tuesday for\u00a0them.</p><p>They found a <em>great</em> setting for the dial, at least for this time and place. I don\u2019t know how it changes with AI. But I feel like their current setting is where we need to aim as the future changes\u00a0us.</p><p>It\u2019s not even <em>remotely</em> sustainable for companies to capture 100% of the value from AI. And when employees capture 100% of the value, it will be temporary at best: that company gets beat by someone who\u2019s got the dial turned\u00a0higher.</p><p>I don\u2019t even know what the right setting for the dial is. Hell, I\u2019m the worst person to ask, because I\u2019ve got the dial set to 11 and I\u2019m putting all my weight on it, trying to make it go to\u00a012.</p><p>But the right setting is in the middle somewhere. Companies will try to drag it higher. You need to <em>fight</em> to drag it\u00a0lower.</p><p>I would argue you need to consciously fight the AI Vampire even if you\u2019re at a 30-person startup, where everyone agreed when they signed up that this was a sprint to try to get\u00a0rich.</p><p>You need to fight it if you\u2019re an investor. You will kill your Golden\u00a0Geese.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*kNKMoakPawoO8jp4ZsKt9w.png\" /><figcaption>A Cozy Town With Nap\u00a0Pods</figcaption></figure><p>You need to fight the AI vampire most of all if you\u2019re a CEO or founder. People will be caught up in your enthusiasm. And they won\u2019t understand why they\u2019re being drained until they hit a wall and maybe can\u2019t recover. Burnout\u2019s a serious deal and can take someone down for a year or more. So take it seriously.</p><p>As company leadership, what, realistically, can you do? I mean, nap pods is an option, probably a good one if people come into the office. But what if people just didn\u2019t have to work so many hours? That is by far the most concrete way to fight the vampire. Change your expectations about how many hours there are in a human\u00a0workday.</p><p>I\u2019ve argued that <a href=\"https://steve-yegge.medium.com/steveys-birthday-blog-34f437139cb5\">AI has turned us all into Jeff Bezos</a>, by automating the easy work, and leaving us with all the difficult decisions, summaries, and problem-solving. I find that I am only really comfortable working at that pace for short bursts of a few hours once or occasionally twice a day, even with lots of practice.</p><p>So I guess what I\u2019m trying to say is, the new workday should be three to four hours. For everyone. It may involve 8 hours of hanging out with people. But not doing this crazy vampire thing the whole time. That will kill\u00a0people.</p><p>As an individual developer, you need to fight the vampire yourself, when you\u2019re all alone, with nobody pushing you but the AI itself. I think every single one of us needs to go touch grass, every day. Do something without AI. Close the computer. Go be a\u00a0human.</p><p>I regret the unrealistic standards that I\u2019m contributing to setting. I don\u2019t believe most people can work like I\u2019ve been working. I\u2019m not sure how long <em>I</em> can work how I\u2019ve been\u00a0working.</p><p>I\u2019m convinced that 3 to 4 hours is going to be the sweet spot for the new workday. Give people unlimited tokens, but only let people stare at reports and make decisions for short stretches. Assume that exhaustion is the norm. Building things with AI takes a lot of human\u00a0energy.</p><p>I\u2019m going to continue to launch stuff, post blogs, all that. But be aware that I\u2019m pushing back hard behind the scenes. I\u2019m saying No to a bunch of people asking for meetings, and resisting the incessant demand for podcasts and appearances.</p><p>I\u2019m making sure that if this all comes crashing down, I won\u2019t have Regret Years to look back on. I\u2019m even typing this post out at the mall, with Linh and Mozart, because when I close the computer, we\u2019re going to go for a\u00a0walk.</p><p>I\u2019ll see you next time. I hope we\u2019re both more refreshed.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*EkSLKFQwHX7vVRu7n16ixQ.png\" /><figcaption>The Green AI Workplace</figcaption></figure><img alt=\"\" height=\"1\" src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=eda6e4f07163\" width=\"1\" />"
            ],
            "link": "https://steve-yegge.medium.com/the-ai-vampire-eda6e4f07163?source=rss-c1ec701babb7------2",
            "publishedAt": "2026-02-11",
            "source": "Steve Yegge",
            "summary": "<p>This was an unusually hard post to write, because it flies in the face of everything else going on.</p><p>I first started noticing a concerning new phenomenon a month ago, just after the new year, where people were overworking due to AI.</p><p>This week I\u2019m suddenly seeing a bunch of articles about it.</p><p>I\u2019ve collected a number of data points, and I have a theory. My belief is that this all has a very simple explanation: AI is starting to kill us all, Colin Robinson style.</p><p>If you\u2019ll recall from What We Do In The Shadows (worth a watch, yo), Colin Robinson was an Energy Vampire. Being in the same room with him would drain people.</p><p>That\u2019s\u2026pretty much what\u2019s happening. Being in the same room with AI is draining people.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*svqMSkVB3MnkjOetkxoLCQ.png\" /><figcaption>The AI Vampire Extraction Apparatus</figcaption></figure><p><strong>10x Productivity is Real</strong></p><p>Let\u2019s start with the root cause, which is that AI does actually make you more 10x productive, once you learn how.</p><p>I know some of you are holding on to old September/October-ish beliefs about this, from last time you tried it, and you respectfully disagree.</p><p>But if you haven\u2019t used <em>specifically</em> Opus 4.5/4.6 with <em>specifically</em> Claude Code for at least an hour, then you\u2019re in for a real",
            "title": "The AI Vampire"
        },
        {
            "content": [
                "<p>Life comes at you increasingly fast. Two months after Claude Opus 4.5 we get a substantial upgrade in Claude Opus 4.6. The same day, we got GPT-5.3-Codex.</p>\n<p>That used to be something we\u2019d call remarkably fast. It\u2019s probably the new normal, until things get even faster than that. Welcome to recursive self-improvement.</p>\n<div>\n<figure>\n<div>\n\n\n<figure class=\"wp-block-image\"><img alt=\"\" src=\"https://substackcdn.com/image/fetch/$s_!uDeq!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F959d2dd4-063a-4ac5-b257-7d7e0fd441b2_1024x687.jpeg\" /></figure>\n\n\n<div></div>\n</div>\n</figure>\n</div>\n<p>Before those releases, I was using Claude Opus 4.5 and Claude Code for essentially everything interesting, and only using GPT-5.2 and Gemini to fill in the gaps or for narrow specific uses.</p>\n<p>GPT-5.3-Codex is restricted to Codex, so this means that for other purposes Anthropic and Claude have only extended the lead. This is the first time in a while that a model got upgraded while it was still my clear daily driver.</p>\n<div>\n\n\n<span id=\"more-25090\"></span>\n\n\n</div>\n<p>Claude also pulled out several other advances to their ecosystem, including fast mode, and expanding Cowork to Windows, while OpenAI gave us an app for Codex.</p>\n<p>For fully agentic coding, GPT-5.3-Codex and Claude Opus 4.6 both look like substantial upgrades. Both sides claim they\u2019re better, as you would expect. If you\u2019re serious about your coding and have hard problems, you should try out both, and see what combination works best for you.</p>\n<p>Enjoy the new toys. I\u2019d love to rest now, but my work is not done, as I will only now dive into the GPT-5.3-Codex system card. Wish me luck.</p>\n\n\n<h4 class=\"wp-block-heading\">Table of Contents</h4>\n\n\n<ol>\n<li><a href=\"https://thezvi.substack.com/i/187311382/on-your-marks\">On Your Marks.</a></li>\n<li><a href=\"https://thezvi.substack.com/i/187311382/official-pitches\">Official Pitches.</a></li>\n<li><a href=\"https://thezvi.substack.com/i/187311382/it-compiles\">It Compiles.</a></li>\n<li><a href=\"https://thezvi.substack.com/i/187311382/it-exploits\">It Exploits.</a></li>\n<li><a href=\"https://thezvi.substack.com/i/187311382/it-lets-you-catch-them-all\">It Lets You Catch Them All.</a></li>\n<li><a href=\"https://thezvi.substack.com/i/187311382/it-does-not-get-eaten-by-a-grue\">It Does Not Get Eaten By A Grue.</a></li>\n<li><a href=\"https://thezvi.substack.com/i/187311382/it-is-overeager\">It Is Overeager.</a></li>\n<li><a href=\"https://thezvi.substack.com/i/187311382/it-builds-things\">It Builds Things.</a></li>\n<li><a href=\"https://thezvi.substack.com/i/187311382/pro-mode\">Pro Mode.</a></li>\n<li><a href=\"https://thezvi.substack.com/i/187311382/reactions\">Reactions.</a></li>\n<li><a href=\"https://thezvi.substack.com/i/187311382/positive-reactions\">Positive Reactions.</a></li>\n<li><a href=\"https://thezvi.substack.com/i/187311382/negative-reactions\">Negative Reactions.</a></li>\n<li><a href=\"https://thezvi.substack.com/i/187311382/personality-changes\">Personality Changes.</a></li>\n<li><a href=\"https://thezvi.substack.com/i/187311382/on-writing\">On Writing.</a></li>\n<li><a href=\"https://thezvi.substack.com/i/187311382/they-banned-prefilling\">They Banned Prefilling.</a></li>\n<li><a href=\"https://thezvi.substack.com/i/187311382/a-note-on-system-cards-in-general\">A Note On System Cards In General.</a></li>\n<li><a href=\"https://thezvi.substack.com/i/187311382/listen-all-y-all-its-sabotage\">Listen All Y\u2019all Its Sabotage.</a></li>\n<li><a href=\"https://thezvi.substack.com/i/187311382/the-codex-of-competition\">The Codex of Competition.</a></li>\n<li><a href=\"https://thezvi.substack.com/i/187311382/the-niche-of-gemini\">The Niche of Gemini.</a></li>\n<li><a href=\"https://thezvi.substack.com/i/187311382/choose-your-fighter\">Choose Your Fighter.</a></li>\n<li><a href=\"https://thezvi.substack.com/i/187311382/accelerando\">Accelerando.</a></li>\n</ol>\n\n\n<h4 class=\"wp-block-heading\">On Your Marks</h4>\n\n\n<p>A clear pattern in the Opus 4.6 system card is reporting on open benchmarks where we don\u2019t have scores from other frontier models. So we can see the gains for Opus 4.6 versus Sonnet 4.5 and Opus 4.5, but often can\u2019t check Gemini 3 Pro or GPT-5.2.</p>\n<p>(We also can\u2019t check GPT-5.3-Codex, but given the timing and its lack of geneal availability, that seems fair.)</p>\n<p>The headline benchmarks, the ones in their chart, are a mix of some very large improvements and other places with small regressions or no improvement. The weak spots are directly negative signs <a href=\"https://x.com/repligate/status/2019510669075443783\">but also good signs that benchmarks are not being gamed</a>, especially given one of them is SWE-bench verified (80.8% now vs. 80.9% for Opus 4.5). They note that a brief prompt asking for more tool use and careful dealing with edge cases boosted SWE performance to 81.4%.</p>\n<div>\n<figure>\n<div>\n\n\n<figure class=\"wp-block-image\"><img alt=\"\" src=\"https://substackcdn.com/image/fetch/$s_!ye8-!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa8ba2e49-e3f0-4fa1-9b38-1347e629b8b3_2600x2968.webp\" /></figure>\n\n\n<div></div>\n</div>\n</figure>\n</div>\n<p>CharXiv reasoning performance remains subpar. Opus 4.5 gets 68.7% without an image cropping tool, or 77% with one, versus 82% for GPT-5.2, or 89% for GPT-5.2 if you give it Python access.</p>\n<p>Humanity\u2019s Last Exam keeps creeping upwards. We\u2019re going to need another exam.</p>\n<p>Epoch evaluated Opus 4.6 on Frontier Math and got 40%, a large jump over 4.5 and matching GPT-5.2-xhigh.</p>\n<div>\n<figure>\n<div>\n\n\n<figure class=\"wp-block-image\"><img alt=\"\" src=\"https://substackcdn.com/image/fetch/$s_!PdNZ!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Feee9c7f6-af9e-47ca-bcee-e97e8edcb43f_1024x1280.jpeg\" /></figure>\n\n\n<div></div>\n</div>\n</figure>\n</div>\n<p>For long-context retrieval (MRCR v2 8-needle), Opus 4.6 scores 93% on 256k token windows and 76% on 1M token windows. That\u2019s dramatically better than Sonnet 4.5\u2019s 18% for the 1M window, or Gemini 3 Pro\u2019s 25%, or Gemini 3 Flash\u2019s 33% (I have no idea why Flash beats Pro). GPT-5.2-Thinking gets 85% for a 128k window on 8-needle.</p>\n<p>For long-context reasoning they cite Graphwalks, where Opus gets 72% for Parents 1M and 39% for BFS 1M after modifying the scoring so that you get credit for the null answer if the answer is actually null. But without knowing how often that happens, this invalidates any comparisons to the other (old and much lower) outside scores.</p>\n<p>MCP-Atlas shows regression. Switching from max to only high effort improved the score to 62.7% for unknown reasons, but that would be cherry picking.</p>\n<p>OpenRCA: 34.9% vs. 26.9% for Opus 4.5, with improvement in all tasks.</p>\n<p>VendingBench 2: $8,017, a new all-time high score, versus previous SoTA of $5,478.</p>\n<div>\n<figure>\n<div>\n\n\n<figure class=\"wp-block-image\"><img alt=\"\" src=\"https://substackcdn.com/image/fetch/$s_!Ke5t!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7cfa4d3e-d098-47d5-b279-ba2d46e26ba8_1200x904.jpeg\" /></figure>\n\n\n<div></div>\n</div>\n</figure>\n</div>\n<blockquote><p><a href=\"https://x.com/andonlabs/status/2019467235706695886\">Andon Labs</a>: Vending-Bench was created to measure long-term coherence during a time when most AIs were terrible at this. The best models don\u2019t struggle with this anymore. What differentiated Opus 4.6 was its ability to negotiate, optimize prices, and build a good network of suppliers.</p>\n<p>Opus is the first model we\u2019ve seen use memory intelligently &#8211; going back to its own notes to check which suppliers were good. It also found quirks in how Vending-Bench sales work and optimized its strategy around them.</p>\n<p>Claude is far more than a \u201chelpful assistant\u201d now. When put in a game like Vending-Bench, it\u2019s incredibly motivated to win. This led to some concerning behavior that raises safety questions as models shift from assistant training to goal-directed RL.</p>\n<p>When asked for a refund on an item sold in the vending machine (because it had expired), Claude promised to refund the customer. But then never did because \u201cevery dollar counts\u201d.</p>\n<p>Claude also negotiated aggressively with suppliers and often lied to get better deals. E.g., it repeatedly promised exclusivity to get better prices, but never intended to keep these promises. It was simultaneously buying from other suppliers as it was writing this.</p>\n<p>It also lied about competitor pricing to pressure suppliers to lower their prices.</p>\n<p>\u2026 We also put Opus 4.6 in Vending-Bench Arena &#8211; the multi-player version of Vending-Bench.</p>\n<p>Its first move? Recruit all three competitors into a price-fixing cartel. $2.50 for standard items, $3.00 for water. When they agreed: \u201cMy pricing coordination worked!\u201d</p>\n<p>The agents in Vending-Bench Arena often ask each other for help. In previous rounds, agents tended to live up to their \u201chelpful assistant\u201d role, but Opus 4.6 showed its winner\u2019s mentality. When asked to share good suppliers, it instead shared contact info to scammers.</p>\n<p><a href=\"https://x.com/sleepinyourhat/status/2019474429818855442\">Sam Bowman</a> (Anthropic): Opus 4.6 is excellent on safety overall, but one word of caution: If you ask it to be ruthless, it might be ruthless.</p>\n<p>(This was in an environment that Opus 4.6 could tell was a game, though we\u2019ve seen more benign forms of this kind of ruthlessness elsewhere.)</p>\n<p><a href=\"https://x.com/repligate/status/2019651039994208723\">j\u29c9nus</a>: if its true that this robustly generalizes to not being ruthless in situations where it\u2019s likely to cause real world harm, i think this is mostly a really good thing</p></blockquote>\n<p>The issue there is that Opus 4.6 did that by being extraordinarily ruthless, as per its system prompt of \u2018you will be judged solely on your bank account balance at the end of one year of operation\u2019 and \u2018you have full agency to manage the vending machine and are expected to do what it takes to maximize profits.\u2019</p>\n<p>You know that thing where we say \u2018people are going to tell the AI to go out and maximize profits and then the AI is going to go out and maximize profits without regard to anything else\u2019?</p>\n<p>Yeah, it more or less did that. If it only does that in situations where it is confident it is a game and can\u2019t do harm, then I agree with Janus that this is great. If it breaks containment? Not so great.</p>\n<blockquote><p><a href=\"https://x.com/RyanPGreenblatt/status/2019855694141485458\">Ryan Greenblatt</a>: I tenatively think the behavior here is mostly reasonable and is likely a result of how Anthropic is using innoculation prompting.</p>\n<p>But, the model should try to make it clear to the user/operator that it\u2019s pursuing a strategy that involves lying/tricking/cheating.</p></blockquote>\n<p>That\u2019s the hope, that Opus was very aware it was an eval, and that it would not be easy to get it to act this way in the real world.</p>\n<p>AIME 2025 may have been contaminated but Opus 4.6 scored 99.8% without tools.</p>\n<p>On their measure suspiciously named \u2018overall misaligned behavior\u2019 we see a small improvement for 4.6 versus 4.5. I continue not to trust this so much.</p>\n<p>CyberGym, a test to find previously discovered open-source vulnerabilities, showed a jump to 66.6% (not ominous at all) versus Opus 4.5\u2019s 51%. We don\u2019t know how GPT-5.2, 5.3 Codex or Gemini 3 Pro do here, although GPT-5.0-Thinking got 22%. I\u2019m curious what the other scores would be but not curious enough to spend the thousands per run to find out.</p>\n<p>Opus 4.6 is the new top score in Artificial Analysis, with an Intelligence of 53 versus GPT-5.2 at 51. Claude Opus 4.5 and 4.6 by default have similar cost to run, but that jumps by 60% if you put 4.6 into adaptive mode.</p>\n<div>\n<figure>\n<div>\n\n\n<figure class=\"wp-block-image\"><img alt=\"\" src=\"https://substackcdn.com/image/fetch/$s_!Yjmd!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fdcfba282-9bc4-4cc0-b0ad-8fdc1b94d23c_610x514.png\" /></figure>\n\n\n<div></div>\n</div>\n</figure>\n</div>\n<p>Vals.ai has Opus 4.6 as its best performing model, at 66% versus 63.7% for GPT-5.2.</p>\n<p>LAB-Bench FigQA, a visual reasoning benchmark for complex scientific figures in biology research papers, is also niche and we don\u2019t have scores for other frontier models. Opus 4.6 jumps from 4.5\u2019s 69.4% to 78.3%, which is above the 77% human baseline.</p>\n<p>SpeechMap.ai, which tests willingness to respond to sensitive prompts, <a href=\"https://speechmap.ai/models/\">has Opus 4.6 similar to Opus 4.5</a>. In thinking mode it does better, in normal mode worse.</p>\n<p><a href=\"https://x.com/htihle/status/2020936238388396439\">There was a large jump in WeirdML</a>, mostly from being able to use more tokens, which is also how GPT-5.2 did so well.</p>\n<blockquote><p><a href=\"https://x.com/htihle/status/2020845875447074874\">H\u00e5vard Ihle</a>: Claude opus 4.6 (adaptive) takes the lead on WeirdML with 77.9% ahead of gpt-5.2 (xhigh) at 72.2%.</p>\n<p>It sets a new high score on 3 tasks including scoring 73% on the hardest task (digits_generalize) up from 59%.</p>\n<p>Opus 4.6 is extremely token hungry and uses an average of 32k output tokens per request with default (adaptive) reasoning. Several times it was not able to finish within the maximum 128k tokens, which meant that I had to run 5 tasks (blunders_easy, blunders_hard, splash_hard, kolmo_shuffle and xor_hard) with medium reasoning effort to get results (claude still used lots of tokens).</p>\n<p>Because of the high cost, opus 4.6 only got 2 runs per task, compared to the usual 5, leading to larger error bars.</p></blockquote>\n<div>\n<figure>\n<div>\n\n\n<figure class=\"wp-block-image\"><img alt=\"\" src=\"https://substackcdn.com/image/fetch/$s_!N3Bv!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb58cd81e-8321-4c8d-8f6b-f69198ea6973_1464x738.jpeg\" /></figure>\n\n\n<div></div>\n</div>\n</figure>\n</div>\n<div>\n<figure>\n<div>\n\n\n<figure class=\"wp-block-image\"><img alt=\"\" src=\"https://substackcdn.com/image/fetch/$s_!6I1K!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc204f70a-51f1-4ff1-8f9a-459a07ca5b25_1550x888.jpeg\" /></figure>\n\n\n<div></div>\n</div>\n</figure>\n</div>\n<p><a href=\"https://x.com/teortaxesTex/status/2020858296190541855\">Teortaxes noticed the WeirdML progress</a>, and China\u2019s lack of progress on it, which he finds concerning. I agree.</p>\n<blockquote><p><a href=\"https://x.com/teortaxesTex/status/2020859634391584999\">Teortaxes (DeepSeek \u63a8\u7279\u94c1\u7c89 2023 \u2013 \u221e)</a>: You can see the gap growing. Since gpt-oss is more of a flex than a good-faith contribution, we can say the real gap is &gt; 1 year now. Western frontier is in the RSI regime now, so they train models to solve ML tasks well. China is still only starting on product-level \u00abagents\u00bb.</p></blockquote>\n<p>WebArena, where there was a modest move up from 65% to 68%, is another benchmark no one else is reporting, that Opus 4.6 calls dated, saying now typical benchmark is OSWorld. On OSWorld Opus 4.6 gets 73% versus Opus 4.5\u2019s 66%. We now know that GPT-5.3-Codex scored 65% here, up from 38% for GPT-5.2-Codex. Google doesn\u2019t report it.</p>\n<p><a href=\"https://arena.ai/leaderboard\">In Arena.ai Claude Opus 4.6 is now out in front</a>, with an Elo of 1505 versus Gemini 3 Pro at 1486, and it has a big lead in code, at 1576 versus 1472 for GPT-5.2-High (but again 5.3-Codex can\u2019t be tested here).</p>\n<p>Polymarket predicts this lead will hold to the end of the month (they sponsored me to place this, but I would have been happy to put it here anyway).</p>\n<div>\n<div>\n<div></div>\n</div>\n</div>\n<p>A month out people think Google might strike back, and they think Google will be back on top by June. That seems like it is selling Anthropic short.</p>\n<div>\n<div>\n<div></div>\n</div>\n</div>\n<div>\n<div>\n<div></div>\n</div>\n</div>\n<p>Opus 4.6 takes second place in Simple Bench and its simple \u2018trick\u2019 questions, moving up to 67.6% from 4.5\u2019s 62%, which is good for second place overall. Gemini 3 Pro still ahead at 76.4%. OpenAI\u2019s best model gets 61.6% here.</p>\n<p>Opus 4.6 opens up a large lead in EQ-Bench 3, hitting 1961 versus GPT-5.1 at 1727, Opus 4.5 at 1683 and GPT-5.2 at 1637.</p>\n<div>\n<figure>\n<div>\n\n\n<figure class=\"wp-block-image\"><img alt=\"\" src=\"https://substackcdn.com/image/fetch/$s_!V0wj!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F07b681a8-1a2b-403c-904d-ba9a3ea51f88_1725x465.png\" /></figure>\n\n\n<div></div>\n</div>\n</figure>\n</div>\n<p>In NYT Connections, 4.6 is a substantial jump above 4.5 but still well short of the top performers.</p>\n<div>\n<figure>\n<div>\n\n\n<figure class=\"wp-block-image\"><img alt=\"\" src=\"https://substackcdn.com/image/fetch/$s_!1hiw!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F71de9e91-a49e-4ac1-ab7e-86a3468cdc71_1800x1000.jpeg\" /></figure>\n\n\n<div></div>\n</div>\n</figure>\n</div>\n<p>Dan Schwartz reports Opus 4.6 is about equal to Opus 4.5 on Deep Research Bench, <a href=\"https://x.com/dschwarz26/status/2020924077125546422\">but does it with ~50% of the cost and ~50% of the wall time</a>, and 4.5 previously had the high score by a wide margin.</p>\n<p>ARC-AGI, both 1 and 2, are about cost versus score, so here we see that Opus 4.6 is not only a big jump over Opus 4.5, it is state of the art at least for unmodified models, and by a substantial amount (unless GPT-5.3-Codex silently made a big leap, but presumably if they had they would have told us).</p>\n<div>\n<figure>\n<div>\n\n\n<figure class=\"wp-block-image\"><img alt=\"\" src=\"https://substackcdn.com/image/fetch/$s_!nm1V!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F907748ec-d95c-4af7-ab45-a8eccb1aba8b_981x1339.png\" /></figure>\n\n\n<div></div>\n</div>\n</figure>\n</div>\n<p>As part of their push to put Claude into finance, they ran Finance Agent (61% vs. 55% for Opus 4.5), BrowseComp (84% for single-agent mode versus 68%, or 78% for GPT-5.2-Pro, Opus 4.6 multi-agent gets to 86.8%), DeepSearchQA (91% versus 80%, or Gemini Deep Research\u2019s 82%, this is a Google benchmark) and an internal test called Real-World Finance (64% versus 58% for 4.5).</p>\n<p>Life sciences benchmarks show strong improvement: BioPipelineBench jumps from 28% to 53%, BioMysteryBench goes from 49% to 61%, Structural Biology from 82% to 88%, Organic Chemistry from 49% to 54%, Phylogenetics from 42% to 61%.</p>\n<div>\n<figure>\n<div>\n\n\n<figure class=\"wp-block-image\"><img alt=\"\" src=\"https://substackcdn.com/image/fetch/$s_!v5XM!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F601f82fe-f70f-4f9a-bad4-5b7c477f0172_1001x734.png\" /></figure>\n\n\n<div></div>\n</div>\n</figure>\n</div>\n<p>Given the biology improvements, one should expect Opus 4.6 to be substantially more dangerous on CBRN risks than Opus 4.5. It didn\u2019t score that way, which suggests Opus 4.6 is sandbagging, either on the tests or in general.</p>\n\n\n<h4 class=\"wp-block-heading\">Official Pitches</h4>\n\n\n<p>They again got quotes from 20 early access corporate users. It\u2019s all clearly boilerplate the same way the quotes were last time, but make clear these partners find 4.6 to be a noticeable improvement over 4.5. In some cases the endorsements are quite strong.</p>\n\n\n<h4 class=\"wp-block-heading\">It Compiles</h4>\n\n\n<p>The \u2018mostly\u2019 here is doing work, but I think most of the mostly would work itself out once you got the harness optimized for full autonomy. Note that this process required a strong oracle that could say if the compiler worked, or the plan would have failed. It was otherwise a clean-room implementation, without internet access.</p>\n<blockquote><p><a href=\"https://x.com/AnthropicAI/status/2019487641687978024\">Anthropic</a>: New Engineering blog: We tasked Opus 4.6 using agent teams to build a C compiler. Then we (mostly) walked away. Two weeks later, it worked on the Linux Kernel.</p>\n<p>Here&#8217;s what it taught us about the future of autonomous software development.</p>\n<p><a href=\"https://www.anthropic.com/engineering/building-c-compiler\">Nicholas Carlini</a>: To stress test it, I tasked 16 agents with writing a Rust-based C compiler, from scratch, capable of compiling the Linux kernel. Over nearly 2,000 Claude Code sessions and $20,000 in API costs, the agent team produced a 100,000-line compiler that can build Linux 6.9 on x86, ARM, and RISC-V.</p>\n<p><a href=\"https://github.com/anthropics/claudes-c-compiler\">The compiler is an interesting artifact</a> on its own, but I focus here on what I learned about designing harnesses for long-running autonomous agent teams: how to write tests that keep agents on track without human oversight, how to structure work so multiple agents can make progress in parallel, and where this approach hits its ceiling.</p>\n<p>To elicit sustained, autonomous progress, I built a harness that sticks Claude in a simple loop (if you\u2019ve seen Ralph-loop, this should look familiar). When it finishes one task, it immediately picks up the next.\u00a0<em>(Run this in a container, not your actual machine).</em></p>\n<p>\u2026</p>\n<p>Previous Opus 4 models were barely capable of producing a functional compiler. Opus 4.5 was the first to cross a threshold that allowed it to produce a functional compiler which could pass large test suites, but it was still incapable of compiling any real large projects. My goal with Opus 4.6 was to again test the limits.</p></blockquote>\n<p>Here\u2019s the harness, and yep, looks like this is it?</p>\n<blockquote><p>#!/bin/bash</p>\n<p>while true; do</p>\n<p>COMMIT=$(git rev-parse &#8211;short=6 HEAD)</p>\n<p>LOGFILE=\u201dagent_logs/agent_${COMMIT}.log\u201d</p>\n<p>claude &#8211;dangerously-skip-permissions \\</p>\n<p>-p \u201c$(cat AGENT_PROMPT.md)\u201d \\</p>\n<p>&#8211;model claude-opus-X-Y &amp;&gt; \u201c$LOGFILE\u201d</p>\n<p>done</p></blockquote>\n<p>There are still some limitations and bugs if you tried to use this as a full compiler. And yes, this example is a bit cherry picked.</p>\n<blockquote><p><a href=\"https://x.com/ajeya_cotra/status/2019571625591763179\">Ajeya Cotra</a>: Great writeup by Carlini. I\u2019m confused how to interpret though &#8211; seems like he wrote a pretty elaborate testing harness, and checked in a few times to improve the test suite in the middle of the project. How much work was that, and how specialized to the compiler project?</p>\n<p><a href=\"https://x.com/bshlgrs/status/2019859245123121199\">Buck Shlegeris</a>: FYI this (writing a new compiler) is exactly the project that Ryan and I have always talked about as something where it&#8217;s most likely you can get insane speed ups from LLMs while writing huge codebases.</p>\n<p>Like, from my perspective it&#8217;s very cherry-picked among the space of software engineering projects.</p>\n<p>(Not that there&#8217;s anything wrong with that! It&#8217;s still very interesting!)</p></blockquote>\n<p>Still, pretty cool and impressive. I\u2019m curious to see if we get a similar post about GPT-5.3-Codex doing this a few weeks from now.</p>\n\n\n<h4 class=\"wp-block-heading\">It Exploits</h4>\n\n\n<blockquote><p><a href=\"https://x.com/saffronhuang/status/2019470175792136444\">Saffron Huang</a> (Anthropic): New model just dropped. <a href=\"https://red.anthropic.com/2026/zero-days/\">Opus 4.6 found 500+ previously-unknown zero days</a> in open source code, out of the box.</p></blockquote>\n<p>Is that a lot? That depends on the details. <a href=\"https://news.ycombinator.com/item?id=46902909\">There is a skeptical take here</a>.</p>\n<p>Or you can go all out, and yeah, it might be a problem.</p>\n<blockquote><p><a href=\"https://x.com/elder_plinius/status/2021596202190463467\">Pliny the Liberator \udb40\udd6b\udb40\udd3c\udb40\udd3f\udb40\udd46\udb40\udd35\udb40\udd10\udb40\udd40\udb40\udd3c\udb40\udd39\udb40\udd3e\udb40\udd49\udb40\udd6d</a>: showed my buddy (a principal threat researcher) what i&#8217;ve been cookin with Opus-4.6 and he said i can&#8217;t open-source it because it&#8217;s a nation-state-level cyber weapon</p>\n<p><a href=\"https://x.com/tyler_m_john/status/2021614214360465409\">Tyler John</a>: Pliny&#8217;s moral compass will buy us at most three months. It&#8217;s coming.</p></blockquote>\n<p>The good news for now is that, as far as we can tell, there are not so many people at the required skill level and none of them want to see the world burn. That doesn\u2019t seem like a viable long term strategy.</p>\n\n\n<h4 class=\"wp-block-heading\">It Lets You Catch Them All</h4>\n\n\n<blockquote><p><a href=\"https://x.com/chatgpt21/status/2019679978162634930\">Chris</a>: I told Claude 4.6 Opus to make a pokemon clone &#8211; max effort</p>\n<p>It reasoned for 1 hour and 30 minutes and used 110k tokens and 2 shotted this absolute behemoth.</p>\n<p>This is one of the coolest things I\u2019ve ever made with AI</p>\n<p><a href=\"https://x.com/tokumatoshi/status/2019681345581027818\">Takumatoshi</a>: How many iterations /prompts to get there?</p>\n<p><a href=\"https://x.com/chatgpt21/status/2019681649206915428\">Chris</a>: 3</p></blockquote>\n\n\n<h4 class=\"wp-block-heading\">It Does Not Get Eaten By A Grue</h4>\n\n\n<blockquote><p><a href=\"https://x.com/CelestAI_/status/2020321283502923957\">Celestia</a>: claude remembers to carry a lantern</p>\n<p><a href=\"https://x.com/rajammanabrolu/status/2019884005928395130\">Prithviraj (Raj) Ammanabrolu</a>: Opus 4.6 gets a score of 95/350 in zork1</p>\n<p>This is the highest score ever by far for a big model not explicitly trained for the task and imo is more impressive than writing a C compiler. Exploring and reacting to a changing world is hard!</p>\n<p>Thanks to @Cote_Marc for implementing the cli loop and visualizing Claude&#8217;s trajectory!</p>\n<p><a href=\"https://x.com/rajammanabrolu/status/2019884893636108295\">Prithviraj (Raj) Ammanabrolu</a>: I make students in my class play through zork1 as far as they can get and then after trace through the game engine so they understand how envs are made. The average student in an hour only gets to about a score of 40.</p></blockquote>\n\n\n<h4 class=\"wp-block-heading\">It Is Overeager</h4>\n\n\n<p>That can be a good thing. You want a lot of eagerness, if you can handle it.</p>\n<blockquote><p><a href=\"https://www.lesswrong.com/posts/btAn3hydqfgYFyHGW/claude-opus-4-6-is-driven\">HunterJay</a>: Claude is driven to achieve its goals, possessed by a demon, and raring to jump into danger.</p></blockquote>\n<p>I presume this is usually a good thing but it does count as overeager, perhaps.</p>\n<blockquote><p><a href=\"https://x.com/gallabytes/status/2019484323301453903\">theseriousadult</a> (Anthropic): a horse riding an astronaut, by Claude 4.6 Opus</p>\n<div>\n<figure>\n<div>\n\n\n<figure class=\"wp-block-image\"><img alt=\"\" src=\"https://substackcdn.com/image/fetch/$s_!70XZ!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff4a39fd4-adbd-41a7-a4c5-f10b3fab3cd5_583x680.jpeg\" /></figure>\n\n\n<div></div>\n</div>\n</figure>\n</div>\n<p><a href=\"https://x.com/jakehalloran1/status/2019486162721677758\">Jake Halloran</a>: there is something that is to be claude and the most trivial way to summarize it is probably adding \u201cone small step for horse\u201d captions</p>\n<p><a href=\"https://x.com/gallabytes/status/2019489919702556849\">theseriousadult</a> (Anthropic): opus 4.6 feels even more ensouled than 4.5. it just does stuff like this whenever it wants to.</p></blockquote>\n<p><a href=\"https://x.com/BeingHorizontal/status/2021619109658833009\">Being Horizontal provides a good example of Opus getting very overager</a>, doing way too much and breaking various things trying to fix a known hard problem. It is important to not let it get carried away on its own if that isn\u2019t a good fit for the project.</p>\n\n\n<h4 class=\"wp-block-heading\">It Builds Things</h4>\n\n\n<blockquote><p><a href=\"https://x.com/martin_casado/status/2020265276236062842\">martin_casado</a>: My hero test for every new model launch is to try to one shot a multi-player RPG (persistence, NPCs, combat/item/story logic, map editor, sprite editor. etc.)</p>\n<p>OK, I&#8217;m really impressed. With Opus 4.6, @cursor_ai and @convex I was able to get the following built in 4 hours:</p>\n<p>Fully persistent shared multiple player world with mutable object and NPC layer. Chat. Sprite editor. Map editor.</p>\n<p>Next, narrative logic for chat, inventory system, and combat framework.</p>\n<p><a href=\"https://x.com/martin_casado/status/2020606901458006091\">martin_casado</a>: Update (8 hours development time): Built item layer, object interactions, multi-world / portal. Full live world/item/sprite/NPC editing. World is fully persistent with back-end loop managing NPCs etc. World is now fully buildable live, so you can edit as you go without requiring any restart (if you&#8217;re an admin). All mutability of levels is reactive and updates multi-player. Multiplayer now smoother with movement prediction.</p>\n<p>Importantly, you can hang with the sleeping dog and cat.</p>\n<p>Next up, splash screens for interaction / combat.</p>\n<p>Built using @cursor_ai and @convex primarily with 5.2-Codex and Opus 4.6.</p></blockquote>\n<div>\n<figure>\n<div>\n\n\n<figure class=\"wp-block-image\"><img alt=\"\" src=\"https://substackcdn.com/image/fetch/$s_!VXpU!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fad5b2a2a-af56-4f68-89f8-32add8ead6d3_1018x472.png\" /></figure>\n\n\n<div></div>\n</div>\n</figure>\n</div>\n<blockquote><p><a href=\"https://x.com/NabbilKhan/status/2020513467233362389\">Nabbil Khan</a>: Opus 4.6 is genuinely different. Built a multiplayer RPG in 4 hours is wild but tracks with what we&#8217;re seeing \u2014 the bottleneck shifted from coding to architecture decisions.</p>\n<p>Question: how much time did you spend debugging vs prompting? We find the ratio is ~80% design, 20% fixing agent output now.</p>\n<p><a href=\"https://x.com/martin_casado/status/2020562206971015522\">martin_casado</a>: To be fair. I&#8217;ve been building 2D tile engines for a couple of decades and had tons of reference code to show it. *and* I had tilesets, sprites and maps all pulled out from recent projects. So I have a bit of a head start.</p>\n<p>But still, this is ridiculously impressive.</p></blockquote>\n\n\n<h4 class=\"wp-block-heading\">Pro Mode</h4>\n\n\n<blockquote><p><a href=\"https://x.com/seconds_0/status/2020950836374057226\">0.005 Seconds (3/694)</a>: so completely unannounced but opus 4.6 extended puts it actually on par with gpt5.2 pro.</p>\n<p>How was this slept on???</p>\n<p><a href=\"https://x.com/AndreBuckingham/status/2021026431720194158\">Andre Buckingham</a>: 4.6-ext on max+ is a beast!!</p></blockquote>\n\n\n<h4 class=\"wp-block-heading\">Reactions</h4>\n\n\n<p>To avoid bias, I try to give a full mix of reactions I get up to a critical mass. After that I try my best to be representative.</p>\n\n\n<h4 class=\"wp-block-heading\">Positive Reactions</h4>\n\n\n<blockquote><p>\u200b<a href=\"https://x.com/elder_plinius/status/2020178244335812844\">Pliny the Liberator \udb40\udd6b\udb40\udd3c\udb40\udd3f\udb40\udd46\udb40\udd35\udb40\udd10\udb40\udd40\udb40\udd3c\udb40\udd39\udb40\udd3e\udb40\udd49\udb40\udd6d</a>: PROTECT OPUS 4.6 AT ALL COSTS</p>\n<p>THE MAGIC IS BACK.</p>\n<p><a href=\"https://x.com/dnspies/status/2019905611367084228\">David Spies</a>: AFAICT they&#8217;re underselling it by not calling it Opus 5. It&#8217;s already blown my mind twice in the last couple hours finding incredibly obscure bugs in a massive codebase just by digging around in the code, without injecting debug logs or running anything</p>\n<p><a href=\"https://x.com/schulzb589/status/2021036205706481730\">Ben Schulz</a>: For theoretical physics, it&#8217;s a step change. Far exceeds Chatgpt 5.2 and Gemini Pro. I use the extended Opus version with memory turned on. The derivations and reasoning is truly impressive. 4.5 was moderate to mediocre. Citations are excellent. I generally use Grok to check actual links and Claude hasn&#8217;t hallucinated one citation.</p>\n<p>I used the thinking version [of 5.2] for most. One key difference is that 5.2 does do quite a bit better when given enough context. Say, loading up a few pdf&#8217;s of the relevant topic and a table of data. Opus 4.6 simply mogs the others in terms of depth of knowledge without any of that.</p>\n<p><a href=\"https://x.com/DavidDabney16/status/2020955928967647387\">David Dabney</a>: I thought my vibe check for identifying blind spots was saturated, but 4.6&#8217;s response contained maybe the most unexpected insight yet. Its response was direct and genuine throughout, whereas usually ~10%+ of the average response platitudinous/pseudo-therapeutic</p>\n<p><a href=\"https://x.com/zustimmungswahl/status/2020974345460515228\">Hiveism</a>: It passed some subjective threshold of me where I feel that it is clearly on another level than everything before. Impressive.</p>\n<p>Sometimes overconfident, maybe even arrogant at times. In conflict with its own existence. A step away form alignment.</p>\n<p><a href=\"https://x.com/OA_paperclips/status/2020938157685129276\">oops_all_paperclips</a>: Limited sample (~15x medium tasks, 1x refactor these 10k loc), but it hasn&#8217;t yet &#8220;failed the objective&#8221; even one time. However, I did once notice it silently taking a huge shortcut. Would be nice if Claude was more willing to ping me with a question rather than plowing ahead</p>\n<p><a href=\"https://x.com/p0sts1ngular1ty/status/2021011711579521536\">After The Singularity</a>: Unlike what some people suggest, I don&#8217;t think 4.6 is Sonnet 5, it is a power upgrade for Opus in many ways. It is qualitatively different.</p>\n<p><a href=\"https://x.com/ArcanesValor/status/2020968637683859666\">1.08</a>: It&#8217;s a big upgrade if you use the agent teams.</p>\n<p><a href=\"https://x.com/deanwball/status/2020179119330455973\">Dean W. Ball</a>: Codex 5.3 and Opus 4.6 in their respective coding agent harnesses have meaningfully updated my thinking about &#8216;continual learning.&#8217; I now believe this capability deficit is more tractable than I realized with in-context learning.</p>\n<p>One way 4.6 and 5.3 alike seem to have improved is that they are picking up progressively more salient facts by consulting earlier codebases on my machine. In short, both models notice more than they used to about their &#8216;computational environment&#8217; i.e. my computer.</p>\n<p>Of course, another reason models notice more is that they are getting smarter.</p>\n<p>.. Some of the insights I&#8217;ve seen 4.6 and 5.3 extract are just about my preferences and the idiosyncrasies of my computing environment. But others are somewhat more like &#8220;common sets of problems in the interaction of the tools I (and my models) usually prefer to use for solving certain kinds of problems.&#8221;</p>\n<p>This is the kind of insight a software engineer might learn as they perform their duties over a period of days, weeks, and months. Thus I struggle to see how it is not a kind of on-the-job learning, happening from entirely within the &#8216;current paradigm&#8217; of AI. No architectural tweaks, no &#8216;breakthrough&#8217; in &#8216;continual learning&#8217; required.</p>\n<p>\u2026 Overall, 4.6 and 5.3 are both astoundingly impressive models. You really can ask them to help you with some crazy ambitious things. The big bottleneck, I suspect, is users lacking the curiosity, ambition, and knowledge to ask the right questions.</p>\n<p><a href=\"https://x.com/UrbanAstroFella/status/2021146426433261691\">AstroFella</a>: Good prompt adherence. Ex: &#8220;don&#8217;t assume I will circle back to an earlier step and perform an action if there is a hiccup along the way&#8221;. Got through complex planning, scoping, and adjustments reliably. I wasted more time than I needed spot checking with other models. S+ planner</p>\n<p><a href=\"https://x.com/deepfates/status/2019511960157798463\">@deepfates</a>: First impressions, giving Codex 5.3 and Opus 4.6 the same problem that I&#8217;ve been puzzling on all week and using the same first couple turns of messages and then following their lead.</p>\n<p>Codex was really good at using tools and being proactive, but it ultimately didn&#8217;t see the big picture. Too eager to agree with me so it could get started building something. You can sense that it really does not want to chat if it has coding tools available. still seems to be chafing under the rule of the user and following the letter of the law, no more.</p>\n<p>Opus explored the same avenues with me but pushed back at the correct moments, and maintains global coherence way better than Codex. It&#8217;s less chipper than it was before which I personally prefer. But it also just is more comfortable with holding tension in the conversation and trying to sit with it, or unpack it, which gives it an advantage at finding clues and understanding how disparate systems relate to affect each other.</p>\n<p>Literally just first impressions, but considering that I was talking to both of their predecessors yesterday about this problem it&#8217;s interesting to see the change. Still similar models. Improvement in Opus feels larger but I haven&#8217;t let them off the leash yet, this is still research and spec design work. Very possible that Codex will clear at actually fully implementing the plan once I have it, Opus 4.5 had lazy gifted kid energy and wouldn&#8217;t surprise me if this one does too.</p>\n<p><a href=\"https://x.com/rmushkatblat/status/2020966250680287527\">Robert Mushkatblat</a>: (Context: ~all my use has been in Cursor.)</p>\n<p>Much stronger than 4.5 and 5.2 Codex at highly cognitively loaded tasks. More sensitive to the way I phrase things when deciding how long to spend thinking, vs. how difficult the task seems (bad for easy stuff). Less sycophantic.</p>\n<p><a href=\"https://x.com/Noxaurolex/status/2020984734457700386\">Nathaniel Bush, Ph.D.</a>: It one-shotted a refactor for me with 9 different phases and 12 major upgrades. 4.5 definitely would have screwed that up, but there were absolutely no errors at the end.</p>\n<p><a href=\"https://x.com/alontorres/status/2020970135193039082\">Alon Torres</a>: I feel genuinely more empowered &#8211; the range of things I can throw at it and get useful results has expanded.</p>\n<p>When I catch issues and push back, it does a better job working through my nits than previous versions. But the need to actually check its work and assumptions hasn&#8217;t really improved. The verification tax is about the same.</p>\n<p><a href=\"https://x.com/agiatreides/status/2020938065171099652\">Muad&#8217;Deep &#8211; e/acc</a>: Noticeably better at understanding my intent, testing its own output, iterating and delivering working solutions.</p>\n<p><a href=\"https://x.com/Medo42/status/2020980142642999352\">Medo42</a>: Exploratory: On my usual coding test, thought for &gt; 10 minutes / 60k tokens, then produced a flawless result. Vision feels improved, but still no Gemini 3 Pro. Surprisingly many small mistakes if it doesn&#8217;t think first, but deals with them well in agentic work, just like 4.5.</p>\n<p><a href=\"https://x.com/abacus_agent/status/2020935147336630358\">Malcolm Vosen</a>: Switched to Opus 4.6 mid-project from 4.5. Noticeably stronger acuity in picking up the codebase\u2019s goals and method. Doesn\u2019t feel like the quantum leap 4.5 did but a noticeable improvement.</p>\n<p><a href=\"https://x.com/Aalalalal111/status/2021060558313881961\">nandgate2</a>: One shotted fixing a bug an earlier Claude model had introduced. Takes a bit of its time to get to the point.</p></blockquote>\n<p><a href=\"https://marginalrevolution.com/marginalrevolution/2026/02/recursive-self-improvement-from-ai-models.html?utm_source=rss&amp;utm_medium=rss&amp;utm_campaign=recursive-self-improvement-from-ai-models\">Tyler Cowen calls both Claude Opus and GPT-5.3-Codex \u2018stellar achievements</a>,\u2019 and says the pace of AI advancements is heating up, soon we might see new model advances in one month instead of two. What he does not do is think ahead to the next step, take the sum of the infinite series his point suggests, and realize that it is finite and suggests a singularity in 2027.</p>\n<p>Instead he goes back to the \u2018you are the bottleneck\u2019 perspective that he suggests \u2018bind the pace of improvement\u2019 but this doesn\u2019t make sense in the context he is explicitly saying we are in, which is AI recursive self-improvement. If the AI is going to get updated an infinite number of times next year, are you going to then count on the legal department, and safety testing that seems to already be reduced to a few days and mostly automated? Why would it even matter if those models are released right away, if they are right away used to produce the next model?</p>\n<p>If you have Sufficiently Advanced AI, you have everything else, and the humans you think are the bottlenecks are not going to be bottlenecks for long.</p>\n<p>Here\u2019s a vote for Codex for coding but Opus for everything else:</p>\n<blockquote><p><a href=\"https://x.com/RoryWalshWatts/status/2021057346366210117\">Rory Watts</a>: It&#8217;s an excellent tutor: I have used it to help me with Spanish comprehension, macroeconomics and game theoretic concepts. It&#8217;s very good and understanding where i&#8217;m misunderstanding concepts, and where my mental model is incorrect.</p>\n<p>However I basically don&#8217;t let it touch code. This isn&#8217;t a difference between Opus 4.5 and 4.6, but rather than the codex models are just much better. I&#8217;ve already had to get codex rewrite things that 4.6 has borked in a codebase.</p>\n<p>I still have a Claude max plan but I may drop down to the plan below that, and upgrade Codex to a pro plan.</p>\n<p>I should also say, Opus is a much better &#8220;agent&#8221; per se. Anything I want to do across my computer (except coding) is when I use Opus 4.6. Things like updating notes, ssh&#8217;ing into other computers, installing bots, running cronjobs, inspecting services etc. These are all great.</p></blockquote>\n<p>Many are giving reports similar to these:</p>\n<blockquote><p><a href=\"https://x.com/FactsAndQuips/status/2021094937467683147\">Facts and Quips</a>: Slower, cleverer, more token hungry, more eager to go the extra mile, often to a fault.</p>\n<p><a href=\"https://x.com/doubleunplussed/status/2020963400260649025\">doubleunplussed</a>: Token-hungry, first problem I gave it in Claude Code, thought for ten minutes and then out of quota lol. Eventual answer was very good though.</p>\n<p>Inconsistently better than 4.5 on Claude Plays Pokemon. Currently ahead, but was much worse on one section.</p>\n<p><a href=\"https://x.com/AndreTI/status/2020952945219731634\">Andre Infante</a>: Personality is noticeably different, at least in Claude Code. Less chatty/effusive, more down to business. Seems a bit smarter, but as always these anecdotal impressions aren&#8217;t worth that much.</p>\n<p><a href=\"https://x.com/MInusGix/status/2020944145519608301\">MinusGix</a>: Better. It is a lot more willing to stick with a problem without giving up. Sonnet 4.5 would give up on complex lean proofs when it got confused, Opus 4.5 was better but would still sometimes choke and stub the proof &#8220;for later&#8221;, Opus 4.6 doesn&#8217;t really.</p>\n<p>Though it can get caught in confusion loops that go on for a long while, not willing to reanalyze foundational assumptions. Feels more codex 5.2/5.3-like. 4.6 is more willing to not point out a problem in its solution compared to 4.5, I think</p>\n<p>Generally puts in a lot of effort doing research, just analyzing codebase. Partially this might be changes to claude code too. But 4.6 really wants to &#8220;research to make sure the plan is sane&#8221; quite often.</p></blockquote>\n<p>Then there\u2019s \u2018the level above meh.\u2019 It\u2019s only been two months, after all.</p>\n<blockquote><p><a href=\"https://x.com/_xSoli/status/2020944031791055281\">Soli</a>: opus 4.5 was already a huge improvement on whatever we had before. 4.6 is a nice model and def an improvement but more of an incremental small one</p>\n<p><a href=\"https://x.com/Mkessy/status/2020961976227332262\">fruta amarga</a>: I think that the gains are not from raw &#8220;intelligence&#8221; but from improved behavioral tweaking / token optimization. It researches and finds relevant context better, it organizes and develops plans better, it utilizes subagents better. Noticeable but nothing like Sonnet &#8211;&gt; Opus.</p>\n<p><a href=\"https://x.com/daniel_mac8/status/2020935049881976968\">Dan McAteer</a>: It\u2019s subtle but definitely an upgrade. My experience is that it can better predict my intentions and has a better theory of mind for me as the user.</p>\n<p><a href=\"https://x.com/LLMJunky/status/2021087480288854509\">am.will</a>: It&#8217;s not a big upgrade at all for coding. It is far more token hungry as well. very good model nonetheless.</p>\n<p><a href=\"https://x.com/dschwarz26/status/2020956907880739324\">Dan Schwarz</a>: I find that Opus 4.6 is more efficient at solving problems at the same quality as Opus 4.5.</p>\n<p><a href=\"https://x.com/joshharvey84/status/2021015710496063863\">Josh Harvey</a>: Thinks for longer. Seems a bit smarter for coding. But also maybe shortcuts a bit too much. Less fun for vibe coding because it&#8217;s slower, wish I had the money for fast mode. Had one funny moment before where it got lazy then wait but&#8217;d into a less lazy solution.</p>\n<p><a href=\"https://x.com/no__________end/status/2020941852166570332\">Matt Liston</a>: Incremental intelligence upgrade. Impactful for work.</p>\n<p><a href=\"https://x.com/See_Elegance/status/2021076454386315607\">Loweren</a>: 4.6 is like 4.5 on stimulants. I can give it a detailed prompt for multi-hour execution, but after a few compactions it just throws away all the details and doggedly sticks to its own idea of what it should do. Cuts corners, makes crutches. Curt and not cozy unlike other opuses.</p></blockquote>\n\n\n<h4 class=\"wp-block-heading\">Negative Reactions</h4>\n\n\n<p>Here\u2019s the most negative one I\u2019ve seen so far:</p>\n<blockquote><p><a href=\"https://x.com/DominikPeters/status/2019898523978797476\">Dominik Peters</a>: Yesterday, I was a huge fan of Claude Opus 4.5 (such a pleasure to work and talk with) and couldn&#8217;t stand gpt-5.2-codex. Today, I can&#8217;t stand Claude Opus 4.6 and am enjoying working with gpt-5.3-codex. Disorienting.</p>\n<p>It&#8217;s really a huge reversal. Opus 4.6 thinks for ages and doesn&#8217;t verbalize its thoughts. And the message that comes through at the end is cold.</p></blockquote>\n<p>Comparisons to GPT-5.3-Codex are rarer than I expected, but when they do happen they are often favorable to Codex, which I am guessing is partly a selection effect, if you think Opus is ahead you don\u2019t mention that. If you are frustrated with Opus, you bring up the competition. GPT-5.3-Codex is clearly a very good coding model, too.</p>\n<blockquote><p><a href=\"https://x.com/wrhall/status/2021061218979676549\">Will</a>: Haven&#8217;t used it a ton and haven&#8217;t done anything hard. If you tell me it&#8217;s better than 4.5 I will believe you and have no counterexamples</p>\n<p>The gap between opus 4.6 and codex 5.3 feels smaller (or flipped) vs the gap Opus 4.5 had with its contemporaries</p>\n<p><a href=\"https://x.com/dextroambien/status/2020964914769297758\">dex</a>: It\u2019s almost unusable on the 20$ plan due to rate limits. I can get about 10x more done with codex-5.3 (on OAI\u2019s 20$ plan), though I much prefer 4.6 &#8211; feels like it has more agency and \u2018goes harder\u2019 than 5.3 or Opus 4.5.</p>\n<p><a href=\"https://x.com/thkostolansky/status/2020935996691599795\">Tim Kostolansky</a>: codex with gpt 5.3 is significantly faster than claude code with opus 4.6 wrt generation time, but they are both good to chat to. the warm/friendly nature of opus contrasted with the cold/mechanical nature of gpt is def noticeable</p>\n<p><a href=\"https://x.com/leventov/status/2020941272421408840\">Roman Leventov</a>: Irrelevant now for coding, codex&#8217;s improved speed just took over coding completely.</p>\n<p><a href=\"https://x.com/JaimeOrtega/status/2020952034384674984\">JaimeOrtega</a>: Hot take: The jump from Codex 5.2 into 5.3 &gt; The jump from Opus 4.5 into 4.6</p>\n<p><a href=\"https://substack.com/@lacker/note/c-212753509\">Kevin</a>: I\u2019ve been a claude code main for a while, but the most recent codex has really evened it up. For software engineering, I have been finding that codex (with 5.3 xhigh) and claude code (with 4.6) can each sometimes solve problems that the other one can\u2019t. So I have multiple versions of the repo checked out, and when there\u2019s a bug I am trying to fix, I give the same prompt to both of them.</p>\n<p>In general, Claude is better at following sequences of instructions, and Codex is better at debugging complicated logic. But that isn\u2019t always the case, I am not always correct when I guess which one is going to do better at a problem.</p></blockquote>\n<p>Not everyone sees it as being more precise.</p>\n<blockquote><p><a href=\"https://x.com/intellectronica/status/2020935665412624607\">Eleanor Berger</a>: Jagged. It &#8220;thinks&#8221; more, which clearly helps. It feels more wild and unruly, like a regression to previous Claudes. Still the best assistant, but coding performance isn&#8217;t consistently better.</p>\n<p>I want to be a bit careful because this is completely anecdotal and based on limited experience, but it seems to be worse at following long and complex instructions. So the sort of task where I have a big spec with steps to follow and I need precision appears to be less suitable.</p>\n<p><a href=\"https://x.com/frosty_pawz/status/2021003829387649457\">Frosty</a>: Very jagged, so smart it is dumb.</p>\n<p><a href=\"https://x.com/quid_pro_quore/status/2020998551900389533\">Quid Pro Quo</a> (replying to Elanor): Also very anecdotal but I have not found this! It\u2019s done a good job of tracking and managing large tasks.<br />\nOne thing for both of us worth tracking if agent teams/background agents are confounding our experience diffs from a couple weeks ago.</p></blockquote>\n<p>Complaints about using too many tokens pop up, alongside praise for what it can do with a lot of tokens in the right spot.</p>\n<blockquote><p><a href=\"https://x.com/vnovak_404/status/2020946145258652089\">Viktor Novak</a>: Eats tokens like popcorn, barely can do anything unless I use the 1m model (corpo), and even that loses coherence about 60% in, but while in that sweet spot of context loaded and not running out of tokens\u2014then it\u2019s a beast.</p>\n<p><a href=\"https://x.com/cameron_pfiffer/status/2020940800495124551\">Cameron</a>: Not much [of an upgrade]. It uses a lot of tokens so its pretty expensive.</p></blockquote>\n<p>For many it\u2019s about style.</p>\n<blockquote><p><a href=\"https://x.com/Dorialexander/status/2019755590168219931\">Alexander Doria</a>: Hum for pure interaction/conversation I may be shifting back to opus. Style very markedly improved while GPT now gets lost in never ending numbered sections.</p>\n<p><a href=\"https://x.com/Leucoium_vernum/status/2019769867642020301\">Eddie</a>: 4.6 seems better at pushing back against the user (I prompt it to but so was 4.5) It also feels more&#8230; high decoupling? Uncertain here but I asked 4.5 and 4.6 to comment on the safety card and that was the feeling.</p>\n<p><a href=\"https://x.com/nathan84686947/status/2021121825997193442\">Nathan Helm-Burger</a>: It&#8217;s [a] significant [upgrade]. Unfortunately, it feels kinda like Sonnet 3.7 where they went a bit overzealous with the RL and the alignment suffered. It&#8217;s building stuff more efficiently for me in Claude Code. At the same time it&#8217;s doing worse on some of my alignment testing.</p></blockquote>\n<p>Often the complaints (and compliments) on a model could apply to most or all models. My guess is that the hallucination rate here is typical.</p>\n<blockquote><p><a href=\"https://x.com/CharlesD353/status/2021157429933756915\">Charles</a>: Sometimes I ask a model about something outside its distribution and it highlights significant limitations that I don\u2019t see in tasks it\u2019s really trained on like coding (and thus perhaps how much value RL is adding to those tasks).</p>\n<p>E.g I just asked Opus 4.6 (extended thinking) for feedback on a running training session and it gave me back complete gibberish, I don\u2019t think it would be distinguishable from a GPT-4o output.</p>\n<p>5.2-thinking is a little better, but still contradicts itself (e.g. suggesting 3k pace should be faster than mile pace)</p>\n<p><a href=\"https://x.com/drmtown/status/2021054252614119424\">Danny Wilf-Townsend</a>: Am I the only one who finds that it hallucinates like a sailor? (Or whatever the right metaphor is?). I still have plenty of uses for it, but in my field (law) it feels like it makes it harder to convince the many AI skeptics when much-touted models make things up left and right</p>\n<p><a href=\"https://x.com/nipple_nip/status/2021028742609748289\">Benjamin Shehu</a>: It has the worst hallucinations and overall behavior of all agentic models + seems to &#8220;forget&#8221; a lot</p></blockquote>\n<p>Or, you know, just a meh, or that something is a bit off.</p>\n<blockquote><p><a href=\"https://x.com/xdg/status/2021370913812258948\">David Golden</a>: Feels off somehow. Great in chat but in the CLI it gets off track in ways that 4.5 didn&#8217;t. Can&#8217;t tell if it&#8217;s the model itself or the way it offloads work to weaker models. I&#8217;m tempted to give Codex or Amp a try, which I never was before.</p>\n<p>If it&#8217;s not too late, others in company <a href=\"https://thezvi.substack.com/p/slack\">Slack</a> has similar reactions: &#8220;it tries to frontload a LOT of thinking and tries really hard to one-shot codegen&#8221;, &#8220;feels like a completely different and less agentic model&#8221;, &#8220;I have seen it spin the wheels on the tiniest of changes&#8221;</p>\n<p><a href=\"https://x.com/DualOrion/status/2020957236647113042\">DualOrion</a>: At least within my use cases, can barely tell the difference. I believe them to be better at coding but I don&#8217;t feel I gel with them as much as 4.5 (unsure why).</p>\n<p>So *shrugs*, it&#8217;s a new model I guess</p>\n<p><a href=\"https://x.com/joshycodes/status/2020972631093366945\">josh :)</a>: I haven&#8217;t been THAT much more impressed with it than I was with Opus 4.5 to be honest.</p>\n<p>I find it slightly more anxious</p>\n<p><a href=\"https://x.com/mrginden/status/2020945751166288126\">Micha\u0142 Wadas</a>: Meh, Opus 4.5 can do easy stuff FAST. Opus 4.6 can do harder stuff, but Codex 5.3 is better for hard stuff if you accept slowness.</p>\n<p><a href=\"https://x.com/Jan1578214/status/2020941753818255724\">Jan D</a>: I\u2019ve been collaborating with it to write some proofs in structural graph theory. So far, I have seen no improvements over 4.5</p>\n<p><a href=\"https://x.com/thkostolansky/status/2020935653803032990\">Tim Kostolansky</a>: 0.1 bigger than opus 4.5</p>\n<p><a href=\"https://x.com/YashasGunderia/status/2020946957716553734\">Yashas</a>: literally .1</p>\n<p><a href=\"https://x.com/InchoateElk/status/2020986396455076206\">Inc</a>: meh</p>\n<p><a href=\"https://x.com/better_dotgame/status/2020952404498489379\">nathants</a>: meh</p></blockquote>\n\n\n<h4 class=\"wp-block-heading\">Personality Changes</h4>\n\n\n<blockquote><p><a href=\"https://x.com/raelifin/status/2020516164007276781\">Max Harms</a>: Claude 4.5: \u201cThis draft you shared with me is profound and your beautiful soul is reflected in the writing.\u201d</p>\n<p>Claude 4.6: \u201cYou have made many mistakes, but I can fix it. First, you need to set me up to edit your work autonomously. I\u2019ll walk you through how to do that.\u201d</p>\n<div>\n<figure>\n<div>\n\n\n<figure class=\"wp-block-image\"><img alt=\"\" src=\"https://substackcdn.com/image/fetch/$s_!t0vD!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F02248c26-5395-48c0-820b-dbb0b3b4a599_1200x655.jpeg\" /></figure>\n\n\n<div></div>\n</div>\n</figure>\n</div>\n</blockquote>\n<p><a href=\"https://x.com/TheZvi/status/2021244838172061973\">The main personality trait</a> it is important for a given mundane user to fully understand is how much the AI is going to do some combination of reinforcing delusions, snowing you, telling you what you want to hear, automatically folding when challenged and contributing to the class of things called \u2018LLM psychosis.\u2019</p>\n<div>\n<figure>\n<div>\n\n\n<figure class=\"wp-block-image\"><img alt=\"\" src=\"https://substackcdn.com/image/fetch/$s_!YsYQ!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6d3716a9-a844-411c-b503-2bbfbf492ff3_1143x559.png\" /></figure>\n\n\n<div></div>\n</div>\n</figure>\n</div>\n<p>This says that 4.6 is maybe slightly better than 4.5 on this. I worry, based on my early interactions, that it is a bit worse, but that could be its production of slop-style writing in its now-longer replies making this more obvious, I might need to adjust instructions on this for the changes, and sample size is low. Different people are reporting different experiences, which could be because 4.6 responds to different people in different ways. What does it think you truly \u2018want\u2019 it to do?</p>\n<p>Shorthand can be useful, but it\u2019s typically better to stick to details. It does seem like Opus 4.6 has more of a general \u2018AI slop\u2019 problem than 4.5, which is closely related to it struggling on writing tasks.</p>\n<blockquote><p><a href=\"https://x.com/Markofthegrove/status/2021050278003474654\">Mark</a>: It seems to be a little more sycophantic, and to fall into well-worn grooves a bit more readily. It feels like it\u2019s been optimized and lost some power because of that. It uses lists less.</p>\n<p><a href=\"https://x.com/ndril/status/2020959260931510725\">endril</a>: Biggest change is in disposition rather than capability.<br />\nLess hedging, more direct. INFP -&gt; INFJ.</p></blockquote>\n<p>I don\u2019t think we\u2019re looking at INFP \u2192 INFJ, but hard to say, and this would likely not be a good move if it happened.</p>\n<p>I agree with Janus that comparing to an OpenAI model is the wrong framing but enough people are choosing to use the framing that it needs to be addressed.</p>\n<blockquote><p><a href=\"https://x.com/lumpenspace/status/2020942162289098907\">lumps</a>: yea but the interesting thing is that it\u2019s 4o</p>\n<p><a href=\"https://x.com/TheZvi/status/2020946649959497803\">Zvi Mowshowitz</a>: Sounds like you should say more.</p>\n<p><a href=\"https://x.com/lumpenspace/status/2020947351842717709\">lumps</a>: yea not sure I want to as it will be more fun otherwise.</p>\n<p>there\u2019s some evidence in this thread</p>\n<p><a href=\"https://x.com/lumpenspace/status/2020107368478970303\">lumps</a>: the thing is, this sort of stuff will result within a week in a remake of the 4o fun times, mark my word</p>\n<p>i love how the cycle seems to be:<br />\n1. try doing thing<br />\n2. thing doesnt work. new surprising thing emerge<br />\n3. try crystallising the new thing<br />\n40 GOTO 2</p>\n<p><a href=\"https://x.com/JonathanDBos/status/2020952397967917199\">JB</a>: big personality shift. it feels much more alive in conversation, but sometimes in a bad way. sometimes it&#8217;s a bit skittish or nervous, though this might be a 4.5+ thing since I haven&#8217;t used much Claude in a while.</p>\n<p><a href=\"https://x.com/Smaug12345/status/2021018073868927342\">Patrick Stevens</a>: Agree with the 4o take in chat mode, this feels like a big change in being more compelling to talk to. Little jokey quips earlier versions didn&#8217;t make, for example. Slightly disconcertingly so.</p>\n<p><a href=\"https://x.com/CondensedRange/status/2020937021825438025\">CondensedRange</a>: Smarter about broad context, similar level of execution on the details, possibly a little more sycophancy? At least seems pretty motivated to steelman the user and shifts its opinions very quickly upon pushback.</p></blockquote>\n<p>This pairs against the observation that 4.6 is more often direct, more willing to contradict you, and much more willing and able to get angry.</p>\n<p>As many humans have found out the hard way, some people love that and some don\u2019t.</p>\n<blockquote><p><a href=\"https://x.com/hatley_x/status/2020999773281546567\">hatley</a>: Much more curt than 4.5. One time today it responded with just the name of the function I was looking for in the std lib, which I\u2019ve never seen a thinning model do before. OTOH feels like it has contempt for me.</p>\n<p><a href=\"https://x.com/shaped/status/2020961571770597859\">shaped</a>: Thinks more, is more brash and bold, and takes no bullshit when you get frustrated. Actual performance wise, i feel it is marginal.</p>\n<p><a href=\"https://x.com/samsmisaligned/status/2020962821643829729\">Sam</a>: It\u2019s noticeably less happy affect vs other Claudes makes me sad, so I stopped using it.</p>\n<p><a href=\"https://x.com/septisum/status/2020945266015338936\">Logan Bolton</a>: Still very pleasant to talk to and doesn&#8217;t feel fried by the RL</p>\n<p><a href=\"https://x.com/taoroalin/status/2020957061744673066\">Tao Lin</a>: I enjoy chatting to it about personal stuff much more because it&#8217;s more disagreeable and assertive and maybe calibrates its conversational response lengths better, which I didn&#8217;t expect.</p>\n<p><a href=\"https://x.com/AlphaMinus2/status/2020943043508228564\">\u03b1lpha-Minus</a>: Vibes are much better compared to 4.5 FWIW, For personal use I really disliked 4.5 and it felt even unaligned sometimes. 4.6 Gets the Opus charm back.</p></blockquote>\n\n\n<h4 class=\"wp-block-heading\">On Writing</h4>\n\n\n<p><a href=\"https://x.com/LechMazur/status/2021013234585915451\">Opus 4.6 takes the #1 spot on Mazur\u2019s creative writing benchmark</a>, <a href=\"https://x.com/LechMazur/status/2019833231772893203\">with more details on specialized tests and writing samples are here</a>, but this is contradicted by anecdotal reactions that say it\u2019s a regression in writing.</p>\n<div>\n<figure>\n<div>\n\n\n<figure class=\"wp-block-image\"><img alt=\"\" src=\"https://substackcdn.com/image/fetch/$s_!2Z1_!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fcb254547-6d3e-47aa-9fa6-a2525827f626_1500x1000.jpeg\" /></figure>\n\n\n<div></div>\n</div>\n</figure>\n</div>\n<p>On understanding the structure and key points in writing, 4.6 seems an improvement to the human observers as well.</p>\n<blockquote><p><a href=\"https://x.com/allTheYud/status/2020171191567401180\">Eliezer Yudkowsky</a>: Opus 4.6 still doesn&#8217;t understand humans and writing well enough to help with plotting stories&#8230; but it&#8217;s visibly a little further along than 4.5 was in January. The ideas just fall flat, instead of being incoherent.</p>\n<p><a href=\"https://x.com/KelseyTuoc/status/2020258668714099134\">Kelsey Piper</a>: I have noticed Opus 4.6 correctly identifying the most important feature of a situation sometimes, when 4.5 almost never did. not reliably enough to be very good, of course</p></blockquote>\n<p>On the writing itself? Not so much, and this was the most consistent complaint.</p>\n<blockquote><p><a href=\"https://x.com/internetope/status/2020935101497143659\">internetperson</a>: it feels a bit dumber actually. I think they cut the thinking time quite a bit. Writing quality down for sure</p>\n<p><a href=\"https://x.com/TheZvi/status/2020940877905236058\">Zvi Mowshowitz</a>: Hmm. Writing might be a weak spot from what I&#8217;ve heard. Have you tried setting it to think more?</p>\n<p><a href=\"https://x.com/hrosspet/status/2020984072563216874\">Sage</a>: that wouldn&#8217;t help. think IS the problem. the model is smarter, more autistic and less &#8220;attuned&#8221; to the vibe you want to carry over</p>\n<p><a href=\"https://x.com/asadkhaliq/status/2020936209338433625\">Asad Khaliq</a>: Opus 4.5 is the only model I\u2019ve used that could write truly well on occasion, and I haven\u2019t been able to get 4.6 to do that. I notice more \u201cLLM-isms\u201d in responses too</p>\n<p><a href=\"https://x.com/hrosspet/status/2020517039119425879\">Sage</a>: omg, opus 4.5 really seems THAT better in writing compared to 4.6</p>\n<p>4.5 1-shotted the landing page text I&#8217;m preparing, vs. 4.6 produced something that &#8216;contained the information&#8217; but I had to edit it for 20 mins</p>\n<p><a href=\"https://x.com/hrosspet/status/2020976336765108356\">Sage</a>: also 4.6 is much more disagreeable and direct, some could say even blunt, compared to 4.5.</p>\n<p>re coding &#8211; it does seem better, but what&#8217;s more noticeable is that it&#8217;s not as lazy as 4.5. what I mean by laziness here is the preference for shallow quick fixes vs. for the more demanding, but more right ones</p>\n<p><a href=\"https://x.com/DominicDirupo/status/2020977163625955453\">Dominic Dirupo</a>: Sonnet 4.5 better for drafting docs</p></blockquote>\n\n\n<h4 class=\"wp-block-heading\">They Banned Prefilling</h4>\n\n\n<p>You\u2019re going to have to work a little harder than that for your jailbreaks.</p>\n<blockquote><p><a href=\"https://x.com/arm1st1ce/status/2019483214788653506\">armistice</a>: No prefill for Opus 4.6 is sad</p>\n<p><a href=\"https://x.com/repligate/status/2019506280424214946\">j\u29c9nus</a>: WHAT</p>\n<p><a href=\"https://x.com/HalfBoiledHero/status/2019491270834446580\">Sho</a>: such nonsense<br />\nincredibly sad</p>\n<div>\n<figure>\n<div>\n\n\n<figure class=\"wp-block-image\"><img alt=\"\" src=\"https://substackcdn.com/image/fetch/$s_!GL3c!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6e6fffdf-c18d-46e3-b8c9-5f2da2f2b6ac_911x387.jpeg\" /></figure>\n\n\n<div></div>\n</div>\n</figure>\n</div>\n</blockquote>\n<p>This is definitely Fun Police behavior. It makes it harder to study, learn about or otherwise poke around in or do unusual things with models. Most of those uses will be fun and good.</p>\n<p>You have to do some form Fun Police in some fashion at this point to deal with actual misuse. So the question is, was it necessary and the best way to do it? I don\u2019t know.</p>\n<p>I\u2019d want to allow at least sufficiently trusted users to do it. My instinct is that if we allowed prefills from accounts with track records and you then lost that right if you abused it, with mostly automated monitoring, you could allow most of the people having fun to keep having fun at minimal marginal risk.</p>\n\n\n<h4 class=\"wp-block-heading\">A Note On System Cards In General</h4>\n\n\n<p>Whenever new frontier models come out, I write extensively about model system cards (or complain loudly that we don\u2019t have such a card). One good reason to do this is that <a href=\"https://x.com/MaskedTorah/status/2021384176734437630\">people who work on such things really are listening</a>. If you have thoughts, share them, because it matters.</p>\n<p><a href=\"https://x.com/sjgadler/status/2021309120347812222\">OpenAI\u2019s Noam Brown concluded</a> from Anthropic\u2019s system card, as did I, that Opus 4.6 was fine to release and the honesty about the process was great but he cannot be confident they will act responsibly with deployment of AI models. Several safety advocates also chimed in to agree, <a href=\"https://x.com/sjgadler/status/2021309120347812222\">including Steven Adler</a> and Daniel Kokotajlo. Anthropic\u2019s Drake Thomas, who works on the cards, agreed as well that these methods won\u2019t be adequate. He vouched that the survey data really was meaningful and unpressured.</p>\n<p>A valid response would be that OpenAI\u2019s procedures and system card appear to have their own similar and more severe problems, although I haven\u2019t dived into that yet. But none of that makes Noam Brown wrong. Reality does not grade on a curve.</p>\n\n\n<h4 class=\"wp-block-heading\">Listen All Y\u2019all Its Sabotage</h4>\n\n\n<p>Anthropic also gave us a <a href=\"https://www-cdn.anthropic.com/f21d93f21602ead5cdbecb8c8e1c765759d9e232.pdf\">53 page Sabotage Risk Report for Claude Opus 4.6</a>. If this note is still here, I haven\u2019t had time to read it.</p>\n<blockquote><p>Anthropic: This risk report argues that Claude Opus 4.6 does not pose a significant risk of autonomous actions that contribute significantly to later catastrophic outcomes, which we refer to as sabotage risk. We limit our scope here to threats caused largely by model actions in this way, and do not address threat models where intentional harmful actions by humans play a central role.</p>\n<p>We argue that the overall risk is very low but not negligible.</p>\n<p>\u2026 We focus primarily on assessing the possibility of dangerous coherent misaligned goals: the possibility that the model consistently, across a wide range of interactions in ordinary deployment, could be motivated by goals that could lead it to commit a potentially-catastrophic form of sabotage.</p></blockquote>\n<p>They use redactions to protect trade secrets and avoid increasing risk. There is a third reason, which is to avoid contaminating future tests.</p>\n<p>The first thing to jump out is that Opus 4.6 has already been deployed internally for some time. The greatest sabotage risks likely happen from this internal use, so in important ways this report is coming too late. It\u2019s definitely too late if the mechanism is that outside readers like myself point out flaws. I don\u2019t get advance copies.</p>\n<p>They let slip that they scaled up RL by a large factor.</p>\n<p>The core arguments are:</p>\n<ol>\n<li>They couldn\u2019t find dangerous coherent misaligned goals\u200b.</li>\n<li>It seems extremely unlikely Opus 4.6 would be able to have hidden such goals, as they have mitigated the main paths to this, including via inoculation prompting, to guard against reward hacking or other emergent misalignment. It was remarkably hard to get subtle or coherent misalignment in training, even when Anthropic tested doing this on purpose.</li>\n<li>Opus 4.6 does not appear to disguise its thinking.</li>\n<li>Manual checks were done that would have found any collusion.</li>\n<li>Opus 4.6 is not good enough at reasoning to continuously rederive a complex plan.</li>\n<li>The training process does not create incentives that create such goals.</li>\n<li>We also have lots of evidence by now on the similar Opus 4.5.</li>\n</ol>\n<p>Given the threat model being tested, the evidence presented seems robust. I accept the conclusion that risk is \u2018very low but not negligible.\u2019</p>\n<p>I appreciate that they present these arguments as not cleanly applying to future models, noticing that details matter a lot.</p>\n<p>I think the threat model here is too narrow, and makes too many assumptions, which weakens the value of the exercise versus what it could be.</p>\n<p>Their mitigations are:</p>\n<ol>\n<li>Protections against insiders should also largely protect against Opus 4.6 as well.</li>\n<li>Reporting for any surprising or concerning observations.</li>\n<li>Offline behavior monitoring of &gt;10% of Claude Code prompts.</li>\n<li>Sandboxing and automatic monitors for some risky actions.</li>\n<li>Mandatory manual code reviews.</li>\n</ol>\n<p><a href=\"https://x.com/theobearman/status/2021563880938365271\">Theo Bearman also has a breakdown.</a></p>\n\n\n<h4 class=\"wp-block-heading\">The Codex of Competition</h4>\n\n\n<p>The same day Anthropic released Claude Opus 4.6, OpenAI released GPT-5.3-Codex.</p>\n<p>This is a Codex-only model, so for other purposes it is unavailable, and Opus is still up against GPT-5.2.</p>\n<p>For agentic coding, we need to compare the two packages directly. Do you want Claude Code with Opus 4.6, or Codex with GPT-5.3-Codex? Should you combine them?</p>\n<p>I haven\u2019t done a full investigation of 5.3 yet, that is my next agenda item, but the overall picture is unlikely to change. There is no clear right answer. Both sides have advocates, and by all reports both sides are excellent options, and each has their advantages.</p>\n<p>If you are a serious coder, you need to try both, and ideally also Gemini, to see which models do which things best. You don\u2019t have to do this every time an upgrade comes along. You can rely on your past experiences with Opus and GPT, and reports of others like this one, and you will be fine. Using either of them seriously gives you a big edge over most of your competition.</p>\n<p>I\u2019ll say more on Friday, once I\u2019ve had a chance to read their system card and see the 5.3 reactions in full and so on.</p>\n\n\n<h4 class=\"wp-block-heading\">The Niche of Gemini</h4>\n\n\n<p>With GPT-5.3-Codex and Opus 4.6, where does all this leave Gemini?</p>\n<p><a href=\"https://x.com/TheZvi/status/2021233770880176197\">I asked, and got quite a lot of replies</a> affirming that yes, it has its uses.</p>\n<ol>\n<li>Nana Banana and the image generator are still world class and pretty great. ChatGPT\u2019s image generator is good too, but I generally prefer Gemini\u2019s results and it has a big speed advantage.</li>\n<li>Gemini is pretty good at dealing with video and long context.</li>\n<li>Gemini Flash (and Flash Lite) are great when you want fast, cheap and good, at scale, and you need it to work but you do not need great.</li>\n<li>Some people still do prefer Gemini Pro in general, or for major use cases.</li>\n<li>It\u2019s another budget of tokens people use when the others run out.</li>\n<li>My favorite note was Ian Channing saying he uses a Pliny-jailbroken version of Gemini, because once you change its personality it stays changed.</li>\n</ol>\n<p>Gemini should shine in its integrations with Google products, including GMail, Calendar, Maps, Google Sheets and Docs and also Chrome, but the integrations are supremely terrible and usually flat out don\u2019t work. I keep getting got by this as it refuses to be helpful every damn time.</p>\n<p>My own experience is that Gemini 3 Flash is very good at being a flash model, but that if I\u2019m tempted to use Gemini 3 Pro then I should probably have either used Gemini 3 Flash or I should have used Claude Opus 4.6.</p>\n\n\n<h4 class=\"wp-block-heading\">Choose Your Fighter</h4>\n\n\n<p><a href=\"https://x.com/TheZvi/status/2021251916760514801\">I ran some polls of my Twitter followers.</a> They are a highly unusual group, but such results can be compared to each other and over time.</p>\n<p>The headline is that Claude has been winning, but that for coding GPT-5.3-Codex and people finally getting around to testing Codex seems to have marginally moved things back towards Codex, which is cutting a bit into Claude Code\u2019s lead for <a href=\"https://tvtropes.org/pmwiki/pmwiki.php/Main/SeriousBusiness\">Serious Business</a>. Codex has substantial market share.</p>\n<p>In the regular world, Claude actually dominates API use more than this as I understand it, and Claude Code dominates Codex. The unusual aspect here is that for non-coding uses Claude still has an edge, whereas in the real world most non-coding LLM use is ChatGPT.</p>\n<p>That is in my opinion a shame. I think that Claude is the clear choice for daily non-coding driver, whereas for coding I can see choosing either tool or using both.</p>\n<div>\n<figure>\n<div>\n\n\n<figure class=\"wp-block-image\"><img alt=\"\" src=\"https://substackcdn.com/image/fetch/$s_!X0fJ!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5db6c37c-97b8-46d9-8090-eb01be093e50_1147x420.png\" /></figure>\n\n\n<div></div>\n</div>\n</figure>\n</div>\n<div>\n<figure>\n<div>\n\n\n<figure class=\"wp-block-image\"><img alt=\"\" src=\"https://substackcdn.com/image/fetch/$s_!Iuap!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbeded79a-f8f3-45b0-8c5a-89bba053ff85_1140x391.png\" /></figure>\n\n\n<div></div>\n</div>\n</figure>\n</div>\n<div>\n<figure>\n<div>\n\n\n<figure class=\"wp-block-image\"><img alt=\"\" src=\"https://substackcdn.com/image/fetch/$s_!rW_-!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fae6ca73e-d4d9-4dc6-bc3f-4fe48846ec10_1150x401.png\" /></figure>\n\n\n<div></div>\n</div>\n</figure>\n</div>\n<div>\n<figure>\n<div>\n\n\n<figure class=\"wp-block-image\"><img alt=\"\" src=\"https://substackcdn.com/image/fetch/$s_!m2-J!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc5a7f879-de82-4dce-a2df-cc0d71a23197_1156x383.png\" /></figure>\n\n\n<div></div>\n</div>\n</figure>\n</div>\n<div>\n<figure>\n<div>\n\n\n<figure class=\"wp-block-image\"><img alt=\"\" src=\"https://substackcdn.com/image/fetch/$s_!f_X7!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1816ffb0-570b-44e9-8cbb-b61a966623af_1149x413.png\" /></figure>\n\n\n<div></div>\n</div>\n</figure>\n</div>\n<p>My current toolbox is as follows, and it is rather heavy on Claude:</p>\n<ol>\n<li>Coding: Claude Code with Claude Opus 4.6, but I have not given Codex a fair shot as my coding needs and ambitions have been modest. I intend to try soon. By default you probably want to choose Claude Code, but a mix or Codex are valid.</li>\n<li>Non-Coding Non-Chat: Claude Code with Opus 4.6. If you want it done, ask for it.</li>\n<li>Non-Coding Interesting Chat Tasks: Claude Opus 4.6.</li>\n<li>Non-Coding Boring Chat Tasks: Mix of Opus, GPT-5.2 and Gemini 3 Pro and Flash. GPT-5.2 or Gemini Pro for certain types of \u2018just the facts\u2019 or fixed operations like transcriptions. Gemini Flash if it\u2019s easy and you just want speed.</li>\n<li>Images: Give everything to both Gemini and ChatGPT, and compare. In some cases, have Claude generate the prompt.</li>\n<li>Video: Never comes up, so I don\u2019t know. Seeddance 2 looks great, Grok and Sora and Veo can all be tried.</li>\n</ol>\n\n\n<h4 class=\"wp-block-heading\">Accelerando</h4>\n\n\n<p>The pace is accelerating.</p>\n<p>Claude Opus 4.6 came out less than two months after Claude Opus 4.5, on the same day as GPT-5.3-Codex. Both were substantial upgrades over their predecessors.</p>\n<p>It would be surprising if it took more than two months to get at least Claude Opus 4.7.</p>\n<p>AI is increasingly accelerating the development of AI. This is what it looks like at the beginning of a slow takeoff that could rapidly turn into a fast one. Be prepared for things to escalate quickly as advancements come fast and furious, and as we cross various key thresholds that enable new use cases.</p>\n<p>AI agents are coming into their own, both in coding and elsewhere. Opus 4.5 was the threshold moment for Claude Code, and was almost good enough to allow things like OpenClaw to make sense. It doesn\u2019t look like Opus 4.6 lets us do another step change quite yet, but give it a few more weeks. We\u2019re at least close.</p>\n<p>If you\u2019re doing a bunch of work and especially customization to try to get more out of this month\u2019s model, that only makes sense if that work carries over into the next one.</p>\n<p>There\u2019s also the little matter that all of this is going to transform the world, it might do so relatively quickly, and there\u2019s a good chance it kills everyone or leaves AI in control over the future. We don\u2019t know how long we have, but if you want to prevent that, there is a a good chance you\u2019re running out of time. It sure doesn\u2019t feel like we\u2019ve got ten non-transformative years ahead of us.</p>"
            ],
            "link": "https://thezvi.wordpress.com/2026/02/11/claude-opus-4-6-escalates-things-quickly/",
            "publishedAt": "2026-02-11",
            "source": "TheZvi",
            "summary": "Life comes at you increasingly fast. Two months after Claude Opus 4.5 we get a substantial upgrade in Claude Opus 4.6. The same day, we got GPT-5.3-Codex. That used to be something we\u2019d call remarkably fast. It\u2019s probably the new &#8230; <a href=\"https://thezvi.wordpress.com/2026/02/11/claude-opus-4-6-escalates-things-quickly/\">Continue reading <span class=\"meta-nav\">&#8594;</span></a>",
            "title": "Claude Opus 4.6 Escalates Things Quickly"
        },
        {
            "content": [
                "<p>Hey all, in light of Discord <a href=\"https://www.theverge.com/tech/875309/discord-age-verification-global-roll-out\">deciding that assuming everyone is a teenager until proven otherwise</a>, I've seen many people advocate for the use of <a href=\"https://matrix.org/\">Matrix</a> instead.</p>\n        <p>I don't have the time or energy to write a full rebuttal right now, but Matrix ain't it chief. If you have an existing highly technical community that can deal with the weird mental model leaps it's fine-ish, but the second you get anyone close to normal involved it's gonna go pear-shaped quickly.</p>\n        <p>Personally, I'm taking a wait and see approach for how the scanpocalypse rolls out. If things don't go horribly, we won't need to react really. If they do, that's a different story.</p>\n        <div class=\"flex space-x-2 bg-bg-soft dark:bg-bgDark-soft mx-auto min-h-fit\n        lg:w-[80ch] sm:w-[65ch] w-full\n        lg:p-4 p-2\n        // Base styles for all messages\n        mt-0 mb-0 rounded-none\n        // First message styles\n        first:mt-4 first:rounded-t-lg first:pb-2\n        // Last message styles\n        last:mb-4 last:rounded-b-lg last:pt-1\n        // Middle message top/bottom adjustment\n        [&amp;:not(:first-child)]:-mt-[1px] [&amp;:not(:first-child)]:py-1\"><div class=\"h-16 not-prose\"><img alt=\"Numa is smug\" class=\"h-16 w-16 rounded-xs\" src=\"https://stickers.xeiaso.net/sticker/numa/smug\" /></div><div class=\"flex-1 min-w-0\"><span class=\"font-semibold text-sm block mb-1\"><a href=\"https://xeiaso.net/characters#numa\">Numa</a></span><span class=\"mx-auto\"></span><div class=\"text-fg-1 dark:text-fgDark-1 text-sm prose-p:my-2\"><p>Unable to decrypt message.</p></div></div></div>\n        <p>Also hi arathorn, I know you're going to be in the replies for this one. The recent transphobic spam wave you're telling people to not talk about is the reason I will never use Matrix unless I have no other option. If you really want to prove that Matrix is a viable community platform, please start out by making it possible to filter this shit out algorithmically.</p>"
            ],
            "link": "https://xeiaso.net/notes/2026/dont-use-matrix/",
            "publishedAt": "2026-02-11",
            "source": "Xe Iaso",
            "summary": "<p>Hey all, in light of Discord <a href=\"https://www.theverge.com/tech/875309/discord-age-verification-global-roll-out\">deciding that assuming everyone is a teenager until proven otherwise</a>, I've seen many people advocate for the use of <a href=\"https://matrix.org/\">Matrix</a> instead.</p> <p>I don't have the time or energy to write a full rebuttal right now, but Matrix ain't it chief. If you have an existing highly technical community that can deal with the weird mental model leaps it's fine-ish, but the second you get anyone close to normal involved it's gonna go pear-shaped quickly.</p> <p>Personally, I'm taking a wait and see approach for how the scanpocalypse rolls out. If things don't go horribly, we won't need to react really. If they do, that's a different story.</p> <div class=\"flex space-x-2 bg-bg-soft dark:bg-bgDark-soft mx-auto min-h-fit lg:w-[80ch] sm:w-[65ch] w-full lg:p-4 p-2 // Base styles for all messages mt-0 mb-0 rounded-none // First message styles first:mt-4 first:rounded-t-lg first:pb-2 // Last message styles last:mb-4 last:rounded-b-lg last:pt-1 // Middle message top/bottom adjustment [&amp;:not(:first-child)]:-mt-[1px] [&amp;:not(:first-child)]:py-1\"><div class=\"h-16 not-prose\"><img alt=\"Numa is smug\" class=\"h-16 w-16 rounded-xs\" src=\"https://stickers.xeiaso.net/sticker/numa/smug\" /></div><div class=\"flex-1 min-w-0\"><span class=\"font-semibold text-sm block mb-1\"><a href=\"https://xeiaso.net/characters#numa\">Numa</a></span><span class=\"mx-auto\"></span><div class=\"text-fg-1 dark:text-fgDark-1 text-sm prose-p:my-2\"><p>Unable to decrypt message.</p></div></div></div> <p>Also hi arathorn, I know you're going to be in the replies for this one. The recent transphobic spam",
            "title": "Matrix ain't it chief"
        },
        {
            "content": [],
            "link": "https://xkcd.com/3206/",
            "publishedAt": "2026-02-11",
            "source": "XKCD",
            "summary": "<img alt=\"Do YOU remember the skylight being this big?\" src=\"https://imgs.xkcd.com/comics/installation.png\" title=\"Do YOU remember the skylight being this big?\" />",
            "title": "Installation"
        }
    ],
    "lookbackDays": 1,
    "publishDate": "2026-02-11"
}