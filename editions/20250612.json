{
    "articles": [
        {
            "content": [
                "<p>Say you\u2019re <a href=\"https://en.wikipedia.org/wiki/Robyn_Denholm\">Robyn Denholm</a>, chair of Tesla\u2019s board. And say you\u2019re thinking about firing Elon Musk. One way to make up your mind would be to have people bet on Tesla\u2019s stock price six months from now in a market where all bets get cancelled <a href=\"https://manifold.markets/benmanns/tsla-close-price-january-16-2026-if-pEUPRCC5qy\">unless Musk is fired</a>. Also, run a second market where bets are cancelled unless <a href=\"https://manifold.markets/benmanns/tsla-close-price-january-16-2026-if\">Musk stays CEO</a>. If people bet on higher stock prices in Musk-fired world, maybe you should fire him.</p>\n\n<p>That\u2019s basically <a href=\"https://en.wikipedia.org/wiki/Futarchy\">Futarchy</a>: Use conditional prediction markets to make decisions.</p>\n\n<p>People often argue about fancy aspects of Futarchy. Are stock prices all you care about? Could Musk use his wealth to bias the market? What if Denholm makes different bets in the two markets, and then fires Musk (or not) to make sure she wins? Are human values and beliefs somehow inseparable?</p>\n\n<p>My objection is more basic: It doesn\u2019t work. You can\u2019t use conditional predictions markets to make decisions like this, because conditional prediction markets reveal <em>probabilistic</em> relationships, not <em>causal</em> relationships. The whole concept is faulty.</p>\n\n<p>There <em>are</em> solutions\u2014ways to force markets to give you causal relationships. But those solutions are <em>painful</em> and I get the shakes when I see everyone acting like you can use prediction markets to conjure causal relationships from thin air, almost for free.</p>\n\n<p>I wrote about this <a href=\"https://dynomight.net/prediction-market-causation/\">back in 2022</a>, but my argument was kind of sprawling and it seems to have failed to convince approximately everyone. So thought I\u2019d give it another try, with more aggression. [Edit: See also <a href=\"https://www.lesswrong.com/posts/xnC68ZfTkPyzXQS8p/prediction-markets-are-confounded-implications-for-the\">this post</a> from Anders_H in 2015, which is the earliest ]</p>\n\n<p><img alt=\"\" src=\"https://dynomight.net/img/futarky/telluride3.jpg\" /></p>\n\n<h2 id=\"conditional-prediction-markets-are-a-thing\">Conditional prediction markets are a thing</h2>\n\n<p>In prediction markets, people trade contracts that pay out if some event happens. There might be a market for \u201cDynomight comes out against aspartame by 2027\u201d contracts that pay out $1 if that happens and $0 if it doesn\u2019t. People often worry about things like market manipulation, liquidity, or herding. Those worries are fair but boring, so let\u2019s ignore them. If a market settles at $0.04, let\u2019s assume that means the \u201ctrue probability\u201d of the event is 4%.</p>\n\n<p>(I pause here in recognition of those who need to yell about Borel spaces or von Mises axioms or Dutch book theorems or whatever. Get it all out. I value you.)</p>\n\n<p>Right. <em>Conditional</em> prediction markets are the same, except they get cancelled unless some other event happens. For example, the \u201cDynomight comes out against aspartame by 2027\u201d market might be conditional on \u201cDynomight de-pseudonymizes\u201d. If you buy a contract for $0.12 then:</p>\n\n<ul>\n  <li>If Dynomight is still pseudonymous at the end of 2027, you\u2019ll get your $0.12 back.</li>\n  <li>If Dynomight is non-pseudonymous, then you get $1 if Dynomight came out against aspartame and $0 if not.</li>\n</ul>\n\n<p>Let\u2019s again assume that if a conditional prediction market settles at $0.12, that means the \u201ctrue\u201d conditional probability is 12%.</p>\n\n<h2 id=\"a-non-causal-kind-of-thing\">A non-causal kind of thing</h2>\n\n<p>But hold on. If we assume that conditional prediction markets give flawless conditional probabilities, then what\u2019s left to complain about?</p>\n\n<p>Simple. <strong>Conditional probabilities are the wrong thing.</strong> If P(A|B)=0.9, that means that if you <em>observe</em> B, then there\u2019s a 90% chance of A. That doesn\u2019t mean anything about the chances of A if you <em>do</em> B.</p>\n\n<p>In the context of statistics, everyone knows that <a href=\"https://en.wikipedia.org/wiki/Correlation_does_not_imply_causation\">correlation does not imply causation</a>. That\u2019s a basic law of science. But really, it\u2019s just another way of saying that <em>conditional probabilities are not what you need to make decisions</em>. And that\u2019s true no matter where the conditional probabilities come from.</p>\n\n<p><a href=\"https://dynomight.net/img/futarky/comic1big.jpg\"><img alt=\"\" src=\"https://dynomight.net/img/futarky/comic1.jpg\" /></a></p>\n\n<p>For example, people with high vitamin D levels are only <a href=\"https://doi.org/10.1001/archinte.168.15.1629\">~56%</a> as likely to die in a given year as people with low vitamin D levels. Does that mean taking vitamin D halves your risk of death? No, because those people are also thinner, richer, less likely to be diabetic, less likely to smoke, more likely to exercise, etc. To make sure we\u2019re seeing the effects of vitamin D itself, we run randomized trials. Those suggest it <a href=\"https://doi.org/10.1001/archinte.167.16.1730\"><em>might</em></a> reduce the risk of death a little. (I take it.)</p>\n\n<p>Futarchy has the same flaw. Even if you think vitamin D does nothing, if there\u2019s a prediction market for if some random person dies, you should pay much less if the market is conditioned on them having high vitamin D. But you should do that mostly because they\u2019re more likely to be rich and thin and healthy, not because of vitamin D itself.</p>\n\n<p>If you like math, conditional prediction markets give you P(A|B). But P(A|B) doesn\u2019t tell you what will happen if you <em>do</em> B. That\u2019s a completely different number with a <a href=\"https://plato.stanford.edu/entries/causal-models/do-calculus.html\">different notation</a>, namely P(A|do(B)). Generations of people have studied the relationship between P(A|B) and P(A|do(B)). We should pay attention to them.</p>\n\n<h2 id=\"this-is-not-hypothetical\">This is not hypothetical</h2>\n\n<p>Say people bet for a lower Tesla stock price when you condition on Musk being fired. Does that mean they think that firing Musk would hurt the stock price? No, because there could be reverse causality\u2014the stock price dropping might cause him to be fired.</p>\n\n<p>You can try to fight this using the fact that things in the future can\u2019t cause things in the past. That is, you can condition on Musk being fired next week and bet on the stock price six months from now. That surely helps, but you still face other problems.</p>\n\n<p>Here\u2019s another example of how lower prices in Musk-fired world may not indicate that firing Musk hurts the stock price. Suppose:</p>\n\n<ol>\n  <li>\n    <p>You think Musk is a mildly crappy CEO. If he\u2019s fired, he\u2019ll be replaced with someone slightly better, which would slightly increase Tesla\u2019s stock price.</p>\n  </li>\n  <li>\n    <p>You\u2019ve heard rumors that Robyn Denholm has recently decided that she hates Musk and wants to dedicate her life to destroying him. Or maybe not, who knows.</p>\n  </li>\n</ol>\n\n<p>If Denholm fired Musk, that would suggest the rumors are true. So she might try to do other things to hurt him, such as trying to destroy Tesla to erase his wealth. So in this situation, Musk being fired leads to lower stock prices even though firing Musk <em>itself</em> would increase the stock price.</p>\n\n<p><a href=\"https://dynomight.net/img/futarky/comic2big.jpg\"><img alt=\"\" src=\"https://dynomight.net/img/futarky/comic2.jpg\" /></a></p>\n\n<p>Or suppose you run prediction markets for the risk of nuclear war, conditional on Trump sending the US military to enforce a no-fly zone over Ukraine (or not). When betting in these markets, people would surely think about the risk that direct combat between the US and Russian militaries could escalate into nuclear war.</p>\n\n<p>That\u2019s good. But people would <em>also</em> consider that no one really knows exactly what Trump is thinking. If he declared a no-fly zone, that would suggest that he\u2019s feeling feisty and might do <em>other</em> things that could <em>also</em> lead to nuclear war. The markets wouldn\u2019t reflect the causal impact of a no-fly zone alone, because conditional probabilities are not causal.</p>\n\n<h2 id=\"putting-markets-in-charge-doesnt-work\">Putting markets in charge doesn\u2019t work</h2>\n\n<p>So far nothing has worked. But what if we let the markets determine what action is taken? If we pre-commit that Musk will be fired (or not) based on market prices, you might hope that something nice happens and magically we get causal probabilities.</p>\n\n<p>I\u2019m pro-hope, but no such magical nice thing happens.</p>\n\n<p><strong>Thought experiment</strong>. Imagine there\u2019s a bent coin that you guess has a 40% chance of landing heads. And suppose I offer to sell you a contract. If you buy it, we\u2019ll flip the coin and you get $1 if it\u2019s heads and $0 otherwise. Assume I\u2019m not doing anything tricky like 3D printing weird-looking coins. If you want, assume I haven\u2019t even seen the coin.</p>\n\n<p>You\u2019d pay something like $0.40 for that contract, right?</p>\n\n<p>(Actually, knowing my readers, I\u2019m pretty sure you\u2019re all gleefully formulating other edge cases. But I\u2019m also sure you see the point that I\u2019m trying to make. If you need to put the $0.40 in escrow and have the coin-flip performed by a Cenobitic monk, that\u2019s fine.)</p>\n\n<p>Now imagine a <strong>variant of that thought experiment</strong>. It\u2019s the same setup, except if you buy the contract, then I\u2019ll have the coin laser-scanned and ask a supercomputer to simulate millions of coin flips. If more than half of those simulated flips are heads, the bet goes ahead. Otherwise, you get your money back.</p>\n\n<p>Now you should pay at least $0.50 for the contract, even though you only think there\u2019s a 40% chance the coin will land heads.</p>\n\n<p>Why? This is a bit subtle, but you should pay more because you don\u2019t know the true bias of the coin. Your mean estimate is 40%. But it <em>could</em> be 20%, or 60%. After the coin is laser-scanned, the bet only activates if there\u2019s at least a 50% chance of heads. So the contract is worth <em>at least</em> $0.50, and strictly more as long as you think it\u2019s <em>possible</em> the coin has a bias above 50%.</p>\n\n<details>\n  (Math for people who like math.)\n  <p>Suppose <var>b</var> is the true bias of the coin (which the supercomputer will compute). Then your expected return in this game is</p>\n\n  <p>\u00a0\u00a0<var>\ud835\udd3c[max(b, 0.50)] = 0.50 + \ud835\udd3c[max(b-0.50, 0)]</var>,</p>\n\n  <p>where the expectations reflect your beliefs over the true bias of the coin. Since <var>\ud835\udd3c[max(b-0.50, 0)]</var> is never less than zero, the contract is always worth at least $0.50. If you think there\u2019s any chance the bias is above 50%, then the contract is worth strictly more than $0.50.</p>\n</details>\n\n<p><a href=\"https://dynomight.net/img/futarky/soldiers.jpg\"><img alt=\"\" src=\"https://dynomight.net/img/futarky/soldiers.jpg\" /></a></p>\n\n<p>To connect to prediction markets, let\u2019s do one <strong>last thought experiment</strong>, replacing the supercomputer with a market. If you buy the contract, then I\u2019ll have lots of other people bid on similar contracts for a while. If the price settles above $0.50, your bet goes ahead. Otherwise, you get your money back.</p>\n\n<p>You should still bid more than $0.40, even though you only think there\u2019s a 40% chance the coin will land heads. Because the market acts like a (worse) laser-scanner plus supercomputer. Assuming prediction markets are good, the market is smarter than you, so it\u2019s more likely to activate if the true bias of the coin is 60% rather than 20%. This chances your incentives, so you won\u2019t bet your true beliefs.</p>\n\n<h2 id=\"no-order-is-not-preserved\">No, order is not preserved</h2>\n\n<p>I hope you now agree that conditional prediction markets are non-causal, and choosing actions based on the market doesn\u2019t magically make that problem go away.</p>\n\n<p>But you still might have hope! Maybe the <em>order</em> is still preserved? Maybe you\u2019ll at least always pay more for coins that have a higher probability of coming up heads? Maybe if you run a market with a bunch of coins, the best one will always earn the highest price? Maybe it all works out?</p>\n\n<details>\n  \nNope. You can create examples where you'll pay more for a contract on a coin that you think has a lower probability.\n\n\n  <p>Suppose there\u2019s a conditional prediction market for two coins. After a week of bidding, the markets will close, whichever coin had contracts trading for more money will be flipped and $1 paid to contract-holders for head. The other market is cancelled.</p>\n\n  <p>Suppose you\u2019re <em>sure</em> that <strong>coin A</strong>, has a bias of 60%. If you flip it lots of times, 60% of the flips will be heads. But you\u2019re convinced <strong>coin B</strong>, is a trick coin. You think there\u2019s a 59% chance it <em>always</em> lands heads, and a 41% chance it always lands tails. You\u2019re just not sure which.</p>\n\n  <p>We <em>want</em> you to pay more for a contract for coin A, since that\u2019s the coin you think is more likely to be heads (60% vs 59%). But if you like money, you\u2019ll pay more for a contract on coin B. You\u2019ll do that because other people might figure out if it\u2019s an always-heads coin or an always-tails coin. If it\u2019s always heads, great, they\u2019ll bid up the market, it will activate, and you\u2019ll make money. If it\u2019s always tails, they\u2019ll bid down the market, and you\u2019ll get your money back.</p>\n\n  <p>You\u2019ll pay more for coin B contracts, even though you think coin A is better in expectation. Order is not preserved. Things do not work out.</p>\n\n</details>\n\n<h2 id=\"no-its-not-easily-fixable\">No, it\u2019s not easily fixable</h2>\n\n<p>Naive conditional prediction markets aren\u2019t causal. Using time doesn\u2019t solve the problem. Having the market choose actions doesn\u2019t solve the problem. But maybe there\u2019s <em>still</em> hope? Maybe it\u2019s possible to solve the problem by screwing around with the payouts?</p>\n\n<p><strong>Theorem.</strong> Nope. You can\u2019t solve the problem by screwing around with the payouts. There does not exist a payout function that will make you always bid your true beliefs.</p>\n\n<details>\n  \n(Click here for a version of that theorem with math. Warning: Math.)\n\n\n  <p>Suppose you run a market where if you pay <var>x</var> and the final market price is <var>y</var> and <var>z</var> happens, then you get a payout of <var>f(x,y,z)</var> dollars. The payout function can be anything, subject only to the constraint that if the final market price is below some constant <var>c</var>, then bets are cancelled, i.e. <var>f(x,y,z)=x</var> for <var>y &lt; c</var>.</p>\n\n  <p>Now, take any two distributions <var>\u2119\u2081</var> and <var>\u2119\u2082</var>. Assume that:</p>\n  <ul>\n    <li><var>\u2119\u2081[Y&lt;c] = \u2119\u2082[Y&lt;c] &gt; 0</var></li>\n    <li><var>\u2119\u2081[Y\u2265c] = \u2119\u2082[Y\u2265c]</var></li>\n    <li><var>\ud835\udd3c\u2081[Z | Y\u2265c] = \ud835\udd3c\u2082[Z | Y\u2265c]</var></li>\n    <li><var>\ud835\udd3c\u2081[Z | Y&lt;c] \u2260 \ud835\udd3c\u2082[Z | Y&lt;c]</var></li>\n  </ul>\n\n  <p>Then the expected return under <var>\u2119\u2081</var> and <var>\u2119\u2082</var> is the same. That is,</p>\n\n  <p><var>\ud835\udd3c\u2081[f(x,Y,Z)]</var><br />\n\u00a0\u00a0<var>= x \u2119\u2081[Y&lt;c] + \u2119\u2081[Y\u2265c] \ud835\udd3c\u2081[f(x,Y,Z) | Y\u2265c]</var><br />\n\u00a0\u00a0<var>= x \u2119\u2082[Y&lt;c] + \u2119\u2082[Y\u2265c] \ud835\udd3c\u2082[f(x,Y,Z) | Y\u2265c]</var><br />\n\u00a0\u00a0<var>= \ud835\udd3c\u2082[f(x,Y,Z)]</var>.</p>\n\n  <p>Thus, you would be willing to pay the same amount for a contract under both distributions.</p>\n\n  <p>Meanwhile, the difference in expected values is</p>\n\n  <p><var>\ud835\udd3c\u2081[Z] - \ud835\udd3c\u2082[Z]</var><br />\n\u00a0\u00a0<var>= \u2119\u2081[Y&lt;c] \ud835\udd3c\u2081[Z | Y&lt;c] - \u2119\u2082[Y&lt;c] \ud835\udd3c\u2082[Z | Y&lt;c]</var><br />\n\u00a0\u00a0\u00a0\u00a0<var>+ \u2119\u2081[Y\u2265c] \ud835\udd3c\u2081[Z | Y\u2265c] - \u2119\u2082[Y\u2265c] \ud835\udd3c\u2082[Z | Y\u2265c]</var><br />\n\u00a0\u00a0<var>= \u2119\u2081[Y&lt;c] (\ud835\udd3c\u2081[Z | Y&lt;c] - \ud835\udd3c\u2082[Z | Y&lt;c])</var><br />\n\u00a0\u00a0<var>\u2260 0</var>.</p>\n\n  <p>The last line uses our assumptions that <var>\u2119\u2081[Y&lt;c] &gt; 0</var> and <var>\ud835\udd3c\u2081[Z | Y&lt;c] \u2260 \ud835\udd3c\u2082[Z | Y&lt;c]</var>.</p>\n\n  <p>Thus, we have simultaneously that</p>\n\n  <p><var>\ud835\udd3c\u2081[f(x,Y,Z)] = \ud835\udd3c\u2082[f(x,Y,Z)]</var>,</p>\n\n  <p>yet</p>\n\n  <p><var>\ud835\udd3c\u2081[Z] \u2260 \ud835\udd3c\u2082[Z]</var>.</p>\n\n  <p>This means that you should pay the same amount for a contract if you believe <var>\u2119\u2081</var> or <var>\u2119\u2082</var>, even though these entail different beliefs about how likely <var>Z</var> is to happen. Since we haven\u2019t assumed anything about the payout function <var>f(x,y,z)</var>, this means that no working payout function can exist. This is bad.</p>\n\n</details>\n\n<h2 id=\"its-not-that-bad\">It\u2019s not <em>that</em> bad</h2>\n\n<p>Just because conditional prediction markets are non-causal does not mean they are worthless. On the contrary, I think we should do more of them! But they should be treated like observational statistics\u2014just one piece of information to consider skeptically when you make decisions.</p>\n\n<p>Also, while I think these issues are neglected, they\u2019re not completely unrecognized. For example, in 2013, Robin Hanson <a href=\"https://mason.gmu.edu/~rhanson/futarchy2013.pdf\">pointed out</a> that confounding variables can be a problem:</p>\n\n<blockquote>\n  <p>Also, advisory decision market prices can be seriously distorted when decision makers might know things that market speculators do not. In such cases, the fact that a certain decision is made can indicate hidden info held by decision makers. Market estimates of outcomes conditional on a decision then become estimates of outcomes given this hidden info, instead of estimates of the effect of the decision on outcomes.</p>\n</blockquote>\n\n<p>Finally, the flaw <em>can</em> be fixed. In statistics, there\u2019s a whole category of techniques to get causal estimates out of data. Many of these methods have analogies as alternative prediction market designs. I\u2019ll talk about those next time. But here\u2019s a preview: None are free.</p>"
            ],
            "link": "https://dynomight.net/futarchy/",
            "publishedAt": "2025-06-12",
            "source": "Dynomight",
            "summary": "<p>Say you\u2019re <a href=\"https://en.wikipedia.org/wiki/Robyn_Denholm\">Robyn Denholm</a>, chair of Tesla\u2019s board. And say you\u2019re thinking about firing Elon Musk. One way to make up your mind would be to have people bet on Tesla\u2019s stock price six months from now in a market where all bets get cancelled <a href=\"https://manifold.markets/benmanns/tsla-close-price-january-16-2026-if-pEUPRCC5qy\">unless Musk is fired</a>. Also, run a second market where bets are cancelled unless <a href=\"https://manifold.markets/benmanns/tsla-close-price-january-16-2026-if\">Musk stays CEO</a>. If people bet on higher stock prices in Musk-fired world, maybe you should fire him.</p> <p>That\u2019s basically <a href=\"https://en.wikipedia.org/wiki/Futarchy\">Futarchy</a>: Use conditional prediction markets to make decisions.</p> <p>People often argue about fancy aspects of Futarchy. Are stock prices all you care about? Could Musk use his wealth to bias the market? What if Denholm makes different bets in the two markets, and then fires Musk (or not) to make sure she wins? Are human values and beliefs somehow inseparable?</p> <p>My objection is more basic: It doesn\u2019t work. You can\u2019t use conditional predictions markets to make decisions like this, because conditional prediction markets reveal <em>probabilistic</em> relationships, not <em>causal</em> relationships. The whole concept is faulty.</p> <p>There <em>are</em> solutions\u2014ways to force markets to give you causal relationships. But those solutions are <em>painful</em> and I get the shakes when I",
            "title": "Futarchy\u2019s fundamental flaw"
        },
        {
            "content": [],
            "link": "https://harper.blog/notes/2025-06-12_e64261169a92_fast-garfield-park-conservator/",
            "publishedAt": "2025-06-12",
            "source": "Harper Reed",
            "summary": "<p>Fast. Garfield Park Conservatory is always amazing.</p> <figure> <img alt=\"image_1.jpg\" height=\"1349\" src=\"https://harper.blog/notes/2025-06-12_e64261169a92_fast-garfield-park-conservator/image_1.jpg\" width=\"1799\" /> </figure> <hr /> <p>Thank you for using RSS. I appreciate you. <a href=\"mailto:harper&#64;modest.com\">Email me</a></p>",
            "title": "Note #258"
        },
        {
            "content": [],
            "link": "https://buttondown.com/hillelwayne/archive/solving-linkedin-queens-with-smt/",
            "publishedAt": "2025-06-12",
            "source": "Hillel Wayne",
            "summary": "<h3>No newsletter next week</h3> <p>I\u2019ll be speaking at <a href=\"https://systemsdistributed.com/\" target=\"_blank\">Systems Distributed</a>. My talk isn't close to done yet, which is why this newsletter is both late and short. </p> <h1>Solving LinkedIn Queens in SMT</h1> <p>The article <a href=\"https://codingnest.com/modern-sat-solvers-fast-neat-underused-part-1-of-n/\" target=\"_blank\">Modern SAT solvers: fast, neat and underused</a> claims that SAT solvers<sup id=\"fnref:SAT\"><a class=\"footnote-ref\" href=\"https://buttondown.com/hillelwayne/rss#fn:SAT\">1</a></sup> are \"criminally underused by the industry\". A while back on the newsletter I asked \"why\": how come they're so powerful and yet nobody uses them? Many experts responded saying the reason is that encoding SAT kinda sucked and they rather prefer using tools that compile to SAT. </p> <p>I was reminded of this when I read <a href=\"https://ryanberger.me/posts/queens/\" target=\"_blank\">Ryan Berger's post</a> on solving \u201cLinkedIn Queens\u201d as a SAT problem. </p> <p>A quick overview of Queens. You\u2019re presented with an NxN grid divided into N regions, and have to place N queens so that there is exactly one queen in each row, column, and region. While queens can be on the same diagonal, they <em>cannot</em> be adjacently diagonal.</p> <p>(Important note: Linkedin \u201cQueens\u201d is a variation on the puzzle game <a href=\"https://starbattle.puzzlebaron.com/\" target=\"_blank\">Star Battle</a>, which is the same except the number of stars you place in each row/column/region varies",
            "title": "Solving LinkedIn Queens with SMT"
        },
        {
            "content": [
                "<p>The simple story about decision market liquidity is: such markets need an expectation of liquidity to induce traders to reveal info there, so that prices can usefully advise decisions. Thus those who value such advice should pay for it by subsidizing market maker liquidity.</p><p>I&#8217;ve been thinking on this topic for the last few weeks, and in this post I&#8217;ll give a more complex story, tied to this diagram:</p><div class=\"captioned-image-container\"><figure><a class=\"image-link image2 is-viewable-img\" href=\"https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3d94cf9e-cac0-4dbe-ab25-db1cd9859d56_898x535.png\" target=\"_blank\"><div class=\"image2-inset\"><source type=\"image/webp\" /><img alt=\"\" class=\"sizing-normal\" height=\"535\" src=\"https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3d94cf9e-cac0-4dbe-ab25-db1cd9859d56_898x535.png\" width=\"898\" /><div class=\"image-link-expand\"><div class=\"pencraft pc-display-flex pc-gap-8 pc-reset\"><div class=\"pencraft pc-reset icon-container restack-image\"><svg class=\"lucide lucide-refresh-cw\" fill=\"none\" height=\"20\" stroke=\"currentColor\" stroke-linecap=\"round\" stroke-linejoin=\"round\" stroke-width=\"2\" viewBox=\"0 0 24 24\" width=\"20\" xmlns=\"http://www.w3.org/2000/svg\"><path d=\"M3 12a9 9 0 0 1 9-9 9.75 9.75 0 0 1 6.74 2.74L21 8\"></path><path d=\"M21 3v5h-5\"></path><path d=\"M21 12a9 9 0 0 1-9 9 9.75 9.75 0 0 1-6.74-2.74L3 16\"></path><path d=\"M8 16H3v5\"></path></svg></div><div class=\"pencraft pc-reset icon-container view-image\"><svg class=\"lucide lucide-maximize2\" fill=\"none\" height=\"20\" stroke=\"currentColor\" stroke-linecap=\"round\" stroke-linejoin=\"round\" stroke-width=\"2\" viewBox=\"0 0 24 24\" width=\"20\" xmlns=\"http://www.w3.org/2000/svg\"><polyline points=\"15 3 21 3 21 9\"></polyline><polyline points=\"9 21 3 21 3 15\"></polyline><line x1=\"21\" x2=\"14\" y1=\"3\" y2=\"10\"></line><line x1=\"3\" x2=\"10\" y1=\"21\" y2=\"14\"></line></svg></div></div></div></div></a></figure></div><p>At the top of the diagram we see a simple decision market. On the left, at the earliest time, we see in blue the price of an unconditional market trading cash for an asset that pays in proportion to a key decision outcome, such as the stock of a firm or the coin of a DAO.</p><p>At time A, two conditional markets are introduced, shown in green and orange, also trading this asset for cash, but each conditional on some specific proposal being accepted (yes) or rejected (no). At time E the decision is made, based at least in part on price differences here between yes and no during the A to E period. At E one of the conditional markets, in this case yes, becomes equal to the unconditional market. The other conditional assets, in this case no, disappear.</p><p>In some uses of futarchy, there is no need for a no market, as the outcome given a no decision is obvious. In some uses of futarchy, the unconditional market doesn&#8217;t exist, as the outcome asset only exists to support this decision. But when an unconditional market exists along with two conditional markets, the three prices imply a probability of the decision being yes. The trading interface can let users make bets on that probability, via trades in these three markets. And that market chance of yes can be helpful in the rest of the system, as we will see.</p><p>In the lower half of the diagram we see a red line showing a plan for how conditional market liquidity provided by subsidized market makers changes with time. There is no liquidity before A or after E, as the conditional markets don&#8217;t exist then. At A, the conditional markets start, but with poorly informed initial prices created by the market system implementors. The first traders thus get to profit from correcting those first price mistakes, but they profit little as liquidity starts out near zero.</p><p>As there are games that traders can play if they can expect to be the only trader both before and after a sudden change in liquidity, it seems safer to just avoid sudden jumps, via have planned market maker liquidity be a continuous function of time.</p><p>The profit that a trader makes from changing a price by delta, from the current price to their expected value of the asset, is the liquidity times that price delta squared (times the chance the condition will be met, for conditional markets). Thus adding more liquidity is offering a higher price for info. As decision market sponsors are likely a monopolist buyer of this info, they do not want to buy it at their value, but instead seek a monopolist profit-maximizing price lower than their info value, a price that trades off getting more info versus paying more for that info.</p><p>In the diagram, liquidity increases steadily from zero at A up to a maximum at B. Someone who is sure that they are a monopolist seller of some piece of info should thus wait until near B to reveal their info to the market. But those who expect that they are only the cheapest supplier of their info should worry that the second cheapest supplier will sell when the price rises to that second lowest cost. Thus the cheapest supplier should sell near the second cheapest cost. A predictably rising liquidity thus induces competition among those with access to the same info, allowing the decision market sponsor to buy that info at a low price.</p><p>The observation period between C and D is intended to show the clearest signal re the price difference between the yes and no markets, and thus be the main focus of decision makers seeking advice from these markets. As there are games that traders might play at the very end of the decision period D, it might be better to make D a random time, such as via a constant chance per time of switching modes.</p><p>We want to minimize both informed and manipulative trading during this observation period. Manipulators will focus on that period because the decision makers will, but as manipulators try to look like informed traders, manipulators are easier to identify and counter when there is less informed trading. We also want to reduce informed trading in this period as that can induce a <a href=\"https://www.overcomingbias.com/p/decision-selection-bias?utm_source=publication-search\">decision selection bias</a> earlier in this period.</p><p>Having a short observation period C to D, and a much higher subsidized liquidity both before and after the observation period, tempts informed traders away from the observation period, This difference needs to be big enough to counter the liquidity added by manipulators, shown in a purple dotted line. The liquidity should be higher before than after the observation period to make the decision as informed as possible. The higher liquidity in the period D to E before the conditional markets end also makes that a good time for traders to coordinate to exit these markets together if there are not thick unconditional market after E.</p><p>As the sponsor pays to subsidize liquidity in decision markets to help inform their decision, such info is less valuable when their decision choice is more clear. It can thus make sense to reduce liquidity provided by a factor of roughly <em>p(1-p)</em>, where <em>p</em> is the market chance of a yes decision. Such a reduction is shown in the diagram as a dotted red line.</p><p>In an advisory futarchy, decision markets can consider the entire price history from A to E, and any other info they can find, in making their decision. The quality of the signal in the decision market prices of the period C to D will be highest if decision makers, or their informed associates, are allowed to trade in this period, if the decision time is clear, and if the period from C to E is short. It can also make sense to weigh price differences in the observation period by <em>p(1-p), </em>as conditional market prices may become less reliable as the chance of their condition becomes very small.</p><p>And there you have it, a more detailed analysis of how best a decision market sponsor can subsidize market makers to induce traders to reveal info there.</p>"
            ],
            "link": "https://www.overcomingbias.com/p/futarchy-liquidity-details",
            "publishedAt": "2025-06-12",
            "source": "Robin Hanson",
            "summary": "The simple story about decision market liquidity is: such markets need an expectation of liquidity to induce traders to reveal info there, so that prices can usefully advise decisions.",
            "title": "Futarchy Liquidity Details"
        },
        {
            "content": [
                "<p>\n          <a href=\"https://www.astralcodexten.com/p/hidden-open-thread-3855\">\n              Read more\n          </a>\n      </p>"
            ],
            "link": "https://www.astralcodexten.com/p/hidden-open-thread-3855",
            "publishedAt": "2025-06-12",
            "source": "SlateStarCodex",
            "summary": "<p> <a href=\"https://www.astralcodexten.com/p/hidden-open-thread-3855\"> Read more </a> </p>",
            "title": "Hidden Open Thread 385.5"
        },
        {
            "content": [
                "<p>This is another heuristic from the same place as <a href=\"https://www.astralcodexten.com/p/if-its-worth-your-time-to-lie-its\">If It&#8217;s Worth Your Time To Lie, It&#8217;s Worth My Time To Correct You</a>. </p><p>If someone proves you are absolutely, 100% wrong about something, it&#8217;s polite to say &#8220;Oh, I guess I was wrong, sorry&#8221; before launching into your next argument.</p><p>That is, instead of:</p><blockquote><p>&#8220;Trump is too corrupt and scandal-ridden to be president! He was responsible for the Watergate break-in!&#8221;</p><p><em>No, you&#8217;re confusing him with Richard Nixon.</em></p><p>&#8220;But what about January 6th?&#8221;</p></blockquote><p>&#8230;better would be:</p><blockquote><p>&#8220;Trump is too corrupt and scandal-ridden to be president! He was responsible for the Watergate break-in!&#8221;</p><p><em>No, you&#8217;re confusing him with Richard Nixon.</em></p><p>&#8220;Oh, you&#8217;re right, sorry. I agree that&#8217;s wasn&#8217;t Trump&#8217;s fault and I&#8217;m sorry for getting it wrong. But still, what about January 6th?&#8221;</p></blockquote><p>I guess that example sounds so fake and contrived that I&#8217;ll break my usual policy of not shaming specific commenters and provide the real-world equivalent. Someone wrote a blog post where they argued a certain calculation showed that the chance of a technological singularity in our lifetime was only 0.33%. I retraced the argument and found that if you did the math correctly, it was actually about 30%. Here&#8217;s the comment they left on that post:</p><div class=\"captioned-image-container\"><figure><a class=\"image-link image2 is-viewable-img\" href=\"https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc47e29bd-01d2-409f-9f5b-5f03a14edd14_820x511.png\" target=\"_blank\"><div class=\"image2-inset\"><source type=\"image/webp\" /><img alt=\"\" class=\"sizing-normal\" height=\"380.1341463414634\" src=\"https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc47e29bd-01d2-409f-9f5b-5f03a14edd14_820x511.png\" width=\"610\" /><div class=\"image-link-expand\"><div class=\"pencraft pc-display-flex pc-gap-8 pc-reset\"><div class=\"pencraft pc-reset icon-container restack-image\"><svg class=\"lucide lucide-refresh-cw\" fill=\"none\" height=\"20\" stroke=\"currentColor\" stroke-linecap=\"round\" stroke-linejoin=\"round\" stroke-width=\"2\" viewBox=\"0 0 24 24\" width=\"20\" xmlns=\"http://www.w3.org/2000/svg\"><path d=\"M3 12a9 9 0 0 1 9-9 9.75 9.75 0 0 1 6.74 2.74L21 8\"></path><path d=\"M21 3v5h-5\"></path><path d=\"M21 12a9 9 0 0 1-9 9 9.75 9.75 0 0 1-6.74-2.74L3 16\"></path><path d=\"M8 16H3v5\"></path></svg></div><div class=\"pencraft pc-reset icon-container view-image\"><svg class=\"lucide lucide-maximize2\" fill=\"none\" height=\"20\" stroke=\"currentColor\" stroke-linecap=\"round\" stroke-linejoin=\"round\" stroke-width=\"2\" viewBox=\"0 0 24 24\" width=\"20\" xmlns=\"http://www.w3.org/2000/svg\"><polyline points=\"15 3 21 3 21 9\"></polyline><polyline points=\"9 21 3 21 3 15\"></polyline><line x1=\"21\" x2=\"14\" y1=\"3\" y2=\"10\"></line><line x1=\"3\" x2=\"10\" y1=\"21\" y2=\"14\"></line></svg></div></div></div></div></a></figure></div><p>Cool thought, but I wish it had started with &#8220;Okay, you&#8217;re right and I&#8217;m wrong about the math, but I think you really want time machines and&#8230;&#8221;</p><p>Why? I don&#8217;t have as many specific arguments here as for the IIWYTTLIWMTTCI principle, but I think it&#8217;s good to make the mental update of realizing you were wrong about something, so that if you notice yourself making that update constantly you can reassess your overall level of conscientiousness.</p><p>And it&#8217;s good for your interlocutor to realize that they&#8217;re not just speaking into a void, that you&#8217;re capable of admitting fault, and that it&#8217;s a real discussion with potential win criteria instead of them getting bombarded again and again until they go away.</p><p>Most people hold most of their beliefs for more than one reason (<a href=\"https://slatestarcodex.com/2015/02/14/how-likely-are-multifactorial-trends/\">is this mysterious?</a>). It&#8217;s fine to admit that one of your arguments was wrong, while continuing to believe the same thing as before. But I think for the sake of other people&#8217;s patience and your own ability to <em>eventually</em> change your mind, you should take note and increment exactly how many of your arguments are getting disproven.</p>"
            ],
            "link": "https://www.astralcodexten.com/p/but-vs-yes-but",
            "publishedAt": "2025-06-12",
            "source": "SlateStarCodex",
            "summary": "<p>This is another heuristic from the same place as <a href=\"https://www.astralcodexten.com/p/if-its-worth-your-time-to-lie-its\">If It&#8217;s Worth Your Time To Lie, It&#8217;s Worth My Time To Correct You</a>. </p><p>If someone proves you are absolutely, 100% wrong about something, it&#8217;s polite to say &#8220;Oh, I guess I was wrong, sorry&#8221; before launching into your next argument.</p><p>That is, instead of:</p><blockquote><p>&#8220;Trump is too corrupt and scandal-ridden to be president! He was responsible for the Watergate break-in!&#8221;</p><p><em>No, you&#8217;re confusing him with Richard Nixon.</em></p><p>&#8220;But what about January 6th?&#8221;</p></blockquote><p>&#8230;better would be:</p><blockquote><p>&#8220;Trump is too corrupt and scandal-ridden to be president! He was responsible for the Watergate break-in!&#8221;</p><p><em>No, you&#8217;re confusing him with Richard Nixon.</em></p><p>&#8220;Oh, you&#8217;re right, sorry. I agree that&#8217;s wasn&#8217;t Trump&#8217;s fault and I&#8217;m sorry for getting it wrong. But still, what about January 6th?&#8221;</p></blockquote><p>I guess that example sounds so fake and contrived that I&#8217;ll break my usual policy of not shaming specific commenters and provide the real-world equivalent. Someone wrote a blog post where they argued a certain calculation showed that the chance of a technological singularity in our lifetime was only 0.33%. I retraced the argument and found that if you did the math correctly, it was actually about 30%. Here&#8217;s the comment they left on that post:</p><div class=\"captioned-image-container\"><figure><a",
            "title": "\"But\" vs. \"Yes, But\""
        }
    ],
    "lookbackDays": 1,
    "publishDate": "2025-06-12"
}