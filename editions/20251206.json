{
    "articles": [
        {
            "content": [
                "<header>\n  <h1>Mechanical Habits</h1>\n  <time class=\"meta\" datetime=\"2025-12-06\">Dec 6, 2025</time>\n</header>\n<p>My schtick as a software engineer is establishing automated processes \u2014 mechanically enforced\npatterns of behavior. I have collected a Santa Claus bag of specific tricks I\u2019ve learned from\ndifferent people, and want to share them in turn.</p>\n<p>Caution: engineering processes can be tricky to apply in a useful way. A process is a logical cut\n\u2014 there\u2019s some goal we <em>actually</em> want, and automation can be a shortcut to achieve it, but\nautomation per se doesn\u2019t explain what the original goal is. Keep the goal and adjust the processes\non the go. Sanity checks: A) automation should reduce toil. If robots create work for humans, down\nwith the robots! B) good automation usually is surprisingly simple, simplistic even. Long live the\nduct tape!</p>\n<section id=\"Weekly-Releases\">\n\n<h2><a href=\"https://matklad.github.io/2025/12/06/mechanical-habits.html#Weekly-Releases\">Weekly Releases</a></h2>\n<p>By far the most impactful trick \u2014 make a release of your software every Friday. The first order\nmotivation here is to reduce the stress and effort required for releases. If releases are small,\nwriting changelogs is easy, assessing the riskiness of release doesn\u2019t require anything more than\nmentally recalling a week\u2019s worth of work, and there\u2019s no need to aim to land features into a\nparticular releases. Delaying a feature by a week is nothing, delaying by a year is a reason to put\nin an all-nighter.</p>\n<p>As an example, this Friday I was filling my US visa application, so I was feeling somewhat tired in\nthe evening. I was also the release manager. So I just messaged \u201csorry, I am feeling too tired to\nmake a release, we are skipping this one\u201d without thinking much about it. It\u2019s cheap to skip the\nrelease, so there\u2019s no temptation to push yourself to get the release done (and quickly follow up\nwith a point release, the usual consequence).</p>\n<p>But the real gem is the second order effect \u2014 weekly releases force you to fix all <em>other</em>\nprocesses to keep the codebase healthy all the time. And it is <em>much</em> easier to keep the flywheel\ngoing at roughly the same speed, rather than periodically to struggle to get it going. Temporal\nlocality is the king: \u201cI don\u2019t have time to fix X right now, I\u2019ll do it before the release\u201d is the\nkiller. By the time of release you\u2019ll need 2X time just to load X in your head! It\u2019s much faster\noverall to immediately make every line of code releasable. Work the iron while it is hot!</p>\n</section>\n<section id=\"Epistemic-Aside\">\n\n<h2><a href=\"https://matklad.github.io/2025/12/06/mechanical-habits.html#Epistemic-Aside\">Epistemic Aside</a></h2>\n<p>I\u2019ve done releases every Friday in IntelliJ Rust, rust-analyzer, and TigerBeetle, to a great\nsuccess. It\u2019s worth reflecting how I got there. The idea has two parents:</p>\n<ul>\n<li>\nPieter Hintjens writings, specifically, the idea of\n<a href=\"https://hintjens.gitbooks.io/social-architecture/content/chapter4.html#:~:text=Maintainers%20SHALL%20merge%20correct%20patches%20rapidly.\">Optimistic Merging</a>\n</li>\n<li>\nRust\u2019s <a href=\"https://blog.rust-lang.org/2014/10/30/Stability/\">six-weekly release process</a> (itself inspired by the browser world)\n</li>\n</ul>\n<p>Both seemed worthwhile to try for me, and I figured that a nicely synthesis would be to release\nevery Monday, not every six weeks (I later moved cutting the release to Friday, so that it can bake\nin beta/fuzzers during the weekend). I just finished University at that point, and had almost zero\nworking experience! The ideas made sense to me not based on my past experiences, or on being\npromulgated by some big names, but because they made sense if you just think about them from first\nprinciples. It\u2019s the other way around \u2014 I fell in love with Rust and Pieter\u2019s writing because of\nthe quality of the ideas. And I only needed common sense to assess the ideas, no decade in the\nindustry required.</p>\n<p>This applies to the present blog post \u2014 engage with ideas, remix them, and improve them. Don\u2019t\ntreat the article as a mere cook book, it is not.</p>\n</section>\n<section id=\"Not-Rocket-Science-Rule\">\n\n<h2><a href=\"https://matklad.github.io/2025/12/06/mechanical-habits.html#Not-Rocket-Science-Rule\">Not Rocket Science Rule</a></h2>\n<p>I feel like I link\n<a class=\"display url\" href=\"https://graydon2.dreamwidth.org/1597.html\">https://graydon2.dreamwidth.org/1597.html</a>\nfrom every second post of mine, so I\u2019ll keep it short this time.</p>\n<ul>\n<li>\nOnly advance the tip of the master branch to a commit hash, for which you <em>already know</em> the tests\nresults. That is, make a detached merge commit, test that, then move the tip.\n</li>\n<li>\nDon\u2019t do it yourself, let the robot do it.\n</li>\n</ul>\n<p>The <em>direct</em> benefit is asynchronizing the process of getting the code in. When you submit PR, you\ndon\u2019t need to wait until CI is complete, and then make a judgement call if the results are fresh\nenough or you need to rebase to the new version of master branch. You just tell the robot \u201cmerge\nwhen the merge commit is green\u201d. The standard setup uses robots to create work for humans. Merge\nqueue inverts this.</p>\n<p>But the true benefit is second-order! You can\u2019t really ask the robot nicely to let your very\nimportant PR in, despite a completely unrelated flaky failure elsewhere. You are forced to keep your\nCI setup tidy.</p>\n<p>There\u2019s also a third-order benefit. NRSR encourages holistic view of your CI, as a set of invariants\nthat <em>actually</em> hold for your software, a type-system of sorts. And that thinking makes you realize\nthat every automatable check can be a test. Again, good epistemology helps: it\u2019s not the idea of bors\nthat is most valuable, it\u2019s the reasoning behind that: \u201cautomatically maintain a repository of code\nthat always passes all the tests\u201d, \u201cmonotonically increasing test coverage\u201d. Go re-read Graydon\u2019s\npost!</p>\n</section>\n<section id=\"Tidy-Script\">\n\n<h2><a href=\"https://matklad.github.io/2025/12/06/mechanical-habits.html#Tidy-Script\">Tidy Script</a></h2>\n<p>This is another idea borrowed from Rust. Use a <code>tidy</code> file to collect various project-specific\nlinting checks as tests. The biggest value of such\n<a href=\"https://github.com/tigerbeetle/tigerbeetle/blob/0.16.66/src/tidy.zig\"><code>tidy.zig</code></a>\nis its mere existence. It\u2019s much easier to add a new check than to create \u201cchecking infrastructure\u201d.\nSome checks we do at TigerBeetle:</p>\n<ul>\n<li>\nNo large binary blobs in big history. Don\u2019t repeat my rust-analyzer mistake here, and look for\nactual git objects, not just files in the working repository. Someone once sneaked 1MiB of\nreverted protobuf nonsense past me and my file-based check.\n</li>\n<li>\nLine &amp; function length.\n</li>\n<li>\nNo problematic (for our use case) std APIs are used.\n</li>\n<li>\nNo <code>// FIXME</code> comments. This is used positively \u2014 I add <code>// FIXME</code> comments to code I want to\nchange before the merge (this one is also from Rust!).\n</li>\n<li>\nNo dead code (Zig specific, as the compiler is not well-positioned to tackle that, due to lazy\ncompilation model).\n</li>\n</ul>\n<p>Pro tip for writing tidings \u2014 shell out to <code>git ls-files -z</code> to figure out what needs tidying.</p>\n</section>\n<section id=\"DevHub\">\n\n<h2><a href=\"https://matklad.github.io/2025/12/06/mechanical-habits.html#DevHub\">DevHub</a></h2>\n<p>I don\u2019t remember the origin here, but\n<a class=\"display url\" href=\"https://deno.com/benchmarks\">https://deno.com/benchmarks</a>\ncertainly is an influence.</p>\n<p>The habit is to maintain, for every large project, a directory with static files which is directly\ndeployed from the master branch as a project\u2019s internal web page. E.g., for TigerBeetle:</p>\n<ul>\n<li>\n<a class=\"url\" href=\"https://github.com/tigerbeetle/tigerbeetle/tree/0.16.66/src/devhub\">https://github.com/tigerbeetle/tigerbeetle/tree/0.16.66/src/devhub</a>\n</li>\n<li>\n<a class=\"url\" href=\"http://tigerbeetle.github.io/tigerbeetle/\">http://tigerbeetle.github.io/tigerbeetle/</a>\n</li>\n</ul>\n<p>Again, motivation is mere existence and removal of friction. This is an office whiteboard which you\ncan just write on, for whatever purpose! Things we use ours for:</p>\n<ul>\n<li>\nRelease rotation.\n</li>\n<li>\nBenchmark&amp;fuzzing results. This is a bit of social engineering: you check DevHub out of anxiety,\nto make sure its not your turn to make a release this week, but you get to spot performance\nregressions!\n</li>\n<li>\nIssues in needs of triaging.\n</li>\n</ul>\n<p>I gave a talk about using DevHub for visualizing fuzzing results for HYTRADBOI\n(<a href=\"https://www.hytradboi.com/2025/c222d11a-6f4d-4211-a243-f5b7fafc8d79-rocket-science-of-simulation-testing\">video</a>)</p>\n<p>Another tip: JSON file in a git repository is a fine database to power such an internal website.\n<a href=\"https://tailscale.com/blog/an-unlikely-database-migration\">JSONMutexDB</a> for the win.</p>\n</section>\n<section id=\"Micro-Benchmarks\">\n\n<h2><a href=\"https://matklad.github.io/2025/12/06/mechanical-habits.html#Micro-Benchmarks\">Micro Benchmarks</a></h2>\n<p>The last one for today, and the one that prompted this article! I am designing a new mechanical\nhabit for TigerBeetle and I want to capture the process while it is still fresh in my mind.</p>\n<p>It starts with something rotten. Micro benchmarks are hard. You write one when you are working on\nthe code, but then it bitrots, and by the time the next person has a brilliant optimization idea,\nthey can not compile the benchmark anymore, and they also have no idea which part of the three pages\nof output is important.</p>\n<p>A useful trick for solving bitrot is to chain a new habit onto an existing one. Avoid multiplying\nentry points (<a href=\"https://matklad.github.io/2023/12/31/O(1)-build-file.html\"><em>O(1) Build File</em></a>). The\nappropriate entry point here are the tests. So each micro benchmark is going to be just a test:</p>\n\n<figure class=\"code-block\">\n\n\n<pre><code><span class=\"line\"><span class=\"hl-keyword\">test</span> <span class=\"hl-string\">&quot;benchmark: binary search&quot;</span> {</span>\n<span class=\"line\">    <span class=\"hl-comment\">// ...</span></span>\n<span class=\"line\">}</span></code></pre>\n\n</figure>\n<p>Bitrot problem solved. Now we have two new ones. <em>First</em> is that you generally want to run the\nbenchmark long enough to push the times into human range (~2 seconds), so that any improvements are\nimmediately, viscerally perceived. But 2 seconds are too slow for a test, <em>and</em> test are usually run\nin Debug mode. The second problem is that you want to see the timing outcome of the benchmark\nprinted when you run that benchmark. But you don\u2019t want to see the output when you run the tests!</p>\n<p>So, we really want two modes here: in the first mode, we really are running a benchmark, it is\ncompiled with optimizations, we aim to make runtime low seconds at least, and we want to print the\nseconds afterwards. In the second mode, we are running our test suite, and we want to run the\nbenchmark just for some token amount of time. DWIM (do what I mean) principle helps here. We run\nthe entire test suite as\n<span class=\"display\"><code>./zig/zig build test</code>,</span>\nand a single benchmark as\n<span class=\"display\"><code>./zig/zig build test -- \"benchmark: search\"</code></span>\nSo we use the shape of CLI invocation to select benchmarking mode.</p>\n<p>This <code>mode</code> then determines whether we should pick large or small parameters. Playing around with\nthe code, it feels like the following is a nice shape of code to get parameter values:</p>\n\n<figure class=\"code-block\">\n\n\n<pre><code><span class=\"line\"><span class=\"hl-keyword\">var</span> bench = Bench.init();</span>\n<span class=\"line\"></span>\n<span class=\"line\"><span class=\"hl-keyword\">const</span> element_count =</span>\n<span class=\"line\">    bench.parameter(<span class=\"hl-string\">&quot;element_count&quot;</span>, <span class=\"hl-numbers\">1_000</span>, <span class=\"hl-numbers\">10_000_000</span>);</span>\n<span class=\"line\"></span>\n<span class=\"line\"><span class=\"hl-keyword\">const</span> search_count =</span>\n<span class=\"line\">    bench.parameter(<span class=\"hl-string\">&quot;search_count&quot;</span>, <span class=\"hl-numbers\">5_000</span>, <span class=\"hl-numbers\">500_000</span>);</span></code></pre>\n\n</figure>\n<p>The small value is test mode, the big value is benchmark mode, and the name is useful to print\nactual parameter value:</p>\n\n<figure class=\"code-block\">\n\n\n<pre><code><span class=\"line\">bench.report(<span class=\"hl-string\">&quot;{s}={}&quot;</span>, .{ name, value });</span></code></pre>\n\n</figure>\n<p>This <code>report</code> function is what decides whether to swallow (test mode) or show (benchmark mode) the\noutput. Printing the values is useful to make copy-pasted benchmarking results obvious without\ncontext. And, now that we have put the names in, we get to override values of parameters via\nenvironmental variables for free!</p>\n<p>And this is more or less it? We now have a standard pattern to grow the set of microbenchmarks,\nwhich feels like it should hold up with the time passing?</p>\n<p><a class=\"url\" href=\"https://github.com/tigerbeetle/tigerbeetle/pull/3405\">https://github.com/tigerbeetle/tigerbeetle/pull/3405</a></p>\n<p>Check back in a couple of years to see if this mechanical habit sticks!</p>\n</section>"
            ],
            "link": "https://matklad.github.io/2025/12/06/mechanical-habits.html",
            "publishedAt": "2025-12-06",
            "source": "Alex Kladov",
            "summary": "My schtick as a software engineer is establishing automated processes --- mechanically enforced patterns of behavior. I have collected a Santa Claus bag of specific tricks I've learned from different people, and want to share them in turn.",
            "title": "Mechanical Habits"
        },
        {
            "content": [
                "<p><img alt=\"\" src=\"https://www.dbreunig.com/img/reliable.jpg\" /></p>\n\n<h3 id=\"enterprise-agents-struggle-to-reach-production-or-find-adoption-due-to-reliability-concerns\">Enterprise agents struggle to reach production or find adoption due to reliability concerns</h3>\n\n<p>Throughout 2025, there\u2019s been a steady drumbeat of reports on the state of AI in the enterprise. On the surface, many appear to disagree. But dig in a little bit, look at how each report was assembled and how they defined their terms and you\u2019ll find a consistent story: <strong>adoption of 3rd party AI apps is surging while 1st party development struggles to find success</strong>.</p>\n\n<p>If you\u2019re short on time, here\u2019s the tl;dr:</p>\n\n<ol>\n  <li>Off-the-shelf AI tools are widely used and valued within the enterprise. (<a href=\"https://knowledge.wharton.upenn.edu/special-report/2025-ai-adoption-report/\">Wharton/GBK\u2019s AI Adoption Report</a>)</li>\n  <li>But internal AI pilots fail to earn adoption. (<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSc8rU8OpQWU44gYDeZyINUZjBFwu--1uTbxixK_PRSVrfaH8Q/viewform\">MIT NANDA\u2019s report</a>)</li>\n  <li>Very few enterprise agents make it past the pilot stage into production. (<a href=\"https://www.mckinsey.com/capabilities/quantumblack/our-insights/the-state-of-ai#/\">McKinsey\u2019s State of AI</a>)</li>\n  <li>To reach production, developers compromise and build simpler agents to achieve reliability. (<a href=\"https://arxiv.org/abs/2512.04123v1\">UC Berkeley\u2019s MAP</a>)</li>\n</ol>\n\n<p>The few custom agents that make it past the gauntlet figure out how to achieve reliability, earn employee trust, and <em>actually find usage</em>. Reliability is the barrier holding back agents, and right now the best way to achieve it is scaling back ambitions.</p>\n\n<hr />\n\n<p>Let\u2019s start with <a href=\"https://fortune.com/2025/08/18/mit-report-95-percent-generative-ai-pilots-at-companies-failing-cfo/\">the notorious MIT NANDA report</a> which generated the headline, \u201c<a href=\"https://fortune.com/2025/08/18/mit-report-95-percent-generative-ai-pilots-at-companies-failing-cfo/\">95% of generative AI pilots at companies are failing</a>.\u201d</p>\n\n<p>Plenty have <a href=\"https://www.futuriom.com/articles/news/why-we-dont-believe-mit-nandas-werid-ai-study/2025/08\">criticized the methodology and conclusions NANDA reaches</a>, but I tend to believe most of the claims in <a href=\"https://docs.google.com/forms/d/e/1FAIpQLSc8rU8OpQWU44gYDeZyINUZjBFwu--1uTbxixK_PRSVrfaH8Q/viewform\">the report</a> provided we keep in mind <em>who</em> was surveyed and understand that \u201cAI pilots\u201d were defined as <em>internally developed applications</em>. Keep this in mind as you review the following two figures:</p>\n\n<p><img alt=\"MIT NANDA's study finds that business leaders can't get employees to adopt internal AI tools. Meanwhile, employees regularly use LLMs elsewhere.\" src=\"https://www.dbreunig.com/img/nanda_chart.jpg\" /></p>\n\n<p><a href=\"https://www.dbreunig.com/2025/09/15/ai-adoption-at-work-play.html\">I wrote in September</a>:</p>\n\n<blockquote>\n  <p>For all the criticism of the NANDA report, it is a survey of many business leaders. We can treat it as such. So while we might take that 95% figure with a grain of salt, we can trust that business leaders believe the biggest reason their AI pilots are failing is because their employees are unwilling to adopt new tools\u2026 While 90% of employees surveyed eagerly use AI tools they procure themselves.</p>\n</blockquote>\n\n<p>Internal applications struggle, while employee-driven use of ChatGPT and Claude is booming.</p>\n\n<hr />\n\n<p>Wharton and GBK\u2019s annual <a href=\"https://knowledge.wharton.upenn.edu/special-report/2025-ai-adoption-report/\">AI adoption report</a> appears to counter NANDA with claims that, \u201cAI is becoming deeply integrated into modern work.\u201d 82% of enterprise leaders use Gen AI weekly and 89% \u201cbelieve Gen AI augments work.\u201d</p>\n\n<p>The Wharton report is an interesting read that details how people are using AI tools throughout their workday. But these are overwhelmingly 3rd party tools:</p>\n\n<p><img alt=\"Off-the-shelf chatbot tools dominate enterprise AI usage, according to Wharton's annual survey.\" src=\"https://www.dbreunig.com/img/wharton_ai_25_02.jpg\" /></p>\n\n<p>ChatGPT, Copilot, and Gemini dominate usage (Claude ranks surprisingly low, likely a function of Wharton\u2019s respondent base). Custom chatbots see less usage than ChatGPT, and even then: the \u201cby/for\u201d in \u201cbuilt specifically by/for my organization\u201d is doing a lot of work.</p>\n\n<p>10 slides later, the report states (emphasis mine), \u201cCustomized Gen AI Solutions <em>May</em> be Coming as Internal R&amp;D Reaches One-Third of Tech Budgets.\u201d The money is being deployed, but customized AI has yet to arrive at scale.</p>\n\n<hr />\n\n<p>Though they appear to disagree, both reports support a common conclusion: <strong>adoption of off-the-shelf tools is growing and valued, but companies struggle to build their own AI tools</strong>. Every enterprise AI report I read brings this reality further into focus.</p>\n\n<p>Google Cloud\u2019s \u201c<a href=\"https://services.google.com/fh/files/misc/google_cloud_ai_trends.pdf\">AI Business Trends</a>\u201d report says agents are being widely used\u2026 But their definition of \u201cagent\u201d includes ChatGPT, CoPilot, and Claude.</p>\n\n<p>McKinsey\u2019s \u201c<a href=\"https://www.mckinsey.com/capabilities/quantumblack/our-insights/the-state-of-ai#/\">State of AI</a>\u201d doesn\u2019t include off-the-shelf tools in their survey, and &lt;10% of respondents report having agents beyond the pilot stage.</p>\n\n<p><img alt=\"Less than 10% of respondents report internal AI tools beyond the pilot phase, according to McKinsey.\" src=\"https://www.dbreunig.com/img/mckinsey_ai_2025.jpg\" /></p>\n\n<hr />\n\n<p>So why is it hard for enterprises to build AI tools? In short: <strong>reliability</strong>.</p>\n\n<p>\u201c<a href=\"https://arxiv.org/abs/2512.04123v1\">Measuring Agents in Production</a>\u201d, recent research led by Melissa Pan, brings this to life by surveying over 300 teams who actually have agents in production. The headline?</p>\n\n<blockquote>\n  <p>Reliability remains the top development challenge, driven by difficulties in ensuring and evaluating agent correctness.</p>\n</blockquote>\n\n<p>Rather than develop technical innovations to address this issue, developers dial down their agent ambitions and adopt simple methods and workflows. Most use off-the-shelf large models, with no fine-tuning, and hand-tuned prompts. Agents have short run-times, with 68% of agents executing fewer than 10 steps before requiring human intervention. Chatbot UX dominates, because it keeps a human in the loop: 92.5% of in-production agents deliver their output to humans, not to other software or agents. Pan writes, \u201c<strong>Organizations deliberately constrain agent autonomy to maintain reliability</strong>.\u201d</p>\n\n<p><img alt=\"Agents in production use shorter prompts and few steps.\" src=\"https://www.dbreunig.com/img/map_charts.png\" /></p>\n\n<p>This aligns with data released by OpenRouter this week, in their \u201c<a href=\"https://openrouter.ai/state-of-ai\">State of AI</a>\u201d report. This report analyzed ~100 trillion tokens passing through OpenRouter, using a projection technique to categorize them by use case.</p>\n\n<p>Prompt and sequence<sup id=\"fnref:sequence\"><a class=\"footnote\" href=\"https://www.dbreunig.com/2025/12/06/the-state-of-agents.html#fn:sequence\" rel=\"footnote\">1</a></sup> lengths are steadily growing for programming use cases, while all other categories remain stagnant:</p>\n\n<p><img alt=\"LLM prompt complexity is stagnant, except for coding agents, according to OpenRouter.\" src=\"https://www.dbreunig.com/img/openrouter_programming_vs_world.jpg\" /></p>\n\n<p>The figures above nicely support Pan\u2019s conclusion that agent builders are keeping their agents simple and short to achieve reliability. Outside of coding agents (whose outlier success is a worth a separate discussion), prompts and agent sequence complexity is stagnant.</p>\n\n<p>And these are the agents that make it into production! MIT NANDA showed that leaders say employee \u201cunwillingness to adopt new tools\u201d is the top barrier facing AI pilots. Pan\u2019s results suggest a more sympathetic explanation: <em>when tools are unreliable, employees don\u2019t adopt them</em>. They\u2019re not stubborn; they\u2019re rational.</p>\n\n<p>In the short term, successful teams will build agents with constrained scope, earn trust, then expand. Delivering on bigger ambitions means building and sharing better tools for reliable AI engineering.</p>\n\n<hr />\n\n<form action=\"https://buttondown.com/api/emails/embed-subscribe/dbreunig\" class=\"embeddable-buttondown-form\" method=\"post\" target=\"popupwindow\">\n  <label for=\"bd-email\">Enter your email to receive the occasional update.</label>\n  <div class=\"form-input\">\n    <input id=\"bd-email\" name=\"email\" type=\"email\" />\n    <input type=\"submit\" value=\"Subscribe\" />\n  </div>\n</form>\n<div class=\"footnotes\">\n  <ol>\n    <li id=\"fn:sequence\">\n      <p>\u201cSequence length is a proxy for task complexity and interaction depth.\u201d\u00a0<a class=\"reversefootnote\" href=\"https://www.dbreunig.com/2025/12/06/the-state-of-agents.html#fnref:sequence\">&#8617;</a></p>\n    </li>\n  </ol>\n</div>"
            ],
            "link": "https://www.dbreunig.com/2025/12/06/the-state-of-agents.html",
            "publishedAt": "2025-12-06",
            "source": "Drew Breunig",
            "summary": "Every enterprise AI report tells the same story once you look past the headlines: chatbot adoption is booming, internal pilots are failing. Reliability is the barrier holding agents back.",
            "title": "Enterprise Agents Have a Reliability Problem"
        },
        {
            "content": [],
            "link": "https://harper.blog/notes/2025-12-06_b43ba7ec2fc9_today-i-got-on-the-blueline-an/",
            "publishedAt": "2025-12-06",
            "source": "Harper Reed",
            "summary": "<p>Today I got on the blueline, and the car I got on, apparently, held a cohort of furries headed downtown. Was very cool.</p> <figure> <img alt=\"image_1.jpg\" height=\"1800\" src=\"https://harper.blog/notes/2025-12-06_b43ba7ec2fc9_today-i-got-on-the-blueline-an/image_1.jpg\" width=\"1800\" /> </figure> <figure> <img alt=\"image_2.jpg\" height=\"1800\" src=\"https://harper.blog/notes/2025-12-06_b43ba7ec2fc9_today-i-got-on-the-blueline-an/image_2.jpg\" width=\"1800\" /> </figure> <hr /> <p>Thank you for using RSS. I appreciate you. <a href=\"mailto:harper&#64;modest.com\">Email me</a></p>",
            "title": "Note #300"
        },
        {
            "content": [],
            "link": "https://www.nytimes.com/2025/12/05/style/modern-love-ai-divorce-rebound-with-robot.html",
            "publishedAt": "2025-12-06",
            "source": "Modern Love - NYT",
            "summary": "Raised on a commune, I resisted technology at every step. Until it saved me.",
            "title": "Healing My Heart for 20 Dollars a Month"
        }
    ],
    "lookbackDays": 1,
    "publishDate": "2025-12-06"
}