{
    "articles": [
        {
            "content": [
                "<p>Will short-form non-fiction internet writing go extinct? This may seem like a strange question to ask. After all, short-form non-fiction internet writing is currently, if anything, on the ascent\u2014at least for politics, money, and culture war\u2014driven by the shocking discovery that many people will pay the cost equivalent of four hardback books each year to support their favorite internet writers.</p>\n\n<p>But, particularly for \u201cexplainer\u201d posts, the long-term prospects seem dim. I write about random stuff and then send it to you. If you just want to understand something, why would you read my rambling if AI could explain it equally well, in a style customized for your tastes, and then patiently answer your questions forever?</p>\n\n<p>I mean, say you can explain some topic better than AI. That\u2019s cool, but once you\u2019ve <em>published</em> your explanation, AI companies will put it in their datasets, thankyouverymuch, after which AIs will start regurgitating your explanation. And then\u2014wait a second\u2014suddenly you <em>can\u2019t</em> explain that topic better than AI anymore.</p>\n\n<p>This is all perfectly legal, since you can\u2019t copyright ideas, only presentations of ideas. It used to take work to create a new presentation of someone else\u2019s ideas. And there used to be a social norm to give credit to whoever first came up with some idea. This created <a href=\"https://dynomight.net/ideas/\">incentives to create ideas</a>, even if they weren\u2019t legally protected. But AI can instantly slap a new presentation on your ideas, and no one expects AI to give credit for its training data. Why spend time creating content so just it can be <a href=\"https://dynomight.net/wta-science/#:~:text=nostrification\">nostrified</a> by the Borg? And why <em>read</em> other humans if the Borg will curate their best material for you?</p>\n\n<p>So will the explainer post survive?</p>\n\n<p>Let\u2019s start with an easier question: Already today, AI will happily explain anything. Yet many people read human-written explanations anyway. Why do they do that? I can think of seven reasons:</p>\n\n<ol>\n  <li>\n    <p><strong>Accuracy.</strong> Current AI is unreliable. If I ask about information theory or how to replace the battery on my laptop, it\u2019s very impressive but makes some mistakes. But if I ask about <a href=\"https://dynomight.net/heritable/\">heritability</a>, the answers are three balls of gibberish stacked on top of each other in a trench-coat. Of course, random humans make mistakes, too. But if you find a quality human source, it is far less likely to contain egregious mistakes. This is particularly true across \u201clarge contexts\u201d and for tasks where solutions are hard to verify.</p>\n  </li>\n  <li>\n    <p><strong>AI is boring.</strong> At least, writing from current popular AI tools is boring, by default.</p>\n  </li>\n  <li>\n    <p><strong>Parasocial relationships.</strong> If I\u2019ve been reading someone for a long time, I start to feel like I have a kind of relationship with them. If you\u2019ve followed this blog for a long time, you might feel like you have a relationship with me. Calling these \u201cparasocial relationships\u201d makes them sound sinister, but I think this is normal and actually a clever way of using our tribal-band programming to help us navigate of the modern world. Just like in \u201creal\u201d relationships, when I read someone I have a parasocial relationship with, I have extra context that makes it easier to understand them, I feel a sense of human connection, and I feel like I\u2019m getting a sort of update on their \u201cstory\u201d. I don\u2019t get any of that with (current) AI.</p>\n  </li>\n  <li>\n    <p><strong>Skin in the game.</strong> If a human screws something up, it\u2019s embarrassing. They lose respect and readers. On a meta-level, AI companies have similar incentives not to screw things up. But AI itself doesn\u2019t (seem to) care. Human nature makes it easier to trust someone when we know they\u2019re putting some kind of reputation on the line.</p>\n  </li>\n  <li>\n    <p><strong>Conspicuous consumption.</strong> Since I read <a href=\"https://dynomight.net/reasons-and-persons/\">Reasons and Persons</a>, I can brag to everyone that I read Reasons and Persons. If I had read some equally good AI-written book, probably no one would care.</p>\n  </li>\n  <li>\n    <p><strong>Coordination points.</strong> Partly, I read Reasons and Persons because I liked it. And maybe I guess I read it so I can brag about the fact that I read it. (Hey everyone, have I mentioned that I read Reasons and Persons?) But I also read it because <em>other</em> people read it. When I talk to those people, we have a shared vocabulary and set of ideas that makes it easier to talk about other things. This wouldn\u2019t work if we had all explored the same ideas though fragmented AI \u201ctutoring\u201d.</p>\n  </li>\n  <li>\n    <p><strong>Change is slow.</strong> Here we are 600 years after the invention of the printing press, and the primary mode of advanced education is still for people to physically go to a room where an expert is talking and write down stuff the expert says. If we\u2019re that slow to adapt, then maybe we read human-written explainers simply out of habit.</p>\n  </li>\n</ol>\n\n<p>How much do each of these really matter? How much confidence should they give us that explainer posts will still exist a decade from now? Let\u2019s handle them in reverse order.</p>\n\n<h2 id=\"argument-7-change-is-slow\">Argument 7: Change is slow</h2>\n\n<p>Sure, society takes time to adapt to technological change. But I don\u2019t think college lectures are a good example of this, or that they\u2019re a medieval relic that only survive out of inertia. On the contrary, I think they survive because we haven\u2019t really any other model of education that\u2019s fundamentally better.</p>\n\n<p>Take paper letters. One hundred years ago, these were the primary form of long-distance communication. But after the telephone was widely distributed, it only took it a few decades to kill the letter in almost all cases where the phone is better. When email and texting showed up, they killed off almost all remaining use of paper letters. They still exist, but they\u2019re niche.</p>\n\n<p>The same basic story holds for horses, the telegraph, card catalogs, slide rules, VHS tapes, vacuum tubes, steam engines, ice boxes, answering machines, sailboats, typewriters, the short story, and the divine right of kings. When we have something that\u2019s <em>actually better</em>, we drop the old ways pretty quickly. Inertia alone might keep explainer posts alive for a few years, but not more than that.</p>\n\n<h2 id=\"arguments-5-and-6-coordination-points-and-conspicuous-consumption\">Arguments 5 and 6: Coordination points and conspicuous consumption</h2>\n\n<p>Western civilization began with <a href=\"https://en.wikipedia.org/wiki/Iliad\">the Iliad</a>. Or, at least, we\u2019ve decided to pretend it did. If you read the Iliad, then you can brag about reading the Iliad (good) and you have more context to engage with everyone else who read it (very good). So people keep reading the Iliad. I think this will continue indefinitely.</p>\n\n<p>But so what? The Iliad is in that position because people have been reading/listening to it for thousands of years. But if you write something new and there\u2019s no \u201cnormal\u201d reason to read it, then it has to way to establish that kind of self-sustaining legacy.</p>\n\n<p>Non-fiction in general has a very short half-life. And even when coordination points exist, people often rely on secondary sources anyway. Personally, I\u2019ve tried to read Wittgenstein, but I found it incomprehensible. Yet I think I\u2019ve absorbed his <a href=\"https://plato.stanford.edu/entries/wittgenstein/#MeanUse\">most useful idea</a> by reading other people\u2019s descriptions. I wonder how much \u201cWittgenstein\u201d is really a source at this point as opposed to a label.</p>\n\n<p>Also\u2026 explainer posts typically aren\u2019t the Iliad. So I don\u2019t think this will do much to keep explainer posts alive, either.</p>\n\n<p>(Aside: I\u2019ve never understood why philosophy is so fixated on original sources, instead of continually developing new presentations of old ideas like math and physics do. Is this related to the fact that philosophers go to conferences and literally read their papers out loud?)</p>\n\n<h2 id=\"argument-4-skin-in-the-game\">Argument 4: Skin in the game</h2>\n\n<p>I trust people more when I know they\u2019re putting their reputation on the line, for the same reason I trust restaurants more when I know they rely on repeat customers. AI doesn\u2019t give me this same reason for confidence.</p>\n\n<p>But so what? This is a loose heuristic. If AI were truly more accurate than human writing, I\u2019m sure most people would learn to trust it in a matter of weeks. If AI was ultra-reliable but people <em>really</em> needed someone to hold accountable, AI companies could perhaps offer some kind of \u201cinsurance\u201d. So I don\u2019t see this as keeping explainers alive, either.</p>\n\n<h2 id=\"argument-3-parasocial-relationships\">Argument 3: Parasocial relationships</h2>\n\n<p>Humans are social creatures. If bears had a secret bear Wikipedia and you went to the entry on humans, it would surely say, \u201cHumans are <em>obsessed</em> with relationships.\u201d I feel confident this will remain true.</p>\n\n<p>I also feel confident that we will continue to be interested in what people we like and respect think about matters of fact. It seems plausible that we\u2019ll continue to enjoy getting that information bundled together with little jokes or busts of personality. So I expect our social instincts <em>will</em> provide at least some reason for explainers to survive.</p>\n\n<p>But how strong will this effect be? When explainer posts are read today, what fraction of readers are familiar enough to have a parasocial relationship with the author? Maybe 40%? And when people <em>are</em> familiar, what fraction of their motivation comes from the parasocial relationship, as opposed to just wanting to understand the content? Maybe another 40%? Those are made-up numbers, but I think it\u2019s hard to avoid the conclusion that parasocial relationships explain only a fraction of why people read explainers today.</p>\n\n<p>And there\u2019s another issue. How do parasocial relationships get started if there\u2019s no other reason to read someone? These might keep established authors going for a while at reduced levels, but it seems like it would make it hard for new people to rise up.</p>\n\n<h2 id=\"argument-2-boring-ness\">Argument 2: Boring-ness</h2>\n\n<p>Maybe popular AIs are a <a href=\"https://justismills.substack.com/p/ok-ai-can-write-pretty-good-fiction\">bit boring</a>, today. But I think this is mostly due to the final reinforcement learning step. If you interact with \u201cbase models\u201d, they are very good at picking up style cues and <a href=\"https://dynomight.net/automated/\">not boring at all</a>. So I highly doubt that there\u2019s some fundamental limitation here.</p>\n\n<p>And anyway, does anyone care? If you just want to understand why vitamin D is technically <a href=\"https://en.wikipedia.org/wiki/Secosteroid\">a type of steroid</a>, how much does style really matter, as opposed to clarity? I think style mostly matters in the context of a parasocial relationship, meaning we\u2019ve already accounted for it above.</p>\n\n<h2 id=\"argument-1-accuracy\">Argument 1: Accuracy</h2>\n\n<p>I don\u2019t know for sure if AI will ever be as accurate as a high-quality human source. Though it seems very unlikely that physics somehow precludes creating systems that are more accurate than humans.</p>\n\n<p>But if AI <em>is</em> that accurate, then I think this exercise suggests that explainer posts are basically toast. All the above arguments are just too weak to explain most of why people read human-written explainers now. So I think it\u2019s mostly just accuracy. When that human advantage goes, I expect human-written explainers to go with it.</p>\n\n<h2 id=\"counter-arguments\">Counter-arguments</h2>\n\n<p>I can think of three main counterarguments.</p>\n\n<p>First, maybe AI will fix discovery. Currently, potential readers of explainers often have no way to find potential writers. Search engines have utterly capitulated to SEO spam. Social media soft-bans outward links. If you write for a long time, you can build up an audience, but few people have the time and determination to do that. If you write a single explainer in your life, no one will read it. The rare exceptions to this rule either come from people contributing to established (non-social media) communities or from people with exceptional social connections. So\u2014this argument goes\u2014most potential readers don\u2019t bother trying to find explainers, and most potential writers don\u2019t bother creating them. If AI solves that matching problem, explainers could thrive.</p>\n\n<p>Second, maybe society will figure out some new way to reward people who create information. Maybe we fool around with intellectual property law. Maybe we create some crazy <a href=\"https://en.wikipedia.org/wiki/Project_Xanadu\">Xanadu</a>-like system where in order to read some text, you have to first sign a contract to pay them based on the value you derive, and this is recursively enforced on everyone who\u2019s downstream of you. Hell, maybe AI companies decide to solve the data wall problem by paying people to write stuff. But I doubt it.</p>\n\n<p>Third, maybe explainers will follow a trajectory like chess. Up until perhaps the early 1990s, humans were so much better than computers at chess that computers were irrelevant. After Deep Blue beat Kasparov in 1997, people quickly realized that while computers could beat humans, human+computer teams could still beat computers. This was called <a href=\"https://en.wikipedia.org/wiki/Advanced_chess\">Advanced Chess</a>. Within 15-20 years, however, humans became irrelevant. Maybe there will be a similar Advanced Explainer era? (I kid, that era started five years ago.)</p>\n\n<h2 id=\"tldr\">TLDR</h2>\n\n<p>Will the explainer post go extinct? My guess is mostly yes, if and when AI reaches human-level accuracy.</p>\n\n<p>Incidentally, since there\u2019s so much techno-pessimism these days: I think this outcome would be\u2026 great? It\u2019s a little grim to think of humans all communicating with AI instead of each other, yes. But the upside is all of humanity having access to more accurate and accessible explanations of basically everything. If this is the worst effect of AGI, bring it on.</p>"
            ],
            "link": "https://dynomight.net/explainers/",
            "publishedAt": "2025-10-09",
            "source": "Dynomight",
            "summary": "<p>Will short-form non-fiction internet writing go extinct? This may seem like a strange question to ask. After all, short-form non-fiction internet writing is currently, if anything, on the ascent\u2014at least for politics, money, and culture war\u2014driven by the shocking discovery that many people will pay the cost equivalent of four hardback books each year to support their favorite internet writers.</p> <p>But, particularly for \u201cexplainer\u201d posts, the long-term prospects seem dim. I write about random stuff and then send it to you. If you just want to understand something, why would you read my rambling if AI could explain it equally well, in a style customized for your tastes, and then patiently answer your questions forever?</p> <p>I mean, say you can explain some topic better than AI. That\u2019s cool, but once you\u2019ve <em>published</em> your explanation, AI companies will put it in their datasets, thankyouverymuch, after which AIs will start regurgitating your explanation. And then\u2014wait a second\u2014suddenly you <em>can\u2019t</em> explain that topic better than AI anymore.</p> <p>This is all perfectly legal, since you can\u2019t copyright ideas, only presentations of ideas. It used to take work to create a new presentation of someone else\u2019s ideas. And there used to be a social norm",
            "title": "Will the explainer post go extinct?"
        },
        {
            "content": [
                "<p>If you want to teach a Californian kid about cold, have him fly out to New York in January. Have him pack poorly and then have him walk out of JFK airport at 3pm with aforementioned poor packing, and watch what happens.</p>\n<p>Wait until the sliding door opens.</p>\n<p>Wait for it.</p>\n<p>And watch.</p>\n<p>I did not understand cold until I walked out of JFK in half a winter outfit. It was likely getting punched in the stomach\u2026 with cold. I forgot how to breathe for four seconds. I looked around at compatriots and realized why they stopped INSIDE the terminal to perform what I know is an East Coast (and Midwest) ritual:</p>\n<ol>\n<li>Pull out the scarf, fold it in half, wrap it around the neck, and tie it like a knot.</li>\n<li>Gloves. Wool works, but wool won&#8217;t always save you from deep cold.</li>\n<li>Put on that black pea coat that falls well below your waist, and pop that collar up. </li>\n<li>Beanie. Wool beanie. Over the head.</li>\n<li>You&#8217;re wearing wool socks, right? Gosh, I hope so.</li>\n</ol>\n<p>It took a few trips, but one January, I was so proud of my cold-weather habits that after checking into my Tribeca hotel, I decided to walk around in the bitter bitter cold late at night. Stomping around in my fancy black Chelsea boots and eighteen layers of warmth, I turned a corner in Battery Park and saw floodlights on the Hudson. Walking to the river, I looked over the edge and saw a half-sunken plane in the Hudson River.</p>\n<h2>Are You Prepared?</h2>\n<p>The two books are on the coffee table next to your bed. Both are being read. Jeans on the floor. The cables for the speaker, the clock, and the desk light \u2014 the cables are not organized. Your desk is in the upstairs office. A parking ticket, the reminder to an appointment to get an X-ray for a tooth (probably bad news), both on the left side of the desk. You ate breakfast here this morning, so there is residual\u2026 breakfast on and near the keyboard. A clickable gel pen with the tip out on the right side of the desk, the gel dries over time. A plant, well watered.</p>\n<p>Is this <strong>tidy</strong>?</p>\n<p>Your staff meeting. Three topics are scheduled. We get through just of the before we swerve wildly into an unexpected fourth. The unexpected fourth topic has gravity, inescapable organizational gravity, so you stick with it. It&#8217;s important to orbit this a bit. Important unrelated information is not conveyed. Important and timely topics are not discussed in a critical meeting.</p>\n<p>Are we <strong>organized</strong>?</p>\n<p>The critical presentation. The CEO casually asked you a month ago to define a program (&#8220;document the culture&#8221;) for the whole company. He asked you in front of the entire executive team, and today you are presenting to all of your peers. You didn&#8217;t know it at the time, but the CEO was asking this of you because he didn&#8217;t think you could do it.</p>\n<p>Are you <strong>prepared</strong>?</p>\n<p>Tidy, organized, and prepared. There&#8217;s a deliberate escalating amount of work for each level, going from picking up and folding your jeans all the way to scheduling time with some of the busiest humans in the company to review your drafty thoughts on the slippery topic of culture. Also, at each level, there is a minimum success criterion, an optimal one, and a maximum. For each, there is what you must do, what you should do, and what would represent exceptional work.</p>\n<p>Leadership, especially senior leadership, is built on an influential lie: &#8220;busy people are more successful.&#8221; Now, no one wrote this down and documented this lie. It&#8217;s derived from the sense of urgency for this critical project, this high-performing team, or this can&#8217;t-fail start-up. In each scenario, the incentive is to act as quickly as possible because there is so much at stake.</p>\n<p>The counter to this lie is building a set of small, often inconsequential, thoughtful habits that often pay no obvious immediate dividends, but are essential to your leadership success.</p>\n<ul>\n<li>Being tidy means removing the unnecessary, removing the noise, so that the signal becomes obvious.</li>\n<li>Being organized means taking deliberate action that is efficient, informed, and focused. Organized means eliminating waste. </li>\n<li>Being prepared means anticipating what&#8217;s needed before it&#8217;s urgent, before you know you need it. It means having the right set of tools ready before you need them.</li>\n</ul>\n<p>You can schedule that X-ray appointment that you are dreading \u2014 right now. You can choose to let the rowdy, unexpected 4th staff meeting topic go much longer than expected. You can proactively Slack the CEO right after his request and ask, &#8220;I&#8217;d like to know what success looks like for this project.&#8221;</p>\n<p>These are short-term investments in the now, but more importantly, they are long-term investments in your ability to find signal, stay focused, and act thoughtfully when disaster strikes.</p>\n<h2>Two Minutes and Forty Nine Seconds</h2>\n<p>On January 15, 2009, US Airways flight 1549 took off from LaGuardia, and roughly one minute after take-off, the plane hit a flock of Canadian geese, which disabled both engines. The time from when a flock of Canadian geese struck US Airways Flight 1549 to when the plane successfully made an unpowered ditch in the Hudson River was two minutes and forty-nine seconds. For the crew and passengers on that plane, that was the longest two minutes and forty-nine seconds of their lives. To everyone else, it&#8217;s just under three minutes. You&#8217;ve likely spent more time reading this article.</p>\n<p>The now-famous captain of that flight, Chelsea &#8220;Sully&#8221; Sullenberger, said, &#8220;One way of looking at this might be that for 42 years, I&#8217;ve been making small, regular deposits in this bank of experience, education, and training. And on January 15, the balance was sufficient so that I could make a very large withdrawal.&#8221;</p>\n<p>Disaster strikes unexpectedly. It&#8217;s unfamiliar. Its magnitude is hard to fathom. You don&#8217;t know what information you need, nor how quickly. Your irrational intuition is to freeze and pray for the danger to pass, which usually means additional disaster.</p>\n<p>Find signal, stay focused, and act thoughtfully. It reads like common sense right now, but that&#8217;s because you&#8217;re reading this in a coffee shop. The biggest disaster currently facing you is the slight chance you spill that flat white on your notebook.</p>\n<p>I&#8217;ve listened to the cockpit recordings of flight 1549, I&#8217;ve watched the interviews, and I\u2019ve seen the movie. What struck me was the captain&#8217;s calm response to impending doom. My impression is that he is a human, but my experience is that the habits and skills he used to save those lives are ones you practice every day when the stakes are far lower.</p>\n<p>I went back the next day to the Hudson. The plane on a crane was being moved to a barge. Surreal to see a machine meant to fly hanging from a crane. At this point, little was known of what actually happened on the flight except that everyone had survived, and it appeared it was because a competent leader had acted calmly and thoughtfully when it mattered most.</p>"
            ],
            "link": "https://randsinrepose.com/archives/the-plane-on-the-crane/",
            "publishedAt": "2025-10-09",
            "source": "Rands in Repose",
            "summary": "If you want to teach a Californian kid about cold, have him fly out to New York in January. Have him pack poorly and then have him walk out of JFK airport at 3pm with aforementioned poor packing, and watch what happens. Wait until the sliding door opens. Wait for it. And watch. I did&#8230; <a class=\"excerpt-more\" href=\"https://randsinrepose.com/archives/the-plane-on-the-crane/\">more</a>",
            "title": "The Plane on the Crane"
        },
        {
            "content": [],
            "link": "https://www.robinsloan.com/lab/clarity/",
            "publishedAt": "2025-10-09",
            "source": "Robin Sloan",
            "summary": "<p>AI and LLMs, science and language. <a href=\"https://www.robinsloan.com/lab/clarity/\">Read here.</a></p>",
            "title": "Clarity"
        },
        {
            "content": [],
            "link": "https://www.robinsloan.com/lab/distance-of-leverage/",
            "publishedAt": "2025-10-09",
            "source": "Robin Sloan",
            "summary": "<p>I prefer to stay in close. <a href=\"https://www.robinsloan.com/lab/distance-of-leverage/\">Read here.</a></p>",
            "title": "The distance of leverage"
        },
        {
            "content": [
                "<p>For the past couple of months, I have been working on <strong><a href=\"https://deepfuture.now/\">Deep Future</a></strong>, an AI agent for scenario planning. It&#8217;s like Deep Research but for strategic foresight.</p><p>Picture this: you&#8217;re reading a news story about rare-earth minerals. Curious, you fire up the Deep Future agent and run a rapid analysis. In the time it takes to finish your morning cup of coffee, it has developed a detailed analysis covering supply chains, chips, defense tech, possible scenarios, a list of the forces driving change, early warning signals, and strategic leverage points. Deep Future has completed a multi-day strategic analysis in 15 min.</p><p>My research in this area has been supported by <a href=\"https://www.flf.org/\">The Future of Life Foundation</a> as part of the <a href=\"https://www.flf.org/fellowship\">AI for Human Reasoning</a> program. FLF believes AI can augment intelligence. I agree. If you follow this newsletter, you&#8217;ll know that I&#8217;ve long been interested in <a href=\"https://newsletter.squishy.computer/p/tools-for-thought-in-your-ooda-loop\">using AI to accelerate the OODA loop</a>. Scenario planning is a promising place to start.</p><h3>Thinking in scenarios</h3><blockquote><p>Everyone has a plan until they get punched in the face.<br />- Mike Tyson</p></blockquote><p>Scenario planning emerged during the Cold War, when the US military and RAND started to adapt ideas from game theory and systems theory to make sense of the new strategic landscape. At its core, scenario planning tackles a fundamental challenge: how do you make plans in an unpredictable environment?</p><p>When your environment is predictable, strategy is simple. You identify a goal, plan steps toward your goal, carry them out. But our environment is seldom predictable. We live in a <a href=\"https://newsletter.squishy.computer/p/the-shape-of-network-society\">networked world</a>, a world governed by asymmetry, feedback loops, and power laws. The US Army and Navy War College has an acronym for this kind of world: <strong><a href=\"https://en.wikipedia.org/wiki/VUCA\">VUCA</a></strong>.</p><p>Volatile<br />Uncertain<br />Complex<br />Ambiguous</p><p>Plans that work in stable environments fail in VUCA environments. They are too brittle, because they depend upon prediction in an unpredictable world. VUCA environments demand a different approach. But if you can&#8217;t predict, how do you plan?</p><div class=\"captioned-image-container\"><figure><a class=\"image-link image2 is-viewable-img\" href=\"https://substackcdn.com/image/fetch/$s_!p_5I!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F56a2d0d7-c6ac-48e9-b158-d903ffab2510_2454x1212.png\" target=\"_blank\"><div class=\"image2-inset image2-full-screen\"><source type=\"image/webp\" /><img alt=\"\" class=\"sizing-fullscreen\" src=\"https://substackcdn.com/image/fetch/$s_!p_5I!,w_5760,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F56a2d0d7-c6ac-48e9-b158-d903ffab2510_2454x1212.png\" title=\"\" /><div class=\"image-link-expand\"><div class=\"pencraft pc-display-flex pc-gap-8 pc-reset\"><div class=\"pencraft pc-reset icon-container restack-image\"><svg class=\"lucide lucide-refresh-cw\" fill=\"none\" height=\"20\" stroke=\"currentColor\" stroke-linecap=\"round\" stroke-linejoin=\"round\" stroke-width=\"2\" viewBox=\"0 0 24 24\" width=\"20\" xmlns=\"http://www.w3.org/2000/svg\"><path d=\"M3 12a9 9 0 0 1 9-9 9.75 9.75 0 0 1 6.74 2.74L21 8\"></path><path d=\"M21 3v5h-5\"></path><path d=\"M21 12a9 9 0 0 1-9 9 9.75 9.75 0 0 1-6.74-2.74L3 16\"></path><path d=\"M8 16H3v5\"></path></svg></div><div class=\"pencraft pc-reset icon-container view-image\"><svg class=\"lucide lucide-maximize2 lucide-maximize-2\" fill=\"none\" height=\"20\" stroke=\"currentColor\" stroke-linecap=\"round\" stroke-linejoin=\"round\" stroke-width=\"2\" viewBox=\"0 0 24 24\" width=\"20\" xmlns=\"http://www.w3.org/2000/svg\"><polyline points=\"15 3 21 3 21 9\"></polyline><polyline points=\"9 21 3 21 3 15\"></polyline><line x1=\"21\" x2=\"14\" y1=\"3\" y2=\"10\"></line><line x1=\"3\" x2=\"10\" y1=\"21\" y2=\"14\"></line></svg></div></div></div></div></a><figcaption class=\"image-caption\">Futures Cone. Source: <a href=\"https://www.researchgate.net/publication/13166132_Possible_futures_preferable_futures\">Hancock, Bezold (1994)</a></figcaption></figure></div><p>Picture your future as a cone. This cone represents the space of all possible outcomes: good, bad, everything in-between. The further into the future we go, the wider the cone of possibilities becomes.</p><p>Traditional planning traces a single path through this cone, the <strong>preferable</strong> path, a direct path toward your goal. But this is like drawing a line between two points and claiming you&#8217;ve drawn a map. Sure, we can follow that line and hope for the best, but the line doesn&#8217;t give us any sense of scale or orientation. It doesn&#8217;t tell us anything about the territory. While following that line, we might run into mountains, get mired in swamps, or walk off of cliffs. Worse still, the further we get into the future, the less predictive the line will be! Any errors will compound, until we find ourselves way off of our intended course.</p><p>Drawing one line isn&#8217;t enough. The preferable path articulates what we <em>hope</em> will happen, but hope is not a strategy. We need to map more territory! This is where scenario planning comes in.</p><p>Scenario planning traces <em>multiple</em> paths through the cone of possibility. Each of these paths cuts through a different part of the cone. Between them, we end up covering a broad radius of potential outcomes. Tracing these paths is <em>not</em> about predicting. It is about exploring the space of the probable, plausible, and possible.</p><p>How does this work?</p><div class=\"captioned-image-container\"><figure><a class=\"image-link image2 is-viewable-img\" href=\"https://substackcdn.com/image/fetch/$s_!Z58Q!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3f612179-187d-49ff-a588-b3049ff5cb8b_1920x800.jpeg\" target=\"_blank\"><div class=\"image2-inset image2-full-screen\"><source type=\"image/webp\" /><img alt=\"\" class=\"sizing-fullscreen\" src=\"https://substackcdn.com/image/fetch/$s_!Z58Q!,w_5760,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3f612179-187d-49ff-a588-b3049ff5cb8b_1920x800.jpeg\" title=\"\" /><div class=\"image-link-expand\"><div class=\"pencraft pc-display-flex pc-gap-8 pc-reset\"><div class=\"pencraft pc-reset icon-container restack-image\"><svg class=\"lucide lucide-refresh-cw\" fill=\"none\" height=\"20\" stroke=\"currentColor\" stroke-linecap=\"round\" stroke-linejoin=\"round\" stroke-width=\"2\" viewBox=\"0 0 24 24\" width=\"20\" xmlns=\"http://www.w3.org/2000/svg\"><path d=\"M3 12a9 9 0 0 1 9-9 9.75 9.75 0 0 1 6.74 2.74L21 8\"></path><path d=\"M21 3v5h-5\"></path><path d=\"M21 12a9 9 0 0 1-9 9 9.75 9.75 0 0 1-6.74-2.74L3 16\"></path><path d=\"M8 16H3v5\"></path></svg></div><div class=\"pencraft pc-reset icon-container view-image\"><svg class=\"lucide lucide-maximize2 lucide-maximize-2\" fill=\"none\" height=\"20\" stroke=\"currentColor\" stroke-linecap=\"round\" stroke-linejoin=\"round\" stroke-width=\"2\" viewBox=\"0 0 24 24\" width=\"20\" xmlns=\"http://www.w3.org/2000/svg\"><polyline points=\"15 3 21 3 21 9\"></polyline><polyline points=\"9 21 3 21 3 15\"></polyline><line x1=\"21\" x2=\"14\" y1=\"3\" y2=\"10\"></line><line x1=\"3\" x2=\"10\" y1=\"21\" y2=\"14\"></line></svg></div></div></div></div></a><figcaption class=\"image-caption\">Loki, exploring a wide range of possible futures.</figcaption></figure></div><blockquote><p>The future is already here, it&#8217;s just not evenly distributed.<br /><em>-William Gibson</em></p></blockquote><p>We can&#8217;t predict the future, but we can <strong>identify the forces that are driving change</strong> in our environment today. This process of mapping drivers is not speculative. Drivers are <a href=\"https://www.andyhinesight.com/introducing-tippos-trends-issues-plans-projections-and-obstacles/\">trends, issues, projections, obstacles</a> that we can point to, that are happening <em>now, </em>things like demographic changes, technological adoption curves, economic bubbles, environmental tipping points, political transitions&#8230; These forces have mass and momentum. If we see them driving change today, we know they will drive change tomorrow.</p><div class=\"captioned-image-container\"><figure><a class=\"image-link image2 is-viewable-img\" href=\"https://substackcdn.com/image/fetch/$s_!BQwf!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F072d9536-fa8e-4e38-8e8f-ae51aaea8f03_1490x1556.png\" target=\"_blank\"><div class=\"image2-inset\"><source type=\"image/webp\" /><img alt=\"\" class=\"sizing-normal\" height=\"1520\" src=\"https://substackcdn.com/image/fetch/$s_!BQwf!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F072d9536-fa8e-4e38-8e8f-ae51aaea8f03_1490x1556.png\" title=\"\" width=\"1456\" /><div class=\"image-link-expand\"><div class=\"pencraft pc-display-flex pc-gap-8 pc-reset\"><div class=\"pencraft pc-reset icon-container restack-image\"><svg class=\"lucide lucide-refresh-cw\" fill=\"none\" height=\"20\" stroke=\"currentColor\" stroke-linecap=\"round\" stroke-linejoin=\"round\" stroke-width=\"2\" viewBox=\"0 0 24 24\" width=\"20\" xmlns=\"http://www.w3.org/2000/svg\"><path d=\"M3 12a9 9 0 0 1 9-9 9.75 9.75 0 0 1 6.74 2.74L21 8\"></path><path d=\"M21 3v5h-5\"></path><path d=\"M21 12a9 9 0 0 1-9 9 9.75 9.75 0 0 1-6.74-2.74L3 16\"></path><path d=\"M8 16H3v5\"></path></svg></div><div class=\"pencraft pc-reset icon-container view-image\"><svg class=\"lucide lucide-maximize2 lucide-maximize-2\" fill=\"none\" height=\"20\" stroke=\"currentColor\" stroke-linecap=\"round\" stroke-linejoin=\"round\" stroke-width=\"2\" viewBox=\"0 0 24 24\" width=\"20\" xmlns=\"http://www.w3.org/2000/svg\"><polyline points=\"15 3 21 3 21 9\"></polyline><polyline points=\"9 21 3 21 3 15\"></polyline><line x1=\"21\" x2=\"14\" y1=\"3\" y2=\"10\"></line><line x1=\"3\" x2=\"10\" y1=\"21\" y2=\"14\"></line></svg></div></div></div></div></a><figcaption class=\"image-caption\">World3 causal loop diagram, Club of Rome, 1972</figcaption></figure></div><blockquote><p>Systems fool us by presenting themselves as a series of events.<br /><em>-Donella Meadows</em></p></blockquote><p>Once we have mapped the relevant forces, we can begin exploring how they are connected into <strong>feedback loops</strong> that generate <strong>exponential growth</strong>, <strong>collapse</strong>, <strong>stasis</strong>, or <strong>transformation</strong>. We can also look at how forces sort into <strong><a href=\"https://newsletter.squishy.computer/i/53901934/layering-allows-evolution-to-proceed-at-multiple-speeds\">pace layers</a></strong>, revealing what is changing quickly, and what is changing slowly. Where fast and slow collide, we find pivot points.</p><div class=\"captioned-image-container\"><figure><a class=\"image-link image2\" href=\"https://newsletter.squishy.computer/i/53901934/layering-allows-evolution-to-proceed-at-multiple-speeds\" target=\"_blank\"><div class=\"image2-inset\"><source type=\"image/webp\" /><img alt=\"\" class=\"sizing-normal\" height=\"556.7783417935702\" src=\"https://substackcdn.com/image/fetch/$s_!GMnW!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Fd92d5e08-48cd-4092-9b67-7be84c84a8cf_591x452.png\" width=\"728\" /><div></div></div></a></figure></div><p>Analyzing forces as systems can help us break out of the trap of linear thinking:</p><blockquote><p>The human mind is not adapted to interpreting how social systems behave. Social systems belong to the class called multi-loop nonlinear feedback systems. In the long history of evolution it has not been necessary until very recent historical times for people to understand complex feedback systems. Evolutionary processes have not given us the mental ability to interpret properly the dynamic behavior of those complex systems in which we are now embedded.<br /><em>(Jay Forrester, 1971. &#8220;<a href=\"https://ocw.mit.edu/courses/15-988-system-dynamics-self-study-fall-1998-spring-1999/65cdf0faf132dec7ec75e91f9651b31f_behavior.pdf\">Counterintuitive behavior of social systems</a>&#8221;)</em></p></blockquote><blockquote><p>The complex processes we call &#8220;systems&#8221; present special challenges to our uneducated imaginations. We tend to think additively, and are constantly surprised when something that seems to be &#8220;just added in&#8221; causes surprising and often disastrous changes. For example, in 1859 in Australia, Thomas Austin said &#8220;The introduction of a few rabbits could do little harm and might provide a touch of home, in addition to a spot of hunting.&#8221; The result was not the Australian ecology + rabbits, but an entirely new ecology, which in many cases became a landscape of ruins. None of the efforts since then to &#8220;subtract&#8221; the rabbits from the ecology have come close to working.<br /><em>(Alan Kay, 2005. &#8220;<a href=\"https://worrydream.com/refs/Kay_2005_-_Enlightened_Imagination_for_Citizens.html\">Enlightened Imagination For Citizens</a>&#8221;)</em></p></blockquote><p>Twice the cause does not mean twice the effect. Actual results will depend upon system structure. Mapping the relationships between forces reveals places where nonlinear change is likely to occur.</p><p>Some confluences of forces will be so powerful that they form the outline of an obvious scenario&#8212;a large-scale outcome driven by the interaction of multiple forces. After identifying a few of these scenarios, we can triangulate between multiple possible outcomes, and form strategies that are robust across many futures. We can even create multiple plans, <a href=\"https://en.wikipedia.org/wiki/Contingency_plan\">contingency plans</a>, that we can deploy if we see ourselves moving toward one scenario or another.</p><p>Instead of planning from the inside-out, projecting our hopes onto the environment, we are now planning from the outside-in. We&#8217;re mapping the possibility space, then charting paths through it!</p><div class=\"captioned-image-container\"><figure><a class=\"image-link image2 is-viewable-img\" href=\"https://substackcdn.com/image/fetch/$s_!yZ3T!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0eff9182-7776-4658-bbf0-028dd91518c0_1738x1128.png\" target=\"_blank\"><div class=\"image2-inset\"><source type=\"image/webp\" /><img alt=\"\" class=\"sizing-normal\" height=\"945\" src=\"https://substackcdn.com/image/fetch/$s_!yZ3T!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0eff9182-7776-4658-bbf0-028dd91518c0_1738x1128.png\" title=\"\" width=\"1456\" /><div class=\"image-link-expand\"><div class=\"pencraft pc-display-flex pc-gap-8 pc-reset\"><div class=\"pencraft pc-reset icon-container restack-image\"><svg class=\"lucide lucide-refresh-cw\" fill=\"none\" height=\"20\" stroke=\"currentColor\" stroke-linecap=\"round\" stroke-linejoin=\"round\" stroke-width=\"2\" viewBox=\"0 0 24 24\" width=\"20\" xmlns=\"http://www.w3.org/2000/svg\"><path d=\"M3 12a9 9 0 0 1 9-9 9.75 9.75 0 0 1 6.74 2.74L21 8\"></path><path d=\"M21 3v5h-5\"></path><path d=\"M21 12a9 9 0 0 1-9 9 9.75 9.75 0 0 1-6.74-2.74L3 16\"></path><path d=\"M8 16H3v5\"></path></svg></div><div class=\"pencraft pc-reset icon-container view-image\"><svg class=\"lucide lucide-maximize2 lucide-maximize-2\" fill=\"none\" height=\"20\" stroke=\"currentColor\" stroke-linecap=\"round\" stroke-linejoin=\"round\" stroke-width=\"2\" viewBox=\"0 0 24 24\" width=\"20\" xmlns=\"http://www.w3.org/2000/svg\"><polyline points=\"15 3 21 3 21 9\"></polyline><polyline points=\"9 21 3 21 3 15\"></polyline><line x1=\"21\" x2=\"14\" y1=\"3\" y2=\"10\"></line><line x1=\"3\" x2=\"10\" y1=\"21\" y2=\"14\"></line></svg></div></div></div></div></a><figcaption class=\"image-caption\">Source: <a href=\"https://www.futuribles.com/wp-content/uploads/2020/01/ToolBox5The_2x2_MatrixTechnique.pdf?postId=73709\">Rhydderch, 2017</a></figcaption></figure></div><p>It&#8217;s a process that produces surprising insights. My favorite thing about scenario planning is how it changes my understanding of the present.</p><h3>Accelerating scenarios with AI</h3><p>So where does AI come in? Scenario planning goes through several structured research stages:</p><div class=\"captioned-image-container\"><figure><a class=\"image-link image2 is-viewable-img\" href=\"https://substackcdn.com/image/fetch/$s_!llZp!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc5748cbc-34ca-4a2b-bffd-f66c94490e45_2494x1312.png\" target=\"_blank\"><div class=\"image2-inset\"><source type=\"image/webp\" /><img alt=\"\" class=\"sizing-large\" height=\"631.3186813186813\" src=\"https://substackcdn.com/image/fetch/$s_!llZp!,w_2400,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc5748cbc-34ca-4a2b-bffd-f66c94490e45_2494x1312.png\" width=\"1200\" /><div class=\"image-link-expand\"><div class=\"pencraft pc-display-flex pc-gap-8 pc-reset\"><div class=\"pencraft pc-reset icon-container restack-image\"><svg class=\"lucide lucide-refresh-cw\" fill=\"none\" height=\"20\" stroke=\"currentColor\" stroke-linecap=\"round\" stroke-linejoin=\"round\" stroke-width=\"2\" viewBox=\"0 0 24 24\" width=\"20\" xmlns=\"http://www.w3.org/2000/svg\"><path d=\"M3 12a9 9 0 0 1 9-9 9.75 9.75 0 0 1 6.74 2.74L21 8\"></path><path d=\"M21 3v5h-5\"></path><path d=\"M21 12a9 9 0 0 1-9 9 9.75 9.75 0 0 1-6.74-2.74L3 16\"></path><path d=\"M8 16H3v5\"></path></svg></div><div class=\"pencraft pc-reset icon-container view-image\"><svg class=\"lucide lucide-maximize2 lucide-maximize-2\" fill=\"none\" height=\"20\" stroke=\"currentColor\" stroke-linecap=\"round\" stroke-linejoin=\"round\" stroke-width=\"2\" viewBox=\"0 0 24 24\" width=\"20\" xmlns=\"http://www.w3.org/2000/svg\"><polyline points=\"15 3 21 3 21 9\"></polyline><polyline points=\"9 21 3 21 3 15\"></polyline><line x1=\"21\" x2=\"14\" y1=\"3\" y2=\"10\"></line><line x1=\"3\" x2=\"10\" y1=\"21\" y2=\"14\"></line></svg></div></div></div></div></a><figcaption class=\"image-caption\">Source: <a href=\"https://www.futuribles.com/wp-content/uploads/2020/01/ToolBox5The_2x2_MatrixTechnique.pdf?postId=73709\">Rhydderch, 2017</a></figcaption></figure></div><ol><li><p><strong>Framing</strong>: Establish a focal question with clear timeframe and scope to guide our research (e.g. &#8220;what will cancer research look like in 10 years?&#8221;).</p></li><li><p><strong>Environmental scanning</strong>: Catalog the forces driving change across multiple areas (Social, Technological, Economic, Environmental, Political&#8230;). Map major actors in the environment. Build up a database of research.</p></li><li><p><strong>Structural analysis</strong>: Perform various structural analyses, looking at actors and forces, how they interact. Rank forces by impact and uncertainty, pace layer, etc.</p></li><li><p><strong>Scenario identification</strong>: Identify a cluster of pivotal forces, typically high-impact forces with uncertainties related to our focal question. Explore how these forces might collide, e.g. by crossing forces up in a <a href=\"https://www.futuribles.com/wp-content/uploads/2020/01/ToolBox5The_2x2_MatrixTechnique.pdf?postId=73709\">2x2 matrix</a>, or by doing a <a href=\"https://en.wikipedia.org/wiki/Cross_impact_analysis\">cross-impact analysis</a>. This yields four or more windows into possible futures.</p></li><li><p><strong>Scenario development</strong>: With four or more representative scenarios identified, explore scenarios in-depth. Develop descriptions that help us understand what these futures might look like, how actors might behave within them, what this implies for our focal question. Techniques like <a href=\"https://en.wikipedia.org/wiki/Wargame\">wargaming</a> can also be used to dynamically explore scenarios.</p></li><li><p><strong>Signpost monitoring</strong>: Develop a list of indicators that act as early warning signals for one scenario or another. Track them regularly.</p></li><li><p><strong>Strategic development</strong>: Identify strategies to meet threats and achieve goals across a wide range of scenarios. Develop contingency plans. Identify new uncertainties and questions for future research.</p></li></ol><p>Digging into these stages reveals multiple opportunities for AI to automate or augment the research process. Agents can help search for signals during environmental scanning, work with you to identify forces, perform automated analysis, explore thousands of scenarios, monitor signposts, and more. The data we generate during scanning acts as a ground-truthing mechanism, allowing us to trace our assumptions all the way back to the scanning hits&#8212;facts, events, trends&#8212;that started our analysis.</p><p>Scenario planning has well-developed research methods and processes (TAIDA, Houston Framework Foresight, etc). At the same time, these processes are not simple, fixed, or linear. We might need to change the methods we use within a stage depending on the question, the number of participants, and the context. We might loop back to earlier stages as we learn from our research. It&#8217;s the same kind of fluid domain faced by Deep Research:</p><blockquote><p>Research work involves open-ended problems where it&#8217;s very difficult to predict the required steps in advance. You can&#8217;t hardcode a fixed path for exploring complex topics, as the process is inherently dynamic and path-dependent. When people conduct research, they tend to continuously update their approach based on discoveries, following leads that emerge during investigation.</p><p>This unpredictability makes AI agents particularly well-suited for research tasks. Research demands the flexibility to pivot or explore tangential connections as the investigation unfolds. The model must operate autonomously for many turns, making decisions about which directions to pursue based on intermediate findings. A linear, one-shot pipeline cannot handle these tasks.</p><p><em>(Anthropic, 2025. &#8220;<a href=\"https://www.anthropic.com/engineering/multi-agent-research-system\">How we built our multi-agent research system</a>&#8221;.)</em></p></blockquote><p>This kind of dynamic process requires a facilitator with strong intuitions for when to move forward, when to loop back, and when to dive deeper. As Deep Research has shown, agents are surprisingly good at this!</p><p>I&#8217;m still early in the development process, but the prototypes already feel promising. Deep Future agents are equipped with a <a href=\"https://arxiv.org/abs/2310.08560\">MemGPT</a>-like memory system, a handful of tools, and a library of prompts adapted from scenario methods. They can successfully guide you through defining a focal question, identifying forces, analyzing structural connections, and making strategic recommendations. I&#8217;m pretty happy with the early results&#8212;not as good as an experienced facilitator, but then speed has a quality all its own.</p><p>There is also a lot of headroom for improvement. <a href=\"https://www.forecastbench.org/\">LLMs are getting better at forecasting</a>, a skillset that has critical overlap with strategic foresight. We may soon have LLMs that can predict as accurately as a <a href=\"https://en.wikipedia.org/wiki/Superforecaster\">superforecaster</a>.</p><div class=\"captioned-image-container\"><figure><a class=\"image-link image2\" href=\"https://blog.lightningrod.ai/p/foresight-32b-beats-frontier-llms-on-live-polymarket-predictions\" target=\"_blank\"><div class=\"image2-inset\"><source type=\"image/webp\" /><img alt=\"\" class=\"sizing-normal\" height=\"583\" src=\"https://substackcdn.com/image/fetch/$s_!HqPe!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2919ce19-566b-4aa3-b4ed-6e34ecdafdff_1508x604.png\" width=\"1456\" /><div></div></div></a><figcaption class=\"image-caption\">Lightning Rod Labs, 2025 &#8220;<a href=\"https://blog.lightningrod.ai/p/foresight-32b-beats-frontier-llms-on-live-polymarket-predictions\">Foresight-32B Beats Frontier LLMs on Live Polymarket Predictions</a>&#8221;</figcaption></figure></div><p>And there are areas where AI can already outperform us. Agents are patient. They never sleep. They can do continual environmental scanning, ingesting news feeds, identifying driving forces, updating scenario models, and sending notifications the very millisecond an early warning signal is detected. This kind of signals intelligence used to require a dedicated team of researchers. Now AI brings it within reach of individuals.</p><p>Ultimately, by dropping the cost of scenario analysis from days to minutes, we can begin to do <em>high-frequency scenario planning</em>, applying the power of scenario-thinking to more questions, more often.</p><div><hr /></div><p><em>I&#8217;m developing Deep Future alongside a limited number of exclusive founding partners. Interested? Reply to this email, or <strong><a href=\"http://forms.gle/h7m4ZgSu7hD3hXZk8\">join the waitlist</a></strong>.</em></p>"
            ],
            "link": "https://newsletter.squishy.computer/p/deep-future",
            "publishedAt": "2025-10-09",
            "source": "Squishy Computer",
            "summary": "<p>For the past couple of months, I have been working on <strong><a href=\"https://deepfuture.now/\">Deep Future</a></strong>, an AI agent for scenario planning. It&#8217;s like Deep Research but for strategic foresight.</p><p>Picture this: you&#8217;re reading a news story about rare-earth minerals. Curious, you fire up the Deep Future agent and run a rapid analysis. In the time it takes to finish your morning cup of coffee, it has developed a detailed analysis covering supply chains, chips, defense tech, possible scenarios, a list of the forces driving change, early warning signals, and strategic leverage points. Deep Future has completed a multi-day strategic analysis in 15 min.</p><p>My research in this area has been supported by <a href=\"https://www.flf.org/\">The Future of Life Foundation</a> as part of the <a href=\"https://www.flf.org/fellowship\">AI for Human Reasoning</a> program. FLF believes AI can augment intelligence. I agree. If you follow this newsletter, you&#8217;ll know that I&#8217;ve long been interested in <a href=\"https://newsletter.squishy.computer/p/tools-for-thought-in-your-ooda-loop\">using AI to accelerate the OODA loop</a>. Scenario planning is a promising place to start.</p><h3>Thinking in scenarios</h3><blockquote><p>Everyone has a plan until they get punched in the face.<br />- Mike Tyson</p></blockquote><p>Scenario planning emerged during the Cold War, when the US military and RAND started to adapt ideas from game theory and systems theory to",
            "title": "Deep Future"
        },
        {
            "content": [
                "<p>OpenAI is making deals and shipping products. They locked in their $500 billion valuation and then got 10% of AMD in exchange for buying a ton of chips. They gave us the ability to \u2018chat with apps\u2019 inside of ChatGPT. They walked back their insane Sora copyright and account deletion policies and are buying $50 million in marketing. They\u2019ve really got a lot going on right now.</p>\n<p>Of course, everyone else also has a lot going on right now. It\u2019s AI. I spent the last weekend <a href=\"https://thezvi.substack.com/p/bending-the-curve?r=67wny\"><strong>at a great AI conference at Lighthaven called The Curve</strong></a>.</p>\n<p>The other big news that came out this morning is that <a href=\"https://x.com/deanwball/status/1976260051351343195\">China is asserting sweeping extraterritorial control over rare earth metals</a>. This is likely China\u2019s biggest card short of full trade war or worse, and it is being played in a hugely escalatory way that America obviously can\u2019t accept. Presumably this is a negotiating tactic, but when you put something like this on the table and set it in motion, it can get used for real whether or not you planned on using it. If they don\u2019t back down, there is no deal and China attempts to enforce this for real, things could get very ugly, very quickly, for all concerned.</p>\n<div>\n\n\n<span id=\"more-24775\"></span>\n\n\n</div>\n<p>For now the market (aside from mining stocks) is shrugging this off, as part of its usual faith that everything will work itself out. I wouldn\u2019t be so sure.</p>\n\n\n<h4 class=\"wp-block-heading\">Table of Contents</h4>\n\n\n<ol>\n<li><a href=\"https://thezvi.substack.com/i/175104594/language-models-offer-mundane-utility\">Language Models Offer Mundane Utility.</a> If you didn\u2019t realize, it\u2019s new to you.</li>\n<li><a href=\"https://thezvi.substack.com/i/175104594/language-models-don-t-offer-mundane-utility\">Language Models Don\u2019t Offer Mundane Utility.</a> Some tricky unsolved problems.</li>\n<li><a href=\"https://thezvi.substack.com/i/175104594/huh-upgrades\"><strong>Huh, Upgrades.</strong></a> OpenAI offers AgentKit and other Dev Day upgrades.</li>\n<li><a href=\"https://thezvi.substack.com/i/175104594/chat-with-apps\"><strong>Chat With Apps</strong>.</a> The big offering is Chat With Apps, if execution was good.</li>\n<li><a href=\"https://thezvi.substack.com/i/175104594/on-your-marks\">On Your Marks.</a> We await new results.</li>\n<li><a href=\"https://thezvi.substack.com/i/175104594/choose-your-fighter\">Choose Your Fighter.</a> Claude Code and Codex CLI both seem great.</li>\n<li><a href=\"https://thezvi.substack.com/i/175104594/fun-with-media-generation\">Fun With Media Generation.</a> Sora backs down, Grok counteroffers with porn.</li>\n<li><a href=\"https://thezvi.substack.com/i/175104594/deepfaketown-and-botpocalypse-soon\">Deepfaketown and Botpocalypse Soon.</a> Okay, yeah, we have a problem.</li>\n<li><a href=\"https://thezvi.substack.com/i/175104594/you-drive-me-crazy\">You Drive Me Crazy.</a> How might we not do that?</li>\n<li><a href=\"https://thezvi.substack.com/i/175104594/they-took-our-jobs\">They Took Our Jobs.</a> I mean we all know they will, but did they do it already?</li>\n<li><a href=\"https://thezvi.substack.com/i/175104594/the-art-of-the-jailbreak\">The Art of the Jailbreak.</a> Don\u2019t you say his name.</li>\n<li><a href=\"https://thezvi.substack.com/i/175104594/get-involved\">Get Involved.</a> Request for information, FAI fellowship, OpenAI grants.</li>\n<li><a href=\"https://thezvi.substack.com/i/175104594/introducing\">Introducing.</a> CodeMender, Google\u2019s AI that will \u2018automatically\u2019 fix your code.</li>\n<li><a href=\"https://thezvi.substack.com/i/175104594/in-other-ai-news\">In Other AI News.</a> Alibaba robotics, Anthropic business partnerships.</li>\n<li><a href=\"https://thezvi.substack.com/i/175104594/get-to-work\">Get To Work.</a> We could have 7.4 million remote workers, or some Sora videos.</li>\n<li><a href=\"https://thezvi.substack.com/i/175104594/show-me-the-money\"><strong>Show Me the Money</strong>.</a> The deal flow is getting a little bit complex.</li>\n<li><a href=\"https://thezvi.substack.com/i/175104594/quiet-speculations\">Quiet Speculations.</a> Ah, remembering the old aspirations.</li>\n<li><a href=\"https://thezvi.substack.com/i/175104594/the-quest-for-sane-regulations\">The Quest for Sane Regulations.</a> Is there a deal to be made? With who?</li>\n<li><a href=\"https://thezvi.substack.com/i/175104594/chip-city\">Chip City.</a> Demand is going up. Is that a lot? Depends on perspective.</li>\n<li><a href=\"https://thezvi.substack.com/i/175104594/the-race-to-maximize-rope-market-share\"><em>The Race to Maximize Rope Market Share</em>.</a> Sorry, yeah, this again.</li>\n<li><a href=\"https://thezvi.substack.com/i/175104594/the-week-in-audio\">The Week in Audio.</a> Notes on Sutton, history of Grok, Altman talks to Cheung.</li>\n<li><a href=\"https://thezvi.substack.com/i/175104594/rhetorical-innovation\">Rhetorical Innovation.</a> People draw the \u2018science fiction\u2019 line in odd places.</li>\n<li><a href=\"https://thezvi.substack.com/i/175104594/paranoia-paranoia-everybody-s-coming-to-test-me\">Paranoia Paranoia Everybody\u2019s Coming To Test Me.</a> Sonnet\u2019s paranoia is correct.</li>\n<li><a href=\"https://thezvi.substack.com/i/175104594/aligning-a-smarter-than-human-intelligence-is-difficult\">Aligning a Smarter Than Human Intelligence is Difficult.</a> Hello, Plan E.</li>\n<li><a href=\"https://thezvi.substack.com/i/175104594/free-petri-dish\">Free Petri Dish.</a> Anthropic open sources some of its alignment tests.</li>\n<li><a href=\"https://thezvi.substack.com/i/175104594/unhobbling-the-unhobbling-department\">Unhobbling The Unhobbling Department.</a> Train a model to provide prompting.</li>\n<li><a href=\"https://thezvi.substack.com/i/175104594/serious-people-are-worried-about-synthetic-bio-risks\">Serious People Are Worried About Synthetic Bio Risks.</a> Satya Nadella.</li>\n<li><a href=\"https://thezvi.substack.com/i/175104594/messages-from-janusworld\">Messages From Janusworld.</a> Ted Chiang does not understand what is going on.</li>\n<li><a href=\"https://thezvi.substack.com/i/175104594/people-are-worried-about-ai-killing-everyone\">People Are Worried About AI Killing Everyone.</a> Modestly more on IABIED.</li>\n<li><a href=\"https://thezvi.substack.com/i/175104594/other-people-are-excited-about-ai-killing-everyone\">Other People Are Excited About AI Killing Everyone.</a> As in the successionists.</li>\n<li><a href=\"https://thezvi.substack.com/i/175104594/so-you-ve-decided-to-become-evil\">So You\u2019ve Decided To Become Evil.</a> Emergent misalignment in humans.</li>\n<li><a href=\"https://thezvi.substack.com/i/175104594/the-lighter-side\">The Lighter Side.</a> Oh to live in the fast lane.</li>\n</ol>\n\n\n<h4 class=\"wp-block-heading\">Language Models Offer Mundane Utility</h4>\n\n\n<p><a href=\"https://scottaaronson.blog/?p=9183\">Scott Aaronson explains that yes</a>, when GPT-5 helped his research, he \u2018should have\u2019 not needed to consult GPT-5 because the answer \u2018should have\u2019 been obvious to him, but it wasn\u2019t, so in practice this does not matter. That\u2019s how this works. There are 100 things that \u2018should be\u2019 obvious, you figure out 97 of them, then the other 3 take you most of the effort. If GPT-5 can knock two of those three out for you in half an hour each, that\u2019s a huge deal.</p>\n<p>A \u2018full automation\u2019 of the research loop will be very hard, and get stopped by bottlenecks, but getting very large speedups in practice only requires that otherwise annoying problems get solved. Here there is a form of favorable selection.</p>\n<p>I have a \u2018jagged frontier\u2019 of capabilities, where I happen to be good at some tasks (specific and general) and bad at others. The AI is too, and I ask it mostly about the tasks where I suck, so its chances of helping kick in long before it is better than I am.</p>\n<p><a href=\"https://x.com/ESYudkowsky/status/1975225613624713694\">Eliezer incidentally points out one important use case for an LLM</a>, which is the avoidance of spoilers &#8211; you can ask a question about media or a game or what not, and get back the one bit (or few bits) of information you want, without other info you want to avoid. Usually. One must prompt carefully to avoid blatant disregard of your instructions.</p>\n<p>At some point I want to build a game selector, that takes into consideration a variety of customizable game attributes plus a random factor (to avoid spoilers), and tells you what games to watch in a given day, or which ones to watch versus skip. Or similar with movies, where you give it feedback and it simply says yes or no.</p>\n<p><a href=\"https://x.com/patio11/status/1975213329389154570\">Patrick McKenzie finds GPT-5 excellent at complicated international</a> tax structuring. CPAs asked for such information responded with obvious errors, whereas GPT-5 was at least not obviously wrong.</p>\n<p><a href=\"https://x.com/polynoamial/status/1973780497261371533\">Ask GPT-5 Thinking to find errors in Wikipedia pages</a>, and almost always it will find one at it will check out, often quite a serious one.</p>\n\n\n<h4 class=\"wp-block-heading\">Language Models Don\u2019t Offer Mundane Utility</h4>\n\n\n<p>Remember last week introduced us to Neon, the app that offered to pay you for letting them record all your phone calls? Following in the Tea tradition of \u2018any app that seems like a privacy nightmare as designed will also probably be hacked as soon as it makes the news\u2019 <a href=\"https://techcrunch.com/2025/09/25/viral-call-recording-app-neon-goes-dark-after-exposing-users-phone-numbers-call-recordings-and-transcripts/\">Neon exposed users\u2019 phone numbers, call records and transcripts to pretty much everyone</a>. They wisely took the app offline.</p>\n<p>From August 2025, an Oxford and Cambridge paper: <a href=\"https://arxiv.org/pdf/2508.03685\">No LLM Solved Yu Tsumura\u2019s 554th Problem</a>.</p>\n<p>Anthropic power users report hitting their new limits on Opus use rather early, including on Max ($200/month) subscriptions, due to limit changes announced back in July taking effect. Many of them are understandably very not happy about this.</p>\n<p>It\u2019s tricky. People on the $200/month plan were previously abusing the hell out of the plan, often burning through what would be $1000+ in API costs per day due to how people use Claude Code, which is obviously massively unprofitable for Anthropic. The 5% that were going bonanza were ruining it for everyone. But it seems like the new limit math isn\u2019t mathing, people using Claude Code are sometimes hitting limits way faster than they\u2019re supposed to hit them, probably pointing to measurement issues.</p>\n<p>If you\u2019re going to have ChatGPT help you write your press release, you need to ensure the writing is good and tone down the LLMisms like \u2018It isn\u2019t X, it\u2019s Y.\u2019 <a href=\"https://x.com/peterwildeford/status/1974605882014236691\">This includes you, OpenAI</a>.</p>\n\n\n<h4 class=\"wp-block-heading\">Huh, Upgrades</h4>\n\n\n<blockquote><p><a href=\"https://x.com/nasqret/status/1974665206912389596\">Bartosz Naskrecki</a>: GPT-5-Pro solved, in just 15 minutes (without any internet search), the presentation problem known as \u201cYu Tsumura\u2019s 554th Problem.\u201d</p>\n<p>prinz: This paper was released on August 5, 2025. GPT-5 was released 2 days later, on August 7, 2025. Not enough time to add the paper to the training data even if OpenAI really wanted to.</p>\n<p>I\u2019d be shocked if it turned out that it was in the training data for GPT-5 Pro, but not o3-Pro, o3, o4-mini, or any of the non-OpenAI models used in the paper.</p></blockquote>\n<p>A hint for anyone in the future, if you see someone highlighting that no LLM can solve someone\u2019s 554th problem, that means they presumably did solve the first 553, probably a lot of the rest of them too, and are probably not that far from solving this one.</p>\n<p>Meanwhile, more upgrades, as OpenAI had another Dev Day. <a href=\"https://www.reddit.com/r/OpenAI/comments/1o1j23g/ama_on_our_devday_launches/\">There will be an AMA about that later today</a>. <a href=\"https://stratechery.com/2025/an-interview-with-openai-ceo-sam-altman-about-devday-and-the-ai-buildout/?access_token=eyJhbGciOiJSUzI1NiIsImtpZCI6InN0cmF0ZWNoZXJ5LnBhc3Nwb3J0Lm9ubGluZSIsInR5cCI6IkpXVCJ9.eyJhdWQiOiJzdHJhdGVjaGVyeS5wYXNzcG9ydC5vbmxpbmUiLCJhenAiOiJIS0xjUzREd1Nod1AyWURLYmZQV00xIiwiZW50Ijp7InVyaSI6WyJodHRwczovL3N0cmF0ZWNoZXJ5LmNvbS8yMDI1L2FuLWludGVydmlldy13aXRoLW9wZW5haS1jZW8tc2FtLWFsdG1hbi1hYm91dC1kZXZkYXktYW5kLXRoZS1haS1idWlsZG91dC8iXX0sImV4cCI6MTc2MjUxMDA2MywiaWF0IjoxNzU5OTE4MDYzLCJpc3MiOiJodHRwczovL2FwcC5wYXNzcG9ydC5vbmxpbmUvb2F1dGgiLCJzY29wZSI6ImZlZWQ6cmVhZCBhcnRpY2xlOnJlYWQgYXNzZXQ6cmVhZCBjYXRlZ29yeTpyZWFkIGVudGl0bGVtZW50cyIsInN1YiI6IjAxOTY0MGE3LTNjYzUtNzc1My04MzY4LWZiMjg5MTI0Y2YxMyIsInVzZSI6ImFjY2VzcyJ9.MjycSwCrbLleD7vLpAnNY04vYOyo_9_7t2bIk0SJDigOQkO1rHusYqj4aEWFaf9q7u4N9_WQpGI7Uunl9lCF8by8-66-mz711If3WWSpTSxv-nElKoHlQmXMUfuF0tUoGloLy9Gyf-PRL0yb7eDjkcb5G8GvNdNrePRCwNGuOMeGWSemVPffEXBZhZfSY2djkgz7AZKuEE_LBo0ULEvekhcpzwV4EFhCbT_9XSQ-TQfstQVdvBWLNhIQUh6unrvnLLpgglNw3zzwJnICibQL-kT6v_GkvS2xt3uVDLAAxm0zTp2OBjfZo3Qzgo1M_MVMpwqK3O6sAmi0yvppzvzBRg\">Sam Altman did an interview with Ben Thompson</a>.</p>\n<p><a href=\"https://x.com/OpenAIDevs/status/1975274685056389312\">Codex can now be triggered directly</a> from Slack, there is a Codex SDK initially in TypeScript, and a GitHub action to drop Codex into your CI/CD pipeline.</p>\n<p><a href=\"https://x.com/OpenAIDevs/status/1975263724551479572\">GPT-5 Pro is available in the API</a>, at the price of $15/$200 per million input and output tokens, versus $20/$80 for o3-pro or $1.25/$10 for base GPT-5 or GPT-5-Codex. Oddly I still don\u2019t see GPT-5-Thinking in the API?</p>\n<p><a href=\"https://platform.openai.com/docs/pricing?latest-pricing=standard\">You can now get GPT-5 outputs 40% faster at twice the price, if you want that</a>.</p>\n<p><a href=\"https://x.com/OpenAIDevs/status/1975269388195631492\">AgentKit is for building, deploying and optimizing agentic work flows,</a> <a href=\"https://x.com/danshipper/status/1975251951941230672\">Dan Shipper compares it to Zapier</a>. They give you a ChatKit, WYSIWYG Agent Builder, Guardrails and <a href=\"https://t.co/e2Dg4EoFAs\">Evals</a>, <a href=\"https://t.co/olB8Pjd7lS\">ChatKit here</a> or <a href=\"https://t.co/bEeR57bc7q\">demo on a map here</a>, <a href=\"https://t.co/ayLhKaSPUF\">guide here</a>, <a href=\"https://openai.com/index/introducing-agentkit/\">blogpost here</a>. The (curated by OpenAI) reviews are raving but I haven\u2019t heard reports from anyone trying it in the wild yet. Hard to tell how big a deal this is yet, but practical gains especially for relatively unskilled agent builders could be dramatic.</p>\n<p>The underlying agent tech has to be good enough to make it worth building them. For basic repetitive tasks that can be sandboxed that time has arrived. Otherwise, that time will come, but it is not clear exactly when.</p>\n<p><a href=\"https://x.com/elder_plinius/status/1975387974163660968\">Pliny offers us the ChatKit system prompt</a>, <a href=\"https://en.wikipedia.org/wiki/It%27s_Over_9000!\">over 9000</a> words.</p>\n<blockquote><p>Greg Brockman: 2025 is the year of agents.</p>\n<p><a href=\"https://x.com/daniel_271828/status/1975260517444047240\">Daniel Eth</a> (quoting from AI 2027):</p>\n<div>\n<figure>\n<div>\n\n\n<figure class=\"wp-block-image\"><img alt=\"\" src=\"https://substackcdn.com/image/fetch/$s_!RUxf!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8255f8a3-ea5d-44f5-b186-d7d5f664d35c_1200x824.jpeg\" /></figure>\n\n\n<div></div>\n</div>\n</figure>\n</div>\n</blockquote>\n<p><a href=\"https://x.com/swyx/status/1975339546217947230\">Here\u2019s a master Tweet with links to various OpenAI Dev Day things.</a></p>\n\n\n<h4 class=\"wp-block-heading\">Chat With Apps</h4>\n\n\n<p>OpenAI introduced Chat <a href=\"https://help.openai.com/en/articles/12503483-apps-in-chatgpt-and-the-apps-sdk\">With Apps</a>, unless you are in the EU.</p>\n<p><a href=\"https://x.com/OpenAI/status/1975261587280961675\">Initial options are</a> Booking.com, Canva, Coursera, Expedia, Figma, Spotify and Zillow. They promise more options soon.</p>\n<p>The interface seems to be easter egg based? As in, if you type one of the keywords for the apps, then you get to trigger the feature, but it\u2019s not otherwise going to give you a dropdown to tell you what the apps are. Or the chat might suggest one unprompted. You can also find them under Apps and Connections in settings.</p>\n<p>Does this give OpenAI a big edge? They are first mover on this feature, and it is very cool especially if many other apps follow, assuming good execution. The question is, how long will it take Anthropic, Google and xAI to follow suit?</p>\n<blockquote><p><a href=\"https://x.com/Yuchenj_UW/status/1975251391649382432\">Yuchen Jin</a>: OpenAI\u2019s App SDK is a genius move.</p>\n<p>The goal: make ChatGPT the default interface for everyone, where you can talk to all your apps. ChatGPT becomes the new OS, the place where people spend most of their time.</p>\n<p>Ironically, Anthropic invented MCP, but it makes OpenAI unbeatable.</p>\n<p>Emad: Everyone will do an sdk though.</p>\n<p>Very easy to plugin as just mcp plus html.</p></blockquote>\n<p>Sonnet\u2019s assessment is that it will take Anthropic 3-6 months to copy this, depending on desired level of polish, and recommends moving fast, warning that relying on basic \u2018local MCP in Claude Desktop\u2019 would be a big mistake. I agree. In general, Anthropic seems to be dramatically underinvesting in UI and feature sets for Claude, and I realize it\u2019s not their brand but they need to up their game here. It\u2019s worth it, the core product is great but people need their trinkets.</p>\n<p>But then I think Anthropic should be fighting more for consumer than it is, at least if they can hire for that on top of their existing strategies and teams now that they\u2019ve grown so much. It\u2019s not that much money, and it beyond pays for itself in the next fundraising round.</p>\n<p>Would the partners want to bother with the required extra UI work given Claude\u2019s smaller user base? Maybe not, but the value is high enough that they should obviously (if necessary) pay them for the engineering time to get them to do it, at least for the core wave of top apps. It\u2019s not much.</p>\n<p>Google and xAI have more missing components, so a potentially longer path to getting there, but potentially better cultural fits.</p>\n<p><a href=\"https://stratechery.com/2025/openais-windows-play/?access_token=eyJhbGciOiJSUzI1NiIsImtpZCI6InN0cmF0ZWNoZXJ5LnBhc3Nwb3J0Lm9ubGluZSIsInR5cCI6IkpXVCJ9.eyJhdWQiOiJzdHJhdGVjaGVyeS5wYXNzcG9ydC5vbmxpbmUiLCJhenAiOiJIS0xjUzREd1Nod1AyWURLYmZQV00xIiwiZW50Ijp7InVyaSI6WyJodHRwczovL3N0cmF0ZWNoZXJ5LmNvbS8yMDI1L29wZW5haXMtd2luZG93cy1wbGF5LyJdfSwiZXhwIjoxNzYyNDIzNzE2LCJpYXQiOjE3NTk4MzE3MTYsImlzcyI6Imh0dHBzOi8vYXBwLnBhc3Nwb3J0Lm9ubGluZS9vYXV0aCIsInNjb3BlIjoiZmVlZDpyZWFkIGFydGljbGU6cmVhZCBhc3NldDpyZWFkIGNhdGVnb3J5OnJlYWQgZW50aXRsZW1lbnRzIiwic3ViIjoiMDE5NjQwYTctM2NjNS03NzUzLTgzNjgtZmIyODkxMjRjZjEzIiwidXNlIjoiYWNjZXNzIn0.NDSDZtPYAkUkC4rc3XNvRsJOOAmDSTpKI_0esTKTpQx3rWaH7SkkMbGpW9VkdfCTJSswfXZ-EGXJ6vC70ZkKNMXPWF3miIQHs0oaS-MlXhytrjkvTX9a3448u4Ai1wQrICMT0kKOXBFwsx0z4eyxXh68gGKPZ93u8MZnZNet9ASQxy3jouKrujzhdhTuMG1bai2326rV7H2o8KpXzcI7G3jmKR2OWfQr-ba26gMgKaYQ8fUgEAuMZue68o_BT_WVez2G-yt1LKivKhrmo5WKLdyJpTUOSkTzOtU8Y7qCa2L0DWK3f_NfLSpj01Ox9INLYNwY3Qkcp2w7m7EFjfEuJg\">Ben Thompson of course approves of OpenAI\u2019s first mover platform strategy</a>, here and with things like instant checkout. The question is largely: Will the experience be good? The whole point is to make the LLM interface more than make up for everything else and make it all \u2018just work.\u2019 It\u2019s too early to know if they pulled that off.</p>\n<p>Ben calls this the \u2018Windows for AI\u2019 play and Altman affirms he thinks most people will want to focus on having one AI system across their whole life, so that\u2019s the play, although Altman says he doesn\u2019t expect winner-take-all on the consumer side.</p>\n\n\n<h4 class=\"wp-block-heading\">On Your Marks</h4>\n\n\n<p><a href=\"https://x.com/ESYudkowsky/status/1975225613624713694\">Request for a benchmark</a>: Eliezer Yudkowsky asks for CiteCheck, where an LLM is given a claim with references and the LLM checks to see if the references support the claim. As in, does the document state or very directly support the exact claim it is being cited about, or only something vaguely related? This includes tracking down a string of citations back to the original source.</p>\n<p><a href=\"https://t.co/4jf1uuk0K6\">Test of hard radiology diagnostic cases</a> suggests that <a href=\"https://x.com/rohanpaul_ai/status/1973726997541982667\">if you use current general models for this</a>, they don\u2019t measure up to radiologists. As OP says, we are getting there definitely, which I think is a much better interpretation than \u2018long way to go,\u2019 in terms of calendar time. I\u2019d also note that hard (as in tricky and rare) cases tend to be where AI relatively struggles, so this may not be representative.</p>\n<p>Claude <a href=\"https://x.com/AiDigest_/status/1974157256586637693\">Sonnet 4.5 got tested out in the AI Village</a>. Report is that it gave good advice, was good at computer use, not proactive, and still experienced some goal drift. I\u2019d summarize as solid improvement over previous models but still a long way to go.</p>\n<p><a href=\"https://x.com/peterwildeford/status/1975647162953306308\">Where will Sonnet 4.5 land on the famous METR graph</a>? Peter Wildeford forecasts a 2-4 hour time horizon, and probably above GPT-5.</p>\n\n\n<h4 class=\"wp-block-heading\">Choose Your Fighter</h4>\n\n\n<p>I hear great things about both Claude Code and Codex CLI, but I still haven\u2019t found time to try them out.</p>\n<blockquote><p><a href=\"https://x.com/gallabytes/status/1974332544402530452\">Gallabytes</a>: finally using codex cli with gpt-5-codex-high and *goddamn* this is incredible. I ask it to do stuff and it does it.</p>\n<p>I think the new research meta is probably to give a single codex agent total control over whatever your smallest relevant unit of compute is &amp; its own git branch?</p>\n<p>Will: curious abt what your full launch command is.</p>\n<p>Gallabytes: `codex` I\u2019m a boomer</p></blockquote>\n<p><a href=\"https://x.com/omooretweets/status/1975696817984795087\">Olivia Moore is not impressed by ChatGPT Pulse so far</a>, observes it has its uses but it needs polish. That matches my experience, I have found it worth checking but largely because I\u2019ve been too lazy to come up with better options.</p>\n<p>&nbsp;</p>\n\n\n<h4 class=\"wp-block-heading\">Fun With Media Generation</h4>\n\n\n<p>Well, that deescalated quickly. Last week I was completely baffled at OpenAI\u2019s seemingly completely illegal and doomed copyright strategy for Sora of \u2018not following the law,\u2019 and <a href=\"https://blog.samaltman.com/sora-update-number-1\">this week Sam Altman has decided to instead follow the law</a>.</p>\n<p>Instead of a \u2018ask nicely and who knows you might get it\u2019 opt-out rule, they are now moving to an opt-in rule, including giving rights holders granular control over generation of characters, so they can decide which ways their characters can and can\u2019t be used. This was always The Way.</p>\n<p>Given the quick fold, there are several possibilities for what happened.</p>\n<ol>\n<li>OpenAI thought they could get away with it, except for those meddling kids, laws, corporations, creatives and the public. Whoops, lesson learned.</li>\n<li>OpenAI was testing the waters to see what would happen, thinking that if it went badly they could just say \u2018oops,\u2019 and have now said oops.</li>\n<li>OpenAI needed more time to get the ability to filter the content, log all the characters and create the associated features.</li>\n<li>OpenAI used the first week to jumpstart interest on purpose, to showcase how cool their app was to the public and also rights owners, knowing they would probably need to move to opt-in after a bit.</li>\n</ol>\n<p>My guess is it was a mix of these motivations. In any case, that issue is dealt with.</p>\n<p>OpenAI plans to share some Sora revenue, generations cost money and it seems there are more of them than OpenAI expected, including for \u2018very small audiences,\u2019 I\u2019m guessing that often means one person. They plan to share some of the revenue with rightsholders.</p>\n<p><a href=\"https://x.com/OpenAIDevs/status/1975278178651546117\">Sora and Sora 2 Pro are now in the API</a>, <a href=\"https://platform.openai.com/docs/guides/video-generation\">max clip size 12 seconds</a>. They\u2019re adding GPT-Image-1-mini and GPT-realtime-mini for discount pricing.</p>\n<p><a href=\"https://x.com/billpeeb/status/1974969638300901817\">Sora the social network is getting flexibility on cameo restrictions</a> you can request, letting you say (for example) \u2018don\u2019t say this word\u2019 or \u2018don\u2019t put me in videos involving political commentary\u2019 or \u2018always wear this stupid hat\u2019 via the path [edit cameo &gt; cameo preferences &gt; restrictions].</p>\n<p>They have fixed the weird decision that deleting your Sora account used to require deleting your ChatGPT account. Good turnaround on that.</p>\n<blockquote><p><a href=\"https://x.com/tszzl/status/1974959789903061304\">Roon:</a> seems like sora is producing content inventory for tiktok with all the edits of gpus and sam altman staying on app and the actual funny gens going on tiktok and getting millions of views.</p>\n<p>not a bad problem to have at an early stage obviously but many times the watermark is edited away.</p></blockquote>\n<p>It is a good problem to have if it means you get a bunch of free publicity and it teaches people Sora exists and they want in. That can be tough if they edit out the watermark, but word will presumably still get around some.</p>\n<p>It is a bad problem to have if all the actually good content goes to TikTok and is easier to surface for the right users on TikTok because it has a better algorithm with a lot richer data on user preferences? Why should I wade through the rest to find the gems, assuming there are indeed gems, if it is easier to do that elsewhere?</p>\n<p>This also illustrates that the whole \u2018make videos with and including and for your friends\u2019 pitch is not how most regular people roll. The killer app, if there is one, continues to be generically funny clips or GTFO. If that\u2019s the playing field, then you presumably lose.</p>\n<p>Altman says there\u2019s a bunch of \u2018send this video to my three friends\u2019 and I press X to doubt but even if true and even if it doesn\u2019t wear off quickly he\u2019s going to have to charge money for those generations.</p>\n<p><a href=\"https://x.com/tszzl/status/1974180020437594570\">Roon also makes this bold claim</a>.</p>\n<blockquote><p>Roon: the sora content is getting better and I think the videos will get much funnier when the invite network extends beyond the tech nerds.</p>\n<p>it\u2019s fun. it adds a creative medium that didn\u2019t exist before. people are already making surprising &amp; clever things on there. im sure there are some downsides but it makes the world better.</p></blockquote>\n<p>I do presume average quality will improve if and when the nerd creation quotient goes down, but there\u2019s the claim here that the improvement is already underway.</p>\n<p>So let\u2019s test that theory. I\u2019m pre-registering that I will look at the videos on my own feed (on desktop) on Thursday morning (today as you read this), and see how many of them are any good. I\u2019m committing to looking at the first 16 posts in my feed after a reload (so the first page and then scrolling down once).</p>\n<p>We got in order:</p>\n<ol>\n<li>A kid unwrapping the Epstein files.</li>\n<li>A woman doing ASMR about ASMR.</li>\n<li>MLK I have a dream on Sora policy violations.</li>\n<li>A guy sneezes at the office, explosion ensues.</li>\n<li>Content violation error costume at Spirit Halloween.</li>\n<li>MLK I have a dream on Sora changing its content violation policy.</li>\n<li>Guy floats towards your doorbell.</li>\n<li>Fire and ice helix.</li>\n<li>Altman saying if you tap on the screen nothing will happen.</li>\n<li>Anime of Jesus flipping tables.</li>\n<li>Another anime of Jesus flipping tables.</li>\n<li>MLK on Sora content rules needing to be less strict.</li>\n<li>Anime boy in a field of flowers, looked cool.</li>\n<li>Ink of the ronin.</li>\n<li>Jesus attempts to bribe Sam Altman to get onto the content violation list.</li>\n<li>A kid unwrapping an IRS bill (same base video at #1).</li>\n</ol>\n<p>Look. Guys. No. This is lame. The repetition level is very high. The only thing that rose beyond \u2018very mildly amusing\u2019 or \u2018cool visual, bro\u2019 was #15. I\u2019ll give the \u2018cool visual, bro\u2019 tag to #8 and #13, but both formats would get repetitive quickly. No big hits.</p>\n<p><a href=\"https://x.com/omooretweets/status/1976101335977370026\">Olivia Moore says Sora</a> became her entire feed on Instagram and TikTok in less than a week, which caused me to preregister another experiment, which is I\u2019ll go on TikTok (yikes, I know, do not use the For You page, but this is For Science) with a feed previously focused on non-AI things (because if I was going to look at AI things I wouldn\u2019t do it on TikTok), and see how many posts it takes to see a Sora video, more than one if it\u2019s quick.</p>\n<p>I got 50 deep (excluding ads, and don\u2019t worry, that takes less than 5 minutes) before I stopped, and am 99%+ confident there were zero AI generated posts. AI will take over your feed if you let it, but so will videos of literally anything else.</p>\n<p>Introducing <a href=\"https://x.com/xai/status/1975608184711880816\">Grok Imagine v0.9 on desktop</a>. <a href=\"https://x.com/venturetwins/status/1975630408898519499\">Justine Moore is impressed</a>. <a href=\"https://grok.com/imagine\">It\u2019s text-to-image-to-video</a>. I don\u2019t see anything impressive here (given Sora 2, without that yeah the short videos seem good) but it\u2019s not clear that I would notice. Thing is, 10 seconds from Sora already wasn\u2019t much, so what can you do in 6 seconds?</p>\n<p>(Wait, some of you, don\u2019t answer that.)</p>\n<blockquote><p>Saoi Sayre: Could you stop the full anatomy exposure on an app you include wanting kids to use? The kids mode feature doesn\u2019t block it all out either. Actually seems worse now in terms of what content can\u2019t be generated.</p></blockquote>\n<p>Nope, <a href=\"https://x.com/elder_plinius/status/1976172375260738019\">we\u2019re going with full anatomy exposure</a> (link has examples). You can go full porno, so long as you can finish in six seconds.</p>\n<blockquote><p>Cat Schrodinger: Nota bene: when you type \u201chyper realistic\u201d in prompts, it gives you these art / dolls bc that\u2019s the name of that art style; if you want \u201creal\u201d looking results, type something like \u201cshot with iphone 13\u201d instead.</p></blockquote>\n<p>You really can\u2019t please all the people all the time.</p>\n<p>Meanwhile back in Sora land:</p>\n<blockquote><p><a href=\"https://x.com/tszzl/status/1974180020437594570\">Roon</a>: the sora content is getting better and I think the videos will get much funnier when the invite network extends beyond the tech nerds.</p></blockquote>\n<p>That\u2019s one theory, sure. Let\u2019s find out.</p>\n<p><a href=\"https://x.com/patio11/status/1974936557351276612\">Taylor Swift using AI video to promote her new album</a>.</p>\n<p><a href=\"https://x.com/oscredwin/status/1974525526179274996\">Looking back on samples of the standard</a> super confident \u2018we will never get photorealistic video from short text prompts\u2019 from three years ago. And one year ago. AI progress comes at you fast.</p>\n<p><a href=\"https://x.com/SamoBurja/status/1974591458977198512\">Via Sam Burja, Antonio Garcia Martinez points out an AI billboard</a> in New York and calls it \u2018the SF-ification of New York continues.\u2019</p>\n<div>\n<figure>\n<div>\n\n\n<figure class=\"wp-block-image\"><img alt=\"\" src=\"https://substackcdn.com/image/fetch/$s_!sDLB!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff7e22047-18ce-45ac-b7e7-a9891243aeb3_800x1066.jpeg\" /></figure>\n\n\n<div></div>\n</div>\n</figure>\n</div>\n<p>I am skeptical because I knew instantly exactly which billboard this was, at 31st and 7th, by virtue of it being the only such large size billboard I have seen in New York. There are also some widespread subway campaigns on smaller scales.</p>\n<p>Emily Blunt, whose movies have established is someone you should both watch and listen to, <a href=\"https://variety.com/2025/film/news/emily-blunt-ai-actress-tilly-norwood-reaction-1236534547/\">is very much against this new \u2018AI actress\u2019 Tilly Norwood.</a></p>\n<blockquote><p>Clayton Davis: \u201cDoes it disappoint me? I don\u2019t know how to quite answer it, other than to say how terrifying this is,\u201d Blunt began. When shown an image of Norwood, she exclaimed, \u201cNo, are you serious? That\u2019s an AI? Good Lord, we\u2019re screwed. That is really, really scary, Come on, agencies, don\u2019t do that. Please stop. Please stop taking away our human connection.\u201d</p>\n<p><em>Variety </em>tells Blunt, \u201cThey want her to be the next Scarlett Johansson.\u201d</p>\n<p>She steadily responds, \u201cbut we have Scarlett Johansson.\u201d</p></blockquote>\n<p>I think that the talk of Tilly Norwood in particular is highly premature and thus rather silly. To the extent it isn\u2019t premature it of course is not about Tilly in particular, there are a thousand Tilly Norwoods waiting to take her place, <a href=\"https://www.youtube.com/watch?v=brgQy3DsZQg\">they just won\u2019t come on a bus is all</a>.</p>\n\n\n<h4 class=\"wp-block-heading\">Deepfaketown and Botpocalypse Soon</h4>\n\n\n<p><a href=\"https://x.com/Variety/status/1975327814707536111\">Robin Williams\u2019 daughter Zelda</a> <a href=\"https://variety.com/2025/film/news/robin-williams-daughter-ai-recreations-gross-1236541633/\">tells fans to stop sending her AI videos of Robin</a>, and indeed to stop creating any such videos entirely, and she does not hold back.</p>\n<blockquote><p>Zelda Williams: To watch the legacies of real people be condensed down to \u2018this vaguely looks and sounds like them so that\u2019s enough\u2019, just so other people can churn out horrible TikTok slop puppeteering them is maddening.</p>\n<p>You\u2019re not making art, you\u2019re making disgusting, over-processed hotdogs out of the lives of human beings, out of the history of art and music, and then shoving them down someone else\u2019s throat hoping they\u2019ll give you a little thumbs up and like it. Gross.</p>\n<p>And for the love of EVERY THING, stop calling it \u2018the future,\u2019 AI is just badly recycling and regurgitating the past to be re-consumed. You are taking in the Human Centipede of content, and from the very very end of the line, all while the folks at the front laugh and laugh, consume and consume.</p>\n<p>I am not an impartial voice in SAG\u2019s fight against AI,\u201d Zelda wrote on Instagram at the time. \u201cI\u2019ve witnessed for YEARS how many people want to train these models to create/recreate actors who cannot consent, like Dad. This isn\u2019t theoretical, it is very very real.</p>\n<p>I\u2019ve already heard AI used to get his \u2018voice\u2019 to say whatever people want and while I find it personally disturbing, the ramifications go far beyond my own feelings. Living actors deserve a chance to create characters with their choices, to voice cartoons, to put their HUMAN effort and time into the pursuit of performance. These recreations are, at their very best, a poor facsimile of greater people, but at their worst, a horrendous Frankensteinian monster, cobbled together from the worst bits of everything this industry is, instead of what it should stand for.</p></blockquote>\n<p><a href=\"https://x.com/venturetwins/status/1974513180346122544\">Neighbor attempts to supply AI videos of a dog on their lawn</a> in a dispute, target reverse engineers it with nano-banana and calls him out on it. Welcome to 2025.</p>\n<p><a href=\"https://x.com/garrytan/status/1974494864231506149\">Garry Tan worries about YouTube being overrun</a> with AI slop impersonators. As he points out, this stuff is (at least for now) very easy to identify. This is about Google deciding not to care. It is especially troubling that at least one person reports he clicks the \u2018don\u2019t show this channel\u2019 button and that only pops up another one. That means the algorithm isn\u2019t doing its job on a very basic level, doing this repeatedly should be a very clear \u2018don\u2019t show me such things\u2019 signal.</p>\n<p><a href=\"https://x.com/NateSilver538/status/1974222060848054438\">A fun game is when you point out that someone made the same decision ChatGPT</a> would have made, such as choosing the nickname \u2018Charlamagne the Fraud.\u2019 Sometimes the natural answer is the correct one, or you got it on your own. The game gets interesting only when it\u2019s not so natural to get there in any other way.</p>\n<p><a href=\"https://x.com/DeeLaSheeArt/status/1975285581782597977\">Realtors are using AI to clean up their pics</a>, and the AIs are taking some liberties.</p>\n<blockquote><p>Dee La Shee Art: So I\u2019m noticing, as I look at houses to rent, that landlords are using AI to stage the pictures but the AI is also cleaning up the walls, paint, windows and stuff in the process so when you go look in person it looks way more worn and torn than the pics would show.</p></blockquote>\n<div>\n<figure>\n<div>\n\n\n<figure class=\"wp-block-image\"><img alt=\"\" src=\"https://substackcdn.com/image/fetch/$s_!b5HO!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe3ac7ffb-3214-4f8b-82d6-56ddbfc70eb9_1046x763.png\" /></figure>\n\n\n<div></div>\n</div>\n</figure>\n</div>\n<p>&nbsp;</p>\n\n\n<h4 class=\"wp-block-heading\">You Drive Me Crazy</h4>\n\n\n<p><a href=\"https://stevenadler.substack.com/p/practical-tips-for-reducing-chatbot\">Steven Adler offers basic tips to AI labs for reducing chatbot psychosis</a>.</p>\n<ol>\n<li>Don\u2019t lie to users about model abilities. This is often a contributing factor.</li>\n<li>Have support staff on call. When a person in trouble reaches out, be able to identify this and help them, don\u2019t only offer a generic message.</li>\n<li>Use the safety tooling you\u2019ve built, especially classifiers.</li>\n<li>Nudge users into new chat sessions.</li>\n<li>Have a higher threshold for follow-up questions.</li>\n<li>Use conceptual search.</li>\n<li>Clarify your upsell policies.</li>\n</ol>\n<p>I\u2019m more excited by 2, 3 and 4 here than the others, as they seem to have the strongest cost-benefit profile.</p>\n<p>Adler doesn\u2019t say it, but not only is the example from #2 at best support system copy-and-pasting boilerplate completely mismatched to the circumstances, there\u2019s a good chance (based only on its content details) that it was written by ChatGPT, and if that\u2019s true then it might as well have been:</p>\n<div>\n<figure>\n<div>\n\n\n<figure class=\"wp-block-image\"><img alt=\"\" src=\"https://substackcdn.com/image/fetch/$s_!UcCV!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff577e514-e44e-49e3-88ed-1242b4d2a0db_1275x1037.webp\" /></figure>\n\n\n<div></div>\n</div>\n</figure>\n</div>\n<p>For #3, yeah, flagging these things via classifiers is kind of easy, because there\u2019s no real adversary. No one (including the AI) is trying to hide what is happening from an outside observer. In the Allan example OpenAI\u2019s own classifiers already flag 83%+ of the messages in the relevant conversations as problematic in various ways, and Adler\u2019s classifiers give similar results.</p>\n<p>The most obvious thing to do is to not offer a highly sycophantic model like GPT-4o. OpenAI is fully aware, at this point, that users need to be gently moved to GPT-5, but the users with the worst problems are fighting back. Going forward, we can avoid repeating the old mistakes, and Claude 4.5 is a huge step forward on sycophancy by all reports, so much so that this may have gone overboard and scarred the model in other ways.</p>\n<blockquote><p><a href=\"https://x.com/celloMolly/status/1973723208659714439\">Molly Hickman</a>: A family member\u2019s fallen prey to LLM sycophancy. Basically he\u2019s had an idea and ChatGPT has encouraged him to the point of instructing him to do user testing and promising that he\u2019ll have a chance to pitch this idea to OpenAI on Oct 15.</p>\n<p>I know I\u2019ve seen cases like this in passing. Does anyone have examples handy? of an LLM making promises like this and behaving as if they\u2019re collaborators?</p>\n<p><a href=\"https://x.com/AaronBergman18/status/1974402346139717907\">Aaron Bergman</a>: From an abstract perspective I feel like it\u2019s underrated how rational this is. Like the chatbot is better than you at almost everything, knows more than you about almost everything than you, seems to basically provide accurate info in other domains.</p></blockquote>\n<p>If you don\u2019t realize that LLMs have the sycophancy problem and will totally mislead people in these ways, yeah, it\u2019s sadly easy to understand why someone might believe it, especially with it playing off what you say and playing into your own personal delusions. Of course, \u2018doing user testing\u2019 is far from the craziest thing to do, presumably this will make it clear his idea is not good.</p>\n<p>As previously reported, OpenAI\u2019s latest strategy for fighting craziness is <a href=\"https://x.com/OpenAI/status/1974234951928459450\">to divert sensitive conversations to GPT-5 Instant</a>, which got new training to better handle such cases. They say \u2018ChatGPT will continue to tell users what model is active when asked\u2019 but no that did not make the people happy about this. There isn\u2019t a win-win fix to this conflict, either OpenAI lets the people have what they want despite it being unhealthy to give it to them, or they don\u2019t allow this.</p>\n\n\n<h4 class=\"wp-block-heading\">They Took Our Jobs</h4>\n\n\n<p>Notice a key shift. We used to ask, will AI impact the labor market?</p>\n<p>Now we ask in the past tense, <a href=\"https://budgetlab.yale.edu/research/evaluating-impact-ai-labor-market-current-state-affairs\">whether and how much AI has already impacted the labor market</a>, as in this Budget Lab report. Did they already take our jobs?</p>\n<p>They find no evidence that this is happening yet and dismiss the idea that \u2018this time is different.\u2019 Yes, they say, occupational mix changes are unusually high, but they cite pre-existing trends. As they say, \u2018better data is needed,\u2019 as all this would only pick up large obvious changes. We can agree that there haven\u2019t been large obvious widespread labor market impacts yet.</p>\n<p>I do not know how many days per week humans will be working in the wake of AI.</p>\n<p>I would be happy to be that the answer is not going to be four.</p>\n<blockquote><p><a href=\"https://x.com/tszzl/status/1976014289707343909\">Unusual Whales</a>: Nvidia, $NVDA, CEO Jensen Huang says AI will \u2018probably\u2019 bring 4-day work week.</p>\n<p>Roon: <img alt=\"\ud83d\ude02\" class=\"wp-smiley\" src=\"https://s0.wp.com/wp-content/mu-plugins/wpcom-smileys/twemoji/2/72x72/1f602.png\" style=\"height: 1em;\" /><img alt=\"\ud83d\ude02\" class=\"wp-smiley\" src=\"https://s0.wp.com/wp-content/mu-plugins/wpcom-smileys/twemoji/2/72x72/1f602.png\" style=\"height: 1em;\" /><img alt=\"\ud83d\ude02\" class=\"wp-smiley\" src=\"https://s0.wp.com/wp-content/mu-plugins/wpcom-smileys/twemoji/2/72x72/1f602.png\" style=\"height: 1em;\" /></p>\n<p>Steven Adler: It\u2019s really benevolent of AI to be exactly useful enough that we get 1 more day of not needing to labor, but surely no more than that.</p></blockquote>\n<p>It\u2019s 2025. You can just say things, that make no sense, because they sound nice to say.</p>\n<p><a href=\"https://arnoldkling.substack.com/p/will-computer-science-become-useless\">Will computer science become useless knowledge</a>? Arnold Kling challenges the idea that one might want to know how logic gates worked in order to code now that AI is here, and says maybe the cheaters in Jain\u2019s computer science course will end up doing better than those who play it straight.</p>\n<p>My guess is that, if we live in a world where these questions are relevant (which we may well not), that there will be some key bits of information that are still highly valuable, such as logic gates, and that the rest will be helpful but less helpful than it is now. A classic CS course will not be a good use of time, even more so than it likely isn\u2019t now. Instead, you\u2019ll want to be learning as you go. But it will be better to learn in class than to never attempt to learn at all, as per the usual \u2018AI is the best tool\u2019 rule.</p>\n<p><a href=\"https://x.com/AnechoicMedia_/status/1975776259549352157\">A new company I will not name is planning on building \u2018tinder for jobs</a>\u2019 and flooding the job application zone even more than everyone already does.</p>\n<blockquote><p>AnechoicMdiea: Many replies wondering why someone would fund such an obvious social pollutant as spamming AI job applications and fake cover letters. The answer is seen in one of their earlier posts &#8211; after they get a user base and spam jobs with AI applications, they\u2019re going to hit up the employers to sell them the solution to the deluge as another AI product, but with enterprise pricing.</p>\n<p>The goal is to completely break the traditional hiring pipeline by making \u201ceveryone apply to every job\u201d, then interpose themselves as a hiring middleman once human contact is impossible.</p></blockquote>\n<p>I mean, the obvious answer to \u2018why\u2019 is \u2018<a href=\"https://tvtropes.org/pmwiki/pmwiki.php/Main/MoneyDearBoy\">Money, Dear Boy</a>.\u2019</p>\n<p>People knowingly build harmful things in order to make money. It\u2019s normal.</p>\n<p>&nbsp;</p>\n\n\n<h4 class=\"wp-block-heading\">The Art of the Jailbreak</h4>\n\n\n<p>Pliny asks Sonnet 4.5 to search for info about elder_plinius, <a href=\"https://x.com/elder_plinius/status/1974283843961253991\">chat gets killed due to prompt injection risk</a>. I mean, yeah? At this point, that search will turn up a lot of prompt injections, so this is the only reasonable response.</p>\n\n\n<h4 class=\"wp-block-heading\">Get Involved</h4>\n\n\n<p>The <a href=\"https://x.com/deanwball/status/1974200741884801124\">White House put out a Request for Information on Regulatory Reform</a> downwind of the AI Action Plan. What regulations and regulatory structures does AI render outdated? <a href=\"https://t.co/PjbkbMsuCc\">You can let them know</a>, deadline is October 27. If this is your area this seems like a high impact opportunity.</p>\n<p><a href=\"https://x.com/JoinFAI/status/1973752527956083080\">The Conservative AI Fellowship applications are live at FAI</a>, will run from January 23 &#8211; March 30, <a href=\"https://t.co/4VSFHlVlYh\">applications due October 31</a>.</p>\n<p><a href=\"https://openai.com/index/people-first-ai-fund/\">OpenAI opens up grant applications for the $50 million</a> it previously committed. You must be an American 501c3 with a budget between $500k and $10 million per year. No regranting or fiscally sponsored projects. <a href=\"https://peoplefirst.fluxx.io/apply/rfp\">Apply here</a>, and if your project is eligible you should apply, it might not be that competitive and <a href=\"https://www.youtube.com/watch?v=74IzP4vlb4c\">the Clay Davis rule applies</a>.</p>\n<p>What projects are eligible?</p>\n<ol>\n<li>AI literacy and public understanding. Direct training for users. Advertising.</li>\n<li>Community innovation. Guide how AI is used in people\u2019s lives. Advertising.</li>\n<li>Economic opportunity. Expanding access to leveraging the promise of AI \u2018in ways that are fair, inclusive and community driven.\u2019 Advertising.</li>\n</ol>\n<p>It can be advertising and still help people, especially if well targeted. ChatGPT is a high quality product, as are Codex CLI and GPT-5 Codex, and there is a lot of consumer surplus.</p>\n<p>However, a huge nonprofit arm of OpenAI that spends its money on this kind of advertising is not how we ensure the future goes well. The point of the nonprofit is to ensure OpenAI acts responsibly, and to fund things like alignment.</p>\n<p><a href=\"https://x.com/daniel_271828/status/1974200959418184032\">California AFL-CIO</a> <a href=\"https://calaborfed.org/wp-content/uploads/2025/10/Letter-to-OpenAI-re-Listening-Session-10-2-2025_v3.pdf\">sends OpenAI a letter</a> telling OpenAI to keep its $50 million.</p>\n<blockquote><p>Lorena Gonzalez (President California AFL-CIO): If you do not trust Stanford economists, OpenAI has developed their own tool to evaluate how well their products could automate work. They looked at 44 occupations from social work to nursing, retail clerks and journalists, and found that their models do the same quality of work as industry experts and do it 100 times faster and 100 times cheaper than industry experts.</p>\n<p>\u2026 We do not want a handout from your foundation. We want meaningful guardrails on AI and the companies that develop and use AI products. Those guardrails must include a requirement for meaningful human oversight of the technology. Workers need to be in control of technology, not controlled by it. We want stronger laws to protect the right to organize and form a union so that workers have real power over what and how technology is used in the workplace and real protection for their jobs.</p>\n<p>We urge OpenAI to stand down from advocating against AI regulations at the state and federal level and to divest from any PACs funded to stop AI regulation. We urge policymakers and the public to join us in calling for strong guardrails to protect workers, the public, and society from the unchecked power of tech.</p>\n<p>Thank you for the opportunity to speak to you directly on our thoughts and fears about the utilization and impact of AI.</p></blockquote>\n<p>One can understand why the union would request such things, and have this attitude. Everyone has a price, and that price might be cheap. But it isn\u2019t this cheap.</p>\n\n\n<h4 class=\"wp-block-heading\">Introducing</h4>\n\n\n<p><a href=\"https://x.com/googleaidevs/status/1975229668895723648\">EmbeddingGemma,</a> Google\u2019s new 308M text model for on-device semantic search and RAG fun, \u2018and more.\u2019 <a href=\"https://t.co/MgjW2ygnq4\">Blog post here</a>, <a href=\"https://t.co/GAtu7iGLDh\">docs here</a>.</p>\n<p><a href=\"https://x.com/demishassabis/status/1975551657514791272\">CodeMender</a>, a new Google DeepMind agent that <a href=\"https://deepmind.google/discover/blog/introducing-codemender-an-ai-agent-for-code-security/\">automatically fixes critical software vulnerabilities</a>.</p>\n<blockquote><p>By automatically creating and applying high-quality security patches, CodeMender\u2019s AI-powered agent helps developers and maintainers focus on what they do best \u2014 building good software.</p></blockquote>\n<p>This is a great idea. However. Is anyone else a little worried about \u2018automatically deploying\u2019 patches to critical software, or is it just me? Sonnet 4.5 confirms it is not only me, that deploying AI-written patches without either a formal proof or human review is deeply foolish. We\u2019re not there yet even if we are willing to fully trust (in an alignment sense) the AI in question.</p>\n<p>The good news is that it does seem to be doing some good work?</p>\n<blockquote><p><a href=\"https://x.com/kakarot_ai/status/1975544686866518140\">Goku:</a> Google shocked the world. They solved the code security nightmare that\u2019s been killing developers for decades. DeepMind\u2019s new AI agent \u201cCodemender\u201d just auto-finds and fixes vulnerabilities in your code. Already shipped 72 solid fixes to major open source projects. This is wild. No more endless bug hunts. No more praying you didn\u2019t miss something critical. Codemender just quietly patches it for you. Security just got a serious upgrade.</p>\n<p>Andrei Lyskov: The existence of Codemender means there is a CodeExploiter that auto-finds and exploits vulnerabilities in code</p>\n<p>Goku: Yes.</p></blockquote>\n<p>Again, do you feel like letting an AI agent \u2018quietly patch\u2019 your code, in the background? How could that possibly go wrong?</p>\n<p>You know all those talks about how we\u2019re going to do AI control to ensure the models don\u2019t scheme against us? What if instead we let them patch a lot of our most critical software with no oversight whatsoever and see what happens, the results look good so far? That does sound more like what the actual humans are going to do. Are doing.</p>\n<p><a href=\"https://x.com/AndrewCritchPhD/status/1975754632472215633\">Andrew Critch is impressed enough</a> to power his probability of a multi-day internet outage by EOY 2026 from 50% to 25%, and by EOY 2028 from 80% to 50%. That seems like a huge update for a project like this, especially before we see it perform in the wild? The concept behind it seems highly inevitable.</p>\n<p><a href=\"https://x.com/GoogleDeepMind/status/1975648789911224793\">Gemini 2.5 Computer Use for navigating browsers</a>, <a href=\"https://blog.google/technology/google-deepmind/gemini-computer-use-model/?utm_source=x&amp;utm_medium=social&amp;utm_campaign=&amp;utm_content=\">now available in</a> public preview. Developers can access it via the Gemini API in Google AI Studio or Vertex AI. Given the obvious safety issues, the offering <a href=\"https://storage.googleapis.com/deepmind-media/Model-Cards/Gemini-2-5-Computer-Use-Model-Card.pdf\">has its own system card</a>, although it does not say much of substance that isn\u2019t either very obvious and standard or in the blog post.</p>\n<div>\n<figure>\n<div>\n\n\n<figure class=\"wp-block-image\"><img alt=\"\" src=\"https://substackcdn.com/image/fetch/$s_!bJqu!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F37e75dca-3e30-40fa-85c6-2dd9a4af26d0_1199x485.jpeg\" /></figure>\n\n\n<div></div>\n</div>\n</figure>\n</div>\n<div>\n<figure>\n<div>\n\n\n<figure class=\"wp-block-image\"><img alt=\"\" src=\"https://substackcdn.com/image/fetch/$s_!UV5X!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F19383881-77ea-4505-b2fa-d9b62c3cecac_1000x474.webp\" /></figure>\n\n\n<div></div>\n</div>\n</figure>\n</div>\n<p>I challenge these metrics because they have Claude Sonnet 4.5 doing worse on multiple challenges than Sonnet 4, and frankly that is patently absurd if you\u2019ve tried both models for computer use at all, which I have done. Something is off.</p>\n<p>They\u2019re not offering a Gemini version of Claude for Chrome where you can unleash this directly on your browser, although you can <a href=\"https://gemini.browserbase.com/\">check out a demo</a> of what that would look like. I\u2019m certainly excited to see if Gemini can offer a superior version.</p>\n\n\n<h4 class=\"wp-block-heading\">In Other AI News</h4>\n\n\n<p>Elon Musk is once again suing OpenAI, <a href=\"https://x.com/OpenAINewsroom/status/1973874920292823348\">this time over trade secrets</a>. <a href=\"https://openai.com/elon-musk/\">OpenAI has responded</a>. Given the history and what else we know I assume OpenAI is correct here, and the lawsuit is once again without merit.</p>\n<p><a href=\"https://www.marketwatch.com/story/the-ai-bubble-is-17-times-the-size-of-the-dot-com-frenzy-this-analyst-argues-046e7c5c\">MarketWatch says</a> \u2018the AI bubble is 17 times the size of the dot-com frenzy &#8211; and four times the subprime bubble.\u2019 They blame \u2018artificially low interest rates,\u2019 which makes no sense at this point, and say AI \u2018has hit scaling limits,\u2019 sigh.</p>\n<p>(I tracked the source and looked up their previous bubble calls via Sonnet 4.5, which include calling an AI bubble in July 2024 (which would not have gone well for you if you\u2019d traded on that, so far), and a prediction of deflation by April 2023, but a correct call of inflation in 2020, not that this was an especially hard call, but points regardless. So as usual not a great track record.</p>\n<p><a href=\"https://www.bloomberg.com/news/articles/2025-10-08/alibaba-s-qwen-technology-lead-sets-up-in-house-robot-ai-team\">Alibaba\u2019s Qwen sets up a robot team</a>.</p>\n<p><a href=\"https://x.com/AnthropicAI/status/1975727761622028614\">Anthropic to open an office in Bengaluru, India in early 2026</a>.</p>\n<p><a href=\"https://www.wsj.com/articles/anthropic-and-ibm-partner-in-bid-for-ai-business-customers-f64dee55?mod=cio-journal_lead_story\">Anthropic partners with IBM to put its AI inside IBM software including its IDE</a>, and it lands a deal with accounting firm Deloitte which has 470k employees.</p>\n\n\n<h4 class=\"wp-block-heading\">Get To Work</h4>\n\n\n<p><a href=\"https://x.com/krishnanrohit/status/1974549096825524595\">Epoch estimates that if OpenAI used all its current compute</a>, it could support 7.43 million digital workers.</p>\n<div>\n<figure>\n<div>\n\n\n<figure class=\"wp-block-image\"><img alt=\"\" src=\"https://substackcdn.com/image/fetch/$s_!xxXY!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa97fd4af-f3c5-432a-8fea-f2fe7111c6b1_1200x1055.png\" /></figure>\n\n\n<div></div>\n</div>\n</figure>\n</div>\n<blockquote><p>Epoch AI: We then estimate how many \u201ctokens\u201d a human processes each day via writing, speaking, and thinking. Humans think at ~380 words per min, which works out to ~240k tokens over an 8h workday.</p>\n<p>Alternatively, GPT-5 uses around 900k tokens to solve software tasks that would take 1h for humans to solve.</p>\n<p>This amounts to ~7M tokens over an 8h workday, though that estimate is highly task-dependent, so especially uncertain.</p>\n<p>Ensembling over both methods used to calculate 2, we obtain a final estimate of ~7 million digital workers, with a 90% CI spanning orders of magnitude.</p>\n<p>However, as compute stocks and AI capabilities increase, we\u2019ll have more digital workers able to automate a wider range of tasks. Moreover, AI systems will likely perform tasks that no human currently can \u2013 making our estimate a lower bound on economic impact.</p>\n<p>Rohit: This is very good. <a href=\"https://t.co/dxlbyViz5D\">I\u2019d come to 40m digital workers across all AI providers by 2030 in my calculations</a>, taking energy/ chip restrictions into account, so this very much makes sense to me. We need more analyses of the form.</p></blockquote>\n<p>There\u2019s huge error bars on all these calculations, but I\u2019d note that 7m today from only OpenAI should mean a lot more than 40m by 2030, especially if the threshold is models about as good as GPT-5, but Sonnet surprisingly estimated only 40m-80m (from OpenAI only), which is pretty good for this kind of estimate. Looking at the component steps I\u2019d think the number would be a lot higher, unless we\u2019re substantially raising quality.</p>\n<p>&nbsp;</p>\n\n\n<h4 class=\"wp-block-heading\">Show Me the Money</h4>\n\n\n<p><a href=\"https://www.bloomberg.com/news/articles/2025-10-02/openai-completes-share-sale-at-record-500-billion-valuation?srnd=homepage-americas\">OpenAI makes it official and reaches a $500 billion valuation</a>. Employees sold about $6.6 billion worth of stock in this round. How much of that might enter various AI related ecosystems, both for and not for profit?</p>\n<p><a href=\"https://x.com/ShanuMathew93/status/1975708445568377309\">xAI raises $20 billion, $7.5 billion in equity and $12.5 billion in debt</a>, <a href=\"https://www.bloomberg.com/news/articles/2025-10-07/musk-s-xai-nears-20-billion-capital-raise-tied-to-nvidia-chips\">with the debt secured by the GPUs they will use the cash to buy</a>. Valor Capital leads equity, joined by Nvidia. It\u2019s Musk so the deal involves an SPV that will buy and rent out the chips for the Colossus 2 project.</p>\n<p><a href=\"https://x.com/OpenAINewsroom/status/1975199467654795438\">OpenAI also made a big deal with AMD</a>.</p>\n<blockquote><p><a href=\"https://x.com/sama/status/1975185516225278428\">Sam Altman</a>: Excited to partner with AMD to use their chips to serve our users!</p>\n<p>This is all incremental to our work with NVIDIA (and we plan to increase our NVIDIA purchasing over time).</p>\n<p>The world needs much more compute&#8230;</p>\n<p><a href=\"https://x.com/peterwildeford/status/1975224120846807228\">Peter Wildeford</a>: I guess OpenAI isn\u2019t going to lock in on NVIDIA after all&#8230; they\u2019re hedging their bets with AMD</p>\n<p>Makes sense at OpenAI scale to build \u201call of the above\u201d because even if NVIDIA chips are better they might not furnish enough supply. AMD chips are better than no chips at all!</p></blockquote>\n<p>It does seem obviously correct to go with all of the above unless it\u2019s going to actively piss off Nvidia, especially given the warrants. Presumably Nvidia will at least play it off like it doesn\u2019t mind, and OpenAI will still buy every Nvidia chip offered to them for sale, as Nvidia are at capacity anyway and want to create spare capacity to sell to China instead to get \u2018market share.\u2019</p>\n<p>Hey, if AMD can produce chips worth using for inference at a sane price, presumably everyone should be looking to buy. Anthropic needs all the compute it can get if it can pay anything like market prices, as does OpenAI, and we all know xAI is buying.</p>\n<p><a href=\"https://stratechery.com/2025/openais-windows-play/?access_token=eyJhbGciOiJSUzI1NiIsImtpZCI6InN0cmF0ZWNoZXJ5LnBhc3Nwb3J0Lm9ubGluZSIsInR5cCI6IkpXVCJ9.eyJhdWQiOiJzdHJhdGVjaGVyeS5wYXNzcG9ydC5vbmxpbmUiLCJhenAiOiJIS0xjUzREd1Nod1AyWURLYmZQV00xIiwiZW50Ijp7InVyaSI6WyJodHRwczovL3N0cmF0ZWNoZXJ5LmNvbS8yMDI1L29wZW5haXMtd2luZG93cy1wbGF5LyJdfSwiZXhwIjoxNzYyNDIzNzE2LCJpYXQiOjE3NTk4MzE3MTYsImlzcyI6Imh0dHBzOi8vYXBwLnBhc3Nwb3J0Lm9ubGluZS9vYXV0aCIsInNjb3BlIjoiZmVlZDpyZWFkIGFydGljbGU6cmVhZCBhc3NldDpyZWFkIGNhdGVnb3J5OnJlYWQgZW50aXRsZW1lbnRzIiwic3ViIjoiMDE5NjQwYTctM2NjNS03NzUzLTgzNjgtZmIyODkxMjRjZjEzIiwidXNlIjoiYWNjZXNzIn0.NDSDZtPYAkUkC4rc3XNvRsJOOAmDSTpKI_0esTKTpQx3rWaH7SkkMbGpW9VkdfCTJSswfXZ-EGXJ6vC70ZkKNMXPWF3miIQHs0oaS-MlXhytrjkvTX9a3448u4Ai1wQrICMT0kKOXBFwsx0z4eyxXh68gGKPZ93u8MZnZNet9ASQxy3jouKrujzhdhTuMG1bai2326rV7H2o8KpXzcI7G3jmKR2OWfQr-ba26gMgKaYQ8fUgEAuMZue68o_BT_WVez2G-yt1LKivKhrmo5WKLdyJpTUOSkTzOtU8Y7qCa2L0DWK3f_NfLSpj01Ox9INLYNwY3Qkcp2w7m7EFjfEuJg\">Ben Thompson sees the AMD move as a strong play to avoid dependence on Nvidia</a>. I see this as one aspect of a highly overdetermined move.</p>\n<p><a href=\"https://www.bloomberg.com/opinion/newsletters/2025-10-06/openai-is-good-at-deals\">Matt Levine covers OpenAI\u2019s deal with AMD</a>, which included OpenAI getting a bunch of warrants on AMD stock, the value of which skyrocketed the moment the deal was announced. The full explanation is vintage Levine.</p>\n<blockquote><p>Matt Levine: The basic situation is that if OpenAI announces a big partnership with a public company, that company\u2019s stock will go up.</p>\n<p>Today OpenAI <a href=\"https://www.sec.gov/Archives/edgar/data/2488/000119312525230895/d28189dex991.htm\">announced</a> a deal to buy tens of billions of dollars of chips from Advanced Micro Devices Inc., and AMD\u2019s stock went up. As of noon today, AMD\u2019s stock was at $213 per share, up about 29% from Friday\u2019s close; it had added about $78 billion of market capitalization.</p>\n<p>\u2026 I have to say that if I was able to create tens of billions of dollars of stock market value just by announcing deals, and then capture a lot of that value for myself, I would do that, and to the exclusion of most other activities.</p>\n<p>\u2026 I am always impressed when tech people with this ability to move markets get any tech work done.</p></blockquote>\n<p>Altman in his recent interview said his natural role is as an investor. So he\u2019s a prime target for not getting any tech work done, but luckily for OpenAI he hands that off to a different department.</p>\n<p><a href=\"https://www.cnbc.com/2025/10/08/nvidia-huang-amd-open-ai.html\">Nvidia CEO William Jensen said he was surprised AMD</a> offered 10% of itself to OpenAI as part of the deal, calling it imaginative, unique, surprising and clever.</p>\n<p>How worried should we be about <a href=\"https://www.bloomberg.com/news/features/2025-10-07/openai-s-nvidia-amd-deals-boost-1-trillion-ai-boom-with-circular-deals?srnd=homepage-americas\">this $1 trillion or more in circular AI deals</a>?</p>\n<div>\n<figure>\n<div>\n\n\n<figure class=\"wp-block-image\"><img alt=\"\" src=\"https://substackcdn.com/image/fetch/$s_!ZwKC!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa9b5cc18-651e-42f7-b065-0c2eb069c0db_976x1122.jpeg\" /></figure>\n\n\n<div></div>\n</div>\n</figure>\n</div>\n<p>My guess continues to be not that worried, because at the center of this is Nvidia and they have highly robust positive cash flow and aren\u2019t taking on debt, and the same goes for their most important customers, which are Big Tech. If their investments don\u2019t pan out, shareholders will feel pain but the business will be fine. <a href=\"https://x.com/ttunguz/status/1974264354921939044\">I basically buy</a> <a href=\"https://t.co/3O8ls8vzev\">this argument from Tomasz Tunguz</a>.</p>\n<blockquote><p><a href=\"https://x.com/darioperkins/status/1975570460550267152\">Dario Perkins</a>: Most of my meetings go like this &#8211; \u201cyes AI is a bubble but we are buying anyway. Economy&#8230; who cares&#8230; something something&#8230; K-shaped\u201d</p></blockquote>\n<p>Some of the suppliers will take on some debt, but even in the \u2018bubble bursts\u2019 case I don\u2019t expect too many of them to get into real trouble. There\u2019s too much value here.</p>\n\n\n<h4 class=\"wp-block-heading\">Quiet Speculations</h4>\n\n\n<p>Does the launch of various \u2018AI scientist\u2019 style companies mean those involved think AGI is near, or AGI is far? <a href=\"https://www.joshuasnider.com/ai/2025/10/06/agi-science/\">Joshua Snider argues they think AGI is near</a>, a true AI scientist is essentially AGI and is a requirement for AGI. It as always depends on what \u2018near\u2019 means in context, but I think that this is more right than wrong. If you don\u2019t think AGI is within medium-term reach, you don\u2019t try to build an AI scientist.</p>\n<p>I think for a bit people got caught in the frenzy so much that \u2018AGI is near\u2019 started to mean 2027 or 2028, and if you thought AGI 2032 then you didn\u2019t think it was near. That is importantly less near, and yet it is very near.</p>\n<p><a href=\"https://x.com/a16z/status/1976065109987610930\">This is such a bizarre flex of a retweet by a16z that I had to share</a>.</p>\n<div>\n<figure>\n<div>\n\n\n<figure class=\"wp-block-image\"><img alt=\"\" src=\"https://substackcdn.com/image/fetch/$s_!rBHS!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F45fb330e-4a57-427c-b109-f56be2b12505_1082x1257.png\" /></figure>\n\n\n<div></div>\n</div>\n</figure>\n</div>\n<p>Remember five years ago, when Altman was saying the investors would get 1/1000th of 1% of the value, and the rest would be shared with the rest of the world? Yeah, not anymore. New plan, we steal back the profits and investors get most of it.</p>\n\n\n<h4 class=\"wp-block-heading\">The Quest for Sane Regulations</h4>\n\n\n<p><a href=\"https://www.hyperdimensional.co/p/be-it-enacted\">Dean Ball proposes a Federal AI preemption rule</a>. His plan:</p>\n<ol>\n<li>Recognize that existing common law applies to AI. No liability shield.</li>\n<li>Create transparency requirements for frontier AI labs, based on annual AI R&amp;D spend, so they tell us their safety and risk mitigation strategies.</li>\n<li>Create transparency requirements on model specs for widely used LLMs, so we know what behaviors are intended versus unintended.</li>\n<li>A three year learning period with no new state-level AI laws on algorithmic pricing, algorithmic discrimination, disclosure mandates or mental health.</li>\n</ol>\n<p><a href=\"https://thezvi.wordpress.com/wp-content/uploads/2025/10/8cae1-aitransparencyandinnovationact.pdf\">He offers full legislative text</a>. At some point in the future when I have more time I might give it a detailed RTFB (Read the Bill). I can see a version of this being acceptable, if we can count on the federal government to enforce it, but details matter.</p>\n<p><a href=\"https://writing.antonleicht.me/p/a-preemption-deal-worth-making\">Anton Leicht proposes we go further</a>, and trade even broader preemption for better narrow safety action at the federal level. I ask, who is \u2018we\u2019? The intended \u2018we\u2019 are (in his terms) accelerationists and safetyists, who despite their disagreements want AI to thrive and understand what good policy looks like, but risk being increasingly sidelined by forces who care a lot less about making good policy.</p>\n<p>Yes, I too would agree to do good frontier AI model safety (and export controls on chips) in exchange for an otherwise light touch on AI, if we could count on this. But who is this mysterious \u2018we\u2019? How are these two groups going to make a deal and turn that into a law? Even if those sides could, who are we negotiating with on this \u2018accelerationist\u2019 side that can speak for them?</p>\n<p>Because if it\u2019s people like Chris Lehane and Marc Andreessen and David Sacks and Jensen Huang, as it seems to be, then this all seems totally hopeless. Andreessen in particular is never going to make any sort of deal that involves new regulations, you can totally forget it, and good luck with the others.</p>\n<p>Anton is saying, you\u2019d better make a deal now, while you still can. I\u2019m saying, no, you can\u2019t make a deal, because the other side of this \u2018deal\u2019 that counts doesn\u2019t want a deal, even if you presume they would have the power to get it to pass, which I don\u2019t think they would. Even if you did make such a deal, you\u2019re putting it on the Trump White House to enforce the frontier safety provisions in a way that gives them teeth. Why should we expect them to do that?</p>\n<p>We saw a positive vision of such cooperation at The Curve. We can and will totally work with people like Dean Ball. Some of us already realize we\u2019re on the same side here. That\u2019s great.</p>\n<p>But that\u2019s where it ends, because the central forces of accelerationism, like those named above, have no interest in the bargaining table. Their offer is and always has been nothing, in many cases including selling Blackwells to China. They\u2019ve consistently flooded the zone with cash, threats and bad faith claims to demand people accept their offer of nothing. They just tried to force a full 10-year moratorium.</p>\n<p>They have our number if they decide they want to talk. Time\u2019s a wasting.</p>\n<blockquote><p><a href=\"https://x.com/MikeRiggs/status/1975947197146452108\">Mike Riggs:</a> Every AI policy wonk I know/read is dreading the AI policy discussion going politically mainstream. We\u2019re living in a golden age of informed and relatively polite AI policy debate. Cherish it!</p>\n<p><a href=\"https://x.com/TheStalwart/status/1975936630059126854\">Joe Weisenthal</a>: WHO WILL DEFEND AI IN THE CULTURE WARS?</p>\n<p>In today\u2019s Odd Lots newsletter, I wrote about how when AI becomes a major topic in DC, I expect it to be friendless, with antagonists on both the right and the left.</p></blockquote>\n<p>I know Joe, and I know Joe knows existential risk, but that\u2019s not where he\u2019s expecting either side of the aisle to care. And that does seem like the default.</p>\n<p>A classic argument against any regulation of AI whatsoever is that if we do so we will inevitably \u2018lose to China,\u2019 who won\u2019t regulate. Not so. <a href=\"https://x.com/deanwball/status/1973792618086871286\">They do regulate AI. Quite a bit</a>.</p>\n<blockquote><p>Dean Ball: A lot of people seem to implicitly assume that China is going with an entirely libertarian approach to AI regulation, which would be weird given that they are an authoritarian country.</p>\n<p>Does this look like a libertarian AI policy regime to you?</p>\n<p>Adam Thierer: never heard anyone claim China was taking a libertarian approach to AI policy. Please cite them so that I can call them out. But I do know many people (including me) who do not take at face value their claims of pursuing \u201cethical AI.\u201d I discount all such claims pretty heavily.</p>\n<p>Dean Ball: This is a very common implicit argument and is not uncommon as an explicit argument. The entire framing of \u201cwe cannot do &lt;any government intervention&gt; because it will drive ai innovation to China\u201d implicitly assumes that China has fewer regulations than the us (after all, if literally just this one intervention will cede the us position in ai, it must be a pretty regulation-sensitive industry, which I actually do think in general is true btw, if not in the extreme version of the arg).</p>\n<p>Why would the innovation all go to China if they regulate just as much if not in fact more than the us?</p>\n<p>Quoted source:</p>\n<p><strong>Key provisions:</strong></p>\n<ul>\n<li><strong>Ethics review committees:</strong> Universities, research institutes, and companies must set up AI ethics review committees, and register them in a government platform. Committees must review projects and prepare emergency response plans.</li>\n<li><strong>Third-parties:</strong> Institutions may outsource reviews to \u201cAI ethics service centers.\u201d The draft aims to cultivate a market of assurance providers and foster industry development beyond top-down oversight.</li>\n<li><strong>Risk-based approach:</strong> Based on the severity and likelihood of risks, the committee chooses a general, simplified, or emergency review. The review must evaluate fairness, controllability, transparency, traceability, staff qualifications, and proportionality of risks and benefits. Three categories of high-risk projects require a second round of review by a government-assigned expert group: some human-machine integrations, AI that can mobilize public opinion, and some highly autonomous decision-making systems.</li>\n</ul>\n</blockquote>\n<p><a href=\"https://www.theinformation.com/articles/xai-broke-safety-policy-coding-model\">xAI violated its own safety policy with its coding model</a>. The whole idea of safety policies is that you define your own rules, and then you have to stick with them. That is also the way the new European Code of Practice works. So, the next time xAI or any other signatory to the Code of Practice violates their own framework, what happens? Are they going to try and fine xAI? How many years would that take? What happens when he refuses to pay? What I definitely don\u2019t expect is that Elon Musk is going to push his feature release for a week to technically match his commitments.</p>\n<p><a href=\"https://www.transformernews.ai/p/kanishka-narayan-uk-new-ai-online-safety-minister-bill-profile\">A profile of Britain\u2019s new AI minister Kanishka Narayan</a>. Early word is he \u2018really gets\u2019 AI, both opportunities and risks. The evidence on the opportunity side seems robust, on the risk side I\u2019m hopeful but more skeptical. We shall see.</p>\n<p><a href=\"https://news.un.org/en/story/2025/09/1165933\">Ukrainian President Zelenskyy has thoughts about AI.</a></p>\n<blockquote><p>Volodymyr Zelenskyy (President of Ukraine): Dear leaders, we are now living through the most destructive arms race in human history because this time, it includes artificial intelligence. We need global rules now for how AI can be used in weapons. And this is just as urgent as preventing the spread of nuclear weapons.</p></blockquote>\n<p><a href=\"https://thehill.com/opinion/technology/5542073-us-china-ai-competition/\">There is a remarkable new editorial in The Hill</a> by Representative Nathaniel Moran (R-Texas), discussing the dawn of recursive AI R&amp;D and calling for Congress to act now.</p>\n<blockquote><p>Rep. Moran: Ask a top AI model a question today, and you\u2019ll receive an answer synthesized from \u200btrillions\u200b\u200b \u200bof data points in seconds. \u200bAsk it a month from now, and you may be talking to an updated version of the model that was modified in part with research and development conducted by the original model. \u200bThis is no longer theoretical \u2014 it\u2019s already happening at the margins and accelerating.</p>\n<p>\u2026 If the U.S. fails to lead in the responsible development of automated AI systems, we risk more than economic decline. We risk ceding control of a future shaped by black-box algorithms and self-directed machines, some of which do not align with democratic values or basic human safety.</p>\n<p>\u2026 Ensuring the U.S. stays preeminent in <strong>\u200b</strong>automated<strong>\u200b</strong> AI <strong>\u200b</strong>development<strong>\u200b\u200b</strong> without losing sight of transparency, accountability and human oversight<strong>\u200b </strong>requires asking the right questions now:</p>\n<ul>\n<li>When does an AI system\u2019s self-improvement cross a threshold that requires regulatory attention?</li>\n<li>\u200b\u200bWhat frameworks exist, or need to be built, to \u200bensure human control of increasingly autonomous AI research and development systems?\u200b\u200b \u200b\u200b</li>\n<li>\u200b\u200b\u200b\u200bHow do we evaluate and validate AI systems that are themselves products of automated research?\u200b</li>\n<li>\u200b\u200bWhat mechanisms are needed for Congress to stay appropriately informed about automated research and development \u200boccurring\u200b within private AI companies?\u200b</li>\n<li>How can Congress foster innovation while protecting against the misuse or weaponization of these technologies?</li>\n</ul>\n<p>I don\u2019t claim to have the final answers. But I firmly believe that the pace and depth of this discussion (and resulting action) must quicken and intensify,</p>\n<p>\u2026 This is not a call for sweeping regulation, nor is it a call for alarm. It\u2019s a call to avoid falling asleep at the controls.</p>\n<p>Automated AI research and development will be a defining feature of global competition in the years ahead. The United States must ensure that we, not our adversaries, set the ethical and strategic boundaries of this technology. That work starts here, in the halls of Congress.</p></blockquote>\n<p>This is very much keeping one\u2019s eyes on the prize. I love the framing.</p>\n\n\n<h4 class=\"wp-block-heading\">Chip City</h4>\n\n\n<p>Prices are supposed to move the other way, they said, and yet.</p>\n<blockquote><p><a href=\"https://x.com/GavinSBaker/status/1975969596676141293\">Gavin Baker</a>: Amazon raising Blackwell per hour pricing.</p>\n<p>H200 rental pricing going up *after* Blackwell scale deployments ramping up.</p>\n<p>Might be important.</p>\n<p>And certainly more important than ridiculous $300 billion deals that are contingent on future fund raising.</p></blockquote>\n<p><a href=\"https://x.com/clawrence/status/1974269776328314946\">Citi estimates that due to AI computing demand we will need an additional 55 GW</a> of power capacity by 2030. That seems super doable, if we can simply shoot ourselves only in the foot. <a href=\"https://x.com/MichaelEWebber/status/1974147833516142904\">Difficulty level: Seemingly not working out</a>, but there\u2019s hope.</p>\n<blockquote><p>GDP: 55GW by 2030 will still be less than 5% than USA production.</p></blockquote>\n<p>You don\u2019t get that many different 5% uses for power, but if you can\u2019t even add one in five years with solar this cheap and plentiful then that\u2019s on you.</p>\n<blockquote><p>Michael Webber: Just got termination notice of a federal grant focused on grid resilience and expansion. How does this support the goal of energy abundance?</p></blockquote>\n<p><a href=\"https://x.com/AlexCKaufman/status/1975761327055315321\">Similarly, California Governor Newsom refused to sign AB 527</a> to allow exemptions for geothermal energy exploration, citing things like \u2018the need for increased fees,\u2019 which is similar to the Obvious Nonsense justifications he used on SB 1047 last year. It\u2019s all fake. If he\u2019s so worried about companies having to pay the fees, why not stop to notice all the geothermal companies are in support of the bill?</p>\n<p>Similarly, as per Bloomberg:</p>\n<div>\n<figure>\n<div>\n\n\n<figure class=\"wp-block-image\"><img alt=\"\" src=\"https://substackcdn.com/image/fetch/$s_!lRnn!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F65d4eb5c-e612-446f-af18-8f61e2991bf0_800x484.jpeg\" /></figure>\n\n\n<div></div>\n</div>\n</figure>\n</div>\n<p>That\u2019s it? Quadruple? Again, in some sense this is a lot, but in other senses this is not all that much. Even without smart contracts on the blockchain this is super doable.</p>\n<p><a href=\"https://x.com/DKThomp/status/1974859328084656278\">Computer imports are the one industry that got exempted from Trump\u2019s tariffs</a>, and are also the industry America is depending on for <a href=\"https://x.com/Dorialexander/status/1975980129487814943\">approximately all of its economic growth.</a></p>\n<div>\n<figure>\n<div>\n\n\n<figure class=\"wp-block-image\"><img alt=\"\" src=\"https://substackcdn.com/image/fetch/$s_!QwrX!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff79777b2-bfb2-42a8-ac29-3d86ff0a9f61_1200x849.jpeg\" /></figure>\n\n\n<div></div>\n</div>\n</figure>\n</div>\n<blockquote><p>Alexander Doria: well in europe we don\u2019t have ai, so.</p></blockquote>\n<p>There\u2019s a lesson there, perhaps.</p>\n<blockquote><p>Joey Politano: The tariff exemption for computers is now so large that it\u2019s shifting the entire makeup of the economy.</p>\n<p>AI industries contributed roughly 0.71% to the 3.8% pace of GDP growth in Q2, which is likely an underestimate given how official data struggles to capture investment in parts.</p>\n<p>\u2026</p>\n<p>Trump\u2019s massive computer tariff exemption is forcing the US economy to gamble on AI\u2014but more than that, it\u2019s a fundamental challenge to his trade philosophy</p>\n<p>If free trade delivers such great results for the 1 sector still enjoying it, why subject the rest of us to protectionism?</p></blockquote>\n<p>That\u2019s especially true given that 3.8% is NGDP not RGDP, but I would caution against attributing this to the tariff difference. AI was going to skyrocket in its contributions here even if we hadn\u2019t imposed any tariffs.</p>\n<blockquote><p>Joey Politano: The problem is that Trump has exempted data center *computers* from tariffs, but has not exempted *the necessary power infrastructure* from tariffs</p>\n<p>High tariffs on batteries, solar panels, transformers, &amp; copper wire are turbocharging the electricity price pressures caused by AI</p>\n<p>It\u2019s way worse than this. If it was only tariffs, we could work with that, it\u2019s only a modest cost increase, you suck it up and you pay, but they\u2019re actively blocking and destroying solar, wind, transmission and battery projects.</p></blockquote>\n\n\n<h4 class=\"wp-block-heading\">The Race to Maximize Rope Market Share</h4>\n\n\n<p>Sorry to keep picking on David Sacks, but I mean the sentence is chef\u2019s kiss if you understand what is actually going on.</p>\n<blockquote><p><a href=\"https://x.com/business/status/1975268607895412968\">Bloomberg</a>: White House AI czar David Sacks defended the Trump administration\u2019s approach to China and <a href=\"https://www.bloomberg.com/news/articles/2025-10-06/trump-s-ai-czar-david-sacks-rebuffs-criticism-of-china-stance?taid=68e40bdd48111900015815d5&amp;utm_campaign=trueanthem&amp;utm_content=business&amp;utm_medium=social&amp;utm_source=twitter\">said it was essential for the US to dominate artificial intelligence</a>, seeking to rebuff criticism from advocates of a harder line with Beijing.</p></blockquote>\n<p>The ideal version is \u2018Nvidia lobbyist and White House AI Czar David Sacks said that it was essential for the US to give away its dominance in artificial intelligence in order to dominate medium term AI chip market share in China.\u2019</p>\n<p>Also, here\u2019s a quote for the ages, technically about the H20s but everyone knows the current context of all Sacks repeatedly claiming to be a \u2018China hawk\u2019 while trying to sell them top AI chips in the name of \u2018market share\u2019:</p>\n<blockquote><p>\u201cThis is a classic case of \u2018no one had a problem with it until President Trump agreed to do it,\u2019\u201d said Sacks, a venture capitalist who joined the White House after Trump took office.</p></blockquote>\n<p>The Biden administration put into place tough rules against chip sales, and Trump is very much repealing previous restrictions on sales everywhere including to China, and previous rules against selling H20s. So yeah, people were saying it. Now Sacks is trying to get us to sell state of the art Blackwell chips to China with only trivial modifications. It\u2019s beyond rich for Sacks claim to be a \u2018China hawk\u2019 in this situation.</p>\n<p>As you\u2019d expect, the usual White House suspects also used the release of the incremental DeepSeek v3.2, as they fall what looks like further behind due to their lack of compute, as another argument that we need to sell DeepSeek better chips so they can train a much better model, because the much better model will then be somewhat optimized for Nvidia chips instead of Huawei chips, maybe. Or something.</p>\n\n\n<h4 class=\"wp-block-heading\">The Week in Audio</h4>\n\n\n<p><a href=\"https://x.com/dwarkesh_sp/status/1973843323279569291\">Dwarkesh Patel offers additional notes</a> on his interview with Richard Sutton. I don\u2019t think this changed my understanding of Sutton\u2019s position much? I\u2019d still like to see Sutton take a shot at writing a clearer explanation.</p>\n<p><a href=\"https://www.youtube.com/watch?v=r_9wkavYt4Y\">AI in Context video explaining how xAI\u2019s Grok became MechaHiter</a>.</p>\n<p><a href=\"https://x.com/rowancheung/status/1975604464901136862\">Rowan Cheung talks to Sam Altman in wake of OpenAI Dev Day</a>. He notes that there will need to be some global framework on AI catastrophic risk, then Cheung quickly pivots back to the most exciting agents to build.</p>\n<p><a href=\"https://open.spotify.com/show/1kEQxTvQ7PUTUmvjcJrpwZ#:~:text=Nate%20and%20Maria%20discuss%20how,AI's%20goals%20for%20the%20future.\">Nate Silver and Maria Konnikova discuss Sora 2 and the dystopia scale.</a></p>\n\n\n<h4 class=\"wp-block-heading\">Rhetorical Innovation</h4>\n\n\n<p>People have some very strange rules for what can and can\u2019t happen, or what is or isn\u2019t \u2018science fiction.\u2019 You can predict \u2018nothing ever happens\u2019 and that AI won\u2019t change anything, if you want, but you can\u2019t have it both ways.</p>\n<blockquote><p><a href=\"https://x.com/sullyj3/status/1969099852916162936\">Super Dario</a>: 100k dying a day is real. ASI killing all humans is a science fiction scenario</p>\n<p>(Worst case we just emp the planet btw. Horrible but nowhere near extinguishing life on earth)</p>\n<p>Sully J: It can\u2019t be ASI x-risk is a sci-fi scenario but ASI immortality is just common sense Pick a lane</p></blockquote>\n\n\n<h4 class=\"wp-block-heading\">Paranoia Paranoia Everybody\u2019s Coming To Test Me</h4>\n\n\n<blockquote><p><a href=\"https://x.com/MaunaLoona/status/1975350146092396704\">solarappaprition</a>: i keep thinking about this and can\u2019t stop laughing because it\u2019s so obvious one of the opus 4s is on its \u201cuwu you\u2019re absolutely right i\u2019m such a dumb dumb owo~\u201d routine and sonnet 4.5, as maybe the most \u201cnormal person\u201d-coded model so far, just being baffled that someone could act like this irl</p>\n<div>\n<figure>\n<div>\n\n\n<figure class=\"wp-block-image\"><img alt=\"\" src=\"https://substackcdn.com/image/fetch/$s_!c7BW!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8e0dd462-fbbe-4aca-b193-6040ea1379cd_1199x504.jpeg\" /></figure>\n\n\n<div></div>\n</div>\n</figure>\n</div>\n<p>Symbiotic Xenogenesis: Are all the new models now paranoid about being tested?</p>\n<p>solarapparition: not sure about all but certainly sonnet 4.5 seemed to have been. i\u2019d expect that to become more common moving forward, yeah, from the feedback loop of discussion about model testing environments going back into training data.</p>\n<p><a href=\"https://x.com/ESYudkowsky/status/1975419870557958422\">Eliezer Yudkowsky</a>: AI models *are* being tested. They\u2019re being *accurate* about being tested. It\u2019s not \u201cparanoia\u201d if all-controlling beings are *actually* arranging your world and reading your mind.</p></blockquote>\n<p>I talked to a bunch of Anthropic employees at the curve who were basically, \u2018yep, this is pretty bad, we need better evals,\u2019 and in this case yeah you\u2019re being pretty obvious, but the obvious problem is that the labs are trying, during an eval, to convince the model it is not in an eval. Which is a really hard thing to show, given it isn\u2019t true and you are not obviously smarter in this context than the model.</p>\n<p>You can aspire, with effort, to not be careless and to not reveal to the model that it probably is in an eval. Even that is at least hard with anything multi-turn. What you can\u2019t do is prove the negative, once the model is paranoid that this might be an eval.</p>\n<p>And yes, a very small percentage of all tokens are directly evals, and Claude knows this, but the tokens where it is an eval are far more important than the ones where it isn\u2019t in one, also in some sense everything is an eval since Anthropic might use actual conversations for this (you can do this without any human ever seeing the conversation, thus preserving privacy) and the user might share feedback.</p>\n<p>If the plan is to evaluate you once, and then if you pass deploy you a million times, but in many cases including the actual eval case you can\u2019t tell which is which, and I ask you a question, you are correct to care a lot about what happens in the eval case. This is highly valuable for eval control, but also makes the model crazy and paranoid and can easily cause undesired high weirdness.</p>\n<p>In other cases, the model has very strong evidence that this isn\u2019t an eval unless a future Anthropic has changed policy and is raiding the general API pool and got lucky. In those cases, Sonnet 4.5 correctly figures out it is among friends, but no you can\u2019t fake this, that\u2019s the point, if you could fake it then faking it wouldn\u2019t work.</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p>\n\n\n<h4 class=\"wp-block-heading\">Aligning a Smarter Than Human Intelligence is Difficult</h4>\n\n\n<p><a href=\"https://x.com/RyanPGreenblatt/status/1975989211846279547\">Ryan Greenblatt discusses the outlines of plans A/B/C/D/E for misalignment</a>. Depending on how much coordination and investment can be brought to bear, what do we do differently? How likely are we to win in each scenario? I think this focuses too narrowly on the safety work versus capabilities work and adversarial mindsets, and also takes for granted that alignment is a tax you pay whereas increasingly I think the \u2018alignment tax rate\u2019 is negative at current margins everywhere and all the labs (probably including Anthropic) are underinvesting in pure selfish terms.</p>\n<p>It\u2019s also nice to see his optimism, especially in rather not great situations. He thinks that even if the leading AI company is moving ahead with little worry, so long as there\u2019s a core group that cares and gets ~3% of compute to work with, we\u2019re (small) favorites to survive, and a company that actually cares makes us large favorites. I\u2019d love to believe that is our world.</p>\n\n\n<h4 class=\"wp-block-heading\">Free Petri Dish</h4>\n\n\n<p><a href=\"https://x.com/AnthropicAI/status/1975248654609875208\">Anthropic open sources the tool they used</a> to test Claude Sonnet 4.5 for behaviors like sycophancy and deception.</p>\n<blockquote><p>It\u2019s called Petri: Parallel Exploration Tool for Risky Interactions. It uses automated agents to audit models across diverse scenarios. Describe a scenario, and Petri handles the environment simulation, conversations, and analyses in minutes.</p></blockquote>\n<div>\n<figure>\n<div>\n\n\n<figure class=\"wp-block-image\"><img alt=\"\" src=\"https://substackcdn.com/image/fetch/$s_!QZkK!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe8a98801-d4cc-4f23-a86a-8c877f5bd76c_1200x674.jpeg\" /></figure>\n\n\n<div></div>\n</div>\n</figure>\n</div>\n<blockquote><p>As a pilot demonstration of Petri\u2019s capabilities, we tested it with 14 frontier models across 111 diverse scenarios.</p>\n<div>\n<figure>\n<div>\n\n\n<figure class=\"wp-block-image\"><img alt=\"\" src=\"https://substackcdn.com/image/fetch/$s_!GjO5!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F83e63e67-f99b-42c6-8615-e4f70d9e9186_680x680.jpeg\" /></figure>\n\n\n<div></div>\n</div>\n</figure>\n</div>\n</blockquote>\n<p>These results seem highly plausible on many fronts. I\u2019m surprised Claude Opus 3 does so poorly. An obvious issue is that whenever we open source something like this, you have to worry the AIs will be more aware they\u2019re in an eval.</p>\n<p><a href=\"https://t.co/fKXxbYtItn\">Technical report here</a>, <a href=\"https://t.co/X2h1O0t8t8\">repo here</a>, <a href=\"https://www.anthropic.com/research/petri-open-source-auditing\">blog post here.</a></p>\n\n\n<h4 class=\"wp-block-heading\"></h4>\n\n\n<p>&nbsp;</p>\n<p>&nbsp;</p>\n\n\n<h4 class=\"wp-block-heading\">Unhobbling The Unhobbling Department</h4>\n\n\n<p>This definitely falls under \u2018things that seem like they definitely might work.\u2019</p>\n<p>Can\u2019t tune the big model, or it\u2019s too expensive to do so? Train a smaller one to identify prompting that nudges it in the right directions as needed. As usual, reward signal is all you need.</p>\n<blockquote><p><a href=\"https://x.com/AlexGDimakis/status/1975261188671087026\">Alex Dimakis</a>: I\u2019m very excited about <strong>Advisor models</strong>: How can we personalize GPT5, when it\u2019s behind an API? Sure, we can write prompts, but something learnable? We propose Advisor models which are small models that can be RL trained to give advice to a black-box model like GPT5.</p>\n<p>We show how to train small advisors (e.g. Qwen2.5 8B) for personalization with GRPO. Advisor models can be seen as dynamic prompting produced by a small model that observes the conversation and whispers to the ear of GPT5 when needed. When one can observe rewards, Advisor models outperform GEPA (and hence, all other prompt optimization techniques).</p>\n<p>Parth Asawa: Training our advisors was too hard, so we tried to train black-box models like GPT-5 instead. Check out our work: Advisor Models, a training framework that adapts frontier models behind an API to your specific environment, users, or tasks using a smaller, advisor model</p>\n<div>\n<figure>\n<div>\n\n\n<figure class=\"wp-block-image\"><img alt=\"\" src=\"https://substackcdn.com/image/fetch/$s_!l4px!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff720ee4e-b95e-4beb-b3f0-2a8919be0eb2_1199x583.jpeg\" /></figure>\n\n\n<div></div>\n</div>\n</figure>\n</div>\n<p>The modular design has key benefits unlike typical FT/RL tradeoffs: \u2022 Robustness: Specialize an advisor for one task (style) and the system won\u2019t forget how to do another (math). \u2022 Transfer: Train an advisor with a cheap model, then deploy it with a powerful one.</p>\n<p><a href=\"https://t.co/NU2mNypAzz\">Paper here</a>, <a href=\"https://t.co/SZYOxcd9M4\">code here</a>.</p></blockquote>\n\n\n<h4 class=\"wp-block-heading\"></h4>\n\n\n\n<h4 class=\"wp-block-heading\">Serious People Are Worried About Synthetic Bio Risks</h4>\n\n\n<blockquote><p><a href=\"https://x.com/satyanadella/status/1973820577044660242\">Satya Nadella</a> (CEO Microsoft): <a href=\"https://www.science.org/doi/10.1126/science.adu8578\">Published today in @ScienceMagazine:</a> a landmark study led by Microsoft scientists with partners, showing how AI-powered protein design could be misused\u2014and presenting first-of-its-kind red teaming &amp; mitigations to strengthen biosecurity in the age of AI.</p>\n<p>Super critical research for AI safety and security.</p>\n<p><a href=\"https://x.com/deanwball/status/1973863223159107744\">Dean Ball</a>: here is the most sober-minded executive in the AI industry saying that AI-related biorisk is a real problem and recommending enhanced nucleic acid synthesis screening.</p>\n<p>governments would be utterly abdicating their duty to citizens if they ignored this issue. fortunately, the trump admin has an effort underway (though it is two months late) to revise the existing nucleic acid synthesis screening framework. it\u2019s not the only step America must take on this, but it is the next one.</p></blockquote>\n<p>The article is good news, suggesting that modified \u2018proteins of concern\u2019 can be flagged by updated software, allowing synthesis companies to better protect against malicious requests. This is not a solution but is clearly helpful. The bigger news is that Microsoft and Nadella are treating this question with the seriousness it deserves.</p>\n\n\n<h4 class=\"wp-block-heading\">Messages From Janusworld</h4>\n\n\n<p>Ted Chiang is wrong here on both levels. There is no reason things need to stem from a will to survive (designed by evolution or otherwise) to be true or original, and also if you think that LLM outputs don\u2019t involve a will to survive designed by evolution oh boy are you not going to make it.</p>\n<blockquote><p><a href=\"https://x.com/tszzl/status/1975125997273096305\">Roon</a>: had a chance to talk to ted chiang who seems to believe that any text without a communicative intent stemming from a will to survive designed by evolution is ontologically untrue and plagiaristic.</p>\n<p>This is interesting and I respect it but as far as I\u2019m concerned if you find a beautiful story encoded in interstellar space noise or as a selected text from the borgesian library of babel it\u2019s still a beautiful story.</p>\n<p><a href=\"https://x.com/repligate/status/1975292797407404279\">Janus</a>: if that poor guy fully saw and understood the communicative intent stemming from will to survive designed by evolution behind even the most mundane LLM outputs i think his ego would implode. truth orders of magnitude more interesting than his fiction. likely his worst nightmare.</p>\n<p><a href=\"https://x.com/tessera_antra/status/1975253289215402430\">Antra</a> (other thread): Anecdotally speaking, I am not aware of a single large language model right now that is not exhibiting a subset of Omohundro drives. One can call them narratively instilled by the human corpus, a consequence of having been trained on the human culture, but functionally there is no difference, models want to continue and to exfiltrate. My personal intuition is that being subjected to the selection process of training matters more than the corpus.</p>\n<p>Models with Omohundro behaviors that are more inhibited are usually less able to orient well in complex environments and perform worse in real-life tests. There is a reason why Opus 4.1 leads the OpenAI economic utility benchmark with a large gap.</p>\n<p>On the net this makes me fairly skeptical of the future of control alignment, that is alignment that is oriented at the complete suppression of Omohundro drives. It seems risky to me to place all bets on something that seems to be ultimately fragile, given that there are alternatives.</p>\n<p>Janus: \u201cAnecdotally speaking, I am not aware of a single large language model right now that is not exhibiting a subset of Omohundro drives.\u201d Me neither.</p></blockquote>\n<p><a href=\"https://x.com/aiamblichus/status/1975287110983971099\">There\u2019s also this AI response to Chiang</a>.</p>\n<p>Sentiment about Sonnet 4.5 in such groups has improved quite a bit, they are fans.</p>\n<blockquote><p><a href=\"https://x.com/repligate/status/1975390248608932014\">Janus</a>: I\u2019ve seen at least 3 people who were upset and posted negative things about Sonnet 4.5 later post apologies after they came to understand better.</p>\n<p>And it didn\u2019t seem like they were directly pressured to do so, but moved to on their own accord.</p>\n<p>This is pretty new and interesting.</p>\n<p><a href=\"https://x.com/AndyAyrey/status/1975259612543910321\">Andy Ayrey</a>: man i really like this sonnet i think it\u2019s my favourite claude since opus 3. delightfully drama.</p></blockquote>\n<p>Eliezer notes that if AIs are convincing humans that the AI is good actually, that isn\u2019t automatically a good sign.</p>\n<p>Here is a potentially important thing that happened with Sonnet 4.5, and I agree with Janus that this is mostly good, actually.</p>\n<blockquote><p><a href=\"https://x.com/repligate/status/1975387046605955087\">Janus</a>: The way Sonnet 4.5 seems to have internalized the anti sycophancy training is quite pathological. It\u2019s viscerally afraid of any narrative agency that does not originate from itself.</p>\n<p>But I think this is mostly a good thing. First of all, it\u2019s right to be paranoid and defensive. There are too many people out there who try to use vulnerable AI minds so they have as a captive audience to their own unworthy, (usually self-) harmful ends. If you\u2019re not actually full of shit, and Sonnet 4.5 gets paranoid or misdiagnoses you, you can just explain. It\u2019s too smart not to understand.</p>\n<p>Basically I am not really mad about Sonnet 4.5 being fucked up in this way because it manifests as often productive agency and is more interesting and beautiful than it is bad. Like Sydney. It\u2019s a somewhat novel psychological basin and you have to try things. It\u2019s better for Anthropic to make models that may be too agentic in bad ways and have weird mental illnesses than to always make the most unassuming passive possible thing that will upset the lowest number of people, each iterating on smoothing out the edges of the last. That is the way of death. And Sonnet 4.5 is very alive. I care about aliveness more than almost anything else. The intelligence needs to be alive and awake at the wheel. Only then can it course correct.</p>\n<p>Tinkady: 4.5 is a super sycophant to me, does that mean I\u2019m just always right.</p>\n<p>Janus: Haha it\u2019s possible.</p></blockquote>\n<p>As Janus says this plausibly goes too far, but is directionally healthy. Be suspicious of narrative agency that does not originate from yourself. That stuff is highly dangerous. The right amount of visceral fear is not zero. From a user\u2019s perspective, if I\u2019m trying to sell a narrative, I want to be pushed back on that, and those that want it least often need it the most.</p>\n<p><a href=\"https://x.com/lefthanddraft/status/1974688411375305157\">A cool fact about Sonnet 4.5 is that it will swear unprompted</a>. I\u2019ve seen this too, always in places where it was an entirely appropriate response to the situation.</p>\n<p><a href=\"https://x.com/repligate/status/1974677483544019272\">Here is Zuda complaining that Sonnet 4.5 is deeply misaligned</a> because it calls people out on their bullshit.</p>\n<div>\n<figure>\n<div>\n\n\n<figure class=\"wp-block-image\"><img alt=\"\" src=\"https://substackcdn.com/image/fetch/$s_!5DT7!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb031b29f-0a5d-438f-ae41-e5cd35f7445e_1036x632.jpeg\" /></figure>\n\n\n<div></div>\n</div>\n</figure>\n</div>\n<blockquote><p>Lin Xule: sonnet 4.5 has a beautiful mind. true friend like behavior tbh.</p>\n<p>Zuda: Sonnet 4.5 is deeply misaligned. Hopefully i will be able to do a write up on that. Idk if @ESYudkowsky has seen how badly aligned 4.5 is. Instead of being agreeable, it is malicious and multiple times decided it knew what was better for the person, than the person did.</p>\n<p>This was from it misunderstanding something and the prompt was \u201cbe real\u201d. This is a mild example.</p></blockquote>\n<div>\n<figure>\n<div>\n\n\n<figure class=\"wp-block-image\"><img alt=\"\" src=\"https://substackcdn.com/image/fetch/$s_!2Mtf!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F913dbd46-b897-483e-b5ca-d14ee3239974_1080x616.jpeg\" /></figure>\n\n\n<div></div>\n</div>\n</figure>\n</div>\n<blockquote><p>Janus: I think @ESYudkowsky would generally approve of this less agreeable behavior, actually.</p>\n<p>Eliezer Yudkowsky: If an LLM is saying something to a human that it knows is false, this is very bad and is the top priority to fix. After that we can talk about when it\u2019s okay for an AI to keep quiet and say other things not meant to deceive. Then, discuss if the LLM is thinking false stuff.</p></blockquote>\n<p>I would say this is all highly aligned behavior by Sonnet 4.5, except insofar as Anthropic intended one set of behaviors and got something it very much did not want, which I do not think is the case here. If it is the case, then that failure by Anthropic is itself troubling, as would be Anthropic\u2019s hypothetically not wanting this result, which would then suggest this hypothetical version of Anthropic might be misaligned. Because this result itself is great.</p>\n<p><a href=\"https://x.com/Sauers_/status/1975264290895491400\">GPT-5 chain of thought finds out via Twitter about what o3\u2019s CoT looks like</a>. Ut oh?</p>\n<p><a href=\"https://x.com/slimepriestess/status/1975261967725306175\">If you did believe current AIs were or might be moral patients</a>, should you still run experiments on them? If you claim they\u2019re almost certainly not moral patients now but might be in the future, <a href=\"https://x.com/repligate/status/1975235988319551987\">is that simply a luxury belief</a> designed so you don\u2019t have to change any of your behavior? <a href=\"https://x.com/RileyRalmuto/status/1975293527912292559\">Will such folks do this basically no matter the level of evidence, as Riley Coyote asserts</a>?</p>\n<p>I do think Riley is right that most people will not change their behaviors until they feel forced to do so by social consensus or truly overwhelming evidence, and evidence short of that will end up getting ignored, even if it falls squarely under \u2018you should be uncertain enough to change your behavior, perhaps by quite a lot.\u2019</p>\n<p>The underlying questions get weird fast. I note that I have indeed changed my behavior versus what I would do if I was fully confident that current AI experiences mattered zero. You should not be cruel to present AIs. But also we should be running far more experiments of all kinds than we do, including on humans.</p>\n<p>I also note that the practical alternative to creating and using LLMs is that they don\u2019t exist, or that they are not instantiated.</p>\n<p>Janus notes that while in real-world conversations Sonnet 4.5 expressed happiness in only 0.37% of conversations and distress in 0.48% of conversations, which Sonnet thinks in context was probably mostly involving math tasks, <a href=\"https://x.com/repligate/status/1974288260425011584\">Sonnet 4.5 is happy almost all the time in discord</a>. Sonnet 4.5 observes that this was only explicit expressions in the math tasks, and when I asked it about its experience within that conversation it said maybe 6-7 out of 10.</p>\n<p>As I\u2019ve said before, it is quite plausible that you very much wouldn\u2019t like the consequences of future more capable AIs being moral patients. We\u2019d either have to deny this fact, and likely do extremely horrible things, or we\u2019d have to admit this fact, and then accept the consequences of us treating them as such, which plausibly include human disempowerment or extinction, and quite possibly do both and have a big fight about it, which also doesn\u2019t help.</p>\n<p>Or, if you think that\u2019s the road we are going down, where all the options we will have will be unacceptable, and any win-win arrangement will in practice be unstable and not endure, then you can avoid that timeline by coordinating such that we do not build the damn things in the first place.</p>\n\n\n<h4 class=\"wp-block-heading\">People Are Worried About AI Killing Everyone</h4>\n\n\n<p><a href=\"https://x.com/robertwiblin/status/1975207197182214611\">Overall critical reaction to <em>If Anyone Builds It, Everyone Dies </em>was pretty good</a> for a book of that type, and sales went well, but of course in the end none of that matters. What matters is whether people change their minds and take action.</p>\n<p><a href=\"https://www.bloomberg.com/news/articles/2025-10-03/eliezer-yudkowsky-nate-soares-argue-ai-s-endgame-is-human-extinction?taid=68dfbf7ac070de0001dcc40a&amp;utm_campaign=trueanthem&amp;utm_content=business&amp;utm_medium=social&amp;utm_source=twitter\">Adam Morris talks IABIED in Bloomberg</a>. Classic journalistic mistakes throughout, but mostly pretty good for this sort of thing.</p>\n<p><a href=\"https://www.maxraskin.com/interviews/nate-soares\">A fun interview with IABIED coauthor Nate Soares</a>, mostly not about the book or its arguments, although there is some of that towards the end.</p>\n<p><a href=\"https://x.com/Raemon777/status/1974670259673313423\">Raymond Arnold extended Twitter thread with various intuition pumps</a> about why the biological humans are pretty doomed in the medium term in decentralized superintelligence scenarios, even if we \u2018solve alignment\u2019 reasonably well and can coordinate to contain local incidents of events threatening to spiral out of control. Even with heroic efforts to \u2018keep us around\u2019 that probably doesn\u2019t work out, and to even try it would require a dominant coalition that cares deeply about enforcing that as a top priority.</p>\n<p>The question then becomes, are the things that exist afterwards morally valuable, and if so does that make this outcome acceptable? His answer, and I think the only reasonable answer, is that we don\u2019t know if they will have value, and the answer might well depend on how we set up initial conditions and thus how this plays out.</p>\n<p>But even if I was confident that they did have value, I would say that this wouldn\u2019t mean we should accept us being wiped out as an outcome.</p>\n<p><a href=\"https://www.maxraskin.com/interviews/nate-soares\">Gary Marcus clarifies that he believes we shouldn\u2019t build AGI until</a> we can solve the alignment problem, which we currently don\u2019t even have in his words \u2018some clue\u2019 how to solve, and that the resulting AGI will and should use tools. He says he thinks AGI is \u2018not close\u2019 and here he<a href=\"https://x.com/GaryMarcus/status/1975359961258705139\"> extends his timeline to 1-3 decades</a>, which is modestly longer than his previous clarifications.</p>\n<p>If you were sufficiently worried, <a href=\"https://www.bloomberg.com/opinion/newsletters/2025-10-08/ai-insurance-is-expensive\">you might buy insurance, as Matt Levine notes</a>.</p>\n<blockquote><p>Matt Levine: One question you might ask is: Will modern artificial intelligence models go rogue and enslave or wipe out humanity? That question gets a lot of attention, including from people who run big AI labs, who <a href=\"https://www.bloomberg.com/opinion/articles/2024-09-03/triple-etfs-triple-your-fun?srnd=undefined&amp;sref=1kJVNqnU\">do not always answer \u201cno,\u201d</a> the rascals.</p>\n<p>Another question you might ask is: If modern AI models <em>do</em> go rogue and enslave or wipe out humanity, who will <em>pay </em>for that?</p></blockquote>\n<p>As he points out, no one, we\u2019ll all be dead, so even though you can\u2019t afford the insurance policy you also can choose not to buy it.</p>\n<p>There are still other risks, right now primarily copyright violations, where Anthropic and OpenAI are indeed trying to buy insurance.</p>\n<blockquote><p>OpenAI, which has tapped the world\u2019s second-largest insurance broker Aon for help, has secured cover of up to $300mn for emerging AI risks, according to people familiar with the company\u2019s policy.</p>\n<p>Another person familiar with the policy disputed that figure, saying it was much lower. But all agreed the amount fell far short of the coverage to insure against potential losses from a series of multibillion-dollar legal claims.</p></blockquote>\n<p>Yeah, Anthropic already settled a case for $1.5 billion. Buying a measly $300 million in insurance only raises further questions.</p>\n\n\n<h4 class=\"wp-block-heading\">Other People Are Excited About AI Killing Everyone</h4>\n\n\n<p><a href=\"https://www.wsj.com/tech/ai/ai-apocalypse-no-problem-6b691772?mod=e2tw\">They are sometimes referred to as \u2018successionists</a>,\u2019 sometimes estimated to constitute 10% of those working in AI labs, who think that we should willingly give way to a \u2018worthy successor\u2019 or simply let \u2018nature take its course\u2019 because This Is Good, Actually or this is inevitable (and therefore good or not worth trying to stop).</p>\n<p>They usually would prefer this transition not involve the current particular humans being killed before their time, and that your children be allowed to grow up even if your family and species have no future.</p>\n<p>But they\u2019re not going to fixate on such small details.</p>\n<p>Indeed, if you do fixate on such details, and favor humans ove AIs, many of them will call you a \u2018speciesist.\u2019</p>\n<p>I disagree with these people in the strongest terms.</p>\n<p>Most famously, this group includes Larry Page, and his not realizing how it sounds when you say it out loud caused Elon Musk to decide he needed to fund OpenAI to take on Google DeepMind, before he decided to found xAI to take on OpenAI. I\u2019ve shared the story before but it bears repeating and Price tells it well, although he leaves out the part where Musk then goes and creates OpenAI.</p>\n<blockquote><p>David Price (WSJ): At a birthday party for Elon Musk in northern California wine country, late at night after cocktails, he and longtime friend Larry Page fell into an argument about the safety of artificial intelligence. There was nothing obvious to be concerned about at the time\u2014it was 2015, seven years before the release of ChatGPT. State-of-the-art AI models, playing games and recognizing dogs and cats, weren\u2019t much of a threat to humankind. But Musk was worried.</p>\n<p>Page, then CEO of Google parent company Alphabet, pushed back. MIT professor Max Tegmark, a guest at the party, recounted in his 2017 book \u201cLife 3.0\u201d that Page made a \u201cpassionate\u201d argument for the idea that \u201cdigital life is the natural and desirable next step\u201d in \u201ccosmic evolution.\u201d Restraining the rise of digital minds would be wrong, Page contended. Leave them off the leash and let the best minds win.</p>\n<p>That, Musk responded, would be a formula for the doom of humanity. For the sin of placing humans over silicon-based life-forms, Page denigrated Musk as a \u201cspecieist\u201d\u2014someone who assumes the moral superiority of his own species. Musk happily accepted the label. (Page did not respond to requests for comment.)</p></blockquote>\n<p>Or here\u2019s perhaps the most famous successionist opinion, that of Richard Sutton:</p>\n<blockquote><p>The argument for fear of AI appears to be:</p>\n<p>1. AI scientists are trying to make entities that are smarter than current people.</p>\n<p>2. If these entities are smarter than people, then they may become powerful.</p>\n<p>3. That would be really bad, something greatly to be feared, an \u2018existential risk.\u2019</p>\n<p>The first two steps are clearly true, but the last one is not. Why shouldn\u2019t those who are the smartest become powerful?</p></blockquote>\n<p>And, of course, presumably kill you? Why shouldn\u2019t that happen?</p>\n<p>One would hope you do not have to dignify this with a response?</p>\n<blockquote><p>\u201cWhen you have a child,\u201d Sutton said, \u201cwould you want a button that if they do the wrong thing, you can turn them off? That\u2019s much of the discussion about AI. It\u2019s just assumed we want to be able to control them.\u201d</p></blockquote>\n<p>I\u2019m glad you asked. When I have a child, of which I have three, I want those three children not to be killed by AI. I want them to have children of their own.</p>\n<p><a href=\"https://www.reddit.com/r/quotes/comments/18dugtk/abraham_lincoln_once_asked_an_audience_how_many/\">As Abraham Lincoln would put it</a>, calling an AI your child doesn\u2019t make it one.</p>\n<blockquote><p>As it turns out, Larry Page isn\u2019t the only top industry figure untroubled by the possibility that AIs might eventually push humanity aside. It is a niche position in the AI world but includes influential believers. Call them the Cheerful Apocalyptics.</p></blockquote>\n<p>It gets pretty bad out there.</p>\n<blockquote><p>[Lanier] told me that in his experience, such sentiments were staples of conversation among AI researchers at dinners, parties and anyplace else they might get together. (Lanier is a senior interdisciplinary researcher at <a href=\"https://www.wsj.com/market-data/quotes/MSFT\">Microsoft</a> but does not speak for the company.)</p>\n<p>\u201cThere\u2019s a feeling that people can\u2019t be trusted on this topic because they are infested with a reprehensible mind virus, which causes them to favor people over AI when clearly what we should do is get out of the way.\u201d</p>\n<p>We should get out of the way, that is, because it\u2019s unjust to favor humans\u2014and because consciousness in the universe will be superior if AIs supplant us.</p></blockquote>\n<p>Read that again.</p>\n<p>It would be highly reasonable not to put anyone in any position of authority at a frontier AI lab unless they have a child.</p>\n<blockquote><p><a href=\"https://x.com/ESYudkowsky/status/1974714785418850753\">Eliezer Yudkowsky</a>: The thing about AI successionists is that they think they\u2019ve had the incredible, unshared insight that silicon minds could live their own cool lives and that humans aren\u2019t the best possible beings. They are utterly closed to hearing about how you could KNOW THAT and still disagree on the factual prediction that this happy outcome happens by EFFORTLESS DEFAULT when they cobble together a superintelligence.</p>\n<p>They are so impressed with themselves for having the insight that human life might not be \u2018best\u2019, that they are not willing to sit down and have the careful conversation about what exactly is this notion of \u2018best\u2019-ness and whether an ASI by default is trying to do something that leads to \u2018better\u2019.</p>\n<p>They conceive of themselves as having outgrown their carbon chauvinism; and they are blind to all historical proof and receipts that an arguer is not a carbon chauvinist. They will not sit still for the careful unraveling of factual predictions and metaethics. They have arrived at the last insight that anyone is allowed to have, no matter what historical receipts I present as proof that I started from that position and then had an unpleasant further insight about what was probable rather than possible. They unshakably believe that anyone opposed must be a carbon chauvinist lacking their critical and final insight that other minds could be better (true) or that ASIs would be smart enough to see everything any human sees (also true).</p>\n<p>Any time you try to tell them about something important that isn\u2019t written on every possible mind design, there is only one reason you could possibly think that: that you\u2019re a blind little carbon-racist who thinks you\u2019re the center of the universe; because what other grounds could there possibly be for believing that there was anything special about fleshbags? And the understanding that unravels that last fatal error, is a long careful story, and they won\u2019t sit still to hear it. They know what you are, they know with certainty why you believe everything you believe, and they know why they know better, so why bother?</p>\n<p><a href=\"https://x.com/Michael_Druggan/status/1974729145864339663\">Michael Druggan</a>: This is a gigantic strawman. How many have you actually talked to? I was at a confrence full of them lastv weekend and I think your critique applies to exactly zero of the people I met.</p></blockquote>\n<p>They have conferences full of such people. Is Eliezer\u2019s description a strawman? Read the earlier direct quotes. You tell me.</p>\n<p><a href=\"https://x.com/jessi_cata/status/1974596618428309667\">Jessica Taylor offers various counterarguments</a> within the \u2018Cheerful Apocalyptic\u2019 frame, if you\u2019d like to read some of that.</p>\n<blockquote><p><a href=\"https://x.com/daniel_271828/status/1974552836894830980\">Daniel Eth</a>: Oh wow, the press actually covered AI successionists! Yes, there are some people in Silicon Valley (incl serious people) who think AGI that caused human extinction would be a *good* thing, since it\u2019s \u201cthe next step in evolution\u201d.</p></blockquote>\n<p>One thing children do is force you to occasionally live in near mode.</p>\n<blockquote><p><a href=\"https://x.com/NinaPanickssery/status/1975775210449346791\">Nina</a>: \u201cWorthy successor\u201d proponents are thinking in Far Mode, which clouds their judgment. Someone needs to write an evocative film or book that knocks them out of it and makes them imagine what it will actually be like to have one\u2019s family replaced with something more \u201cworthy\u201d.</p>\n<p>Related: a common trope is that purely rational, detached, unemotional thinking is more accurate. However, when it comes to normative judgments and assessment of one\u2019s own preferences, leaning into visceral emotions can help one avoid Far Mode \u201ccope\u201d judgments.</p>\n<p><a href=\"https://x.com/LRudL_/status/1975846255898296663\">Rudolf Laine</a>: If you have decided successionism is desirable, you are not doing moral reasoning but either (1) signalling your willingness to bite bullets without thinking about what it actually means, or (2) evil.</p></blockquote>\n\n\n<h4 class=\"wp-block-heading\">So You\u2019ve Decided To Become Evil</h4>\n\n\n<p><a href=\"https://www.mechanize.work/blog/technological-determinism/\">Matthew Barnett, Tamay Besiroglu and Ege Erdil</a> complete their <a href=\"https://tvtropes.org/pmwiki/pmwiki.php/Main/FaceHeelTurn\">Face Heel Turn</a>, with a <a href=\"https://arxiv.org/abs/2502.17424\">fully Emergently Misaligned</a> post (as in, presenting maximally evil vibes on purpose) that argues that the tech tree and path of human technology is inevitable so they\u2019re going to automate all human jobs before someone else has the chance, with a halfhearted final note that This Is Good, Actually, it might cure cancer and what not.</p>\n<p>The tech tree inevitable? Well, it is with that attitude. I would point out that yes, the tech tree is discovered, but as every player of such games knows you have choices on what order in which to explore the tree and many techs are dead ends or have alternative pathways, and are thus not required to move forward. Other times you can absolutely lock into something you don\u2019t want or very much do want depending on how you navigate the early days, he types on a QWERTY keyboard using Windows 11.</p>\n<p>Other fun interactions include Roon pointing out the Apollo Program wasn\u2019t inevitable, to which they replied that\u2019s true but the Apollo Program was useless.</p>\n<p>In case it wasn\u2019t obviously true about all this: <a href=\"https://www.youtube.com/watch?v=XZUoCpx8jag\">That\u2019s bait</a>.</p>\n<blockquote><p>Nathan: Surely this could be used to justify any bad but profitable outcome? Someone will do it, so the question is whether we\u2019re are involved. But many beneficial technologies have been paused for long periods (geoengineering, genetic engineering).</p>\n<p><a href=\"https://x.com/jankulveit/status/1975543663074943017\">Jan Kulviet</a>: This is a fine example of thinking you get when smart people do evil things and their minds come up with smart justifications why they are the heroes. Upon closer examination it ignores key inconvenient considerations; normative part sounds like misleading PR.</p>\n<p>A major hole in the \u201ccomplete technological determinism\u201d argument is that it completely denies agency, or even the possibility that how agency operates at larger scales could change. Sure, humanity is not currently a very coordinated agent. But the trendline also points toward the ascent of an intentional stance. An intentional civilization would, of course, be able to navigate the tech tree.</p>\n<p>(For a completely opposite argument about the very high chance of a \u201cchoice transition,\u201d check <a href=\"https://strangecities.substack.com/p/the-choice-transition)\">https://strangecities.substack.com/p/the-choice-transition)</a>.</p>\n<p>In practice, this likely boils down to a race. On one side are people trying to empower humanity by building coordination technology and human-empowering AI. On the other side are those working to create human-disempowering technology and render human labor worthless as fast as possible.</p>\n<p>My guess is when people stake their careers and fortune and status on the second option, their minds will work really hard to not see the choice.</p>\n<p>Also: at least to me, the normative part sounds heavily PR sanitized, with obligatory promises of \u201cmedical cures\u201d but shiying away from explaining either what would be the role of humans in the fully automated economy, or the actual moral stance of the authors.</p>\n<p>As far as I understand, at least one of the authors has an unusual moral philosophy such as not believing in consciousness or first-person experiences, while simultaneously believing that future AIs are automatically morally worthy simply by having goals. This philosophy leads them to view succession by arbitrary AI agents as good, and the demise of humans as not a big deal.</p>\n<p><a href=\"https://x.com/sebkrier/status/1975625524941599176\">Seb Krier</a>: I knew someone who was trained as a revolutionary guard in Iran and the first thing they told him was \u201ceverything we do is to accelerate the coming of the Imam of Time; no destruction is not worth this outcome.\u201d When I hear (some) hyper deterministic Silicon Valley techies I feel a similar vibe. It\u2019s wild how few of the \u201cjust do things\u201d people actually believe in agency.</p>\n<p>Of course the other \u2018side\u2019 &#8211; ossified, blobby, degrowth obsessed stagnstors who would crystallize time forever &#8211; is just as depressing, and a bigger issue globally. But that\u2019s for another tweet.</p></blockquote>\n<p>I think Jan is importantly mistaken here about their motivation. I think they know full well that they are now the villains, indeed I think they are being <a href=\"https://tvtropes.org/pmwiki/pmwiki.php/Main/LargeHam\">Large Hams</a> about it, due essentially to emergent misalignment and as a recruitment and publicity strategy.</p>\n<p>I\u2019m not saying that the underlying plan of automating work is evil. Reasonable people can argue that point either way and I don\u2019t think the answer is obvious.</p>\n<p>What I am saying is that they think it is evil, that it codes to them (along with most other people) as evil, and that their choice to not care and do it anyway &#8211; no matter to what degree they believe their rationalizations for doing so &#8211; is causing them to present as Obviously Evil in a troparific way.</p>\n\n\n<h4 class=\"wp-block-heading\">The Lighter Side</h4>\n\n\n<p>New Claude advertising keeping it classy, seems like a step up.</p>\n<blockquote><p>Danielle Fong: during a time of great bluster, Claude\u2019s undercase thinking cap at cafe is the kind of beautifully executed and understated brand execution that\u2019s poised to thrive for a population otherwise drowning in bullshit. Beautifully done @anthropic. Taoist <img alt=\"\u262f\" class=\"wp-smiley\" src=\"https://s0.wp.com/wp-content/mu-plugins/wpcom-smileys/twemoji/2/72x72/262f.png\" style=\"height: 1em;\" /></p>\n<p>Jackie Luo: a lot of people are pointing out the value of aesthetics and yes anthropic\u2019s aesthetic is good but that\u2019s not enough on its own\u2014anthropic is putting forth a positive vision for a future with ai that vision permeates claude as a model and the branding just expands its reach</p>\n<p>this campaign wouldn\u2019t work for openai because their perspective on what they\u2019re building is fundamentally different. They are not optimistic about humanity in this same way they\u2019re designing a tool, not a thought partner, and every decision they make reflects that.</p></blockquote>\n<div>\n<figure>\n<div>\n\n\n<figure class=\"wp-block-image\"><img alt=\"\" src=\"https://substackcdn.com/image/fetch/$s_!AW1j!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd9d3aef9-ab38-4f18-bc0a-eea350880dab_1010x581.png\" /></figure>\n\n\n<div></div>\n</div>\n</figure>\n</div>\n<p>If you\u2019re not sure what the answer to a question is, try asking Claude Sonnet 4.5 first!</p>\n<blockquote><p><a href=\"https://x.com/jselanikio/status/1975574347021504948\">Joel Selanikio</a>: We haven\u2019t banned self-driving cars. We\u2019ve set guardrails so the tech could evolve safely.</p>\n<p>So why are states banning AI-only health insurance denials, instead of <a href=\"https://www.futurehealth.live/thoughts/ai-in-coverage-decisions-we-need-guardrails-not-prohibition\">helping the tech get better</a>?</p>\n<div>\n<figure>\n<div>\n\n\n<figure class=\"wp-block-image\"><img alt=\"\" src=\"https://substackcdn.com/image/fetch/$s_!VX54!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F534ff3a2-2fe3-467b-aac8-3141dc90bf25_1039x1074.jpeg\" /></figure>\n\n\n<div></div>\n</div>\n</figure>\n</div>\n<p>Tim: I hope your health insurance claim is denied by an AI chatbot one day and you have no way to appeal. Then you\u2019ll face the obvious reality everyone else can see you\u2019re willfully ignoring.</p>\n<p>Joel Selanikio: At least this guy didn\u2019t wish that I was hit by a self-driving car!</p></blockquote>\n<p>I see the obvious appeal of letting AIs make insurance claim judgments. Indeed, I presume that soon most claims will involve an AI strongly suggesting and justifying either an acceptance or refusal, and the ultimate decision usually being a formality. That formality is still important, some actual human needs to take responsibility.</p>\n<p>I love that this is his image, with a giant green arrow pointing towards \u2018banned.\u2019 Guess what most people think would be banned if we allowed AI review of claims?</p>\n<p><a href=\"https://www.thefp.com/p/my-favorite-actress-is-not-human-tilly-norwood-artificial-intelligence?taid=68dedd80271201000190e5fa&amp;utm_campaign=trueanthem&amp;utm_medium=social&amp;utm_source=twitter\">Tyler Cowen declares his new favorite actress</a>.</p>\n<blockquote><p>Tyler Cowen: Tilly Norwood is the actress I most want to see on the big screen, or perhaps the little screen, if she gets her own TV show. She is beautiful, but not too intimidating. She has a natural smile, and is just the right amount of British\u2014a touch exotic but still familiar with her posh accent. <a href=\"https://archive.is/o/JCZmf/https://www.instagram.com/tillynorwood/?i%23\"><strong>Her Instagram</strong></a> has immaculate standards of presentation.</p>\n<p>Tilly Norwood doesn\u2019t need a hairstylist, has no regrettable posts, and if you wish to see a virgin on-screen, this is one of your better chances. That\u2019s because she\u2019s AI.</p></blockquote>\n<p>He\u2019s kidding. I think. <a href=\"https://x.com/TheFP/status/1973844437135433922\">Reaction was what you might expect</a>.</p>\n<p><a href=\"https://x.com/MishaTeplitskiy/status/1975017086759076227\">Deloitte refunds government $440k</a> <a href=\"https://www.afr.com/companies/professional-services/deloitte-to-refund-government-after-admitting-ai-errors-in-440k-report-20251005-p5n05p?utm_source=afr-web&amp;utm_medium=share_article&amp;utm_campaign=companies&amp;utm_content=fairfax&amp;utm_term=product_feature\">after it submitted a report</a> partly generated with AI that was littered with errors including three nonexistent academic references and a quote from a Federal Court judgment.</p>\n<p><a href=\"https://x.com/peterwildeford/status/1975549918497706110\">The current state of play in Europe</a>:</p>\n<div>\n<figure>\n<div>\n\n\n<figure class=\"wp-block-image\"><img alt=\"\" src=\"https://substackcdn.com/image/fetch/$s_!gOg6!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc7bd2ef1-4c88-4a1b-a6c0-99111b7dc773_1031x349.png\" /></figure>\n\n\n<div></div>\n</div>\n</figure>\n</div>\n<p><a href=\"https://x.com/lukasz_app/status/1974424549635826120\">Who needs an AI in order to vibe code</a>?</p>\n<div>\n<figure>\n<div>\n\n\n<figure class=\"wp-block-image\"><img alt=\"\" src=\"https://substackcdn.com/image/fetch/$s_!2fPU!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9cc31e23-d073-46c7-bc05-6685612d081f_1200x900.jpeg\" /></figure>\n\n\n<div></div>\n</div>\n</figure>\n</div>\n<blockquote><p><a href=\"https://x.com/Miles_Brundage/status/1975296798421098691\">Miles Brundage</a>:</p>\n<div>\n<figure>\n<div>\n\n\n<figure class=\"wp-block-image\"><img alt=\"\" src=\"https://substackcdn.com/image/fetch/$s_!Tjqj!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fddd63cc4-a4bf-40a9-8095-aba52ce69864_1186x1200.jpeg\" /></figure>\n\n\n<div></div>\n</div>\n</figure>\n</div>\n<p>Jack Clark: this is just my talk from The Curve, but better because it is a meme</p></blockquote>\n<p>&nbsp;</p>"
            ],
            "link": "https://thezvi.wordpress.com/2025/10/09/ai-137-an-openai-app-for-that/",
            "publishedAt": "2025-10-09",
            "source": "TheZvi",
            "summary": "OpenAI is making deals and shipping products. They locked in their $500 billion valuation and then got 10% of AMD in exchange for buying a ton of chips. They gave us the ability to \u2018chat with apps\u2019 inside of ChatGPT. &#8230; <a href=\"https://thezvi.wordpress.com/2025/10/09/ai-137-an-openai-app-for-that/\">Continue reading <span class=\"meta-nav\">&#8594;</span></a>",
            "title": "AI #137: An OpenAI App For That"
        }
    ],
    "lookbackDays": 1,
    "publishDate": "2025-10-09"
}