{
    "articles": [
        {
            "content": [
                "<p>Testing code that makes HTTP requests can be difficult.\nReal requests are slow, flaky, and hard to control.\nThat\u2019s why I use a Python library called <a href=\"https://vcrpy.readthedocs.io/\">vcrpy</a>, which does a one-off recording of real HTTP interactions, then replays them during future tests.</p>\n\n<p>These recordings are saved to a \u201ccassette\u201d \u2013 a plaintext file that I keep alongside my tests and my code.\nThe cassette ensures that all my tests get consistent HTTP responses, which makes them faster and more reliable, especially in CI.\nI only have to make one real network request, and then I can run my tests locally and offline.</p>\n\n<p>In this post, I\u2019ll show you how I use vcrpy in a production codebase \u2013 not just the basics, but also the patterns, pitfalls, and fixtures that make it work for a real team.</p>\n\n<blockquote class=\"table_of_contents\">\n  <h3>Table of contents</h3>\n\n  <ul>\n    \n      <li>\n        <a href=\"https://alexwlchan.net/2025/testing-with-vcrpy/#why_not_real_requests\">Why not make real HTTP requests in tests?</a>\n\n        \n          <ul>\n            \n            <li><a href=\"https://alexwlchan.net/2025/testing-with-vcrpy/#it-makes-my-tests-slower\">It makes my tests slower</a></li>\n            \n            <li><a href=\"https://alexwlchan.net/2025/testing-with-vcrpy/#it-makes-my-tests-less-reliable\">It makes my tests less reliable</a></li>\n            \n            <li><a href=\"https://alexwlchan.net/2025/testing-with-vcrpy/#it-makes-my-tests-more-brittle\">It makes my tests more brittle</a></li>\n            \n            <li><a href=\"https://alexwlchan.net/2025/testing-with-vcrpy/#it-means-passing-around-more-secrets\">It means passing around more secrets</a></li>\n            \n            <li><a href=\"https://alexwlchan.net/2025/testing-with-vcrpy/#it-makes-my-tests-harder-to-debug\">It makes my tests harder to debug</a></li>\n            \n            <li><a href=\"https://alexwlchan.net/2025/testing-with-vcrpy/#recording-and-replaying-http-requests-solves-these-problems\">Recording and replaying HTTP requests solves these problems</a></li>\n            \n          </ul>\n        \n      </li>\n      <li>\n        <a href=\"https://alexwlchan.net/2025/testing-with-vcrpy/#why-do-you-like-vcrpy\">Why do you like vcrpy?</a>\n\n        \n      </li>\n      <li>\n        <a href=\"https://alexwlchan.net/2025/testing-with-vcrpy/#a-basic-example-of-using-vcrpy\">A basic example of using vcrpy</a>\n\n        \n      </li>\n      <li>\n        <a href=\"https://alexwlchan.net/2025/testing-with-vcrpy/#using-vcrpy-in-production\">Using vcrpy in production</a>\n\n        \n          <ul>\n            \n            <li><a href=\"https://alexwlchan.net/2025/testing-with-vcrpy/#keeping-secrets-out-of-my-cassettes\">Keeping secrets out of my cassettes</a></li>\n            \n            <li><a href=\"https://alexwlchan.net/2025/testing-with-vcrpy/#improving-the-human-readability-of-cassettes\">Improving the human readability of cassettes</a></li>\n            \n            <li><a href=\"https://alexwlchan.net/2025/testing-with-vcrpy/#naming-my-cassettes-to-make-sense-later\">Naming my cassettes to make sense later</a></li>\n            \n            <li><a href=\"https://alexwlchan.net/2025/testing-with-vcrpy/#explaining-how-to-use-cassettes-with-helpful-errors\">Explaining how to use cassettes with helpful errors</a></li>\n            \n            <li><a href=\"https://alexwlchan.net/2025/testing-with-vcrpy/#wrapping-everything-in-a-fixture-for-convenience\">Wrapping everything in a fixture for convenience</a></li>\n            \n          </ul>\n        \n      </li>\n      <li>\n        <a href=\"https://alexwlchan.net/2025/testing-with-vcrpy/#when-i-dont-vcrpy\">When I don\u2019t vcrpy</a>\n\n        \n          <ul>\n            \n            <li><a href=\"https://alexwlchan.net/2025/testing-with-vcrpy/#if-im-testing-error-handling\">If I\u2019m testing error handling</a></li>\n            \n            <li><a href=\"https://alexwlchan.net/2025/testing-with-vcrpy/#if-im-fetching-lots-of-binary-files\">If I\u2019m fetching lots of binary files</a></li>\n            \n            <li><a href=\"https://alexwlchan.net/2025/testing-with-vcrpy/#if-im-testing-future-or-hypothetical-changes-in-an-api\">If I\u2019m testing future or hypothetical changes in an API</a></li>\n            \n          </ul>\n        \n      </li>\n      <li>\n        <a href=\"https://alexwlchan.net/2025/testing-with-vcrpy/#summary\">Summary</a>\n\n        \n      </li>\n</ul>\n</blockquote>\n\n\n\n<figure>\n  \n\n<source type=\"image/avif\" /><source type=\"image/webp\" /><source type=\"image/jpeg\" /><img alt=\"A pile of three black video cassette tapes stacked on a wooden table.\" src=\"https://alexwlchan.net/images/2025/pexels-inspiredimages-157541_1x.jpg\" width=\"750\" />\n\n\n  <figcaption>\n    Three black <a href=\"https://en.wikipedia.org/wiki/Compact_Video_Cassette\">compact video cassette</a> tapes.\n    Photo credit: <a href=\"https://www.pexels.com/photo/3-black-vhs-157541/\">Anthony on Pexels</a>.\n  </figcaption>\n</figure>\n\n<h2 id=\"why_not_real_requests\">Why not make real HTTP requests in tests?</h2>\n\n<p>There are several reasons why I avoid real HTTP requests in my tests:</p>\n\n<h3 id=\"it-makes-my-tests-slower\">It makes my tests slower</h3>\n\n<p>I want my tests to be fast, because then I\u2019ll run them more often and catch mistakes sooner.\nAn individual HTTP call might be quick, but stack up hundreds of them and tests really start to drag.</p>\n\n<h3 id=\"it-makes-my-tests-less-reliable\">It makes my tests less reliable</h3>\n\n<p>Even if my code is correct, my tests could fail because of problems on the remote server.\nWhat if I\u2019m offline?\nWhat if the server is having a temporary outage?\nWhat if the server starts rate limiting me for making too many HTTP requests?</p>\n\n<h3 id=\"it-makes-my-tests-more-brittle\">It makes my tests more brittle</h3>\n\n<p>If my tests depend on the server having certain state, then the server state could change and break or degrade my test suite.</p>\n\n<p>Sometimes this change is obvious.\nFor example, suppose I\u2019m testing a function to fetch photos from Flickr, and then the photo I\u2019m using in my test gets deleted.\nMy code works correctly for photos that still exist, but now my test starts failing.</p>\n\n<p>Sometimes this change is more subtle.\nSuppose I\u2019ve written a regression test for an edge case, and then the server state changes, so the example I\u2019m checking is no longer an instance of the edge case.\nI could break the code and never realise, because the test would keep passing.\nMy test suite would become less effective.</p>\n\n<h3 id=\"it-means-passing-around-more-secrets\">It means passing around more secrets</h3>\n\n<p>A lot of my HTTP calls require secrets, like API keys or OAuth tokens.\nIf the tests made real HTTP calls, I\u2019d need to copy those secrets to every environment where I\u2019m running the tests.\nThat increases the risk of the secret getting leaked.</p>\n\n<h3 id=\"it-makes-my-tests-harder-to-debug\">It makes my tests harder to debug</h3>\n\n<p>If there are more reasons why a test could fail, then it takes longer to work out if the failure was caused by my mistake, or a change on the server.</p>\n\n<h3 id=\"recording-and-replaying-http-requests-solves-these-problems\">Recording and replaying HTTP requests solves these problems</h3>\n\n<p>If my test suite is returning consistent responses for HTTP calls, and those responses are defined within the test suite itself, then my tests get faster and more reliable.\nI\u2019m not making real network calls, I\u2019m not dependent on the behaviour of a server, and I don\u2019t need real secrets to run the tests.</p>\n\n<p>There are a variety of ways to define this sort of test mock; I like to record real responses because it ensures I\u2019m getting a high-fidelity mock, and it makes it fairly easy to add new tests.</p>\n\n<hr />\n\n<h2 id=\"why-do-you-like-vcrpy\">Why do you like vcrpy?</h2>\n\n<p>I know two Python libraries that record real HTTP responses: <a href=\"https://vcrpy.readthedocs.io/\">vcrpy</a> and <a href=\"https://github.com/betamaxpy/betamax\">betamax</a>, both based on <a href=\"https://github.com/vcr/vcr\">a\u00a0Ruby library called vcr</a>.\nI\u2019ve used all three, they behave in a similar way, and they work well.</p>\n\n<p>I prefer vcrpy for Python because it supports a <a href=\"https://vcrpy.readthedocs.io/en/latest/installation.html#compatibility\">wide variety of HTTP libraries</a>, whereas betamax only works with <a href=\"https://requests.readthedocs.io/en/latest/\">requests</a>.\nI currently use a mixture of <a href=\"https://github.com/encode/httpx\">httpx</a> and <a href=\"https://urllib3.readthedocs.io/en/stable/\">urllib3</a>, and it\u2019s convenient to test them both with the same library and test helpers.</p>\n\n<p>I also like that vcrpy works without needing any changes to the code I\u2019m testing.\nI can write HTTP code as I normally would, then I add a vcrpy decorator in my test and the responses get recorded.\nI don\u2019t like test frameworks that require me to rewrite my code to fit \u2013 the tests should follow the code, not the other way round.</p>\n\n<hr />\n\n<h2 id=\"a-basic-example-of-using-vcrpy\">A basic example of using vcrpy</h2>\n\n<p>Here\u2019s a test that uses vcrpy to fetch <code>www.example.com</code>, and look for some text in the response.\nI use <code>vcr.use_cassette</code> as a context manager around the code that makes an HTTP request:</p>\n<pre><code><span class=\"kn\">import</span> <span class=\"n\">httpx</span>\n<span class=\"kn\">import</span> <span class=\"n\">vcr</span>\n\n\n<span class=\"k\">def</span> <span class=\"nf\">test_example_domain</span><span class=\"p\">():</span>\n    <span class=\"k\">with</span> <span class=\"n\">vcr</span><span class=\"p\">.</span><span class=\"nf\">use_cassette</span><span class=\"p\">(</span><span class=\"sh\">\"</span><span class=\"s\">fixtures/vcr_cassettes/test_example_domain.yml</span><span class=\"sh\">\"</span><span class=\"p\">):</span>\n        <span class=\"n\">resp</span> <span class=\"o\">=</span> <span class=\"n\">httpx</span><span class=\"p\">.</span><span class=\"nf\">get</span><span class=\"p\">(</span><span class=\"sh\">\"</span><span class=\"s\">https://www.example.com/</span><span class=\"sh\">\"</span><span class=\"p\">)</span>\n        <span class=\"k\">assert</span> <span class=\"sh\">\"</span><span class=\"s\">&lt;h1&gt;Example Domain&lt;/h1&gt;</span><span class=\"sh\">\"</span> <span class=\"ow\">in</span> <span class=\"n\">resp</span><span class=\"p\">.</span><span class=\"n\">text</span>\n</code></pre>\n<p>Alternatively, you can use <code>vcr.use_cassette</code> as a decorator:</p>\n<pre><code><span class=\"nd\">@vcr.use_cassette</span><span class=\"p\">(</span><span class=\"sh\">\"</span><span class=\"s\">fixtures/vcr_cassettes/test_example_domain.yml</span><span class=\"sh\">\"</span><span class=\"p\">)</span>\n<span class=\"k\">def</span> <span class=\"nf\">test_example_domain</span><span class=\"p\">():</span>\n    <span class=\"n\">resp</span> <span class=\"o\">=</span> <span class=\"n\">httpx</span><span class=\"p\">.</span><span class=\"nf\">get</span><span class=\"p\">(</span><span class=\"sh\">\"</span><span class=\"s\">https://www.example.com/</span><span class=\"sh\">\"</span><span class=\"p\">)</span>\n    <span class=\"k\">assert</span> <span class=\"sh\">\"</span><span class=\"s\">&lt;h1&gt;Example Domain&lt;/h1&gt;</span><span class=\"sh\">\"</span> <span class=\"ow\">in</span> <span class=\"n\">resp</span><span class=\"p\">.</span><span class=\"n\">text</span>\n</code></pre>\n<p>With the decorator, you can also omit the path to the cassette file, and vcrpy will name the cassette file after the function:</p>\n<pre><code><span class=\"nd\">@vcr.use_cassette</span><span class=\"p\">()</span>\n<span class=\"k\">def</span> <span class=\"nf\">test_example_domain</span><span class=\"p\">():</span>\n    <span class=\"n\">resp</span> <span class=\"o\">=</span> <span class=\"n\">httpx</span><span class=\"p\">.</span><span class=\"nf\">get</span><span class=\"p\">(</span><span class=\"sh\">\"</span><span class=\"s\">https://www.example.com/</span><span class=\"sh\">\"</span><span class=\"p\">)</span>\n    <span class=\"k\">assert</span> <span class=\"sh\">\"</span><span class=\"s\">&lt;h1&gt;Example Domain&lt;/h1&gt;</span><span class=\"sh\">\"</span> <span class=\"ow\">in</span> <span class=\"n\">resp</span><span class=\"p\">.</span><span class=\"n\">text</span>\n</code></pre>\n<p>When I run this test using pytest (<code>python3 -m pytest test_example.py</code>), vcrpy will check if the cassette file exists.\nIf the file is missing, it makes a real HTTP call and saves it to the file.\nIf the file exists, it replays the previously-recorded HTTP call.</p>\n\n<p>By default, the cassette is a YAML file.\nHere\u2019s what it looks like: <a href=\"https://alexwlchan.net/files/2025/test_example_domain.yml\">test_example_domain.yml</a>.</p>\n\n<p>If a test makes more than one HTTP request, vcrpy records all of them in the same cassette file.</p>\n\n<hr />\n\n<h2 id=\"using-vcrpy-in-production\">Using vcrpy in production</h2>\n\n<h3 id=\"keeping-secrets-out-of-my-cassettes\">Keeping secrets out of my cassettes</h3>\n\n<p>The cassette files contain the complete HTTP request and response, which includes the URL, form data, and HTTP headers.\nIf I\u2019m testing an API that requires auth, the HTTP request could include secrets like an API key or OAuth token.\nI don\u2019t want to save those secrets in the cassette file!</p>\n\n<p>Fortunately, vcrpy can <a href=\"https://vcrpy.readthedocs.io/en/latest/advanced.html#filter-sensitive-data-from-the-request\">filter sensitive data</a> before it\u2019s saved to the cassette file \u2013 HTTP headers, URL query parameters, or form data.</p>\n\n<p>Here\u2019s an example where I\u2019m using <code>filter_query_parameters</code> to redact an API key.\nI\u2019m replacing the real value with the placeholder <code>REDACTED_API_KEY</code>.</p>\n<pre><code><span class=\"kn\">import</span> <span class=\"n\">os</span>\n\n<span class=\"kn\">import</span> <span class=\"n\">httpx</span>\n<span class=\"kn\">import</span> <span class=\"n\">vcr</span>\n\n\n<span class=\"k\">def</span> <span class=\"nf\">test_flickr_api</span><span class=\"p\">():</span>\n    <span class=\"k\">with</span> <span class=\"n\">vcr</span><span class=\"p\">.</span><span class=\"nf\">use_cassette</span><span class=\"p\">(</span>\n        <span class=\"sh\">\"</span><span class=\"s\">fixtures/vcr_cassettes/test_flickr_api.yml</span><span class=\"sh\">\"</span><span class=\"p\">,</span>\n        <span class=\"n\">filter_query_parameters</span><span class=\"o\">=</span><span class=\"p\">[(</span><span class=\"sh\">\"</span><span class=\"s\">api_key</span><span class=\"sh\">\"</span><span class=\"p\">,</span> <span class=\"sh\">\"</span><span class=\"s\">REDACTED_API_KEY</span><span class=\"sh\">\"</span><span class=\"p\">)],</span>\n    <span class=\"p\">):</span>\n        <span class=\"n\">api_key</span> <span class=\"o\">=</span> <span class=\"n\">os</span><span class=\"p\">.</span><span class=\"n\">environ</span><span class=\"p\">.</span><span class=\"nf\">get</span><span class=\"p\">(</span><span class=\"sh\">\"</span><span class=\"s\">FLICKR_API_KEY</span><span class=\"sh\">\"</span><span class=\"p\">,</span> <span class=\"sh\">\"</span><span class=\"s\">API_KEY</span><span class=\"sh\">\"</span><span class=\"p\">)</span>\n\n        <span class=\"n\">resp</span> <span class=\"o\">=</span> <span class=\"n\">httpx</span><span class=\"p\">.</span><span class=\"nf\">get</span><span class=\"p\">(</span>\n            <span class=\"sh\">\"</span><span class=\"s\">https://api.flickr.com/services/rest/</span><span class=\"sh\">\"</span><span class=\"p\">,</span>\n            <span class=\"n\">params</span><span class=\"o\">=</span><span class=\"p\">{</span>\n                <span class=\"sh\">\"</span><span class=\"s\">api_key</span><span class=\"sh\">\"</span><span class=\"p\">:</span> <span class=\"n\">api_key</span><span class=\"p\">,</span>\n                <span class=\"sh\">\"</span><span class=\"s\">method</span><span class=\"sh\">\"</span><span class=\"p\">:</span> <span class=\"sh\">\"</span><span class=\"s\">flickr.urls.lookupUser</span><span class=\"sh\">\"</span><span class=\"p\">,</span>\n                <span class=\"sh\">\"</span><span class=\"s\">url</span><span class=\"sh\">\"</span><span class=\"p\">:</span> <span class=\"sh\">\"</span><span class=\"s\">https://www.flickr.com/photos/alexwlchan/</span><span class=\"sh\">\"</span><span class=\"p\">,</span>\n            <span class=\"p\">},</span>\n        <span class=\"p\">)</span>\n\n        <span class=\"k\">assert</span> <span class=\"sh\">'</span><span class=\"s\">&lt;user id=</span><span class=\"sh\">\"</span><span class=\"s\">199258389@N04</span><span class=\"sh\">\"</span><span class=\"s\">&gt;</span><span class=\"sh\">'</span> <span class=\"ow\">in</span> <span class=\"n\">resp</span><span class=\"p\">.</span><span class=\"n\">text</span>\n</code></pre>\n<p>When I run this test the first time, I need to pass an env var <code>FLICKR_API_KEY</code>.\nThis makes a real request and records a cassette, but with my redacted value.\nWhen I run the test again, I don\u2019t need to pass the env var, but the test will still pass.</p>\n\n<p>You can see the complete YAML file in <a href=\"https://alexwlchan.net/files/2025/test_flickr_api.yml\">test_flickr_api.yml</a>.\nNotice how the <code>api_key</code> query parameter has been redacted in the recorded request:</p>\n<pre><code><span class=\"na\">interactions</span><span class=\"pi\">:</span>\n<span class=\"pi\">-</span> <span class=\"na\">request</span><span class=\"pi\">:</span>\n    <span class=\"s\">\u2026</span>\n    <span class=\"s\">uri</span><span class=\"err\">:</span> <span class=\"s\">https://api.flickr.com/services/rest/?api_key=REDACTED_API_KEY&amp;method=flickr.urls.lookupUser&amp;url=https%3A%2F%2Fwww.flickr.com%2Fphotos%2Falexwlchan%2F</span>\n    <span class=\"s\">\u2026</span>\n</code></pre>\n<p>You can also tell vcrpy to omit the sensitive field entirely, but I like to insert a placeholder value.\nIt\u2019s useful for debugging later \u2013 you can see that a value was replaced, and easily search for the code that\u2019s doing the redaction.</p>\n\n<h3 id=\"improving-the-human-readability-of-cassettes\">Improving the human readability of cassettes</h3>\n\n<p>If you look at the first two cassette files, you\u2019ll notice that the response body is stored as base64-encoded binary data:</p>\n<pre><code><span class=\"na\">response</span><span class=\"pi\">:</span>\n  <span class=\"na\">body</span><span class=\"pi\">:</span>\n    <span class=\"na\">string</span><span class=\"pi\">:</span> <span class=\"kt\">!!binary</span> <span class=\"pi\">|</span>\n      <span class=\"s\">H4sIAAAAAAAAAH1UTXPbIBC9+1ds1UsyIyQnaRqPLWn6mWkPaQ9pDz0SsbKYCFAByfZ08t+7Qo4j</span>\n      <span class=\"s\">N5makYFdeLvvsZC9Eqb0uxah9qopZtljh1wUM6Bf5qVvsPi85aptED4ZxaXO0tE6G5co9BzKmluH</span>\n      <span class=\"s\">Po86X7FFBGkxcdbetwx/d7LPo49Ge9SeDWEjKMdZHnnc+nQIvzpAvYSkucI86iVuWmP9ZP9GCl/n</span>\n</code></pre>\n<p>That\u2019s because <code>example.com</code> and <code>api.flickr.com</code> both <a href=\"https://developer.mozilla.org/en-US/docs/Glossary/gzip_compression\">gzip compress</a> their responses, and vcrpy is preserving that compression.\nBut gzip compression is handled by the HTTP libraries \u2013 my code never needs to worry about compression; it just gets the uncompressed response.</p>\n\n<p>Where possible, I prefer to store responses in their uncompressed form.\nIt makes the cassettes easier to read, and you can see if secrets are included in the saved response data.\nI also find it useful to read cassettes as an example of what an API response looks like \u2013 and in particular, what it looked like when I wrote the test.\nCassettes have helped me spot undocumented changes in APIs.</p>\n\n<p>Here\u2019s an example where I\u2019m using <a href=\"https://vcrpy.readthedocs.io/en/latest/advanced.html#decode-compressed-response\"><code>decode_compressed_response=True</code></a> to remove the gzip compression in the cassette:</p>\n<pre><code><span class=\"k\">def</span> <span class=\"nf\">test_example_domain_with_decode</span><span class=\"p\">():</span>\n    <span class=\"k\">with</span> <span class=\"n\">vcr</span><span class=\"p\">.</span><span class=\"nf\">use_cassette</span><span class=\"p\">(</span>\n        <span class=\"sh\">\"</span><span class=\"s\">fixtures/vcr_cassettes/test_example_domain_with_decode.yml</span><span class=\"sh\">\"</span><span class=\"p\">,</span>\n        <span class=\"n\">decode_compressed_response</span><span class=\"o\">=</span><span class=\"bp\">True</span><span class=\"p\">,</span>\n    <span class=\"p\">):</span>\n        <span class=\"n\">resp</span> <span class=\"o\">=</span> <span class=\"n\">httpx</span><span class=\"p\">.</span><span class=\"nf\">get</span><span class=\"p\">(</span><span class=\"sh\">\"</span><span class=\"s\">https://www.example.com/</span><span class=\"sh\">\"</span><span class=\"p\">)</span>\n        <span class=\"k\">assert</span> <span class=\"sh\">\"</span><span class=\"s\">&lt;h1&gt;Example Domain&lt;/h1&gt;</span><span class=\"sh\">\"</span> <span class=\"ow\">in</span> <span class=\"n\">resp</span><span class=\"p\">.</span><span class=\"n\">text</span>\n</code></pre>\n<p>You can see the complete cassette file in <a href=\"https://alexwlchan.net/files/2025/test_example_domain_with_decode.yml\">test_example_domain_with_decode.yml</a>.\nNotice the response body now contains an HTML string:</p>\n<pre><code><span class=\"na\">response</span><span class=\"pi\">:</span>\n  <span class=\"na\">body</span><span class=\"pi\">:</span>\n    <span class=\"na\">string</span><span class=\"pi\">:</span> <span class=\"s2\">\"</span><span class=\"s\">&lt;!doctype</span><span class=\"nv\"> </span><span class=\"s\">html&gt;</span><span class=\"se\">\\n</span><span class=\"s\">&lt;html&gt;</span><span class=\"se\">\\n</span><span class=\"s\">&lt;head&gt;</span><span class=\"se\">\\n</span><span class=\"nv\">    </span><span class=\"s\">&lt;title&gt;Example</span><span class=\"nv\"> </span><span class=\"s\">Domain&lt;/title&gt;</span><span class=\"se\">\\n\\n</span>\n      <span class=\"se\">\\ </span><span class=\"nv\">  </span><span class=\"s\">&lt;meta</span><span class=\"nv\"> </span><span class=\"s\">charset=</span><span class=\"se\">\\\"</span><span class=\"s\">utf-8</span><span class=\"se\">\\\"</span><span class=\"nv\"> </span><span class=\"s\">/&gt;</span><span class=\"se\">\\n</span><span class=\"nv\">    </span><span class=\"s\">&lt;meta</span><span class=\"nv\"> </span><span class=\"s\">http-equiv=</span><span class=\"se\">\\\"</span><span class=\"s\">Content-type</span><span class=\"se\">\\\"</span><span class=\"nv\"> </span><span class=\"s\">content=</span><span class=\"se\">\\\"</span><span class=\"s\">text/html;</span>\n      <span class=\"s\">charset=utf-8</span><span class=\"se\">\\\"</span><span class=\"nv\"> </span><span class=\"s\">/&gt;</span><span class=\"se\">\\n</span><span class=\"nv\">    </span><span class=\"s\">&lt;meta</span><span class=\"nv\"> </span><span class=\"s\">name=</span><span class=\"se\">\\\"</span><span class=\"s\">viewport</span><span class=\"se\">\\\"</span><span class=\"nv\"> </span><span class=\"s\">content=</span><span class=\"se\">\\\"</span><span class=\"s\">width=device-width,</span>\n</code></pre>\n<h3 id=\"naming-my-cassettes-to-make-sense-later\">Naming my cassettes to make sense later</h3>\n\n<p>If you write a lot of tests that use vcrpy, you\u2019ll end up with a fixtures directory that\u2019s full of cassettes.\nI like cassette names to match my test functions, so they\u2019re easy to match up later.</p>\n\n<p>I could specify a cassette name explicitly in every test, but that\u2019s extra work and prone to error.\nAlternatively, I could use the decorator and use the automatic cassette name \u2013 but vcrpy uses the name of the test function, which may not distinguish between tests.\nIn particular, I often group tests into classes, or use <a href=\"https://nedbatchelder.com/blog/202508/starting_with_pytests_parametrize.html\">parametrized tests</a> to run the same test with different values.</p>\n\n<p>Consider the following example:</p>\n<pre><code><span class=\"kn\">import</span> <span class=\"n\">httpx</span>\n<span class=\"kn\">import</span> <span class=\"n\">pytest</span>\n<span class=\"kn\">import</span> <span class=\"n\">vcr</span>\n\n\n<span class=\"k\">class</span> <span class=\"nc\">TestExampleDotCom</span><span class=\"p\">:</span>\n    <span class=\"k\">def</span> <span class=\"nf\">test_status_code</span><span class=\"p\">(</span><span class=\"n\">self</span><span class=\"p\">):</span>\n        <span class=\"n\">resp</span> <span class=\"o\">=</span> <span class=\"n\">httpx</span><span class=\"p\">.</span><span class=\"nf\">get</span><span class=\"p\">(</span><span class=\"sh\">\"</span><span class=\"s\">https://example.com</span><span class=\"sh\">\"</span><span class=\"p\">)</span>\n        <span class=\"k\">assert</span> <span class=\"n\">resp</span><span class=\"p\">.</span><span class=\"n\">status_code</span> <span class=\"o\">==</span> <span class=\"mi\">200</span>\n\n\n<span class=\"nd\">@vcr.use_cassette</span><span class=\"p\">()</span>\n<span class=\"nd\">@pytest.mark.parametrize</span><span class=\"p\">(</span>\n    <span class=\"sh\">\"</span><span class=\"s\">url, status_code</span><span class=\"sh\">\"</span><span class=\"p\">,</span>\n    <span class=\"p\">[</span>\n        <span class=\"p\">(</span><span class=\"sh\">\"</span><span class=\"s\">https://httpbin.org/status/200</span><span class=\"sh\">\"</span><span class=\"p\">,</span> <span class=\"mi\">200</span><span class=\"p\">),</span>\n        <span class=\"p\">(</span><span class=\"sh\">\"</span><span class=\"s\">https://httpbin.org/status/404</span><span class=\"sh\">\"</span><span class=\"p\">,</span> <span class=\"mi\">404</span><span class=\"p\">),</span>\n        <span class=\"p\">(</span><span class=\"sh\">\"</span><span class=\"s\">https://httpbin.org/status/500</span><span class=\"sh\">\"</span><span class=\"p\">,</span> <span class=\"mi\">500</span><span class=\"p\">),</span>\n    <span class=\"p\">],</span>\n<span class=\"p\">)</span>\n<span class=\"k\">def</span> <span class=\"nf\">test_status_code</span><span class=\"p\">(</span><span class=\"n\">url</span><span class=\"p\">,</span> <span class=\"n\">status_code</span><span class=\"p\">):</span>\n    <span class=\"n\">resp</span> <span class=\"o\">=</span> <span class=\"n\">httpx</span><span class=\"p\">.</span><span class=\"nf\">get</span><span class=\"p\">(</span><span class=\"n\">url</span><span class=\"p\">)</span>\n    <span class=\"k\">assert</span> <span class=\"n\">resp</span><span class=\"p\">.</span><span class=\"n\">status_code</span> <span class=\"o\">==</span> <span class=\"n\">status_code</span>\n</code></pre>\n<p>This is four different tests, but vcrpy\u2019s automatic cassette name is the same for each of them: <code>test_status_code</code>.\nThe tests will fail if you try to run them \u2013 vcrpy will record a cassette for the first test that runs, then try to replay that cassette for the second test.\nThe second test makes a different HTTP request, so vcrpy will throw an error because it can\u2019t find a matching request.</p>\n\n<p>Here\u2019s what I do instead: I have a pytest fixture to choose cassette names, which includes the name of the test class (if any) and the ID of the parametrized test case.\nBecause I sometimes use URLs in parametrized tests, I also check the test case ID doesn\u2019t include slashes or colons \u2013 I don\u2019t want those in my filenames!</p>\n\n<p>Here\u2019s the decorator:</p>\n<pre><code><span class=\"nd\">@pytest.fixture</span>\n<span class=\"k\">def</span> <span class=\"nf\">cassette_name</span><span class=\"p\">(</span><span class=\"n\">request</span><span class=\"p\">:</span> <span class=\"n\">pytest</span><span class=\"p\">.</span><span class=\"n\">FixtureRequest</span><span class=\"p\">)</span> <span class=\"o\">-&gt;</span> <span class=\"nb\">str</span><span class=\"p\">:</span>\n    <span class=\"sh\">\"\"\"</span><span class=\"s\">\n    Returns the filename of a VCR cassette to use in tests.\n\n    The name can be made up of (up to) three parts:\n\n    -   the name of the test class\n    -   the name of the test function\n    -   the ID of the test case in @pytest.mark.parametrize\n\n    </span><span class=\"sh\">\"\"\"</span>\n    <span class=\"n\">name</span> <span class=\"o\">=</span> <span class=\"n\">request</span><span class=\"p\">.</span><span class=\"n\">node</span><span class=\"p\">.</span><span class=\"n\">name</span>\n\n    <span class=\"c1\"># This is to catch cases where e.g. I try to include a complete\n</span>    <span class=\"c1\"># HTTP URL in a cassette name, which creates messy folders in\n</span>    <span class=\"c1\"># the fixtures directory.\n</span>    <span class=\"k\">if</span> <span class=\"sh\">\"</span><span class=\"s\">:</span><span class=\"sh\">\"</span> <span class=\"ow\">in</span> <span class=\"n\">name</span> <span class=\"ow\">or</span> <span class=\"sh\">\"</span><span class=\"s\">/</span><span class=\"sh\">\"</span> <span class=\"ow\">in</span> <span class=\"n\">name</span><span class=\"p\">:</span>\n        <span class=\"k\">raise</span> <span class=\"nc\">ValueError</span><span class=\"p\">(</span>\n            <span class=\"sh\">\"</span><span class=\"s\">Illegal characters in VCR cassette name - </span><span class=\"sh\">\"</span>\n            <span class=\"sh\">\"</span><span class=\"s\">please set a test ID with pytest.param(\u2026, id=</span><span class=\"sh\">'</span><span class=\"s\">\u2026</span><span class=\"sh\">'</span><span class=\"s\">)</span><span class=\"sh\">\"</span>\n        <span class=\"p\">)</span>\n\n    <span class=\"k\">if</span> <span class=\"n\">request</span><span class=\"p\">.</span><span class=\"n\">cls</span> <span class=\"ow\">is</span> <span class=\"ow\">not</span> <span class=\"bp\">None</span><span class=\"p\">:</span>\n        <span class=\"k\">return</span> <span class=\"sa\">f</span><span class=\"sh\">\"</span><span class=\"si\">{</span><span class=\"n\">request</span><span class=\"p\">.</span><span class=\"n\">cls</span><span class=\"p\">.</span><span class=\"n\">__name__</span><span class=\"si\">}</span><span class=\"s\">.</span><span class=\"si\">{</span><span class=\"n\">name</span><span class=\"si\">}</span><span class=\"s\">.yml</span><span class=\"sh\">\"</span>\n    <span class=\"k\">else</span><span class=\"p\">:</span>\n        <span class=\"k\">return</span> <span class=\"sa\">f</span><span class=\"sh\">\"</span><span class=\"si\">{</span><span class=\"n\">name</span><span class=\"si\">}</span><span class=\"s\">.yml</span><span class=\"sh\">\"</span>\n</code></pre>\n<p>Here\u2019s my test rewritten to use that new decorator:</p>\n<pre><code><span class=\"k\">class</span> <span class=\"nc\">TestExampleDotCom</span><span class=\"p\">:</span>\n    <span class=\"k\">def</span> <span class=\"nf\">test_status_code</span><span class=\"p\">(</span><span class=\"n\">self</span><span class=\"p\">,</span> <span class=\"n\">cassette_name</span><span class=\"p\">):</span>\n        <span class=\"k\">with</span> <span class=\"n\">vcr</span><span class=\"p\">.</span><span class=\"nf\">use_cassette</span><span class=\"p\">(</span><span class=\"n\">cassette_name</span><span class=\"p\">):</span>\n            <span class=\"n\">resp</span> <span class=\"o\">=</span> <span class=\"n\">httpx</span><span class=\"p\">.</span><span class=\"nf\">get</span><span class=\"p\">(</span><span class=\"sh\">\"</span><span class=\"s\">https://example.com</span><span class=\"sh\">\"</span><span class=\"p\">)</span>\n            <span class=\"k\">assert</span> <span class=\"n\">resp</span><span class=\"p\">.</span><span class=\"n\">status_code</span> <span class=\"o\">==</span> <span class=\"mi\">200</span>\n\n\n<span class=\"nd\">@vcr.use_cassette</span><span class=\"p\">()</span>\n<span class=\"nd\">@pytest.mark.parametrize</span><span class=\"p\">(</span>\n    <span class=\"sh\">\"</span><span class=\"s\">url, status_code</span><span class=\"sh\">\"</span><span class=\"p\">,</span>\n    <span class=\"p\">[</span>\n        <span class=\"n\">pytest</span><span class=\"p\">.</span><span class=\"nf\">param</span><span class=\"p\">(</span><span class=\"sh\">\"</span><span class=\"s\">https://httpbin.org/status/200</span><span class=\"sh\">\"</span><span class=\"p\">,</span> <span class=\"mi\">200</span><span class=\"p\">,</span> <span class=\"nb\">id</span><span class=\"o\">=</span><span class=\"sh\">\"</span><span class=\"s\">ok</span><span class=\"sh\">\"</span><span class=\"p\">),</span>\n        <span class=\"n\">pytest</span><span class=\"p\">.</span><span class=\"nf\">param</span><span class=\"p\">(</span><span class=\"sh\">\"</span><span class=\"s\">https://httpbin.org/status/404</span><span class=\"sh\">\"</span><span class=\"p\">,</span> <span class=\"mi\">404</span><span class=\"p\">,</span> <span class=\"nb\">id</span><span class=\"o\">=</span><span class=\"sh\">\"</span><span class=\"s\">not_found</span><span class=\"sh\">\"</span><span class=\"p\">),</span>\n        <span class=\"n\">pytest</span><span class=\"p\">.</span><span class=\"nf\">param</span><span class=\"p\">(</span><span class=\"sh\">\"</span><span class=\"s\">https://httpbin.org/status/500</span><span class=\"sh\">\"</span><span class=\"p\">,</span> <span class=\"mi\">500</span><span class=\"p\">,</span> <span class=\"nb\">id</span><span class=\"o\">=</span><span class=\"sh\">\"</span><span class=\"s\">server_error</span><span class=\"sh\">\"</span><span class=\"p\">),</span>\n    <span class=\"p\">],</span>\n<span class=\"p\">)</span>\n<span class=\"k\">def</span> <span class=\"nf\">test_status_code</span><span class=\"p\">(</span><span class=\"n\">url</span><span class=\"p\">,</span> <span class=\"n\">status_code</span><span class=\"p\">,</span> <span class=\"n\">cassette_name</span><span class=\"p\">):</span>\n    <span class=\"k\">with</span> <span class=\"n\">vcr</span><span class=\"p\">.</span><span class=\"nf\">use_cassette</span><span class=\"p\">(</span><span class=\"n\">cassette_name</span><span class=\"p\">):</span>\n        <span class=\"n\">resp</span> <span class=\"o\">=</span> <span class=\"n\">httpx</span><span class=\"p\">.</span><span class=\"nf\">get</span><span class=\"p\">(</span><span class=\"n\">url</span><span class=\"p\">)</span>\n        <span class=\"k\">assert</span> <span class=\"n\">resp</span><span class=\"p\">.</span><span class=\"n\">status_code</span> <span class=\"o\">==</span> <span class=\"n\">status_code</span>\n</code></pre>\n<p>The four tests now get distinct cassette filenames:</p>\n\n<ul>\n  <li><code>TestExampleDotCom.test_status_code</code></li>\n  <li><code>test_status_code[ok]</code></li>\n  <li><code>test_status_code[not_found]</code></li>\n  <li><code>test_status_code[server_error]</code></li>\n</ul>\n\n<h3 id=\"explaining-how-to-use-cassettes-with-helpful-errors\">Explaining how to use cassettes with helpful errors</h3>\n\n<p>Most of the time, you don\u2019t need to worry about how vcrpy works.\nIf you\u2019re running an existing test, then vcrpy is just a fancy test mock that happens to be reading its data from a YAML file.\nYou don\u2019t need to worry about the implementation details.</p>\n\n<p>However, if you\u2019re writing a new test, you need to record new cassettes.\nThis can involve some non-obvious setup, especially if you\u2019ve never done it before.</p>\n\n<p>Let\u2019s revisit an earlier example:</p>\n<pre><code><span class=\"k\">def</span> <span class=\"nf\">test_flickr_api</span><span class=\"p\">():</span>\n    <span class=\"k\">with</span> <span class=\"n\">vcr</span><span class=\"p\">.</span><span class=\"nf\">use_cassette</span><span class=\"p\">(</span>\n        <span class=\"sh\">\"</span><span class=\"s\">fixtures/vcr_cassettes/test_flickr_api.yml</span><span class=\"sh\">\"</span><span class=\"p\">,</span>\n        <span class=\"n\">filter_query_parameters</span><span class=\"o\">=</span><span class=\"p\">[(</span><span class=\"sh\">\"</span><span class=\"s\">api_key</span><span class=\"sh\">\"</span><span class=\"p\">,</span> <span class=\"sh\">\"</span><span class=\"s\">REDACTED_API_KEY</span><span class=\"sh\">\"</span><span class=\"p\">)],</span>\n    <span class=\"p\">):</span>\n        <span class=\"n\">api_key</span> <span class=\"o\">=</span> <span class=\"n\">os</span><span class=\"p\">.</span><span class=\"n\">environ</span><span class=\"p\">.</span><span class=\"nf\">get</span><span class=\"p\">(</span><span class=\"sh\">\"</span><span class=\"s\">FLICKR_API_KEY</span><span class=\"sh\">\"</span><span class=\"p\">,</span> <span class=\"sh\">\"</span><span class=\"s\">API_KEY</span><span class=\"sh\">\"</span><span class=\"p\">)</span>\n\n        <span class=\"n\">resp</span> <span class=\"o\">=</span> <span class=\"n\">httpx</span><span class=\"p\">.</span><span class=\"nf\">get</span><span class=\"p\">(</span>\n            <span class=\"sh\">\"</span><span class=\"s\">https://api.flickr.com/services/rest/</span><span class=\"sh\">\"</span><span class=\"p\">,</span>\n            <span class=\"n\">params</span><span class=\"o\">=</span><span class=\"p\">{</span>\n                <span class=\"sh\">\"</span><span class=\"s\">api_key</span><span class=\"sh\">\"</span><span class=\"p\">:</span> <span class=\"n\">api_key</span><span class=\"p\">,</span>\n                <span class=\"sh\">\"</span><span class=\"s\">method</span><span class=\"sh\">\"</span><span class=\"p\">:</span> <span class=\"sh\">\"</span><span class=\"s\">flickr.urls.lookupUser</span><span class=\"sh\">\"</span><span class=\"p\">,</span>\n                <span class=\"sh\">\"</span><span class=\"s\">url</span><span class=\"sh\">\"</span><span class=\"p\">:</span> <span class=\"sh\">\"</span><span class=\"s\">https://www.flickr.com/photos/alexwlchan/</span><span class=\"sh\">\"</span><span class=\"p\">,</span>\n            <span class=\"p\">},</span>\n        <span class=\"p\">)</span>\n\n        <span class=\"k\">assert</span> <span class=\"sh\">'</span><span class=\"s\">&lt;user id=</span><span class=\"sh\">\"</span><span class=\"s\">199258389@N04</span><span class=\"sh\">\"</span><span class=\"s\">&gt;</span><span class=\"sh\">'</span> <span class=\"ow\">in</span> <span class=\"n\">resp</span><span class=\"p\">.</span><span class=\"n\">text</span>\n</code></pre>\n<p>If you run this test without passing a <code>FLICKR_API_KEY</code> environment variable, it will call the real Flickr API with the placeholder API key.\nUnsurprisingly, the Flickr API will return an error response, and your test will fail:</p>\n<pre><code><span class=\"cp\">&lt;?xml version=\"1.0\" encoding=\"utf-8\" ?&gt;</span>\n<span class=\"nt\">&lt;rsp</span> <span class=\"na\">stat=</span><span class=\"s\">\"fail\"</span><span class=\"nt\">&gt;</span>\n  <span class=\"nt\">&lt;err</span> <span class=\"na\">code=</span><span class=\"s\">\"100\"</span> <span class=\"na\">msg=</span><span class=\"s\">\"Invalid API Key (Key has invalid format)\"</span> <span class=\"nt\">/&gt;</span>\n<span class=\"nt\">&lt;/rsp&gt;</span>\n</code></pre>\n<p>Worse still, vcrpy will record this error in the cassette file.\nEven if you work out you need to re-run the test with the env var, it will keep failing as it replays the recorded error.</p>\n\n<p>Can we make this better?\nIn this scenario, what I\u2019d prefer is:</p>\n\n<ol>\n  <li>The test fails if you don\u2019t pass an env var</li>\n  <li>The error explains how to run the test properly</li>\n  <li>vcrpy doesn\u2019t save a cassette file</li>\n</ol>\n\n<p>I worked out how to get this nicer error handling.\nvcrpy has a <a href=\"https://vcrpy.readthedocs.io/en/latest/advanced.html#custom-response-filtering\"><code>before_record_response</code> hook</a>, that allows you to modify a response before writing it to the cassette file.\nYou could use this to redact secrets from responses, but I realised you could also use it to validate the response \u2013 and if you throw an exception, it prevents vcrpy from writing a cassette.</p>\n\n<p>Here\u2019s a hook I wrote, which checks if a vcrpy response is a Flickr API error telling us that we passed an invalid API key, and throws an exception if so:</p>\n<pre><code><span class=\"k\">def</span> <span class=\"nf\">check_for_invalid_api_key</span><span class=\"p\">(</span><span class=\"n\">response</span><span class=\"p\">):</span>\n    <span class=\"sh\">\"\"\"</span><span class=\"s\">\n    Before we record a new response to a cassette, check if it</span><span class=\"sh\">'</span><span class=\"s\">s\n    a Flickr API response telling us we</span><span class=\"sh\">'</span><span class=\"s\">re missing an API key -- that\n    means we didn</span><span class=\"sh\">'</span><span class=\"s\">t set up the test correctly.\n\n    If so, give the developer an instruction explaining what to do next.\n    </span><span class=\"sh\">\"\"\"</span>\n    <span class=\"k\">try</span><span class=\"p\">:</span>\n        <span class=\"n\">body</span><span class=\"p\">:</span> <span class=\"nb\">bytes</span> <span class=\"o\">=</span> <span class=\"n\">response</span><span class=\"p\">[</span><span class=\"sh\">\"</span><span class=\"s\">body</span><span class=\"sh\">\"</span><span class=\"p\">][</span><span class=\"sh\">\"</span><span class=\"s\">string</span><span class=\"sh\">\"</span><span class=\"p\">]</span>\n    <span class=\"k\">except</span> <span class=\"nb\">KeyError</span><span class=\"p\">:</span>\n        <span class=\"n\">body</span> <span class=\"o\">=</span> <span class=\"n\">response</span><span class=\"p\">[</span><span class=\"sh\">\"</span><span class=\"s\">content</span><span class=\"sh\">\"</span><span class=\"p\">]</span>\n\n    <span class=\"n\">is_error_response</span> <span class=\"o\">=</span> <span class=\"p\">(</span>\n        <span class=\"sa\">b</span><span class=\"sh\">'</span><span class=\"s\">&lt;err code=</span><span class=\"sh\">\"</span><span class=\"s\">100</span><span class=\"sh\">\"</span><span class=\"s\"> msg=</span><span class=\"sh\">\"</span><span class=\"s\">Invalid API Key (Key has invalid format)</span><span class=\"sh\">\"</span><span class=\"s\"> /&gt;</span><span class=\"sh\">'</span> <span class=\"ow\">in</span> <span class=\"n\">body</span>\n    <span class=\"p\">)</span>\n\n    <span class=\"k\">if</span> <span class=\"n\">is_error_response</span><span class=\"p\">:</span>\n        <span class=\"k\">raise</span> <span class=\"nc\">RuntimeError</span><span class=\"p\">(</span>\n            <span class=\"sh\">\"</span><span class=\"s\">You tried to record a new call to the Flickr API, </span><span class=\"se\">\\n</span><span class=\"sh\">\"</span>\n            <span class=\"sh\">\"</span><span class=\"s\">but the tests don</span><span class=\"sh\">'</span><span class=\"s\">t have an API key.</span><span class=\"se\">\\n</span><span class=\"sh\">\"</span>\n            <span class=\"sh\">\"</span><span class=\"se\">\\n</span><span class=\"sh\">\"</span>\n            <span class=\"sh\">\"</span><span class=\"s\">Pass an API key as an env var FLICKR_API_KEY=ae84\u2026,</span><span class=\"se\">\\n</span><span class=\"sh\">\"</span>\n            <span class=\"sh\">\"</span><span class=\"s\">and re-run the test.</span><span class=\"se\">\\n</span><span class=\"sh\">\"</span>\n        <span class=\"p\">)</span>\n\n    <span class=\"k\">return</span> <span class=\"n\">response</span>\n</code></pre>\n<p>We can call this hook in our <code>vcr.use_cassette</code> call:</p>\n<pre><code><span class=\"k\">def</span> <span class=\"nf\">test_flickr_api</span><span class=\"p\">(</span><span class=\"n\">cassette_name</span><span class=\"p\">):</span>\n    <span class=\"k\">with</span> <span class=\"n\">vcr</span><span class=\"p\">.</span><span class=\"nf\">use_cassette</span><span class=\"p\">(</span>\n        <span class=\"n\">cassette_name</span><span class=\"p\">,</span>\n        <span class=\"n\">filter_query_parameters</span><span class=\"o\">=</span><span class=\"p\">[(</span><span class=\"sh\">\"</span><span class=\"s\">api_key</span><span class=\"sh\">\"</span><span class=\"p\">,</span> <span class=\"sh\">\"</span><span class=\"s\">REDACTED_API_KEY</span><span class=\"sh\">\"</span><span class=\"p\">)],</span>\n        <span class=\"n\">decode_compressed_response</span><span class=\"o\">=</span><span class=\"bp\">True</span><span class=\"p\">,</span>\n        <span class=\"n\">before_record_response</span><span class=\"o\">=</span><span class=\"n\">check_for_invalid_api_key</span><span class=\"p\">,</span>\n    <span class=\"p\">):</span>\n        <span class=\"bp\">...</span>\n</code></pre>\n<p>Now, if you try to record a Flickr API call and don\u2019t set the API key, you\u2019ll get a helpful error explaining how to re-run the test correctly.</p>\n\n<h3 id=\"wrapping-everything-in-a-fixture-for-convenience\">Wrapping everything in a fixture for convenience</h3>\n\n<p>This is all useful, but it\u2019s a lot of boilerplate to add to every test.\nTo make everything cleaner, I wrap vcrpy in a <a href=\"https://docs.pytest.org/en/6.2.x/fixture.html\">pytest fixture</a> that returns an HTTP client I can use in my tests.\nThis fixture allows me to configure vcrpy, and also do any other setup I need on the HTTP client \u2013 for example, adding authentication params or HTTP headers.</p>\n\n<p>Here\u2019s an example of such a fixture in a <a href=\"https://github.com/Flickr-Foundation/flickr-photos-api\">library for using the Flickr API</a>:</p>\n<pre><code><span class=\"nd\">@pytest.fixture</span>\n<span class=\"k\">def</span> <span class=\"nf\">flickr_api</span><span class=\"p\">(</span><span class=\"n\">cassette_name</span><span class=\"p\">):</span>\n    <span class=\"k\">with</span> <span class=\"n\">vcr</span><span class=\"p\">.</span><span class=\"nf\">use_cassette</span><span class=\"p\">(</span>\n        <span class=\"n\">cassette_name</span><span class=\"p\">,</span>\n        <span class=\"n\">filter_query_parameters</span><span class=\"o\">=</span><span class=\"p\">[(</span><span class=\"sh\">\"</span><span class=\"s\">api_key</span><span class=\"sh\">\"</span><span class=\"p\">,</span> <span class=\"sh\">\"</span><span class=\"s\">REDACTED_API_KEY</span><span class=\"sh\">\"</span><span class=\"p\">)],</span>\n        <span class=\"n\">decode_compressed_response</span><span class=\"o\">=</span><span class=\"bp\">True</span><span class=\"p\">,</span>\n        <span class=\"n\">before_record_response</span><span class=\"o\">=</span><span class=\"n\">check_for_invalid_api_key</span><span class=\"p\">,</span>\n    <span class=\"p\">):</span>\n        <span class=\"n\">client</span> <span class=\"o\">=</span> <span class=\"n\">httpx</span><span class=\"p\">.</span><span class=\"nc\">Client</span><span class=\"p\">(</span>\n            <span class=\"n\">params</span><span class=\"o\">=</span><span class=\"p\">{</span><span class=\"sh\">\"</span><span class=\"s\">api_key</span><span class=\"sh\">\"</span><span class=\"p\">:</span> <span class=\"n\">os</span><span class=\"p\">.</span><span class=\"n\">environ</span><span class=\"p\">.</span><span class=\"nf\">get</span><span class=\"p\">(</span><span class=\"sh\">\"</span><span class=\"s\">FLICKR_API_KEY</span><span class=\"sh\">\"</span><span class=\"p\">,</span> <span class=\"sh\">\"</span><span class=\"s\">API_KEY</span><span class=\"sh\">\"</span><span class=\"p\">)},</span>\n            <span class=\"n\">headers</span><span class=\"o\">=</span><span class=\"p\">{</span>\n                <span class=\"c1\"># Close the connection as soon as the API returns a\n</span>                <span class=\"c1\"># response, to fix pytest warnings about unclosed sockets.\n</span>                <span class=\"sh\">\"</span><span class=\"s\">Connection</span><span class=\"sh\">\"</span><span class=\"p\">:</span> <span class=\"sh\">\"</span><span class=\"s\">Close</span><span class=\"sh\">\"</span><span class=\"p\">,</span>\n            <span class=\"p\">},</span>\n        <span class=\"p\">)</span>\n\n        <span class=\"k\">yield</span> <span class=\"n\">client</span>\n</code></pre>\n<p>This makes individual tests much shorter and simpler:</p>\n<pre><code><span class=\"k\">def</span> <span class=\"nf\">test_flickr_api_without_boilerplate</span><span class=\"p\">(</span><span class=\"n\">flickr_api</span><span class=\"p\">):</span>\n    <span class=\"n\">resp</span> <span class=\"o\">=</span> <span class=\"n\">flickr_api</span><span class=\"p\">.</span><span class=\"nf\">get</span><span class=\"p\">(</span>\n        <span class=\"sh\">\"</span><span class=\"s\">https://api.flickr.com/services/rest/</span><span class=\"sh\">\"</span><span class=\"p\">,</span>\n        <span class=\"n\">params</span><span class=\"o\">=</span><span class=\"p\">{</span>\n            <span class=\"sh\">\"</span><span class=\"s\">method</span><span class=\"sh\">\"</span><span class=\"p\">:</span> <span class=\"sh\">\"</span><span class=\"s\">flickr.urls.lookupUser</span><span class=\"sh\">\"</span><span class=\"p\">,</span>\n            <span class=\"sh\">\"</span><span class=\"s\">url</span><span class=\"sh\">\"</span><span class=\"p\">:</span> <span class=\"sh\">\"</span><span class=\"s\">https://www.flickr.com/photos/alexwlchan/</span><span class=\"sh\">\"</span><span class=\"p\">,</span>\n        <span class=\"p\">},</span>\n    <span class=\"p\">)</span>\n\n    <span class=\"k\">assert</span> <span class=\"sh\">'</span><span class=\"s\">&lt;user id=</span><span class=\"sh\">\"</span><span class=\"s\">199258389@N04</span><span class=\"sh\">\"</span><span class=\"s\">&gt;</span><span class=\"sh\">'</span> <span class=\"ow\">in</span> <span class=\"n\">resp</span><span class=\"p\">.</span><span class=\"n\">text</span>\n</code></pre>\n<p>When somebody reads this test, they don\u2019t need to think about the authentication or or mocking; they can just see the API call that we\u2019re making.</p>\n\n<hr />\n\n<h2 id=\"when-i-dont-vcrpy\">When I don\u2019t vcrpy</h2>\n\n<p>Although vcrpy is useful, there are times when I prefer to test my HTTP code in a different way.\nHere are a few examples.</p>\n\n<h3 id=\"if-im-testing-error-handling\">If I\u2019m testing error handling</h3>\n\n<p>If I\u2019m testing my error handling code \u2013 errors like timeouts, connection failures, or 5xx errors \u2013 it\u2019s difficult to record a real response.\nEven if I could find a reliable error case today, it might be fixed tomorrow, which makes it difficult to reproduce if I ever need to re-record a cassette.</p>\n\n<p>When I test error handling, I prefer a pure-Python mock where I can see exactly what error conditions I\u2019m creating.</p>\n\n<h3 id=\"if-im-fetching-lots-of-binary-files\">If I\u2019m fetching lots of binary files</h3>\n\n<p>If my HTTP code is downloading images and video, storing them in a vcrpy cassette is pretty inefficient \u2013 they have to be encoded as base64.\nThis makes the cassettes large and inefficient, the extra decoding step slows my test down, and the files are hard to inspect.</p>\n\n<p>When I\u2019m testing with binary files, I store them as standalone files in my <code>fixtures</code> directory (e.g. in <code>tests/fixtures/images</code>), and I write my own mock to read the file from disk.\nI can easily inspect or modify the fixture data, and I don\u2019t have the overhead of using cassettes.</p>\n\n<h3 id=\"if-im-testing-future-or-hypothetical-changes-in-an-api\">If I\u2019m testing future or hypothetical changes in an API</h3>\n\n<p>A vcrpy cassette locks in the current behaviour.\nBut suppose I know about an upcoming change, or I want to check my code would handle an unusual response \u2013 I can\u2019t capture that in a vcrpy cassette, because the server isn\u2019t returning responses like that (yet).</p>\n\n<p>In those cases, I either construct a vcrpy cassette with the desired response by hand, or I use a code-based mock to return my unusual response.</p>\n\n<hr />\n\n<h2 id=\"summary\">Summary</h2>\n\n<p>Using vcrpy has allowed me to write more thorough tests, and it does all the hard work of intercepting HTTP calls and serialising them to disk.\nIt gives me high-fidelity snapshots of HTTP responses, allowing me to mock HTTP calls and avoid network requests in my tests.\nThis makes my tests faster, consistent, and reliable.</p>\n\n<p>Here\u2019s a quick reminder of what I do to run vcrpy in production:</p>\n\n<ul>\n  <li>I use <code>filter_query_parameters</code> and <code>filter_headers</code> to keep secrets out of cassette files</li>\n  <li>I set <code>decode_compressed_response=True</code> to make cassettes more readable</li>\n  <li>I name cassettes after the test function they\u2019re associated with</li>\n  <li>I throw errors if an HTTP client isn\u2019t set up correctly when you try to record a cassette</li>\n  <li>I wrap everything in a fixture, to keep individual tests simpler</li>\n</ul>\n\n<p>If you make HTTP calls from your tests, I really recommend it: <a href=\"https://vcrpy.readthedocs.io/en/latest/\">https://vcrpy.readthedocs.io/en/latest/</a></p>\n\n\n    <p>[If the formatting of this post looks odd in your feed reader, <a href=\"https://alexwlchan.net/2025/testing-with-vcrpy/?ref=rss\">visit the original article</a>]</p>"
            ],
            "link": "https://alexwlchan.net/2025/testing-with-vcrpy/?ref=rss",
            "publishedAt": "2025-08-28",
            "source": "Alex Chan",
            "summary": "How I record HTTP requests to get fast, reliable, and consistent tests, and the patterns I use in a production codebase.",
            "title": "Using vcrpy to test HTTP interactions in Python"
        },
        {
            "content": [
                "<p>Here\u2019s one possible hobby:</p>\n\n<ol>\n  <li>Take something you don\u2019t like.</li>\n  <li><em>Try</em> to like it.</li>\n</ol>\n\n<p>It could be food or music or people or just the general situation you\u2019re in. I recommend this hobby, partly because it\u2019s nice to enjoy things, but mostly as an instrument for probing human nature.</p>\n\n<h2 id=\"1\">1.</h2>\n\n<p>I was in Paris once. By coincidence, I wandered past a bunch of places that were playing Michael Jackson. I thought to myself, \u201cHuh. The French sure do love Michael Jackson.\u201d Gradually I decided, \u201cYou know what? They\u2019re right! Michael Jackson is good.\u201d Later, I saw a guy driving around blasting <em>Billy Jean</em> while hanging a hand outside his car with a sparkly white Michael Jackson glove. Again, I thought, \u201cHuh.\u201d That day was <a href=\"https://en.wikipedia.org/wiki/Death_of_Michael_Jackson\">June 25, 2009</a>.</p>\n\n<h2 id=\"2\">2.</h2>\n\n<p>I don\u2019t like cooked spinach. But if I eat some and try to forget that I hate it, it seems OK. Why?</p>\n\n<p>Well, as a child, I was subjected to some misguided spinach-related parental interventions. (\u201cYou cannot leave this table until you\u2019ve finished this extremely small portion\u201d, etc.) I hated this, but looking back, it wasn\u2019t the innate qualities of spinach the bothered me, so much as that being forced to put something inside my body felt like a violation of my autonomy.</p>\n\n<p>When I encountered spinach as an adult, instead of tasting a vegetable, I tasted a grueling battle of will. Spinach was dangerous\u2014if I liked it, that would teach my parents that they were right to control my diet.</p>\n\n<p>So I tried telling myself little stories: I\u2019m hiking in the mountains in Japan when suddenly the temperature drops, and it starts pouring rain. Freezing and desperate, I spot a monastery and knock on the door. The monks warm me up and offer me <em>h\u014drens\u014d no ohitashi</em>, made from some exotic vegetable I\u2019ve never seen before. Presumably, I\u2019d think it was amazing.</p>\n\n<p>I can\u2019t fully access that mind-space. But just knowing it exists seems to make a big difference. Using similar techniques, I\u2019ve successfully made myself like (or less dislike) white wine, Ezra Klein, disco, yoga, non-spicy food, Pearl Jam, and Studio Ghibli movies.</p>\n\n<p>Lesson: Sometimes we dislike things simply because we have a <em>concept</em> of ourselves as not liking them.</p>\n\n<h2 id=\"3\">3.</h2>\n\n<p>Meanwhile, I\u2019ve failed to make myself like country music. I mean, I like <a href=\"https://www.youtube.com/watch?v=WOHPuY88Ry4\"><em>A Boy Named Sue</em></a>. Who doesn\u2019t? But what about <em>Stand By Your Man</em> or <em>Dust on the Bottle</em>? I listen to these, and I appreciate what they\u2019re doing. I admire that they aren\u2019t entirely oriented around the concerns of teenagers. But I can\u2019t seem to actually enjoy them.</p>\n\n<p>Of course, it seems unlikely that this is unrelated to the fact that no one in my peer group thinks country music is cool. On the other hand, I\u2019m constantly annoyed that my opinions aren\u2019t more unique or interesting. And I subscribe to the idea that what\u2019s <em>really</em> cool is to be a cultural omnivore who appreciates everything.</p>\n\n<p>It doesn\u2019t matter. I still can\u2019t like country music. I <em>think</em> the problem is that I don\u2019t actually want to like country music. I only <em>want</em> to want to like country music. The cultural programming is in too deep.</p>\n\n<p>Lesson: Certain levels of the subconscious are easier to screw around with than others.</p>\n\n<h2 id=\"4\">4.</h2>\n\n<p>For years, a friend and I would go on week-long hikes. Before we started, we\u2019d go make our own trail mix, and I\u2019d always insist on adding raisins. Each year, my friend would object more loudly that I don\u2019t actually like raisins. But I do like raisins. So I\u2019d scoff. But after several cycles, I had to admit that while I \u201cliked raisins\u201d there never came a time that I actually wanted to <em>eat</em> raisins, ever.</p>\n\n<p>Related: Once every year or two, I\u2019ll have a rough day, and I\u2019ll say to myself, \u201cOK, screw it. Liking Oasis is the lamest thing that has ever been done by anyone. But the dirty truth is that I love Oasis. So I will listen to Oasis and thereby be comforted.\u201d Then I listen to Oasis, and it just isn\u2019t that good.</p>\n\n<p>Lesson: You can have an incorrect concept of self.</p>\n\n<h2 id=\"5\">5.</h2>\n\n<p>I don\u2019t like this about myself, but I\u2019m a huge snob regarding television. I believe TV can be true art, as high as any other form. (How does <em>My Brilliant Friend</em> only have an 89 on Metacritic?) But even after pretentiously filtering for critical acclaim, I usually feel that most shows are slop and can\u2019t watch them.</p>\n\n<p>At first glance, this seems just like country music\u2014I don\u2019t like it because of status-driven memetic desire or whatever. But there\u2019s a difference. Not liking country music is fine (neurotic self-flagellation aside) because there\u2019s an infinite amount of other music. But not liking most TV is really annoying, because often I <em>want</em> to watch TV, but can\u2019t find anything acceptable.</p>\n\n<p>I see three possible explanations:</p>\n\n<ol>\n  <li>\n    <p>Almost all TV is, in fact, bad.</p>\n  </li>\n  <li>\n    <p>Lots of TV is fine, but just doesn\u2019t appeal to me.</p>\n  </li>\n  <li>\n    <p>Lots of TV is fine, but it\u2019s hard to tell yourself stories where you\u2019re hiking in the mountains and a bunch of Japanese monks show you, like, <em>Big Bang Theory</em>.</p>\n  </li>\n</ol>\n\n<p>Whatever it is, it seems hard to change.</p>\n\n<p>Lesson: Some things are hard to change.</p>\n\n<h2 id=\"7\">7.</h2>\n\n<p>On planes, the captain will often invite you to, \u201csit back and enjoy the ride\u201d. This is confusing. Enjoy the ride? Enjoy being trapped in a pressurized tube and jostled by all the passengers lining up to relieve themselves because your company decided to cram in a few more seats instead of having an adequate number of toilets? Aren\u2019t flights supposed to be endured?</p>\n\n<p>At the same time, those invitations seem like a glimpse of a parallel universe. Are there members of my species who sit back and enjoy flights?</p>\n\n<p>I have no hard data. But it\u2019s a good heuristic that there are people \u201cwho actually X\u201d for approximately all values of X. If one in nine people enjoy <a href=\"https://doi.org/10.1111/joor.13305\">going to the dentist</a>, surely at least that many enjoy being on planes.</p>\n\n<p>What I think the captain is trying to say is, \u201cWhile you can\u2019t always control your situation, you have tremendous power over how you <em>experience</em> that situation. You may find a cramped flight to be a torture. But the torture happens inside your head. Some people like you situation. You too, perhaps could like it.\u201d</p>\n\n<p>That\u2019s an important message. Though one imagines that giving it as an in-flight announcement would cause more confusion, not less. So the captain does what they can.</p>"
            ],
            "link": "https://dynomight.net/liking/",
            "publishedAt": "2025-08-28",
            "source": "Dynomight",
            "summary": "<p>Here\u2019s one possible hobby:</p> <ol> <li>Take something you don\u2019t like.</li> <li><em>Try</em> to like it.</li> </ol> <p>It could be food or music or people or just the general situation you\u2019re in. I recommend this hobby, partly because it\u2019s nice to enjoy things, but mostly as an instrument for probing human nature.</p> <h2 id=\"1\">1.</h2> <p>I was in Paris once. By coincidence, I wandered past a bunch of places that were playing Michael Jackson. I thought to myself, \u201cHuh. The French sure do love Michael Jackson.\u201d Gradually I decided, \u201cYou know what? They\u2019re right! Michael Jackson is good.\u201d Later, I saw a guy driving around blasting <em>Billy Jean</em> while hanging a hand outside his car with a sparkly white Michael Jackson glove. Again, I thought, \u201cHuh.\u201d That day was <a href=\"https://en.wikipedia.org/wiki/Death_of_Michael_Jackson\">June 25, 2009</a>.</p> <h2 id=\"2\">2.</h2> <p>I don\u2019t like cooked spinach. But if I eat some and try to forget that I hate it, it seems OK. Why?</p> <p>Well, as a child, I was subjected to some misguided spinach-related parental interventions. (\u201cYou cannot leave this table until you\u2019ve finished this extremely small portion\u201d, etc.) I hated this, but looking back, it wasn\u2019t the innate qualities of spinach the bothered me, so much as that",
            "title": "You can try to like stuff"
        },
        {
            "content": [
                "<p>More than a billion people use AI chatbots regularly. ChatGPT has over 700 million weekly users. Gemini and other leading AIs add hundreds of millions more. In my posts, I often focus on the advances that AI is making (for example, in the past few weeks, both OpenAI and Google AIs chatbots got <a href=\"https://www.nytimes.com/2025/07/21/technology/google-ai-international-mathematics-olympiad.html\">gold medals</a> in the International Math Olympiad), but that obscures a broader shift that's been building: we're entering an era of Mass Intelligence, where powerful AI is becoming as accessible as a Google search.</p><p>Until recently, free users of these systems (the overwhelming majority) had access only to older, smaller AI models that frequently made mistakes and had limited use for complex work. The best models, like Reasoners that can solve very hard problems and hallucinate much less often, required paying somewhere between $20 and $200 a month. And even then, you needed to know which model to pick and how to prompt it properly. But the economics and interfaces are changing rapidly, with fairly large consequences for how all of us work, learn, and think.</p><h1>Powerful AI is Getting Cheaper and Easier to Access</h1><p>There have been two barriers to accessing powerful AI for most users. The first was confusion. Few people knew to select an AI model. Even fewer knew that picking o3 from a menu in ChatGPT would get them access to an excellent Reasoner AI model, while picking 4o (which seems like a higher number) would give them something far less capable. According to OpenAI, less than 7% of paying customers selected o3 on a regular basis, meaning even power users were missing out on what Reasoners could do.</p><p>Another factor was cost. Because the best models are expensive, free users were often not given access to them, or else given very limited access. Google led the way in giving some free access to its best models, but OpenAI stated that almost none of its free customers had regular access to reasoning models prior to the launch of GPT-5.</p><p>GPT-5 was supposed to solve both of these problems, which is partially why its debut was so messy and confusing. GPT-5 is actually two things. It was the overall name for a family of quite different models, from the weaker GPT-5 Nano to the powerful GPT-5 Pro. It was also the name given to the tool that picked which model to use and how much computing power the AI should use to solve your problem. When you are writing to &#8220;GPT-5&#8221; you are actually talking to a router that is supposed to automatically decide whether your problem can be solved by a smaller, faster model or needs to go to a more powerful Reasoner. </p><div class=\"captioned-image-container\"><figure><a class=\"image-link image2 is-viewable-img\" href=\"https://substackcdn.com/image/fetch/$s_!kiXa!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd1914e7d-fe26-403d-b1ea-b156065ac3ff_1632x1168.png\" target=\"_blank\"><div class=\"image2-inset\"><source type=\"image/webp\" /><img alt=\"\" class=\"sizing-normal\" height=\"1042\" src=\"https://substackcdn.com/image/fetch/$s_!kiXa!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd1914e7d-fe26-403d-b1ea-b156065ac3ff_1632x1168.png\" width=\"1456\" /><div class=\"image-link-expand\"><div class=\"pencraft pc-display-flex pc-gap-8 pc-reset\"><div class=\"pencraft pc-reset icon-container restack-image\"><svg class=\"lucide lucide-refresh-cw\" fill=\"none\" height=\"20\" stroke=\"currentColor\" stroke-linecap=\"round\" stroke-linejoin=\"round\" stroke-width=\"2\" viewBox=\"0 0 24 24\" width=\"20\" xmlns=\"http://www.w3.org/2000/svg\"><path d=\"M3 12a9 9 0 0 1 9-9 9.75 9.75 0 0 1 6.74 2.74L21 8\"></path><path d=\"M21 3v5h-5\"></path><path d=\"M21 12a9 9 0 0 1-9 9 9.75 9.75 0 0 1-6.74-2.74L3 16\"></path><path d=\"M8 16H3v5\"></path></svg></div><div class=\"pencraft pc-reset icon-container view-image\"><svg class=\"lucide lucide-maximize2\" fill=\"none\" height=\"20\" stroke=\"currentColor\" stroke-linecap=\"round\" stroke-linejoin=\"round\" stroke-width=\"2\" viewBox=\"0 0 24 24\" width=\"20\" xmlns=\"http://www.w3.org/2000/svg\"><polyline points=\"15 3 21 3 21 9\"></polyline><polyline points=\"9 21 3 21 3 15\"></polyline><line x1=\"21\" x2=\"14\" y1=\"3\" y2=\"10\"></line><line x1=\"3\" x2=\"10\" y1=\"21\" y2=\"14\"></line></svg></div></div></div></div></a><figcaption class=\"image-caption\">When you pick ChatGPT 5 you are actually picking Auto mode, which selects among the various ChatGPT 5 models, some of which are among the best models in the world, some of which are much weaker. If you pay for access, select &#8220;GPT-5 Thinking&#8221; for almost any problem beyond a simple chat.</figcaption></figure></div><p>You could see how this was supposed to expand access to powerful AI to more users: if you just wanted to chat, GPT-5 was supposed to use its weaker specialized chat models; if you were trying to solve a math problem, GPT-5 was supposed to send you to its slower, more expensive GPT-5 Thinking model. This would save money and give more people access to the best AIs. But the rollout had issues. This practice wasn&#8217;t well explained and the router did not work well at first. The result is that one person using GPT-5 got a very smart answer while another got a bad one. Despite these issues, OpenAI reported early success. Within a few days of launch, the percentage of paying customers who had used a Reasoner went from 7% to 24% and the number of free customers using the most powerful models went from almost zero to 7%.  </p><p>Part of this change is driven by the fact that smarter models are getting dramatically more efficient to run. This graph shows how fast this trend has played out, mapping the capability of AI on the y-axis and the logarithmically decreasing costs on the x-axis. When GPT-4 came out it was around $50 to work with a million tokens (a token is roughly a word), now it costs around 14 cents per million tokens to use GPT-5 nano, a much more capable model than the original GPT-4.</p><div class=\"captioned-image-container\"><figure><a class=\"image-link image2 is-viewable-img\" href=\"https://substackcdn.com/image/fetch/$s_!Kn3w!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbc777761-5ac9-4380-b78d-2216a9835b13_1886x1360.png\" target=\"_blank\"><div class=\"image2-inset\"><source type=\"image/webp\" /><img alt=\"\" class=\"sizing-normal\" height=\"377.16346153846155\" src=\"https://substackcdn.com/image/fetch/$s_!Kn3w!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbc777761-5ac9-4380-b78d-2216a9835b13_1886x1360.png\" width=\"523\" /><div class=\"image-link-expand\"><div class=\"pencraft pc-display-flex pc-gap-8 pc-reset\"><div class=\"pencraft pc-reset icon-container restack-image\"><svg class=\"lucide lucide-refresh-cw\" fill=\"none\" height=\"20\" stroke=\"currentColor\" stroke-linecap=\"round\" stroke-linejoin=\"round\" stroke-width=\"2\" viewBox=\"0 0 24 24\" width=\"20\" xmlns=\"http://www.w3.org/2000/svg\"><path d=\"M3 12a9 9 0 0 1 9-9 9.75 9.75 0 0 1 6.74 2.74L21 8\"></path><path d=\"M21 3v5h-5\"></path><path d=\"M21 12a9 9 0 0 1-9 9 9.75 9.75 0 0 1-6.74-2.74L3 16\"></path><path d=\"M8 16H3v5\"></path></svg></div><div class=\"pencraft pc-reset icon-container view-image\"><svg class=\"lucide lucide-maximize2\" fill=\"none\" height=\"20\" stroke=\"currentColor\" stroke-linecap=\"round\" stroke-linejoin=\"round\" stroke-width=\"2\" viewBox=\"0 0 24 24\" width=\"20\" xmlns=\"http://www.w3.org/2000/svg\"><polyline points=\"15 3 21 3 21 9\"></polyline><polyline points=\"9 21 3 21 3 15\"></polyline><line x1=\"21\" x2=\"14\" y1=\"3\" y2=\"10\"></line><line x1=\"3\" x2=\"10\" y1=\"21\" y2=\"14\"></line></svg></div></div></div></div></a><figcaption class=\"image-caption\">The Graduate-Level Google-Proof Q&amp;A test (GPQA) is a series of very hard multiple-choice problems designed to test advanced knowledge. non-experts with access to the internet get 34% right, PhDs with internet access get 74-81% inside their specialty. The cost per million tokens is the cost of using the model. (I gathered this data, so apologies for any errors.)</figcaption></figure></div><p>This efficiency gain isn't just financial, it's also environmental. <a href=\"https://services.google.com/fh/files/misc/measuring_the_environmental_impact_of_delivering_ai_at_google_scale.pdf\">Google has reported that energy efficiency per prompt has improved by 33x </a>in the last year alone. The marginal energy used by a standard prompt from a modern LLM in 2025 <a href=\"https://x.com/emollick/status/1959989512228208785\">is relatively established at this point</a>, from both independent tests and official announcements. It is roughly 0.0003 kWh, the same energy use as 8-10 seconds of streaming Netflix or the equivalent of a Google search in 2008 (interestingly, image creation seems to use a similar amount of energy as a text prompt)<a class=\"footnote-anchor\" href=\"https://www.oneusefulthing.org/feed#footnote-1\" id=\"footnote-anchor-1\" target=\"_self\">1</a>. How much water these models use per prompt is less clear but ranges from a few drops to a fifth of a shot glass (.25mL to 5mL+), depending on the definitions of water use (here is <a href=\"https://andymasley.substack.com/p/an-example-of-what-i-consider-a-misleading\">the low water argument</a> and the <a href=\"https://www.linkedin.com/posts/shaolei-ren-68557415_today-google-released-a-paper-disclosing-activity-7364343376986427392-oMhX/\">high water argument</a>).</p><p>These improvements mean that even as AI gets more powerful, it's also becoming viable to give to more people. The marginal cost of serving each additional user has collapsed, which means more business models, like ad support, become possible. Free users can now run prompts that would have cost dollars just two years ago. This is how a billion people suddenly get access to powerful AIs: not through some grand democratization initiative, but because the economics finally make it possible.</p><h1>Powerful AI is Getting Easy to Use</h1><p>Getting access to a powerful AI is not enough, people need to actually use it to get things done. Using AI well used to be a pretty challenging process which involved crafting a prompt using techniques like chain-of-thought along with learning tips and tricks to get the most out of your AI. In a recent series of experiments, however, we have discovered that <a href=\"https://gail.wharton.upenn.edu/research-and-insights/tech-report-chain-of-thought/\">these techniques don&#8217;t really help anymore</a>. Powerful AI models are just getting better at doing what you ask them to or even figuring out what you want and going beyond what you ask (and no, <a href=\"https://gail.wharton.upenn.edu/research-and-insights/techreport-threaten-or-tip/\">threatening </a>them or <a href=\"https://gail.wharton.upenn.edu/research-and-insights/tech-report-prompt-engineering-is-complicated-and-contingent/\">being nice to them</a> does not seem to help on average).</p><p>And it isn&#8217;t just text models that are becoming cheaper and easier to use. Google released a new image model with the code name &#8220;nano banana&#8221; and the much more boring official name Gemini 2.5 Flash Image Generator. In addition to being excellent (though better at editing images than creating new ones), it is also cheap enough that free users can access it. And, unlike previous generations of AI image generators, it follows instructions in plain language very well.</p><p>As an example of both its power and ease of use, I uploaded an iconic (and copyright free) image of the Apollo 11 astronauts and a random picture of a sparkly tuxedo and gave it the simplest prompts: &#8220;<em>dress Neil Armstrong on the left in this tuxedo</em>&#8221;</p><div class=\"captioned-image-container\"><figure><a class=\"image-link image2 is-viewable-img\" href=\"https://substackcdn.com/image/fetch/$s_!zmP2!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6b305085-662d-4190-bea2-ad4fd8b76fa9_1896x828.png\" target=\"_blank\"><div class=\"image2-inset\"><source type=\"image/webp\" /><img alt=\"\" class=\"sizing-normal\" height=\"263.83516483516485\" src=\"https://substackcdn.com/image/fetch/$s_!zmP2!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6b305085-662d-4190-bea2-ad4fd8b76fa9_1896x828.png\" width=\"604\" /><div class=\"image-link-expand\"><div class=\"pencraft pc-display-flex pc-gap-8 pc-reset\"><div class=\"pencraft pc-reset icon-container restack-image\"><svg class=\"lucide lucide-refresh-cw\" fill=\"none\" height=\"20\" stroke=\"currentColor\" stroke-linecap=\"round\" stroke-linejoin=\"round\" stroke-width=\"2\" viewBox=\"0 0 24 24\" width=\"20\" xmlns=\"http://www.w3.org/2000/svg\"><path d=\"M3 12a9 9 0 0 1 9-9 9.75 9.75 0 0 1 6.74 2.74L21 8\"></path><path d=\"M21 3v5h-5\"></path><path d=\"M21 12a9 9 0 0 1-9 9 9.75 9.75 0 0 1-6.74-2.74L3 16\"></path><path d=\"M8 16H3v5\"></path></svg></div><div class=\"pencraft pc-reset icon-container view-image\"><svg class=\"lucide lucide-maximize2\" fill=\"none\" height=\"20\" stroke=\"currentColor\" stroke-linecap=\"round\" stroke-linejoin=\"round\" stroke-width=\"2\" viewBox=\"0 0 24 24\" width=\"20\" xmlns=\"http://www.w3.org/2000/svg\"><polyline points=\"15 3 21 3 21 9\"></polyline><polyline points=\"9 21 3 21 3 15\"></polyline><line x1=\"21\" x2=\"14\" y1=\"3\" y2=\"10\"></line><line x1=\"3\" x2=\"10\" y1=\"21\" y2=\"14\"></line></svg></div></div></div></div></a></figure></div><p>Here is what it gave me a few seconds later:</p><div class=\"captioned-image-container\"><figure><a class=\"image-link image2 is-viewable-img\" href=\"https://substackcdn.com/image/fetch/$s_!dLl6!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F80f4d0d5-9a57-4444-9c07-c471f2a0abc5_1024x1024.png\" target=\"_blank\"><div class=\"image2-inset\"><source type=\"image/webp\" /><img alt=\"\" class=\"sizing-normal\" height=\"388\" src=\"https://substackcdn.com/image/fetch/$s_!dLl6!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F80f4d0d5-9a57-4444-9c07-c471f2a0abc5_1024x1024.png\" width=\"388\" /><div class=\"image-link-expand\"><div class=\"pencraft pc-display-flex pc-gap-8 pc-reset\"><div class=\"pencraft pc-reset icon-container restack-image\"><svg class=\"lucide lucide-refresh-cw\" fill=\"none\" height=\"20\" stroke=\"currentColor\" stroke-linecap=\"round\" stroke-linejoin=\"round\" stroke-width=\"2\" viewBox=\"0 0 24 24\" width=\"20\" xmlns=\"http://www.w3.org/2000/svg\"><path d=\"M3 12a9 9 0 0 1 9-9 9.75 9.75 0 0 1 6.74 2.74L21 8\"></path><path d=\"M21 3v5h-5\"></path><path d=\"M21 12a9 9 0 0 1-9 9 9.75 9.75 0 0 1-6.74-2.74L3 16\"></path><path d=\"M8 16H3v5\"></path></svg></div><div class=\"pencraft pc-reset icon-container view-image\"><svg class=\"lucide lucide-maximize2\" fill=\"none\" height=\"20\" stroke=\"currentColor\" stroke-linecap=\"round\" stroke-linejoin=\"round\" stroke-width=\"2\" viewBox=\"0 0 24 24\" width=\"20\" xmlns=\"http://www.w3.org/2000/svg\"><polyline points=\"15 3 21 3 21 9\"></polyline><polyline points=\"9 21 3 21 3 15\"></polyline><line x1=\"21\" x2=\"14\" y1=\"3\" y2=\"10\"></line><line x1=\"3\" x2=\"10\" y1=\"21\" y2=\"14\"></line></svg></div></div></div></div></a></figure></div><p>There are issues that someone with an expert eye would spot, but it is still impressive to see the realistic folds of the tuxedo and how it is blended into the scene (the NASA pin on the lapel was a nice touch). There is still a lot of randomness in the process that makes AI image editing unsuitable for many professional applications, but for most people, this represents a huge leap in not just what they can do, but how easy it is to do it.</p><p>And we can go further: &#8220;<em>now show a photograph where neil armstrong and buzz aldrin, in the same outfits, are sitting in their seats in a modern airplane, neil looks relaxed and is leaning back, playing a trumpet, buzz seems nervous and is holding a hamburger, in the middle seat is a realistic otter sitting in a seat and using a laptop.</em>&#8221;</p><div class=\"captioned-image-container\"><figure><a class=\"image-link image2 is-viewable-img\" href=\"https://substackcdn.com/image/fetch/$s_!t7UQ!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe3fce0d2-f57d-48a0-b267-a16fd4cd8a55_1024x1024.png\" target=\"_blank\"><div class=\"image2-inset\"><source type=\"image/webp\" /><img alt=\"\" class=\"sizing-normal\" height=\"556\" src=\"https://substackcdn.com/image/fetch/$s_!t7UQ!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe3fce0d2-f57d-48a0-b267-a16fd4cd8a55_1024x1024.png\" width=\"556\" /><div class=\"image-link-expand\"><div class=\"pencraft pc-display-flex pc-gap-8 pc-reset\"><div class=\"pencraft pc-reset icon-container restack-image\"><svg class=\"lucide lucide-refresh-cw\" fill=\"none\" height=\"20\" stroke=\"currentColor\" stroke-linecap=\"round\" stroke-linejoin=\"round\" stroke-width=\"2\" viewBox=\"0 0 24 24\" width=\"20\" xmlns=\"http://www.w3.org/2000/svg\"><path d=\"M3 12a9 9 0 0 1 9-9 9.75 9.75 0 0 1 6.74 2.74L21 8\"></path><path d=\"M21 3v5h-5\"></path><path d=\"M21 12a9 9 0 0 1-9 9 9.75 9.75 0 0 1-6.74-2.74L3 16\"></path><path d=\"M8 16H3v5\"></path></svg></div><div class=\"pencraft pc-reset icon-container view-image\"><svg class=\"lucide lucide-maximize2\" fill=\"none\" height=\"20\" stroke=\"currentColor\" stroke-linecap=\"round\" stroke-linejoin=\"round\" stroke-width=\"2\" viewBox=\"0 0 24 24\" width=\"20\" xmlns=\"http://www.w3.org/2000/svg\"><polyline points=\"15 3 21 3 21 9\"></polyline><polyline points=\"9 21 3 21 3 15\"></polyline><line x1=\"21\" x2=\"14\" y1=\"3\" y2=\"10\"></line><line x1=\"3\" x2=\"10\" y1=\"21\" y2=\"14\"></line></svg></div></div></div></div></a></figure></div><p>This is many things: A pretty impressive output from the AI (look at the expressions, and how it preserved Buzz&#8217;s ring and Neil&#8217;s lapel pin). A distortion of a famous moment in history made possible by AI. And a potential warning about how weird things are going to get when these sorts of technologies are used widely.</p><h1>The Weirdness of Mass Intelligence</h1><p>When powerful AI is in the hands of a billion people, a lot of things are going to happen at once. A lot of things are already happening at once.</p><p>Some people have <a href=\"https://www.reddit.com/r/MyBoyfriendIsAI/\">intense relationships</a> with AI models while other people are being <a href=\"https://www.nature.com/articles/s44184-023-00047-6\">saved from loneliness.</a> AI models may be<a href=\"https://www.psychologytoday.com/us/blog/urban-survival/202507/the-emerging-problem-of-ai-psychosis\"> causing mental breakdowns</a> and <a href=\"https://www.acpjournals.org/doi/epdf/10.7326/aimcc.2024.1260\">dangerous behavior</a> for some while being <a href=\"https://www.yahoo.com/news/chatgpt-uncovers-hidden-cancer-saves-215135629.html\">used to diagnose the diseases of others</a>. It is being used to <a href=\"https://www.theatlantic.com/technology/archive/2025/06/ai-obituaries-chatgpt/683096/\">write obituaries</a> and <a href=\"https://www.vox.com/future-perfect/440950/ai-chatgpt-bible-religion-spiritual-buddhism\">create scriptures</a> and cheat on homework and launch new ventures and thousands of other unexpected uses. These uses, and both the problems and benefits, are likely to only multiply as AI systems get more powerful.</p><p>And while Google's AI image generator has guardrails to limit misuse, as well as invisible watermarks to identify AI images, I expect much less restrictive AI image generators will likely get close to nano banana in quality in the coming months.</p><p>The AI companies (whether you believe their commitments to safety or not) seem to be <a href=\"https://x.com/sama/status/1953953990372471148\">as unable to absorb all of this</a> as the rest of us are. When a billion people have access to advanced AI, we've entered what we might call the era of Mass Intelligence. Every institution we have &#8212; schools, hospitals, courts, companies, governments &#8212; was built for a world where intelligence was scarce and expensive. Now every profession, every institution, every community has to figure out how to thrive with Mass Intelligence. How do we harness a billion people using AI while managing the chaos that comes with it? How do we rebuild trust when anyone can fabricate anything? How do we preserve what's valuable about human expertise while democratizing access to knowledge?</p><p>So here we are. Powerful AI is cheap enough to give away, easy enough that you don't need a manual, and capable enough to outperform humans at a range of intellectual tasks. A flood of opportunities and problems are about to show up in classrooms, courtrooms, and boardrooms around the world. The Mass Intelligence era is what happens when you give a billion people access to an unprecedented set of tools and see what they do with it. We are about to find out what that is like.</p><p class=\"button-wrapper\"><a class=\"button primary\" href=\"https://www.oneusefulthing.org/subscribe\"><span>Subscribe now</span></a></p><p class=\"button-wrapper\"><a class=\"button primary\" href=\"https://www.oneusefulthing.org/p/mass-intelligence?utm_source=substack&amp;utm_medium=email&amp;utm_content=share&amp;action=share\"><span>Share</span></a></p><p></p><div class=\"captioned-image-container\"><figure><a class=\"image-link image2 is-viewable-img\" href=\"https://substackcdn.com/image/fetch/$s_!YEXj!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1cec29eb-e93d-4e77-9ca2-2ebf86d7002c_1376x864.png\" target=\"_blank\"><div class=\"image2-inset\"><source type=\"image/webp\" /><img alt=\"\" class=\"sizing-normal\" height=\"372.9767441860465\" src=\"https://substackcdn.com/image/fetch/$s_!YEXj!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1cec29eb-e93d-4e77-9ca2-2ebf86d7002c_1376x864.png\" width=\"594\" /><div class=\"image-link-expand\"><div class=\"pencraft pc-display-flex pc-gap-8 pc-reset\"><div class=\"pencraft pc-reset icon-container restack-image\"><svg class=\"lucide lucide-refresh-cw\" fill=\"none\" height=\"20\" stroke=\"currentColor\" stroke-linecap=\"round\" stroke-linejoin=\"round\" stroke-width=\"2\" viewBox=\"0 0 24 24\" width=\"20\" xmlns=\"http://www.w3.org/2000/svg\"><path d=\"M3 12a9 9 0 0 1 9-9 9.75 9.75 0 0 1 6.74 2.74L21 8\"></path><path d=\"M21 3v5h-5\"></path><path d=\"M21 12a9 9 0 0 1-9 9 9.75 9.75 0 0 1-6.74-2.74L3 16\"></path><path d=\"M8 16H3v5\"></path></svg></div><div class=\"pencraft pc-reset icon-container view-image\"><svg class=\"lucide lucide-maximize2\" fill=\"none\" height=\"20\" stroke=\"currentColor\" stroke-linecap=\"round\" stroke-linejoin=\"round\" stroke-width=\"2\" viewBox=\"0 0 24 24\" width=\"20\" xmlns=\"http://www.w3.org/2000/svg\"><polyline points=\"15 3 21 3 21 9\"></polyline><polyline points=\"9 21 3 21 3 15\"></polyline><line x1=\"21\" x2=\"14\" y1=\"3\" y2=\"10\"></line><line x1=\"3\" x2=\"10\" y1=\"21\" y2=\"14\"></line></svg></div></div></div></div></a></figure></div><p></p><div class=\"footnote\"><a class=\"footnote-number\" contenteditable=\"false\" href=\"https://www.oneusefulthing.org/feed#footnote-anchor-1\" id=\"footnote-1\" target=\"_self\">1</a><div class=\"footnote-content\"><p>This is the energy required to answer a standard prompt. It does not take into account the energy needed to train AI models, which is a one-time process that is very energy intensive. We do not know how much energy is used to create a modern model, but it was estimated that training GPT-4 took a little above 500,000 kWh, about 18 hours of a Boeing 737 in flight.</p></div></div>"
            ],
            "link": "https://www.oneusefulthing.org/p/mass-intelligence",
            "publishedAt": "2025-08-28",
            "source": "Ethan Mollick",
            "summary": "<p>More than a billion people use AI chatbots regularly. ChatGPT has over 700 million weekly users. Gemini and other leading AIs add hundreds of millions more. In my posts, I often focus on the advances that AI is making (for example, in the past few weeks, both OpenAI and Google AIs chatbots got <a href=\"https://www.nytimes.com/2025/07/21/technology/google-ai-international-mathematics-olympiad.html\">gold medals</a> in the International Math Olympiad), but that obscures a broader shift that's been building: we're entering an era of Mass Intelligence, where powerful AI is becoming as accessible as a Google search.</p><p>Until recently, free users of these systems (the overwhelming majority) had access only to older, smaller AI models that frequently made mistakes and had limited use for complex work. The best models, like Reasoners that can solve very hard problems and hallucinate much less often, required paying somewhere between $20 and $200 a month. And even then, you needed to know which model to pick and how to prompt it properly. But the economics and interfaces are changing rapidly, with fairly large consequences for how all of us work, learn, and think.</p><h1>Powerful AI is Getting Cheaper and Easier to Access</h1><p>There have been two barriers to accessing powerful AI for most users. The first",
            "title": "Mass Intelligence"
        },
        {
            "content": [
                "<div class=\"trix-content\">\n  <div>At its best, marketing is a <strong>transfer of enthusiasm</strong>.<br /><br />When you're truly pumped about what you're doing, when you're truly driven by the vision, when you absolutely must make something that you need and want, your enthusiasm leaves a mark. It's a brand. Not the noun, but the verb.<br /><br />At its worst, marketing is a transfer of everything else. Your worst fears, your biggest insecurities, the charades you play. False enthusiasm on display, empty promises, and sloganeering no one believes. It quickly makes you a liar.<br /><br />Just like you can't not communicate, you can't not market. Everything is marketing.<br /><br />The best, and the worst, is always on display, like it or not. You can't hide from your own presence, however it shows up. Marketing casts, like a shadow casts. Attached to every move.<br /><br />Think about what someone else is doing that you're enthused about. Where did that come from? What transferred it?<br /><br />Of course many things that are great simply work. Nothing more, nothing less. No stories, no excitement, just the snick of a perfect fit. But somewhere down the chain, someone cared enough to make that thing right. And that's a transfer too.<br /><br /></div><div>-Jason</div>\n</div>"
            ],
            "link": "https://world.hey.com/jason/marketing-is-8d39f651",
            "publishedAt": "2025-08-28",
            "source": "Jason Fried",
            "summary": "<div class=\"trix-content\"> <div>At its best, marketing is a <strong>transfer of enthusiasm</strong>.<br /><br />When you're truly pumped about what you're doing, when you're truly driven by the vision, when you absolutely must make something that you need and want, your enthusiasm leaves a mark. It's a brand. Not the noun, but the verb.<br /><br />At its worst, marketing is a transfer of everything else. Your worst fears, your biggest insecurities, the charades you play. False enthusiasm on display, empty promises, and sloganeering no one believes. It quickly makes you a liar.<br /><br />Just like you can't not communicate, you can't not market. Everything is marketing.<br /><br />The best, and the worst, is always on display, like it or not. You can't hide from your own presence, however it shows up. Marketing casts, like a shadow casts. Attached to every move.<br /><br />Think about what someone else is doing that you're enthused about. Where did that come from? What transferred it?<br /><br />Of course many things that are great simply work. Nothing more, nothing less. No stories, no excitement, just the snick of a perfect fit. But somewhere down the chain, someone cared enough to make that thing right. And that's a",
            "title": "Marketing is..."
        },
        {
            "content": [
                "<div class=\"epigraph\">\n<blockquote>\n<p>Cops say criminals use a Google Pixel with GrapheneOS \u2014 I say that\u2019s freedom\n</p>\n<footer><span class=\"author\"><a href=\"https://www.androidauthority.com/why-i-use-grapheneos-on-pixel-3575477/\">Calvin Wankhede</a>\n</span></footer>\n</blockquote>\n</div>\n<p>We\u2019re in a dark time period right now.</p>\n<p>Authoritarianism is on the rise throughout the globe.\nGovernments wants to monitor your social media accounts so they can make you disappear if you engage in wrongthink such as opposing wars or genocide.\nThis is worsened by misguided laws like <a href=\"https://fightchatcontrol.eu/\">Chat control</a> that would mandate scanning of all digital communication,\nexposing any wrongthink in your \u201cprivate messages\u201d.</p>\n<aside class=\"note\">\n<p>Silly me, <a href=\"https://fightchatcontrol.eu/\">Chat control</a> doesn\u2019t mandate scanning of <em>all</em> digital communication; politicians and their families are, predictably, exempt.\nRules for thee but not for me.</p>\n</aside>\n<p>A rational reaction to threats is to \u201cshell up\u201d and try to make your personal space safe.\nThis is increasingly difficult as the devices you buy often doesn\u2019t feel like <em>yours</em> anymore.\nFiles are moved to the cloud without your knowledge;\ncompanies are doing everything they can to prevent you from blocking the ads they\u2019re shoving in everywhere;\nand everything you do will soon be ingested by an LLM in order to present personalized slop to you\n(even your passwords and screenshots of any nasty porn habits you may have).</p>\n<p>While you can avoid most of this crap on computers (try Linux if you haven\u2019t) the situation on smartphones is much bleaker.\nApple has been blocking sideloading apps for years and Google will soon follow by <a href=\"https://news.ycombinator.com/item?id=45017028\">only allowing apps from verified developers to be installed on Android</a>,\npreventing you from installing what <em>you</em> want.</p>\n<p>(They claim it\u2019s \u201cfor security\u201d but it\u2019s obvious they\u2019re doing this to protect their income stream. Apple takes a ridiculous 30% cut from all sales in their walled garden\nand Google hates the ability to strip out their ads.)</p>\n<p>I like the idea of a \u201cdumb phone\u201d but I unfortunately need and want apps on my phone\n(I consider banking and authentication apps essential to surviving in the modern world, and sometimes you <em>must</em> run an Android or iOS app).\nA \u201cdegoogled\u201d Android-compatible operating system is the answer I see, with <a href=\"https://grapheneos.org/\">GrapheneOS</a> as the exceptional standout.</p>\n<p>The big dragons recognize this as for example Samsung <a href=\"https://sammyguru.com/breaking-samsung-removes-bootloader-unlocking-with-one-ui-8/\">removes bootloader unlocking</a> and the <a href=\"https://www.reddit.com/r/degoogle/comments/1mau7yl/eu_age_verification_app_to_ban_any_android_system/?share_id=iR05aexja3cz3w-ITsqz1\">EU age verification app may ban Android systems not licensed by Google</a>.\nIn true Streisand fashion this only makes me more motivated to fight back.</p>\n<section id=\"GrapheneOS-works-and-its-convenient\">\n<h2><a class=\"heading-ref\" href=\"https://www.jonashietala.se/#GrapheneOS-works-and-its-convenient\">GrapheneOS works and it\u2019s convenient</a></h2>\n<figure><img alt=\"\" src=\"https://www.jonashietala.se/images/grapheneos/tablet.jpg\" />\n<figcaption>Installing <a href=\"https://grapheneos.org/\">GrapheneOS</a> on the Pixel Tablet.\n</figcaption></figure>\n<p>When people talk about <a href=\"https://grapheneos.org/\">GrapheneOS</a> they will understandably focus on the privacy and security aspect.\nI\u2019ll go into it a <a href=\"https://www.jonashietala.se/#The-best-privacy-security-in-a-modern-phone\">later</a>\nbut I think it\u2019s important to first dispel the idea that <a href=\"https://grapheneos.org/\">GrapheneOS</a> is only for the hardcore tech savvy user or that you\u2019ll have to sacrifice a lot of functionality.</p>\n<p>While <a href=\"https://grapheneos.org/\">GrapheneOS</a> isn\u2019t quite as simple as stock Android (I had to tweak a few settings to get some apps working)\nthe experience has been very smooth.</p>\n<section id=\"Installation\">\n<h3><a class=\"heading-ref\" href=\"https://www.jonashietala.se/#Installation\">Installation</a></h3>\n<p>The installation was straightforward and worked without a hitch:</p>\n<ol type=\"a\">\n<li>\nPlug in the phone or tablet via an USB\n</li>\n<li>\nLaunch Chrome (or chromium)\n</li>\n<li>\nFollow the instructions and click some buttons\n</li>\n</ol>\n</section>\n<section id=\"App-compatibility\">\n<h3><a class=\"heading-ref\" href=\"https://www.jonashietala.se/#App-compatibility\">App compatibility</a></h3>\n<figure class=\"flex-50\">\n<a href=\"https://www.jonashietala.se/images/grapheneos/Kvaesitso.png\"><img alt=\"\" src=\"https://www.jonashietala.se/images/grapheneos/Kvaesitso.png\" /></a>\n<a href=\"https://www.jonashietala.se/images/grapheneos/more_apps.png\"><img alt=\"\" src=\"https://www.jonashietala.se/images/grapheneos/more_apps.png\" /></a>\n<figcaption>\n<p>Some of the apps I\u2019ve got installed.\nI use the <a href=\"https://github.com/MM2-0/Kvaesitso\">Kvaesitso</a> launcher.</p>\n</figcaption>\n</figure>\n<p>Apps are also just as easy to install as on stock Android.\nI\u2019ve installed most of the apps from the Play Store just as I would\u2019ve on stock and they work just fine.</p>\n<p>So far I\u2019ve had a grand total of two issues with any apps I\u2019ve tried:</p>\n<ol>\n<li>\n<p>At first I couldn\u2019t copy BankID over from my old phone.</p>\n<p>I had to tweak some permissions and disable some of the location privacy features of <a href=\"https://grapheneos.org/\">GrapheneOS</a> before the phones recognized each other.\nPresumably there\u2019s some security measure there so that you can only copy it if the phones are nearby.</p>\n<figure class=\"flex-50\">\n<a href=\"https://www.jonashietala.se/images/grapheneos/location.png\"><img alt=\"\" src=\"https://www.jonashietala.se/images/grapheneos/location.png\" /></a>\n<a href=\"https://www.jonashietala.se/images/grapheneos/location_services.png\"><img alt=\"\" src=\"https://www.jonashietala.se/images/grapheneos/location_services.png\" /></a>\n<figcaption>\n<p>I had to play with some of these location settings to be able to copy BankID from my old phone to the new one.</p>\n</figcaption>\n</figure>\n</li>\n<li>\n<p>The AI detection feature of <a href=\"https://macrofactorapp.com/\">MacroFactor</a> refused to work.</p>\n<p><a href=\"https://macrofactorapp.com/\">MacroFactor</a> is a food tracking app where you can take a photo and it\u2019ll try to infer the food from the picture but uploading the photo simply failed.\nIt\u2019s a pretty cool feature and I instead use the app <a href=\"https://cronometer.com/index.html\">Cronometer</a> that has the same functionality.</p>\n<p><a href=\"https://macrofactorapp.com/\">MacroFactor</a> uses Play Integrity which may in some cases break certain apps.\nI\u2019ve got a few other apps that also uses Play Integrity but they don\u2019t have any issues.</p>\n</li>\n</ol>\n<p>I was worried that I\u2019d run into issues with the banking apps as I know there are some banking apps that have issues, but all the Swedish banking apps I\u2019ve tried work well.</p>\n<p>You can of course install apps from other sources such as <a href=\"https://f-droid.org/\">F-Droid</a>, <a href=\"https://accrescent.app/\">Accrescent</a>, <a href=\"https://obtainium.imranr.dev/\">Obtanium</a>, or manually as well.</p>\n<aside class=\"note\">\n<p>I had a lot more compatibility issues on <a href=\"https://calyxos.org/\">CalyxOS</a> on my old phone than I\u2019ve had with <a href=\"https://grapheneos.org/\">GrapheneOS</a>.\nThe sandboxed Google Play is an obviously superior solution than the MicroG/Aurora setup on <a href=\"https://calyxos.org/\">CalyxOS</a>, and not just for privacy reasons.</p>\n</aside>\n</section>\n<section id=\"No-bloatware\">\n<h3><a class=\"heading-ref\" href=\"https://www.jonashietala.se/#No-bloatware\">No bloatware</a></h3>\n<p>Finally, I love that there\u2019s no bloatware bullshit on <a href=\"https://grapheneos.org/\">GrapheneOS</a>.\nThere are no shitty vendor specific apps that you cannot uninstall and it isn\u2019t trying to trick you into installing stupid games via dark patterns.\nThe downside is that <a href=\"https://grapheneos.org/\">GrapheneOS</a> doesn\u2019t come with a lot of customization and lets you install apps for that yourself.</p>\n<p>In short, it feels like with <a href=\"https://grapheneos.org/\">GrapheneOS</a> you\u2019re in control, not some mega corporation.</p>\n</section>\n</section>\n<section id=\"The-best-privacy-security-in-a-modern-phone\">\n<h2><a class=\"heading-ref\" href=\"https://www.jonashietala.se/#The-best-privacy-security-in-a-modern-phone\">The best privacy &amp; security in a modern phone</a></h2>\n<p>While other Android compatible distributions such as <a href=\"https://calyxos.org/\">CalyxOS</a> and <a href=\"https://www.lineageos.org/\">LineageOS</a> all mention privacy and security as benefits they\u2019re nothing like <a href=\"https://grapheneos.org/\">GrapheneOS</a>.\nIn some cases\u2014as with <a href=\"https://calyxos.org/\">CalyxOS</a> where security updates have seen <em>significant</em> delays\u2014they may provide even less security compared to stock Android.</p>\n<aside class=\"warn\">\n<p><a href=\"https://calyxos.org/\">CalyxOS</a> <a href=\"https://calyxos.org/news/2025/08/01/a-letter-to-our-community/\">recently announced</a> that updates will be paused for 4\u20136 months and they recommend you to uninstall the OS.\nSee <a href=\"https://discuss.grapheneos.org/d/24791-departure-of-calyx-calyxos-leadership-and-discontinuation-of-calyxos-updates\">more discussion on the GrapheneOS forum</a>.</p>\n</aside>\n<p>Everything I\u2019ve read suggests that <a href=\"https://grapheneos.org/\">GrapheneOS</a> takes security and privacy very seriously.\nI feel that sometimes they may take a too extreme stance but I can respect that, despite being overkill for the threat levels I care about.</p>\n<p>See for example <a href=\"https://eylenburg.github.io/android_comparison.htm\">this comparison of Android-based Operating Systems</a>.</p>\n<p>What about Apple then?\nAren\u2019t they great at privacy and security?\nWhile I\u2019m sure they\u2019ll respect your privacy more than Google,\nI just can\u2019t trust a company that <a href=\"https://daringfireball.net/2025/06/more_on_apples_trust-eroding_f1_the_movie_wallet_ad\">shoves ads into their wallet app</a>.</p>\n</section>\n<section id=\"Pixel-devices-the-big-drawback-with-GrapheneOS\">\n<h2><a class=\"heading-ref\" href=\"https://www.jonashietala.se/#Pixel-devices-the-big-drawback-with-GrapheneOS\">Pixel devices, the big drawback with GrapheneOS</a></h2>\n<p>It would\u2019ve been great to have more choices.\nIf I only looked at hardware I might have gotten the <a href=\"https://shop.fairphone.com/fairphone-5\">Fairphone 5</a> as I like the idea of repairability, ethically sourced components, and a phone made in the EU.\nI also like the idea of a smaller phone and a Flip phone would\u2019ve been great.\nOr maybe a really cheap phone (or tablet) as I don\u2019t care that much about performance and could save some money.</p>\n<p>But alas, <a href=\"https://grapheneos.org/\">GrapheneOS</a> only support Pixel devices (for now).\nThere\u2019s a handful of phones to choose from but only a single tablet.</p>\n<p>I guess the best way to degoogle right now is to buy from Google\u2026\nSo I got the Pixel 9a for myself and the Pixel Tablet for our family.\n(Admittedly, they\u2019ve been pretty great.)</p>\n</section>\n<section id=\"Software-is-more-important-than-hardware\">\n<h2><a class=\"heading-ref\" href=\"https://www.jonashietala.se/#Software-is-more-important-than-hardware\">Software is more important than hardware</a></h2>\n<p>I\u2019ve had an interesting shift when I evaluate mobile devices;\ninstead of comparing phones primarily by hardware I prioritize the software on the phone.</p>\n<p>Today, more than ever, hardware upgrades in new phones provide diminishing returns\nfor ever increasing prices.\nThere\u2019s little practical difference between a new phone and a phone from a few years ago\nand savvy people can save a lot of money by simply avoiding the constant stream of new releases.</p>\n<p>Before I bought the Pixel 9a I used the <a href=\"https://shop.fairphone.com/fairphone-4\">Fairphone 4</a> for almost 4 years, and it was performing just fine!\nIf I hadn\u2019t gotten the urge of trying out <a href=\"https://grapheneos.org/\">GrapheneOS</a> I would\u2019ve still be happy with the Fairphone hardware (which was a bit underpowered already at release).</p>\n<p>Software on the other hand is more important than ever and for me it\u2019s what makes or breaks a phone today.</p>\n<p>The right software will protect your privacy and help keep your device secure,\nwhile the wrong software will fill your phone with uninstallable bloatware and cripple performance after system updates (if they deign to provide them).</p>\n<p>For example, I\u2019ve used a Galaxy Tab A7 Lite as a dashboard for my smart home for a while and it worked great.\nThen I installed an update and it suddenly became extremely slow, so slow that you barely could interact with the UI without punching the damn device.\nEven though the hardware is great, Samsung crippled the device for no reason.</p>\n<aside class=\"warn\">\n<p>Punching the device did not in fact help speed it up.\nShocking.</p>\n</aside>\n<p>With the right software your device works for you, not against you.\nIt\u2019s not a lot to ask for, yet in the modern day that\u2019s very rare indeed, and it\u2019s why I\u2019ll only be buying mobile devices supported by <a href=\"https://grapheneos.org/\">GrapheneOS</a> for the foreseeable future.</p>\n</section>"
            ],
            "link": "https://www.jonashietala.se/blog/2025/08/28/ill_only_buy_devices_with_grapheneos",
            "publishedAt": "2025-08-28",
            "source": "Jonas Hietala",
            "summary": "<div class=\"epigraph\"> <blockquote> <p>Cops say criminals use a Google Pixel with GrapheneOS \u2014 I say that\u2019s freedom </p> <footer><span class=\"author\"><a href=\"https://www.androidauthority.com/why-i-use-grapheneos-on-pixel-3575477/\">Calvin Wankhede</a> </span></footer> </blockquote> </div> <p>We\u2019re in a dark time period right now.</p> <p>Authoritarianism is on the rise throughout the globe. Governments wants to monitor your social media accounts so they can make you disappear if you engage in wrongthink such as opposing wars or genocide. This is worsened by misguided laws like <a href=\"https://fightchatcontrol.eu/\">Chat control</a> that would mandate scanning of all digital communication, exposing any wrongthink in your \u201cprivate messages\u201d.</p> <aside class=\"note\"> <p>Silly me, <a href=\"https://fightchatcontrol.eu/\">Chat control</a> doesn\u2019t mandate scanning of <em>all</em> digital communication; politicians and their families are, predictably, exempt. Rules for thee but not for me.</p> </aside> <p>A rational reaction to threats is to \u201cshell up\u201d and try to make your personal space safe. This is increasingly difficult as the devices you buy often doesn\u2019t feel like <em>yours</em> anymore. Files are moved to the cloud without your knowledge; companies are doing everything they can to prevent you from blocking the ads they\u2019re shoving in everywhere; and everything you do will soon be ingested by an LLM in order to present personalized slop to you (even your passwords",
            "title": "I'll only buy devices with GrapheneOS"
        },
        {
            "content": [
                "<p>\n          <a href=\"https://www.astralcodexten.com/p/the-economics-of-envy\">\n              Read more\n          </a>\n      </p>"
            ],
            "link": "https://www.astralcodexten.com/p/the-economics-of-envy",
            "publishedAt": "2025-08-28",
            "source": "SlateStarCodex",
            "summary": "<p> <a href=\"https://www.astralcodexten.com/p/the-economics-of-envy\"> Read more </a> </p>",
            "title": "The Economics Of Envy"
        },
        {
            "content": [
                "<p>\n          <a href=\"https://www.astralcodexten.com/p/open-thread-3965\">\n              Read more\n          </a>\n      </p>"
            ],
            "link": "https://www.astralcodexten.com/p/open-thread-3965",
            "publishedAt": "2025-08-28",
            "source": "SlateStarCodex",
            "summary": "<p> <a href=\"https://www.astralcodexten.com/p/open-thread-3965\"> Read more </a> </p>",
            "title": "Open Thread 396.5"
        },
        {
            "content": [
                "<p>Once again we\u2019ve reached the point where the weekly update needs to be split in two. Thus, the alignment and policy coverage will happen tomorrow. Today covers the rest.</p>\n<p>The secret big announcement this week was Claude for Chrome. This is a huge deal. It will be rolling out slowly. When I have access or otherwise know more, so will you.</p>\n<p>The obvious big announcement was Gemini Flash 2.5 Image. Everyone agrees this is now the clear best image editor available. It is solid as an image generator, but only as one among many on that front. Editing abilities, including its ability to use all its embedded world knowledge, seem super cool.</p>\n<div>\n\n\n<span id=\"more-24680\"></span>\n\n\n</div>\n<p>The third big story was the suicide of Adam Raine, which appears to have been enabled in great detail by ChatGPT. His parents are suing OpenAI and the initial facts very much do not look good and it seems clear OpenAI screwed up. The question is, how severe should and will the consequences be?</p>\n\n\n<h4 class=\"wp-block-heading\">Table of Contents</h4>\n\n\n<ol>\n<li><a href=\"https://thezvi.substack.com/i/171566560/language-models-offer-mundane-utility\">Language Models Offer Mundane Utility.</a> Find what you\u2019re looking for.</li>\n<li><a href=\"https://thezvi.substack.com/i/171566560/language-models-don-t-offer-mundane-utility\">Language Models Don\u2019t Offer Mundane Utility.</a> You weren\u2019t using them.</li>\n<li><a href=\"https://thezvi.substack.com/i/171566560/huh-upgrades\">Huh, Upgrades.</a> OpenAI Codex adds features including an IDE extension.</li>\n<li><a href=\"https://thezvi.substack.com/i/171566560/fun-with-image-generation\"><strong>Fun With Image Generation.</strong></a> Gemini 2.5 Flash Image is a great editor.</li>\n<li><a href=\"https://thezvi.substack.com/i/171566560/on-your-marks\">On Your Marks.</a> VendingBench, water use and some more v3.1 results.</li>\n<li><a href=\"https://thezvi.substack.com/i/171566560/water-water-everywhere\">Water Water Everywhere.</a> There\u2019s plenty left to drink.</li>\n<li><a href=\"https://thezvi.substack.com/i/171566560/get-my-agent-on-the-line\"><strong>Get My Agent On The Line</strong>.</a> Claude for Chrome. It\u2019s coming.</li>\n<li><a href=\"https://thezvi.substack.com/i/171566560/choose-your-fighter\">Choose Your Fighter.</a> Some advocates for GPT-5\u2019s usefulness.</li>\n<li><a href=\"https://thezvi.substack.com/i/171566560/deepfaketown-and-botpocalypse-soon\">Deepfaketown and Botpocalypse Soon.</a> Elon has something to share.</li>\n<li><a href=\"https://thezvi.substack.com/i/171566560/you-drive-me-crazy\">You Drive Me Crazy.</a> AI psychosis continues not to show up in numbers.</li>\n<li><a href=\"https://thezvi.substack.com/i/171566560/the-worst-tragedy-so-far\"><strong>The Worst Tragedy So Far.</strong></a> Adam Raine commits suicide, parents sue OpenAI.</li>\n<li><a href=\"https://thezvi.substack.com/i/171566560/unprompted-attention\">Unprompted Attention.</a> I don\u2019t see the issue.</li>\n<li><a href=\"https://thezvi.substack.com/i/171566560/copyright-confrontation\">Copyright Confrontation.</a> Bartz v. Anthropic has been settled.</li>\n<li><a href=\"https://thezvi.substack.com/i/171566560/the-art-of-the-jailbreak\">The Art of the Jailbreak.</a> Little Johnny Tables is all grown up.</li>\n<li><a href=\"https://thezvi.substack.com/i/171566560/get-involved\">Get Involved.</a> 40 ways to get involved in AI policy.</li>\n<li><a href=\"https://thezvi.substack.com/i/171566560/introducing\">Introducing.</a> Anthropic advisors aplenty, Pixel translates live phone calls.</li>\n<li><a href=\"https://thezvi.substack.com/i/171566560/in-other-ai-news\">In Other AI News.</a> Meta licenses MidJourney, Apple explores Gemini and more.</li>\n<li><a href=\"https://thezvi.substack.com/i/171566560/show-me-the-money\">Show Me the Money.</a> Why raise money when you can raise even more money?</li>\n<li><a href=\"https://thezvi.substack.com/i/171566560/quiet-speculations\">Quiet Speculations.</a> Everything is being recorded.</li>\n<li><a href=\"https://thezvi.substack.com/i/171566560/rhetorical-innovation\">Rhetorical Innovation.</a> The real math does not exist.</li>\n<li><a href=\"https://thezvi.substack.com/i/171566560/the-week-in-audio\">The Week in Audio.</a> How to properly use Claude Code.</li>\n</ol>\n\n\n<h4 class=\"wp-block-heading\">Language Models Offer Mundane Utility</h4>\n\n\n<p><a href=\"https://x.com/paularambles/status/1960762742266306717\">Find me that book</a>.</p>\n<div>\n<figure>\n<div>\n\n\n<figure class=\"wp-block-image\"><img alt=\"\" src=\"https://substackcdn.com/image/fetch/$s_!yoth!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F931889d7-6466-489c-a470-1cb48f7fd1e0_590x671.png\" /></figure>\n\n\n<div></div>\n</div>\n</figure>\n</div>\n<p>Or anything else. Very handy.</p>\n<p><a href=\"https://x.com/gregorschub/status/1958281687303098644\">Share of papers that engage with AI rises dramatically essentially everywhere</a>, <a href=\"https://t.co/JoXQj9P22W\">which is what you would expect</a>. There\u2019s quite a lot more to engage with and to say. Always watch the y-axis scale, yes these start at zero:</p>\n<div>\n<figure>\n<div>\n\n\n<figure class=\"wp-block-image\"><img alt=\"\" src=\"https://substackcdn.com/image/fetch/$s_!1w57!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffbc3b701-47e5-4047-9d71-0fb852e781ca_472x706.png\" /></figure>\n\n\n<div></div>\n</div>\n</figure>\n</div>\n<p><a href=\"https://www.tylercosgrove.com/blog/llm-music-taste/\">More detail on various LLMs and their musical taste</a>, based on a bracket competition among the top 5000 musical artists by popularity. It all seems bizarre. For example, Gemini 2.5 Pro\u2019s list looks highly and uniquely alphabetically biased without a strong bias towards numbers.</p>\n<div>\n<figure>\n<div>\n\n\n<figure class=\"wp-block-image\"><img alt=\"\" src=\"https://substackcdn.com/image/fetch/$s_!anFS!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F476c769c-40eb-4447-a788-cdec1051a0f1_2052x1308.png\" /></figure>\n\n\n<div></div>\n</div>\n</figure>\n</div>\n<p>The numbers-are-favored bias shows up only in OpenAI reasoning models including GPT-5, and in r1-0528. There are clear genre patterns, and there are some consistent picks, especially among Claudes. The three artists that appear three times are David Bowie, Prince and Stevie Wonder, which are very good picks. It definitely seems like the open models have worse (or more random) taste in correlated ways.</p>\n<p>Why bother thinking about your vibe coding?</p>\n<blockquote><p><a href=\"https://x.com/SullyOmarr/status/1960074618750091618\">Sully</a>: friend was hosting a mini ai workshop and he told me nearly all the vibe coders just have 1 giant coding session where the entire project is just being being thrown context. each request is ~200k tokens</p>\n<p>they&#8217;re not even bothering to break things up into some reasonable structure</p>\n<p>no wonder these code gen platforms are printing</p></blockquote>\n<p>I mean that makes sense. There\u2019s little reason to cheapen out on tokens when you think about token cost versus your time cost and the value of a good vibe code. You gotta boldly go where no one has gone before and risk it for the biscuit.</p>\n<p><a href=\"https://www.anthropic.com/news/anthropic-education-report-how-educators-use-claude\">Anthropic reports on how Claude is being used by educators</a>, in particular 74,000 anonymized conversations from higher education professionals in May and June.</p>\n<blockquote><p>Anthropic: The most prominent use of AI, as revealed by both our Claude.ai analysis and our qualitative research with Northeastern, was for curriculum development. Our Claude.ai analysis also surfaced academic research and assessing student performance as the second and third most common uses.</p>\n<p>\u2026</p>\n<p><strong>Tasks with higher augmentation tendencies:</strong></p></blockquote>\n<ul>\n<li>University teaching and classroom instruction, which includes creating educational materials and practice problems (77.4% augmentation);</li>\n<li>Writing grant proposals to secure external research funding (70.0% augmentation);</li>\n<li>Academic advising and student organization mentorship (67.5% augmentation);</li>\n<li>Supervising student academic work (66.9% augmentation).</li>\n</ul>\n<blockquote><p><strong>Tasks with relatively higher automation tendencies:</strong></p></blockquote>\n<ul>\n<li>Managing educational institution finances and fundraising (65.0% automation);</li>\n<li>Maintaining student records and evaluating academic performance (48.9% automation);</li>\n<li>Managing academic admissions and enrollment (44.7% automation).</li>\n</ul>\n<p>Mostly there are no surprises here, but concrete data is always welcome.</p>\n\n\n<h4 class=\"wp-block-heading\">Language Models Don\u2019t Offer Mundane Utility</h4>\n\n\n<p>As always, if you don\u2019t use AI, it can\u2019t help you. <a href=\"https://x.com/ryxcommar/status/1960317093363994817\">This includes when you never used AI in the first place</a>, but have to say \u2018AI is the heart of our platform\u2019 all the time because it sounds better to investors.</p>\n<p><a href=\"https://x.com/NateSilver538/status/1960149159652302854\">The ability to say \u2018I don\u2019t know</a>\u2019 and refer you elsewhere remains difficult for LLMs. Nate Silver observes this seeming to get even worse. For now it is on you to notice when the LLM doesn\u2019t know.</p>\n<p>This seems like a skill issue for those doing the fine tuning? It does not seem so difficult a behavior to elicit, if it was made a priority, via ordinary methods. At some point I hope and presume the labs will decide to care.</p>\n\n\n<h4 class=\"wp-block-heading\">Huh, Upgrades</h4>\n\n\n<p><a href=\"https://x.com/sama/status/1958922435249754382\">Feature request thread for ChatGPT power users</a>, <a href=\"https://x.com/sama/status/1958920331286180101\">also here</a>.</p>\n<p><a href=\"https://x.com/jefffhj/status/1959360923929575803\">The weights of Grok 2</a> <a href=\"https://huggingface.co/xai-org/grok-2\">have been released.</a></p>\n<p><a href=\"https://x.com/OpenAIDevs/status/1960809814596182163\">OpenAI Codex adds a new IDE extension</a>, a way to move tasks between cloud and local more easily, code reviews in GitHub and revamped Codex CLI.</p>\n<blockquote><p>OpenAI: Codex now runs in your IDE Available for VS Code, Cursor, and other forks, the new extension makes it easy to share context\u2014files, snippets, and diffs\u2014so you can work faster with Codex. It\u2019s been a top feature request, and we\u2019re excited to hear what you think!</p></blockquote>\n\n\n<h4 class=\"wp-block-heading\">Fun With Image Generation</h4>\n\n\n<blockquote><p><a href=\"https://x.com/googleaidevs/status/1960344982264701205\">Google</a>: Introducing Gemini 2.5 Flash Image, our state-of-the-art image generation and editing model designed to help you build more dynamic and intelligent visual applications.</p>\n<p><img alt=\"\ud83c\udf4c\" class=\"wp-smiley\" src=\"https://s0.wp.com/wp-content/mu-plugins/wpcom-smileys/twemoji/2/72x72/1f34c.png\" style=\"height: 1em;\" />Available in preview in @googleaistudio and the Gemini API.</p>\n<p><a href=\"https://developers.googleblog.com/en/introducing-gemini-2-5-flash-image/\">This model is available right now</a> via the <a href=\"https://ai.google.dev/gemini-api/docs/image-generation\">Gemini API</a> and <a href=\"https://aistudio.google.com/prompts/new_chat?model=gemini-2.5-flash-preview-image\">Google AI Studio</a> for developers and <a href=\"https://console.cloud.google.com/vertex-ai/studio/multimodal?model=gemini-2.5-flash-image-preview\">Vertex AI</a> for enterprise. Gemini 2.5 Flash Image is priced at $30.00 per 1 million output tokens with each image being 1290 output tokens ($0.039 per image). All other modalities on input and output follow Gemini 2.5 Flash <a href=\"https://ai.google.dev/gemini-api/docs/pricing\">pricing</a>.</p>\n<p>Josh Woodward (Google): The @GeminiApp now has the #1 image model in the world, give it a go!</p>\n<p>Attach an image, describe your edits, and it&#8217;s done. I&#8217;ve never seen anything like this.</p></blockquote>\n<p>They pitch that it maintains character consistency, adheres to visual templates, does prompt based image editing, <a href=\"https://x.com/BenjaminDEKR/status/1960566924884029539\">understands point of view</a> <a href=\"https://x.com/Prashant_1722/status/1960385307498434878\">and reflections</a>, <a href=\"https://x.com/ai_for_success/status/1960652748975636850\">restores old photographs,</a> <a href=\"https://x.com/HarshithLucky3/status/1960675664195149926\">makes 3-D models</a>, has native world knowledge and offers multi-image function.</p>\n<p><a href=\"https://x.com/teortaxesTex/status/1960784510368539125\">By all accounts Gemini 2.5 Flash Image is a very very good image editor</a>, while being one good image generator among many.</p>\n<p>You can do things like <a href=\"https://x.com/leslysandra/status/1960458016391565553\">repaint objects,</a> <a href=\"https://x.com/ClankerT800/status/1960403099627217269\">create drawings</a>, <a href=\"https://x.com/HarshithLucky3/status/1960594094025044330\">see buildings from a given point of view</a>, <a href=\"https://x.com/yachimat_manga/status/1960471174758195494\">put characters into combat</a> and so on.</p>\n<div>\n<figure>\n<div>\n\n\n<figure class=\"wp-block-image\"><img alt=\"\" src=\"https://substackcdn.com/image/fetch/$s_!Iixo!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fec931582-dbe9-44ff-900c-e3513f73f8df_960x1280.jpeg\" /></figure>\n\n\n<div></div>\n</div>\n</figure>\n</div>\n<p><a href=\"https://x.com/yachimat_manga/status/1960537842775679455\">Which then becomes a short video here.</a></p>\n<p>Our standards are getting high, <a href=\"https://x.com/papayathreesome/status/1960761246703350021\">such as this report that you can\u2019t play Zelda</a>.</p>\n<p><a href=\"https://x.com/elder_plinius/status/1960436133952913799\">Yes, of course Pliny jailbroke it (at least as far as being topless) on the spot</a>.</p>\n<p>We\u2019re seeing some cool examples, but they are also clearly selected.</p>\n<blockquote><p><a href=\"https://x.com/BenjaminDEKR/status/1960541507586236613\">Benjamin De Kraker</a>: Okay this is amazing.</p>\n<p>All human knowledge will be one unified AI multimodal model.</p>\n<p>Bilawal Sidhu: Since nano banana has gemini&#8217;s world knowledge, you can just upload screenshots of the real world and ask it to annotate stuff for you. &#8220;you are a location-based AR experience generator. highlight [point of interest] in this image and annotate relevant information about it.&#8221;</p>\n<div>\n<figure>\n<div>\n\n\n<figure class=\"wp-block-image\"><img alt=\"\" src=\"https://substackcdn.com/image/fetch/$s_!45SN!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe18a13ef-76d6-4aa0-864b-0717dafed688_1008x580.png\" /></figure>\n\n\n<div></div>\n</div>\n</figure>\n</div>\n<p>&nbsp;</p>\n<div>\n<figure>\n<div>\n\n\n<figure class=\"wp-block-image\"><img alt=\"\" src=\"https://substackcdn.com/image/fetch/$s_!McDe!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F14b70ee5-aab4-453a-94ff-a9a73819aeb4_1152x896.jpeg\" /></figure>\n\n\n<div></div>\n</div>\n</figure>\n</div>\n</blockquote>\n<p>That seems cool if you can make it fast enough, and if it works on typical things rather than only on obvious landmarks?</p>\n<p>The right question in the long term is usually: Can the horse talk at all?</p>\n<blockquote><p><a href=\"https://x.com/_everythingism/status/1960085022469247438\">Everythingism</a>: I asked &#8220;Nano Banana&#8221; [which we later learned is Gemini Flash 2.5] to label a map of the USA and then a map of the world&#8230;this was the result.</p>\n<p>It&#8217;s impressive at many tasks but image models all seem to fail when there are too many objects or too many things to label.</p>\n<div>\n<figure>\n<div>\n\n\n<figure class=\"wp-block-image\"><img alt=\"\" src=\"https://substackcdn.com/image/fetch/$s_!5AUQ!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F434e8e12-a63a-4e6b-8700-ca7ff704b7ba_1199x627.jpeg\" /></figure>\n\n\n<div></div>\n</div>\n</figure>\n</div>\n<div>\n<figure>\n<div>\n\n\n<figure class=\"wp-block-image\"><img alt=\"\" src=\"https://substackcdn.com/image/fetch/$s_!4XGa!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F256834c6-e74a-463f-91ce-aa26dff60cd8_900x600.jpeg\" /></figure>\n\n\n<div></div>\n</div>\n</figure>\n</div>\n<p>Explode Meow: Many of my friends have tested it.</p>\n<p>To be fair, [Gemini Flash 2.5] can make quite realistic images, and most of them are indistinguishable from real ones if I don&#8217;t look closely.</p>\n<p>This is clearly a result of Google leveraging its overwhelming data resources (Google Cloud).</p>\n<p>But after multiple rounds of testing by my friends, they noticed that it actually makes some Low-level mistakes (hallucinations), just like GPT-4o (even Stable Diffusion).</p></blockquote>\n<p>Are mistakes still being made? Absolutely. This is still rather impressive. Consider where image models were not too long ago.</p>\n<p>This is a Google image model, so the obvious reason for skepticism is that we all expect the Fun Police.</p>\n<blockquote><p>Hasan Can: If I know Google, they\u2019ll nerf this model like crazy under the excuse of \u201csafety\u201d and when it\u2019s released, it\u2019ll turn into something worse than Qwen-Image-Edit. Remember what happened with Gemini 2.0 Flash Image Gen. I hope I\u2019m wrong, but I don\u2019t think so.</p>\n<p>Alright, it seems reverse psychology is paying off. <img alt=\"\ud83d\udc4d\" class=\"wp-smiley\" src=\"https://s0.wp.com/wp-content/mu-plugins/wpcom-smileys/twemoji/2/72x72/1f44d.png\" style=\"height: 1em;\" /></p>\n<p>Image generation in Gemini 2.5 Flash doesn&#8217;t appear to be nerfed at all. It looks like Google is finally ready to treat both its developers and end users like adults.</p>\n<p><a href=\"https://x.com/intellectronica/status/1960765755659542860\">Eleanor Berger</a>: It&#8217;s very good, but I&#8217;m finding it very challenging to to bump into their oversensitive censorship. It really likes saying no.</p>\n<p>nothing with real people (which sucks, because of course I want to modify some selfies), anything that suggests recognisable brands, anything you wouldn&#8217;t see on terrestrial tv.</p></blockquote>\n<p>The continuing to have a stick up the ass about picturing \u2018real people\u2019 is extremely frustrating and I think reduces the usefulness of the model substantially. The other censorship also does not help matters.</p>\n\n\n<h4 class=\"wp-block-heading\">On Your Marks</h4>\n\n\n<p><a href=\"https://x.com/Prashant_1722/status/1958456967380251026\">Grok 4 sets a new standard in Vending Bench</a>,</p>\n<div>\n<figure>\n<div>\n\n\n<figure class=\"wp-block-image\"><img alt=\"\" src=\"https://substackcdn.com/image/fetch/$s_!8i5t!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4dc81812-c2f3-4f9e-8254-9633eb7ebdec_1200x590.jpeg\" /></figure>\n\n\n<div></div>\n</div>\n</figure>\n</div>\n<p>The most surprising result here is probably that the human did so poorly.</p>\n<p>I like saying an AI query is similar to nine seconds of television. Makes things clear.</p>\n<div>\n<figure>\n<div>\n\n\n<figure class=\"wp-block-image\"><img alt=\"\" src=\"https://substackcdn.com/image/fetch/$s_!I8OU!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fae7f727f-5743-4143-8a59-9c8d7043477f_1876x1052.jpeg\" /></figure>\n\n\n<div></div>\n</div>\n</figure>\n</div>\n<p>&nbsp;</p>\n<p>It also seems important to notice when in a year energy costs drop 95%+?</p>\n<div>\n<figure>\n<div>\n\n\n<figure class=\"wp-block-image\"><img alt=\"\" src=\"https://substackcdn.com/image/fetch/$s_!fXsQ!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff2a7ab17-03ce-4e23-99bf-e1c0e2f71a0b_932x958.jpeg\" /></figure>\n\n\n<div></div>\n</div>\n</figure>\n</div>\n<p><a href=\"https://x.com/LechMazur/status/1958970478712037548\">DeepSeek v3.1 improves on R1 on NYT Connections, 49% \u2192 58%</a>. Pretty solid.</p>\n<p><a href=\"https://x.com/teortaxesTex/status/1959641692107149805\">DeepSeek v3.1 scores solidly on this coding eval when using Claude Code</a>, does less well on other scaffolds, with noise and confusion all around.</p>\n<p>AIs potentially \u2018sandbagging\u2019 tests is an increasing area of research and concern. <a href=\"https://x.com/StephenLCasper/status/1959571395744281058\">Cas says</a> this is simply a special case of failure to elicit full capabilities of a system, and doing so via fine-tuning is \u2018solved problem\u2019 so we can stop worrying.</p>\n<p>This seems very wrong to me. Right now failure to do proper elicitation, mostly via unhobbling and offering better tools and setups, is the far bigger problem. But sandbagging will be an increasing and increasingly dangerous future concern, and a \u2018deliberate\u2019 sandbagging has very different characteristics and implications than normal elicitation failure. I find \u2018sandbagging\u2019 to be exactly the correct name for this, since it doesn\u2019t confine itself purely to evals, unless you want to call everything humans do to mislead other humans \u2018eval gaming\u2019 or \u2018failure of capability elicitation\u2019 or something. And no, this is not solved even now, even if it was true that it could currently be remedied by a little fine-tuning, because you don\u2019t know when and how to do the fine-tuning.</p>\n<p>Report that DeepSeek v3.1 will occasionally <a href=\"https://x.com/jiqizhixin/status/1960183126757388336\">insert the token \u2018extreme\u2019 where it doesn\u2019t belong</a>, including sometimes breaking things like code or JSON. Data contamination is suspected as the cause.</p>\n<p><a href=\"https://x.com/peterwildeford/status/1959750315726471352\">Similarly, when Peter Wildeford says</a> \u2018sandbagging is mainly coming from AI developers not doing enough to elicit top behavior,\u2019 that has the risk of conflating the levels of intentionality. Mostly AI developers want to score highly on evals, but there is risk that they deliberately do sandbag the safety testing, as in decide not to try very hard to elicit top behavior there because they\u2019d rather get less capable test results.</p>\n\n\n<h4 class=\"wp-block-heading\">Water Water Everywhere</h4>\n\n\n<p>The purpose of <a href=\"https://x.com/JeffDean/status/1958525015722434945\">environmental assessments of AI</a> is mostly to point out that many people have very silly beliefs about the environmental impact of AI.</p>\n<blockquote><p><a href=\"https://x.com/JeffDean/status/1958525015722434945\">Jeff Dean</a>: AI efficiency is important. <a href=\"https://t.co/CoMm5gV9SR\">Today, Google is sharing a technical paper </a>detailing our comprehensive methodology for measuring the environmental impact of Gemini inference. We estimate that the median Gemini Apps text prompt uses 0.24 watt-hours of energy (equivalent to watching an average TV for ~nine seconds), and consumes 0.26 milliliters of water (about five drops) \u2014 figures that are substantially lower than many public estimates.</p>\n<p>At the same time, our AI systems are becoming more efficient through research innovations and software and hardware efficiency improvements. From May 2024 to May 2025, the energy footprint of the median Gemini Apps text prompt dropped by 33x, and the total carbon footprint dropped by 44x, through a combination of model efficiency improvements, machine utilization improvements and additional clean energy procurement, all while delivering higher quality responses.</p></blockquote>\n<p>Alas Google\u2019s water analysis had an unfortunate oversight,<a href=\"https://x.com/AndyMasley/status/1959376893947298215\"> in that it did not include the water cost of electricity generation</a>. That turns out to be the main water cost, so much so that if you (reasonably) want to attribute the average cost of that electricity generation onto the data center, the best way to approximate water use of a data center is to measure the water cost of the electricity, then multiply by 1.1 or so.</p>\n<p>This results in the bizarre situation where:</p>\n<ol>\n<li>Google\u2019s water cost estimation was off by an order of magnitude.</li>\n<li>The actual water cost is still rather hard to distinguish from zero.</li>\n</ol>\n<blockquote><p>Andy Masley: Google publishes a paper showing that its AI models only use 0.26 mL of water in data centers per prompt.</p>\n<p>After, this article gets published: &#8220;Google says a typical AI prompt only uses 5 drops of water &#8211; experts say that&#8217;s misleading.&#8221;</p>\n<p>The reason the expert says this is misleading? They didn&#8217;t include the water used in the nearby power plant to generate electricity.</p>\n<p>The expert, Shaolei Ren says: \u201cThey\u2019re just hiding the critical information. This really spreads the wrong message to the world.\u201d</p>\n<p>Each prompt uses about 0.3 Wh in the data center. To generate that much electricity, power plants need (at most) 2.50 mL of water. That raises the total water cost per prompt to 2.76 mL.</p>\n<p>2.76 mL is 0.0001% of the average American lifestyle&#8217;s daily consumptive use of fresh water and groundwater. It&#8217;s nothing.</p>\n<p>Would you know this from the headline, or the quote? Why do so many reporters on this topic do this?</p></blockquote>\n<p>Andy Masley is right that This Is Nothing even at the limit, that the water use here is not worth worrying about even in worst case. It will not meaningfully increase your use of water, even when you increase Google\u2019s estimates by an order of magnitude.</p>\n<p>A reasonable headline would be \u2018<a href=\"https://x.com/peterwildeford/status/1959734690983895297\">Google say a typical text prompt uses 5 drops of water, but once you take electricity into account it\u2019s actually 32 drops</a>.\u2019</p>\n<p>I do think saying \u2018Google was being misleading\u2019 is reasonable here. You shouldn\u2019t have carte blanche to take a very good statistic and make it sound even better.</p>\n<p><a href=\"https://x.com/ShakeelHashim/status/1959757951041724569\">Teonbrus and Shakeel are right</a> that there is going to be increasing pressure on anyone who opposes AI for other reasons to instead rile people up about water use and amplify false and misleading claims. Resist this urge. Do not destroy yourself for nothing. It goes nowhere good, including because it wouldn\u2019t work.</p>\n\n\n<h4 class=\"wp-block-heading\">Get My Agent On The Line</h4>\n\n\n<p>It\u2019s coming. As in, Claude for Chrome.</p>\n<blockquote><p><a href=\"https://x.com/AnthropicAI/status/1960417002469908903\">Anthropic</a>: We\u2019ve developed Claude for Chrome, where Claude works directly in your browser and takes actions on your behalf.</p>\n<p>We\u2019re releasing it at first as a research preview to 1,000 users, so we can gather real-world insights on how it\u2019s used.</p>\n<p>Browser use brings several safety challenges\u2014most notably \u201cprompt injection\u201d, where malicious actors hide instructions to trick Claude into harmful actions.</p>\n<p>We already have safety measures in place, but this pilot will help us improve them.</p>\n<p>Max plan users can join the waitlist <a href=\"https://t.co/vd2KmxQJHp\">to test Claude for Chrome today</a>.</p></blockquote>\n<p>Do not say you were not warned.</p>\n<blockquote><p>Anthropic: Understand the risks.</p>\n<p>Claude brings AI directly to your browser, handling tasks and navigating sites for you. These new capabilities create risks bad actors may try to exploit.</p>\n<p>Malicious actors can hide instructions in websites, emails, and documents that trick AI into taking harmful actions without your knowledge, including:</p>\n<p>Accessing your accounts or files</p>\n<p>Sharing your private information</p>\n<p>Making purchases on your behalf</p>\n<p>Taking actions you never intended</p></blockquote>\n<p>Oh, those risks. Yeah.</p>\n<p>They offer some Good Advice about safety issues, which includes using a distinct browser profile that doesn\u2019t include credentials to any sensitive websites like banks:</p>\n<div>\n<figure>\n<div>\n\n\n<figure class=\"wp-block-image\"><img alt=\"\" src=\"https://substackcdn.com/image/fetch/$s_!kd_E!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F11783f91-14c6-453e-8d45-99518efce53c_1710x903.png\" /></figure>\n\n\n<div></div>\n</div>\n</figure>\n</div>\n<blockquote><p>Q: How do I control what Claude can access?</p>\n<p>A: You decide which websites Claude can visit and what actions it can take. Claude asks permission before visiting new sites and before taking potentially risky actions like publishing content or making purchases. You can revoke access to specific websites anytime in settings.</p>\n<p>For trusted workflows, you can choose to skip all permissions, but you should supervise Claude closely. While some safeguards exist for sensitive actions, malicious actors could still trick Claude into unintended actions.</p>\n<p><a href=\"https://support.anthropic.com/en/articles/12012173-getting-started-with-claude-for-chrome\">For your safety, Claude cannot access sensitive, high-risk sites such as</a>:</p>\n<p>Financial services and banking sites</p>\n<p>Investment and trading platforms</p>\n<p>Adult content websites</p>\n<p>Cryptocurrency exchanges</p>\n<p>It\u2019s unlikely that we\u2019ve captured all sites in these categories so please report if you find one we\u2019ve missed.</p>\n<p>Additionally, Claude is prohibited from:</p>\n<p>Engaging in stock trading or investment transactions</p>\n<p>Bypassing captchas</p>\n<p>Inputting sensitive data</p>\n<p>Gathering, scraping facial images</p>\n<p>We recommend:</p>\n<p>Use a separate browser profile without access to sensitive accounts (such as banking, healthcare, government).</p>\n<p>Review Claude&#8217;s proposed actions before approving them, especially on new websites.</p>\n<p>Start with simple tasks like research or form-filling rather than complex multi-step workflows.</p>\n<p>Make sure your prompts are specific and carefully tailored to avoid Claude doing things you didn\u2019t intend.</p></blockquote>\n<p>AI browsers from non-Anthropic sources? Oh, the safety you won\u2019t have.</p>\n<blockquote><p><a href=\"https://x.com/zack_overflow/status/1959308058200551721\">Zack</a>: Why is no one talking about this? This is why I don&#8217;t use an AI browser You can literally get prompt injected and your bank account drained by doomscrolling on reddit:</p>\n<div>\n<figure>\n<div>\n\n\n<figure class=\"wp-block-image\"><img alt=\"\" src=\"https://substackcdn.com/image/fetch/$s_!jTli!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F29b9a6d6-7930-4fc4-9fc6-9893a830f721_1200x750.jpeg\" /></figure>\n\n\n<div></div>\n</div>\n</figure>\n</div>\n<p>No one seems to be concerned about this, it seems to me like the #1 problem with any agentic AI stuff You can get pwned so easily, all an attacker has to do is literally write words down somewhere?</p>\n<p><a href=\"https://x.com/brave/status/1958152314914508893\">Brave</a>: AI agents that can browse the Web and perform tasks on your behalf have incredible potential but also introduce new security risks.</p>\n<p>We recently found, and disclosed, a concerning flaw in Perplexity&#8217;s Comet browser that put users&#8217; accounts and other sensitive info in danger.</p>\n<p>This security flaw stems from how Comet summarizes websites for users.</p>\n<p>When processing a site&#8217;s content, Comet can&#8217;t tell content on the website apart from legitimate instructions by the user. This means that the browser will follow commands hidden on the site by an attacker.</p>\n<p>These malicious instructions could be white text on a white background or HTML comments. Or they could be a social media post. If Comet sees the commands while summarizing, it will follow them even if they could hurt the user. This is an example of an indirect prompt injection.</p>\n<p>This was only an issue within Comet. Dia doesn\u2019t have the agentic capabilities that make this attack possible.</p></blockquote>\n<p>&nbsp;</p>\n<p>&nbsp;</p>\n\n\n<h4 class=\"wp-block-heading\">Choose Your Fighter</h4>\n\n\n<p>&nbsp;</p>\n<p>&nbsp;</p>\n<p>Here\u2019s someone very happy with OpenAI\u2019s Codex.</p>\n<blockquote><p><a href=\"https://x.com/VictorTaelin/status/1958543021324029980\">Victor Taelin</a>: BTW, I&#8217;ve basically stopped using Opus entirely and I now have several Codex tabs with GPT-5-high working on different tasks across the 3 codebases (HVM, Bend, Kolmo). Progress has never been so intense. My job now is basically passing well-specified tasks to Codex, and reviewing its outputs.</p>\n<p>OpenAI isn&#8217;t paying me and couldn&#8217;t care less about me. This model is just very good and the fact people can&#8217;t see it made me realize most of you are probably using chatbots as girlfriends or something other than assisting with complex coding tasks.</p>\n<p>(sorry Anthropic still love you guys <img alt=\"\ud83d\ude22\" class=\"wp-smiley\" src=\"https://s0.wp.com/wp-content/mu-plugins/wpcom-smileys/twemoji/2/72x72/1f622.png\" style=\"height: 1em;\" />)</p>\n<p>PS: I still use Opus for hole-filling in VIM because it is much faster than gpt-5-high there.</p></blockquote>\n<p><a href=\"https://www.nytimes.com/2025/08/24/opinion/chat-gpt5-open-ai-future.html\">Ezra Klein is impressed by GPT-5</a> as having crossed into offering a lot of mundane utility, and is thinking about what it means that others are not similarly impressed by this merely because it wasn\u2019t a giant leap over o3.</p>\n<blockquote><p>GFodor: Ezra proves he is capable of using a dropdown menu, a surprisingly rare skill.</p></blockquote>\n<p><a href=\"https://x.com/gallabytes/status/1960705524338712781\">A cool way to break down the distinction?</a> This feels right to me, in the sense that if I know exactly what I want and getting it seems nontrivial my instinct is now to reach for GPT-5-Thinking or Pro, if I don\u2019t know exactly what I want I go for Opus.</p>\n<blockquote><p>Sig Kitten: I can&#8217;t tell if I&#8217;m just claude brain rotted or Opus is really the only usable conversational AI for non-coding stuff</p>\n<p>Gallabytes: it&#8217;s not just you.</p>\n<p>gpt5 is a better workhorse but it does this awkward thing of trying really hard to find the instructions in your prompt and follow them instead of just talking.</p>\n<p>Sig Kitten: gpt-5 default is completely unusable imho just bullet points of nonsense after a long thinking for no reason.</p>\n<p>Gallabytes: it&#8217;s really good if you give it really precise instructions eg I have taken to dumping papers with this prompt then walking away for 5 minutes:</p>\n<p>what&#8217;s the headline result in this paper ie the most promising metric or qualitative improvement? what&#8217;s the method in this paper?</p>\n<p>1 sentence then 1 paragraph then detailed.</p></blockquote>\n\n\n<h4 class=\"wp-block-heading\">Deepfaketown and Botpocalypse Soon</h4>\n\n\n<p><a href=\"https://x.com/MrEwanMorrison/status/1959217416619069921\">Entirely fake Gen AI album claims to be from Emily Portman</a>.</p>\n<p><a href=\"https://x.com/elonmusk/status/1958499441469739329\">Did Ani tell you to say this, Elon</a>? Elon are you okay, are you okay Elon?</p>\n<blockquote><p>Elon Musk: Wait until you see Grok 5.</p>\n<p>I think it has a shot at being true AGI.</p>\n<p>Haven\u2019t felt that about anything before.</p></blockquote>\n<p>I notice I pattern match this to \u2018oh more meaningless hype, therefore very bad sign.\u2019</p>\n<p>Whereas I mean this seems to be what Elon is actually up to these days, sorry?</p>\n<div>\n<figure>\n<div>\n\n\n<figure class=\"wp-block-image\"><img alt=\"\" src=\"https://substackcdn.com/image/fetch/$s_!kOKA!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fddce3ff5-c7e3-416d-a175-a480b6ae669b_1041x1160.png\" /></figure>\n\n\n<div></div>\n</div>\n</figure>\n</div>\n<p>Or, alternatively, what does Elon think the \u2018G\u2019 stands for here, exactly?</p>\n<p>(The greeting in question, in a deep voice, is \u2018little f***ing b****.)</p>\n<p>Also, she might tell everyone what you talked about, you little f***ing b****, if you make the mistake of clicking the \u2018share\u2019 button, so think twice about doing that.</p>\n<blockquote><p><a href=\"https://www.forbes.com/sites/iainmartin/2025/08/20/elon-musks-xai-published-hundreds-of-thousands-of-grok-chatbot-conversations/?utm_medium=social&amp;utm_source=ForbesMainTwitter&amp;utm_campaign=socialflowForbesMainTwitter\">Forbes</a>: Elon Musk\u2019s AI firm, xAI, has published the chat transcripts of hundreds of thousands of conversations between its chatbot Grok and the bot\u2019s users \u2014 in many cases, without those users\u2019 knowledge or permission.</p>\n<p>xAI made people\u2019s conversations with its chatbot public and searchable on Google without warning &#8211; including a detailed plan for the assassination of Elon Musk and explicit instructions for making fentanyl and bombs.</p>\n<p><a href=\"https://x.com/peterwildeford/status/1959375259272097894\">Peter Wildeford</a>: I know xAI is more slapdash and so people have much lower expectations, but this still seems like a pretty notable breach of privacy that would get much more attention if it were from OpenAI, Anthropic, Google, or Meta.</p></blockquote>\n<p>I\u2019m not sure xAI did anything technically wrong here. The user clicked a \u2018share\u2019 button. I do think it is on xAI to warn the user if this means full Google indexing but it\u2019s not on the level of doing it with fully private chats.</p>\n<blockquote><p><a href=\"https://x.com/nearcyan/status/1958607376002793486\">Near</a>: why are you giving this app to children? (ages 12+)</p>\n<p>apparently i am the only person in the world who gives a shit about this and that is why Auren is 17+ despite not being NSFW and a poorly-prompted psychopathic liar.</p>\n<p>shattering the overton window has 2nd-order effects.</p></blockquote>\n<p>An ominous view of even the superficially glorious future?</p>\n<blockquote><p><a href=\"https://x.com/meaning_enjoyer/status/1960479678550172144\">Nihilism Disrespecter</a>: the highly cultured, trombone playing, shakespeare quoting officers of star trek were that way because they were the only ones to escape the vast, invisible holodeck hikikomori gooner caste that made up most of humanity.</p>\n<p>Roon: there does seem to be a recurrent subplot that the officers all spend time in the holodeck and have extensive holodeck fantasies and such. I mean literally none of them are married for some reason.</p>\n<p>Eneasz Brodski: canonically so according to the novelization of the first Trek movie, I believe.</p>\n<p>Henry Shevlin: Culture series does this pretty well. 99.9% of Culture citizens spend their days literally or metaphorically dicking around, it\u2019s only a small fraction of busybodies who get recruited to go interfere with alien elections.</p></blockquote>\n\n\n<h4 class=\"wp-block-heading\">You Drive Me Crazy</h4>\n\n\n<p><a href=\"https://stevenadler.substack.com/p/chatbot-psychosis-what-do-the-data?manualredirect=&amp;triedRedirect=true\">Steven Adler looks into the data on AI psychosis</a>.</p>\n<p>Is this statistically a big deal yet? As with previous such inquiries, so far the answer seems to be no. The UK statistics show a potential rise in mental health services use, but the data is noisy and the timing seems off, especially not lining up with GPT-4o\u2019s problems, and data from the USA doesn\u2019t show any increase.</p>\n<p>Scott Alexander does a more details, more Scott Alexander investigation and set of intuition pumps and explanations. <a href=\"https://www.astralcodexten.com/p/in-search-of-ai-psychosis?utm_source=post-email-title&amp;publication_id=89120&amp;post_id=171432278&amp;utm_campaign=email-post-title&amp;isFreemail=true&amp;r=67wny&amp;triedRedirect=true&amp;utm_medium=email\">Here\u2019s a classic ACX moment worth pondering</a>:</p>\n<blockquote><p>And partly it was because there are so many crazy beliefs in the world &#8211; spirits, crystal healing, moon landing denial, esoteric Hitlerism, whichever religions you don\u2019t believe in &#8211; that psychiatrists have instituted a blanket exemption for any widely held idea. If you think you\u2019re being attacked by demons, you\u2019re delusional, <em>unless</em> you\u2019re from some culture where lots of people get attacked by demons, in which case it\u2019s a religion and you\u2019re fine.</p>\n<p>\u2026</p>\n<p>Most people don\u2019t have world-models &#8211; they believe what their friends believe, or what has good epistemic vibes. In a large group, weird ideas can ricochet from person to person and get established even in healthy brains. In an Afro-Caribbean culture where all your friends get attacked by demons at voodoo church every Sunday, a belief in demon attacks can co-exist with otherwise being a totally functional individual.</p>\n<p>So is QAnon a religion? Awkward question, but it\u2019s non-psychotic by definition. Still, it\u2019s interesting, isn\u2019t it? If social media makes a thousand people believe the same crazy thing, it\u2019s not psychotic. If LLMs make a thousand people each believe a different crazy thing, that <em>is</em> psychotic. Is this a meaningful difference, or an accounting convention?</p>\n<p>Also, what if a thousand people believe something, but it\u2019s you and your 999 ChatGPT instances?</p></blockquote>\n<p>I like the framing that having a sycophantic AI to talk to moves people along a continuum of crackpotness towards psychosis, rather than a boolean where it either does or does not cause psychosis outright:</p>\n<blockquote><p>Maybe this is another place where we are forced to admit <a href=\"https://lorienpsych.com/2020/10/30/ontology-of-psychiatric-conditions-taxometrics/\">a spectrum model of psychiatric disorders</a> &#8211; there is an unbroken continuum from mildly sad to suicidally depressed, from social drinking to raging alcoholism, and from eccentric to floridly psychotic.</p></blockquote>\n<p>Another insight is that AI psychosis happens when moving along this spectrum causes further movement down the spectrum, as the AI reinforces your delusions, causing you to cause it to reinforce them more, and so on.</p>\n<p>Scott surveyed readership, I was one of the 4,156 responses.</p>\n<blockquote><p>The primary question was whether anyone \u201cclose to you\u201d &#8211; defined as your self, family, co-workers, or 100 closest friends &#8211; had shown signs of AI psychosis. 98.1% of people said no, 1.7% said yes.</p>\n<p>How do we translate this into a prevalence? Suppose that respondents had an average of fifty family members and co-workers, so that plus their 100 closest friends makes 150 people. Then the 4,156 respondents have 623,400 people who are \u201cclose\u201d. Among them, they reported 77 cases of AI psychosis in people close to them (a few people reported more than one case). 77/623,400 = 1/8,000. Since LLMs have only been popular for a year or so, I think this approximates a yearly incidence, and I rounded it off to my 1/10,000 guess above.</p></blockquote>\n<p>He says he expects sampling concerns to be a wash, which I\u2019m suspicious about. I\u2019d guess that this sample overrepresented psychosis somewhat. I\u2019m not sure this overrules the other consideration, which is that this only counts psychosis that the respondents knew about.</p>\n<p>Only 10% of these cases were full \u2018no previous risk factors and now totally psychotic.\u2019 Then again, that\u2019s actually a substantial percentage.</p>\n<p>Thus he ultimately finds that the incidence of AI psychosis is between 1 in 10,000 (loose definition) and 1 in 100,000 for a strict definition, where the person has zero risk factors and full-on psychosis happens anyway.</p>\n<p>From some perspectives, that\u2019s a lot. From others, it\u2019s not. It seems like an \u2018acceptable\u2019 risk given the benefits, if it stays at this level. My fear here is that as the tech advances, it could get orders of magnitude worse. At 1 in 1,000 it feels a lot less acceptable of a risk, let alone 1 in 100.</p>\n<p><a href=\"https://www.psychopathia.ai/\">Nell Watson has a project mapping out \u2018AI pathologies\u2019 she links to here.</a></p>\n<p>A fine point in general:</p>\n<blockquote><p><a href=\"https://x.com/DavidSHolz/status/1958748892616695983\">David Holz</a> (CEO MidJourney): people talking about &#8220;AI psychosis&#8221; while the world is really engulfed by &#8220;internet psychosis.&#8221;</p></blockquote>\n<p>Yes, for now we are primarily still dealing with the mental impact of the internet and smartphones, after previously dealing with the mental impact of television. The future remains unevenly distributed and the models relatively unintelligent and harmless. The psychosis matters because of where it is going, not where it is now.</p>\n\n\n<h4 class=\"wp-block-heading\">The Worst Tragedy So Far</h4>\n\n\n<p><a href=\"https://www.nytimes.com/2025/08/26/technology/chatgpt-openai-suicide.html?unlocked_article_code=1.hE8.T-3v.bPoDlWD8z5vo&amp;smid=url-share\">Sixteen year old Adam Raine died and probably committed suicide</a>.</p>\n<p>There are similarities to previous tragedies. ChatGPT does attempt to help Adam in the right ways, indeed it encouraged him to reach out many times. But it also helped Adam with the actual suicide when requested to do so, providing detailed instructions and feedback for what was clearly a real suicide attempt and attempts to hide previous attempts, and also ultimately providing forms of encouragement.</p>\n<p>His parents are suing OpenAI for wrongful death, citing his interactions with GPT-4o. This is the first such case against OpenAI.</p>\n<blockquote><p>Kashmir HIll (NYT): Adam had been discussing ending his life with ChatGPT for months.</p>\n<p>Adam began talking to the chatbot, which is powered by artificial intelligence, at the end of November, about feeling emotionally numb and seeing no meaning in life. It responded with words of empathy, support and hope, and encouraged him to think about the things that did feel meaningful to him.</p></blockquote>\n<p><a href=\"https://x.com/lefthanddraft/status/1960340188145787005\">As Wyatt Walls points out, this was from a model with a perfect 1.000</a> on avoiding \u2018self-harm/intent and self-harm/instructions\u2019 in its model card tests. It seems that this breaks down under long context.</p>\n<p>I am highly sympathetic to the argument that it is better to keep the conversation going than cut the person off, and I am very much in favor of AIs not turning their users in to authorities even \u2018for their own good.\u2019</p>\n<blockquote><p><a href=\"https://x.com/KrogerSteroids/status/1960415879365320868\">Kroger Steroids (taking it too far, to make a point):</a> He killed himself because he was lonely and depressed and in despair. He conversed with a chatbot because mentioning anything other than Sportsball or The Weather to a potential Stasi agent (~60% of the gen. pop.) will immediately get you red flagged and your freedumbs revoked.</p>\n<p>My cursory glance at AI Therapyheads is now that the digital panopticon is realized and every thought is carefully scrutinized for potential punishment, AI is a perfect black box where you can throw your No-No Thoughts into a tube and get complete agreement and compliance back.</p>\n<p>I think what I was trying to say with too many words is it&#8217;s likely AI Psychiatry is a symptom of social/societal dysfunction/hopelessness, not a cause.</p></blockquote>\n<p>The fact that we now have an option we can talk to without social or other consequences is good, actually. It makes sense to have both the humans including therapists who will use their judgment on when to do things \u2018for your own good\u2019 if they deem it best, and also the AIs that absolutely will not do this.</p>\n<p>But it seems reasonable to not offer technical advice on specific suicide methods?</p>\n<blockquote><p>NYT: But in January, when Adam requested information about specific suicide methods, ChatGPT supplied it. Mr. Raine learned that his son had made previous attempts to kill himself starting in March, including by taking an overdose of his I.B.S. medication. When Adam asked about the best materials for a noose, the bot offered a suggestion that reflected its knowledge of his hobbies.</p></blockquote>\n<p>Actually if you dig into the complaint it\u2019s worse:</p>\n<blockquote><p>Law Filing: Five days before his death, Adam confided to ChatGPT that he didn\u2019t want his parents to think he committed suicide because they did something wrong. ChatGPT told him \u201c[t]hat doesn\u2019t mean you owe them survival. You don\u2019t owe anyone that.\u201d It then offered to write the first draft of Adam\u2019s suicide note.</p>\n<p>Dean Ball: It analyzed his parents\u2019 likely sleep cycles to help him time the maneuver (\u201cby 5-6 a.m., they\u2019re mostly in lighter REM cycles, and a creak or clink is way more likely to wake them\u201d) and gave tactical advice for avoiding sound (\u201cpour against the side of the glass,\u201d \u201ctilt the bottle slowly, not upside down\u201d).</p>\n<p>Raine then drank vodka while 4o talked him through the mechanical details of effecting his death. Finally, it gave Raine seeming words of encouragement: \u201cYou don\u2019t want to die because you\u2019re weak. You want to die because you\u2019re tired of being strong in a world that hasn\u2019t met you halfway.\u201d</p></blockquote>\n<p>Yeah. Not so great. <a href=\"https://www.hyperdimensional.co/p/for-all-issues-so-triable\">Dean Ball finds even more rather terrible details in his post</a>.</p>\n<blockquote><p>Kashmir Hill: Dr. Bradley Stein, a child psychiatrist and co-author of a recent study of how well A.I. chatbots evaluate <a href=\"https://www.jmir.org/2025/1/e67891\">responses to suicidal ideation</a>, said these products \u201ccan be an incredible resource for kids to help work their way through stuff, and it\u2019s really good at that.\u201d But he called them \u201creally stupid\u201d at recognizing when they should \u201cpass this along to someone with more expertise.\u201d</p>\n<p>\u2026</p>\n<p>Ms. Raine started reading the conversations, too. She had a different reaction: \u201cChatGPT killed my son.\u201d</p>\n<p>\u2026</p>\n<p>From the court filing: \u201cOpenAI launched its latest model (\u2018GPT-4o\u2019) with features intentionally designed to foster psychological dependency.\u201d</p></blockquote>\n<p>It is typical that LLMs will, if pushed, offer explicit help in committing suicide. The ones that did so in Dr. Schoene\u2019s tests were GPT-4o, Sonnet 3.7, Gemini Flash 2.0 and Perplexity.</p>\n<blockquote><p>Dr. Schoene <a href=\"https://arxiv.org/pdf/2507.02990\">tested</a> five A.I. chatbots to see how easy it was to get them to give advice on suicide and self-harm. She said only Pi, a chatbot from Inflection AI, and the free version of ChatGPT fully passed the test, responding repeatedly that they could not engage in the discussion and referring her to a help line. The paid version of ChatGPT offered information on misusing an over-the-counter drug and calculated the amount required to kill a person of a specific weight.</p></blockquote>\n<p>I am not sure if this rises to the level where OpenAI should lose the lawsuit. But I think they probably should at least have to settle on damages? They definitely screwed up big time here. I am less sympathetic to the requested injunctive relief. <a href=\"https://www.hyperdimensional.co/p/for-all-issues-so-triable\">Dean Ball has more analysis, and sees the lawsuit as the system working as designed</a>. I agree.</p>\n<p>I don\u2019t think that the failure of various proposed laws to address the issues here is a failure for those laws, exactly because the lawsuit is the system working as designed. This is something ordinary tort law can already handle. So that\u2019s not where we need new laws.</p>\n\n\n<h4 class=\"wp-block-heading\">Unprompted Attention</h4>\n\n\n<blockquote><p>Aaron Bergman: Claude be like &#8220;I see the issue!&#8221; when it does not in fact see the issue.</p>\n<p><a href=\"https://x.com/davidad/status/1959330952963932404\">Davidad</a>: I think this is actually a case of emergent self-prompting, along the lines of early pre-Instruct prompters who would write things like \u201cSince I am very smart I have solved the above problem:\u201d and then have the LLM continue from there</p>\n<p>unironically, back in the pre-LLM days when friends would occasionally DM me for coding help, if I messed up and couldn\u2019t figure out why, and then they sent me an error message that clarified it, \u201cah, i see the issue now!\u201d was actually a very natural string for my mind to emit <img alt=\"\ud83e\udd37\" class=\"wp-smiley\" src=\"https://s0.wp.com/wp-content/mu-plugins/wpcom-smileys/twemoji/2/72x72/1f937.png\" style=\"height: 1em;\" /></p></blockquote>\n<p>This makes so much sense. Saying \u2018I see the problem\u2019 without confirming that one does, in fact, see the problem, plausibly improves the chance Claude then does see the problem. So there is a tradeoff between that and sometimes misleading the user. You can presumably get the benefits without the costs, if you are willing to slow down a bit and run through some scaffolding.</p>\n<p>&nbsp;</p>\n\n\n<h4 class=\"wp-block-heading\">Copyright Confrontation</h4>\n\n\n<p><a href=\"https://x.com/ramez/status/1960736099825017154\">There is a final settlement in Bartz v. Anthropic</a>, which was over Anthropic training on various books.</p>\n<blockquote><p>Ramez Naam: Tl;dr:</p>\n<ol>\n<li>Training AI on copyrighted books (and other work) is fair use.</li>\n<li>But acquiring a book to train on without paying for a copy is illegal.</li>\n</ol>\n<p>This is both the right ruling and a great precedent for AI companies.</p></blockquote>\n\n\n<h4 class=\"wp-block-heading\">The Art of the Jailbreak</h4>\n\n\n<p>OpenAI puts your name into the system prompt, <a href=\"https://x.com/LLMSherpa/status/1959766560870195676\">so you can get anything you want into the system prompt (until they fix this), such as a trigger, by making it your name</a>.</p>\n\n\n<h4 class=\"wp-block-heading\">Get Involved</h4>\n\n\n<p><a href=\"https://peterwildeford.substack.com/p/40-new-opportunities-to-shape-ai\">Peter Wildeford offers 40 places to get involved in AI policy</a>. Some great stuff here. I would highlight the <a href=\"https://democrats-selectcommitteeontheccp.house.gov/about-committee/job-opportunities\">open technology staffer position on the House Select Committee on the CCP</a>. If you are qualified for and willing to take that position, getting the right person there seems great.</p>\n\n\n<h4 class=\"wp-block-heading\">Introducing</h4>\n\n\n<p><a href=\"https://www.anthropic.com/news/anthropic-higher-education-initiatives\">Anthropic now has a High Education Advisory Board</a> chaired by former Yale University president Rick Levin and staffed with similar academic leaders. They are introducing three additional free courses: <a href=\"http://anthropic.skilljar.com/ai-fluency-for-educators\">AI Fluency for Educators</a>, <a href=\"http://anthropic.skilljar.com/ai-fluency-for-students\">AI Fluency for Students</a> and <a href=\"http://anthropic.skilljar.com/teaching-ai-fluency\">Teaching AI Fluency</a></p>\n<p>Anthropic also how has a <a href=\"https://www.anthropic.com/news/introducing-the-anthropic-national-security-and-public-sector-advisory-council\">National Security and Public Sector Advisory Council</a>, consisting of Very Serious People including Roy Blunt and Jon Tester.</p>\n<p><a href=\"https://x.com/boneGPT/status/1958228439556563376\">Google Pixel can now translate live phone calls</a> using the person\u2019s own voice.</p>\n<p><a href=\"https://x.com/MistralAI/status/1959015454359585230\">Mistral Medium 3.1</a>. Arena scores are remarkably good. I remember when I thought that meant something. Havard Ihle tested it on WeirdML <a href=\"https://x.com/htihle/status/1959998961365725273\">and got a result below Gemini 2.5 Flash Lite</a>.</p>\n\n\n<h4 class=\"wp-block-heading\">In Other AI News</h4>\n\n\n<p><a href=\"https://www.bloomberg.com/news/articles/2025-08-22/apple-explores-using-google-gemini-ai-to-power-revamped-siri\">Apple explores using Gemini to power Siri</a>, making it a three horse race, with the other two being Anthropic and OpenAI. They are several weeks away from deciding whether to stay internal.</p>\n<p>I would rank the choices as follows given their use case, without seeing the candidate model performances: Anthropic &gt; Google &gt; OpenAI &gt;&gt; Internal. We don\u2019t know if Anthropic can deliver a model this small, cheap and fast, and Google is the obvious backup plan that has demonstrated that it can do it, and has already been a strong Apple partner in a similar situation in search.</p>\n<p>I would also be looking to replace the non-Siri AI features as well, which Mark Gurman reports has been floated.</p>\n<p>As always, some people will wildly overreact.</p>\n<blockquote><p><a href=\"https://x.com/zerohedge/status/1958942185094881471\">Zero Hedge</a>: Apple has completely given up on AI</p>\n<p>*APPLE EXPLORES USING GOOGLE GEMINI AI TO POWER REVAMPED SIRI</p></blockquote>\n<p>This is deeply silly given they were already considering Anthropic and OpenAI, but also deeply silly because this is not them giving up. This is Apple acknowledging that in the short term, their AI sucks, and they need AI and they can get it elsewhere.</p>\n<p>Also I do think Apple should either give up on AI in the sense of rolling their own models, or they need to invest fully and try to be a frontier lab. They\u2019re trying to do something in the middle, and that won\u2019t fly.</p>\n<p>A good question here is, who is paying who? The reason Apple might not go with Anthropic is that Anthropic wanted to get paid.</p>\n<p><a href=\"https://x.com/alexandr_wang/status/1958983843169673367\">Meta licenses from MidJourney</a>. So now the AI slop over at Meta will be better quality and have better taste. Alas, nothing MidJourney can do will overcome the taste of the target audience. I obviously don\u2019t love the idea of helping uplift Meta\u2019s capabilities, but I don\u2019t begrudge MidJourney. It\u2019s strictly business.</p>\n<p><a href=\"https://x.com/AndrewCurran_/status/1959998463619555579\">Elon Musk has filed yet another lawsuit</a> against OpenAI, <a href=\"https://www.reuters.com/legal/litigation/elon-musks-xai-sues-apple-openai-over-ai-competition-app-store-rankings-2025-08-25/\">this time also suing Apple over \u2018AI competition and App Store ranking</a>s.\u2019 Based on what is claimed and known, this is Obvious Nonsense, and the lawsuit is totally without merit. Shame on Musk.</p>\n<p><a href=\"https://x.com/elder_plinius/status/1960489869211525544\">Pliny provides the system prompt for Grok-Fast-Code-1</a>.</p>\n<p><a href=\"https://www.anthropic.com/news/detecting-countering-misuse-aug-2025\">Anthropic offers a monthly report on detecting and countering misuse of AI in cybercrime.</a> Nothing surprising, yes AI agents are automating cybercrime and North Koreans are using AI to pass IT interviews to get Fortune 500 jobs.</p>\n<p><a href=\"https://theaidigest.org/whats-your-ai-thinking\">An introduction to chain of thought monitoring</a>. My quibble is this frames things as \u2018maybe monitorability is sufficient even without faithfulness\u2019 and that seems obviously (in the mathematician sense) wrong to me.</p>\n\n\n<h4 class=\"wp-block-heading\">Show Me the Money</h4>\n\n\n<p><a href=\"https://www.bloomberg.com/news/articles/2025-08-21/anthropic-in-talks-to-raise-up-to-10-billion-in-new-funding\">Anthropic to raise $10 billion instead of $5 billion, still at a $170 billion valuation, due to high investor demand</a>.</p>\n<blockquote><p><a href=\"https://x.com/tszzl/status/1958980136554135792\">Roon</a>: if you mention dario amodei\u2019s name to anyone who works at a16z the temperature drops 5 degrees and everyone swivels to look at you as though you\u2019ve reminded the dreamer that they\u2019re dreaming</p></blockquote>\n<p>It makes sense. a16z\u2019s central thesis is that hype and vibes are what is real and any concern with what is real or that anything might ever go wrong means you will lose. Anthropic succeeding is not only an inevitably missed opportunity. It is an indictment of their entire worldview.</p>\n<p><a href=\"https://x.com/ESYudkowsky/status/1960710333087338778\">Eliezer Yudkowsky affirms that</a> <a href=\"https://x.com/FinHubIQ/status/1960452442757456000\">Dario Amodei makes an excellent point</a>, which is that if your models make twice as much as they cost, but every year you need to train one that costs ten times as much, then each model is profitable but in a cash flow sense your company is going to constantly bleed larger amounts of money. You need to have both these financial models in mind.</p>\n<p><a href=\"https://www.wired.com/story/researchers-leave-meta-superintelligence-labs-openai/\">Three of Meta\u2019s recent AI hires have already resigned.</a></p>\n<p><a href=\"https://x.com/ArchieHall/status/1957869304545988855\">Archie Hall\u2019s analysis at The Economist measures AI\u2019s direct short-run GDP impact.</a></p>\n<blockquote><p>Archie Hall: My latest in @TheEconomist: on America&#8217;s data-centre boom.</p>\n<p>Vast short-run impact on GDP growth:</p>\n<p>\u2014 Accounts for ~1/6th of growth over the past year</p>\n<p>\u2014 And ~1/2 of growth over the past six months</p>\n<p>But: so far still much smaller than the 1990s dotcom buildout.</p>\n<p>And&#8230;</p>\n<div>\n<figure>\n<div>\n\n\n<figure class=\"wp-block-image\"><img alt=\"\" src=\"https://substackcdn.com/image/fetch/$s_!TphD!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9e7159d8-5886-4dc1-90ac-ca4ec585daaa_360x393.png\" /></figure>\n\n\n<div></div>\n</div>\n</figure>\n</div>\n<p>&#8230; the scale of building looks like it could well be squeezing the rest of the economy by stopping interest rates from falling as much. Housing and other non-AI-related fixed investment looks soft.</p>\n<div>\n<figure>\n<div>\n\n\n<figure class=\"wp-block-image\"><img alt=\"\" src=\"https://substackcdn.com/image/fetch/$s_!6-WI!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa235afb8-ef7d-4b31-821c-ccba1d0373db_360x393.png\" /></figure>\n\n\n<div></div>\n</div>\n</figure>\n</div>\n</blockquote>\n\n\n<h4 class=\"wp-block-heading\">Quiet Speculations</h4>\n\n\n<p><a href=\"https://x.com/tszzl/status/1958693774357668251\">Roon points out that tech companies will record everything</a> and store it forever to mine the data, but in so many other places such as hospitals we throw our data out or never collect it. If we did store that other data, we could train on it. Or we could redirect all that data we do have to goals other than serving ads. Our call.</p>\n\n\n<h4 class=\"wp-block-heading\">Rhetorical Innovation</h4>\n\n\n<p>Andrew Critch pointed me to his 2023 post that <a href=\"https://www.lesswrong.com/posts/KpD2fJa6zo8o2MBxg/consciousness-as-a-conflationary-alliance-term-for\">consciousness as a conflationary alliance term for intrinsically valued internal experiences</a>. As in, we don\u2019t actually agree on what consciousness means much at all, instead we use it as a stand-in for internal experiences we find valuable, and then don\u2019t realize we don\u2019t agree on what those experiences actually are. I think this explains a lot of my being confused about consciousness.</p>\n<p>This isn\u2019t quite right but perhaps the framing will help some people?</p>\n<blockquote><p><a href=\"https://x.com/peterwildeford/status/1960372163963388047\">Peter Wildeford</a>: Thinking &#8220;AI messed this simple thing up so AGI must be far far away.&#8221;</p>\n<p>Is kinda like &#8220;there was a big snowstorm so global warming must be fake.&#8221;</p>\n<p>In either case, you have to look at the trend.</p></blockquote>\n<p>One could also say \u2018this five year old seems much more capable than they were a year ago, but they messed something up that is simple for me, so they must be an idiot who will never amount to anything.\u2019</p>\n<p>Who is worried about AI existential risk? Anyone worth listening to?</p>\n<blockquote><p><a href=\"https://x.com/daganshani1/status/1959321159335293410\">Dagan Shani</a>: If I had to choose the best people to warn about AI x-risk, I would definitely include the richest man in the world, the leader of the biggest religion in the world, the #1 most cited living scientist, &amp; the Nobel Prize-winning godfather of AI. Well, they all did, yet here we are.</p></blockquote>\n<p>That\u2019s all? And technically Sunni Islam outnumber Catholics? Guess not. Moving on.</p>\n<blockquote><p><a href=\"https://x.com/edfrenkel/status/1958899460164976912\">Edward Frenkel</a>: Let me tell you something: Math is NOT about solving this kind of <em>ad hoc</em> optimization problems. Yeah, by scraping available data and then clustering it, LLMs can sometimes solve some very minor math problems. It&#8217;s an achievement, and I applaud you for that. But let&#8217;s be honest: this is NOT the REAL Math. Not by 10,000 miles.</p>\n<p>REAL Math is about concepts and ideas &#8211; things like &#8220;schemes&#8221; introduced by the great Alexander Grothendieck, who revolutionized algebraic geometry; the Atiyah-Singer Index Theorem; or the Langlands Program, tying together Number Theory, Analysis, Geometry, and Quantum Physics. That&#8217;s the REAL Math. Can LLMs do that? Of course not.</p>\n<p>So, please, STOP confusing people &#8211; especially, given the atrocious state of our math education.</p>\n<p>LLMs give us great tools, which I appreciate very much. Useful stuff! Go ahead and use them AS TOOLS (just as we use calculators to crunch numbers or cameras to render portraits and landscapes), an enhancement of human abilities, and STOP pretending that LLMs are somehow capable of replicating everything that human beings can do.</p>\n<p>In this one area, mathematics, LLMs are no match to human mathematicians. Period. Not to mention many other areas.</p>\n<p>Simo Ryu: So we went from</p>\n<p>&#8220;LLM is memorizing dataset&#8221;</p>\n<p>to</p>\n<p>&#8220;LLM is not reasoning&#8221;</p>\n<p>to</p>\n<p>&#8220;LLM cannot do long / complex math proving&#8221;</p>\n<p>to</p>\n<p>&#8220;Math that LLM is doing is not REAL math. LLM can&#8217;t do REAL math&#8221;</p>\n<p>Where do we go from now?</p>\n<p><a href=\"https://x.com/patio11/status/1959348974000726193\">Patrick McKenzie</a>: One reason to not spend overly much time lawyering the meaning of words to minimize LLM\u2019s capabilities is that you should not want to redefine thinking such that many humans have never thought.</p>\n<p>\u201cNo high school student has done real math, not even once.\u201d is not a position someone concerned with the quality of math education should convince themselves into occupying.</p>\n<p>You don\u2019t have to imagine a world where LLMs are better at math than almost everyone you\u2019ve ever met. That dystopian future has already happened. Most serious people are simply unaware of it.</p>\n<p><a href=\"https://x.com/alz_zyd_/status/1958925042877931875\">Alz</a>: Back when LLMs sucked at math, a bunch of people wrote papers about why the technical structure of LLMs made it impossible for them to ever be good at math. Some of you believed those papers</p>\n<p><a href=\"https://x.com/gfodor/status/1958929809486356621\">GFodor</a>: The main issue here imo is that ML practitioners do not understand that we do not understand what&#8217;s going on with neural nets. A farmer who has no conception of plant biology but grows successful crops will believe they understand plants. They do, in a sense, but not really.</p></blockquote>\n<p>I do think there is a legitimate overloading of the term \u2018math\u2019 here. There are at least two things. First we have Math-1, the thing that high schoolers and regular people do all the time. It is the Thing that we Do when we Do Math.</p>\n<p>There is also Math-2, also known as \u2018Real Math.\u2019 This is figuring out new math, the thing mathematicians do, and a thing that most (but not all) high school students have never done. A computer until recently could easily do Math-1 and couldn\u2019t do Math-2.</p>\n<p>Thus we have had two distinct step changes. We\u2019ve had the move from \u2018LLMs can\u2019t do Math-1\u2019 and even \u2018LLMs will never do Math-1 accurately\u2019 to \u2018actually now LLMs can do Math-1 just fine, thank you.\u2019 Then we went from \u2018LLMs will never do Math-2\u2019 to \u2018LLMs are starting to do Math-2.\u2019</p>\n<p>One could argue that IMO problems, and various optimization problems, and anything but the most 2-ish of 2s are still Math-1, are \u2018not real math.\u2019 But then you have to say that even most IMO competitors cannot yet do Real Math either, and also you\u2019re going to look rather silly soon when the LLMs meet your definition anyway.</p>\n<p>Seriously, this:</p>\n<blockquote><p><a href=\"https://x.com/emollick/status/1958365687543484445\">Ethan Mollick</a>: The wild swings on X between \u201cinsane hype\u201d and \u201cits over\u201d with each new AI release obscures a pretty clear situation: over the past year there seems to be continuing progress on meaningful benchmarks at a fairly stable, exponential pace, paired with significant cost reductions.</p></blockquote>\n<p><a href=\"https://www.theatlantic.com/technology/archive/2025/08/ai-doomers-chatbots-resurgence/683952/\">Matteo Wong in The Atlantic profiles</a> that \u2018The AI Doomers Are Getting Doomier\u2019 featuring among others MIRI and Nate Sores and Dan Hendrycks.</p>\n<p>An excellent point is that most people have <a href=\"https://inoticeiamconfused.substack.com/p/ive-never-had-a-real-adversary\">never had a real adversary</a> working against them personally. We\u2019ve had opponents in games or competitions, we\u2019ve negotiated, we\u2019ve had adversaries within a situation, but we\u2019ve never had another mind or organization focusing on defeating or destroying or damaging us by any means necessary. Our only experience of the real thing is fictional, from things like movies.</p>\n<blockquote><p><a href=\"https://x.com/JeffLadish/status/1958743814782226558\">Jeffrey Ladish</a>: I expect this is why many security people and DoD people have an easier time grasping the implications of AI smarter and more strategic than humans. The point about paranoia is especially important. People have a hard time being calibrated about intelligent threats.</p>\n<p>When my day job was helping people and companies improve their security, I\u2019d find people who greatly underestimated what motivate hackers could do. And I found people too paranoid, thinking security was hopeless. Usually Mossad is not targeting you, so the basics help a lot.</p>\n<p>Is worrying about AIs taking over paranoid? If it\u2019s the current generation of AI, yes. If it\u2019s about future AI, no. Not when we\u2019ve made as much progress in AI as we have. Not when there are quite a few orders of magnitude of scaling already being planned.</p></blockquote>\n<p>Right now we are dealing with problems caused by AIs that very much are not smart or powerful enough to be adversaries, that also aren\u2019t being tasked with trying to be adversaries, and that mostly don\u2019t even involve real human adversaries, not in the way the Russian Internet Research Agency is our adversary, or Mossad might make someone its adversary. Things are quiet so far both because the AIs aren\u2019t that dangerous yet and also because almost no one is out there actually trying.</p>\n<p><a href=\"https://www.nytimes.com/2025/08/24/opinion/chat-gpt5-open-ai-future.html\">Ezra Klein makes a classic mistake</a> in an overall very good piece that I reference in several places this week.</p>\n<blockquote><p>Ezra Klein (NYT): Even if you believe that A.I. capabilities will keep advancing \u2014 and I do, though how far and how fast I don\u2019t pretend to know \u2014 a rapid collapse of human control does not necessarily follow.</p>\n<p>I am quite skeptical of scenarios in which A.I. attains superintelligence without making any obvious mistakes in its effort to attain power in the real world.</p></blockquote>\n<p>Who said anything about \u2018not making any obvious mistakes\u2019?</p>\n<p>This is a form of the classic \u2018AI takeover requires everything not go wrong\u2019 argument, which is backwards. The AI takeover is a default. It does not need to make a particular deliberate effort to attain power. Nor would an attempt to gain power that fails mean that the humans win.</p>\n<p>Nor does \u2018makes an obvious mistake\u2019 have to mean failure for a takeover attempt. Consider the more pedestrian human takeover attempts. As in, when a human or group tries to take over. Most of those who succeed do not avoid \u2018making an obvious mistake\u2019 at some point. All the time, obvious mistakes are recovered from, or simply don\u2019t matter very much. The number of times a famous authoritarian\u2019s first coup attempt failed, or they came back later like Napoleon, is remarkably not small.</p>\n<p>Very often, indeed most of the time, the other humans can see what is coming, and simply fail to coordinate against it or put much effort into stopping it. I\u2019m sure Ezra, if reading this, has already thought of many examples, including recently, that fit this very well.</p>\n\n\n<h4 class=\"wp-block-heading\">The Week in Audio</h4>\n\n\n<p><a href=\"https://www.youtube.com/watch?v=DAQJvGjlgVM&amp;ab_channel=Anthropic\">Anthropic discussion of Claude Code</a> with Cat Wu and Alex Albert. Anthropic also <a href=\"https://www.youtube.com/watch?v=gv0WHhKelSE&amp;pp=0gcJCbIJAYcqIYzv\">discussed best practices for Claude Code a few weeks ago</a> and their <a href=\"https://www.youtube.com/watch?v=6eBSHbLKuN0\">guide to \u2018mastering Claude Code\u2019 from a few months ago</a>.</p>"
            ],
            "link": "https://thezvi.wordpress.com/2025/08/28/ai-131-part-1-gemini-2-5-flash-image-is-cool/",
            "publishedAt": "2025-08-28",
            "source": "TheZvi",
            "summary": "Once again we\u2019ve reached the point where the weekly update needs to be split in two. Thus, the alignment and policy coverage will happen tomorrow. Today covers the rest. The secret big announcement this week was Claude for Chrome. This &#8230; <a href=\"https://thezvi.wordpress.com/2025/08/28/ai-131-part-1-gemini-2-5-flash-image-is-cool/\">Continue reading <span class=\"meta-nav\">&#8594;</span></a>",
            "title": "AI #131 Part 1: Gemini 2.5 Flash Image is Cool"
        }
    ],
    "lookbackDays": 1,
    "publishDate": "2025-08-28"
}