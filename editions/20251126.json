{
    "articles": [
        {
            "content": [],
            "link": "https://www.nytimes.com/2025/11/26/style/tiny-modern-love-stories-no-hugging-and-definitely-nothing-lewd.html",
            "publishedAt": "2025-11-26",
            "source": "Modern Love - NYT",
            "summary": "Modern Love in miniature, featuring reader-submitted stories of no more than 100 words.",
            "title": "Tiny Love Stories: \u2018No Hugging \u2026 and Definitely Nothing Lewd\u2019"
        },
        {
            "content": [],
            "link": "https://www.nytimes.com/2025/11/26/podcasts/nedra-tawwab-holidays-family-thanksgiving.html",
            "publishedAt": "2025-11-26",
            "source": "Modern Love - NYT",
            "summary": "Family holidays can be stressful. They don\u2019t have to be.",
            "title": "A Therapist\u2019s Advice on How to Navigate the Holiday Season With Family"
        },
        {
            "content": [
                "<img src=\"https://tonsky.me/blog/hiring-ai/cover.webp\" /><p>It\u2019s 2025 and you are applying for a software engineer position. They give you a test assignment. You complete it yourself, send it over, and get rejected. Why?</p>\n<p>Because it looked like AI.</p>\n<p>Unfortunately, it\u2019s 2025, AI is spreading like glitter in a kindergarten, and it\u2019s really easy to mistake hard human labor for soulless, uninspired machine slop.</p>\n<p>Following are the main <em>red flags</em> in test assignments that should be <strong>avoided</strong>:</p>\n<ul>\n  <li>The assignment was read and understood in full.</li>\n  <li>All parts are implemented.</li>\n  <li>Industry-standard tools and frameworks are used.</li>\n  <li>The code is split into small, readable functions.</li>\n  <li>Variables have descriptive names.</li>\n  <li>Complex parts have comments.</li>\n  <li>Errors are handled, error messages are easy to follow.</li>\n  <li>Source files are organized reasonably.</li>\n  <li>The web interface looks nice.</li>\n  <li>There are tests.</li>\n</ul>\n<p>Avoid these AI giveaways and spread the word!</p>"
            ],
            "link": "https://tonsky.me/blog/hiring-ai/",
            "publishedAt": "2025-11-26",
            "source": "Nikita Prokopov",
            "summary": "<img src=\"https://tonsky.me/blog/hiring-ai/cover.webp\" /><p>It\u2019s 2025 and you are applying for a software engineer position. They give you a test assignment. You complete it yourself, send it over, and get rejected. Why?</p> <p>Because it looked like AI.</p> <p>Unfortunately, it\u2019s 2025, AI is spreading like glitter in a kindergarten, and it\u2019s really easy to mistake hard human labor for soulless, uninspired machine slop.</p> <p>Following are the main <em>red flags</em> in test assignments that should be <strong>avoided</strong>:</p> <ul> <li>The assignment was read and understood in full.</li> <li>All parts are implemented.</li> <li>Industry-standard tools and frameworks are used.</li> <li>The code is split into small, readable functions.</li> <li>Variables have descriptive names.</li> <li>Complex parts have comments.</li> <li>Errors are handled, error messages are easy to follow.</li> <li>Source files are organized reasonably.</li> <li>The web interface looks nice.</li> <li>There are tests.</li> </ul> <p>Avoid these AI giveaways and spread the word!</p>",
            "title": "How to get hired in 2025"
        },
        {
            "content": [],
            "link": "https://simonwillison.net/2025/Nov/26/data-renegades-podcast/#atom-entries",
            "publishedAt": "2025-11-26",
            "source": "Simon Willison",
            "summary": "<p>I talked with CL Kao and Dori Wilson for an episode of their new <a href=\"https://www.heavybit.com/library/podcasts/data-renegades\">Data Renegades podcast</a> titled <a href=\"https://www.heavybit.com/library/podcasts/data-renegades/ep-2-data-journalism-unleashed-with-simon-willison\">Data Journalism Unleashed with Simon Willison</a>.</p> <p>I fed the transcript into Claude Opus 4.5 to extract this list of topics with timestamps and illustrative quotes. It did such a good job I'm using what it produced almost verbatim here - I tidied it up a tiny bit and added a bunch of supporting links.</p> <ul> <li> <p>What is data journalism and why it's the most interesting application of data analytics [02:03]</p> <blockquote> <p>\"There's this whole field of data journalism, which is using data and databases to try and figure out stories about the world. It's effectively data analytics, but applied to the world of news gathering. And I think it's fascinating. I think it is the single most interesting way to apply this stuff because everything is in scope for a journalist.\"</p> </blockquote> </li> <li> <p>The origin story of Django at a small Kansas newspaper [02:31]</p> <blockquote> <p>\"We had a year's paid internship from university where we went to work <a href=\"https://simonwillison.net/2025/Jul/13/django-birthday/\">for this local newspaper</a> in Kansas with this chap <a href=\"https://holovaty.com/\">Adrian Holovaty</a>. And at the time we thought we",
            "title": "Highlights from my appearance on the Data Renegades podcast with CL Kao and Dori Wilson"
        },
        {
            "content": [
                "<p>If we worry too much about AI safety, will this make us &#8220;lose the race with China&#8221;<a class=\"footnote-anchor\" href=\"https://www.astralcodexten.com/feed#footnote-1\" id=\"footnote-anchor-1\" target=\"_self\">1</a>?</p><p>(here &#8220;AI safety&#8221; means long-term concerns about alignment and hostile superintelligence, as opposed to &#8220;AI ethics&#8221; concerns like bias or intellectual property.)</p><p>Everything has tradeoffs, regulation vs. progress is a common dichotomy, and the more important you think AI will be, the more important it is that the free world get it first. If you believe in superintelligence, the technological singularity, etc, then you think AI is maximally important, and this issue ought to be high on your mind.</p><p>But when you look at this concretely, it becomes clear that this is too small to matter - so small that even the sign is uncertain.</p><h2>The State Of The Race</h2><p>We can divide the AI race into three levels: <strong>compute</strong>, <strong>models</strong>, and <strong>applications</strong><a class=\"footnote-anchor\" href=\"https://www.astralcodexten.com/feed#footnote-2\" id=\"footnote-anchor-2\" target=\"_self\">2</a>.  Companies use compute - chips deployed in data centers - to train models like GPT and Claude. Then they use those models in various applications. For now, those applications are things like Internet search and image generation. In the future, they might become geopolitically relevant fields like manufacturing and weapons systems.</p><p><strong>Compute</strong>: America is far ahead. We have better chips (thanks, NVIDIA) and can produce many more of them (thanks, TSMC). Our recent capex boom, where companies like Google and Microsoft spend hundreds of billions of dollars on data centers, has no Chinese equivalent. By the simplest measure - total FLOPs on each sides - we have 10x as much compute as China, and our advantage is growing every day. A 10x compute advantage corresponds to about a 1-2 year time advantage, or an 0.5 - 1 generation advantage (eg GPT-4 to GPT-5).</p><p><strong>Models: </strong>The quality of foundation models - giant multi-purpose AIs like GPT or Claude - primarily depends on the amount of compute used to train them, so America&#8217;s compute advantage carries over to this level. In theory, clever training methods and advanced algorithms can make one model more or less compute-efficient than another, but this doesn&#8217;t seem to be affecting the current state of the race much - most advances by one country are quickly diffused to (or stolen by) the other. Despite some early concerns, neither DeepSeek nor Kimi K2 Chinese models provide strong evidence of a Chinese advantage in computational efficiency (<a href=\"https://www.seangoedecke.com/is-deepseek-fast/\">1</a>, <a href=\"https://www.gleech.org/paper\">2</a>).</p><p><strong>Applications: </strong>This is where China is most likely to dominate<a class=\"footnote-anchor\" href=\"https://www.astralcodexten.com/feed#footnote-3\" id=\"footnote-anchor-3\" target=\"_self\">3</a>. They already outdo America at most forms of advanced manufacturing and infrastructure deployment (eg solar, high-speed rail). And as a command economy, they have more ability to steamroll over concerns like job loss, intellectual property, et cetera. </p><p>China knows all of this and is building their AI strategy around it. The plan, which some observers have dubbed &#8220;fast follow&#8221;, goes like this:</p><ol><li><p>Work hard to catch up with US chip production. They are very far behind here, but also have a long history of catching up to the West on things when they put their mind to it, so they feel up to the challenge. They estimate this will take ten years.</p></li><li><p>For the next ten years, accept that they may lag somewhat behind America in compute, and therefore on models. But if they can smuggle in chips and steal US technological advances, they can keep this to a manageable 1-2 year gap, rather than a disastrous 4-5 year gap.</p></li><li><p><a href=\"https://mattsheehan.substack.com/p/chinas-big-ai-diffusion-plan-is-here\">Leverage their applications advantage as hard as possible.</a> They imagine that sure, maybe America will have AI that&#8217;s 1-2 years more advanced than theirs. But if our smarter AI is still just sitting in a data center answering user queries - and their dumber AI is already integrated with tens of thousands of humanoid robots, automated drones, missile targeting systems, etc - then they still win.</p></li></ol><p>This is a very practical strategy from a very practical country. The Chinese <a href=\"https://www.chinatalk.media/p/is-china-agi-pilled\">don&#8217;t really believe in</a> recursive self-improvement or superintelligence<a class=\"footnote-anchor\" href=\"https://www.astralcodexten.com/feed#footnote-4\" id=\"footnote-anchor-4\" target=\"_self\">4</a>. If they did, they wouldn&#8217;t be so blas&#233; about the possibility of America having AIs 1-2 years more advanced than theirs - if our models pass the superintelligence threshold while theirs are still approaching it, then their advantage in humanoids and drones no longer seems so impressive.</p><p>What is the optimal counter-strategy for America? We&#8217;re still debating specifics, but a skeletal, obvious-things-only version might be to preserve our compute advantage as long as possible, protect our technological secrets from Chinese espionage, and put up as much of a fight as possible on the application layer.</p><h2>The State Of AI Safety Policy</h2><p>It&#8217;s worth being specific about what we mean by &#8220;AI safety regulation&#8221;.</p><p>The two most discussed AI safety bills of the past year - California&#8217;s <a href=\"https://legiscan.com/CA/text/SB53/id/3262148\">SB53</a> and New York&#8217;s <a href=\"https://www.nysenate.gov/legislation/bills/2025/A6453/amendment/A\">RAISE Act</a> - as well as <a href=\"https://www.hyperdimensional.co/p/be-it-enacted\">Dean Ball&#8217;s proposed federal AI safety preemption bill</a> - all focus on a few key topics:</p><ul><li><p>The biggest companies (eg OpenAI, Anthropic, Google) must disclose their model spec, ie the internal document saying what their models are vs. aren&#8217;t banned from doing.</p></li><li><p>These companies should come up with some kind of safety policy and disclose it.</p></li><li><p>These companies can&#8217;t retaliate against whistleblowers who report violations of their safety policy.</p></li><li><p>These companies should do some kind of evaluation to see if their AIs can hack critical infrastructure, create biological weapons, or do other mass casualty events.</p></li><li><p>If they find that the answer is yes, they should tell the government.</p></li><li><p>If one of these things actually happens during testing, they should <em>definitely</em> tell the government.</p></li></ul><p>These are relatively cheap asks. For example, the evaluation to see whether AIs can hack infrastructure will require hiring people who can conduct the evaluation, allocating compute to the evaluation, etc. But on the scale of an AI training run, the sums involved are tiny. Currently, two nonprofits - METR and Apollo Research - do similar tests on publicly-available models. I estimate their respective budgets at $5 million and <a href=\"https://www.alignmentforum.org/posts/yrephKDBFL6h9zAFv/beth-barnes-s-shortform?commentId=w3e6cbHeozQJaJH5c\">$15 million</a> per year. Nonprofits can always pay lower salaries than big companies, so it may cost more for OpenAI to replicate their work - for the sake of argument, $25 million. Meanwhile, the likely cost to train GPT-6 will probably be about <a href=\"https://peterwildeford.substack.com/p/ai-is-probably-not-a-bubble\">$25</a> - <a href=\"https://www.astralcodexten.com/p/sam-altman-wants-7-trillion\">$75</a> billion, with a b. So the safety testing might increase the total cost by 1/1000th. I asked some people who work in AI labs whether this seemed right; they said that most of the cost would be in complexity, personnel, and delay, and suggested an all-things-considered number ten times higher - 1% of training costs.</p><p>But all activists start out with small asks, then move up to larger ones. Is there a risk that the next generation of AI safety regulations will be more burdensome? From what I hear, if we win this round beyond our expectations, the next generation of AI safety asks is third-party safety auditing and location verification for chips. I don&#8217;t know the exact details, but these don&#8217;t seem order-of-magnitude worse than the current bills. Maybe another 1%.</p><p>What about extreme far-future asks? Aren&#8217;t there safetyists who want to pause AI progress entirely?</p><p>Most people who discuss this want a <em>mutual</em> pause. The most extreme organization in this category, Pause AI, has this on <a href=\"https://pauseai.info/faq#if-we-pause-what-about-china\">their FAQ</a>:</p><blockquote><p><strong>Q: </strong>If we pause, what about China?</p><p><strong>A:</strong> [&#8230;] We are primarily asking for an <em>international</em> pause, enforced by a treaty. Such a treaty also needs to be signed by China. If the treaty guarantees that other nations will stop as well, and there are sufficient enforcement mechanisms in place, this should be something that China will want to see as well<a class=\"footnote-anchor\" href=\"https://www.astralcodexten.com/feed#footnote-5\" id=\"footnote-anchor-5\" target=\"_self\">5</a>.</p></blockquote><p>When we look at concrete AI safety demands, they aren&#8217;t of the type or magnitude to affect the race with China very much - maybe 1-2%.</p><h2>So Is It Impossible For Regulation To Erode The US Lead?</h2><p>Running the numbers: we started with a 10x compute advantage over China.</p><p>Safety legislation risks adding 1-2% to the cost of training runs.</p><p>So if we were able to train a model 10x bigger than China&#8217;s best model before safety legislation, we can train a model ~9.8x bigger than China&#8217;s best model after safety legislation.</p><p>Does that mean that America&#8217;s chip advantage is so big that no regulation can possibly lose us the race?</p><p>Not necessarily. Consider AI ethics regulations like the <a href=\"https://www.healthlawadvisor.com/will-colorados-historic-ai-law-go-live-in-2026-its-fate-hangs-in-the-balance-in-2025\">Colorado AI Act</a> of 2024. It legislates that any institution which uses AI to make decisions (schools, hospitals, businesses, etc) must perform yearly impact assessments evaluating whether the models might engage in &#8220;algorithmic discrimination&#8221;,  a poorly-defined concept from the 2010s that doesn&#8217;t really make sense in reference to modern language models. Anyone who could possibly be affected by an AI decision (students, patients, employees, etc) must be notified about the existence of the AI, its inputs and methods, and given an opportunity to appeal any decision which goes against them (for example, if a business used AI when deciding not to hire a job candidate). </p><p>In the three-part division that we discussed earlier, the Colorado act most affects the application layer. Instead of imposing a fixed per-training run cost on trillion-dollar companies that don&#8217;t care, it places a constant miasma of fear and bureaucracy over small businesses and nonprofits. Some end users might never adopt AI at all. Some startups might be strangled in their infancy. Some niches might end up dominated by one big company with a good legal team that establishes itself as &#8220;standard of care&#8221; and keeps customers too afraid of regulatory consequences to try anything else. None of this is easy to measure in compute costs, nor does a compute advantage necessarily counterbalance it.</p><p>China is relying on this. They know they can&#8217;t compete on the compute and model layers in the near-term<a class=\"footnote-anchor\" href=\"https://www.astralcodexten.com/feed#footnote-6\" id=\"footnote-anchor-6\" target=\"_self\">6</a>, so they&#8217;re hoping to win on applications. They imagine America having a slightly better model - GPT-7 instead of GPT-6 - but our GPT-7 is sitting in a data center answering user questions and generating porn, while their GPT-6 is helping to run schools, optimize factories, and pilot drones. America&#8217;s task isn&#8217;t micro-optimizing our already large compute and model advantages - gunning to bring the score to GPT-7.01 vs. GPT-6. It&#8217;s responding to the application-layer challenge that China has set us.</p><p>AI safety only tangentially intersects the application layer. There&#8217;s no sense in which schools and hospitals need to be doing yearly impact assessments to see whether they have created a hostile superintelligence. Aside from the AI companies themselves, our interest in end users is limited to those who control weapons of mass destruction - biohazard labs, nuclear missile silos, and the like. These institutions should harden themselves against AI attack. All our other asks are concentrated on the model layer, where China isn&#8217;t interested in competing and the American position is already strong.</p><h2>But What If I Really Care About A 1% Model-Layer Gap?</h2><p>One might argue that every little bit helps. Even though I claim that AI safety regulation only increases training costs by 1%, maybe I&#8217;m off by an order of magnitude and it&#8217;s 10%, and maybe there will be ten things like that, and when you combine them all then we&#8217;re getting to things that might genuinely tip close races. What then?</p><p>Here it&#8217;s helpful to zoom out and look at the scale of other issues that affect the US-China AI balance, of which the most important is export controls.</p><p>America&#8217;s biggest advantage in the AI race is our superior chips, which provide the 10x compute advantage mentioned above. Until about 2023, we had few export controls on these. China bought them up and used them to power their own AI industry.</p><p>In 2023, the US realized it was in an AI race with China and slashed chip exports. Chinese access to compute dropped dramatically. They began accelerating onshore chip development, but this will take a decade or more to pay off. For now, the Chinese AIs you&#8217;ve heard of - DeepSeek, Kimi, etc - are primarily trained on a combination of stockpiled American chips from before the export controls, and American chips smuggled in through third parties, especially Singapore and Malaysia.</p><p>Institute For Progress has a great report <a href=\"https://ifp.org/the-b30a-decision/\">analyzing the stakes</a>. They project how much compute each country will add in 2026. </p><div class=\"captioned-image-container\"><figure><a class=\"image-link image2\" href=\"https://substackcdn.com/image/fetch/$s_!cOWV!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4e50a403-b46d-4f95-a834-97bc39600a23_703x130.png\" target=\"_blank\"><div class=\"image2-inset\"><source type=\"image/webp\" /><img alt=\"\" class=\"sizing-normal\" height=\"119.27453769559033\" src=\"https://substackcdn.com/image/fetch/$s_!cOWV!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4e50a403-b46d-4f95-a834-97bc39600a23_703x130.png\" width=\"645\" /><div></div></div></a></figure></div><p>Because our compute advantage keeps growing, we look better in flows than stocks: in a world without smuggling, America adds 31x more compute than China next year. But if China can continue to smuggle at their accustomed rate, the lead collapses from 31x to 10x.</p><p>If the US knows about Chinese chip smuggling strategies, why can&#8217;t it crack down? The main barriers are a combination of corporate lobbying and poor funding. That is, chip companies want to continue to sell to Singapore and Malaysia without too many awkward questions about where the chips end up. And the Bureau of Industry and Security, the government department charged with countering smuggling, gets about $50 million/year to spend on chips, which experts say is not enough to plug all the holes. To put that number in context, Mark Zuckerberg recently made job offers as high as $1 billion <em>per AI researcher</em>. If America cared about winning the race against China even a tenth as much as Mark Zuckerberg cares about winning the race against OpenAI, we would be in a much better position!</p><p>It gets worse. NVIDIA, America&#8217;s biggest company, constantly lobbies to be allowed to sell its advanced chips to China. It&#8217;s not afraid to play dirty, and stands accused of trying to get China hawks pushed out of government for resisting; <a href=\"https://stevenadler.substack.com/p/the-45-trillion-dollar-elephant-in\">Steven Adler reports </a>&#8220;widespread fear among think tank researchers and policy experts who publish work against NVIDIA&#8217;s interests&#8221;. Foundation for American Innovation fellow David Cowan goes further, saying that <a href=\"https://www.compactmag.com/article/nvidia-is-a-national-security-risk/\">&#8220;NVIDIA is a national security risk&#8221;</a>.</p><p>All of this lobbying has paid off: the administration <a href=\"https://www.theguardian.com/world/2025/aug/12/nvidia-chip-china-sale-trump-blackwell\">keeps proposing changing the rules to allow direct chip sales to China</a>. So far cooler heads have prevailed each time, but the <a href=\"https://www.reuters.com/world/china/trump-weighing-advanced-nvidia-chip-sales-china-bloomberg-news-reports-2025-11-24/\">deal keeps popping back onto the table</a>. NVIDIA tries to argue that the models being proposed for export are only second-rate chips that won&#8217;t affect the compute balance, but this is false - last month&#8217;s talks involved <a href=\"https://ifp.org/the-b30a-decision/#the-b30a-has-similar-price-performance-to-the-b300\">the most price-performant chip on the market</a>. Here&#8217;s IFP&#8217;s calculation for how caving on this issue would affect the AI race:</p><div class=\"captioned-image-container\"><figure><a class=\"image-link image2 is-viewable-img\" href=\"https://substackcdn.com/image/fetch/$s_!8n99!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fce4f74e8-37fb-4173-a806-3e5802f0cba3_703x290.png\" target=\"_blank\"><div class=\"image2-inset\"><source type=\"image/webp\" /><img alt=\"\" class=\"sizing-normal\" height=\"290\" src=\"https://substackcdn.com/image/fetch/$s_!8n99!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fce4f74e8-37fb-4173-a806-3e5802f0cba3_703x290.png\" width=\"703\" /><div class=\"image-link-expand\"><div class=\"pencraft pc-display-flex pc-gap-8 pc-reset\"><button class=\"pencraft pc-reset pencraft icon-container restack-image\" tabindex=\"0\" type=\"button\"><svg class=\"lucide lucide-refresh-cw\" fill=\"none\" height=\"20\" stroke=\"currentColor\" stroke-linecap=\"round\" stroke-linejoin=\"round\" stroke-width=\"2\" viewBox=\"0 0 24 24\" width=\"20\" xmlns=\"http://www.w3.org/2000/svg\"><path d=\"M3 12a9 9 0 0 1 9-9 9.75 9.75 0 0 1 6.74 2.74L21 8\"></path><path d=\"M21 3v5h-5\"></path><path d=\"M21 12a9 9 0 0 1-9 9 9.75 9.75 0 0 1-6.74-2.74L3 16\"></path><path d=\"M8 16H3v5\"></path></svg></button><button class=\"pencraft pc-reset pencraft icon-container view-image\" tabindex=\"0\" type=\"button\"><svg class=\"lucide lucide-maximize2 lucide-maximize-2\" fill=\"none\" height=\"20\" stroke=\"currentColor\" stroke-linecap=\"round\" stroke-linejoin=\"round\" stroke-width=\"2\" viewBox=\"0 0 24 24\" width=\"20\" xmlns=\"http://www.w3.org/2000/svg\"><polyline points=\"15 3 21 3 21 9\"></polyline><polyline points=\"9 21 3 21 3 15\"></polyline><line x1=\"21\" x2=\"14\" y1=\"3\" y2=\"10\"></line><line x1=\"3\" x2=\"10\" y1=\"21\" y2=\"14\"></line></svg></button></div></div></div></a></figure></div><p>It would decrease our compute advantage from 10-30x to about 2x. You can read the report for more scenarios, including one where aggressive chip exports actually give China a compute <em>advantage</em>.</p><p>Commentators have struggled to describe how bad an idea this is. Some say it would be like selling Russia our nukes during the Cold War, or selling them our Saturn V rockets during the space race. The problem isn&#8217;t just that Russia gets free rockets. It&#8217;s also that every rocket we sell to Russia is one that we can&#8217;t use ourselves. We&#8217;re crippling our own capacity in order to enrich our rivals.</p><p>Yet some of the loudest voices warning against AI safety regulation on &#8220;race with China&#8221; grounds <em>support</em> NVIDIA chip exports! For example, White House &#8220;AI and crypto czar&#8221; David Sacks, a strident opponent of AI safety regulation, <a href=\"https://www.nytimes.com/2025/07/17/technology/nvidia-trump-ai-chips-china.html\">has been instrumental in trying to dismantle export controls and anti-smuggling efforts</a>. According to NYT:</p><blockquote><p>Mr. Sacks disliked another Biden administration rule that controlled A.I. chip sales around the world. He also questioned Washington&#8217;s consensus that selling A.I. chips abroad would be bad for the United States.</p></blockquote><p>Some people argue that giving China our chips prevents them from learning to make their own. I think this is historically naive: has giving China our advanced technology <em>ever</em> worked before? &#8220;Maybe letting China access our technology will open up new markets for American goods&#8221; is the &#8220;maybe the stripper really likes you&#8221; of international trade. We have tried this for decades; every time, China has stolen the tech and made their own, better versions. China is obsessed with autarky - the idea that after a &#8220;century of humiliations&#8221;, they shouldn&#8217;t depend industrially on any outside power. They aren&#8217;t going to give up on chip manufacturing, a vital dual-use technology. We shouldn&#8217;t blow our entire present-day AI lead in the hopes that China will do the thing which it has never done once in history and which its entire industrial culture is centered around not doing. If we give them chips, they&#8217;ll both use our chips <em>and</em> develop their own (remember, China is a command economy, and they don&#8217;t have to stop developing their own chips just because there&#8217;s a lower-cost option). Then they&#8217;ll use their AIs, built with our chips, to compete with American AIs on the international market. </p><p>Others argue that chip sanctions just encourage China to be smarter and more compute-efficient, and that we&#8217;ll regret training them into a scrappy battle-hardened colossus. I think this is insulting to American and Chinese researchers, who are already working maximally hard to discover efficiency improvements regardless of our relative compute standing. More important, it doesn&#8217;t seem to be true - Chinese AIs are <a href=\"https://www.seangoedecke.com/is-deepseek-fast/\">no more compute-efficient than American models</a>, with most claims to the contrary being failures of chip accounting. I&#8217;m not even sure the people making this argument believe their own claims. When I play devil&#8217;s advocate and ask them whether America should perhaps pass lots of AI safety regulations a hundred times stricter than the ones actually under consideration - since that would increase training costs, reduce the number of chips we can afford, and cripple us in the same way that chip sanctions cripple China - these people suddenly forget about their bad-things-are-good argument and go back to believing that bad things are bad again.</p><p>A final argument for chip exports: right now, chip autarky is something like China&#8217;s number five national priority. But if our AI lead becomes too great, they might increase it to number one, and catch up quicker. If we allow some chip exports to China, we can keep our lead modest, and prevent them from panicking and working even harder to catch up. This is too 4D chess for me - we have to keep our lead small now so it can be bigger later? But again, if you support keeping our lead small to avoid scaring China, you can&#8217;t turn around and say you&#8217;re against AI safety regulation because it might shrink our lead!</p><p>Absent galaxy-brained takes like these, reducing our 30x compute advantage relative to China to a 1.7x compute advantage is extremely bad - orders of magnitude worse than any safety regulation. So why do so many of the same people who panic over AI safety regulation - who call us &#8220;traitors&#8221; for even considering it- completely fail to talk about the export situation at all, or engage with it in dumb and superficial ways?</p><p>I don&#8217;t think this combination of positions comes from a sober analysis of the AI race. I think people have narratives they want to tell about government, regulation and safetyism, and the AI safety movement - which has &#8220;safety&#8221; right in the name! - makes a convenient villain. The topics that really matter, like export controls, don&#8217;t lend themselves to these stories equally well - you would have to support something with &#8220;controls&#8221; right in the name!<a class=\"footnote-anchor\" href=\"https://www.astralcodexten.com/feed#footnote-7\" id=\"footnote-anchor-7\" target=\"_self\">7</a> - so they get pushed to the sideline.</p><p>But the people who care most about the race against China are focusing most of their energy on export controls, some energy on application-layer regulations like the one in Colorado, and barely think about AI safety at all.</p><h2>It&#8217;s Too Early To Even Know The Sign Of AI Safety Regulations</h2><p>Narratives about regulation stifling progress are attractive because they are often true. A time may come when the Overton Window shifts to a set of AI safety regulations strong enough to substantially slow American AI. Perhaps this will happen at the same time that China finally solves its own chip shortage - likely sometime in the 2030s - and America can no longer rely on its compute advantage for breathing room. Then the threat of Chinese ascendancy will be a relevant response to concerns about safety. Perhaps people raising these arguments now believe that they are protecting themselves against that future - better to cut the safety movement out at its root, before it starts to really matter. Be that as it may, their public communications present the case that AI safety regulation is already a big threat. This is false, and should be called out as such.</p><p>But also, it&#8217;s based on a flawed idea that the only way AI safety can affect the race with China is to slow us down. I&#8217;ve already argued that the magnitude of any deceleration is trivial. But I&#8217;ll go further and say it&#8217;s too early even to know what the <em>sign</em> of AI safety regulations is; whether they might actually <em>speed us up</em> relative to China.</p><p>First, safety-inspired regulation is leading the way in keeping data centers secure. Secure data centers prevent hostile AIs from hacking their way out, but they also prevent Chinese spies from hacking their way in<em>. </em>The safety-inspired SB 53 is the strictest AI cybersecurity regulation on the books, demanding that companies report &#8220;cybersecurity practices and how the large developer secures unreleased model weights from unauthorized modification or transfer by internal or external parties.&#8221; So far, no other political actor has been equally interested in the types of measures that would prevent the Chinese from stealing US secrets and model weights; this is a key factor in developing a model-layer lead.</p><p>Second, safetyists are pushing for compute governance: tags on chips that let governments track their location and use. This would be a key technology for monitoring any future international pause, but incidentally would also make it much easier to end smuggling and prevent the slow trickle of American chips to Chinese companies.</p><p>Third, China is having its own debate over whether it can prioritize safety without losing the race against America! See for example <a href=\"https://time.com/7308857/china-isnt-ignoring-ai-regulation-the-u-s-shouldnt-either/\">TIME - China Is Taking AI Safety Seriously. So Must The US.</a> If America signals that it takes safety seriously, this might give pro-safety Chinese factions more room to operate on their side of the ocean, leaving both countries better off.</p><p>Finally, small regulations now could prevent bigger regulations later. In the wake of a catastrophe, governments over-react. If something went wrong with AI - even something very small, like a buggy AI inserting deliberate malware into code that brought down a few websites, or a terrorist group using an AI-assisted bioweapon to make a handful of people sick - the resulting panic could affect the AI industry the same way 9/11 affected aviation. If safety regulations halve the likelihood of a near-term catastrophe at the cost of adding 1% to training runs, it&#8217;s probably worth it.</p><p>More generally, industry leaders tend to play up how much they want to win the race with China when it&#8217;s convenient for them - for example, as a way of avoiding regulation - then turn around and sell China our top technology when it serves their bottom line. Safetyists may have some other priorities layered on top, but we <em>actually</em> want to win the race with China, because a full appreciation of the potential of superintelligence produces a natural reluctance to let it fall into the hands of dictators. <a href=\"https://archive.is/7bSYM\">A recent </a><em><a href=\"https://archive.is/7bSYM\">Washington Examiner</a></em><a href=\"https://archive.is/7bSYM\"> article</a> pointed to &#8220;effective altruists&#8221; in DC as responsible for some of the strongest bills aimed at preserving American AI supremacy, both during the last administration and the current one. </p><p>When the wind changes, and the position of industry leaders changes with it, you may be glad to have us around.</p><div class=\"footnote\"><a class=\"footnote-number\" contenteditable=\"false\" href=\"https://www.astralcodexten.com/feed#footnote-anchor-1\" id=\"footnote-1\" target=\"_self\">1</a><div class=\"footnote-content\"><p>For purposes of this post, I am accepting the race framework as a given; for challenges to it, <a href=\"https://stevenadler.substack.com/p/contain-and-verify-the-endgame-of\">see eg here</a>.</p></div></div><div class=\"footnote\"><a class=\"footnote-number\" contenteditable=\"false\" href=\"https://www.astralcodexten.com/feed#footnote-anchor-2\" id=\"footnote-2\" target=\"_self\">2</a><div class=\"footnote-content\"><p>This section comes mostly from personal conversations, but is pretty similar to the conclusions of <a href=\"https://secondthoughts.ai/p/what-i-saw-around-the-curve\">Nathan Barnard</a> and <a href=\"https://www.hyperdimensional.co/p/the-bitter-lessons\">Dean Ball</a>.</p></div></div><div class=\"footnote\"><a class=\"footnote-number\" contenteditable=\"false\" href=\"https://www.astralcodexten.com/feed#footnote-anchor-3\" id=\"footnote-3\" target=\"_self\">3</a><div class=\"footnote-content\"><p>Especially in hardware applications. The US has a good software ecosystem, and more advanced models might let us keep an edge in AI-enabled software applications like Cursor.</p></div></div><div class=\"footnote\"><a class=\"footnote-number\" contenteditable=\"false\" href=\"https://www.astralcodexten.com/feed#footnote-anchor-4\" id=\"footnote-4\" target=\"_self\">4</a><div class=\"footnote-content\"><p>With the notable exception of Liang Wenfeng, CEO of DeepSeek. This is maybe not so different from the US, where tech company CEOs believe in superintelligence while the government tends towards more practical near-term thinking. But in America, companies are more influential relative to government than in China. In particular, DeepSeek is much poorer than the American tech giants and has little access to VC funding. So where the US tech giants can engage in massive data center buildup on their own, a similar capex push in China will need to be led by the government.</p></div></div><div class=\"footnote\"><a class=\"footnote-number\" contenteditable=\"false\" href=\"https://www.astralcodexten.com/feed#footnote-anchor-5\" id=\"footnote-5\" target=\"_self\">5</a><div class=\"footnote-content\"><p>It&#8217;s more complicated than this, because the US is in a stage of the race where it&#8217;s mostly working on building AIs, and China is in a stage of the race where it&#8217;s mostly working on developing chips. If a treaty bans both sides from building AI, China can still develop its chips, and be in a better place vis-a-vis the United States when the treaty ends than when it began. A truly fair treaty would have to either wait until China had finished developing its chips and was also in the building-AI stage of the race (5-10 years), or place restrictions on Chinese chip development, or otherwise compensate the US for this asymmetry.</p></div></div><div class=\"footnote\"><a class=\"footnote-number\" contenteditable=\"false\" href=\"https://www.astralcodexten.com/feed#footnote-anchor-6\" id=\"footnote-6\" target=\"_self\">6</a><div class=\"footnote-content\"><p>It will take until about 2035 for China to be able to seriously compete on compute. After that, they most likely end up with a large compute <em>advantage</em> due to their superior manufacturing base, energy infrastructure, state capacity, and lack of NVIDIA profit margins (see footnote 7 below). If America doesn&#8217;t have superintelligence by then, we are in trouble. I don&#8217;t know of anyone who has a great plan for this besides trying to improve on all these fronts, and I also don&#8217;t have a great plan for this.</p></div></div><div class=\"footnote\"><a class=\"footnote-number\" contenteditable=\"false\" href=\"https://www.astralcodexten.com/feed#footnote-anchor-7\" id=\"footnote-7\" target=\"_self\">7</a><div class=\"footnote-content\"><p>Future developments may threaten these people&#8217;s China hawkery even further. NVIDIA has a 90% profit margin on every advanced chip sold in the US. China is still working on developing advanced chips, but once they get them, the government will make Huawei sell at minimal profit margins, to support the national interest of winning the AI race. That means that at technological parity, US chips will cost 10x Chinese chips, and it may become a live topic of debate whether the US government should force NVIDIA to cut its own profit margins. I can only vaguely predict who will take which side of these debate, but I bet it won&#8217;t line up with current levels of China hawkery.</p></div></div>"
            ],
            "link": "https://www.astralcodexten.com/p/why-ai-safety-wont-make-america-lose",
            "publishedAt": "2025-11-26",
            "source": "SlateStarCodex",
            "summary": "<p>If we worry too much about AI safety, will this make us &#8220;lose the race with China&#8221;<a class=\"footnote-anchor\" href=\"https://www.astralcodexten.com/feed#footnote-1\" id=\"footnote-anchor-1\" target=\"_self\">1</a>?</p><p>(here &#8220;AI safety&#8221; means long-term concerns about alignment and hostile superintelligence, as opposed to &#8220;AI ethics&#8221; concerns like bias or intellectual property.)</p><p>Everything has tradeoffs, regulation vs. progress is a common dichotomy, and the more important you think AI will be, the more important it is that the free world get it first. If you believe in superintelligence, the technological singularity, etc, then you think AI is maximally important, and this issue ought to be high on your mind.</p><p>But when you look at this concretely, it becomes clear that this is too small to matter - so small that even the sign is uncertain.</p><h2>The State Of The Race</h2><p>We can divide the AI race into three levels: <strong>compute</strong>, <strong>models</strong>, and <strong>applications</strong><a class=\"footnote-anchor\" href=\"https://www.astralcodexten.com/feed#footnote-2\" id=\"footnote-anchor-2\" target=\"_self\">2</a>. Companies use compute - chips deployed in data centers - to train models like GPT and Claude. Then they use those models in various applications. For now, those applications are things like Internet search and image generation. In the future, they might become geopolitically relevant fields like manufacturing and weapons systems.</p><p><strong>Compute</strong>: America is far ahead. We have",
            "title": "Why AI Safety Won't Make America Lose The Race With China"
        },
        {
            "content": [
                "<p>There remain lots of great charitable giving opportunities out there.</p>\n<p>I have now had three opportunities to be a recommender for the Survival and Flourishing Fund (SFF).<a href=\"https://thezvi.substack.com/p/zvis-thoughts-on-the-survival-and?utm_source=publication-search\"> I wrote in detail about my first experienc</a>e back in 2021, where I struggled to find worthy applications.</p>\n<p>The second time around in 2024, there was an abundance of worthy causes. In 2025 there were even more high quality applications, many of which were growing beyond our ability to support them.</p>\n<p>Thus this is the second edition of The Big Nonprofits Post, primarily aimed at sharing my findings on various organizations I believe are doing good work, to help you find places to consider donating in the cause areas and intervention methods that you think are most effective, and to offer my general perspective on how I think about choosing where to give.</p>\n<div>\n\n\n<span id=\"more-24887\"></span>\n\n\n</div>\n<p>This post combines my findings from the 2024 and 2025 rounds of SFF, and also includes some organizations that did not apply to either round, so inclusion does not mean that they necessarily applied at all.</p>\n<p>This post is already very long, so the bar is higher for inclusion this year than it was last year, especially for new additions.</p>\n<p>If you think they are better places to give and better causes to back, act accordingly, especially if they\u2019re illegible or obscure. You don\u2019t need my approval.</p>\n<p><a href=\"https://nonprofits.zone/\"><strong>The Big Nonprofits List 2025 is also available as a website</strong></a>, where you can sort by mission, funding needed or confidence, or do a search and have handy buttons.</p>\n\n\n<h4 class=\"wp-block-heading\">Table of Contents</h4>\n\n\n<p>Organizations where I have the highest confidence in straightforward modest donations now, if your goals and model of the world align with theirs, are in bold, for those who don\u2019t want to do a deep dive.</p>\n<ol>\n<li><a href=\"https://thezvi.substack.com/i/173386103/table-of-contents\">Table of Contents.</a></li>\n<li><a href=\"https://thezvi.substack.com/i/173386103/a-word-of-warning\">A Word of Warning.</a></li>\n<li><a href=\"https://thezvi.substack.com/i/173386103/a-note-to-charities\">A Note To Charities.</a></li>\n<li><a href=\"https://thezvi.substack.com/i/173386103/use-your-personal-theory-of-impact\">Use Your Personal Theory of Impact.</a></li>\n<li><a href=\"https://thezvi.substack.com/i/173386103/use-your-local-knowledge\">Use Your Local Knowledge.</a></li>\n<li><a href=\"https://thezvi.substack.com/i/173386103/unconditional-grants-to-worthy-individuals-are-great\">Unconditional Grants to Worthy Individuals Are Great.</a></li>\n<li><a href=\"https://thezvi.substack.com/i/173386103/do-not-think-only-on-the-margin-and-also-use-decision-theory\">Do Not Think Only On the Margin, and Also Use Decision Theory.</a></li>\n<li><a href=\"https://thezvi.substack.com/i/173386103/compare-notes-with-those-individuals-you-trust\">Compare Notes With Those Individuals You Trust.</a></li>\n<li><a href=\"https://thezvi.substack.com/i/173386103/beware-becoming-a-fundraising-target\">Beware Becoming a Fundraising Target.</a></li>\n<li><a href=\"https://thezvi.substack.com/i/173386103/and-the-nominees-are\">And the Nominees Are.</a></li>\n<li><a href=\"https://thezvi.substack.com/i/173386103/organizations-that-are-literally-me\">Organizations that Are Literally Me.</a></li>\n<li><a href=\"https://thezvi.substack.com/i/173386103/balsa-research\"><strong>Balsa Research</strong>.</a></li>\n<li><a href=\"https://thezvi.substack.com/i/173386103/don-t-worry-about-the-vase\"><strong>Don\u2019t Worry About the Vase</strong>.</a></li>\n<li><a href=\"https://thezvi.substack.com/i/173386103/organizations-focusing-on-ai-non-technical-research-and-education\">Organizations Focusing On AI Non-Technical Research and Education.</a></li>\n<li><a href=\"https://thezvi.substack.com/i/173386103/lightcone-infrastructure\"><strong>Lightcone Infrastructure</strong>.</a></li>\n<li><a href=\"https://thezvi.substack.com/i/173386103/the-ai-futures-project\">The AI Futures Project.</a></li>\n<li><a href=\"https://thezvi.substack.com/i/173386103/effective-institutions-project-eip-for-their-flagship-initiatives\"><strong>Effective Institutions Project (EIP) (For Their Flagship Initiatives)</strong>.</a></li>\n<li><a href=\"https://thezvi.substack.com/i/173386103/artificial-intelligence-policy-institute-aipi\"><strong>Artificial Intelligence Policy Institute (AIPI)</strong>.</a></li>\n<li><a href=\"https://thezvi.substack.com/i/173386103/palisade-research\"><strong>Palisade Research</strong>.</a></li>\n<li><a href=\"https://thezvi.substack.com/i/173386103/ai-safety-info-robert-miles\"><strong>AI Safety Info (Robert Miles)</strong>.</a></li>\n<li><a href=\"https://thezvi.substack.com/i/173386103/intelligence-rising\">Intelligence Rising.</a></li>\n<li><a href=\"https://thezvi.substack.com/i/173386103/convergence-analysis\">Convergence Analysis.</a></li>\n<li><a href=\"https://thezvi.substack.com/i/173386103/organizations-related-to-potentially-pausing-ai-or-otherwise-having-a-strong-international-ai-treaty\">Organizations Related To Potentially Pausing AI Or Otherwise Having A Strong International AI Treaty.</a></li>\n<li><a href=\"https://thezvi.substack.com/i/173386103/pause-ai-and-pause-ai-global\">Pause AI and Pause AI Global.</a></li>\n<li><a href=\"https://thezvi.substack.com/i/173386103/miri\"><strong>MIRI.</strong></a></li>\n<li><a href=\"https://thezvi.substack.com/i/173386103/existential-risk-observatory\">Existential Risk Observatory.</a></li>\n<li><a href=\"https://thezvi.substack.com/i/173386103/organizations-focusing-primary-on-ai-policy-and-diplomacy\">Organizations Focusing Primary On AI Policy and Diplomacy.</a></li>\n<li><a href=\"https://thezvi.substack.com/i/173386103/center-for-ai-safety-and-the-cais-action-fund\"><strong>Center for AI Safety and the CAIS Action Fund</strong>.</a></li>\n<li><a href=\"https://thezvi.substack.com/i/173386103/foundation-for-american-innovation-fai\"><strong>Foundation for American Innovation (FAI)</strong>.</a></li>\n<li><a href=\"https://thezvi.substack.com/i/173386103/encode-ai-formerly-encode-justice\">Encode AI (Formerly Encode Justice).</a></li>\n<li><a href=\"https://thezvi.substack.com/i/173386103/the-future-society\">The Future Society.</a></li>\n<li><a href=\"https://thezvi.substack.com/i/173386103/safer-ai\">Safer AI.</a></li>\n<li><a href=\"https://thezvi.substack.com/i/173386103/institute-for-ai-policy-and-strategy-iaps\"><strong>Institute for AI Policy and Strategy (IAPS).</strong></a></li>\n<li><a href=\"https://thezvi.substack.com/i/173386103/ai-standards-lab\">AI Standards Lab.</a></li>\n<li><a href=\"https://thezvi.substack.com/i/173386103/safer-ai-forum\">Safer AI Forum.</a></li>\n<li><a href=\"https://thezvi.substack.com/i/173386103/center-for-long-term-resilience-at-founders-pledge\">Center For Long Term Resilience at Founders Pledge.</a></li>\n<li><a href=\"https://thezvi.substack.com/i/173386103/simon-institute-for-longterm-governance\">Simon Institute for Longterm Governance.</a></li>\n<li><a href=\"https://thezvi.substack.com/i/173386103/legal-advocacy-for-safe-science-and-technology\">Legal Advocacy for Safe Science and Technology.</a></li>\n<li><a href=\"https://thezvi.substack.com/i/173386103/organizations-doing-ml-alignment-research\">Organizations Doing ML Alignment Research.</a></li>\n<li><a href=\"https://thezvi.substack.com/i/173386103/model-evaluation-and-threat-research-metr\"><strong>Model Evaluation and Threat Research (METR)</strong>.</a></li>\n<li><a href=\"https://thezvi.substack.com/i/173386103/alignment-research-center-arc\">Alignment Research Center (ARC).</a></li>\n<li><a href=\"https://thezvi.substack.com/i/173386103/apollo-research\"><strong>Apollo Research</strong>.</a></li>\n<li><a href=\"https://thezvi.substack.com/i/173386103/cybersecurity-lab-at-university-of-louisville\">Cybersecurity Lab at University of Louisville.</a></li>\n<li><a href=\"https://thezvi.substack.com/i/173386103/timaeus\">Timaeus.</a></li>\n<li><a href=\"https://thezvi.substack.com/i/173386103/simplex\">Simplex.</a></li>\n<li><a href=\"https://thezvi.substack.com/i/173386103/far-ai\">Far AI.</a></li>\n<li><a href=\"https://thezvi.substack.com/i/173386103/alignment-in-complex-systems-research-group\">Alignment in Complex Systems Research Group.</a></li>\n<li><a href=\"https://thezvi.substack.com/i/173386103/apart-research\">Apart Research.</a></li>\n<li><a href=\"https://thezvi.substack.com/i/173386103/transluce\">Transluce.</a></li>\n<li><a href=\"https://thezvi.substack.com/i/173386103/organizations-doing-math-decision-theory-and-agent-foundations\">Organizations Doing Math, Decision Theory and Agent Foundations.</a></li>\n<li><a href=\"https://thezvi.substack.com/i/173386103/orthogonal\"><strong>Orthogonal</strong>.</a></li>\n<li><a href=\"https://thezvi.substack.com/i/173386103/topos-institute\"><strong>Topos Institute</strong>.</a></li>\n<li><a href=\"https://thezvi.substack.com/i/173386103/eisenstat-research\">Eisenstat Research.</a></li>\n<li><a href=\"https://thezvi.substack.com/i/173386103/affine-algorithm-design\">AFFINE Algorithm Design.</a></li>\n<li><a href=\"https://thezvi.substack.com/i/173386103/coral-computational-rational-agents-laboratory\">CORAL (Computational Rational Agents Laboratory).</a></li>\n<li><a href=\"https://thezvi.substack.com/i/173386103/mathematical-metaphysics-institute\">Mathematical Metaphysics Institute.</a></li>\n<li><a href=\"https://thezvi.substack.com/i/173386103/focal-at-cmu\">Focal at CMU.</a></li>\n<li><a href=\"https://thezvi.substack.com/i/173386103/organizations-doing-cool-other-stuff-including-tech\">Organizations Doing Cool Other Stuff Including Tech.</a></li>\n<li><a href=\"https://thezvi.substack.com/i/173386103/allfed\">ALLFED.</a></li>\n<li><a href=\"https://thezvi.substack.com/i/173386103/good-ancestor-foundation\">Good Ancestor Foundation.</a></li>\n<li><a href=\"https://thezvi.substack.com/i/173386103/charter-cities-institute\">Charter Cities Institute.</a></li>\n<li><a href=\"https://thezvi.substack.com/i/173386103/carbon-copies-for-independent-minds\">Carbon Copies for Independent Minds.</a></li>\n<li><a href=\"https://thezvi.substack.com/i/173386103/organizations-focused-primarily-on-bio-risk\">Organizations Focused Primarily on Bio Risk. (Blank)</a></li>\n<li><a href=\"https://thezvi.substack.com/i/173386103/secure-dna\">Secure DNA.</a></li>\n<li><a href=\"https://thezvi.substack.com/i/173386103/blueprint-biosecurity\">Blueprint Biosecurity.</a></li>\n<li><a href=\"https://thezvi.substack.com/i/173386103/pour-domain\">Pour Domain.</a></li>\n<li><a href=\"https://thezvi.substack.com/i/173386103/organizations-that-can-advise-you-further\">Organizations That Can Advise You Further.</a></li>\n<li><a href=\"https://thezvi.substack.com/i/173386103/effective-institutions-project-eip-as-a-donation-advisor\">Effective Institutions Project (EIP) (As A Donation Advisor).</a></li>\n<li><a href=\"https://thezvi.substack.com/i/173386103/longview-philanthropy\">Longview Philanthropy.</a></li>\n<li><a href=\"https://thezvi.substack.com/i/173386103/organizations-that-then-regrant-to-fund-other-organizations\">Organizations That then Regrant to Fund Other Organizations.</a></li>\n<li><a href=\"https://thezvi.substack.com/i/173386103/sff-itself\"><strong>SFF Itself (!)</strong>.</a></li>\n<li><a href=\"https://thezvi.substack.com/i/173386103/manifund\">Manifund.</a></li>\n<li><a href=\"https://thezvi.substack.com/i/173386103/ai-risk-mitigation-fund\">AI Risk Mitigation Fund.</a></li>\n<li><a href=\"https://thezvi.substack.com/i/173386103/long-term-future-fund\">Long Term Future Fund.</a></li>\n<li><a href=\"https://thezvi.substack.com/i/173386103/foresight\">Foresight.</a></li>\n<li><a href=\"https://thezvi.substack.com/i/173386103/centre-for-enabling-effective-altruism-learning-research-ceelar\">Centre for Enabling Effective Altruism Learning &amp; Research (CEELAR).</a></li>\n<li><a href=\"https://thezvi.substack.com/i/173386103/organizations-that-are-essentially-talent-funnels\">Organizations That are Essentially Talent Funnels.</a></li>\n<li><a href=\"https://thezvi.substack.com/i/173386103/ai-safety-camp\">AI Safety Camp.</a></li>\n<li><a href=\"https://thezvi.substack.com/i/173386103/center-for-law-and-ai-risk\">Center for Law and AI Risk.</a></li>\n<li><a href=\"https://thezvi.substack.com/i/173386103/speculative-technologies\">Speculative Technologies.</a></li>\n<li><a href=\"https://thezvi.substack.com/i/173386103/talos-network\">Talos Network.</a></li>\n<li><a href=\"https://thezvi.substack.com/i/173386103/mats-research\"><strong>MATS Research</strong>.</a></li>\n<li><a href=\"https://thezvi.substack.com/i/173386103/epistea\">Epistea.</a></li>\n<li><a href=\"https://thezvi.substack.com/i/173386103/emergent-ventures\">Emergent Ventures.</a></li>\n<li><a href=\"https://thezvi.substack.com/i/173386103/ai-safety-cape-town\">AI Safety Cape Town.</a></li>\n<li><a href=\"https://thezvi.substack.com/i/173386103/impact-academy-limited\">Impact Academy Limited.</a></li>\n<li><a href=\"https://thezvi.substack.com/i/173386103/atlas-computing\">Atlas Computing.</a></li>\n<li><a href=\"https://thezvi.substack.com/i/173386103/principles-of-intelligence-formerly-pibbss\">Principles of Intelligence (Formerly PIBBSS).</a></li>\n<li><a href=\"https://thezvi.substack.com/i/173386103/tarbell-fellowship-at-ppf\">Tarbell Fellowship at PPF.</a></li>\n<li><a href=\"https://thezvi.substack.com/i/173386103/catalyze-impact\">Catalyze Impact.</a></li>\n<li><a href=\"https://thezvi.substack.com/i/173386103/cesia-within-effisciences\">CeSIA within EffiSciences.</a></li>\n<li><a href=\"https://thezvi.substack.com/i/173386103/stanford-existential-risk-initiative-seri\">Stanford Existential Risk Initiative (SERI).</a></li>\n<li><a href=\"https://thezvi.substack.com/i/173386103/final-reminders\">Final Reminders</a></li>\n</ol>\n<div>\n<figure>\n<div>\n\n\n<figure class=\"wp-block-image\"><img alt=\"A vibrant and heartwarming scene depicting various charity fundraising efforts happening in a community park. People of diverse backgrounds are engaged in activities like a bake sale with colorful desserts displayed on tables, a charity run with participants wearing numbers on their shirts, a live music performance on a small stage, and a silent auction with art pieces on display. Children are running a lemonade stand, while volunteers distribute flyers and collect donations in jars. The atmosphere is lively and festive, with balloons, banners, and smiling faces everywhere.\" src=\"https://substackcdn.com/image/fetch/$s_!exq8!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd76f8a4b-7a15-4d53-921d-9246faa2bdea_1456x832.jpeg\" /></figure>\n\n\n<div></div>\n</div>\n</figure>\n</div>\n\n\n<h4 class=\"wp-block-heading\">A Word of Warning</h4>\n\n\n<p>The SFF recommender process is highly time constrained, and in general I am highly time constrained.</p>\n<p>Even though I used well beyond the number of required hours in both 2024 and 2025, there was no way to do a serious investigation of all the potentially exciting applications. Substantial reliance on heuristics was inevitable.</p>\n<p>Also your priorities, opinions, and world model could be very different from mine.</p>\n<p>If you are considering donating a substantial (to you) amount of money, please do the level of personal research and consideration commensurate with the amount of money you want to give away.</p>\n<p>If you are considering donating a small (to you) amount of money, or if the requirement to do personal research might mean you don\u2019t donate to anyone at all, I caution the opposite: Only do the amount of optimization and verification and such that is worth its opportunity cost. Do not let the perfect be the enemy of the good.</p>\n<p>For more details of how the SFF recommender process works, see<a href=\"https://thezvi.substack.com/p/zvis-thoughts-on-his-2nd-round-of\"> my post</a> on the process.</p>\n<p>Note that donations to some of the organizations below may not be tax deductible.</p>\n\n\n<h4 class=\"wp-block-heading\">A Note To Charities</h4>\n\n\n<p>I apologize in advance for any errors, any out of date information, and for anyone who I included who I did not realize would not want to be included. I did my best to verify information, and to remove any organizations that do not wish to be included.</p>\n<p>If you wish me to issue a correction of any kind, or to update your information, I will be happy to do that at least through the end of the year.</p>\n<p>If you wish me to remove your organization entirely, for any reason, I will do that, too.</p>\n<p>What I unfortunately cannot do, in most cases, is take the time to analyze or debate beyond that. I also can\u2019t consider additional organizations for inclusion. My apologies.</p>\n<p>The same is true <a href=\"https://nonprofits.zone/\"><strong>for the website version</strong></a>.</p>\n<p>I am giving my full opinion on all organizations listed, but where I feel an organization would be a poor choice for marginal dollars even within its own cause and intervention area, or I anticipate my full opinion would not net help them, they are silently not listed.</p>\n\n\n<h4 class=\"wp-block-heading\">Use Your Personal Theory of Impact</h4>\n\n\n<p>Listen to arguments and evidence. But do not let me, or anyone else, tell you any of:</p>\n<ol>\n<li>What is important.</li>\n<li>What is a good cause.</li>\n<li>What types of actions are best to make the change you want to see in the world.</li>\n<li>What particular strategies are most promising.</li>\n<li>That you have to choose according to some formula or you\u2019re an awful person.</li>\n</ol>\n<p>This is especially true when it comes to policy advocacy, and especially in AI.</p>\n<p>If an organization is advocating for what you think is bad policy, or acting in a way that does bad things, don\u2019t fund them!</p>\n<p>If an organization is advocating or acting in a way you think is ineffective, don\u2019t fund them!</p>\n<p>Only fund people you think advance good changes in effective ways.</p>\n<p>Not cases where I think that. Cases where you think that.</p>\n<p>During SFF, I once again in 2025 chose to deprioritize all meta-level activities and talent development. I see lots of good object-level work available to do, and I expected others to often prioritize talent and meta activities.</p>\n<p>The counterargument to this is that quite a lot of money is potentially going to be freed up soon as employees of OpenAI and Anthropic gain liquidity, including access to DAFs (donor advised funds). This makes expanding the pool more exciting.</p>\n<p>I remain primarily focused on those who in some form were helping ensure AI does not kill everyone. I continue to see highest value in organizations that influence lab or government AI policies in the right ways, and continue to value Agent Foundations style and other off-paradigm technical research approaches.</p>\n\n\n<h4 class=\"wp-block-heading\">Use Your Local Knowledge</h4>\n\n\n<p>I believe that the best places to give are the places where you have local knowledge.</p>\n<p>If you know of people doing great work or who could do great work, based on your own information, then you can fund and provide social proof for what others cannot.</p>\n<p>The less legible to others the cause, and the harder it is to fit it into the mission statements and formulas of various big donors, the more excited you should be to step forward, if the cause is indeed legible to you. This keeps you grounded, helps others find the show (as Tyler Cowen says), is more likely to be counterfactual funding, and avoids information cascades or looking under streetlights for the keys.</p>\n<p>Most importantly it avoids adverse selection. The best legible opportunities for funding, the slam dunk choices? Those are probably getting funded. The legible things that are left are the ones that others didn\u2019t sufficiently fund yet.</p>\n<p>If you know why others haven\u2019t funded, because they don\u2019t know about the opportunity? That\u2019s a great trade.</p>\n\n\n<h4 class=\"wp-block-heading\">Unconditional Grants to Worthy Individuals Are Great</h4>\n\n\n<p>The process of applying for grants, raising money, and justifying your existence sucks.</p>\n<p>A lot.</p>\n<p>It especially sucks for many of the creatives and nerds that do a lot of the best work.</p>\n<p>It also sucks to have to worry about running out of money, or to have to plan your work around the next time you have to justify your existence, or to be unable to be confident in choosing ambitious projects.</p>\n<p>If you have to periodically go through this process, and are forced to continuously worry about making your work legible and how others will judge it, that will substantially hurt your true productivity. At best it is a constant distraction. By default, it is a severe warping effect. A version of this phenomenon is doing huge damage to academic science.</p>\n<p>As I noted in my AI updates, the reason this blog exists is that I received generous, essentially unconditional, anonymous support to \u2018be a public intellectual\u2019 and otherwise pursue whatever I think is best. My benefactors offer their opinions when we talk because I value their opinions, but they never try to influence my decisions, and I feel zero pressure to make my work legible in order to secure future funding.</p>\n<p>If you have money to give, and you know individuals who should clearly be left to do whatever they think is best without worrying about raising or earning money, who you are confident would take advantage of that opportunity and try to do something great, then giving them unconditional grants is a great use of funds, including giving them \u2018don\u2019t worry about reasonable expenses\u2019 levels of funding.</p>\n<p>This is especially true when combined with \u2018retrospective funding,\u2019 based on what they have already done. It would be great if we established a tradition and expectation that people who make big contributions can expect such rewards.</p>\n<p>Not as unconditionally, it\u2019s also great to fund specific actions and projects and so on that you see not happening purely through lack of money, especially when no one is asking you for money.</p>\n<p>This includes things that you want to exist, but that don\u2019t have a path to sustainability or revenue, or would be importantly tainted if they needed to seek that. Fund the project you want to see in the world. This can also be purely selfish, often in order to have something yourself you need to create it for everyone, and if you\u2019re tempted there\u2019s a good chance that\u2019s a great value.</p>\n\n\n<h4 class=\"wp-block-heading\">Do Not Think Only On the Margin, and Also Use Decision Theory</h4>\n\n\n<p>Resist the temptation to think purely on the margin, asking only what one more dollar can do. The incentives get perverse quickly. Organizations are rewarded for putting their highest impact activities in peril. Organizations that can \u2018run lean\u2019 or protect their core activities get punished.</p>\n<p>If you always insist on being a \u2018funder of last resort\u2019 that requires key projects or the whole organization otherwise be in trouble, you\u2019re defecting. Stop it.</p>\n<p>Also, you want to do some amount of retrospective funding. If people have done exceptional work in the past, you should be willing to give them a bunch more rope in the future, above and beyond the expected value of their new project.</p>\n<p>Don\u2019t make everyone constantly reprove their cost effectiveness each year, or at least give them a break. If someone has earned your trust, then if this is the project they want to do next, presume they did so because of reasons, although you are free to disagree with those reasons.</p>\n\n\n<h4 class=\"wp-block-heading\">Compare Notes With Those Individuals You Trust</h4>\n\n\n<p>This especially goes for AI lab employees. There\u2019s no need for everyone to do all of their own research, you can and should compare notes with those who you can trust, and this is especially great when they\u2019re people you know well.</p>\n<p>What I do worry about is too much outsourcing of decisions to larger organizations and institutional structures, including those of Effective Altruism but also others, or letting your money go directly to large foundations where it will often get captured.</p>\n\n\n<h4 class=\"wp-block-heading\">Beware Becoming a Fundraising Target</h4>\n\n\n<p>Jaan Tallinn created SFF in large part to intentionally take his donation decisions out of his hands, so he could credibly tell people those decisions were out of his hands, so he would not have to constantly worry that people he talked to were attempting to fundraise.</p>\n<p>This is a huge deal. Communication, social life and a healthy information environment can all be put in danger by this.</p>\n\n\n<h4 class=\"wp-block-heading\">And the Nominees Are</h4>\n\n\n<p>Time to talk about the organizations themselves.</p>\n<p>Rather than offer precise rankings, I divided by cause category and into three confidence levels.</p>\n<ol>\n<li>High confidence means I have enough information to be confident the organization is at least a good pick.</li>\n<li>Medium or low confidence means exactly that &#8211; I have less confidence that the choice is wise, and you should give more consideration to doing your own research.</li>\n<li>If my last investigation was in 2024, and I haven\u2019t heard anything, I will have somewhat lower confidence now purely because my information is out of date.</li>\n</ol>\n<p>Low confidence is still high praise, and very much a positive assessment! Most organizations would come nowhere close to making the post at all.</p>\n<p>If an organization is not listed, that does not mean I think they would be a bad pick &#8211; they could have asked not to be included, or I could be unaware of them or their value, or I could simply not have enough confidence to list them.</p>\n<p>I know how Bayesian evidence works, but this post is not intended as a knock on anyone, in any way. Some organizations that are not here would doubtless have been included, if I\u2019d had more time.</p>\n<p>I try to give a sense of how much detailed investigation and verification I was able to complete, and what parts I have confidence in versus not. Again, my lack of confidence will often be purely about my lack of time to get that confidence.</p>\n<p>Unless I already knew them from elsewhere, assume no organizations here got as much attention as they deserve before you decide on what for you is a large donation.</p>\n<p>I\u2019m tiering based on how I think about donations from you, from outside SFF.</p>\n<p>I think the regranting organizations were clearly wrong choices from within SFF, but are reasonable picks if you don\u2019t want to do extensive research, especially if you are giving small.</p>\n<p>In terms of funding levels needed, I will similarly divide into three categories.</p>\n<p>They roughly mean this, to the best of my knowledge:</p>\n<p>Low: Could likely be fully funded with less than ~$250k.</p>\n<p>Medium: Could plausibly be fully funded with between ~$250k and ~$2 million.</p>\n<p>High: Could probably make good use of more than ~$2 million.</p>\n<p>These numbers may be obsolete by the time you read this. If you\u2019re giving a large amount relative to what they might need, check with the organization first, but also do not be so afraid of modest amounts of \u2018overfunding\u2019 as relieving fundraising pressure is valuable and as I noted it is important not to only think on the margin.</p>\n<p>A lot of organizations are scaling up rapidly, looking to spend far more money than they have in the past. This was true in 2024, and 2025 has only accelerated this trend. A lot more organizations are in \u2018High\u2019 now but I decided not to update the thresholds.</p>\n<p>Everyone seems eager to double their headcount. I\u2019m not putting people into the High category unless I am confident they can scalably absorb more funding after SFF.</p>\n<p>The person who I list as the leader of an organization will sometimes accidentally be whoever was in charge of fundraising rather than strictly the leader. Partly the reason for listing it is to give context and some of you can go \u2018oh right, I know who that is,\u2019 and the other reason is that all organization names are often highly confusing &#8211; adding the name of the organization\u2019s leader allows you a safety check, to confirm that you are indeed pondering the same organization I am thinking of!</p>\n\n\n<h4 class=\"wp-block-heading\">Organizations that Are Literally Me</h4>\n\n\n<p>This is my post, so I get to list Balsa Research first. (I make the rules here.)</p>\n<p>If that\u2019s not what you\u2019re interested in, you can of course skip the section.</p>\n\n\n<h4 class=\"wp-block-heading\">Balsa Research</h4>\n\n\n<p>Focus: Groundwork starting with studies to allow repeal of the Jones Act</p>\n<p>Leader: Zvi Mowshowitz</p>\n<p>Funding Needed: Medium</p>\n<p>Confidence Level: High</p>\n<p>Our first target continues to be<a href=\"https://thezvi.substack.com/p/repeal-the-jones-act-of-1920\"> the Jones Act</a>. With everything happening in 2025, it is easy to get distracted. We have decided to keep eyes on the prize.</p>\n<p>We\u2019ve commissioned two studies. Part of our plan is to do more of them, and also do things like draft model repeals and explore ways to assemble a coalition and to sell and spread the results, to enable us to have a chance at repeal.</p>\n<p>We also are networking, gathering information, publishing findings where there are <a href=\"https://www.balsaresearch.com/jones-act-vessels\">information holes</a> or where we can offer <a href=\"https://www.balsaresearch.com/jones-act-index\">superior presentations</a>, planning possible collaborations, and responding quickly in case of a crisis in related areas. We believe we meaningfully reduced the probability that certain very damaging additional maritime regulations could have become law,<a href=\"https://thezvi.substack.com/p/balsa-update-springtime-in-dc?utm_source=publication-search\"> as described in this post.</a></p>\n<p>Other planned cause areas include NEPA reform and federal housing policy (to build more housing where people want to live).</p>\n<p>We have one full time worker on the case and are trying out a potential second one.</p>\n<p>I don\u2019t intend to have Balsa work on AI or assist with my other work, or to take personal compensation, unless I get substantially larger donations than we have had previously, that are either dedicated to those purposes or that at least come with the explicit understanding I should consider doing that.</p>\n<p>Further donations would otherwise be for general support.</p>\n<p>The pitch for Balsa, and the reason I am doing it, is in two parts.</p>\n<p>I believe Jones Act repeal and many other abundance agenda items are neglected, tractable and important, and that my way of focusing on what matters can advance them. That the basic work that needs doing is not being done, it would be remarkably cheap to do a lot of it and do it well, and that this would give us a real if unlikely chance to get a huge win if circumstances break right. Chances for progress currently look grim, but winds can change quickly, we need to be ready, and also we need to stand ready to mitigate the chance things get even worse.</p>\n<p>I also believe that if people do not have hope for the future, do not have something to protect and fight for, or do not think good outcomes are possible, that people won\u2019t care about protecting the future. And that would be very bad, because we are going to need to fight to protect our future if we want to have one, or have a good one.</p>\n<p>You got to give them hope.</p>\n<p>I could go on, but I\u2019ll stop there.</p>\n<p>Donate <a href=\"https://www.balsaresearch.com/donate\">here</a>, or get in touch at donations@balsaresearch.com.</p>\n\n\n<h4 class=\"wp-block-heading\">Don\u2019t Worry About the Vase</h4>\n\n\n<p>Focus: Zvi Mowshowitz writes a lot of words, really quite a lot.</p>\n<p>Leader: Zvi Mowshowitz</p>\n<p>Funding Needed: None, but it all helps, could plausibly absorb a lot</p>\n<p>Confidence Level: High</p>\n<p>You can also of course always donate directly to my favorite charity.</p>\n<p>By which I mean me. I always appreciate your support, however large or small.</p>\n<p>The easiest way to help on a small scale (of course) is<a href=\"https://thezvi.substack.com/\"> a Substack</a> subscription or<a href=\"https://www.patreon.com/c/thezvi\"> Patreon</a>. Paid substack subscriptions punch above their weight because they assist with the sorting algorithm, and also for their impact on morale.</p>\n<p>If you want to go large then reach out to me.</p>\n<p>Thanks to generous anonymous donors, I am able to write full time and mostly not worry about money. That is what makes this blog possible.</p>\n<p>I want to as always be 100% clear: I am totally, completely fine as is, as is the blog.</p>\n<p>Please feel zero pressure here, as noted throughout there are many excellent donation opportunities out there.</p>\n<p>Additional funds are still welcome. There are levels of funding beyond not worrying.</p>\n<p>Such additional support is always highly motivating.</p>\n<p>Also there are absolutely additional things I could and would throw money at to improve the blog, potentially including hiring various forms of help or even expanding to more of a full news operation or startup.</p>\n\n\n<h4 class=\"wp-block-heading\">Organizations Focusing On AI Non-Technical Research and Education</h4>\n\n\n<p>As a broad category, these are organizations trying to figure things out regarding AI existential risk, without centrally attempting to either do technical work or directly to influence policy and discourse.</p>\n<p>Lightcone Infrastructure is my current top pick across all categories. If you asked me where to give a dollar, or quite a few dollars, to someone who is not me, I would tell you to fund Lightcone Infrastructure.</p>\n\n\n<h4 class=\"wp-block-heading\">Lightcone Infrastructure</h4>\n\n\n<p>Focus: Rationality community infrastructure,<a href=\"https://www.lesswrong.com/\"> LessWrong</a>, the Alignment Forum and<a href=\"https://www.lighthaven.space/\"> Lighthaven</a>.</p>\n<p>Leaders: Oliver Habryka and Rafe Kennedy</p>\n<p>Funding Needed: High</p>\n<p>Confidence Level: High</p>\n<p>Disclaimer: I am on the CFAR board which used to be the umbrella organization for Lightcone and still has some lingering ties. My writing appears on LessWrong. I have long time relationships with everyone involved. I have been to several reliably great workshops or conferences at their campus at<a href=\"https://www.lighthaven.space/\"> Lighthaven</a>. So I am conflicted here.</p>\n<p>With that said, Lightcone is my clear number one. I think they are doing great work, both in terms of LessWrong and also Lighthaven. There is the potential, with greater funding, to enrich both of these tasks, and also for expansion.</p>\n<p>There is a large force multiplier here (although that is true of a number of other organizations I list as well).</p>\n<p><a href=\"https://www.lesswrong.com/posts/5n2ZQcbc7r4R8mvqc/the-lightcone-is-nothing-without-its-people\">They made their 2024 fundraising pitch here, I encourage reading it</a>.</p>\n<p>Where I am beyond confident is that if LessWrong, the Alignment Forum or the venue<a href=\"https://www.lighthaven.space/\"> Lighthaven</a> were unable to continue, any one of these would be a major, quite bad unforced error.</p>\n<p>LessWrong and the Alignment Forum a central part of the infrastructure of the meaningful internet.</p>\n<p>Lighthaven is miles and miles away the best event venue I have ever seen. I do not know how to convey how much the design contributes to having a valuable conference, designed to facilitate the best kinds of conversations via a wide array of nooks and pathways designed with the principles of Christopher Alexander. This contributes to and takes advantage of the consistently fantastic set of people I encounter there.</p>\n<p>The marginal costs here are large (~$3 million per year, some of which is made up by venue revenue), but the impact here is many times that, and I believe they can take on more than ten times that amount and generate excellent returns.</p>\n<p>If we can go beyond short term funding needs, they can pay off the mortgage to secure a buffer, and buy up surrounding buildings to secure against neighbors (who can, given this is Berkeley, cause a lot of trouble) and to secure more housing and other space. This would secure the future of the space.</p>\n<p>I would love to see them then expand into additional spaces. They note this would also require the right people.</p>\n<p>Donate through <a href=\"https://www.every.org/lightcone-infrastructure?donateTo=lightcone-infrastructure#/donate/\">every.org</a>, or contact <a href=\"mailto:team@lesswrong.com\">team@lesswrong.com</a>.</p>\n\n\n<h4 class=\"wp-block-heading\">The AI Futures Project</h4>\n\n\n<p>Focus: AI forecasting research projects, governance research projects, and policy engagement, in that order.</p>\n<p>Leader: Daniel Kokotajlo, with Eli Lifland</p>\n<p>Funding Needed: None Right Now</p>\n<p>Confidence Level: High</p>\n<p>Of all the \u2018shut up and take my money\u2019 applications in the 2024 round where I didn\u2019t have a conflict of interest, even before I got to participate in their tabletop wargame exercise, I judged this the most \u2018shut up and take my money\u2019-ist. At The Curve, I got to participate in the exercise and participate in discussions around it, I\u2019ve since done several more, and I\u2019m now even more confident this is an excellent pick.</p>\n<p>I continue to think it is a super strong case for retroactive funding as well. Daniel walked away from OpenAI, and what looked to be most of his net worth, to preserve his right to speak up. That led to us finally allowing others at OpenAI to speak up as well.</p>\n<p>This is how he wants to speak up, and try to influence what is to come, based on what he knows. I don\u2019t know if it would have been my move, but the move makes a lot of sense, and it has already paid off big. AI 2027 was read by the Vice President, who took it seriously, along with many others, and greatly informed the conversation. I believe the discourse is much improved as a result, and the possibility space has improved.</p>\n<p>Note that they are comfortably funded through the medium term via private donations and their recent SFF grant.</p>\n<p>Donate through <a href=\"https://www.every.org/ai-futures-project\">every.org</a>, or contact <a href=\"mailto:jonas.vollmer@gmail.com\">Jonas Vollmer</a>.</p>\n\n\n<h4 class=\"wp-block-heading\">Effective Institutions Project (EIP) (For Their Flagship Initiatives)</h4>\n\n\n<p>Focus: AI governance, advisory and research, finding how to change decision points</p>\n<p>Leader: Ian David Moss</p>\n<p>Funding Needed: Medium</p>\n<p>Confidence Level: High</p>\n<p>EIP operates on two tracks. They have their flagship initiatives and attempts to intervene directly. They also serve as donation advisors, which I discuss in that section.</p>\n<p>Their current flagship initiative plans are to focus on the intersection of AI governance and the broader political and economic environment, especially risks of concentration of power and unintentional power shifts from humans to AIs.</p>\n<p>Can they indeed identify ways to target key decision points, and make a big difference? One can look at their track record. I\u2019ve been asked to keep details confidential, but based on my assessment of private information, I confirmed they\u2019ve scored some big wins including that they helped improve safety practices at a major AI lab, and will plausibly continue to be able to have high leverage and punch above their funding weight.<a href=\"https://www.founderspledge.com/research/effective-institutions-project-ai-governance\"> You can read about some of the stuff that they can talk about here</a> in a Founders Pledge write up.</p>\n<p>It seems important that they be able to continue their work on all this.</p>\n<p>I also note that in SFF I allocated less funding to EIP than I would in hindsight have liked to allocate, due to quirks about the way matching funds worked and my attempts to adjust my curves to account for it.</p>\n<p>Donate through <a href=\"https://www.every.org/effective-institutions-project/f/eip-core-funding\">every.org</a>, or contact info@effectiveinstitutionsproject.org.</p>\n\n\n<h4 class=\"wp-block-heading\">Artificial Intelligence Policy Institute (AIPI)</h4>\n\n\n<p>Focus: Primarily polls about AI, also lobbying and preparing for crisis response.</p>\n<p>Leader: Daniel Colson.</p>\n<p>Also Involved: Mark Beall and Daniel Eth</p>\n<p>Funding Needed: High</p>\n<p>Confidence Level: High</p>\n<p>Those polls about how the public thinks about AI, including several from last year around SB 1047 including an adversarial collaboration with Dean Ball?</p>\n<p>Remarkably often, these are the people that did that. Without them, few would be asking those questions. Ensuring that someone is asking is super helpful. With some earlier polls I was a bit worried that the wording was slanted, and that will always be a concern with a motivated pollster, but I think recent polls have been much better at this, and they are as close to neutral as one can reasonably expect.</p>\n<p>There are those who correctly point out that even now in 2025 the public\u2019s opinions are weakly held and low salience, and that all you\u2019re often picking up is \u2018the public does not like AI and it likes regulation.\u2019</p>\n<p>Fair enough. Someone still has to show this, and show it applies here, and put a lie to people claiming the public goes the other way, and measure how things change over time. We need to be on top of what the public is thinking, including to guard against the places it wants to do dumb interventions.</p>\n<p>They don\u2019t only do polling. They also do lobbying and prepare for crisis responses.</p>\n<p>Donate <a href=\"https://theaipi.org/donate/\">here</a>, or use their <a href=\"https://theaipi.org/contact/\">contact form</a> to get in touch.</p>\n\n\n<h4 class=\"wp-block-heading\">AI Lab Watch</h4>\n\n\n<p>Focus: Monitoring the AI safety record and plans of the frontier AI labs</p>\n<p>Leader: Zach Stein-Perlman</p>\n<p>Funding Needed: Low</p>\n<p>Confidence Level: High</p>\n<p>Zach has consistently been one of those on top of the safety and security plans, the model cards and other actions of the major labs, both writing up detailed feedback from a skeptical perspective and also compiling the website and its scores in various domains. Zach is definitely in the \u2018demand high standards that would actually work and treat everything with skepticism\u2019 school of all this, which I feel is appropriate, and I\u2019ve gotten substantial benefit of his work several times.</p>\n<p>However, due to uncertainty about whether this is the best thing for him to work on, and thus not being confident he will have this ball, Zach is not currently accepting funding, but would like people who are interested in donations to contact him via Intercom on the AI Lab Watch website.</p>\n\n\n<h4 class=\"wp-block-heading\">Palisade Research</h4>\n\n\n<p>Focus: AI capabilities demonstrations to inform decision makers on capabilities and loss of control risks</p>\n<p>Leader: Jeffrey Ladish</p>\n<p>Funding Needed: High</p>\n<p>Confidence Level: High</p>\n<p>This is clearly an understudied approach. People need concrete demonstrations. Every time I get to talking with people in national security or otherwise get closer to decision makers who aren\u2019t deeply into AI and in particular into AI safety concerns, you need to be as concrete and specific as possible &#8211; that\u2019s why I wrote<a href=\"https://thezvi.substack.com/p/danger-ai-scientist-danger\"> Danger, AI Scientist, Danger</a> the way I did. We keep getting rather on-the-nose fire alarms, but it would be better if we could get demonstrations even more on the nose, and get them sooner, and in a more accessible way.</p>\n<p>Since last time, I\u2019ve had a chance to see their demonstrations in action several times, and I\u2019ve come away feeling that they have mattered.</p>\n<p>I have confidence that Jeffrey is a good person to continue to put this plan into action.</p>\n<p>To donate,<a href=\"https://www.every.org/palisade-research?viewport=desktop#/donate/card\"> click here</a> or email donate@palisaderesearch.org.</p>\n\n\n<h4 class=\"wp-block-heading\">CivAI</h4>\n\n\n<p>Focus: Visceral demos of AI risks</p>\n<p>Leader: Sid Hiregowdara</p>\n<p>Funding Needed: High</p>\n<p>Confidence Level: Medium</p>\n<p>I was impressed by the demo I was given (so a demo demo?). There\u2019s no question such demos fill a niche and there aren\u2019t many good other candidates for the niche.</p>\n<p>The bear case is that the demos are about near term threats, so does this help with the things that matter? It\u2019s a good question. My presumption is yes, that raising situational awareness about current threats is highly useful. That once people notice that there is danger, that they will ask better questions, and keep going. But I always do worry about drawing eyes to the wrong prize.</p>\n<p>To donate, <a href=\"http://every.org/civai?suggestedAmounts=&amp;theme_color=23527f&amp;method=card%2Cbank%2Cpaypal%2Ccrypto%2Cstocks%2Cdaf&amp;utm_campaign=donate-link#/donate\">click here</a> or email contact@civai.org.</p>\n\n\n<h4 class=\"wp-block-heading\">AI Safety Info (Robert Miles)</h4>\n\n\n<p>Focus: Making YouTube videos about AI safety, starring Rob Miles</p>\n<p>Leader: Rob Miles</p>\n<p>Funding Needed: Low</p>\n<p>Confidence Level: High</p>\n<p>I think these are pretty great videos in general, and given what it costs to produce them we should absolutely be buying their production. If there is a catch, it is that I am very much not the target audience, so you should not rely too much on my judgment of what is and isn\u2019t effective video communication on this front, and you should confirm you like the cost per view.</p>\n<p>To donate, join his<a href=\"https://www.patreon.com/robertskmiles\"> patreon</a> or contact him at robertmilesai@robertskmiles.com.</p>\n\n\n<h4 class=\"wp-block-heading\">Intelligence Rising</h4>\n\n\n<p>Focus: Facilitation of the AI scenario roleplaying exercises including Intelligence Rising</p>\n<p>Leader: Shahar Avin</p>\n<p>Funding Needed: Low</p>\n<p>Confidence Level: High</p>\n<p>I haven\u2019t had the opportunity to play Intelligence Rising, but I have read the rules to it, and heard a number of strong after action reports (AARs). They offered<a href=\"https://arxiv.org/abs/2410.03092\"> this summary of insights in 2024</a>. The game is clearly solid, and it would be good if they continue to offer this experience and if more decision makers play it, in addition to the AI Futures Project TTX.</p>\n<p>To donate, reach out to <em>team@intelligencerising.org.</em></p>\n\n\n<h4 class=\"wp-block-heading\">Convergence Analysis</h4>\n\n\n<p>Focus: A series of sociotechnical reports on<a href=\"https://www.convergenceanalysis.org/programs/scenario-research\"> key AI scenarios</a>,<a href=\"https://www.convergenceanalysis.org/programs/governance-research\"> governance recommendations</a> and conducting<a href=\"https://www.convergenceanalysis.org/programs/ai-awareness\"> AI awareness efforts</a>.</p>\n<p>Leader:<a href=\"https://www.convergenceanalysis.org/team/david-kristofferson\"> David Kristoffersson</a></p>\n<p>Funding Needed: High (combining all tracks)</p>\n<p>Confidence Level: Low</p>\n<p>They do a variety of AI safety related things. Their<a href=\"https://docs.google.com/document/d/1Xrh5qTPOuJQjeO-gCf2S4eB9Nidf2xVw78Tc5ps1rsE/edit?tab=t.0#heading=h.sna1ru7mngie\"> Scenario Planning</a> continues to be what I find most exciting, although I\u2019m also somewhat interested in their modeling cooperation initiative as well. It\u2019s not as neglected as it was a year ago, but we could definitely use more work than we\u2019re getting. For track record you<a href=\"https://www.convergenceanalysis.org/publications/timelines-to-transformative-ai-an-investigation\"> check out</a><a href=\"https://www.convergenceanalysis.org/publications/ai-clarity-an-initial-research-agenda\"> their reports</a> from 2024 in this area, and see if you think that was good work, and the rest of their website has more.</p>\n<p>Their donation page is<a href=\"https://www.convergenceanalysis.org/donate\"> here</a>, or you can contact contact@convergenceanalysis.org.</p>\n\n\n<h4 class=\"wp-block-heading\">IASEAI (International Association for Safe and Ethical Artificial Intelligence)</h4>\n\n\n<p>Focus: Grab bag of AI safety actions, research, policy, community, conferences, standards</p>\n<p>Leader: Mark Nitzberg</p>\n<p>Funding Needed: High</p>\n<p>Confidence Level: Low</p>\n<p>There are some clearly good things within the grab bag, including some good conferences and it seems substantial support for Geoffrey Hinton, but for logistical reasons I didn\u2019t do a close investigation to see if the overall package looked promising. I\u2019m passing the opportunity along.</p>\n<p>Donate <a href=\"https://www.iaseai.org/donate\">here</a>, or contact them at info@iaseai.org.</p>\n\n\n<h4 class=\"wp-block-heading\">The AI Whistleblower Initiative</h4>\n\n\n<p>Focus: Whistleblower advising and resources for those in AI labs warning about catastrophic risks, including via<a href=\"http://third-opinion.org\"> Third Opinion</a>.</p>\n<p>Leader: Larl Koch</p>\n<p>Funding Needed: High</p>\n<p>Confidence Level: Medium</p>\n<p>I\u2019ve given them advice, and at least some amount of such resourcing is obviously highly valuable. We certainly should be funding Third Opinion, so that if someone wants to blow the whistle they can have help doing it. The question is whether if it scales this loses its focus.</p>\n<p>Donate <a href=\"https://aiwi.org/support-us/\">here</a>, or reach out to hello@aiwi.org.</p>\n\n\n<h4 class=\"wp-block-heading\">Organizations Related To Potentially Pausing AI Or Otherwise Having A Strong International AI Treaty</h4>\n\n\n\n<h4 class=\"wp-block-heading\">Pause AI and Pause AI Global</h4>\n\n\n<p>Focus: Advocating for a pause on AI, including via in-person protests</p>\n<p>Leader: Holly Elmore (USA) and Joep Meindertsma (Global)</p>\n<p>Funding Level: Low</p>\n<p>Confidence Level: Medium</p>\n<p>Some people say that those who believe we should pause AI would be better off staying quiet about it, rather than making everyone look foolish.</p>\n<p>I disagree.</p>\n<p>I don\u2019t think pausing right now is a good idea. I think we should be working on the transparency, state capacity, technical ability and diplomatic groundwork to enable a pause in case we need one, but that it is too early to actually try to implement one.</p>\n<p>But I do think that if you believe we should pause? Then you should say that we should pause. I very much appreciate people standing up, entering the arena and saying what they believe in, including quite often in my comments. Let the others mock all they want.</p>\n<p>If you agree with Pause AI that the right move is to Pause AI, and you don\u2019t have strong strategic disagreements with their approach, then you should likely be excited to fund this. If you disagree, you have better options.</p>\n<p>Either way, they are doing what they, given their beliefs, should be doing.</p>\n<p>Donate <a href=\"https://pauseai.info/donate\">here</a>, or reach out to joep@pauseai.info.</p>\n\n\n<h4 class=\"wp-block-heading\">MIRI</h4>\n\n\n<p>Focus: At this point, primarily AI policy advocacy, letting everyone know that <em>If Anyone Builds It, Everyone Dies</em> and all that, plus some research</p>\n<p>Leaders: Malo Bourgon, Eliezer Yudkowsky</p>\n<p>Funding Needed: High</p>\n<p>Confidence Level: High</p>\n<p>MIRI, concluding that it is highly unlikely alignment will make progress rapidly enough otherwise,<a href=\"https://intelligence.org/2024/01/04/miri-2024-mission-and-strategy-update/\"> has shifted its strategy</a> to largely advocate for major governments coming up with an international agreement to halt AI progress and to do communications, although research still looks to be a large portion of the budget, and they have dissolved its agent foundations team. Hence the book.</p>\n<p>That is not a good sign for the world, but it does reflect their beliefs.</p>\n<p>They have accomplished a lot. The book is at least a modest success on its own terms in moving things forward.</p>\n<p>I strongly believe they should be funded to continue to fight for a better future however they think is best, even when I disagree with their approach.</p>\n<p>This is very much a case of \u2018do this if and only if this aligns with your model and preferences.\u2019</p>\n<p>Donate <a href=\"https://intelligence.org/donate/\">here</a>, or reach out to harlan@intelligence.org.</p>\n\n\n<h4 class=\"wp-block-heading\">Existential Risk Observatory</h4>\n\n\n<p>Focus: Pause-relevant research</p>\n<p>Leader: Otto Barten</p>\n<p>Funding Needed: Low</p>\n<p>Confidence Level: Medium</p>\n<p>Mostly this is the personal efforts of Otto Barten, ultimately advocating for a conditional pause. For modest amounts of money, in prior years he\u2019s managed to have a hand in<a href=\"https://www.youtube.com/watch?v=vGQDctxwy2E&amp;list=PLY36Zq4jtCFFd8piJN19URqk82avV-QMT\"> some high profile existential risk events</a> and<a href=\"https://time.com/6258483/uncontrollable-ai-agi-risks/\"> get the first x-risk related post into TIME magazine</a>. He\u2019s now pivoted to pause-relevant research (as in how to implement one via treaties, off switches, evals and threat models).</p>\n<p>The track record and my prior investigation is less relevant now, so I\u2019ve bumped them down to low confidence, but it would definitely be good to have the technical ability to pause and not enough work is being done on that.</p>\n<p>To donate,<a href=\"https://whydonate.com/nl/fundraising/support-existential-risk-observatory\"> click here</a>, or get in touch at info@existentialriskobservatory.org.</p>\n\n\n<h4 class=\"wp-block-heading\">Organizations Focusing Primary On AI Policy and Diplomacy</h4>\n\n\n<p>Some of these organizations also look at bio policy or other factors, but I judge those here as being primarily concerned with AI.</p>\n<p>In this area, I am especially keen to rely on people with good track records, who have shown that they can build and use connections and cause real movement. It\u2019s so hard to tell what is and isn\u2019t effective, otherwise. Often small groups can pack a big punch, if they know where to go, or big ones can be largely wasted &#8211; I think that most think tanks on most topics are mostly wasted even if you believe in their cause.</p>\n\n\n<h4 class=\"wp-block-heading\">Center for AI Safety and the CAIS Action Fund</h4>\n\n\n<p>Focus: AI research, field building and advocacy</p>\n<p>Leaders: Dan Hendrycks</p>\n<p>Funding Needed: High</p>\n<p>Confidence Level: High</p>\n<p>They did the<a href=\"https://www.safe.ai/work/statement-on-ai-risk\"> CAIS Statement on AI Risk</a>, helped SB 1047 get as far as it did, and have improved things in many other ways. Some of these other ways are non-public. Some of those non-public things are things I know about and some aren\u2019t. I will simply say the counterfactual policy world is a lot worse. They\u2019ve clearly been punching well above their weight in the advocacy space. The other arms are no slouch either, lots of great work here. Their meaningful rolodex and degree of access is very strong and comes with important insight into what matters.</p>\n<p>They take a lot of big swings and aren\u2019t afraid of taking risks or looking foolish. I appreciate that, even when a given attempt doesn\u2019t fully work.</p>\n<p>If you want to focus on their policy, then you can fund their 501(c)(4), the Action Fund, since 501c(3)s are limited in how much they can spend on political activities, keeping in mind the tax implications of that. If you don\u2019t face any tax implications I would focus first on the 501(c)(4).</p>\n<p>We should definitely find a way to fund at least their core activities.</p>\n<p>Donate to the <a href=\"https://action.safe.ai/donate\">Action Fund for funding political activities</a>, or the <a href=\"https://safe.ai/donate\">501(c)(3) for research</a>. They can be contacted at contact@safe.ai.</p>\n\n\n<h4 class=\"wp-block-heading\">Foundation for American Innovation (FAI)</h4>\n\n\n<p>Focus: Tech policy research, thought leadership, educational outreach to government, fellowships.</p>\n<p>Leader: Grace Meyer</p>\n<p>Funding Needed: High</p>\n<p>Confidence Level: High</p>\n<p>FAI is centrally about innovation. Innovation is good, actually, in almost all contexts, as is building things and letting people do things.</p>\n<p>AI is where this gets tricky. People \u2018supporting innovation\u2019 are often using that as an argument against all regulation of AI, and indeed I am dismayed to see so many push so hard on this exactly in the one place I think they are deeply wrong, when we could work together on innovation (and abundance) almost anywhere else.</p>\n<p>FAI and resident AI studiers Samuel Hammond and Dean Ball are in an especially tough spot, because they are trying to influence AI policy from the right and not get expelled from that coalition or such spaces. There\u2019s a reason we don\u2019t have good alternative options for this. That requires striking a balance.</p>\n<p>I\u2019ve definitely had my disagreements with Hammond, including<a href=\"https://thezvi.substack.com/p/i-got-95-theses-but-a-glitch-aint\"> strong disagreements with his 95 theses on AI</a> although I agreed far more than I disagreed, and I had many disagreements with his<a href=\"https://www.secondbest.ca/p/ai-and-leviathan-part-i\"> AI and Leviathan</a> as well. He\u2019s talked on the Hill about \u2018open model diplomacy.\u2019</p>\n<p>I\u2019ve certainly had many strong disagreements with Dean Ball as well, both in substance and rhetoric. Sometimes he\u2019s the voice of reason and careful analysis, other times (from my perspective) he can be infuriating, most recently in discussions of the Superintelligence Statement, remarkably often he does some of both in the same post. He was perhaps the most important opposer of SB 1047 and went on to a stint at the White House before joining FAI.</p>\n<p>Yet here is FAI, rather high on the list. They\u2019re a unique opportunity, you go to war with the army you have, and both Ball and Hammond have stuck their neck out in key situations. Hammond came out opposing the moratorium. They\u2019ve been especially strong on compute governance.</p>\n<p>I have private reasons to believe that FAI has been effective and we can expect that to continue, and its other initiatives also mostly seem good. We don\u2019t have to agree on everything else, so long as we all want good things and are trying to figure things out, and I\u2019m confident that is the case here.</p>\n<p>I am especially excited that they can speak to the Republican side of the aisle in the R\u2019s native language, which is difficult for most in this space to do.</p>\n<p>An obvious caveat is that if you are not interested in the non-AI pro-innovation part of the agenda (I certainly approve, but it\u2019s not obviously a high funding priority for most readers) then you\u2019ll want to ensure it goes where you want it.</p>\n<p>To donate,<a href=\"https://www.thefai.org/donate\"> click here</a>, or contact them using the form <a href=\"https://www.thefai.org/contact-us\">here</a>.</p>\n\n\n<h4 class=\"wp-block-heading\">Encode AI (Formerly Encode Justice)</h4>\n\n\n<p>Focus: Youth activism on AI safety issues</p>\n<p>Leader: Sneha Revanur</p>\n<p>Funding Needed: Medium</p>\n<p>Confidence Level: High</p>\n<p>They started out doing quite a lot on a shoestring budget by using volunteers, helping with SB 1047 and in several other places. Now they are turning pro, and would like to not be on a shoestring. I think they have clearly earned that right. The caveat is risk of ideological capture. Youth organizations tend to turn to left wing causes.</p>\n<p>The risk here is that this effectively turns mostly to AI ethics concerns. It\u2019s great that they\u2019re coming at this without having gone through the standard existential risk ecosystem, but that also heightens the ideological risk.</p>\n<p>I continue to believe it is worth the risk.</p>\n<p>To donate, go<a href=\"https://buy.stripe.com/14k8wx33u0j9f2UfYY\"> here</a>. They can be contacted at sneha@encodeai.org.</p>\n\n\n<h4 class=\"wp-block-heading\">The Future Society</h4>\n\n\n<p>Focus: AI governance standards and policy.</p>\n<p>Leader: Nicolas Mo\u00ebs</p>\n<p>Funding Needed: High</p>\n<p>Confidence Level: High</p>\n<p>I\u2019ve seen credible sources saying they do good work, and that they substantially helped orient the EU AI Act to at least care at all about frontier general AI. The EU AI Act was not a good bill, but it could easily have been a far worse one, doing much to hurt AI development while providing almost nothing useful for safety.</p>\n<p>We should do our best to get some positive benefits out of the whole thing. And indeed, they helped substantially improve the EU Code of Practice, which was in hindsight remarkably neglected otherwise.</p>\n<p>They\u2019re also active around the world, including the USA and China.</p>\n<p>Donate <a href=\"https://thefuturesociety.org/donate/\">here</a>, or contact them <a href=\"https://thefuturesociety.org/contact/\">here</a>.</p>\n\n\n<h4 class=\"wp-block-heading\">Safer AI</h4>\n\n\n<p>Focus: Specifications for good AI safety, also directly impacting EU AI policy</p>\n<p>Leader: Henry Papadatos</p>\n<p>Funding Needed: Medium</p>\n<p>Confidence Level: Low</p>\n<p>I\u2019ve been impressed by Simeon and his track record, including here. Simeon is stepping down as leader to start a company, which happened post-SFF, so they would need to be reevaluated in light of this before any substantial donation.</p>\n<p>Donate <a href=\"https://www.safer-ai.org/donate\">here</a>, or contact them at contact@safer-ai.org.</p>\n\n\n<h4 class=\"wp-block-heading\">Institute for AI Policy and Strategy (IAPS)</h4>\n\n\n<p>Focus: Papers and projects for \u2018serious\u2019 government circles, meetings with same, policy research</p>\n<p>Leader: Peter Wildeford</p>\n<p>Funding Needed: Medium</p>\n<p>Confidence Level: High</p>\n<p>I have a lot of respect for Peter Wildeford, and they\u2019ve clearly put in good work and have solid connections down, including on the Republican side where better coverage is badly needed, and the only other solid lead we have is FAI. Peter has also increasingly been doing strong work directly via Substack and Twitter that has been helpful to me and that I can observe directly. They are strong on hardware governance and chips in particular (as is FAI).</p>\n<p>Given their goals and approach, funding from outside the traditional ecosystem sources would be extra helpful, ideally such efforts are fully distinct from OpenPhil.</p>\n<p>With the shifting landscape and what I\u2019ve observed, I\u2019m moving them up to high confidence and priority.</p>\n<p>Donate <a href=\"https://www.iaps.ai/donate\">here</a>, or contact them at jmarron@iaps.ai.</p>\n\n\n<h4 class=\"wp-block-heading\">AI Standards Lab (Holtman Research)</h4>\n\n\n<p>Focus: Accelerating the writing of AI safety standards</p>\n<p>Leaders: Koen Holtman and Chin Ze Shen</p>\n<p>Funding Needed: Medium</p>\n<p>Confidence Level: High</p>\n<p>They help facilitate the writing of AI safety standards, for EU/UK/USA, including on the recent EU Code of Practice. They have successfully gotten some of their work officially incorporated, and another recommender with a standards background was impressed by the work and team.</p>\n<p>This is one of the many things that someone has to do, and where if you step up and do it and no one else does that can go pretty great. Having now been involved in bill minutia myself, I know it is thankless work, and that it can really matter, both for public and private standards, and they plan to pivot somewhat to private standards.</p>\n<p>I\u2019m raising my confidence to high that this is at least a good pick, if you want to fund the writing of standards.</p>\n<p>To donate, go <a href=\"https://give.cornerstone.cc/aistandardslab\">here</a> or reach out to hello@aistandardslab.org.</p>\n\n\n<h4 class=\"wp-block-heading\">Safe AI Forum</h4>\n\n\n<p>Focus: International AI safety conferences</p>\n<p>Leader: Fynn Heide and Sophie Thomson</p>\n<p>Funding Needed: Medium</p>\n<p>Confidence Level: Low</p>\n<p>They run the IDAIS series of conferences, including successful ones involving China. I do wish I had a better model of what makes such a conference actually matter versus not mattering, but these sure seem like they should matter, and certainly well worth their costs to run them.</p>\n<p>To donate, contact them using the form at the bottom of the page <a href=\"https://saif.org/about-and-contact/\">here</a>.</p>\n\n\n<h4 class=\"wp-block-heading\">Center For Long Term Resilience</h4>\n\n\n<p>Focus: UK Policy Think Tank focusing on \u2018extreme AI risk and biorisk policy.\u2019</p>\n<p>Leader: Angus Mercer</p>\n<p>Funding Needed: High</p>\n<p>Confidence Level: Low</p>\n<p>The UK has shown promise in its willingness to shift its AI regulatory focus to frontier models in particular. It is hard to know how much of that shift to attribute to any particular source, or otherwise measure how much impact there has been or might be on final policy.</p>\n<p>They have endorsements of their influence from philosopher Toby Ord, Former Special Adviser to the UK Prime Minister Logan Graham, and Senior Policy Adviser Nitarshan Rajkumar.</p>\n<p>I reached out to a source with experience in the UK government who I trust, and they reported back they are a fan and pointed to some good things they\u2019ve helped with. There was a general consensus that they do good work, and those who investigated where impressed.</p>\n<p>However, I have concerns. Their funding needs are high, and they are competing against many others in the policy space, many of which have very strong cases. I also worry their policy asks are too moderate, which might be an advantage for others.</p>\n<p>My lower confidence this year is a combination of worries about moderate asks, worry about organizational size, and worries about the shift in governments in the UK and the UK\u2019s ability to have real impact elsewhere. But if you buy the central idea of this type of lobbying through the UK and are fine with a large budget, go for it.</p>\n<p>Donate <a href=\"https://www.longtermresilience.org/donate/\">here</a>, or reach out to info@longtermresilience.org.</p>\n\n\n<h4 class=\"wp-block-heading\">Simon Institute for Longterm Governance</h4>\n\n\n<p>Focus: Foundations and demand for international cooperation on AI governance and differential tech development</p>\n<p>Leader: Konrad Seifert and Maxime Stauffer</p>\n<p>Funding Needed: High</p>\n<p>Confidence Level: Low</p>\n<p>As with all things diplomacy, hard to tell the difference between a lot of talk and things that are actually useful. Things often look the same either way for a long time. A lot of their focus is on the UN, so update either way based on how useful you think that approach is, and also that makes it even harder to get a good read.</p>\n<p>They previously had a focus on the Global South and are pivoting to China, which seems like a more important focus.</p>\n<p>To donate, scroll down on <a href=\"https://simoninstitute.ch/support\">this page</a> to access their donation form, or contact them at contact@simoninstitute.ch.</p>\n\n\n<h4 class=\"wp-block-heading\">Legal Advocacy for Safe Science and Technology</h4>\n\n\n<p>Focus: Legal team for lawsuits on catastrophic risk and to defend whistleblowers.</p>\n<p>Leader: Tyler Whitmer</p>\n<p>Funding Needed: Medium</p>\n<p>Confidence Level: Medium</p>\n<p>I wasn\u2019t sure where to put them, but I suppose lawsuits are kind of policy by other means in this context, or close enough?</p>\n<p>I buy the core idea of having a legal team on standby for catastrophic risk related legal action in case things get real quickly is a good idea, and I haven\u2019t heard anyone else propose this, although I do not feel qualified to vet the operation. They were one of the organizers of the <a href=\"http://notforprivategain.org\">NotForPrivateGain.org</a> campaign against the OpenAI restructuring.</p>\n<p>I definitely buy the idea of an AI Safety Whistleblower Defense Fund, which they are also doing. Knowing there will be someone to step up and help if it comes to that changes the dynamics in helpful ways.</p>\n<p>Donors who are interested in making relatively substantial donations or grants should contact tyler@lasst.org, for smaller amounts<a href=\"https://www.every.org/lasst\"> click here.</a></p>\n\n\n<h4 class=\"wp-block-heading\">Institute for Law and AI</h4>\n\n\n<p>Focus: Legal research on US/EU law on transformational AI, fellowships, talent</p>\n<p>Leader: Moritz von Knebel</p>\n<p>Involved: Gabe Weil</p>\n<p>Funding Needed: High</p>\n<p>Confidence Level: Low</p>\n<p>I\u2019m confident that they should be funded at all, the question is if this should be scaled up quite a lot, and what aspects of this would scale in what ways. If you can be convinced that the scaling plans are worthwhile this could justify a sizable donation.</p>\n<p>Donate <a href=\"https://www.every.org/law-ai\">here</a>, or contact them at hello@law-ai.org.</p>\n\n\n<h4 class=\"wp-block-heading\">Macrostrategy Research Institute</h4>\n\n\n<p>Focus: Amplify<a href=\"https://nickbostrom.com/\"> Nick Bostrom</a></p>\n<p>Leader: Toby Newberry</p>\n<p>Funding Needed: High</p>\n<p>Confidence Level: Low</p>\n<p>If you think Nick Bostrom is doing great work and want him to be more effective, then this is a way to amplify that work. In general, \u2018give top people support systems\u2019 seems like a good idea that is underexplored.</p>\n<p>Get in touch at toby.newberry@gmail.com.</p>\n\n\n<h4 class=\"wp-block-heading\">Secure AI Project</h4>\n\n\n<p>Focus: Advocacy for public safety and security protocols (SSPs) and related precautions</p>\n<p>Leader: Nick Beckstead</p>\n<p>Funding Needed: High</p>\n<p>Confidence Level: High</p>\n<p>I\u2019ve had the opportunity to consult and collaborate with them and I\u2019ve been consistently impressed. They\u2019re the real deal, they pay attention to detail and care about making it work for everyone, and they\u2019ve got results. I\u2019m a big fan.</p>\n<p>Donate <a href=\"https://donorbox.org/secure-ai-project-general-donations\">here</a>, or contact them at info@secureaiproject.org.</p>\n\n\n<h4 class=\"wp-block-heading\">Organizations Doing ML Alignment Research</h4>\n\n\n<p>This category should be self-explanatory. Unfortunately, a lot of good alignment work still requires charitable funding. The good news is that (even more than last year when I wrote the rest of this introduction) there is a lot more funding, and willingness to fund, than there used to be, and also the projects generally look more promising.</p>\n<p>The great thing about interpretability is that you can be confident you are dealing with something real. The not as great thing is that this can draw too much attention to interpretability, and that you can fool yourself into thinking that All You Need is Interpretability.</p>\n<p>The good news is that several solid places can clearly take large checks.</p>\n<p>I didn\u2019t investigate too deeply on top of my existing knowledge here in 2024, because at SFF I had limited funds and decided that direct research support wasn\u2019t a high enough priority, partly due to it being sufficiently legible.</p>\n<p>We should be able to find money previously on the sidelines eager to take on many of these opportunities. Lab employees are especially well positioned, due to their experience and technical knowledge and connections, to evaluate such opportunities, and also to provide help with access and spreading the word.</p>\n\n\n<h4 class=\"wp-block-heading\">Model Evaluation and Threat Research (METR)</h4>\n\n\n<p>Formerly ARC Evaluations.</p>\n<p>Focus: Model evaluations</p>\n<p>Leaders: Beth Barnes, Chris Painter</p>\n<p>Funding Needed: High</p>\n<p>Confidence Level: High</p>\n<p>Originally I wrote that we hoped to be able to get large funding for METR via non-traditional sources. That happened last year, and METR got major funding. That\u2019s great news. Alas, they once again have to hit the fundraising trail.</p>\n<p>METR has proven to be the gold standard for outside evaluations of potentially dangerous frontier model capabilities, and has proven its value even more so in 2025.</p>\n<p>We very much need these outside evaluations, and to give the labs every reason to use them and no excuse not to use them, and their information has been invaluable. In an ideal world the labs would be fully funding METR, but they\u2019re not.</p>\n<p>So this becomes a place where we can confidently invest quite a bit of capital, make a legible case for why it is a good idea, and know it will probably be well spent.</p>\n<p>If you can direct fully \u2018square\u2019 \u2018outside\u2019 funds that need somewhere legible to go and are looking to go large? I love METR for that.</p>\n<p>To donate,<a href=\"https://metr.org/donate\"> click here.</a> They can be contacted at info@metr.org.</p>\n\n\n<h4 class=\"wp-block-heading\">Alignment Research Center (ARC)</h4>\n\n\n<p>Focus: Theoretically motivated alignment work</p>\n<p>Leader: Jacob Hilton</p>\n<p>Funding Needed: Medium</p>\n<p>Confidence Level: High</p>\n<p>There\u2019s a long track record of good work here, and Paul Christiano remained excited as of 2024. If you are looking to fund straight up alignment work and don\u2019t have a particular person or small group in mind, this is certainly a safe bet to put additional funds to good use and attract good talent.</p>\n<p>Donate <a href=\"https://www.alignment.org/donate/\">here</a>, or reach out to jacob@alignment.org.</p>\n\n\n<h4 class=\"wp-block-heading\">Apollo Research</h4>\n\n\n<p>Focus: Scheming, evaluations, and governance</p>\n<p>Leader: Marius Hobbhahn</p>\n<p>Funding Needed: Medium</p>\n<p>Confidence Level: High</p>\n<p>This is an excellent thing to focus on, and one of the places we are most likely to be able to show \u2018fire alarms\u2019 that make people sit up and notice.<a href=\"https://www.apolloresearch.ai/blog/the-first-year-of-apollo-research\"> Their first year seems to have gone well</a>, one example would be their presentation at the UK safety summit that<a href=\"https://arxiv.org/abs/2311.07590\"> LLMs can strategically deceive their primary users when put under pressure</a>. They will need serious funding to fully do the job in front of them, hopefully like METR they can be helped by the task being highly legible.</p>\n<p>They suggest looking at this paper, and also<a href=\"https://arxiv.org/abs/2504.12170\"> this one</a>. I can verify that they are the real deal and doing the work.</p>\n<p>To donate, reach out to info@apolloresearch.ai.</p>\n\n\n<h4 class=\"wp-block-heading\">Cybersecurity Lab at University of Louisville</h4>\n\n\n<p>Focus: Support for Roman Yampolskiy\u2019s lab and work</p>\n<p>Leader: Roman Yampolskiy</p>\n<p>Funding Needed: Low</p>\n<p>Confidence Level: High</p>\n<p>Roman Yampolskiy is the most pessimistic known voice about our chances of not dying from AI, and got that perspective on major platforms like Joe Rogan and Lex Fridman. He\u2019s working on a book and wants to support PhD students.</p>\n<p>Supporters can make a tax detectable gift to <a href=\"https://engineering.louisville.edu/give/\">the University</a>, specifying that they intend to fund Roman Yampolskiy and the Cyber Security lab.</p>\n\n\n<h4 class=\"wp-block-heading\">Timaeus</h4>\n\n\n<p>Focus: Interpretability research</p>\n<p>Leader:Jesse Hoogland, Daniel Murfet, Stan van Wingerden</p>\n<p>Funding Needed: High</p>\n<p>Confidence Level: High</p>\n<p>Timaeus focuses on interpretability work and sharing their results. The set of advisors is excellent, including Davidad and Evan Hubinger. Evan, John Wentworth and Vanessa Kosoy have offered high praise, and there is evidence they have impacted top lab research agendas. They\u2019re done what I think is solid work, although I am not so great at evaluating papers directly.</p>\n<p>If you\u2019re interested in directly funding interpretability research, that all makes this seem like a slam dunk. I\u2019ve confirmed that this all continues to hold true in 2025.</p>\n<p>To donate, get in touch with Jesse at jesse@timaeus.co. If this is the sort of work that you\u2019re interested in doing, they also have a discord at<a href=\"http://devinterp.com/discord\"> http://devinterp.com/discord</a>.</p>\n\n\n<h4 class=\"wp-block-heading\">Simplex</h4>\n\n\n<p>Focus: Mechanistic interpretability of how inference breaks down</p>\n<p>Leader: Paul Riechers and Adam Shai</p>\n<p>Funding Needed: Medium</p>\n<p>Confidence Level: High</p>\n<p>I am not as high on them as I am on Timaeus, but they have given reliable indicators that they will do good interpretability work. I\u2019d (still) feel comfortable backing them.</p>\n<p>Donate <a href=\"https://www.simplexaisafety.com/donate\">here</a>, or contact them via <a href=\"https://www.simplexaisafety.com/contact\">webform</a>.</p>\n\n\n<h4 class=\"wp-block-heading\">Far AI</h4>\n\n\n<p>Focus: Interpretability and other alignment research, incubator, hits based approach</p>\n<p>Leader: Adam Gleave</p>\n<p>Funding Needed: High</p>\n<p>Confidence Level: Medium</p>\n<p>They take the hits based approach to research, which is correct. I\u2019ve gotten confirmation that they\u2019re doing the real thing here. In an ideal world everyone doing the real thing would get supported, and they\u2019re definitely still funding constrained.</p>\n<p>To donate,<a href=\"https://www.far.ai/donate\"> click here</a>. They can be contacted at hello@far.ai.</p>\n\n\n<h4 class=\"wp-block-heading\">Alignment in Complex Systems Research Group</h4>\n\n\n<p>Focus: AI alignment research on hierarchical agents and multi-system interactions</p>\n<p>Leader: Jan Kulveit</p>\n<p>Funding Needed: Medium</p>\n<p>Confidence Level: High</p>\n<p>I liked ACS last year, and since then we\u2019ve seen Gradual Disempowerment and other good work, which means this now falls into the category \u2018this having funding problems would be an obvious mistake.\u2019 I ranked them very highly in SFF, and there should be a bunch more funding room.</p>\n<p>To donate, reach out to hello@epistea.org, and note that you are interested in donating to ACS specifically.</p>\n\n\n<h4 class=\"wp-block-heading\">Apart Research</h4>\n\n\n<p>Focus: AI safety hackathons, MATS-style programs and AI safety horizon scanning.</p>\n<p>Leaders: Esben Kran, Jason Schreiber</p>\n<p>Funding Needed: Medium</p>\n<p>Confidence Level: Low</p>\n<p>I\u2019m (still) confident in their execution of the hackathon idea, which was the central pitch at SFF although they inform me generally they\u2019re more centrally into the MATS-style programs. My doubt for the hackathons is on the level of \u2018is AI safety something that benefits from hackathons.\u2019 Is this something one can, as it were, hack together usefully? Are the hackathons doing good counterfactual work? Or is this a way to flood the zone with more variations on the same ideas?</p>\n<p>As with many orgs on the list, this one makes sense if and only if you buy the plan, and is one of those \u2018I\u2019m not excited but can see it being a good fit for someone else.\u2019</p>\n<p><a href=\"https://apartresearch.com/donate\">To donate, click here.</a> They can be reached at hello@apartresearch.com.</p>\n\n\n<h4 class=\"wp-block-heading\">Transluce</h4>\n\n\n<p>Focus: Specialized superhuman systems for understanding and overseeing AI</p>\n<p>Leaders: Jacob Steinhardt, Sarah Schwettmann</p>\n<p>Funding Needed: High</p>\n<p>Confidence Level: Medium</p>\n<p>Last year they were a new org. Now they have now grown to 14 people and now have a solid track record and want to keep growing. I have confirmation the team is credible. The plan for scaling themselves is highly ambitious, with planned scale well beyond what SFF can fund. I haven\u2019t done anything like the investigation into their plans and capabilities you would need before placing a bet that big, as AI research of all kinds gets expensive quickly.</p>\n<p>If there is sufficient appetite to scale the amount of privately funded direct work of this type, then this seems like a fine place to look. I am optimistic on them finding interesting things, although on a technical level I am skeptical of the larger plan.</p>\n<p>To donate, reach out to info@transluce.org.</p>\n\n\n<h4 class=\"wp-block-heading\">Organizations Doing Other Technical Work</h4>\n\n\n\n<h4 class=\"wp-block-heading\">AI Analysts @ RAND</h4>\n\n\n<p>Focus: Developing \u2018AI analysts\u2019 that can assist policy makers.</p>\n<p>Leaders: John Coughlan</p>\n<p>Funding Needed: High</p>\n<p>Confidence Level: Medium</p>\n<p>This is a thing that RAND should be doing and that should exist. There are obvious dangers here, but I don\u2019t think this makes them substantially worse and I do think this can potentially improve policy a lot. RAND is well placed to get the resulting models to be actually used. That would enhance state capacity, potentially quite a bit.</p>\n<p>The problem is that doing this is not cheap, and while funding this shouldn\u2019t fall to those reading this, it plausibly does. This could be a good place to consider sinking quite a large check, if you believe in the agenda.</p>\n<p>Donate <a href=\"https://www.rand.org/support-us/rfi.html\">here</a>.</p>\n\n\n<h4 class=\"wp-block-heading\">Organizations Doing Math, Decision Theory and Agent Foundations</h4>\n\n\n<p>Right now it looks likely that AGI will be based around large language models (LLMs). That doesn\u2019t mean this is inevitable. I would like our chances better if we could base our ultimate AIs around a different architecture, one that was more compatible with being able to get it to do what we would like it to do.</p>\n<p>One path for this is agent foundations, which involves solving math to make the programs work instead of relying on inscrutable giant matrices.</p>\n<p>Even if we do not manage that, decision theory and game theory are potentially important for navigating the critical period in front of us, for life in general, and for figuring out what the post-transformation AI world might look like, and thus what choice we make now might do to impact that.</p>\n<p>There are not that many people working on these problems. Actual Progress would be super valuable. So even if we expect the median outcome does not involve enough progress to matter, I think it\u2019s still worth taking a shot.</p>\n<p>The flip side is you worry about people \u2018doing decision theory into the void\u2019 where no one reads their papers or changes their actions. That\u2019s a real issue. As is the increased urgency of other options. Still, I think these efforts are worth supporting, in general.</p>\n\n\n<h4 class=\"wp-block-heading\">Orthogonal</h4>\n\n\n<p>Focus: AI alignment via agent foundations</p>\n<p>Leaders: Tamsin Leake</p>\n<p>Funding Needed: Medium</p>\n<p>Confidence Level: High</p>\n<p>I have funded Orthogonal in the past. They are definitely doing the kind of work that, if it succeeded, might actually amount to something, and would help us get through this to a future world we care about. It\u2019s a long shot, but a long shot worth trying. They very much have the \u2018old school\u2019 Yudkowsky view that relatively hard takeoff is likely and most alignment approaches are fools errands. My sources are not as enthusiastic as they once were, but there are only a handful of groups trying that have any chance at all, and this still seems like one of them.</p>\n<p>Donate <a href=\"https://www.every.org/orthogonal?donateTo=orthogonal#/donate/card\">here</a>, or get in touch at tammy@orxl.org.</p>\n\n\n<h4 class=\"wp-block-heading\">Topos Institute</h4>\n\n\n<p>Focus: Math for AI alignment</p>\n<p>Leaders: Brendan Fong and David Spivak.</p>\n<p>Funding Needed: High</p>\n<p>Confidence Level: High</p>\n<p>Topos is essentially Doing Math to try and figure out what to do about AI and AI Alignment. I\u2019m very confident that they are qualified to (and actually will) turn donated money (partly via coffee) into math, in ways that might help a lot. I am also confident that the world should allow them to attempt this.</p>\n<p>They\u2019re now working with ARIA. That seems great.</p>\n<p>Ultimately it all likely amounts to nothing, but the upside potential is high and the downside seems very low. I\u2019ve helped fund them in the past and am happy about that.</p>\n<p>To donate, go<a href=\"https://topos.institute/contact\"> here</a>, or get in touch at info@topos.institute.</p>\n\n\n<h4 class=\"wp-block-heading\">Eisenstat Research</h4>\n\n\n<p>Focus: Two people doing research at MIRI, in particular Sam Eisenstat</p>\n<p>Leader: Sam Eisenstat</p>\n<p>Funding Needed: Medium</p>\n<p>Confidence Level: High</p>\n<p>Given Sam Eisenstat\u2019s previous work, including from 2025, it seems worth continuing to support him, including supporting researchers. I still believe in this stuff being worth working on, obviously only support if you do as well. He\u2019s funded for now but that\u2019s still only limited runway.</p>\n<p>To donate, contact sam@intelligence.org.</p>\n\n\n<h4 class=\"wp-block-heading\">AFFINE Algorithm Design</h4>\n\n\n<p>Focus: Johannes Mayer does agent foundations work</p>\n<p>Leader: Johannes Mayer</p>\n<p>Funding Needed: Low</p>\n<p>Confidence Level: Medium</p>\n<p>Johannes Mayer does solid agent foundations work, and more funding would allow him to hire more help.</p>\n<p>To donate, contact j.c.mayer240@gmail.com.</p>\n\n\n<h4 class=\"wp-block-heading\">CORAL (Computational Rational Agents Laboratory)</h4>\n\n\n<p>Focus: Examining intelligence</p>\n<p>Leader: Vanessa Kosoy</p>\n<p>Funding Needed: Medium</p>\n<p>Confidence Level: High</p>\n<p>This is Vanessa Kosoy and Alex Appel, who have another research agenda formerly funded by MIRI that now needs to stand on its own after their refocus. I once again believe this work to be worth continuing even if the progress isn\u2019t what one might hope. I wish I had the kind of time it takes to actually dive into these sorts of theoretical questions, but alas I do not, or at least I\u2019ve made a triage decision not to.</p>\n<p><a href=\"https://www.every.org/coral-research#/donate/card\">To donate, click here</a>. For larger amounts contact directly at vanessa@alter.org.il</p>\n\n\n<h4 class=\"wp-block-heading\">Mathematical Metaphysics Institute</h4>\n\n\n<p>Focus: Searching for a mathematical basis for metaethics.</p>\n<p>Leader: Alex Zhu</p>\n<p>Funding Needed: Low</p>\n<p>Confidence Level: Low</p>\n<p>Alex Zhu has run iterations of the Math &amp; Metaphysics Symposia, which had some excellent people in attendance, and intends partly to do more things of that nature. He thinks eastern philosophy contains much wisdom relevant to developing a future \u2018decision-theoretic basis of metaethics\u2019 and plans on an 8+ year project to do that.</p>\n<p>I\u2019ve seen plenty of signs that the whole thing is rather bonkers, but also strong endorsements from a bunch of people I trust that there is good stuff here, and the kind of crazy that is sometimes crazy enough to work. So there\u2019s a lot of upside. If you think this kind of approach has a chance of working, this could be very exciting. For additional information, you can see <a href=\"https://docs.google.com/document/d/1IH69QAU5T6DcBt7gAvtCyDqhMGSuCnvjSrYBFB-1iL0/edit?usp=sharing\">this google doc</a>.</p>\n<p>To donate, message Alex at alex@mathematicalmetaphysics.org.</p>\n\n\n<h4 class=\"wp-block-heading\">Focal at CMU</h4>\n\n\n<p>Focus: Game theory for cooperation by autonomous AI agents</p>\n<p>Leader: Vincent Conitzer</p>\n<p>Funding Needed: Medium</p>\n<p>Confidence Level: Low</p>\n<p>This is an area MIRI and the old rationalist crowd thought about a lot back in the day. There are a lot of ways for advanced intelligences to cooperate that are not available to humans, especially if they are capable of doing things in the class of sharing source code or can show their decisions are correlated with each other.</p>\n<p>With sufficient capability, any group of agents should be able to act as if it is a single agent, and we shouldn\u2019t need to do the game theory for them in advance either. I think it\u2019s good things to be considering, but one should worry that even if they do find answers it will be \u2018into the void\u2019 and not accomplish anything. Based on my technical analysis I wasn\u2019t convinced Focal was going to sufficiently interesting places with it, but I\u2019m not at all confident in that assessment.</p>\n<p>They note they\u2019re also interested in the dynamics prior to Ai becoming superintelligent, as the initial conditions plausibly matter a lot.</p>\n<p>To donate, reach out to Vincent directly at conitzer@cs.cmu.edu to be guided through the donation process.</p>\n\n\n<h4 class=\"wp-block-heading\">Organizations Doing Cool Other Stuff Including Tech</h4>\n\n\n<p>This section is the most fun. You get unique projects taking big swings.</p>\n\n\n<h4 class=\"wp-block-heading\">ALLFED</h4>\n\n\n<p>Focus: Feeding people with resilient foods after a potential nuclear war</p>\n<p>Leaders: David Denkenberger</p>\n<p>Funding Needed: High</p>\n<p>Confidence Level: Medium</p>\n<p>As far as I know, no one else is doing the work ALLFED is doing. A resilient food supply ready to go in the wake of a nuclear war (or other major disaster with similar dynamics) could be everything. There\u2019s a small but real chance that the impact is enormous. In my 2021 SFF round, I went back and forth with them several times over various issues, ultimately funding them, <a href=\"https://thezvi.substack.com/i/45469698/nuclear-war\">you can read about those details here</a>.</p>\n<p>I think all of the concerns and unknowns from last time essentially still hold, as does the upside case, so it\u2019s a question of prioritization, how likely you view nuclear war scenarios and how much promise you see in the tech.</p>\n<p>If you are convinced by the viability of the tech and ability to execute, then there\u2019s a strong case that this is a very good use of funds.</p>\n<p>I think that this is a relatively better choice if you expect AI to remain a normal technology for a while or if your model of AI risks includes a large chance of leading to a nuclear war or other cascading impacts to human survival, versus if you don\u2019t think this.</p>\n<p>Research and investigation on the technical details seems valuable here. If we do have a viable path to alternative foods and don\u2019t fund it, that\u2019s a pretty large miss, and I find it highly plausible that this could be super doable and yet not otherwise done.</p>\n<p>Donate <a href=\"https://allfed.info/donate\">here</a>, or reach out to info@allfed.info.</p>\n\n\n<h4 class=\"wp-block-heading\">Good Ancestor Foundation</h4>\n\n\n<p>Focus: Collaborations for tools to increase civilizational robustness to catastrophes</p>\n<p>Leader: Colby Thompson</p>\n<p>Funding Needed: High</p>\n<p>Confident Level: High</p>\n<p>The principle of \u2018a little preparation now can make a huge difference to resilience and robustness in a disaster later, so it\u2019s worth doing even if the disaster is not so likely\u2019 generalizes. Thus, the Good Ancestor Foundation, targeting nuclear war, solar flares, internet and cyber outages, and some AI scenarios and safety work.</p>\n<p>A particular focus is archiving data and tools, enhancing synchronization systems and designing a novel emergency satellite system (first one goes up in June) to help with coordination in the face of disasters. They\u2019re also coordinating on hardening critical infrastructure and addressing geopolitical and human rights concerns.</p>\n<p>They\u2019ve also given out millions in regrants.</p>\n<p>One way I know they make good decisions is they continue to help facilitate the funding for my work, and make that process easy. They have my sincerest thanks. Which also means there is a conflict of interest, so take that into account.</p>\n<p>Donate <a href=\"https://goodancestor.com/donations/\">here</a>, or contact them at good@goodancestor.com.</p>\n\n\n<h4 class=\"wp-block-heading\">Charter Cities Institute</h4>\n\n\n<p>Focus: Building charter cities</p>\n<p>Leader: Kurtis Lockhart</p>\n<p>Funding Needed: Medium</p>\n<p>Confidence Level: Medium</p>\n<p>I do love charter cities. There is little question they are attempting to do a very good thing and are sincerely going to attempt to build a charter city in Africa, where such things are badly needed. Very much another case of it being great that someone is attempting to do this so people can enjoy better institutions, even if it\u2019s not the version of it I would prefer that would focus on regulatory arbitrage more.</p>\n<p>Seems like a great place for people who don\u2019t think transformational AI is on its way but do understand the value here.</p>\n<p>Donate to them <a href=\"https://chartercitiesinstitute.org/donate-to-cci/\">here</a>, or contact them via <a href=\"https://chartercitiesinstitute.org/contact/\">webform</a>.</p>\n\n\n<h4 class=\"wp-block-heading\">Carbon Copies for Independent Minds</h4>\n\n\n<p>Focus: Whole brain emulation</p>\n<p>Leader: Randal Koene</p>\n<p>Funding Needed: Medium</p>\n<p>Confidence Level: Low</p>\n<p>At this point, if it worked in time to matter, I would be willing to roll the dice on emulations. What I don\u2019t have is much belief that it will work, or the time to do a detailed investigation into the science. So flagging here, because if you look into the science and you think there is a decent chance, this becomes a good thing to fund.</p>\n<p>Donate <a href=\"https://carboncopies.org/Donate/\">here</a>, or contact them at contact@carboncopies.org.</p>\n\n\n<h4 class=\"wp-block-heading\">Organizations Focused Primarily on Bio Risk</h4>\n\n\n\n<h4 class=\"wp-block-heading\">Secure DNA</h4>\n\n\n<p>Focus: Scanning DNA synthesis for potential hazards</p>\n<p>Leader: Kevin Esvelt, Andrew Yao and Raphael Egger</p>\n<p>Funding Needed: Medium</p>\n<p>Confidence Level: Medium</p>\n<p>It is certainly an excellent idea. Give everyone fast, free, cryptographically screening of potential DNA synthesis to ensure no one is trying to create something we do not want anyone to create. AI only makes this concern more urgent. I didn\u2019t have time to investigate and confirm this is the real deal as I had other priorities even if it was, but certainly someone should be doing this.</p>\n<p>There is also another related effort, Secure Bio, if you want to go all out. I would fund Secure DNA first.</p>\n<p>To donate, contact them at contact@securedna.org.</p>\n\n\n<h4 class=\"wp-block-heading\">Blueprint Biosecurity</h4>\n\n\n<p>Focus: Increasing capability to respond to future pandemics, Next-gen PPE, Far-UVC.</p>\n<p>Leader: Jake Swett</p>\n<p>Funding Needed: Medium</p>\n<p>Confidence Level: Medium</p>\n<p>There is no question we should be spending vastly more on pandemic preparedness, including far more on developing and stockpiling superior PPE and in Far-UVC. It is rather a shameful that we are not doing that, and Blueprint Biosecurity plausibly can move substantial additional investment there. I\u2019m definitely all for that.</p>\n<p>To donate, reach out to donations@blueprintbiosecurity.org or head to the<a href=\"https://www.paypal.com/us/fundraiser/charity/4967107\"> Blueprint Bio PayPal Giving Fund</a>.</p>\n\n\n<h4 class=\"wp-block-heading\">Pour Domain</h4>\n\n\n<p>Focus: EU policy for AI enabled biorisks, among other things.</p>\n<p>Leader: Patrick Stadler</p>\n<p>Funding Needed: Low</p>\n<p>Confidence Level: Low</p>\n<p>Everything individually looks worthwhile but also rather scattershot. Then again, who am I to complain about a campaign for e.g. improved air quality? My worry is still that this is a small operation trying to do far too much, some of it that I wouldn\u2019t rank too high as a priority, and it needs more focus, on top of not having that clear big win yet. They are a French nonprofit.</p>\n<p>Donation details are at the very bottom of<a href=\"https://www.pourdemain.ngo/en/uber-uns\"> this page</a>, or you can contact them at info@pourdemain.ngo.</p>\n\n\n<h4 class=\"wp-block-heading\">ALTER Israel</h4>\n\n\n<p>Focus: AI safety and biorisk for Israel</p>\n<p>Leader: David Manheim</p>\n<p>Funding Needed: Low</p>\n<p>Confidence Level: Medium</p>\n<p>Israel has Ilya\u2019s company SSI (Safe Superintelligence) and otherwise often punches above its weight in such matters but is getting little attention. This isn\u2019t where my attention is focused but David is presumably choosing this focus for good reason.</p>\n<p>To support them, get in touch at contact@alter.org.il.</p>\n\n\n<h4 class=\"wp-block-heading\">Organizations That Can Advise You Further</h4>\n\n\n<p>The first best solution, as I note above, is to do your own research, form your own priorities and make your own decisions. This is especially true if you can find otherwise illegible or hard-to-fund prospects.</p>\n<p>However, your time is valuable and limited, and others can be in better positions to advise you on key information and find opportunities.</p>\n<p>Another approach to this problem, if you have limited time or actively want to not be in control of these decisions, is to give to regranting organizations, and take the decisions further out of your own hands.</p>\n\n\n<h4 class=\"wp-block-heading\">Effective Institutions Project (EIP) (As A Donation Advisor)</h4>\n\n\n<p>Focus: AI governance, advisory and research, finding how to change decision points</p>\n<p>Leader: Ian David Moss</p>\n<p>Confidence Level: High</p>\n<p>I discussed their direct initiatives earlier. This is listing them as a donation advisor and in their capacity of attempting to be a resource to the broader philanthropic community.</p>\n<p>They report that they are advising multiple major donors, and would welcome the opportunity to advise additional major donors. I haven\u2019t had the opportunity to review their donation advisory work, but what I have seen in other areas gives me confidence. They specialize in advising donors who have brad interests across multiple areas, and they list AI safety, global health, democracy and (peace and security).</p>\n<p>To donate,<a href=\"https://www.every.org/effective-institutions-project/f/eip-core-funding\"> click here</a>. If you have further questions or would like to be advised, contact them at info@effectiveinstitutionsproject.org.</p>\n\n\n<h4 class=\"wp-block-heading\">Longview Philanthropy</h4>\n\n\n<p>Focus: Conferences and advice on x-risk for those giving &gt;$1 million per year</p>\n<p>Leader: Simran Dhaliwal</p>\n<p>Funding Needed: None</p>\n<p>Confidence Level: Low</p>\n<p>Longview is not seeking funding, instead they are offering support to large donors, and you can give to their regranting funds, including the Emerging Challenges Fund on catastrophic risks from emerging tech, which focuses non-exclusively on AI.</p>\n<p>I had a chance to hear a pitch for them at The Curve and check out their current analysis and donation portfolio. It was a good discussion. There were definitely some areas of disagreement in both decisions and overall philosophy, and I worry they\u2019ll be too drawn to the central and legible (a common issue with such services).</p>\n<p>On the plus side, they\u2019re clearly trying, and their portfolio definitely had some good things in it. So I wouldn\u2019t want to depend on them or use them as a sole source if I had the opportunity to do something higher effort, but if I was donating on my own I\u2019d find their analysis useful. If you\u2019re considering relying heavily on them or donating to the funds, I\u2019d look at the fund portfolios in detail and see what you think.</p>\n<p>I pointed them to some organizations they hadn\u2019t had a chance to evaluate yet.</p>\n<p>They clearly seem open to donations aimed at particular RFPs or goals.</p>\n<p>To inquire about their services, contact them at <a href=\"mailto:info@longview.org\">info@longview.org</a>.</p>\n\n\n<h4 class=\"wp-block-heading\">Organizations That then Regrant to Fund Other Organizations</h4>\n\n\n<p>There were lots of great opportunities in SFF in both of my recent rounds. I was going to have an embarrassment of riches I was excited to fund.</p>\n<p>Thus I decided quickly that I would not be funding any regrating organizations. If you were in the business of taking in money and then shipping it out to worthy causes, well, I could ship directly to highly worthy causes.</p>\n<p>So there was no need to have someone else do that, or expect them to do better.</p>\n<p>That does not mean that others should not consider such donations.</p>\n<p>I see three important advantages to this path.</p>\n<ol>\n<li>Regranters can offer smaller grants that are well-targeted.</li>\n<li>Regranters save you a lot of time.</li>\n<li>Regranters avoid having others try to pitch on donations.</li>\n</ol>\n<p>Thus, if you are making a \u2018low effort\u2019 donation, and think others you trust that share your values to invest more effort, it makes more sense to consider regranters.</p>\n<p>In particular, if you\u2019re looking to go large, I\u2019ve been impressed by SFF itself, and there\u2019s room for SFF to scale both its amounts distributed and level of rigor.</p>\n\n\n<h4 class=\"wp-block-heading\">SFF Itself (!)</h4>\n\n\n<p>Focus: Give out grants based on recommenders, primarily to 501c(3) organizations</p>\n<p>Leaders: Andrew Critch and Jaan Tallinn</p>\n<p>Funding Needed: High</p>\n<p>Confidence Level: High</p>\n<p>If I had to choose a regranter right now to get a large amount of funding, my pick would be to partner with and participate in the SFF process as an additional funder. The applicants and recommenders are already putting in their effort, with plenty of room for each round to scale. It is very clear there are plenty of exciting places to put additional funds.</p>\n<p>With more funding, the decisions could improve further, as recommenders would be better motivated to devote more time, and we could use a small portion of additional funds to make them better resourced.</p>\n<p>The downside is that SFF can\u2019t \u2018go small\u2019 efficiently on either funders or causes.</p>\n<p>SFF does not accept donations but they are interested in partnerships with people or institutions who are interested in participating as a Funder in a future S-Process round. The minimum requirement for contributing as a Funder to a round is $250k. They are particularly interested in forming partnerships with American donors to help address funding gaps in 501(c)(4)\u2019s and other political organizations.</p>\n<p>This is a good choice if you\u2019re looking to go large and not looking to ultimately funnel towards relatively small funding opportunities or individuals.</p>\n\n\n<h4 class=\"wp-block-heading\">Manifund</h4>\n\n\n<p>Focus: Regranters to AI safety, existential risk, EA meta projects, creative mechanisms</p>\n<p>Leader: Austin Chen (austin at manifund.org).</p>\n<p>Funding Needed: Medium</p>\n<p>Confidence Level: Medium</p>\n<p>This is a regranter that gives its money to its own regranters, one of which was me, for unrestricted grants. They\u2019re the charity donation offshoot of Manifold. They\u2019ve played with crowdfunding, and with impact certificates, and ACX grants. They help run Manifest.</p>\n<p>You\u2019re essentially hiring these people to keep building a website and trying alternative funding allocation mechanisms, and for them to trust the judgment of selected regranters. That seems like a reasonable thing to do if you don\u2019t otherwise know where to put your funds and want to fall back on a wisdom of crowds of sorts. Or, perhaps, if you actively want to fund the cool website.</p>\n<p>Manifold itself did not apply, but I would think that would also be a good place to invest or donate in order to improve the world. It wouldn\u2019t even be crazy to go around subsidizing various markets. If you send me manna there, I will set aside and use that manna to subsidize markets when it seems like the place to do that.</p>\n<p>If you want to support Manifold itself, you can either donate or buy a SAFE by contacting Austin at austin@manifund.org.</p>\n<p>Also I\u2019m a regranter at Manifund, so if you wanted to, <a href=\"https://manifund.com/ZviMowshowitz\">you could use that to entrust me with funds to regrant</a>. As you can see I certainly feel I have plenty of good options here if I can\u2019t find a better local one, and if it\u2019s a substantial amount I\u2019m open to general directions (e.g. ensuring it happens relatively quickly, or a particular cause area as long as I think it\u2019s net positive, or the method of action or theory of impact). However, I\u2019m swamped for time, so I\u2019d probably rely mostly on what I already know.</p>\n\n\n<h4 class=\"wp-block-heading\">AI Risk Mitigation Fund</h4>\n\n\n<p>Focus: Spinoff of LTFF, grants for AI safety projects</p>\n<p>Leader: Thomas Larsen</p>\n<p>Funding Needed: Medium</p>\n<p>Confidence Level: High</p>\n<p>Seems very straightforwardly exactly what it is, a regranter that is usually in the low six figure range. Fellow recommenders were high on Larsen\u2019s ability to judge projects. If you think this is better than you can do on your own and you want to fund such projects, then go for it.</p>\n<p>I\u2019ve talked to them on background about their future plans and directions, and without sharing details their plans make me more excited here.</p>\n<p>Donate <a href=\"https://www.every.org/ai-risk-mitigation-fund\">here</a> or contact them at info@airiskfund.com.</p>\n\n\n<h4 class=\"wp-block-heading\">Long Term Future Fund</h4>\n\n\n<p>Focus: Grants of 4-6 figures mostly to individuals, mostly for AI existential risk</p>\n<p>Leader: Caleb Parikh (among other fund managers)</p>\n<p>Funding Needed: High</p>\n<p>Confidence Level: Low</p>\n<p>The pitch on LTFF is that it is a place for existential risk people who need modest cash infusions to ask for them, and to get them without too much overhead or distortion. Looking over the list of grants, there is at least a decent hit rate.</p>\n<p>One question is, are the marginal grants a lot less effective than the average grant?</p>\n<p>My worry is that I don\u2019t know the extent to which the process is accurate, fair, favors insiders or extracts a time or psychic tax on participants, favors legibility, or rewards \u2018being in the EA ecosystem\u2019 or especially the extent to which the net effects are distortionary and bias towards legibility and standardized efforts. Or the extent to which people use the system to extract funds without actually doing anything.</p>\n<p>That\u2019s not a \u2018I think this is bad,\u2019 it is a true \u2018I do not know.\u2019 I doubt they know either.</p>\n<p>What do we know? They say applications should take 1-2 hours to write and between 10 minutes and 10 hours to evaluate, although that does not include time forming the plan, and this is anticipated to be an ~yearly process long term. And I don\u2019t love that this concern is not listed under <a href=\"https://funds.effectivealtruism.org/funds/far-future#cons\">reasons not to choose to donate to the fund</a> (although the existence of that list at all is most welcome, and the reasons to donate don\u2019t consider the flip side either).</p>\n<p>Given their current relationship to EA funds, you likely should consider LTFF if and only if you both want to focus on AI existential risk via regrants and also want to empower and strengthen the existing EA formal structures and general ways of being.</p>\n<p>That\u2019s not my preference, but it could be yours.</p>\n<p>Donate <a href=\"https://funds.effectivealtruism.org/funds/far-future\">here</a>, or contact the fund managers at longtermfuture@effectivealtruismfunds.org.</p>\n\n\n<h4 class=\"wp-block-heading\">Foresight</h4>\n\n\n<p>Focus: Regrants, fellowships and events</p>\n<p>Leader: Allison Duettmann</p>\n<p>Funding Needed: Medium</p>\n<p>Confidence Level: Low</p>\n<p>Foresight also does other things. I\u2019m focusing here on their AI existential risk grants, which they offer on a rolling basis. I\u2019ve advised them on a small number of potential grants, but they rarely ask.</p>\n<p>The advantage on the regrant side would be to get outreach that wasn\u2019t locked too tightly into the standard ecosystem. The other Foresight activities all seem clearly like good things, but the bar these days is high and since they weren\u2019t the topic of the application I didn\u2019t investigate.</p>\n<p>Donate <a href=\"https://events.foresight.org/donate-join/\">here</a>, or reach out to foresight@foresight.org.</p>\n\n\n<h4 class=\"wp-block-heading\">Centre for Enabling Effective Altruism Learning &amp; Research (CEELAR)</h4>\n\n\n<p>Focus: Strategic incubator and launchpad for EA talent, research, and high-impact initiatives, with emphasis on AI safety, GCR reduction, and longtermist work</p>\n<p>Leader: Attila Ujvari</p>\n<p>Funding Needed: High</p>\n<p>Confidence Level: Low</p>\n<p>I loved the simple core concept of a \u2018catered hotel\u2019 where select people can go to be supported in whatever efforts seem worthwhile. They are now broadening their approach, scaling up and focusing on logistical and community supports, incubation and a general infrastructure play on top of their hotel. This feels less unique to me now and more of a typical (EA UK) community play, so you should evaluate it on that basis.</p>\n<p>Donate <a href=\"http://ceealar.org/donate\">here</a>, or reach out to contact@ceealar.org.</p>\n\n\n<h4 class=\"wp-block-heading\">Organizations That are Essentially Talent Funnels</h4>\n\n\n<p>I am less skeptical of prioritizing AI safety talent funnels than I was last year, but I remain skeptical.</p>\n<p>The central reason remains simple. If we have so many good organizations already, in need of so much funding, why do we need more talent funnels? Is talent our limiting factor? Are we actually in danger of losing important talent?</p>\n<p>The clear exception is leadership and management. There remains, it appears, a clear shortage of leadership and management talent across all charitable space, and startup space, and probably flat out all of space.</p>\n<p>Which means if you are considering stepping up and doing leadership and management, then that is likely more impactful than you might at first think.</p>\n<p>If there was a strong talent funnel specifically for leadership or management, that would be a very interesting funding opportunity. And yes, of course there still need to be some talent funnels. Right now, my guess is we have enough, and marginal effort is best spent elsewhere.</p>\n<p>What about for other talent? What about placements in government, or in the AI labs especially Anthropic of people dedicated to safety? What about the prospects for much higher funding availability by the time we are ready to put people to work?</p>\n<p>If you can pull it off, empowering talent can have a large force multiplier, and the opportunity space looks better than a year ago. It seems plausible that frontier labs will soak up every strong safety candidate they can find, since the marginal returns there are very high and needs are growing rapidly.</p>\n<p>Secondary worries include the danger you end up feeding capability researchers to AI labs, and the discount for the time delays involved.</p>\n<p>My hunch is this will still receive relatively more attention and funding than is optimal, but marginal funds here will still be useful if deployed in places that are careful to avoid being lab talent funnels.</p>\n\n\n<h4 class=\"wp-block-heading\">AI Safety Camp</h4>\n\n\n<p>Focus: Learning by doing, participants work on a concrete project in the field</p>\n<p>Leaders: Remmelt Ellen and Linda Linsefors and Robert Kralisch</p>\n<p>Funding Needed: Low</p>\n<p>Confidence Level: High</p>\n<p>By all accounts they are the gold standard for this type of thing. Everyone says they are great, I am generally a fan of the format, I buy that this can punch way above its weight or cost. If I was going to back something in this section, I\u2019d start here.</p>\n<p>Donors can reach out to Remmelt at remmelt@aisafety.camp, or <a href=\"https://www.every.org/ai-safety-camp/f/match-donation-drive\">leave a matched donation to support next projects</a>.</p>\n\n\n<h4 class=\"wp-block-heading\">Center for Law and AI Risk</h4>\n\n\n<p>Focus: Paying academics small stipends to move into AI safety work</p>\n<p>Leaders: Peter Salib (psalib @ central.uh.edu), Yonathan Arbel (yarbel @ law.ua.edu) and Kevin Frazier (kevin.frazier @ law.utexas.edu).</p>\n<p>Funding Needed: Low</p>\n<p>Confidence Level: High</p>\n<p>This strategy is potentially super efficient. You have an academic that is mostly funded anyway, and they respond to remarkably small incentives to do something they are already curious about doing. Then maybe they keep going, again with academic funding. If you\u2019re going to do \u2018field building\u2019 and talent funnel in a world short on funds for those people, this is doubly efficient. I like it. They\u2019re now moving into hiring an academic fellow, the theory being ~1 year of support to create a permanent new AI safety law professor.</p>\n<p>To donate, message one of leaders at the emails listed above.</p>\n\n\n<h4 class=\"wp-block-heading\">Speculative Technologies</h4>\n\n\n<p>Focus: Enabling ambitious research programs that are poor fits for both academia and VC-funded startups including but not limited to Drexlerian functional nanomachines, high-throughput tools and discovering new superconductors.</p>\n<p>Leader: Benjamin Reinhardt</p>\n<p>Funding Needed: Medium</p>\n<p>Confidence Level: Medium</p>\n<p>I have confirmation that Reinhardt knows his stuff, and we certainly could use more people attempting to build revolutionary hardware. If the AI is scary enough to make you not want to build the hardware, it would figure out how to build the hardware anyway. You might as well find out now.</p>\n<p>If you\u2019re looking to fund a talent funnel, this seems like a good choice.</p>\n<p>To donate, go<a href=\"https://spec.tech/get-involved\"> here</a> or reach out to info@spec.tech.</p>\n\n\n<h4 class=\"wp-block-heading\">Talos Network</h4>\n\n\n<p>Focus: Fellowships to other organizations, such as Future Society, Safer AI and FLI.</p>\n<p>Leader: Chiara Gerosa</p>\n<p>Funding Needed: Medium</p>\n<p>Confidence Level: Low</p>\n<p>They run two fellowship cohorts a year. They seem to place people into a variety of solid organizations, and are exploring the ability to get people into various international organizations like the OECD, UN or European Commission or EU AI Office.</p>\n<p>The more I am convinced people will actually get inside meaningful government posts, the more excited I will be.</p>\n<p>To donate, contact team@talosnetwork.org.</p>\n\n\n<h4 class=\"wp-block-heading\">MATS Research</h4>\n\n\n<p>Focus: Researcher mentorship for those new to AI safety.</p>\n<p>Leaders: Ryan Kidd and Christian Smith.</p>\n<p>Funding Needed: High</p>\n<p>Confidence Level: Medium</p>\n<p>MATS is by all accounts very good at what they do and they have good positive spillover effects on the surrounding ecosystem. The recruiting classes they\u2019re getting are outstanding.</p>\n<p>If (and only if) you think that what they do, which is support would-be alignment researchers starting out and especially transitioning from other professions, is what you want to fund, then you should absolutely fund them. That\u2019s a question of prioritization.</p>\n<p>Donate <a href=\"https://www.matsprogram.org/donate\">here</a>, or contact them via <a href=\"https://www.matsprogram.org/contact\">webform</a>.</p>\n\n\n<h4 class=\"wp-block-heading\">Epistea</h4>\n\n\n<p>Focus: X-risk residencies, workshops, coworking in Prague, fiscal sponsorships</p>\n<p>Leader: Irena Kotikova</p>\n<p>Funding Needed: Medium</p>\n<p>Confidence Level: Medium</p>\n<p>I see essentially two distinct things here.</p>\n<p>First, you have the umbrella organization, offering fiscal sponsorship for other organizations. Based on what I know from the charity space, this is a highly valuable service &#8211; it was very annoying getting Balsa a fiscal sponsor while we waited to become a full 501c3, even though we ultimately found a very good one that did us a solid, and also annoying figuring out how to be on our own going forward.</p>\n<p>Second, you have various projects around Prague, which seem like solid offerings in that class of action of building up EA-style x-risk actions in the area, if that is what you are looking for. So you\u2019d be supporting some mix of those two things.</p>\n<p>To donate, contact hello@epistea.org.</p>\n\n\n<h4 class=\"wp-block-heading\">Emergent Ventures</h4>\n\n\n<p>Focus: Small grants to individuals to help them develop their talent</p>\n<p>Leader: Tyler Cowen</p>\n<p>Funding Needed: Medium</p>\n<p>Confidence Level: High</p>\n<p>Emergent Ventures are not like the other talent funnels in several important ways.</p>\n<ol>\n<li>It\u2019s not about AI Safety. You can definitely apply for an AI Safety purpose, he\u2019s granted such applications in the past, but it\u2019s rare and topics run across the board, well beyond the range otherwise described in this post.</li>\n<li>Decisions are quick and don\u2019t require paperwork or looking legible. Tyler Cowen makes the decision, and there\u2019s no reason to spend much time on your end either.</li>\n<li>There isn\u2019t a particular cause area this is trying to advance. He\u2019s not trying to steer people to do any particular thing. Just to be more ambitious, and be able to get off the ground and build connections and so on. It\u2019s not prescriptive.</li>\n</ol>\n<p>I strongly believe this is an excellent way to boost the development of more talent, as long as money is serving as a limiting factor on the project, and that it is great to develop talent even if you don\u2019t get to direct or know where it is heading. Sure, I get into rhetorical arguments with Tyler Cowen all the time, around AI and also other things, and we disagree strongly about some of the most important questions where I don\u2019t understand how he can continue to have the views he expresses, but this here is still a great project, an amazingly cost-efficient intervention.</p>\n<p>Donate <a href=\"https://mercatus.donorsupport.co/page/donatehome\">here</a> (specify \u201cEmergent Ventures\u201d in notes), or reach out to emergentventures@mercatus.gmu.edu.</p>\n\n\n<h4 class=\"wp-block-heading\">AI Safety Cape Town</h4>\n\n\n<p>Focus: AI safety community building and research in South Africa</p>\n<p>Leaders: Leo Hyams and Benjamin Sturgeon</p>\n<p>Funding Needed: Low</p>\n<p>Confidence Level: Low</p>\n<p>This is a mix of AI research and building up the local AI safety community. One person whose opinion I value gave the plan and those involved in it a strong endorsement, so including it based on that.</p>\n<p>To donate, reach out to leo@aisafetyct.com.</p>\n\n\n<h4 class=\"wp-block-heading\">ILINA Program</h4>\n\n\n<p>Focus: Talent for AI safety in Africa</p>\n<p>Leaders: Cecil Abungu</p>\n<p>Funding Needed: Low</p>\n<p>Confidence Level: Low</p>\n<p>I have a strong endorsement in hand in terms of their past work, if you think this is a good place to go in search of talent.</p>\n<p>To donate, reach out to cecil@ilinaprogram.org.</p>\n\n\n<h4 class=\"wp-block-heading\">Impact Academy Limited</h4>\n\n\n<p>Focus: Global talent accelerator and hiring partner for technical AI safety, supporting worker transitions into AI safety.</p>\n<p>Leader: Roy Hagemann and Varun Agarwal</p>\n<p>Funding Needed: Medium</p>\n<p>Confidence Level: Low</p>\n<p>They previously focused on India, one place with lots of talent, they\u2019re now global. A lot has turned over in the last year, so you\u2019ll want to check them out anew.</p>\n<p>To donate, contact info@impactacademy.org.</p>\n\n\n<h4 class=\"wp-block-heading\">Atlas Computing</h4>\n\n\n<p>Focus: Mapping &amp; creating missing orgs for AI safety (aka Charity Entrepreneurship for AI risk)</p>\n<p>Leaders: Evan Miyazono</p>\n<p>Funding Needed: Medium</p>\n<p>Confidence Level: Low</p>\n<p>There was a pivot this past year from technical research to creating \u2018missing orgs\u2019 in the AI risk space. That makes sense as a strategy if and only if you expect the funding necessary to come in, or you think they can do especially strong targeting. Given the change they will need to be reevaluated.</p>\n<p>They receive donations from<a href=\"https://opencollective.com/atlas-computing\"> here</a>, or you can email them at hello@atlascomputing.org.</p>\n\n\n<h4 class=\"wp-block-heading\">Principles of Intelligence (Formerly PIBBSS)</h4>\n\n\n<p>Focus: Fellowships and affiliate programs for new alignment researchers</p>\n<p>Leader: Lucas Teixeira and Dusan D. Nesic</p>\n<p>Funding Needed: High</p>\n<p>Confidence Level: Low</p>\n<p>There are some hits here. Gabriel Weil in particular has impressed me in our interactions and with his work and they cite a good technical paper. But also that\u2019s with a lot of shots on goal, and I\u2019d have liked to see some bigger hits by now.</p>\n<p>A breakdown revealed that, largely because they start with relatively senior people, most of them get placed in a way that doesn\u2019t require additional support. That makes them a better bet than many similar rivals.</p>\n<p>To donate, reach out to contact@princint.ai, or fund them through Manifund <a href=\"https://manifund.org/projects/pibbss---affiliate-program-funding-6-months-6-affiliates-or-more\">here</a>.</p>\n\n\n<h4 class=\"wp-block-heading\">Tarbell Center</h4>\n\n\n<p>Focus: Journalism fellowships for oversight of AI companies.</p>\n<p>Leader: Cillian Crosson (Ex-Talos Network; still on their board.)</p>\n<p>Funding Needed: High</p>\n<p>Confidence Level: Medium</p>\n<p>They offer fellowships to support journalism that helps society navigate the emergence of increasingly advanced AI, and a few other journalism ventures. They have sponsored at least one person who went on to do good work in the area. They also sponsor article placement, which seems reasonably priced in the grand scheme of things, I think?</p>\n<p>I am not sure this is a place we need to do more investment, or if people trying to do this even need fellowships. Hard to say. There\u2019s certainly a lot more tech reporting and more every day, if I\u2019m ever short of material I have no trouble finding more.</p>\n<p>It is still a small amount of money per person that can meaningfully help people get on their feet and do something useful. We do in general need better journalism. They seem to be in a solid place but also I\u2019d be fine with giving a bunch more funding to play with, they seem pretty unique.</p>\n<p>Donate <a href=\"https://www.tarbellcenter.org/donate\">here</a>, or reach out to them via <a href=\"https://www.tarbellcenter.org/contact\">webform</a>.</p>\n\n\n<h4 class=\"wp-block-heading\">Catalyze Impact</h4>\n\n\n<p>Focus: Incubation of AI safety organizations</p>\n<p>Leader: Alexandra Bos</p>\n<p>Funding Needed: Medium</p>\n<p>Confidence Level: Low</p>\n<p>Why funnel individual talent when you can incubate entire organizations? I am not convinced that on the margin we currently need more of either, but I\u2019m more receptive to the idea of an incubator. Certainly incubators can be high leverage points for getting valuable new orgs and companies off the ground, especially if your model is that once the org becomes fundable it can unlock additional funding.</p>\n<p>If you think an incubator is worth funding, then the question is whether this is the right team. The application was solid all around, and their track record includes Timaeus and Carma, although counterfactuals are always difficult. Beyond that I don\u2019t have a differentiator on why this is the team.</p>\n<p>To donate, contact them at info@catalyze-impact.org.</p>\n\n\n<h4 class=\"wp-block-heading\">CeSIA within EffiSciences</h4>\n\n\n<p>Focus: New AI safety org in Paris, discourse, R&amp;D collaborations, talent pipeline</p>\n<p>Leaders: Charbel-Raphael Segerie, Florent Berthet</p>\n<p>Funding Needed: Low</p>\n<p>Confidence Level: Low</p>\n<p>They\u2019re doing all three of discourse, direct work and talent funnels. They run the only university AI safety course in Europe, maintain the AI Safety Atlas, and have had their recommendations integrated verbatim into the EU AI Act\u2019s Code of Practice. Their two main priorities are supporting the enforcement of the EU AI Act, and driving international agreements on AI red lines.</p>\n<p>To donate, go<a href=\"https://www.securite-ia.fr/en/agir\"> here</a>, or contact them at contact@securite-ia.fr.</p>\n\n\n<h4 class=\"wp-block-heading\">Stanford Existential Risk Initiative (SERI)</h4>\n\n\n<p>Focus: Recruitment for existential risk causes</p>\n<p>Leader: Steve Luby</p>\n<p>Funding Needed: Medium</p>\n<p>Confidence Level: Low</p>\n<p>Stanford students certainly are one place to find people worth educating about existential risk. It\u2019s also an expensive place to be doing it, and a place that shouldn\u2019t need extra funding. And that hates fun. And it\u2019s not great that AI is listed third on their existential risk definition. So I\u2019m not high on them, but it sure beats giving unrestricted funds to your Alma Mater.</p>\n<p>Interested donors should contact Steve Luby directly at sluby@stanford.edu.</p>\n\n\n<h4 class=\"wp-block-heading\">Non-Trivial</h4>\n\n\n<p>Focus: Talent funnel directly to AI safety and biosecurity out of high school</p>\n<p>Leader: Peter McIntyre</p>\n<p>Funding Needed: Low</p>\n<p>Confidence Level: Low</p>\n<p>Having high school students jump straight to research and placement sounds good to me, and plausibly the best version of a talent funnel investment. I haven\u2019t confirmed details but I like the theory.</p>\n<p>To donate, get in touch at info@non-trivial.org.</p>\n\n\n<h4 class=\"wp-block-heading\">CFAR</h4>\n\n\n<p>Focus: Teaching rationality skills, seeking to make sense of the world and how to think</p>\n<p>Leader: Anna Salamon</p>\n<p>Funding Needed: High</p>\n<p>Confidence Level: High</p>\n<p>I am on the board of CFAR, so there is a direct and obvious conflict. Of course, I am on the board of CFAR exactly because I think this is a worthwhile use of my time, and also because Anna asked me. I\u2019ve been involved in various ways since the beginning, including the discussions about whether and how to create CFAR in the first place.</p>\n<p>CFAR is undergoing an attempted revival. There weren\u2019t workshops for many years, for a variety of reasons including safety concerns and also a need to reorient. The workshops are now starting up again, with a mix of both old and new units, and I find much of the new material interesting and potentially valuable. I\u2019d encourage people to<a href=\"https://www.rationality.org/workshops/upcoming\"> consider attending workshops</a>, and also donating.</p>\n<p>To donate,<a href=\"https://www.rationality.org/donate\"> click here</a>, or reach out to contact@rationality.org.</p>\n\n\n<h4 class=\"wp-block-heading\">The Bramble Center</h4>\n\n\n<p>Focus: Workshops in the style of CFAR but focused on practical courage, forming high value relationships between attendees with different skill sets and learning to care for lineages, in the hopes of repairing the anglosphere and creating new capable people to solve our problems including AI in more grounded ways.</p>\n<p>Leader: Anna Salamon</p>\n<p>Funding Needed: Low</p>\n<p>Confidence Level: High</p>\n<p>LARC is kind of a spin-off of CFAR, a place to pursue a different kind of agenda. I absolutely do not have high confidence that this will succeed, but I do have high confidence that this is a gamble worth taking, and that if those involved here (especially Anna Salamon but also others that I know) want to devote their time to trying this, that we should absolutely give them that opportunity.</p>\n<p>Donate <a href=\"https://every.org/bramble\">here</a>.</p>\n\n\n<h4 class=\"wp-block-heading\">Final Reminders</h4>\n\n\n<p>If an organization was not included here, or was removed for the 2025 edition, again, that does not mean they aren\u2019t good, or even that I wouldn\u2019t endorse them if asked.</p>\n<p>It could be because I am not aware of the organization, or lack sufficient knowledge at this point to be confident in listing them, or I fear my knowledge is obsolete.</p>\n<p>It could be that they asked to be excluded, which happened in several cases.</p>\n<p>If by accident I included you and you didn\u2019t want to be included and I failed to remove you, or you don\u2019t like the quote here, I sincerely apologize and will edit you out right away, no questions asked.</p>\n<p>If an organization is included here, that is a good thing, but again, it does not mean you should donate without checking if it makes sense based on what you think is true, how you think the world works, what you value and what your priorities are. There are no universal right answers.</p>"
            ],
            "link": "https://thezvi.wordpress.com/2025/11/26/the-big-nonprofits-post-2025/",
            "publishedAt": "2025-11-26",
            "source": "TheZvi",
            "summary": "There remain lots of great charitable giving opportunities out there. I have now had three opportunities to be a recommender for the Survival and Flourishing Fund (SFF). I wrote in detail about my first experience back in 2021, where I &#8230; <a href=\"https://thezvi.wordpress.com/2025/11/26/the-big-nonprofits-post-2025/\">Continue reading <span class=\"meta-nav\">&#8594;</span></a>",
            "title": "The Big Nonprofits Post 2025"
        },
        {
            "content": [],
            "link": "https://xkcd.com/3173/",
            "publishedAt": "2025-11-26",
            "source": "XKCD",
            "summary": "<img alt=\"Every weekend I take an ATV out into the desert and spend a day tracing a faint &quot;(C) GOOGLE 2009&quot; watermark across the landscape.\" src=\"https://imgs.xkcd.com/comics/satellite_imagery.png\" title=\"Every weekend I take an ATV out into the desert and spend a day tracing a faint &quot;(C) GOOGLE 2009&quot; watermark across the landscape.\" />",
            "title": "Satellite Imagery"
        },
        {
            "content": [],
            "link": "https://zed.dev/blog/nerd-sniped-project-search",
            "publishedAt": "2025-11-26",
            "source": "Zed Blog",
            "summary": "Why was it slow, and why is it still slow?",
            "title": "Nerd-sniped: Project Search"
        }
    ],
    "lookbackDays": 1,
    "publishDate": "2025-11-26"
}