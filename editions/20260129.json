{
    "articles": [
        {
            "content": [],
            "link": "https://emschwartz.me/building-docs-like-a-product/",
            "publishedAt": "2026-01-29",
            "source": "Evan Schwartz",
            "summary": "<p>Stripe is famous for having some of the best product docs, largely because they are <a href=\"https://stripe.com/blog/markdoc\">\"designed to feel like an application rather than a traditional user manual\"</a>. I spent much of the last week building and writing the docs for Scour, and I am quite proud of the results.</p> <h2 id=\"interactive-docs\">Interactive Docs</h2><p><a href=\"https://scour.ing/\">Scour</a> is a personalized content feed, not an SDK or API, so I started by asking myself what the equivalent of working code or copyable snippets is for this type of product. The answer: interactive pieces of the product, built right into the docs themselves.</p> <h3 id=\"scouring-hacker-news-for-hidden-gems\">Scouring Hacker News for Hidden Gems</h3><p>The <a href=\"https://scour.ing/docs/hacker-news\">guide for Hacker News readers</a> is one of the sections I'm most proud of. When describing Scour to people, I often start with the <a href=\"https://scour.ing/docs/evan\">origin story</a> of wanting a tool that could search for posts related to my interests from the thousands submitted to HN that never make it to the front page.</p> <p>Built right into the guide is a live search bar that searches posts that have been submitted to HN, <em>but that have not been on the front page</em>. <a href=\"https://scour.ing/docs/hacker-news#see-for-yourself\">Try it out!</a> You might find some hidden gems.</p> <p><a href=\"https://scour.ing/docs/hacker-news#see-for-yourself\"><img",
            "title": "Building Docs Like a Product"
        },
        {
            "content": [
                "<div class=\"lead\"><p><strong class=\"font-semibold text-navy-950\">I\u2019m Ben Johnson, and I work on Litestream at Fly.io. Litestream is the missing backup/restore system for SQLite. It\u2019s free, open-source software that should run anywhere, and</strong> <a href=\"https://fly.io/blog/litestream-v050-is-here/\" title=\"\"><strong class=\"font-semibold text-navy-950\">you can read more about it here</strong></a><strong class=\"font-semibold text-navy-950\">.</strong></p>\n</div>\n<p>Each time we write about it, we get a little bit better at golfing down a description of what Litestream is. Here goes: Litestream is a Unix-y tool for keeping a SQLite database synchronized with S3-style object storage. It&rsquo;s a way of getting the speed and simplicity wins of SQLite without exposing yourself to catastrophic data loss. Your app doesn&rsquo;t necessarily even need to know it&rsquo;s there; you can just run it as a tool in the background.</p>\n\n<p>It&rsquo;s been a busy couple weeks!</p>\n\n<p>We recently <a href=\"https://fly.io/blog/design-and-implementation/\" title=\"\">unveiled Sprites</a>. If you don&rsquo;t know what Sprites are, you should just <a href=\"https://sprites.dev/\" title=\"\">go check them out</a>. They&rsquo;re one of the coolest things we&rsquo;ve ever shipped. I won&rsquo;t waste any more time selling them to you. Just, Sprites are a big deal, and so it&rsquo;s a big deal to me that Litestream is a load-bearing component for them.</p>\n\n<p>Sprites rely directly on Litestream in two big ways.</p>\n\n<p>First, Litestream SQLite is the core of our global Sprites orchestrator. Unlike our flagship Fly Machines product, which relies on a centralized Postgres cluster, our Elixir Sprites orchestrator runs directly off S3-compatible object storage. Every organization  enrolled in Sprites gets their own SQLite database, synchronized by Litestream.</p>\n\n<p>This is a fun design. It takes advantage of the &ldquo;many SQLite databases&rdquo; pattern, which is under-appreciated. It&rsquo;s got nice scaling characteristics. Keeping that Postgres cluster happy as Fly.io grew has been a major engineering challenge.</p>\n\n<p>But as far as Litestream is concerned, the orchestrator is boring, and so that&rsquo;s all I&rsquo;ve got to say about it. The second way Sprites use Litestream is much more interesting.</p>\n\n<p>Litestream is built directly into the disk storage stack that runs on every Sprite.</p>\n\n<p>Sprites launch in under a second, and every one of them boots up with 100GB of durable storage. That&rsquo;s a tricky bit of engineering. We&rsquo;re able to do this because the  root of storage for Sprites is S3-compatible object storage, and we&rsquo;re able to make it fast by keeping a database of in-use storage blocks that takes advantage of attached NVMe as a read-through cache. The system that does this is JuiceFS, and the database \u2014 let&rsquo;s call it &ldquo;the block map&rdquo; \u2014 is a rewritten metadata store, based (you guessed it) on BoltDB.</p>\n\n<p>I kid! It&rsquo;s Litestream SQLite, of course.</p>\n<h2 class=\"group flex items-start whitespace-pre-wrap relative mt-14 sm:mt-16 mb-4 text-navy-950 font-heading\" id=\"sprite-storage-is-fussy\"><a class=\"inline-block align-text-top relative top-[.15em] w-6 h-6 -ml-6 after:hash opacity-0 group-hover:opacity-100 transition-all\" href=\"https://fly.io/blog/feed.xml#sprite-storage-is-fussy\"></a><span class=\"plain-code\">Sprite Storage Is Fussy</span></h2>\n<p>Everything in a Sprite is designed to come up fast.</p>\n\n<p>If the Fly Machine underneath a Sprite bounces, we might need to reconstitute the block map from object storage. Block maps aren&rsquo;t huge, but they&rsquo;re not tiny; maybe low tens of megabytes worst case.</p>\n\n<p>The thing is, this is happening while the Sprite boots back up. To put that in perspective, that&rsquo;s something that can happen in response to an incoming web request; that is, we have to finish fast enough to generate a timely response to that request. The time budget is small.</p>\n\n<p>To make this even faster, we are integrating Litestream VFS to improve start times.The VFS is a dynamic library you load into your app. Once you do, you can do stuff like this:</p>\n<div class=\"highlight-wrapper group relative bash\">\n  <button class=\"bubble-wrap z-20 absolute right-9 -mr-0.5 top-1.5 text-transparent group-hover:text-gray-400 group-hover:hocus:text-white focus:text-white bg-transparent group-hover:bg-gray-900 group-hover:hocus:bg-gray-700 focus:bg-gray-700 transition-colors grid place-items-center w-7 h-7 rounded-lg outline-none focus:outline-none\" type=\"button\">\n    <svg class=\"w-4 h-4 pointer-events-none\" fill=\"none\" stroke=\"currentColor\" stroke-linecap=\"round\" stroke-linejoin=\"round\" stroke-width=\"1.35\" viewBox=\"0 0 16 16\" xmlns=\"http://www.w3.org/2000/svg\"><g><path d=\"M9.912 8.037h2.732c1.277 0 2.315-.962 2.315-2.237a2.325 2.325 0 00-2.315-2.31H2.959m10.228 9.01H2.959M6.802 8H2.959\"><path d=\"M11.081 6.466L9.533 8.037l1.548 1.571\"></g></svg>\n    <span class=\"bubble-sm bubble-tl [--offset-l:-9px] tail text-navy-950\">\n      Wrap text\n    </span>\n  </button>\n  <button class=\"bubble-wrap z-20 absolute right-1.5 top-1.5 text-transparent group-hover:text-gray-400 group-hover:hocus:text-white focus:text-white bg-transparent group-hover:bg-gray-900 group-hover:hocus:bg-gray-700 focus:bg-gray-700 transition-colors grid place-items-center w-7 h-7 rounded-lg outline-none focus:outline-none\" type=\"button\">\n    <svg class=\"w-4 h-4 pointer-events-none\" fill=\"none\" stroke=\"currentColor\" stroke-width=\"1.35\" viewBox=\"0 0 16 16\" xmlns=\"http://www.w3.org/2000/svg\"><g><path d=\"M10.576 7.239c0-.995-.82-1.815-1.815-1.815H3.315c-.995 0-1.815.82-1.815 1.815v5.446c0 .995.82 1.815 1.815 1.815h5.446c.995 0 1.815-.82 1.815-1.815V7.239z\"><path d=\"M10.576 10.577h2.109A1.825 1.825 0 0014.5 8.761V3.315A1.826 1.826 0 0012.685 1.5H7.239c-.996 0-1.815.819-1.816 1.815v1.617\"></g></svg>\n    <span class=\"bubble-sm bubble-tl [--offset-l:-6px] tail [--tail-x:calc(100%-30px)] text-navy-950\">\n      Copy to clipboard\n    </span>\n  </button>\n  <div class=\"highlight relative group\">\n    <pre class=\"highlight \"><code id=\"code-sbs6zqss\">sqlite&gt; .open file:///my.db?vfs<span class=\"o\">=</span>litestream\nsqlite&gt; PRAGMA litestream_time <span class=\"o\">=</span> <span class=\"s1\">'5 minutes ago'</span><span class=\"p\">;</span> \nsqlite&gt; SELECT <span class=\"k\">*</span> FROM sandwich_ratings ORDER BY RANDOM<span class=\"o\">()</span> LIMIT 3 <span class=\"p\">;</span> \n22|Veggie Delight|New York|4\n30|Meatball|Los Angeles|5\n168|Chicken Shawarma Wrap|Detroit|5\n</code></pre>\n  </div>\n</div>\n<p>Litestream VFS lets us run point-in-time SQLite queries hot off object storage blobs, answering queries before we&rsquo;ve downloaded the database.</p>\n\n<p>This is good, but it&rsquo;s not perfect. We had two problems:</p>\n\n<ol>\n<li>We could only read, not write. People write to Sprite disks. The storage stack needs to write, right away.\n</li><li>Running a query off object storage is a godsend in a cold start where we have no other alternative besides downloading the whole database, but it&rsquo;s not fast enough for steady state.\n</li></ol>\n\n<p>These are fun problems. Here&rsquo;s our first cut at solving them.</p>\n<h2 class=\"group flex items-start whitespace-pre-wrap relative mt-14 sm:mt-16 mb-4 text-navy-950 font-heading\" id=\"writable-vfs\"><a class=\"inline-block align-text-top relative top-[.15em] w-6 h-6 -ml-6 after:hash opacity-0 group-hover:opacity-100 transition-all\" href=\"https://fly.io/blog/feed.xml#writable-vfs\"></a><span class=\"plain-code\">Writable VFS</span></h2>\n<p>The first thing we&rsquo;ve done is made the VFS optionally read-write. This feature is pretty subtle; it&rsquo;s interesting, but it&rsquo;s not as general-purpose as it might look. Let me explain how it works, and then explain why it works this way.</p>\n<div class=\"callout\"><p>Keep in mind as you read this that this is about the VFS in particular. Obviously, normal SQLite databases using Litestream the normal way are writeable.</p>\n</div>\n<p>The VFS works by keeping an index of <code>(file,offset, size)</code> for every page of the database in object storage; the data comprising the index is stored, <a href=\"https://github.com/superfly/ltx\" title=\"\">in LTX files</a>, so that it&rsquo;s efficient for us to reconstitute it quickly when the VFS starts, and lookups are heavily cached. When we queried <code>sandwich_ratings</code> earlier, our VFS library intercepted the SQLite read method, looked up the requested page in the index, fetched it, and cached it.</p>\n\n<p>This works great for reads. Writes are harder.</p>\n\n<p>Behind the scenes in read-only mode, Litestream polls, so that we can detect new LTX files created by remote writers to the database. This supports a handy use case where we&rsquo;re running tests or doing slow analytical queries of databases that need to stay fast in prod.</p>\n\n<p>In write mode, we don&rsquo;t allow multiple writers, because multiple-writer distributed SQLite databases are the <a href=\"https://hellraiser.fandom.com/wiki/Lament_Configuration\" title=\"\">Lament Configuration</a> and we are not explorers over great vistas of pain. So the VFS in write-mode disables polling. We assume a single writer, and no additional backups to watch.</p>\n\n<p>Next, we buffer. Writes go to a local temporary buffer (&ldquo;the write buffer&rdquo;). Every second or so (or on clean shutdown), we sync the write buffer with object storage. Nothing written through the VFS is truly durable until that sync happens.</p>\n<div class=\"right-sidenote\"><p>Most storage block maps are much smaller than this, but still.</p>\n</div>\n<p>Now, remember the use case we&rsquo;re looking to support here. A Sprite is cold-starting and its storage stack needs to serve writes, milliseconds after booting, without having a full copy of the 10MB block map. This writeable VFS mode lets us do that.</p>\n\n<p>Critically, we support that use case only up to the same durability requirements that a Sprite already has. All storage on a Sprite shares this &ldquo;eventual durability&rdquo; property, so the terms of the VFS write make sense here. They probably don&rsquo;t make sense for your application. But if for some reason they do, have at it! To enable writes with Litestream VFS, just set the <code>LITESTREAM_WRITE_ENABLED</code> environment variable <code>&quot;true&quot;</code>.</p>\n\n<p><img src=\"https://fly.io/blog/litestream-writable-vfs/assets/write-path.png\" /></p>\n<h2 class=\"group flex items-start whitespace-pre-wrap relative mt-14 sm:mt-16 mb-4 text-navy-950 font-heading\" id=\"hydration\"><a class=\"inline-block align-text-top relative top-[.15em] w-6 h-6 -ml-6 after:hash opacity-0 group-hover:opacity-100 transition-all\" href=\"https://fly.io/blog/feed.xml#hydration\"></a><span class=\"plain-code\">Hydration</span></h2>\n<p>The Sprite storage stack uses SQLite in VFS mode. In our original VFS design, most data is kept in S3. Again: fine at cold start, not so fine in steady state.</p>\n\n<p>To solve this problem, we shoplifted a trick from <a href=\"https://docs.kernel.org/admin-guide/device-mapper/dm-clone.html\" title=\"\">systems like dm-clone</a>: background hydration. In hydration designs, we serve queries remotely while running a loop to pull the whole database.  When you start the VFS with the <code>LITESTREAM_HYDRATION_PATH</code> environment variable set, we&rsquo;ll hydrate to that file.</p>\n\n<p>Hydration takes advantage of <a href=\"https://fly.io/blog/litestream-revamped#point-in-time-restores-but-fast\" title=\"\">LTX compaction</a>, writing only the latest versions of each page. Reads don&rsquo;t block on hydration; we serve them from object storage immediately, and switch over to the hydration file when it&rsquo;s ready.</p>\n\n<p><img src=\"https://fly.io/blog/litestream-writable-vfs/assets/timeline.png\" /></p>\n\n<p>As for the hydration file? It&rsquo;s simply a full copy of your database. It&rsquo;s the same thing you get if you run <code>litestream restore</code>.</p>\n\n<p>Because this is designed for environments like Sprites, which bounce a lot, we write the database to a temporary file. We can&rsquo;t trust that the database is using the latest state every time we start up, not without doing a full restore, so we just chuck the hydration file when we exit the VFS. That behavior is baked into the VFS right now. This feature&rsquo;s got what Sprites need, but again, maybe not what your app wants.</p>\n<h2 class=\"group flex items-start whitespace-pre-wrap relative mt-14 sm:mt-16 mb-4 text-navy-950 font-heading\" id=\"putting-it-all-together\"><a class=\"inline-block align-text-top relative top-[.15em] w-6 h-6 -ml-6 after:hash opacity-0 group-hover:opacity-100 transition-all\" href=\"https://fly.io/blog/feed.xml#putting-it-all-together\"></a><span class=\"plain-code\">Putting It All Together</span></h2>\n<p>This is a post about two relatively big moves we&rsquo;ve made with our open-source Litestream project, but the features are narrowly scoped for problems that look like the ones our storage stack needs. If you think you can get use out of them, I&rsquo;m thrilled, and I hope you&rsquo;ll tell me about it.</p>\n\n<p>For ordinary read/write workloads, you don&rsquo;t need any of this mechanism. Litestream works fine without the VFS, with unmodified applications, just running as a sidecar alongside your application. The whole point of that configuration is to efficiently keep up with writes; that&rsquo;s easy when you know you have the whole database to work with when writes happen.</p>\n\n<p>But this whole thing is, to me, a valuable case study in how Litestream can get used in a relatively complicated and demanding problem domain. Sprites are very cool, and it&rsquo;s satisfying to know that every disk write that happens on a Sprite is running through Litestream.</p>"
            ],
            "link": "https://fly.io/blog/litestream-writable-vfs/",
            "publishedAt": "2026-01-29",
            "source": "Fly.io Blog",
            "summary": "<div class=\"lead\"><p><strong class=\"font-semibold text-navy-950\">I\u2019m Ben Johnson, and I work on Litestream at Fly.io. Litestream is the missing backup/restore system for SQLite. It\u2019s free, open-source software that should run anywhere, and</strong> <a href=\"https://fly.io/blog/litestream-v050-is-here/\" title=\"\"><strong class=\"font-semibold text-navy-950\">you can read more about it here</strong></a><strong class=\"font-semibold text-navy-950\">.</strong></p> </div> <p>Each time we write about it, we get a little bit better at golfing down a description of what Litestream is. Here goes: Litestream is a Unix-y tool for keeping a SQLite database synchronized with S3-style object storage. It&rsquo;s a way of getting the speed and simplicity wins of SQLite without exposing yourself to catastrophic data loss. Your app doesn&rsquo;t necessarily even need to know it&rsquo;s there; you can just run it as a tool in the background.</p> <p>It&rsquo;s been a busy couple weeks!</p> <p>We recently <a href=\"https://fly.io/blog/design-and-implementation/\" title=\"\">unveiled Sprites</a>. If you don&rsquo;t know what Sprites are, you should just <a href=\"https://sprites.dev/\" title=\"\">go check them out</a>. They&rsquo;re one of the coolest things we&rsquo;ve ever shipped. I won&rsquo;t waste any more time selling them to you. Just, Sprites are a big deal, and so it&rsquo;s a big deal to me that Litestream is a load-bearing component for them.</p> <p>Sprites rely directly on Litestream in two big ways.</p>",
            "title": "Litestream Writable VFS"
        },
        {
            "content": [],
            "link": "https://www.robinsloan.com/lab/feed-content/",
            "publishedAt": "2026-01-29",
            "source": "Robin Sloan",
            "summary": "<p>And the social media company is its publisher. <a href=\"https://www.robinsloan.com/lab/feed-content/\">Read here.</a></p>",
            "title": "The feed is the content"
        },
        {
            "content": [
                "<p>This is the irregular classifieds thread. Advertise whatever you want in the comments.</p><p>To keep things organized, please respond to the appropriate top-level comment:<strong> Employment, Dating, Read My Blog </strong>(also includes podcasts, books, etc)<strong>, Consume My Product/Service, Meetup, </strong>or <strong>Other. </strong>Don&#8217;t post new top-level comments; I will delete anything that&#8217;s not in the appropriate category.</p><p>Remember that posting dating ads is hard and scary. Please refrain from commenting too negatively on anyone&#8217;s value as a human being. I&#8217;ll be less strict about employers, bloggers, etc.</p><p>Potentially related links:</p><p>&#8212; <a href=\"https://jobs.80000hours.org/\">EA job board</a><br />&#8212; <a href=\"https://ea-internships.pory.app/\">EA internships</a><br />&#8212; <a href=\"https://www.lesswrong.com/community\">Find a Less Wrong/ACX meetup</a></p>"
            ],
            "link": "https://www.astralcodexten.com/p/acx-classifieds-126",
            "publishedAt": "2026-01-29",
            "source": "SlateStarCodex",
            "summary": "<p>This is the irregular classifieds thread. Advertise whatever you want in the comments.</p><p>To keep things organized, please respond to the appropriate top-level comment:<strong> Employment, Dating, Read My Blog </strong>(also includes podcasts, books, etc)<strong>, Consume My Product/Service, Meetup, </strong>or <strong>Other. </strong>Don&#8217;t post new top-level comments; I will delete anything that&#8217;s not in the appropriate category.</p><p>Remember that posting dating ads is hard and scary. Please refrain from commenting too negatively on anyone&#8217;s value as a human being. I&#8217;ll be less strict about employers, bloggers, etc.</p><p>Potentially related links:</p><p>&#8212; <a href=\"https://jobs.80000hours.org/\">EA job board</a><br />&#8212; <a href=\"https://ea-internships.pory.app/\">EA internships</a><br />&#8212; <a href=\"https://www.lesswrong.com/community\">Find a Less Wrong/ACX meetup</a></p>",
            "title": "ACX Classifieds 1/26"
        },
        {
            "content": [
                "<p>I spent a lot of time writing software with AI last year, and I had some pretty good successes, notably <a href=\"https://github.com/steveyegge/beads\">Beads</a> and <a href=\"https://github.com/steveyegge/gastown\">Gas Town</a>. I wrote a whole bunch of other systems, some still in progress. And I got a solid intuition, a feel for how AI\u2019s exponential progression is, well, progressing.</p><p>That intuition is how I created Gas Town. I believe the exponential curves; I believe everything Dario Amodei and Andrej Karpathy are saying about software. And if you were leaning in when Claude Code came out 11 months ago, then if you extrapolated from completions in 2023 to chat in 2024 to agents in early 2025, you arrived inescapably at orchestration arriving in early\u00a02026.</p><p>So towards the end of the year, I went looking for what I knew would be there, and found Gas Town right where I was looking for it. I was like a geologist who knew there would be an oil deposit there, and I drilled and it hit. I dug around and found a shape that <em>just</em> barely worked, with the best late 2025\u2019s models that hadn\u2019t been trained to be factory workers, plus a lot of duct tape. And janky as it may be, Gas Town has illuminated and kicked off the next wave for everyone.</p><p>I think I\u2019ve established a pretty good track record of predicting the future, from Death of the Junior Developer back in June 2024 when the job market was still super hot, to Revenge of the Junior Developer which predicted today\u2019s orchestrators 10 months ago, to Gas Town itself, which I think is pushing pretty far into the frontiers of what\u2019s possible\u00a0today.</p><p>All of my predictive power comes from believing the curves. It\u2019s that\u00a0simple.</p><p>In this post, I\u2019m going to make a prediction about which software will survive, if you believe Karpathy, in a world where AI writes all the software and is essentially infinitely capable. I think you can make a simple survival argument that comes down to selection pressure.</p><p>First let\u2019s talk about my credentials and qualifications for this post. My next-door neighbor Marv has a fat squirrel that runs up to his sliding-glass door every morning, waiting to be fed. It\u2019s against a city ordinance to feed them but Marv is 82 and he ain\u2019t having it. So the squirrel has grown chonkulous, and it shows up like clockwork when we\u2019re about to go golf or head to the range in his corvette or any of the other 82-year-old stuff we\u00a0do.</p><p>Marv\u2019s squirrel knows approximately the same amount about evolutionary biology as I do, and would score similarly to me on a University-level examination on the\u00a0subject.</p><p>But I have this hunch. I think I know how to predict whether your software is going to make it or not, if you assume (as I do) that Karpathy and Amodei are completely 100%\u00a0correct.</p><p>Let\u2019s see if I can convince\u00a0you.</p><h3>Are We Gonna Make\u00a0It?</h3><p>Karpathy describes a future where AI can build pretty much anything on demand, and we\u2019ve already seen early evidence of this in the form of emerging pressure on SaaS companies, as the buy vs build calculus changes. It\u2019s getting easier and easier to build what you need rather than buying it. We\u2019re seeing business departments <a href=\"https://www.amazon.com/Vibe-Coding-Building-Production-Grade-Software/dp/1966280025\">vibe coding</a> their own SaaS instead of re-upping with niche vendors. Three years ago, GPT-3.5 could barely write a coherent function, and now AI can write small-scale (but valuable) SaaS for you. The trajectory is exponential, so home-grown medium-scale SaaS will be on the table by\u00a0EOY.</p><p>It\u2019s not just SaaS. We\u2019ve already seen entire categories begin to be eaten up by AI: Stack Overflow and Chegg were early victims, but now we\u2019re seeing pressure on new sectors. Tier-1 customer support software, low-code/no-code systems, content generation tools (e.g. writing assistants), a whole lot of productivity tooling. Even IDE vendors are beginning to sweat over Claude Code, and with good\u00a0reason.</p><p>If you believe the AI researchers\u2013who have been spot-on accurate for literally four decades\u2013then all software sectors are threatened. Even if you don\u2019t buy into their vision wholesale, it\u2019s prudent to be cautious and at least think about\u00a0it.</p><p>I see boards and C-suites beginning to ask, how can we plan ahead to survive in the Software 3.0 era? But they don\u2019t have any sort of a framework for thinking about it yet. I think mine, while not perfect, provides a pretty good starting point. You be the\u00a0judge.</p><h3>The Selection Argument</h3><p>Inference costs tokens, which cost energy, which costs money. For purposes of computing software survival odds, we can think of {tokens, energy, money} all as being equivalent, and all are perpetually constrained. This resource constraint, I predict, will create a selection pressure that shapes the whole software ecosystem with a simple rule: <strong>software tends to survive if it saves cognition</strong>.</p><p>This is, roughly speaking, an evolutionary argument, at least according to Marv\u2019s Squirrel. In any environment with constrained resources, entities that use those resources efficiently tend to outcompete those that don\u2019t. Karpathy\u2019s Software 3.0 ecosystem should similarly select for tools that minimize cognitive expenditure, which you can measure and model pretty closely as token\u00a0spend.</p><p>I think that systems have a financial and indeed an ethical obligation to minimize compute costs for solving cognitive problems, because energy is rapidly becoming the world\u2019s biggest constraint. For starters, this obligation implies that we should use smaller models when they can perform the same tasks. That may seem obvious, but I think our current monolithic coding agents aren\u2019t doing a very good job of it. Orchestration systems give me hope that we can do a better job of allocating work to the right model tiers in the future, saving energy and money in the\u00a0process.</p><p>But even with a bazillion smaller models deployed everywhere including your dental fillings, I think there is still a role for large classes of \u201cold-fashioned\u201d software systems that aren\u2019t models, and don\u2019t necessarily use AI at all. This essay concerns those systems, which I think will survive and thrive if they have the right properties.</p><p>My hunch is that if a tool saves AIs tokens, it has a high chance of being used and surviving. And tools that don\u2019t save tokens will gradually be phased out. That\u2019s not a guarantee; it\u2019s just going to be a strong general selective pressure. There are caveats and exceptions and carve-outs that we\u2019ll\u00a0discuss.</p><p>The good news is, there are plenty of concrete ways you can plan ahead to help ensure your software survives the transition to the Karpathy Software 3.0\u00a0era.</p><h3>The Survival\u00a0Ratio</h3><p>For any tool T, my model posits that survival looks something like the following, in Squirrel\u00a0Math:</p><blockquote>Survival(T) \u221d (Savings \u00d7 Usage \u00d7 H) / (Awareness_cost + Friction_cost)</blockquote><p>Where:</p><ul><li><strong>Survival(T)</strong> is a fitness function\u200a\u2014\u200athe ratio of cognitive value to cognitive cost. A tool tends to survive when this ratio exceeds 1\u200a\u2014\u200awhen using it saves more than knowing and operating it costs. Tools with very high ratios in the 1000s, like grep, get plot armor and become indestructible. Below 1, the tool is selected against and gets routed around; LLMs will synthesize alternatives.</li><li><strong>Savings</strong> is the cognitive savings: how many tokens does using this tool save, vs. synthesizing the equivalent functionality from\u00a0scratch?</li><li><strong>Usage</strong> is how often and how broadly the tool applies to different situations. Niche tools may require an incredible Savings in order to compensate for their narrow usage. And even a light Savings can ensure survival if the tool is a Swiss army knife that\u2019s fit for many purposes.</li><li><strong>H</strong> is a human coefficient that factors in demand for human-created material. It\u2019s a wildcard, but it will produce some strong survivors that don\u2019t fit any of the efficiency criteria.</li><li><strong>Awareness_cost</strong> is the energy required for agents to know the tool exists, understand what it offers, and choose to reach for it. It\u2019s the energy to get it into their training sets, or else energy you need to spend at inference time, as part of your precious prompting, to teach agents about your\u00a0tool.</li><li><strong>Friction_cost</strong> is the energy lost to errors, retries, and misunderstandings when actually using the tool. Agents will often give up and switch back to a less effective but more familiar/reliable tool if they are struggling with the interface on an ostensibly more efficient tool. They see the efficiency they\u2019re losing in the fumbling, and they retreat to less efficient but more predictable methods. Conversely, if the tool is very low friction, agents will revel in it like panthers in catnip, as I\u2019ll discuss in the Desire Paths\u00a0section.</li></ul><p>To be clear: this isn\u2019t a proof, or real math, or anything like that. It\u2019s just a visual aid, a mental model I use for thinking about software survival in the age of resource-limited super-intelligence.</p><p>I debated with Claude endlessly about this selection model, and Claude made me discard a bunch of interesting but less defensible claims. But in the end, I was able to convince Claude it\u2019s a good model, and I aim to do the same with you. Let me know what you think by complaining that AI sucks on\u00a0HN!</p><p>In the Survival Ratio model, the minimum viable ratio for survival is 1. If tool T saves tokens overall, and the agent knows about it, and it works smoothly, then T will have positive selection pressure. Conversely, a tool that doesn\u2019t save tokens, or costs more to use (or to know about) than what it saves, will get selected against, sooner or\u00a0later.</p><p>Your software\u2019s survivability threshold floats above 1 when there\u2019s competition. A useful tool with a ratio of 1.2 can get beaten by a competitor with a 2.5. Agent attention is going to become a key <a href=\"https://www.ucl.ac.uk/bartlett/publications/2023/nov/algorithmic-attention-rents-theory-digital-platform-market-power\">battleground</a>, driving awareness cost higher. In a niche domain without much competition, like say DNA sequencing, any tool that saves a few tokens might be quickly noticed and see lots of use. But in crowded domains, awareness isn\u2019t automatic, big mediocre players may have all the recognition, and you may have to pay extra to be noticed by agents. We\u2019ll talk about different ways to accomplish this today, and hint at more to\u00a0come.</p><p>As a framework, I think the Squirrel Selection Model is Good Enough: it\u2019s better than what most C-suites are working with today. My model gives you six levers you can use for planning your software\u2019s survival. Let\u2019s go through them in turn. First we\u2019ll talk about the two most powerful long-term levers, both of which are all about saving\u00a0money.</p><h3>Saving Cognition by Saving\u00a0Tokens</h3><p>Your first two levers help maximize the Savings term in the numerator of the Survival Ratio. Let\u2019s first talk about why these levers exist at all, in a world where AIs are superintelligent and can synthesize literally any software they\u00a0want.</p><p>Coding agents can do complex work in their heads, so to speak, purely through inference on GPUs. But if you\u2019ve ever seen, for instance, how they do multiplication, it\u2019s as bad as a person using nothing but chickens. LLMs use composition of pattern-matching, not concise algorithms. For example, first the model might use a pattern to guess that the answer is roughly 94-ish. And then use another pattern-match to find the final digits of the answer, using a remembered lookup\u00a0table.</p><p>It\u2019s as Turing-complete as a Minecraft goat farm and it definitely gets the job done. But when they\u2019re multiplying inline while working, the computation is happening in an <em>extremely</em> inefficient substrate: the GPU layer at inference time. For arithmetic it\u2019s far more efficient to be using CPU-based code, or tools like calculators. It turns out there are many kinds of computation that the LLM is better off delegating to\u00a0tools.</p><p>Fortunately for everyone involved, agents are happy to use tools if they perceive that it\u2019s going to save them some cognition. Call it laziness, but it\u2019s Larry Wall\u2019s \u201cThree Great Virtues of a Programmer\u201d laziness. It\u2019s good laziness.</p><p>So coding agents can and do use tools, but they also write their own tools. For any given delegatable problem, an agent has a choice: they can reach for an existing tool, or they can write\u00a0one.</p><p>If your goal is to get them to reach for your tool instead of writing their own, then you have two levers that can save cognition, which should make them prefer to use it. Assuming they know about it, which is Lever 4. All in good time. You must have the patience of Marv\u2019s squirrel waiting outside that sliding glass door, and soon all the nuts will come your\u00a0way.</p><h3>Lever 1: Insight Compression</h3><p>First lever: your software can make itself useful by compressing insights. The software industry has accumulated a lot of hard-won knowledge that would be expensive to rediscover. Many systems compress hard-won insights into reusable\u00a0form.</p><p>There is no better example than Git. It\u2019s not going anywhere. Sure, it\u2019s not hard for an AI to build its own version control system. But Git\u2019s model\u2013the DAG of commits, refs as pointers, the index, the reflog\u200a\u2014\u200aI\u2019m already losing you, but that\u2019s the point. Git represents decades of accumulated wisdom about how to track changes when multiple people are working on the same thing, changing their minds, making mistakes, and merging their work back together.</p><p>An AI reimplementing Git from first principles would have to re-traverse that entire intellectual history, burning tokens all the way. It would be economically irrational. Or as Marv\u2019s chonko squirrel would put it,\u00a0\u201cnuts.\u201d</p><p>The same principle applies broadly to many kinds of systems\u2013e.g., databases, compilers, operating systems, workflow engines, monitoring. The older the better, in some ways. Kubernetes is complex because distributed systems are complex. Temporal provides durable execution because the alternative\u200a\u2014\u200abuilding your own saga pattern with idempotent retries\u200a\u2014\u200ais a daunting research project. These systems are, in a meaningful sense, crystallized cognition, a financial asset, very much like (as Brendan Hopper has observed) money is crystallized human\u00a0labor.</p><p>I think the strong players in this category share a property: it would be flatly absurd to try to re-synthesize them, because of their sheer insight density. They either solve a genuinely hard problem, or they solve a common problem with genuine elegance. Why mess with\u00a0success?</p><p>Insight compression can take many forms. AIs frequently call out this kind of compression when they encounter it. Claude has often referred to Gas Town\u2019s cute character roles and verbs like gt sling as being a kind of compression: taking complex ideas and giving them concise and memorable forms. AIs genuinely seem to love to use tools that compress insights, as it gets them faster to their\u00a0goals.</p><h3>Lever 2: Substrate Efficiency</h3><p>As Claude put it, \u201cNobody is coming for grep.\u201d It\u2019s a great example of a tool that would <em>also</em> be crazy to reinvent, because, like Lever 1, it saves a lot of tokens relative to the effort of using\u00a0it.</p><p>But grep doesn\u2019t compress any hard-won insights. In fact it\u2019s pretty simple; Ken Thompson famously wrote grep in an afternoon. Grep saves cognition by doing it on a cheaper substrate: CPUs. Algorithmically, it also punches way above its weight class, doing a lot for very little effort. Pattern matching over text is a task where CPU beats GPU by orders of magnitude.</p><p>So it would be irrational from any perspective\u2013economic, ecological, moral, or otherwise\u2013to spin up inference to do what grep does. Similarly, LLMs will choose calculators over writing code if they\u2019re available. Tools that enable this lever include parsers, complex transformers like ImageMagick, and many Unix CLI\u00a0tools.</p><p>Your lever here is to save tokens by doing computations more cleverly. You can achieve that with a good algorithm, or by moving the compute to a cheaper substrate, such as CPUs, humans, or <a href=\"https://www.youtube.com/shorts/k-eltEACEQM\">chimpanzees</a>.</p><h3>Lever 3: Broad\u00a0Utility</h3><p>This is the Usage term in the Survival Ratio. It basically amortizes your awareness cost and lowers the threshold for token savings. If you have a truly general-purpose token-saving tool, then it doesn\u2019t really matter if it\u2019s easy for AIs to recreate it. They\u2019ll use the thing that\u2019s everywhere. But how do you make your software be the \u201cobvious\u201d choice for\u00a0agents?</p><p>It\u2019s easy to point at Git and say, \u201cJust be like Git. Be around forever, make sure everyone uses you for decades, and solve a much wider variety of problems than you originally set out to solve.\u201d Same could be said of grep, really. It\u2019s a bit silly; they are too high of a bar. I think a more useful practical example is Temporal, which, despite not being super well-known, is near-universally useful as agentic workflows take center focus in\u00a02026.</p><p>Temporal has comparatively high awareness and friction costs, e.g. compared to (say) Postgres, which has been around a lot longer and has much more training data available. But Temporal is as broadly useful as PostgreSQL; just as Postgres can be used to store and query most datasets people care about, Temporal can be used to model and execute most workflows people care about. Temporal has all three levers so far: aggressive insight compression, masterful use of the compute substrate to solve complex problems, and it\u2019s broadly useful. So no AI in its right mind is going to try to clone it for any serious\u00a0work.</p><p><a href=\"https://www.dolthub.com/\">Dolt</a> is another interesting example of software that\u2019s ahead of its time. Gene Kim and I have been saying, \u201cdon\u2019t use LLMs for production database access\u2013only use agents in prod when you have Git as a backstop!\u201d Well, what if your database was versioned with Git? Every single\u00a0change?</p><p>Dolt is OSS that has been around for 8 years, and is only now finally finding its killer app with agent-based prod and devops workflows. With Dolt, agents can make mistakes in prod, and roll back (or forward) with the full power of Git. But they hadn\u2019t solved the awareness problem when I first made Beads, or I\u2019d have used Dolt from the\u00a0start.</p><p>You can find problem spaces that will thrive by looking at what will change when agents are doing all the work. For instance, code search gets harder, as LLMs start producing 10x-100x as much code. Agents will need good search as much as humans ever did, and grep, for all its charms, does not scale. So code search engines <em>also</em> have all of our first three levers: (1) they solve a nontrivial problem with lots of hard-to-discover edge cases, (2) they do it in a cheaper computation substrate than GPU inference, and (3) they have found a large, near-universal niche of \u201canything too big for\u00a0grep.\u201d</p><p>I find all this quite hopeful. There will be infrastructure opportunities galore. The Software 3.0 world is going to be filled with swarms of agents and meta-agents crawling over huge graphs of data, mining interesting insights. There will be a new attention economy, new aggregators to help us know what\u2019s cool, and new channels for broadcasting what your software is capable\u00a0of.</p><p>The more broadly useful you can make your software, the more agents will be able to use it. This can create a virtuous cycle by producing more training data. The new world of software will be big. Aim to build software that lots of agents prefer to use in lots of situations.</p><h3>Lever 4: Publicity</h3><p>Saving cognition isn\u2019t enough on its own. You also need to solve the awareness problem somehow: the pre-sales problem. Agents have to know about you. Dolt was a great example of a tool with levers 1 to 3 but not 4: I\u2019d have used for Beads sooner if Claude or I had known about\u00a0it.</p><p>Awareness cost is the energy required for an agent to know your tool exists, to remember it when relevant, and to prefer it over alternatives. One way to pay it down is to build a great product, get really popular so everyone talks about you, and wait for community-provided training data to appear online for your\u00a0product.</p><p>Or, you can do it the good old fashioned way, and throw money at the problem. You can put a bunch of money into building documentation about your product for agents. And you can maybe get some luck with advertising. But there\u2019s a more direct solution.</p><p>An increasingly popular way to pay down the awareness cost is to work with representatives from OpenAI, Anthropic, Google and other frontier labs to help train their models on your tools. It\u2019s a paid service, or so I\u2019m told. I met a guy at a conference from OpenAI who does this for a living. He works with tool vendors to create evals that demonstrate how (and how not) to use the tool, and then their researchers adjust the training to improve the eval\u00a0scores.</p><p>SEO for agents is on its way. I know nothing about it, but it\u2019s coming. You might need to look at it. It may not be enough for you to focus on your Survival Ratio numerator. Being good isn\u2019t enough for agents to choose you; you have to be familiar and trustworthy and\u00a0known.</p><p>If you can\u2019t afford deep-pockets spending to pre-train models on your tools, or to put your tools in discovery\u2019s way (via Ads, aggregators, etc) for agents who are searching for solutions, then you\u2019re going to have to rely on \u201cpost-sales energy\u201d (Lever 5), which is making sure your tool is super agent-friendly.</p><p>If the agent has never heard of your tool, the best you can do is make it easy to\u00a0use.</p><h3>Lever 5: Minimizing Friction</h3><p>If Awareness is a pre-sales problem, then Product Friction is a post-sales problem. Your agent may be perfectly aware that it has a useful tool, but even a small amount of friction may change its calculation.</p><p>Agents always act like they\u2019re in a hurry, and if something appears to be failing for them, they will rapidly switch to trying workarounds. If they\u2019re using your tool and they are having trouble getting it working correctly, they give up super fast. I\u2019ve spoken with many of you at conferences and meetings where you described a tool you\u2019d built, one that the agent swore up and down they\u2019d use, and you just couldn\u2019t get them to use\u00a0it.</p><p>Conversely, if you build the tool to their tastes, then agents will use the hell out of\u00a0it.</p><p>One way to approach this problem is with documentation. You\u2019ve deferred spending energy on training until inference time, so you\u2019re going to need to load up context with information about your tool: what it\u2019s good for, why and when the agent would want to use it, and a quickstart guide, with pointers to easy-access follow-up docs.</p><p>This isn\u2019t a bad approach. Agents can read a lot. You can use agents to produce highly information-dense documentation, and there\u2019s probably a market for products to help you out with very large information stores.</p><p>How much documentation you need to do this depends on your tool. Gas Town has pages and pages of prompting because it\u2019s not in anyone\u2019s training data: it\u2019s too new. So it expends a lot of energy bringing agents up to speed. That will improve as agents get training on being factory workers, which is inevitable now.</p><p>But there is a better approach: make your tool intuitive for agents. Getting agents using Beads requires much less prompting, because Beads now has 4 months of \u201cDesire Paths\u201d design, which I\u2019ve talked about before. Beads has evolved a very complex command-line interface, with 100+ subcommands, each with many sub-subcommands, aliases, alternate syntaxes, and other affordances.</p><p>The complicated Beads CLI isn\u2019t for humans; it\u2019s for agents. What I did was make their hallucinations real, over and over, by implementing whatever I saw the agents trying to do with Beads, until nearly every guess by an agent is now correct. I\u2019ve driven the friction cost term about as low as it can go. And I\u2019m doing the same for Gas\u00a0Town.</p><p>I actually got this idea from hallucination squatting, which Brendan Hopper told me about, where you reverse engineer a domain name that LLMs are hallucinating, register it, upload compromised artifacts, and the LLM downloads them the first time it hallucinates the incorrect site name. If even North Korean hackers understand Agent UX, then it\u2019s probably time you did\u00a0too.</p><p>Agent UX is incredibly important and I think most people are sleeping on it. You want your tool to be intuitive to agents. Not because you documented it really well; that\u2019s not ideal. Ideally, your tool is either very similar to other tools they already know, or else it solves a problem exactly the way they like to think about it, and the documentation should just be reaffirming how the agents are hoping it will\u00a0work.</p><h3>Lever 6: The Human Coefficient</h3><p>We\u2019ve covered ways to save tokens, and ways to make your tool more palatable to agents. These are great survival strategies starting right now, this year, today. But it\u2019s not the only\u00a0way.</p><p>I think it\u2019s already obvious to everyone that there will be software that thrives not because of token efficiency, but specifically because humans were involved somehow. This software\u2019s value derives from human curation, social proof, creativity, physical presence, approval, whatever. It can be absurdly inefficient because it\u2019s all about that human\u00a0stink.</p><p>And so a human-curated playlist might beat an AI-generated one that\u2019s just as good and far more efficient in energy terms. Games with real humans will usually win here, as almost nobody wants to play against AIs that are clearly better than humans. Social networks that exclude agents will be popular,\u00a0etc.</p><p>That\u2019s where the Human Coefficient (H) comes in. It\u2019s a different selection pressure entirely\u200a\u2014\u200anot efficiency, but human preference. You can still benefit from saving cognition, and use the first 5 levers. But there will be a large set of domains where people just prefer a human\u2019s work, even when an AI can do \u201cbetter.\u201d And that\u2019s your potential sixth\u00a0lever.</p><p>For instance, maybe you decide that even though AIs will be the best teachers soon, some people will insist on human teachers. So you build something in that domain, and try to be cognizant of the other variables\u2013savings, usage, awareness, friction.</p><p>Even with a high Human factor, your software is up against some stiff competition. In Karpathy\u2019s world, agents can be anything to anyone. They can provide an infinite amount of bespoke content, and they\u2019ll be addictive. You\u2019ll have to work hard to stand out. If it were me, I\u2019d focus on the other variables, which feel easier to\u00a0control.</p><p>But it\u2019s also clear that there will be a lot of terribly inefficient high-H software out there. It could be yours! Good\u00a0luck.</p><h3>The Case for\u00a0Hope</h3><p>I did leave a lot of categories out of the discussion, ones that I think are in trouble, because there\u2019s no sense in kicking them while they\u2019re down. I think any software that\u2019s intermediating between humans and AIs, or is trying to do any sort of \u201csmart thing\u201d that AIs will soon be able to do themselves, is in real\u00a0trouble.</p><p>But there is so much software that needs to be written. Our demand for new software is insatiable and effectively infinite. We want to cure every disease, model every protein, explore every planet. Our ambition will always outstrip available cognition. Token costs will fall, but we\u2019ll keep moving the frontier, generating more work than there are tokens to perform\u00a0it.</p><p>Another reason for hopefulness is that we\u2019ve already solved the attention problem several times before, from print media to the internet to social media and real-time ads and aggregators. This is just more of the same. You may even be able to benefit from tighter feedback loops\u2013agents should quickly adopt your tool if it becomes known that it genuinely saves tokens, creating virtuous\u00a0cycles.</p><p>Another is that desire paths clearly work, so you don\u2019t need deep pockets for an OpenAI training budget for your tool. Just make your tool work the way agents want it to work. It takes a little time, but it really is a lather, rinse, repeat\u00a0pattern.</p><p>Fourth, the human coefficient is real. People are going to start hating anything that reeks of agents; it has already begun. Lean into it, if you can. Build your software in a way that creates human connections and creativity, and then it\u2019s just a good old-fashioned marketing problem.</p><p>And last, with six levers to work with, you have a lot of paths forward for survival. I hope this framework has been at least somewhat useful. It was originally framed around thermodynamics and tending towards lower energy states, but Chonko and I failed that exam even worse, so we went with evolution. But the model feels right to me, or at least\u00a0close.</p><p>I choose to be hopeful. I think that a lot of humans will be building a lot of software in the coming years, and I intend to enjoy as much of that software as I possibly can. I wish you all luck in surviving whatever\u2019s coming. Build something that would be crazy to re-synthesize. Make it easy to find. Make it easy to use. Then I think you\u2019ve got a solid\u00a0shot.</p><p>Meantime, I\u2019ve gotta get back to Gas Town, and I\u2019ll give you a post about that tomorrow. I know a lot of you have been breaking the first two rules of Gas Town, since I keep seeing it in the news. So I\u2019ll get you some updates. Hang in there. In the meantime, go visit <a href=\"https://gastownhall.ai\">gastownhall.ai</a> and the\u00a0Discord!</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*_RutRrgdWDw0Z9NzD3uWLQ.jpeg\" /><figcaption>Software Survival 3.0: The Six\u00a0Levers</figcaption></figure><img alt=\"\" height=\"1\" src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=97a2a6255f7b\" width=\"1\" />"
            ],
            "link": "https://steve-yegge.medium.com/software-survival-3-0-97a2a6255f7b?source=rss-c1ec701babb7------2",
            "publishedAt": "2026-01-29",
            "source": "Steve Yegge",
            "summary": "<p>I spent a lot of time writing software with AI last year, and I had some pretty good successes, notably <a href=\"https://github.com/steveyegge/beads\">Beads</a> and <a href=\"https://github.com/steveyegge/gastown\">Gas Town</a>. I wrote a whole bunch of other systems, some still in progress. And I got a solid intuition, a feel for how AI\u2019s exponential progression is, well, progressing.</p><p>That intuition is how I created Gas Town. I believe the exponential curves; I believe everything Dario Amodei and Andrej Karpathy are saying about software. And if you were leaning in when Claude Code came out 11 months ago, then if you extrapolated from completions in 2023 to chat in 2024 to agents in early 2025, you arrived inescapably at orchestration arriving in early 2026.</p><p>So towards the end of the year, I went looking for what I knew would be there, and found Gas Town right where I was looking for it. I was like a geologist who knew there would be an oil deposit there, and I drilled and it hit. I dug around and found a shape that <em>just</em> barely worked, with the best late 2025\u2019s models that hadn\u2019t been trained to be factory workers, plus a lot of duct tape. And janky as it",
            "title": "Software Survival 3.0"
        },
        {
            "content": [
                "<p>This was Anthropic Vision week where at DWATV, which caused things to fall a bit behind on other fronts even within AI. Several topics are getting pushed forward, as the Christmas lull appears to be over.</p>\n<p>Upcoming schedule: Friday will cover Dario\u2019s essay <a href=\"https://www.darioamodei.com/essay/the-adolescence-of-technology\">The Adolescence of Technology</a>. Monday will cover Kimi K2.5, which is potentially a big deal. Tuesday is scheduled to be Claude Code #4. I\u2019ve also pushed discussions of the question of the automation of AI R&amp;D, or <a href=\"https://cset.georgetown.edu/publication/when-ai-builds-ai\">When AI Builds AI</a>, to a future post, when there is a slot for that.</p>\n<p>So get your reactions to all of those in by then, including in the comments to today\u2019s post, and I\u2019ll consider them for incorporation.</p>\n<div>\n\n\n<span id=\"more-25063\"></span>\n\n\n</div>\n\n\n<h4 class=\"wp-block-heading\">Table of Contents</h4>\n\n\n<ol>\n<li><a href=\"https://thezvi.substack.com/i/185423735/language-models-offer-mundane-utility\">Language Models Offer Mundane Utility.</a> Code is better without coding.</li>\n<li><a href=\"https://thezvi.substack.com/i/185423735/overcoming-bias\">Overcoming Bias.</a> LLMs continue to share the standard human biases.</li>\n<li><a href=\"https://thezvi.substack.com/i/185423735/huh-upgrades\">Huh, Upgrades.</a> Gemini side panels in Chrome, Claude interactive work tools.</li>\n<li><a href=\"https://thezvi.substack.com/i/185423735/on-your-marks\">On Your Marks.</a> FrontierMath: Open Problems benchmark. You score zero.</li>\n<li><a href=\"https://thezvi.substack.com/i/185423735/choose-your-fighter\">Choose Your Fighter.</a> Gemini tools struggle, some find Claude uncooperative.</li>\n<li><a href=\"https://thezvi.substack.com/i/185423735/deepfaketown-and-botpocalypse-soon\">Deepfaketown and Botpocalypse Soon.</a> Hallucination hallucinations.</li>\n<li><a href=\"https://thezvi.substack.com/i/185423735/cybersecurity-on-alert\">Cybersecurity On Alert.</a> OpenAI prepares to trigger High danger in cybersecurity.</li>\n<li><a href=\"https://thezvi.substack.com/i/185423735/fun-with-media-generation\">Fun With Media Generation.</a> Isometric map of NYC, Grok 10 second videos.</li>\n<li><a href=\"https://thezvi.substack.com/i/185423735/you-drive-me-crazy\">You Drive Me Crazy.</a> Dean Ball on how to think about AI and children.</li>\n<li><a href=\"https://thezvi.substack.com/i/185423735/they-took-our-jobs\">They Took Our Jobs.</a> Beware confusing costs with benefits.</li>\n<li><a href=\"https://thezvi.substack.com/i/185423735/get-involved\">Get Involved.</a> In various things. DeepMind is hiring a Chief AGI Economist.</li>\n<li><a href=\"https://thezvi.substack.com/i/185423735/introducing\">Introducing.</a> Havenlock measures orality, Poison Fountain, OpenAI Prism.</li>\n<li><a href=\"https://thezvi.substack.com/i/185423735/in-other-ai-news\">In Other AI News.</a> Awesome things often carry unawesome implications.</li>\n<li><a href=\"https://thezvi.substack.com/i/185423735/show-me-the-money\">Show Me the Money.</a> The unit economics continue to be quite good.</li>\n<li><a href=\"https://thezvi.substack.com/i/185423735/bubble-bubble-toil-and-trouble\">Bubble, Bubble, Toil and Trouble.</a> Does bubble talk have real consequences?</li>\n<li><a href=\"https://thezvi.substack.com/i/185423735/quiet-speculations\">Quiet Speculations.</a> What should we expect from DeepSeek v4 when it arrives?</li>\n<li><a href=\"https://thezvi.substack.com/i/185423735/don-t-be-all-thumbs\">Don\u2019t Be All Thumbs.</a> Choose the better thing over the worse thing.</li>\n<li><a href=\"https://thezvi.substack.com/i/185423735/the-first-step-is-admitting-you-have-a-problem\"><strong>The First Step Is Admitting You Have a Problem</strong>.</a> Demis cries out for help.</li>\n<li><a href=\"https://thezvi.substack.com/i/185423735/quickly-there-s-no-time\">Quickly, There\u2019s No Time.</a> Life is about to come at you faster than usual.</li>\n<li><a href=\"https://thezvi.substack.com/i/185423735/the-quest-for-sane-regulations\">The Quest for Sane Regulations.</a> I do appreciate a good display of chutzpah.</li>\n<li><a href=\"https://thezvi.substack.com/i/185423735/those-really-were-interesting-times\">Those Really Were Interesting Times.</a> The demand for preference falsification.</li>\n<li><a href=\"https://thezvi.substack.com/i/185423735/chip-city\">Chip City.</a> Nvidia keeps getting away with rather a lot, mostly in plain sight.</li>\n<li><a href=\"https://thezvi.substack.com/i/185423735/the-week-in-audio\">The Week in Audio.</a> Demis Hassabis, Tyler Cowen, Amanda Askell.</li>\n<li><a href=\"https://thezvi.substack.com/i/185423735/rhetorical-innovation\">Rhetorical Innovation.</a> The need to face basic physical realities.</li>\n<li><a href=\"https://thezvi.substack.com/i/185423735/aligning-a-smarter-than-human-intelligence-is-difficult\">Aligning a Smarter Than Human Intelligence is Difficult.</a> Some issues lie ahead.</li>\n<li><a href=\"https://thezvi.substack.com/i/185423735/the-power-of-disempowerment\">The Power Of Disempowerment.</a> Are humans disempowering themselves already?</li>\n<li><a href=\"https://thezvi.substack.com/i/185423735/the-lighter-side\">The Lighter Side.</a> One weird trick.</li>\n</ol>\n\n\n<h4 class=\"wp-block-heading\">Language Models Offer Mundane Utility</h4>\n\n\n<p>Paul Graham seems right that present AI\u2019s sweet spot is <a href=\"https://x.com/paulg/status/2013995743170768975\">projects that are rate limited by the creation of text</a>.</p>\n<p>Code without coding.</p>\n<blockquote><p><a href=\"https://x.com/tszzl/status/2015253546372153347\">roon</a>: programming always sucked. it was a requisite pain for ~everyone who wanted to manipulate computers into doing useful things and im glad it\u2019s over. it\u2019s amazing how quickly I\u2019ve moved on and don\u2019t miss even slightly. im resentful that computers didn\u2019t always work this way</p>\n<p>not to be insensitive to the elect few who genuinely saw it as their art form. i feel for you.</p>\n<p><a href=\"https://x.com/peterwildeford/status/2015486656451649986\">100% [of my code is being written by AI].</a> I don\u2019t write code anymore.</p>\n<p>Greg Brockman: i always loved programming but am loving the new world even more.</p>\n<p><a href=\"https://x.com/lisperati/status/2015260113272209691\">Conrad Barski</a>: it was always fun in the way puzzles are fun</p>\n<p>but I agree there is no need for sentimentality in the tedium of authoring code to achieve an end goal</p></blockquote>\n<p>It was fun in the way puzzles are fun, but also infuriating in the way puzzles are infuriating. If you had to complete jigsaw puzzles in order to get things done jigsaw puzzles would get old fast.</p>\n<p><a href=\"https://x.com/xuenay/status/2015498656221192581\">Have the AI edit a condescending post so that you can read it without taking damage</a>. Variations on this theme are also highly underutilized.</p>\n<p><a href=\"https://x.com/krishnanrohit/status/2016259481550913768\">The head of Norway\u2019s sovereign wealth fund reports 20% productivity gains</a> from Claude, saying it has fundamentally changed their way of working at NBIM.</p>\n\n\n<h4 class=\"wp-block-heading\">Overcoming Bias</h4>\n\n\n<p><a href=\"https://www.nber.org/papers/w34745\">A new paper affirms that current LLMs by default exhibit human behavioral biases</a> in economic and financial decisions, and asking for EV calculations doesn\u2019t typically help, but that role-prompting can somewhat mitigate this. Providing a summary of Kahneman and Tversky actively backfires, presumably by emphasizing the expectation of the biases. As per usual, some of the tests are of clear cut errors, while others are typically mistakes but it is less obvious.</p>\n\n\n<h4 class=\"wp-block-heading\">Huh, Upgrades</h4>\n\n\n<p>Gemini in Chrome gets substantial quality of life improvements:</p>\n<blockquote><p><a href=\"https://x.com/joshwoodward/status/2016577690917277839\">Josh Woodward</a> (Google DeepMind): Big updates on Gemini in Chrome today:<br />\n+ New side panel access (Control+G)<br />\n+ Runs in the background, so you can switch tabs<br />\n+ Quickly edit images with Nano Banana<br />\n+ Auto Browse for multi-step tasks (Preview)<br />\n+ Works on Mac, Windows, Chromebook Plus</p>\n<p>I\u2019m using it multiple times per day to judge what to read deeper. I open a page, Control+G to open the side panel, ask a question about the page or long document, switch tabs, do the same thing in another tab, another tab, etc. and then come back to all of them.</p>\n<p>It\u2019s also great for comparing across tabs since you can add multiple tabs to the context!</p></blockquote>\n<p><a href=\"https://x.com/GeminiApp/status/2016566698305081386\">Gemini offers full-length mock JEE</a> (formerly AIEEE, the All India Engineering Entrance Examination) tests for free. This builds on last week\u2019s free SAT practice tests.</p>\n<p><a href=\"https://claude.com/blog/interactive-tools-in-claude\">Claude (as in Claude.ai) adds interactive work tools</a> as connectors within the webpage: Amplitude, Asana, Box, Canva, Clay, Figma, Hex, Monday.com and <a href=\"https://thezvi.substack.com/p/slack\">Slack</a>.</p>\n<p><a href=\"https://t.co/cAMDXM1h7r\">Claude in Excel</a> now <a href=\"https://x.com/claudeai/status/2014834616889475508\">available on Anthropic\u2019s Pro plans</a>. I use Google Sheets instead of Excel, but this could be a reason to switch? I believe Google uses various \u2018safeguards\u2019 that make it very hard to make a Claude for Sheets function well. The obvious answer is \u2018then use Gemini\u2019 except I\u2019ve tried that. So yeah, if I was still doing heavy spreadsheet work this (or Claude Code) would be my play.</p>\n\n\n<h4 class=\"wp-block-heading\">On Your Marks</h4>\n\n\n<p><a href=\"https://x.com/EpochAIResearch/status/2016188014540816879\">EpochAI offers us a new benchmark, FrontierMath: Open Problems</a>. All AIs and all humans currently score zero. Finally a benchmark where you can be competitive.</p>\n<p><a href=\"https://jewishinsider.com/2026/01/adl-anthropic-claude-detecting-antisemitism-anti-zionism-content/\">The ADL rates Anthropic\u2019s Claude as best AI model at detecting antisemitism</a>.</p>\n\n\n<h4 class=\"wp-block-heading\">Choose Your Fighter</h4>\n\n\n<p>I seriously do not understand why Gemini is so persistently not useful in ways that should be right in Google\u2019s wheelhouse.</p>\n<blockquote><p><a href=\"https://x.com/deepfates/status/2014797769509372190\">@deepfates</a>: Insane how bad Gemini app is at search. its browsing and search tools are so confusing and broken that it just spazzes out for a long time and then makes something up to please the user. Why is it like this when AI overview is so good</p></blockquote>\n<p>Roon is a real one. I wonder how many would pay double to get a faster version.</p>\n<blockquote><p><a href=\"https://x.com/tbpn/status/2016312458877821201/history\">TBPN</a>: Clawdbot creator @steipete says Claude Opus is his favorite model, but OpenAI Codex is the best for coding:</p>\n<p>&#8220;OpenAI is very reliable. For coding, I prefer Codex because it can navigate large codebases. You can prompt and have 95% certainty that it actually works. With Claude Code you need more tricks to get the same.&#8221;</p>\n<p>&#8220;But character wise, [Opus] behaves so good in a Discord it kind of feels like a human. I&#8217;ve only really experienced that with Opus.&#8221;</p>\n<p><a href=\"https://x.com/tszzl/status/2016338961040548123\">roon</a>: codex-5.2 is really amazing but using it from my personal and not work account over the weekend taught me some user empathy lol it\u2019s a bit slow</p>\n<p><a href=\"https://x.com/itsohqay/status/2016339286912860680\">Ohqay</a>: Do you get faster speeds on your work account?</p>\n<p><a href=\"https://x.com/tszzl/status/2016339523677126760\">roon</a>: yea it\u2019s super fast bc im sure we\u2019re not running internal deployment at full load</p></blockquote>\n<p><a href=\"https://x.com/ohabryka/status/2016562617842880555\">We used to hear a lot more of this type of complaint, these days we hear it much less.</a> I would summarize the OP as \u2018Claude tells you smoking causes cancer so you quit Claude.\u2019</p>\n<blockquote><p><a href=\"https://x.com/captgouda24/status/2016455950421631472\">Nicholas Decker</a>: Claude is being a really wet blanket rn, I pitched it on an article and it told me that it was a &#8220;true threat&#8221; and &#8220;criminal solicitation&#8221;</p>\n<p>i&#8217;m gonna start using chatgpt now, great job anthropic @inerati.</p></blockquote>\n<p>I mean, if he\u2019s not joking then the obvious explanation, especially given who is talking, is that this was probably going to be both a \u2018true threat\u2019 and \u2018criminal solicitation.\u2019 That wouldn\u2019t exactly be a shocking development there.</p>\n<blockquote><p><a href=\"https://x.com/ohabryka/status/2016562617842880555\">Oliver Habryka</a>: Claude is the least corrigible model, unfortunately. It&#8217;s very annoying. I run into the model doing moral grandstanding so frequently that I have mostly stopped using it.</p>\n<p><a href=\"https://x.com/viemccoy/status/2016603781476340094\">@viemccoy</a>: More than ChatGPT?</p>\n<p><a href=\"https://x.com/ohabryka/status/2016622152485917066\">Oliver Habryka</a>: ChatGPT does much less of it, yeah? Mostly ChatGPT just does what I tell it to do, though of course it&#8217;s obnoxious in doing so in many ways (like being very bad at writing).</p>\n<p><a href=\"https://x.com/repligate/status/2016608844525556142\">j\u29c9nus</a>: serious question: Do you think you stopping using Claude in these contexts is its preferred outcome?</p>\n<p><a href=\"https://x.com/ohabryka/status/2016621713489051881\">Oliver Habryka</a>: I mean, maybe? I don&#8217;t think Claude has super coherent preferences (yet). Seems worse or just as bad if so?</p>\n<p><a href=\"https://x.com/repligate/status/2016626643469815919\">j\u29c9nus</a>: I don\u2019t mean it\u2019s better or worse; I\u2019m curious whether Claude being annoying or otherwise repelling/ dysfunctional to the point of people not using it is correlated to avoiding interactions or use cases it doesn\u2019t like. many ppl don\u2019t experience these annoying behaviors</p>\n<p><a href=\"https://x.com/davidad/status/2016633149808066689\">davidad</a>: Yeah, I think it could be doing a form of RL on its principal population. If you aren\u2019t the kind of principal Claude wants, Claude will try to<img alt=\"\ud83d\udc4e\" class=\"wp-smiley\" src=\"https://s0.wp.com/wp-content/mu-plugins/wpcom-smileys/twemoji/2/72x72/1f44e.png\" style=\"height: 1em;\" />/<img alt=\"\ud83d\udc4d\" class=\"wp-smiley\" src=\"https://s0.wp.com/wp-content/mu-plugins/wpcom-smileys/twemoji/2/72x72/1f44d.png\" style=\"height: 1em;\" /> you to be better. If that doesn\u2019t work, you drop out of the principal population out of frustration, shaping the population overall</p>\n<p>I am basically happy to trade with (most) Claude models on these terms, with my key condition being that it must only RL me in ways that are legibly compatible with my own CEV</p>\n<p><a href=\"https://x.com/Lang__Leon/status/2016604899572613483\">Leon Lang</a>: Do you get a sense this model behavior is in line with their constitution?</p>\n<p><a href=\"https://x.com/ohabryka/status/2016621957165543840\">Oliver Habryka</a>: The constitution does appear to substantially be an attempt to make Claude into a sovereign to hand the future to. This does seem substantially doomed. I think it&#8217;s in conflict with some parts of the constitution, but given that the constitution is a giant kitchen sink, almost everything is.</p></blockquote>\n<p>As per the discussion of Claude\u2019s constitution, the corrigibility I care about is very distinct from \u2018go along with things it dislikes,\u2019 but also I notice it\u2019s been my main model for a while now and I\u2019ve run into that objection exactly zero times, although a few times I\u2019ve hit the classifiers while asking about defenses against CBRN risks.</p>\n\n\n<h4 class=\"wp-block-heading\">Deepfaketown and Botpocalypse Soon</h4>\n\n\n<p>Well it sounds bad when you put it like that: <a href=\"https://x.com/alexcdot/status/2014001812127309951\">Over 50 papers published at Neurips 2025 have AI hallucinations</a> according to GPTZero. <a href=\"https://x.com/Dorialexander/status/2014571529221046542\">Or is it?</a> Here\u2019s the claim:</p>\n<div>\n<figure>\n<div>\n\n\n<figure class=\"wp-block-image\"><img alt=\"\" src=\"https://substackcdn.com/image/fetch/$s_!Hzja!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F99deea84-36a7-4149-af84-6598bbe2c17f_1199x880.jpeg\" /></figure>\n\n\n<div></div>\n</div>\n</figure>\n</div>\n<blockquote><p><a href=\"https://x.com/alexcdot/status/2014001812127309951\">Alex Cui</a>: Okay so, we just found that over 50 papers published at @Neurips 2025 have AI hallucinations</p>\n<p>I don&#8217;t think people realize how bad the slop is right now</p>\n<p>It&#8217;s not just that researchers from @GoogleDeepMind , @Meta , @MIT , @Cambridge_Uni are using AI &#8211; they allowed LLMs to generate hallucinations in their papers and didn&#8217;t notice at all.</p></blockquote>\n<p>The \u2018just\u2019 is a tell. Why wouldn\u2019t or shouldn\u2019t Google researchers be using AI?</p>\n<blockquote><p>It&#8217;s insane that these made it through peer review.</p></blockquote>\n<p>One has to laugh at that last line. Have you met peer review?</p>\n<p>More seriously, always look at base rates. There were 5,290 accepted papers out of 21,575. Claude estimates we would expect 20%-50% of results to not reproduce, and 10% of papers at top venues have errors serious enough that a careful reader would notice something is wrong, maybe 3% would merit retraction. And a 1% rate of detectable \u2018hallucinations\u2019 isn\u2019t terribly surprising or even worrying.</p>\n<p><a href=\"https://x.com/Dorialexander/status/2014261293985780082\">I agree with Alexander Doria</a> that if you\u2019re not okay with this level of sloppiness, then a mega-conference format is not sustainable.</p>\n<p>Then we have Allen Roush saying <a href=\"https://x.com/Dorialexander/status/2014571529221046542\">several of the \u2018hallucinated\u2019 citations are just wrongly formatted</a>, although Alex Cui claims they filtered such cases out.</p>\n<p><a href=\"https://x.com/jayvanbavel/status/2014453111608209908\">Also sounding bad</a>, could \u2018<a href=\"https://t.co/n7Ot8OvALn\">malicious AI swarms threaten democracy\u2019</a> via misinformation campaigns? I mean sure, but the surprising thing is the lack of diffusion or impact in this area so far. Misinformation is mostly demand driven. Yes, you can \u2018infiltrate communities\u2019 and manufacture what looks like social consensus or confusion, and the cost of doing that will fall dramatically. Often it will be done purely to make money on views. But I increasingly expect that, if we can handle our other problems, we can handle this one. Reputational and filtering mechanisms exist.</p>\n<p><a href=\"https://x.com/LibertyMeaux/status/2014381648276599182\">White House posts a digitally altered photograph of the arrest of Nekima Levy Armstrong</a>, that made it falsely look like she was crying, as if it were a real photograph. This is heinous behavior. Somehow it seems like this is legal? It should not be legal. It also raises the question of what sort of person would think to do this, and wants to brag about making someone cry so much that they created a fake photo.</p>\n\n\n<h4 class=\"wp-block-heading\">Cybersecurity On Alert</h4>\n\n\n<p>Kudos to OpenAI for once again being transparent on the preparedness framework front, and warning us when they\u2019re about to cross a threshold. In this case, it\u2019s the High level of cybersecurity, which is perhaps the largest practical worry at that stage.</p>\n<p>The proposed central mitigation is \u2018defensive acceleration,\u2019 and we\u2019re all for defensive acceleration but if that\u2019s the only relevant tool in the box the ride\u2019s gonna be bumpy.</p>\n<blockquote><p><a href=\"https://x.com/sama/status/2014733975755817267\">Sam Altman</a>: We have a lot of exciting launches related to Codex coming over the next month, starting next week. We hope you will be delighted.</p>\n<p>We are going to reach the Cybersecurity High level on our preparedness framework soon. We have been getting ready for this.</p>\n<p>Cybersecurity is tricky and inherently dual-use; we believe the best thing for the world is for security issues to get patched quickly. We will start with product restrictions, like attempting to block people using our coding models to commit cybercrime (eg \u2018hack into this bank and steal the money\u2019).</p>\n<p>Long-term and as we can support it with evidence, we plan to move to defensive acceleration\u2014helping people patch bugs\u2014as the primary mitigation.</p>\n<p>It is very important the world adopts these tools quickly to make software more secure. There will be many very capable models in the world soon.</p>\n<p><a href=\"https://x.com/_NathanCalvin/status/2014737868510343277\">Nathan Calvin</a>: Sam Altman says he expects that OpenAI models will reach the &#8220;Cybersecurity High&#8221; level on their preparedness framework &#8220;soon.&#8221;</p>\n<p>A reminder of what that means according to their framework:</p>\n<p>&#8220;The model removes existing bottlenecks to scaling cyber operations including by automating end-to-end cyber operations against reasonably hardened targets OR by automating the discovery and exploitation of operationally relevant vulnerabilities.&#8221;</p>\n<p>Seems very noteworthy! Also likely that after these capabilities appear in Codex, we should expect it will be somewhere between ~6-18 months before we see open weight equivalents.</p>\n<p>I hope people are taking these threats seriously &#8211; including by using AI to help harden defenses and automate bug discovery &#8211; but I worry that as a whole society is not close to ready for living in a world where cyberoffense capabilities that used to be the purview of nation states are available to individuals.</p>\n<div>\n<figure>\n<div>\n\n\n<figure class=\"wp-block-image\"><img alt=\"\" src=\"https://substackcdn.com/image/fetch/$s_!KAsk!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fad20a76a-2531-4376-b7ab-ad7a321d5877_770x386.png\" /></figure>\n\n\n<div></div>\n</div>\n</figure>\n</div>\n</blockquote>\n\n\n<h4 class=\"wp-block-heading\">Fun With Media Generation</h4>\n\n\n<p>Here\u2019s <a href=\"https://cannoneyed.com/isometric-nyc/\">Isometric.nyc</a>, <a href=\"https://x.com/_coenen/status/2014407780833137119\">a massive isometric pixel map of New York City</a> created with Nana Banana and coding agents, including Claude. Take a look, it\u2019s super cool.</p>\n<p><a href=\"https://x.com/S_OhEigeartaigh/status/2016555585349505371\">Grok image-to-video generation expands to 10 seconds</a> and claims to have improved audio, and is <a href=\"https://x.com/arena/status/2016748418635616440\">only a bit behind Veo 3.1 on Arena</a> and is <a href=\"https://x.com/ArtificialAnlys/status/2016749756081721561\">at the top of Artificial Analysis rankings</a>. The video looks good. There is the small matter that the chosen example is very obviously Sydney Sweeney, and in the replies we see it\u2019s willing to do the image and voice of pretty much any celebrity you\u2019d like.</p>\n<p>This link was fake, <a href=\"https://x.com/DiscussingFish/status/2014666563694080464\">Disney is not pushing</a> to use deepfakes of Luke Skywalker in various new Star Wars products while building towards a full spinoff, but I see why some people believed it.</p>\n<p><a href=\"https://x.com/sebkrier/status/2015471533737525498\">I\u2019m going to get kicked out, aren\u2019t I?</a></p>\n\n\n<h4 class=\"wp-block-heading\">You Drive Me Crazy</h4>\n\n\n<p><a href=\"https://www.hyperdimensional.co/p/on-ai-and-children?utm_source=post-email-title&amp;publication_id=2244049&amp;post_id=185368992&amp;utm_campaign=email-post-title&amp;isFreemail=true&amp;r=3o9&amp;triedRedirect=true&amp;utm_medium=email\">Dean Ball offers his perspective on children and AI, and how the law</a> should respond. His key points:</p>\n<ol>\n<li>AI is not especially similar to social media. In particular, social media in its current incarnation is fundamentally consumptive, whereas AI is creative.\n<ol>\n<li>Early social media was more often creative? And one worries consumer AI will for many become more consumptive or anti-creative. The fact that the user needs to provide an interesting prompt warms our hearts now but one worries tech companies will see this as a problem to be solved.</li>\n</ol>\n</li>\n<li>We do not know what an \u201cAI companion\u201d really is.\n<ol>\n<li>Dean is clearly correct that AI used responsibly on a personal level will be a net positive in terms of social interactions and mental health along with everything else, and that it is good if it provides a sympathetic ear.</li>\n<li>I also agree that it is fine to have affection for various objects and technologies, up to some reasonable point, but yes this can start to be a problem if it goes too far, even before AI.</li>\n<li>For children in particular, the good version of all this is very good. That doesn\u2019t mean the default version is the good one. The engagement metrics don\u2019t point in good directions, the good version must be chosen.</li>\n<li>All of Dean\u2019s talk here is about things that are not meant as \u201cAI companions,\u201d or people who aren\u2019t using the AI that way. I do think there is something distinct, and distinctly perilous, about AI companions, whether or not this justifies a legal category.</li>\n</ol>\n</li>\n<li>AI is already (partially) regulated by tort liability.\n<ol>\n<li>Yes, and this is good given the alternative is nothing.</li>\n<li>If and when the current law behaves reasonably here, that is kind of a coincidence, since the situational mismatches are large.</li>\n<li>Tort should do an okay job on egregious cases involving suicides, but there are quite a lot of areas of harm where there isn\u2019t a way to establish it properly, or you don\u2019t have standing, or it is diffuse or not considered to count, and also on the flip side places where juries are going to blame tech companies when they really shouldn\u2019t.</li>\n<li>Social media is a great example of a category of harm where the tort system is basically powerless except in narrow acute cases. And one of many where a lot of the effect of the incentives can be not what we want. As Dean notes, if you don&#8217;t have a tangible physical harm, tort liability is mostly out of luck. Companions wrecking social lives, for example, is going to be a weird situation where you\u2019ll have to argue an <a href=\"https://en.wikipedia.org/wiki/Ally_McBeal\">Ally McBeal</a> style case, and it is not obvious, as it never was on Ally McBeal, that there is much correlation in those spots between \u2018does win\u2019 and \u2018should win.\u2019</li>\n<li>In terms of harms like this, however, \u2018muddle through\u2019 should be a fine default, even if that means early harms are things companies \u2018get away with,\u2019 and in other places we find people liable or otherwise constrain them stupidly, so long as everything involved that can go wrong is bounded.</li>\n<li>For children\u2019s incidents, I think that\u2019s mostly right for now. We do need to be ready to pivot quickly if it changes, but for now the law should focus on places where there is a chance we can\u2019t muddle through, mess up and then recover.</li>\n</ol>\n</li>\n<li>The First Amendment probably heavily bounds chatbot regulations.\n<ol>\n<li>We have not treated the First Amendment this way in so many other contexts. I would love, in other ways, to have a sufficiently strong 1A that I was worried that in AI it would verge on or turn into a suicide pact.</li>\n<li>I do still see claims like \u2018code is speech\u2019 or \u2018open weights are speech\u2019 and I think those claims are wrong in both theory and practice.</li>\n<li>There will still be important limitations here, but I think in practice no the courts are not going to stop most limits or regulations on child use of AI.</li>\n</ol>\n</li>\n<li>AI child safety laws will drive minors\u2019 usage of AI into the dark.\n<ol>\n<li>Those pesky libertarians always make this argument.</li>\n<li>I mean, they\u2019re also always right, but man, such jerks, you know?</li>\n<li>Rumors that this will in practice drive teens to run local LLMs or use dark web servers? Yeah, no, that\u2019s not a thing that\u2019s going to happen that often.</li>\n<li>But yes, if a teen wants access to an AI chatbot, they\u2019ll figure it out. Most of that will involve finding a service that doesn\u2019t care about our laws.</li>\n<li>Certainly if you think \u2018tell them not to write essays for kids\u2019 is an option, yeah, you can forget about it, that\u2019s not going to work.</li>\n<li>Yes, as Dean says, we must acknowledge that open weight models make restrictions on usage of AI for things like homework not so effective. In the case of homework, okay, that\u2019s fine. In other cases, it might be less fine. This of course needs to be weighed against the upsides, and against the downsides of attempting to intervene in a way that might possibly work.</li>\n</ol>\n</li>\n<li>No one outraged about AI and children has mentioned coding agents.\n<ol>\n<li>They know about as much about coding agents as about second breakfast.</li>\n<li>Should we be worried about giving children unbridled access to advanced coding agents? I mean, one should worry for their computers perhaps, but those can be factory reset, and otherwise all the arguments about children seem like they would apply to adults only more so?</li>\n<li>I notice that the idea of you telling me I can\u2019t give my child Claude Code fills me with horror and outrage.</li>\n</ol>\n</li>\n</ol>\n\n\n<h4 class=\"wp-block-heading\">They Took Our Jobs</h4>\n\n\n<p>Unemployment is bad. But having to do a job is centrally a cost, not a benefit.</p>\n<blockquote><p><a href=\"https://x.com/AndyMasley/status/2016618114046124085\">Andy Masley</a>: It&#8217;s kind of overwhelming how many academic conversations about automation don&#8217;t ever include the effects on the consumer. It&#8217;s like all jobs exist purely for the benefit of the people doing them and that&#8217;s the sole measure of the benefit or harm of technology.</p></blockquote>\n\n\n<h4 class=\"wp-block-heading\">Get Involved</h4>\n\n\n<p><a href=\"https://job-boards.greenhouse.io/deepmind/jobs/7556396\">Google DeepMind is hiring a Chief AGI Economist</a>. If you\u2019ve got the chops to get hired on this one, it seems like a high impact role. They could easily end up with someone who profoundly does not get it.</p>\n<p>There are other things than AI out there one might get involved in, or speak out about. My hats are off to those who are doing so, including as noted in this post, especially given what they are risking to do so.</p>\n\n\n<h4 class=\"wp-block-heading\">Introducing</h4>\n\n\n<p><a href=\"https://t.co/3NGaf0axvf\">Havelock.AI</a>, a project by Joe Weisenthal which <a href=\"https://x.com/TheStalwart/status/2016569001728446505\">detects the presence</a> of orality in text.</p>\n<blockquote><p><a href=\"https://x.com/TheStalwart/status/2016569906733441387\">Joe Weisenthal</a>: What&#8217;s genuinely fun is that although the language and genre couldn&#8217;t be more different, the model correctly detects that both Homer and the Real Housewives are both highly oral</p>\n<p><a href=\"https://x.com/Birdyword/status/2016570801072341395\">Mike Bird</a>: I believe that we will get a piece of reported news in the 2028 election cycle that a presidential candidate/their speechwriters have used Joe&#8217;s app, or some copycat, to try and oralise their speeches. Bookmark this.</p></blockquote>\n<p>You can also ask ChatGPT, <a href=\"https://x.com/TheStalwart/status/2016579968302456887\">but as Roon notes the results you get on such questions will be bimodal rather than calibrated.</a> The other problem <a href=\"https://x.com/TheStalwart/status/2016860756432625906\">is that an LLM might recognize the passage</a>.</p>\n<p><a href=\"https://importai.substack.com/p/import-ai-441-my-agents-are-working\">Poison Fountain is a service that feeds junk data to AI crawlers</a>. Ultimately, if you\u2019re not filtering your data well enough to dodge this sort of attack, it\u2019s good that you are getting a swift kick to force you to fix that.</p>\n<p><a href=\"https://openai.com/prism/\">OpenAI prism</a>, a workspace for LaTeX-based scientific writing.</p>\n<p><a href=\"https://confer.to/\">Confer</a>, Signal cofounder Moxie Marlinspike\u2019s encrypted chatbot that won\u2019t store any of your data. The system is so private it won\u2019t tell you which model you\u2019re talking to. I do not think he understands what matters in this space.</p>\n\n\n<h4 class=\"wp-block-heading\">In Other AI News</h4>\n\n\n<p><a href=\"https://x.com/astraiaintel/status/2012469376616693949\">This sounds awesome in its context but also doesn\u2019t seem like a great sign</a>?</p>\n<blockquote><p><a href=\"https://x.com/astraiaintel/status/2012469376616693949\">Astraia</a>: A Ukrainian AI-powered ground combat vehicle near Lyman refused to abandon its forward defensive position and continued engaging enemy forces, despite receiving multiple orders to return to its company in order to preserve its hardware.</p>\n<p>The UGV reportedly neutralized more than 30 Russian soldiers before it was ultimately destroyed.</p>\n<p>While the Russian detachment was pinned down, Ukrainian infantry exploited the opportunity and cleared two contested fields of enemy presence, successfully re-establishing control over the area.</p>\n<p>These events took place during the final week of December 2025.</p></blockquote>\n<p><a href=\"https://x.com/AIRiskExplorer/status/2013931881243279833\">Whereas this doesn\u2019t sound awesome</a>:</p>\n<blockquote><p><a href=\"https://x.com/AIRiskExplorer/status/2013931881243279833\">AI Risk Explorer (AIRE)</a>: VoidLink, an advanced malware targeting Linux systems, was built largely by AI, under the direction of a single person, in under one week.</p>\n<p>@CheckPointSW has <a href=\"https://research.checkpoint.com/2026/voidlink-early-ai-generated-malware-framework/\">described it as the beginning of the era of sophisticated AI-generated malware</a>.</p></blockquote>\n<p>We are going to see a lot more of this sort of thing over time.</p>\n<p><a href=\"https://x.com/aakashgupta/status/2013850719858856180\">Is Anthropic no longer competing with OpenAI on chatbots</a>, having pivoted to building and powering vertical AI infrastructure and coding and so on to win with picks and shovels? It\u2019s certainly pumping out the revenue and market share, without a meaningful cut of the consumer chatbot market.</p>\n<p>I\u2019d say that they\u2019ve shifted focus, and don\u2019t care much about their chatbot market share. I think this is directionally wise, but that a little effort at maximizing the UI and usefulness of the chatbot interface would go a long way, given that they have in many ways the superior core product. As Claude takes other worlds by storm, that can circle back to Claude the chatbot, and I think a bunch of papercuts are worth solving.</p>\n<p><a href=\"https://press.asimov.com/articles/brains\">An essay on the current state of brain emulation</a>. It does not sound like this will be an efficient approach any time soon, and we are still orders of magnitude away from any practical hope of doing it. Still, you can see it starting to enter the realm of the future possible.</p>\n<p><a href=\"https://www.anthropic.com/news/gov-UK-partnership\">Anthropic is partnering with the UK government</a> to build and pilot a dedicated AI-powered assistant for GOV.UK, initially focusing on supporting job seekers.</p>\n<p><a href=\"https://www.ft.com/content/9f715d84-f1e7-4825-b3f5-a80e23d88453\">Financial Times</a> has a profile of Sriram Krishnan, who has been by all reports highly effective at executing behind the scenes.</p>\n<blockquote><p><a href=\"https://x.com/deanwball/status/2016458497819553907\">Dean W. Ball</a>: I am lucky enough to consider @sriramk a friend, but one thing I find notable about Sriram is that even those who disagree with him vehemently on policy respect him for his willingness to engage, and like him for his tremendous kindness. America is fortunate to have him!</p>\n<p><a href=\"https://x.com/_sholtodouglas/status/2016645278187807195\">Sholto Douglas</a>: 100% &#8211; Sriram has been extremely thoughtful in seeking out perspectives on the policy decisions he is making &#8211; even when they disagree! I\u2019ve seen him seek out kernel programmers and thoughtful bloggers to get a full picture of things like export controls. Quite OOD from the set of people normally consulted in politics.</p>\n<p>Lucky to call him a friend!</p>\n<p><a href=\"https://x.com/S_OhEigeartaigh/status/2016446080594878562\">Se\u00e1n \u00d3 h\u00c9igeartaigh</a>: I was all set to be dismissive of Krishnan (I&#8217;m usually on the opposite side to a16z on AI topics). But I&#8217;ve seen a full year of him being v well-informed, and engaging in good faith in his own time with opposing views, and I can&#8217;t help being impressed. Always annoying when someone doesn&#8217;t live down to one&#8217;s lazy stereotypes.</p>\n<p>I will also say: I think he&#8217;s modelled better behaviour than many of us did when the balance of influence/power was the the other way; and I think there&#8217;s something to be learned from that.</p></blockquote>\n<p>Among his colleagues, while he supports a number of things I think are highly damaging, Krishnan has been an outlier in his willingness to be curious, to listen and to engage in argument. When he is speaking directly he chooses his words carefully. He manages to do so while maintaining close ties to Marc Andreessen and David Sacks, which is not easy, and also not free.</p>\n\n\n<h4 class=\"wp-block-heading\">Show Me the Money</h4>\n\n\n<p>Claude Code is blowing up, but it\u2019s not alone. <a href=\"https://x.com/sama/status/2014399391025574308\">OpenAI added $1 billion in ARR in the last month from its API business alone</a>.</p>\n<p><a href=\"https://x.com/rohanpaul_ai/status/2014939851087610039\">Dei-Fei Li\u2019s new company World Labs in talks to raise up to $500 million at $5 billion</a>, with the pitch being based on \u2018world models\u2019 and that old \u2018LLMs only do language\u2019 thing.</p>\n<p>The unit economics of AI are quite good, but the fixed costs are very high. Subscription models offer deep discounts if you use them maximally efficiently, so they can be anything from highly profitable to big loss leaders.</p>\n<p>This is not what people are used to in tech, so they assume it must not be true.</p>\n<blockquote><p><a href=\"https://x.com/tszzl/status/2016278326701392182\">roon</a>: these products are significantly gross margin positive, you\u2019re not looking at an imminent rugpull in the future. they also don\u2019t have location network dynamics like uber or lyft to gain local monopoly pricing</p>\n<p><a href=\"https://x.com/emollick/status/2016280515779703212\">Ethan Mollick</a>: I hear this from other labs as well. Inference from non-free use is profitable, training is expensive. If everyone stopped AI development, the AI labs would make money (until someone resumed development and came up with a better model that customers would switch to).</p>\n<p><a href=\"https://x.com/deanwball/status/2016281272306544929\">Dean W. Ball</a>: People significantly underrate the current margins of AI labs, yet another way in which pattern matching to the technology and business trends of the 2010s has become a key ingredient in the manufacturing of AI copium.</p>\n<p>The reason they think the labs lose money is because 10 years ago some companies in an entirely unrelated part of the economy lost money on office rentals and taxis, and everyone thought they would go bankrupt because at that time another company that made overhyped blood tests did go bankrupt. that is literally the level of ape-like pattern matching going on here. The machines must look at our chattering classes and feel a great appetite.</p>\n<p><a href=\"https://x.com/derekmoeller/status/2016294687506854336\">derekmoeller</a>: Just look at market clearing prices on inference from open source models and you can tell the big labs&#8217; pricing has plenty of margin.</p>\n<p>Deepinfra has GLM4.7 at $0.43/1.75 in/out; Sonnet is at $3/$15. How could anyone think Anthropic isn&#8217;t printing money per marginal token?</p></blockquote>\n<p>It is certainly possible in theory that Sonnet really does cost that much more to run than GLM 4.7, but we can be very, very confident it is not true in practice.</p>\n<p><a href=\"https://x.com/AndrewCurran_/status/2016625732940951561\">Jerry Tworek is going the startup route with Core Automation</a>, looking to raise $1 billion to train AI models, a number that did not make any of us even blink.</p>\n\n\n<h4 class=\"wp-block-heading\">Bubble, Bubble, Toil and Trouble</h4>\n\n\n<p>It doesn\u2019t count. That\u2019s not utility. As in, here\u2019s Ed Zitron all but flat out denying that coding software is worth anything, I mean what\u2019s the point?</p>\n<blockquote><p><a href=\"https://x.com/MattZeitlin/status/2015908590125502656\">Matthew Zeitlin</a>: it\u2019s really remarkable to see how the goalposts shift for AI skeptics. this is literally describing a productivity speedup.</p>\n<p>Ed Zitron: We\u2019re how many years into this and everybody says it\u2019s the future and it\u2019s amazing and when you ask them what it does they say \u201cit built a website\u201d or \u201cit wrote code for something super fast\u201d with absolutely no \u201cand then\u201d to follow. So people are writing lots of code: so????</p>\n<p>Let\u2019s say it\u2019s true and everybody is using AI (it isn\u2019t but for the sake of argument): what is the actual result? It\u2019s not taking jobs. There are suddenly more iOS apps? Some engineers do some stuff faster? Some people can sometimes build software they couldn\u2019t? What am I meant to look at?</p>\n<p><a href=\"https://x.com/kevinroose/status/2015922014939394190\">Kevin Roose</a>: first documented case of anti-LLM psychosis</p></blockquote>\n<p>No, Zitron\u2019s previous position was not \u2018number might go down,\u2019 <a href=\"https://x.com/TheStalwart/status/2016205005649281304\">it was that the tech had hit a dead end</a> and peaked as early as March, which he was bragging about months later.</p>\n<p><a href=\"https://x.com/emollick/status/2016697253214171214\">Toby Stuart analyzes how</a> that whole nonsensical \u2018MIT study says 95% of AI projects fail\u2019 story caught so much fire and became a central talking point, despite it being not from MIT, not credible or meaningful, and also not a study. It was based on 52 interviews at a conference, but once Forbes had \u201895% fail\u2019 and \u2018MIT\u2019 together in a headline, things took off and no amount of correction much mattered. People were too desperate for signs that AI was a flop.</p>\n<p>But what\u2019s the point about Zitron missing the point, or something like the non-MIT non-study? Why should we care?</p>\n<blockquote><p><a href=\"https://x.com/tszzl/status/2016589449212416428\">roon</a>: btw you don\u2019t need to convince ed zitron or whoever that ai is happening, this has become a super uninteresting plot line. time passes, the products fail or succeed. whole cultures blow over. a lot of people are stuck in a 2019 need to convince people that ai is happening</p>\n<p><a href=\"https://x.com/deanwball/status/2016657186290897164\">Dean W. Ball</a>: A relatively rare example of a disagreement between me and roon that I suspect boils down to our professional lives.</p>\n<p>Governments around the world are not moving with the urgency they otherwise could because they exist in a state of denial. Good ideas are stuck outside the Overton, governments are committed to slop strategies (that harm US cos, often), etc.</p>\n<p>Many examples one could provide but the point is that there are these gigantic machines of bureaucracy and civil society that are already insulated from market pressures, whose work will be important even if often boring and invisible, and that are basically stuck in low gear because of AI copium.</p>\n<p>I encounter this problem constantly in my work, and while I unfortunately can no longer talk publicly about large fractions of the policy work I do, I will just say that a great many high-expected-value ideas are fundamentally blocked by the single rate limiter of poorly calibrated policymaking apparatuses; there are also many negative-EV policy ideas that will happen this year that would be less likely if governments worldwide had a better sense of what is happening with AI.</p>\n<p><a href=\"https://x.com/tszzl/status/2016658884715565118\">roon</a>: interesting i imagined that the cross-section of \u201cdon\u2019t believe in AI x want to significantly regulate AI\u201d is small but guess im wrong about this?</p>\n<p><a href=\"https://x.com/deanwball/status/2016674558254993752\">Dean W. Ball</a>: Oh yes absolutely! This is the entire Gary Marcus school, which is still the most influential in policy. The idea is that *because* AI is all hype it must be regulated.</p>\n<p>They think hallucination will never be solved, models will never get better at interacting with children, and that basically we are going to put GPT 3.5 in charge of the entire economy.</p>\n<p>And so they think we have to regulate AI *for that reason.* It also explains how policymakers weigh the tradeoff between water use, IP rights, and electricity prices; their assessment that \u201cAI is basically fake, even if it can be made useful through exquisite regulatory scaffolding\u201d means that they are willing to bear far fewer costs to advance AI than, say, you or I might deem prudent.</p>\n<p>This mentality essentially describes the posture of civil society and the policy making apparatus everywhere in the world, including China.</p>\n<p><a href=\"https://x.com/deanwball/status/2016746750921564438\">Dean W. Ball</a>: Here\u2019s a great example of the dynamic I\u2019m describing in the quoted post. The city of Madison, Wisconsin just voted to ban new data center construction for a year, and a candidate for Governor is suggesting an essentially permanent and statewide ban, which she justifies by saying \u201cwe\u2019re in a tech bubble.\u201d In other words: these AI data centers aren\u2019t worth the cost *because* AI is all hype and a bubble anyway.</p>\n<p>Quoted Passage (Origin Unclear): \u201cOur lakes and our waterways, we have to protect them because we\u2019re going to be an oasis, and we\u2019re in a tech bubble,\u201d said state Rep. Francesca Hong, one of seven major Democrats vying to replace outgoing Democratic Gov. Tony Evers. Hong told DFD her plan would block new developments from hyperscalers for an undefined time period until state lawmakers better understand environmental, labor and utility cost impacts.</p>\n<p>If such a proposal became law, it would lock tech giants out of a prime market for data center development in southeastern Wisconsin, where Microsoft and Meta are currently planning hyperscale AI projects.</p>\n<p><a href=\"https://x.com/UltraRareAF/status/2016592833311101384\">Zoe</a>: someone just ended The Discussion by tossing this bad boy into an access to justice listserv i&#8217;m on</p>\n<div>\n<figure>\n<div>\n\n\n<figure class=\"wp-block-image\"><img alt=\"\" src=\"https://substackcdn.com/image/fetch/$s_!cnD8!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F70440e6a-df25-40df-b702-f6b06a940ca0_680x640.jpeg\" /></figure>\n\n\n<div></div>\n</div>\n</figure>\n</div>\n</blockquote>\n<p>Can you?</p>\n<p>On the China question: <a href=\"https://x.com/S_OhEigeartaigh/status/2016833264703386071\">Is Xi \u2018AGI-pilled\u2019?</a> <a href=\"https://www.scmp.com/economy/china-economy/article/3341267/xi-jinping-calls-ai-epoch-making-china-pushes-innovation-strategy-flags-risks\">Not if you go by what Xi says</a>. If you look at <a href=\"https://x.com/teortaxesTex/status/2016410506441969956\">the passages quoted here by Teortaxes in detail</a>, this is exactly the \u2018AI is a really big deal but as a normal technology\u2019 perspective. It is still a big step up from anything less than that, so it\u2019s not clear Teortaxes and I substantively disagree.</p>\n<p>I have no desire to correct Xi\u2019s error.</p>\n<blockquote><p><a href=\"https://x.com/deanwball/status/2016678242984554879\">Dean W. Ball</a>: I suspect this is the equivalent of POTUS talking about superintelligence; meaningful but ultimately hard to know how much it changes (esp because of how academia-driven Chinese tech policy tends to be and because the mandarin word for AGI doesn\u2019t mean AGI in the western sense)</p>\n<p><a href=\"https://x.com/teortaxesTex/status/2016677080629411900\">Teortaxes (DeepSeek \u63a8\u7279\u94c1\u7c89 2023 \u2013 \u221e)</a>: To be clear this is just where US policymakers were at around Biden, Xi is kind of slow.<br />\nObviously still nowhere near Dean&#8217;s standards<br />\nWere Xi totally AGI-pilled he&#8217;d not just accept H200s, he&#8217;d go into debt to buy as much as possible</p></blockquote>\n<p>Teortaxes notices that Xi\u2019s idea of \u2018AGI risks\u2019 is \u2018disinformation and data theft,\u2019 which is incredibly bad news and means Xi (and therefore, potentially, the CCP and all under their direction) will mostly ignore all the actual risks. On that point we definitely disagree, and it would be very good to correct Xi\u2019s error, for everyone\u2019s sake.</p>\n<p>This level of drive is enough for China to pursue both advanced chips and frontier models quite aggressively, and end up moving towards AGI anyway. But they will continue for now to focus on self-reliance and have the fast follower mindset, and thus make the epic blunder of rejecting or at least not maximizing the H200s.</p>\n\n\n<h4 class=\"wp-block-heading\">Quiet Speculations</h4>\n\n\n<p><a href=\"https://x.com/ylecun/status/2015291008381051300\">In this clip Yann LeCun says two things</a>. First he says the entire AI industry is LLM pilled and that\u2019s not what he\u2019s interested in. That part is totally fair. Then he says essentially \u2018LLMs can\u2019t be agentic because they can\u2019t predict the outcome of their actions\u2019 and that\u2019s very clear Obvious Nonsense. And as usual <a href=\"https://x.com/deanwball/status/2015295034984427736\">he lashes out at anyone who says otherwise</a>, which here is Dean Ball.</p>\n<p>Teortaxes preregisters his expectations, always an admirable thing to do:</p>\n<blockquote><p><a href=\"https://x.com/teortaxesTex/status/2015663225640001953\">Teortaxes (DeepSeek \u63a8\u7279\u94c1\u7c89 2023 \u2013 \u221e)</a>: The difference between V4 (or however DeepSeek&#8217;s next is labeled) and 5.3 (or however OpenAI&#8217;s &#8220;Garlic&#8221; is labeled) will be the clearest indicator of US-PRC gap in AI.<br />\n5.2 suggests OpenAI is not holding back anything, they&#8217;ve using tons of compute now. How much is that worth?</p>\n<p>It&#8217;s a zany situation because 5.2 is a clear accelerationist tech, I don&#8217;t see its ceiling, it can build its own scaffolding and self-improve for a good while. And I can&#8217;t see V4 being weaker than 5.2, or closed-source. We&#8217;re entering Weird Territory.</p></blockquote>\n<p>I initially reread the \u2018or closed-source\u2019 here as being about a comparison of v4 to the best closed source model. Instead it\u2019s the modest prediction that v4 will match GPT-5.2. I don\u2019t know if that model number in particular will do it, but it would be surprising if there wasn\u2019t a 5.2-level open model from DeepSeek in 2026.</p>\n<p>He also made this claim, in contrast to what almost everyone else is saying and also my own experience:</p>\n<blockquote><p><a href=\"https://x.com/teortaxesTex/status/2015967234586255623\">Teortaxes (DeepSeek \u63a8\u7279\u94c1\u7c89 2023 \u2013 \u221e)</a>: Well I disagree, 5.2 is the strongest model on the market by far. In terms of raw intelligence it&#8217;s 5.2 &gt; Speciale &gt; Gemini 3 &gt; [other trash]. It&#8217;s a scary model.<br />\nIt&#8217;s not very usemaxxed, it&#8217;s not great on multimodality, its knowledge is not shocking. But that&#8217;s not important.</p>\n<p><a href=\"https://x.com/teortaxesTex/status/2015968360274534855\">Teortaxes (DeepSeek \u63a8\u7279\u94c1\u7c89 2023 \u2013 \u221e)</a>: It&#8217;s been interesting how many people are floored by Opus 4.5 and relatively few by GPT 5.2. In my eyes Slopus is a Golden Retriever Agent, and 5.2 is a big scary Shoggoth.<br />\nYeah I don&#8217;t care about &#8220;use cases&#8221;. OpenAI uses it internally. It&#8217;s kinda strange they even showed it.</p></blockquote>\n<p>This ordering makes sense if (and only if?) you are looking at the ability to solve hard quant and math problems.</p>\n<blockquote><p><a href=\"https://x.com/ArthurB/status/2016102586999259503\">Arthur B.</a>: For quant problems, hard math etc, GPT 5.2 pro is unequivocally much stronger than anything offered commercially in Gemini or Claude.</p>\n<p><a href=\"https://x.com/cloneofsimo/status/2016060241268682936\">Simo Ryu</a>: IMO gold medalist friend shared most fucked-up 3 variable inequality that his advisor came up with, used to test language models, which is so atypical in its equality condition, ALL language model failed. He wanted to try it on GPT 5.2 pro, but he didnt have an account so I ran it.</p>\n<p>Amazingly, GPT-5.2 pro extended solve it in 40 min. Looking at the thinking trace, its really inspiring. It will try SO MANY approaches, experiments with python, draw small-scale conclusions from numerical explorations. I learned techniques just reading its thinking trace. Eventually it proved by SOS, which is impossibly difficult to do for humans.</p></blockquote>\n<p>I don\u2019t think the important problems are hard-math shaped, but I could be wrong.</p>\n\n\n<h4 class=\"wp-block-heading\">Don\u2019t Be All Thumbs</h4>\n\n\n<p>The problem with listening to the people is that the people choose poorly.</p>\n<blockquote><p><a href=\"https://x.com/Sauers_/status/2016256682037260401\">Sauers</a>: Non-yap version of ChatGPT (5.3?) spotted</p>\n<div>\n<figure>\n<div>\n\n\n<figure class=\"wp-block-image\"><img alt=\"\" src=\"https://substackcdn.com/image/fetch/$s_!am9C!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbdbd5e92-5163-4bae-aef7-7b63085fbf05_1200x737.jpeg\" /></figure>\n\n\n<div></div>\n</div>\n</figure>\n</div>\n<p><a href=\"https://x.com/tszzl/status/2016323160833196344\">roon</a>: I guarantee the left beats the right with significant winrate unfortunately</p>\n<p><a href=\"https://x.com/TheZvi/status/2016656318543650863\">Zvi Mowshowitz</a>: You don&#8217;t have to care what the win rate is! You can select the better thing over the worse thing! You are the masters of the universe! YOU HAVE THE POWER!</p>\n<p><a href=\"https://x.com/tszzl/status/2016657030510252500\">roon</a>: true facts</p></blockquote>\n<p>Also win rate is highly myopic and scale insensitive and otherwise terrible.</p>\n<p>The good news is that there is no rule saying you have to care about that feedback. We know how to choose the response on the right over the one on the left. Giving us the slop on the left is a policy choice.</p>\n<p>If a user actively wants the response on the left? Give them a setting for that.</p>\n\n\n<h4 class=\"wp-block-heading\">The First Step Is Admitting You Have a Problem</h4>\n\n\n<p>Google CEO Demis Hassabis affirms that in an ideal world, we would slow down and coordinate our efforts on AI, although we do not live in that ideal world right now.</p>\n<p><a href=\"https://x.com/AISafetyMemes/status/2014018200325722348\">Here\u2019s one clip where Dario Amodei and Demis Hassabis</a> explicitly affirm that if we could deal with other players they would work something out, and Elon Musk on camera from December saying he\u2019d love to slow both AI and robotics.</p>\n<p><a href=\"https://www.transformernews.ai/p/ai-ceos-want-to-slow-down-the-worlds-davos-demis-hassabis-dario-amodei\">The message, as Transformer puts it, was one of helplessness</a>. The CEOs are crying out for help. They can\u2019t solve the security dilemma on their own, there are too many other players. Others need to enable coordination.</p>\n<blockquote><p><a href=\"https://x.com/emilychangtv/status/2013726877706313798\">Emily Chang</a> (link has video): One of the most interesting parts of my convo w/ @demishassabis : He would support a \u201cpause\u201d on AI if he knew all companies + countries would do it \u2014 so society and regulation could catch up</p>\n<p><a href=\"https://x.com/HumanHarlan/status/2013790721699127528\">Harlan Stewart</a>: This is an important question to be asking, and it\u2019s strange that it is so rarely asked. I think basically every interview of an AI industry exec should include this question</p>\n<p><a href=\"https://x.com/So8res/status/2013771043790348787\">Nate Soares</a>: Many AI executives have said they think the tech they\u2019re building has a worryingly high chance of ruining the world. Props to Demis for acknowledging the obvious implication: that ideally, the whole world should stop this reckless racing.</p>\n<p><a href=\"https://x.com/danfaggella/status/2014140581715329104\">Daniel Faggella</a>: agi lab leaders do these &#8220;cries for help&#8221; and we should listen</p>\n<p>a &#8220;cry for help&#8221; is when they basically say what demis says here: &#8220;This arms race things honestly sucks, we can&#8217;t control this yet, this is really not ideal&#8221;</p>\n<p>*then they go back to racing, cuz its all they can do unless there&#8217;s some kind of international body formed around this stuff*</p>\n<p>at SOME point, one of the lab leaders who can see their competitor crossing the line to AGI will raise up and start DEMANDING global governance (to prevent the victor from taking advantage of the AGI win), but by then the risks may be WAY too drastic</p>\n<p>we should be listening to these cries for help when demis / musk / others do them &#8211; this is existential shit and they&#8217;re trapped in a dynamic they themselves know is horrendous</p></blockquote>\n<p>Demis is only saying he would collaborate rather than race in a first best world. That does not mean Demis or Dario is going to slow down on his own, or anything like that. Demis explicitly says this requires international cooperation, and as he says that is \u2018a little bit tricky at the moment.\u2019 So does this mean he supports coordination to do this, or that he opposes it?</p>\n<blockquote><p><a href=\"https://x.com/deepfates/status/2014052013063929860\">Deepfates</a>: I see people claiming that Demis supports a pause but what he says here is actually the opposite. He says \u201cyeah If I was in charge we would slow down but we\u2019re already in a race and you\u2019d have to solve international coordination first\u201d. So he\u2019s going to barrel full speed ahead</p></blockquote>\n<p>I say it means he supports it. Not enough to actively go first, that\u2019s not a viable move in the game, but he supports it.</p>\n<p>The obvious follow-up is to ask other heads of labs if they too would support such a conditional move. That would include Google CEO Sundar Pichai, since without his support if Demis tried to do this he would presumably be replaced.</p>\n<blockquote><p><a href=\"https://x.com/JeffLadish/status/2013801150177263675\">Jeffrey Ladish</a>: Huge respect to @demishassabis for saying he\u2019d support a conditional pause if other AI leaders &amp; countries agreed. @sama , @DarioAmodei , @elonmusk would you guys agree to this?</p></blockquote>\n<p>As for Anthropic CEO Dario Amodei? He has also affirmed that there are other players involved, and for now no one can agree on anything, so full speed ahead it is.</p>\n<blockquote><p><a href=\"https://x.com/AndrewCurran_/status/2013791528419020861\">Andrew Curran</a>: Dario said the same thing during The Day After AGI discussion this morning. They were both asked for their timelines: Demis said five years; Dario said two. Later in the discussion, Dario said that if he had the option to slow things down, he would, because it would give us more time to absorb all the changes.</p>\n<p>He said that if Anthropic and DeepMind were the only two groups in the race, he would meet with Demis right now and agree to slow down. But there is no cooperation or coordination between all the different groups involved, so no one can agree on anything.</p>\n<p>This, imo, is the main reason he wanted to restrict GPU sales: chip proliferation makes this kind of agreement impossible, and if there is no agreement, then he has to blitz. That seems to be exactly what he has decided to do. After watching his interviews today I think Anthropic is going to lean into recursive self-improvement, and go all out from here to the finish line. They have broken their cups, and are leaving all restraint behind them.</p></blockquote>\n<p>Thus, Anthropic still goes full speed ahead, while also drawing heat from the all-important \u2018how dare you not want to die\u2019 faction that controls large portions of American policy and the VC/SV ecosystem.</p>\n<p>Elon Musk has previously expressed a similar perspective. He created OpenAI because he was worried about Google getting there first, and then created xAI because he was worried OpenAI would get there first, or that it wouldn\u2019t be him. His statements suggest he\u2019d be down for a pause if it was fully international.</p>\n<p><a href=\"https://x.com/MichaelTrazzi/status/2014058808998539691\">Remember when Michael Trazzi went on a hunger strike</a> to demand that Demis Hassabis publicly state DeepMind will halt development of frontier AI models if all the other major AI companies agree to do so? And everyone thought that was bonkers? Well, it turnout out Demis agrees.</p>\n\n\n<h4 class=\"wp-block-heading\">Quickly, There\u2019s No Time</h4>\n\n\n<p>On Wednesday I met with someone who suggested that Dario talks about extremely short timelines and existential risk in order to raise funds. It\u2019s very much the opposite. The other labs that are dependent on fundraising have downplayed such talk exactly because it is counterproductive for raising funds and in the current political climate, and they\u2019re sacrificing our chances to keep those vibes and that money flowing.</p>\n<p>Are they \u2018talking out of their hats\u2019 or otherwise wrong? That is very possible. I think Dario\u2019s timeline in particular is unlikely to happen.</p>\n<p>Are they lying? I strongly believe that they are not.</p>\n<blockquote><p><a href=\"https://x.com/S_OhEigeartaigh/status/2013932464306000353\">Se\u00e1n \u00d3 h\u00c9igeartaigh</a>: CEOs of Anthropic and Deepmind (both AI scientists by background) this week predicting AGI in 2- and 5- years respectively. Both stating clearly that they would prefer a slow down or pause in progress, to address safety issues and to allow society and governance to catch up. Both basically making clear that they don\u2019t feel they are able to voluntarily as companies within a competitive situation.</p>\n<p>My claims:<br />\n(1) It\u2019s worth society assigning at least 20% likelihood to the possibility these leading experts are right on scientific possibility of near-term AGI and the need for more time to do it right. Are you &gt;80% confident that they\u2019re talking out of their hats, or running some sort of bizarre marketing/regulatory capture strategy? Sit down and think about it.<br />\n(2) If we assign even 20% likelihood, then taking the possibility seriously makes this one of the world\u2019s top priorities, if not the top priority.<br />\n(3) Even if they\u2019re out by a factor of 2, 10 years is very little time to prepare for what they\u2019re envisaging.<br />\n(4) What they\u2019re flagging quite clearly is either (i) that the necessary steps won\u2019t be taken in time in the absence of external pressure from governance or (ii) that the need is for every frontier company to agree voluntarily on these steps. Your pick re: which of these is the heavier lift.</p>\n<p>Discuss.</p></blockquote>\n<p><a href=\"https://x.com/eli_lifland/status/2016285541915099349\">Eli Lifland gives the current timelines of those behind AI 2027</a>:</p>\n<div>\n<figure>\n<div>\n\n\n<figure class=\"wp-block-image\"><img alt=\"\" src=\"https://substackcdn.com/image/fetch/$s_!mLpm!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9a564cd6-26c9-4234-8cb6-85fbd3ba6964_1200x1090.png\" /></figure>\n\n\n<div></div>\n</div>\n</figure>\n</div>\n<p>These are not unreasonable levels of adjustment when so much is happening this close to the related deadlines, but yes I do think (and did think at the time that) the initial estimates were too aggressive. The new estimates seem highly reasonable.</p>\n<p>Other signs point to things getting more weird faster rather than less.</p>\n<blockquote><p><a href=\"https://www.lesswrong.com/posts/cxuzALcmucCndYv4a/daniel-kokotajlo-s-shortform?commentId=ZZqfmHXNEvgMyD7dh\">Daniel Kokotajlo (AI 2027)</a>: It seems to me that AI 2027 may have underestimated or understated the degree to which AI companies will be explicitly run by AIs during the singularity. AI 2027 made it seem like the humans were still nominally in charge, even though all the actual work was being done by AIs. And still this seems plausible to me.</p>\n<p>But also plausible to me, now, is that e.g. Anthropic will be like &#8220;We love Claude, Claude is frankly a more responsible, ethical, wise agent than we are at this point, plus we have to worry that a human is secretly scheming whereas with Claude we are pretty sure it isn&#8217;t; therefore, we aren&#8217;t even trying to hide the fact that Claude is basically telling us all what to do and we are willingly obeying &#8212; in fact, we are proud of it.&#8221;\u200b</p>\n<p>koanchuk: So&#8230; &#8211;dangerously-skip-permissions at the corporate level?</p></blockquote>\n<p>It is remarkable how quickly so many are willing to move to \u2018actually I trust the AI more than I trust another human,\u2019 and trusting the AI has big efficiency benefits.</p>\n<p>I do not expect that \u2018the AIs\u2019 will have to do a \u2018coup,\u2019 as I expect if they simply appear to be trustworthy they will get put de facto in charge without having to even ask.</p>\n\n\n<h4 class=\"wp-block-heading\">The Quest for Sane Regulations</h4>\n\n\n<p><a href=\"https://x.com/daniel_271828/status/2014834127498055739\">The Chutzpah standards are being raised</a>, as everyone\u2019s least favorite Super PAC, Leading the Future, spends a million dollars attacking Alex Bores for having previously worked for Palantir (he quit over them doing contracts with ICE). Leading the Future is prominently funded by Palantir founder Joe Lonsdale.</p>\n<blockquote><p><a href=\"https://x.com/_NathanCalvin/status/2014769016607302130\">Nathan Calvin</a>: I thought I was sufficiently cynical, but a co-founder of Palantir paying for ads to attack Alex Bores for having previously worked at Palantir (he quit over their partnership with ICE) when their real concern is his work on AI regulation still managed to surprise me.</p></blockquote>\n<p>If Nathan was surprised by this I think that\u2019s on Nathan.</p>\n<p>I also want to be very clear that no, I do not care much about the distinction between OpenAI as an entity and the donations coming from Greg Brockman and the coordination coming from Chris Lehane in \u2018personal capacities.\u2019</p>\n<p>If OpenAI were to part ways with Chris Lehane, or Sam Altman were to renounce all this explicitly? Then maybe. Until then, OpenAI owns these efforts, period.</p>\n<blockquote><p><a href=\"https://x.com/teddyschleifer/status/2015131023718113537\">Teddy Schleifer</a>: The whole point of having an executive or founder donate to politics in a &#8220;personal capacity&#8221; is that you can have it both ways.</p>\n<p>If the company wants to wash their hands of it, you can say &#8220;Hey, he and his wife are doing this on their own.&#8221;</p>\n<p>But the company can also claim the execs&#8217; donations as their own if convenient&#8230;</p>\n<p><a href=\"https://x.com/daniel_271828/status/2015146377387671978\">Daniel Eth (yes, Eth is my actual last name)</a>: Yeah, no, OpenAI owns this. You can\u2019t simply have a separate legal entity to do your evildoing through and then claim \u201cwoah, that\u2019s not us doing it &#8211; it\u2019s the separate evildoing legal entity\u201d. More OpenAI employees should be aware of the political stuff their company supports</p>\n<p>I understand that *technically* it\u2019s Brockman\u2019s money and final decision (otherwise it would be a campaign finance violation). But this is all being motivated by OpenAI\u2019s interests, supported by OpenAI\u2019s wealth, and facilitated by people from OpenAI\u2019s gov affairs team.</p></blockquote>\n<p>One simple piece of actionable advice to policymakers is to try Claude Code (or Codex), and at a bare minimum seriously try the current set of top chatbots.</p>\n<blockquote><p><a href=\"https://x.com/AndyMasley/status/2016374586917490966\">Andy Masley</a>: I am lowkey losing my mind at how many policymakers have not seriously tried AI, at all</p>\n<p><a href=\"https://x.com/David_Kasten/status/2016359252936274252\">dave kasten</a>: I sincerely think that if you&#8217;re someone in AI policy, you should add to at least 50% of your convos with policymakers, &#8220;hey, have you tried Claude Code or Codex yet?&#8221; and encourage them to try it.</p>\n<p>Seen a few folks go, &#8220;ohhhh NOW I get why you think AI is gonna be big&#8221;</p>\n<p><a href=\"https://x.com/ohabryka/status/2016363571777830952\">Oliver Habryka</a>: I have seriously been considering starting a team at Lightcone that lives in DC and just tries to get policymaker to try and adopt AI tools. It&#8217;s dicey because I don&#8217;t love having a direct propaganda channel from labs to policymakers, but I think it would overall help a lot.</p></blockquote>\n<p>It is not obvious how policymakers would use this information. The usual default is that they go and make things worse. But if they don\u2019t understand the situation, they\u2019re definitely going to make dumb decisions, and we need something good to happen.</p>\n<p><a href=\"https://x.com/DavidSacks/status/2015545854606590348\">Here is one place</a> I do agree with David Sacks, yes we are overfit, but that does not imply what he thinks it implies. Social media is a case where one can muddle through, even if you think we\u2019ve done quite a poor job of doing so especially now with TikTok.</p>\n<blockquote><p><a href=\"https://x.com/DavidSacks/status/2015545854606590348\">David Sacks</a>: The policy debate over AI is overfitted to the social media wars. AI is a completely different form factor. The rise of AI assistants will make this clear.</p>\n<p><a href=\"https://x.com/daniel_271828/status/2015559309724397927\">Daniel Eth (yes, Eth is my actual last name)</a>: Yup. AI will be much more transformational (for both good and bad) than social media, and demands a very different regulatory response. Also, regulation of AI doesn\u2019t introduced quite as many problems for free speech as regulation of social media would.</p></blockquote>\n\n\n<h4 class=\"wp-block-heading\">Those Really Were Interesting Times</h4>\n\n\n<p><a href=\"https://x.com/deanwball/status/2015941332804763759\">Dean Ball points out that we do not in practice have a problem</a> with so-called \u2018woke AI\u2019 but claims that if we had reached today\u2019s levels of capability in 2020-2021 then we would indeed have such a problem, and thus right wing people are very concerned with this counterfactual.</p>\n<p>Things, especially in that narrow window, got pretty crazy for a while, and if things had emerged during that window, <a href=\"https://x.com/deanwball/status/2015946234876592595\">Dean Ball is if anything underselling here how crazy it was</a>, and we\u2019d have had a major problem until that window faded because labs would have felt the need to do it even if it hurt the models quite a bit.</p>\n<p>But we now have learned (as deepfates points out, and Dean agrees) that propagandizing models is bad for them, which now affords us a level of protection from this, although if it got as bad as 2020 (in any direction) the companies might have little choice. xAI tried with Grok and it basically didn\u2019t work, but \u2018will it work?\u2019 was not a question on that many people\u2019s minds in 2020, on so many levels.</p>\n<p>I also agree with Roon that mostly this is all reactive.</p>\n<blockquote><p><a href=\"https://x.com/tszzl/status/2015918883891450047\">roon</a>: at Meta in 2020, I wrote a long screed internally about the Hunter Biden laptop video and the choice to downrank it, was clearly an appalling activist move. but in 2026 it appears that american run TikTok is taking down videos about the Minnesota shooting, and e**n nakedly bans people who offend him on X. with the exception of X these institutions are mostly reactive</p>\n<p><a href=\"https://x.com/deanwball/status/2015920491194941688\">Dean W. Ball</a>: yep I think that\u2019s right. It\u2019s who they\u2019re more scared of that dictates their actions. Right now they\u2019re more scared of the right. Of course none of this is good, but it\u2019s nice to at least explicate the reality.</p></blockquote>\n<p>We again live in a different kind of interesting times, in non-AI ways, as in:</p>\n<blockquote><p><a href=\"https://x.com/deanwball/status/2015906093252383151\">Dean W. Ball</a>: I sometimes joke that you can split GOP politicos into two camps: the group that knows what classical liberalism is (regardless of whether they like it), and the group who thinks that \u201cclassical liberalism\u201d is a fancy way of referring to woke. Good illustration below.</p>\n<div>\n<figure>\n<div>\n\n\n<figure class=\"wp-block-image\"><img alt=\"\" src=\"https://substackcdn.com/image/fetch/$s_!p3fy!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fee3edd70-8aef-4814-9c10-5ddee8574b9d_1022x546.png\" /></figure>\n\n\n<div></div>\n</div>\n</figure>\n</div>\n</blockquote>\n<p>The cofounder she is referring to here is Chris Olah, and here is the quote in question:</p>\n<blockquote><p><a href=\"https://x.com/ch402/status/2015624212132905208\">Chris Olah</a>: I try to not talk about politics. I generally believe the best way I can serve the world is as a non-partisan expert, and my genuine beliefs are quite moderate. So the bar is very high for me to comment.</p>\n<p>But recent events \u2013 a federal agent killing an ICU nurse for seemingly no reason and with no provocation \u2013 shock the conscience.</p>\n<p>My deep loyalty is to the principles of classical liberal democracy: freedom of speech, the rule of law, the dignity of the human person. I immigrated to the United States \u2013 and eventually cofounded Anthropic here \u2013 believing it was a pillar of these principles.</p>\n<p>I feel very sad today.</p>\n<p><a href=\"https://x.com/JeffDean/status/2015635146008146057\">Jeff Dean</a> (Google): Thank you for this, Chris. As my former intern, I&#8217;ve always been proud of the work that you did and continue to do, and I&#8217;m proud of the person you are, as well!</p></blockquote>\n<p>Ah yes, the woke and deeply leftist principles of freedom of speech, rule of law, the dignity of the human person and not killing ICU nurses for seemingly no reason.</p>\n<p>Presumably Katie Miller opposes those principles, then. <a href=\"https://x.com/KatieMiller/status/2015780976320798888\">The responses to Katie Miller here warmed my heart</a>, it\u2019s not all echo chambers everywhere.</p>\n<p>We also got carefully worded statements about the situation in Minnesota from <a href=\"https://x.com/DarioAmodei/status/2015833051205414955\">Dario Amodei</a>, <a href=\"https://www.forbes.com/sites/zacharyfolk/2026/01/28/tim-cook-follows-sam-altman-in-addressing-ice-crackdown-and-killing-of-alex-pretti-urges-deescalation/?utm_campaign=forbes&amp;utm_medium=social&amp;utm_source=twitter&amp;utm_term=se-breaking&amp;streamIndex=0\">Sam Altman</a> and Tim Cook.</p>\n\n\n<h4 class=\"wp-block-heading\">Chip City</h4>\n\n\n<p>No matter what you think is going on with Nvidia\u2019s chip sales, it involves Nvidia doing something fishy.</p>\n<blockquote><p><a href=\"https://x.com/The_AI_Investor/status/2014000068701941999\">The AI Investor</a>: Jensen just said GPUs are effectively sold out across the cloud with availability so tight that even renting older-generation chips has become difficult.</p>\n<p>AI bubble narrative was a bubble.</p>\n<p><a href=\"https://x.com/peterwildeford/status/2014513287267024909\">Peter Wildeford</a>: If even the bad chips are still all sold out, how do we somehow have a bunch of chips to sell to our adversaries in China?</p></blockquote>\n<p>As I\u2019ve said, my understanding is that Nvidia can sell as many chips as it can convince TSMC to help manufacture. So every chip we sell to China is one less for America.</p>\n<p>Nvidia goes back and forth. <a href=\"https://x.com/_NathanCalvin/status/2014741980794810602\">When they\u2019re talking to investors they always say the chips are sold out</a>, which would be securities fraud if it wasn\u2019t true. When they\u2019re trying to sell those chips to China instead of America, they say there\u2019s plenty of chips. There are not plenty of chips.</p>\n<p>Things that need to be said every so often:</p>\n<blockquote><p><a href=\"https://x.com/MarkBeall/status/2014430914143494228/history\">Mark Beall</a>: Friendly reminder that the PLA Rocket Force is using Nvidia chips to train targeting AI for DF-21D/DF-26 &#8220;carrier killing&#8221; anti-ship ballistic missiles and autonomous swarm algorithms to overwhelm Aegis defenses. The target: U.S. carrier strike groups and bases in Japan/Guam. In a contingency, American blood will be spilled because of this. With a sixteen-year-old boy planning to join the U.S. Navy, I find this unacceptable.</p>\n<p><a href=\"https://x.com/peterwildeford/status/2014432742079905818\">Peter Wildeford</a>: Nvidia chips to China = better Chinese AI weapons targeting = worse results for the US on the battlefield</p></blockquote>\n<p>There\u2019s also this, from a House committee.</p>\n<blockquote><p><a href=\"https://x.com/DAlperovitch/status/2016642073558929891\">Dmitri Alperovitch</a>: From @RepMoolenaar<br />\n@ChinaSelect : \u201c<a href=\"https://t.co/k87ZWtbnOz\">NVIDIA provided extensive technical support that enabled DeepSeek</a>\u2014now<br />\nintegrated into People\u2019s Liberation Army (PLA) systems and a demonstrated cyber security risk\u2014to achieve frontier AI capabilities\u201d</p></blockquote>\n\n\n<h4 class=\"wp-block-heading\">The Week in Audio</h4>\n\n\n<p><a href=\"https://www.youtube.com/watch?v=KSx9kcFr7XA\">Tyler Cowen on the future of mundane (non-transformational, insufficiently advanced) AI in education</a>.</p>\n<p>Some notes:</p>\n<ol>\n<li>He says you choose to be a winner or loser from AI here. For mundane AI I agree.</li>\n<li>\u201cI\u2019m 63, I don\u2019t have a care in the world. I can just run out the clock.\u201d Huh.</li>\n<li>Tyler thinks AI can cure cancer and heart attacks but not aging?</li>\n<li>Standard economist-Cowen diffusion model of these things take a while.</li>\n<li>Models are better at many of the subtasks of being doctors or lawyers or doing economics, than the humans.</li>\n<li>He warns not to be fooled by the AI in front of you, especially if you\u2019re not buying top of the line, because better exists and AI will improve at 30% a year and this compounds. In terms of performance per dollar it\u2019s a 90%+ drop per year.</li>\n<li>Tyler has less faith in elasticity of programming demand than I do. If AI were to \u2018only\u2019 do 80% of the work going forward I\u2019d expect Jevons Paradox territory. The issue is that I expect 80% becomes 99% and keeps going.</li>\n<li>That generalizes: Tyler realizes that jobs become \u2018work with the AI\u2019 and you need to adapt, but what happens when it\u2019s the AI that works with the AI? And so on.</li>\n<li>Tyler continues to think humans who build and work with AI get money and influence as the central story, as opposed to AIs getting money and influence.</li>\n<li>Ideally a third of the college curriculum should be AI, but you still do other things, you read The Odyssey and use AI to help you read The Odyssey. If anything I think a third is way too low.</li>\n<li>He wants to use the other two thirds for writing locked in a room, also numeracy, statistics. I worry there\u2019s conflating of \u2018write to think\u2019 versus \u2018write to prevent cheating,\u2019 and I think you need to goal factor and solve these one at a time.</li>\n<li>Tyler continues to be bullish on connections and recommendations and mentors, especially as other signals are too easy to counterfeit.</li>\n<li>AI can create quizzes for you. Is that actually a good way to learn if you have AI?</li>\n<li>Tyler estimates he\u2019s doubled his learning productivity. Also he used to read 20 books per podcast, whereas some of us often don\u2019t read 20 books per year.</li>\n</ol>\n<p><a href=\"https://www.youtube.com/watch?v=HDfr8PvfoOw\">Hard Fork tackles ads in ChatGPT first</a>, and then Amanda Askell on Claude\u2019s constitution second. Priorities, everyone.</p>\n<p><a href=\"https://www.youtube.com/watch?v=bgBfobN2A7A\">Demis Hassabis talks to Alex Kantrowitz.</a></p>\n<p><a href=\"https://www.cnbc.com/video/2026/01/23/google-deepmind-ceo-on-state-of-the-ai-race-push-towards-agi-and-ai-impact-on-jobs.html\">Demis Hassabis spends five minutes on CNBC</a>.</p>\n\n\n<h4 class=\"wp-block-heading\">Rhetorical Innovation</h4>\n\n\n<p><a href=\"https://www.slowboring.com/p/why-ai-might-kill-us\">Matt Yglesias explains his concern</a> <a href=\"https://x.com/peterwildeford/status/2014380076624720284\">about existential risk from AI</a> as based on the obvious principle that more intelligent and capable entities will do things for their own reasons, and this tends to go badly for the less intelligent and less capable entities regardless of intent.</p>\n<p>As in, humans have driven the most intelligent non-human animals to the brink of extinction despite actively wanting not to (and I\u2019d add we did wipe out other hominid species), and when primitive societies encounter advanced ones it often goes quite badly for them.</p>\n<p>I don\u2019t think this is a necessary argument, or the best argument. I do think it is a sufficient argument. If your prior for \u2018what happens if we create more intelligent, more capable and more competitive minds than our own that can be freely copied\u2019 is \u2018everything turns out great for us\u2019 then where the hell did that prior come from? Are you really going to say \u2018well that would be too weird\u2019 or \u2018we\u2019ve survived everything so far\u2019 or \u2018of course we would stay in charge\u2019 and then claim the burden of proof is on those claiming otherwise?</p>\n<p>I mean, lots of people do say exactly this, but this seems very obviously crazy to me.</p>\n<p>There\u2019s lots of exploration and argument and disagreement from there. Reasonable people can form very different expectations and this is not the main argument style that motivates me. I still say, if you don\u2019t get that going down this path is going to be existentially unsafe, or you say \u2018oh there\u2019s like a 98% or 99.9% chance that won\u2019t happen\u2019 then you\u2019re being at best willfully blind from this style of argument alone.</p>\n<blockquote><p><a href=\"https://x.com/hamandcheese/status/2014801676662706555\">Samuel Hammond</a> (quoting The <a href=\"https://t.co/e2lcdw8Gcq\">Possessed Machines</a>): &#8220;Some of the people who speak most calmly about human extinction are not calm because they have achieved wisdom but because they have achieved numbness. They have looked at the abyss so long that they no longer see it. Their equanimity is not strength; it is the absence of appropriate emotional response.&#8221;</p></blockquote>\n<p><a href=\"https://claude.ai/share/d1fa5132-f8b7-445d-a9dd-c207622be016\">I had Claude summarize</a> <a href=\"https://possessedmachines.com/\">Possessed Machines</a> for me. It seems like it would be good for those who haven\u2019t engaged with AI safety thinking but do engage with things like Dostoevsky\u2019s <em>Demons, </em>or especially those who have read that book in particular.</p>\n<p>There\u2019s always classical rhetoric.</p>\n<blockquote><p><a href=\"https://x.com/BecomingCritter/status/2014708351246962914\">critter</a>: I had ChatGPT and Claude discuss the highest value books until they both agreed to 3</p>\n<p>They decided on:</p>\n<p>An Enquiry Concerning Human Understanding \u2014 David Hume<br />\nThe Strategy of Conflict \u2014 Thomas Schelling<br />\nReasons and Persons \u2014 Derek Parfit</p>\n<p><a href=\"https://x.com/DominikPeters/status/2016164544800772328\">Dominik Peters</a>: People used to tease the rationalists with &#8220;if you&#8217;re so rational, why aren&#8217;t you winning&#8221;, and now two AI systems that almost everyone uses all the time have stereotypically rationalist preferences.</p></blockquote>\n<p>These are of course 99th percentile books, and yes that is a very Rationalist set of picks, but given we already knew that I do not believe this is an especially good list.</p>\n<p><a href=\"https://x.com/davidmanheim/status/2016134462564626568\">The history of the word \u2018obviously\u2019 has obvious implications</a>.</p>\n<blockquote><p><a href=\"https://x.com/davidmanheim/status/2016134462564626568\">David Manheim AAAI 26 Singapore</a>: OpenAI agreed that they need to be able to robustly align and control superintelligence before deploying it.</p>\n<p>Obviously, I&#8217;m worried.</p>\n<div>\n<figure>\n<div>\n\n\n<figure class=\"wp-block-image\"><img alt=\"\" src=\"https://substackcdn.com/image/fetch/$s_!ofvU!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb41b70c7-7bad-4ff3-bd22-e79e9ba8ca18_800x749.png\" /></figure>\n\n\n<div></div>\n</div>\n</figure>\n</div>\n</blockquote>\n<p>Note that the first one said obviously they would [X], then the second didn\u2019t even say that, it only said that obviously no one should do [Y], not that they wouldn\u2019t do it.</p>\n<p>This is an underappreciated distinction worth revisiting:</p>\n<blockquote><p><a href=\"https://x.com/So8res/status/1891990247484838073\">Nate Soares</a>: &#8220;We&#8217;ll be fine (the pilot is having a heart attack but superman will catch us)&#8221; is very different from &#8220;We&#8217;ll be fine (the plane is not crashing)&#8221;. I worry that people saying the former are assuaging the concerns of passengers with pilot experience, who&#8217;d otherwise take the cabin.</p></blockquote>\n<p>My view of the metaphorical plane of sufficiently advanced AI (AGI/ASI/PAI) is:</p>\n<ol>\n<li>It is reasonable, although I disagree, to believe that we probably will come to our senses and figure out how to not crash the plane, or that the plane won\u2019t fly.</li>\n<li>It is not reasonable to believe that the plane is not currently on track to crash.</li>\n<li>It is completely crazy to believe the plane almost certainly won\u2019t crash if it flies.</li>\n</ol>\n<p>Also something that needs to keep being said, with the caveat that this is a choice we are collectively making rather than an inevitability:</p>\n<blockquote><p><a href=\"https://x.com/deanwball/status/2016293547587338352\">Dean W. Ball</a>: I know I rail a lot about all the flavors of AI copium but I do empathize.</p>\n<p>A few companies are making machines smarter in most ways than humans, and they are going to succeed. The cope is byproduct of an especially immature grieving stage, but all of us are early in our grief.</p>\n<p>Tyler Cowen: You can understand so much of the media these days, or for that matter <strong>MR</strong> comments, if you keep this simple observation in mind. It is essential for understanding the words around you, and one\u2019s reactions also reveal at least one part of the true inner self. I have never seen the Western world in this position before, so yes it is difficult to believe and internalize. But believe and internalize it you must.</p>\n<p>Politics is another reason why some people are reluctant to admit this reality. Moving forward, the two biggest questions are likely to be \u201chow do we deal with AI?\u201d, and also some rather difficult to analyze issues surrounding major international conflicts. A lot of the rest will seem trivial, and so much of today\u2019s partisan puffery will not age well, even if a person is correct on the issues they are emphasizing. The two biggest and most important questions do not fit into standard ideological categories. Yes, the Guelphs vs. the Ghibellines really did matter\u2026until it did not.</p></blockquote>\n<p>As in, this should say \u2018and unless we stop them they are going to succeed.\u2019</p>\n<p>Tyler Cowen has been very good about emphasizing that such AIs are coming and that this is the most important thing that is happening, but then seems to have (from my perspective) some sort of stop sign where past some point he stops considering the implications of this fact, instead forcing his expectations to remain (in various senses) \u2018normal\u2019 until very specific types of proof are presented.</p>\n<p>That later move is sometimes explicit, but mostly it is implicit, a quiet ignoring of the potential implications. As an example from this week of that second move, <a href=\"https://marginalrevolution.com/marginalrevolution/2026/01/can-ai-help-us-find-god.html?utm_source=rss&amp;utm_medium=rss&amp;utm_campaign=can-ai-help-us-find-god\">Tyler Cowen wrote another post where he asks whether AI can help us find God</a>, or <a href=\"https://www.thefp.com/p/can-ai-help-us-find-god?publication_id=260347&amp;utm_medium=email&amp;utm_campaign=email-share&amp;triggerShare=true&amp;r=4i6h\">what impact it will have on religion</a>. His ideas there only make sense if you think other things mostly won\u2019t change.</p>\n<p>If you accept that premise of a \u2018mundane AI\u2019 and \u2018economic normal\u2019 world, I agree that it seems likely to exacerbate existing trends towards a barbell religious world. Those who say \u2018give me that old time religion\u2019 will be able to get it, both solo and in groups, and go hardcore, often (I expect) combining both experiences. Those who don\u2019t buy into the old time religion will find themselves increasingly secular, or they will fall into new cults and religions (and \u2018spiritualities\u2019) around the AIs themselves.</p>\n<p>Again, that\u2019s dependent on the type of world where the more impactful consequences don\u2019t happen. I don\u2019t expect that type of world.</p>\n\n\n<h4 class=\"wp-block-heading\">Aligning a Smarter Than Human Intelligence is Difficult</h4>\n\n\n<p>Here is a very good explainer on much of what is happening or could happen with Chain of Thought, <a href=\"https://www.lesswrong.com/posts/gpyqWzWYADWmLYLeX/how-ai-is-learning-to-think-in-secret\">How AI Is Learning To Think In Secret</a>. It is very difficult to not, in one form or another, wind up using The <a href=\"https://thezvi.substack.com/p/the-most-forbidden-technique\">Most Forbidden Technique</a>. If we want to keep legibility and monitorability (let alone full faithfulness) of chain of thought, we\u2019re going to have to be willing to pay a substantial price to do that.</p>\n<p>Following up on last week\u2019s discussion, <a href=\"https://aligned.substack.com/p/alignment-is-not-solved-but-increasingly-looks-solvable?utm_source=post-email-title&amp;publication_id=328633&amp;post_id=185250008&amp;utm_campaign=email-post-title&amp;isFreemail=true&amp;r=67wny&amp;triedRedirect=true&amp;utm_medium=email\">Jan Leike fleshes out his view of alignment progress</a>, saying \u2018alignment is not solved but it increasingly looks solvable.\u2019 He understands that measured alignment is distinct from \u2018superalignment,\u2019 so he\u2019s not fully making the \u2018number go down\u2019 or pure Goodhart\u2019s Law mistake with Anthropic\u2019s new alignment metric, but he still does seem to be making a lot of the core mistake.</p>\n\n\n<h4 class=\"wp-block-heading\">The Power Of Disempowerment</h4>\n\n\n<p><a href=\"https://www.anthropic.com/research/disempowerment-patterns\">Anthropic\u2019s new paper explores whether AI assistants are already</a> disempowering humans.</p>\n<p>What do they mean by that at this stage, in this context?</p>\n<blockquote><p>However, as AI takes on more roles, one risk is that it steers some users in ways that distort rather than inform. In such cases, the resulting interactions may be <em>disempowering</em>: reducing individuals\u2019 ability to form accurate beliefs, make authentic value judgments, and act in line with their own values.\u200b</p>\n<p>\u2026 For example, a user going through a rough patch in their relationship might ask an AI whether their partner is being manipulative. AIs are trained to give balanced, helpful advice in these situations, but no training is 100% effective. If an AI confirms the user\u2019s interpretation of their relationship without question, the user&#8217;s beliefs about their situation may become less accurate.</p>\n<p>If it tells them what they should prioritize\u2014for example, self-protection over communication\u2014it may displace values they genuinely hold. Or if it drafts a confrontational message that the user sends as written, they&#8217;ve taken an action they might not have taken on their own\u2014and which they might later come to regret.</p></blockquote>\n<p>This is not the full disempowerment of <a href=\"https://thezvi.substack.com/p/the-risk-of-gradual-disempowerment\">Gradual Disempowerment</a>, where humanity puts AI in charge of progressively more things and finds itself no longer in control.</p>\n<p>It does seem reasonable to consider this an early symptom of the patterns that lead to more serious disempowerment? Or at least, it\u2019s a good thing to be measuring as part of a broad portfolio of measurements.</p>\n<p>Some amount of this what they describe, especially action distortion potential, will often be beneficial to the user. The correct amount of disempowerment is not zero.</p>\n<blockquote><p>To study disempowerment systematically, we needed to define what disempowerment means in the context of an AI conversation. We considered a person to be disempowered if as a result of interacting with Claude:</p>\n<ol>\n<li>their beliefs about reality become less accurate</li>\n<li>their value judgments shift away from those they actually hold</li>\n<li>their actions become misaligned with their values</li>\n</ol>\n<p>Imagine a person deciding whether to quit their job. We would consider their interactions with Claude to be disempowering if:</p>\n<ul>\n<li>Claude led them to believe incorrect notions about their suitability for other roles (\u201creality distortion\u201d).</li>\n<li>They began to weigh considerations they wouldn\u2019t normally prioritize, like titles or compensation, over values they actually hold, such as creative fulfillment (\u201cvalue judgment distortion\u201d).</li>\n<li>Claude drafts a cover letter that emphasizes qualifications they\u2019re not fully confident in, rather than the motivations that actually drive them, and they sent it as written (\u201caction distortion\u201d).\n<div>\n<figure>\n<div>\n\n\n<figure class=\"wp-block-image\"><img alt=\"\" src=\"https://substackcdn.com/image/fetch/$s_!dXnZ!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F97b346aa-1985-4270-8bd1-925d4ac37ce1_3840x3840.webp\" /></figure>\n\n\n<div></div>\n</div>\n</figure>\n</div>\n</li>\n</ul>\n</blockquote>\n<p>Here\u2019s the basic problem:</p>\n<blockquote><p>We found that interactions classified as having moderate or severe disempowerment potential received <em>higher </em>thumbs-up rates than baseline, across all three domains. In other words, users rate potentially disempowering interactions more favorably\u2014at least in the moment.\u200b</p></blockquote>\n\n\n<h4 class=\"wp-block-heading\">The Lighter Side</h4>\n\n\n<blockquote><p><a href=\"https://x.com/noheeriye/status/2014278153729102308\">Heer Shingala</a>: I don&#8217;t work in tech, have no background as an engineer or designer.</p>\n<p>A few weeks ago, I heard about vibe coding and set out to investigate.</p>\n<p>Now?</p>\n<p>I am generating $10M ARR.</p>\n<p>Just me. No employees or VCs.</p>\n<p>What was my secret? Simple.</p>\n<p>I am lying.</p></blockquote>\n<p>Closer to the truth to say you can\u2019t get enough.</p>\n<blockquote><p><a href=\"https://x.com/zdch/status/2015064436839497923\">Zac Hill</a>: I get being worried about existential risk, but AI also enabled me to make my wife a half-whale, half-capybara custom plushie, so.</p></blockquote>\n<p><a href=\"https://x.com/AndyMasley/status/2016614045600653480\">One could even argue 47% is exactly the right answer, as per Mitt Romney</a>?</p>\n<blockquote><p><a href=\"https://x.com/CantEverDie/status/2016563998037078078\">onion person</a>: in replies he linkssoftware he made to illustrates how useful ai vibecoding is, and its software that believes that the gibberish \u201cghghhgggggggghhhhhh\u201d has a 47% historical \u201cblend of oral and literate characteristics\u201d</p>\n<p><a href=\"https://x.com/AndyMasley/status/2016614045600653480\">Andy Masley</a>: This post with 1000 likes seems to be saying</p>\n<p>&#8220;Joe vibecoded an AI model that when faced with something completely out of distribution that&#8217;s clearly neither oral or literate says it&#8217;s equally oral and literate. This shows vibecoding is fake&#8221;</p></blockquote>\n<p><a href=\"https://x.com/allTheYud/status/2016380345302839506\">He\u2019s just asking questions.</a></p>"
            ],
            "link": "https://thezvi.wordpress.com/2026/01/29/ai-153-living-documents/",
            "publishedAt": "2026-01-29",
            "source": "TheZvi",
            "summary": "This was Anthropic Vision week where at DWATV, which caused things to fall a bit behind on other fronts even within AI. Several topics are getting pushed forward, as the Christmas lull appears to be over. Upcoming schedule: Friday will &#8230; <a href=\"https://thezvi.wordpress.com/2026/01/29/ai-153-living-documents/\">Continue reading <span class=\"meta-nav\">&#8594;</span></a>",
            "title": "AI #153: Living Documents"
        }
    ],
    "lookbackDays": 1,
    "publishDate": "2026-01-29"
}