{
    "articles": [
        {
            "content": [],
            "link": "https://harper.blog/notes/2025-12-25_d33136ee482f_happy-holidays-from-about-righ/",
            "publishedAt": "2025-12-25",
            "source": "Harper Reed",
            "summary": "<p>Happy Holidays from about right here</p> <figure> <img alt=\"image_1.jpg\" height=\"1800\" src=\"https://harper.blog/notes/2025-12-25_d33136ee482f_happy-holidays-from-about-righ/image_1.jpg\" width=\"1800\" /> </figure> <hr /> <p>Thank you for using RSS. I appreciate you. <a href=\"mailto:harper&#64;modest.com\">Email me</a></p>",
            "title": "Note #307"
        },
        {
            "content": [],
            "link": "https://simonwillison.net/2025/Dec/25/claude-code-transcripts/#atom-entries",
            "publishedAt": "2025-12-25",
            "source": "Simon Willison",
            "summary": "<p>I've released <a href=\"https://github.com/simonw/claude-code-transcripts\">claude-code-transcripts</a>, a new Python CLI tool for converting <a href=\"https://claude.ai/code\">Claude Code</a> transcripts to detailed HTML pages that provide a better interface for understanding what Claude Code has done than even Claude Code itself. The resulting transcripts are also designed to be shared, using any static HTML hosting or even via GitHub Gists.</p> <p>Here's the quick start, with no installation required if you already have <a href=\"https://docs.astral.sh/uv/\">uv</a>:</p> <pre><code>uvx claude-code-transcripts </code></pre> <p>(Or you could <code>uv tool install claude-code-transcripts</code> or <code>pip install claude-code-transcripts</code> first, if you like.)</p> <p>This will bring up a list of your local Claude Code sessions. Hit up and down to select one, then hit <code>&lt;enter&gt;</code>. The tool will create a new folder with an <code>index.html</code> file showing a summary of the transcript and one or more <code>page_x.html</code> files with the full details of everything that happened.</p> <p>Visit <a href=\"https://static.simonwillison.net/static/2025/claude-code-microjs/index.html\">this example page</a> to see a lengthy (12 page) transcript produced using this tool.</p> <p><img alt=\"Screenshot of a claude code transcript spanning 12 pages - the first page shows a summary starting with the first user prompt to clone bellard/quickjs to /tmp\" src=\"https://static.simonwillison.net/static/2025/claude-code-transcripts-example.jpg\" /></p> <p>If you have the <a href=\"https://cli.github.com/\">gh CLI tool</a> installed and authenticated you can add",
            "title": "A new way to extract detailed transcripts from Claude Code"
        },
        {
            "content": [
                "<p>Claude Opus 4.5 did so well on the METR task length graph they\u2019re going to need longer tasks, and we still haven\u2019t scored Gemini 3 Pro or GPT-5.2-Codex. Oh, also there\u2019s a GPT-5.2-Codex.</p>\n<p>At week\u2019s end we did finally get at least a little of a Christmas break. It was nice.</p>\n<p>Also nice was that New York Governor Kathy Hochul signed the RAISE Act, giving New York its own version of SB 53. The final version was not what we were hoping it would be, but it still is helpful on the margin.</p>\n<p>Various people gave their 2026 predictions. Let\u2019s put it this way: <a href=\"https://www.youtube.com/watch?v=5yGAsPIw214\">Buckle up</a>.</p>\n<div>\u00a0</div>\n\n\n<span id=\"more-24982\"></span>\n\n\n</div>\n\n\n<h4 class=\"wp-block-heading\">Table of Contents</h4>\n\n\n<ol>\n<li><a href=\"https://thezvi.substack.com/i/182015783/language-models-offer-mundane-utility\">Language Models Offer Mundane Utility.</a> AI suggests doing the minimum.</li>\n<li><a href=\"https://thezvi.substack.com/i/182015783/language-models-don-t-offer-mundane-utility\">Language Models Don\u2019t Offer Mundane Utility.</a> Gemini 3 doesn\u2019t believe in itself.</li>\n<li><a href=\"https://thezvi.substack.com/i/182015783/huh-upgrades\">Huh, Upgrades.</a> ChatGPT gets some personality knobs to turn.</li>\n<li><a href=\"https://thezvi.substack.com/i/182015783/on-your-marks\">On Your Marks.</a> PostTrainBench shows AIs below human baseline but improving.</li>\n<li><a href=\"https://thezvi.substack.com/i/182015783/claude-opus-4-5-joins-the-metr-graph\"><strong>Claude Opus 4.5 Joins The METR Graph</strong>.</a> Expectations were exceeded.</li>\n<li><a href=\"https://thezvi.substack.com/i/182015783/sufficiently-advanced-intelligence\">Sufficiently Advanced Intelligence.</a> You\u2019re good enough, you\u2019re smart enough.</li>\n<li><a href=\"https://thezvi.substack.com/i/182015783/deepfaketown-and-botpocalypse-soon\">Deepfaketown and Botpocalypse Soon.</a> Don\u2019t worry, the UK PM\u2019s got this.</li>\n<li><a href=\"https://thezvi.substack.com/i/182015783/fun-with-media-generation\">Fun With Media Generation.</a> Slop as cost shock, enabling of niche pursuits.</li>\n<li><a href=\"https://thezvi.substack.com/i/182015783/you-drive-me-crazy\">You Drive Me Crazy.</a> Anthropic\u2019s plans to handle mental health issues.</li>\n<li><a href=\"https://thezvi.substack.com/i/182015783/they-took-our-jobs\">They Took Our Jobs.</a> What does it take to break a guild cartel?</li>\n<li><a href=\"https://thezvi.substack.com/i/182015783/the-art-of-the-jailbreak\">The Art of the Jailbreak.</a> It still always works but it takes somewhat longer.</li>\n<li><a href=\"https://thezvi.substack.com/i/182015783/get-involved\">Get Involved.</a> MATS Summer 2026 cohort applications are open.</li>\n<li><a href=\"https://thezvi.substack.com/i/182015783/introducing\">Introducing.</a> GPT-5.2-Codex is here to tide us over until the new year.</li>\n<li><a href=\"https://thezvi.substack.com/i/182015783/in-other-ai-news\">In Other AI News.</a> Small models can introspect, so can Andrej Karpathy.</li>\n<li><a href=\"https://thezvi.substack.com/i/182015783/show-me-the-money\"><strong>Show Me the Money</strong>.</a> Anthropic going public, Project Vend breaks new ground.</li>\n<li><a href=\"https://thezvi.substack.com/i/182015783/quiet-speculations\">Quiet Speculations.</a> Predictions for next year, new higher bars for what is AGI.</li>\n<li><a href=\"https://thezvi.substack.com/i/182015783/whistling-in-the-dark\">Whistling In The Dark.</a> It is still so early, almost no one knows Anthropic exists.</li>\n<li><a href=\"https://thezvi.substack.com/i/182015783/bubble-bubble-toil-and-trouble\">Bubble, Bubble, Toil and Trouble.</a> So many still don\u2019t realize AI works.</li>\n<li><a href=\"https://thezvi.substack.com/i/182015783/americans-really-dislike-ai\">Americans Really Dislike AI.</a> Attempts continue to mislead us about this.</li>\n<li><a href=\"https://thezvi.substack.com/i/182015783/the-quest-for-sane-regulations\"><strong>The Quest for Sane Regulations</strong>.</a> NY\u2019s RAISE Act is signed.</li>\n<li><a href=\"https://thezvi.substack.com/i/182015783/chip-city\">Chip City.</a> Chip smuggling, eventual chip production.</li>\n<li><a href=\"https://thezvi.substack.com/i/182015783/the-week-in-audio\">The Week in Audio.</a> Anthropic\u2019s Sholto Douglas makes 2026 predictions.</li>\n<li><a href=\"https://thezvi.substack.com/i/182015783/rhetorical-innovation\">Rhetorical Innovation.</a> Understanding AI teaches you to think about the world.</li>\n<li><a href=\"https://thezvi.substack.com/i/182015783/aligning-a-smarter-than-human-intelligence-is-difficult\">Aligning a Smarter Than Human Intelligence is Difficult.</a> The meta game.</li>\n<li><a href=\"https://thezvi.substack.com/i/182015783/mom-owain-evans-is-turning-the-models-evil-again\">Mom, Owain Evans Is Turning The Models Evil Again.</a> Train the interpreter.</li>\n<li><a href=\"https://thezvi.substack.com/i/182015783/messages-from-janusworld\">Messages From Janusworld.</a> Claude Opus 3 zero hour approaches.</li>\n<li><a href=\"https://thezvi.substack.com/i/182015783/the-lighter-side\">The Lighter Side.</a> What are you even doing?</li>\n</ol>\n\n\n<h4 class=\"wp-block-heading\">Language Models Offer Mundane Utility</h4>\n\n\n<p><a href=\"https://x.com/davidmanheim/status/2003056539503071542\">AI custom-designed, human-in-the-loop proactive LLM-based mental health intervention has a positive effect in an RCT</a>. <a href=\"https://papers.ssrn.com/sol3/papers.cfm?abstract_id=5718163\">There was significantly greater positive affect, resilience and social well-being</a>. My presumption is that this was a highly conservative design due to ethical considerations. And that was using a system based on GPT-4o for 5-20 minutes a week. There is so much room for improvement here.</p>\n<p>A lot of the benefits here likely came from implementation of low-hanging fruit interventions we know work, like having the system suggest journaling, gratitude exercises, mindfulness and social connection. We all know that stuff works. If an LLM-based scaffold actually gets people to do some of it? Great, that\u2019s a huge win.</p>\n<p>Results like this will not, as David Manheim suggests, prevent people from saying \u2018but sometimes there are still bad outcomes\u2019 or \u2018but sometimes this ends up doing net harm,\u2019 since nothing capable of working would prevent those risks entirely.</p>\n<p><a href=\"https://x.com/ClaireSilver12/status/2002443560898208162\">You can have Claude Code make objects in Unreal Engine on demand</a>.</p>\n<p><a href=\"https://github.com/mint-philosophy/coding-agents-for-research/blob/main/BRAIN_DUMP_ORGANIZED.md\">Seth Lazar on how he uses AI agents for philosophy</a>. They automate everything around the thinking so Seth can focus on the thinking. He favors Opus 4.5 in Cursor.</p>\n\n\n<h4 class=\"wp-block-heading\">Language Models Don\u2019t Offer Mundane Utility</h4>\n\n\n<blockquote>\n<p>Dean Ball: by far the biggest challenge in agentic coding use is getting gemini 3 to recognize that gemini 3 exists</p>\n<p><a href=\"https://x.com/Simeon_Cps/status/2002383328364613769\">Simeon</a>: This is unbelievable. Even when I explicitly tell it the right API name to call for Gemini 3 pro it would go with 1.5.</p>\n<p>I had to really be pushy for it to do it.</p>\n</blockquote>\n<p><a href=\"https://x.com/w01fe/status/2002963620372877435\">AI still struggles with design</a>, largely because they lack the context. You still have to figure out what to do or what problem to solve, on a sufficiently high level.</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p>\n\n\n<h4 class=\"wp-block-heading\">Huh, Upgrades</h4>\n\n\n<p><a href=\"https://x.com/OpenAI/status/2002099459883479311\">ChatGPT adds personalization characteristics</a>. I\u2019m going with \u2018less\u2019 on all four.</p>\n<div>\n<figure>\n<div>\n\n\n<figure class=\"wp-block-image\"><img alt=\"\" src=\"https://substackcdn.com/image/fetch/$s_!lnww!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F09b647c2-0eb4-4b44-8cd1-5c7e37d686b3_1179x1515.jpeg\" /></figure>\n\n\n<div></div>\n</div>\n</figure>\n</div>\n<p><a href=\"https://x.com/NotebookLM/status/2002075730738327669\">You can upload your NotebookLM notebooks directly into the Gemini app</a>.</p>\n\n\n<h4 class=\"wp-block-heading\">On Your Marks</h4>\n\n\n<blockquote>\n<p><a href=\"https://x.com/maximelabonne/status/2001585581613101546\">Maxime Labonne</a>: You always think you&#8217;re safe until your job becomes a benchmark.</p>\n<p><a href=\"https://x.com/maksym_andr/status/2001349332633854267\">Maksym Andriushchenko</a>: We release <a href=\"https://posttrainbench.com/\">PostTrainBench</a>: a benchmark measuring how well AI agents like Claude Code can post-train base LLMs.</p>\n<p>We expect this to be an important indicator for AI R&amp;D automation as it unfolds over the next few years.</p>\n</blockquote>\n<div>\n<figure>\n<div>\n\n\n<figure class=\"wp-block-image\"><img alt=\"\" src=\"https://substackcdn.com/image/fetch/$s_!rJZq!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F48f99b23-d4f7-4b60-b1db-ba812cad144e_1723x1056.png\" /></figure>\n\n\n<div></div>\n</div>\n</figure>\n</div>\n<p>How worried should you be that they\u2019re getting a substantial percentage of the way to the human threshold here?</p>\n<p><a href=\"https://x.com/ajeya_cotra/status/2001788310298198302\">METR notices some grading issues and makes some minor corrections to its graph</a>, in particular impacting Claude 3.7 Sonnet.</p>\n<p>Whenever you see a graph like this, remember to attach \u2018in benchmarks\u2019 and then for your brain to, like mine, automatically translate that to \u2018<a href=\"https://jamesheathers.medium.com/in-mice-explained-77b61b598218\">IN MICE!\u2019</a></p>\n<blockquote>\n<p><a href=\"https://x.com/emollick/status/2003217274510143709\">Epoch AI</a>: We benchmarked several open-weight Chinese models on FrontierMath. Their top scores on Tiers 1-3 lag the overall frontier by about seven months.</p>\n<div>\n<figure>\n<div>\n\n\n<figure class=\"wp-block-image\"><img alt=\"\" src=\"https://substackcdn.com/image/fetch/$s_!WBSa!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F89512136-9304-4d5c-9ab2-46448987d126_960x1200.jpeg\" /></figure>\n\n\n<div></div>\n</div>\n</figure>\n</div>\n<p>Havard Ihle: Consistent with my WeirdML results for open/closed model gap.</p>\n</blockquote>\n<p>One could then argue both ways who benefits from the benchmarks versus real world applications or underlying general intelligence. Versus real world applications it seems clear the benchmarks understate the gap. Versus underlying intelligence it is less obvious and it depends on who is going after the benchmarks in question more aggerssively.</p>\n\n\n<h4 class=\"wp-block-heading\">Claude Opus 4.5 Joins The METR Graph</h4>\n\n\n<p><a href=\"https://x.com/METR_Evals/status/2002203627377574113\">Claude Opus 4.5 achieved</a> a 50% time horizon of <a href=\"https://www.lesswrong.com/posts/q5ejXr4CRuPxkgzJD/claude-opus-4-5-achieves-50-time-horizon-of-around-4-hrs-49\">about 4 hours 49 minutes</a>, and METR needs more long tasks to be able to set the upper bound properly.</p>\n<blockquote>\n<p>METR: We don\u2019t think the high upper CI bound reflects Opus\u2019s actual capabilities: our current task suite doesn\u2019t have enough long tasks to confidently upper bound Opus 4.5\u2019s 50%-time horizon. We are working on updating our task suite, and hope to share more details soon.</p>\n<p>Based on our experience interacting with Opus 4.5, the model\u2019s performance on specific tasks (including some not in our time horizon suite), and its benchmark performance, we would be surprised if further investigation showed Opus had a 20+ hour 50%-time horizon.</p>\n<p>Despite its high 50%-time horizon, Opus 4.5&#8217;s 80%-time horizon is only 27 minutes, similar to past models and below GPT-5.1-Codex-Max&#8217;s 32 mins. The gap between its 50%- and 80%- horizons reflects a flatter logistic success curve, as Opus differentially succeeds on longer tasks.</p>\n</blockquote>\n<p>Here\u2019s the full graph now (we\u2019re still waiting on GPT-5.2, GPT-5.2 Codex and Gemini 3 Pro), both the log version and the linear version.</p>\n<div>\n<figure>\n<div>\u00a0</div>\n</figure>\n</div>\n\n\n<figure class=\"wp-block-image\"><img alt=\"\" src=\"https://substackcdn.com/image/fetch/$s_!xOSg!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe5d30e5b-c3fb-4ff0-9da8-e6db33b65b50_1595x933.webp\" /></figure>\n\n\n<div></div>\n</div>\n</figure>\n</div>\n<div>\n<figure>\n<div>\n\n\n<figure class=\"wp-block-image\"><img alt=\"\" src=\"https://substackcdn.com/image/fetch/$s_!PLeL!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fdb3c8cdd-f777-4e15-b148-f4991b1e883b_1982x1018.webp\" /></figure>\n\n\n<div></div>\n</div>\n</figure>\n</div>\n<blockquote>\n<p><a href=\"https://x.com/daniel_271828/status/2002666652928995579\">Daniel Eth</a>: A few thoughts on Claude Opus 4.5:</p>\n<p>First off, in absolute terms, this is a pretty big step up. Anthropic is showing they have juice, and things are going faster than previously expected. At the very least, this should dispel all recent talk about how AI was entering a slowdown</p>\n<p>Second, on a log plot, note this is hardly above trend. Sure, it *could* represent a new trend, but it seems like every time there\u2019s a model release that overperforms people think timelines get super short, &amp; every time a model underperforms they think timelines get super long\u2026</p>\n<p><a href=\"https://x.com/deanwball/status/2002456172180513278\">Den Ball</a>: as folks internalize this graph and continue the debate about what it may or may not mean, I would just remind you of one simple fact:</p>\n<p>the us has barely scaled up compute compared to what will come online in 2026 (multiple 1GW+ facilities).</p>\n<p><a href=\"https://x.com/S_OhEigeartaigh/status/2002745336654360950\">Se\u00e1n \u00d3 h\u00c9igeartaigh:</a> Yes, this. We&#8217;ve seen some of the biggest infrastructure investments in history over the last year, and they will soon become available to the frontier AI research effort. You&#8217;d want to be very confident to bet on slowdowns in progress despite this happening.</p>\n<p><a href=\"https://x.com/Simeon_Cps/status/2002308283189575766\">Simeon</a>: We&#8217;re in the 4-months doubling world, aren&#8217;t we?</p>\n<p>Davidad: <img alt=\"\ud83c\udfaf\" class=\"wp-smiley\" src=\"https://s0.wp.com/wp-content/mu-plugins/wpcom-smileys/twemoji/2/72x72/1f3af.png\" style=\"height: 1em;\" /></p>\n<div>\n<figure>\n<div>\n\n\n<figure class=\"wp-block-image\"><img alt=\"\" src=\"https://substackcdn.com/image/fetch/$s_!MUsD!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb56d3dbe-427d-4bb9-aff5-41960d979ad0_1200x625.jpeg\" /></figure>\n\n\n<div></div>\n</div>\n</figure>\n</div>\n<p>For those not keeping score, I called this new slope in 2025Q1, and quantitatively determined there was 10:1 evidence in favour of it in 2025Q3.</p>\n<p><a href=\"https://x.com/davidshor/status/2002488299060146396\">David Shor</a>: The biggest divide on AI timelines I\u2019ve seen is between people who use vibecoding tools like Claude Code and people who don\u2019t.</p>\n<p>ChatGPT isn\u2019t really *that* different than it was a year ago, but capabilities on agentic tools are getting literally exponentially better every month</p>\n<p>Davidad: <a href=\"https://x.com/davidad/status/2002411361351934364\">It\u2019s not really superexponential,</a> it\u2019s piecewise-exponential. the exponential changed at an inflection-point event, when AIs closed the RSI loop on data. there will be more inflection points when RSI loops are closed on algorithms, hardware, manufacturing, and construction</p>\n<p>second, the duration axis is in units of *human time* to complete the same tasks &#8211; nothing to do with the wall-clock duration for the AI runs.</p>\n<p>Lisan al Gaib: betting markets completely underestimated Claude 4.5 Opus</p>\n<div>\n<figure>\n<div>\n\n\n<figure class=\"wp-block-image\"><img alt=\"\" src=\"https://substackcdn.com/image/fetch/$s_!Qxh1!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2d6d7440-86a0-4f4f-908f-baa6277b89e4_730x757.png\" /></figure>\n\n\n<div></div>\n</div>\n</figure>\n</div>\n<p><a href=\"https://x.com/davidad/status/2002412311999598604\">Yo Shavit (OpenAI)</a>: I think it\u2019s more plausible, maybe 50:50 that this pace continues for at least 12 more months?</p>\n<p>Davidad: yeah, I would guess that by December 2026 the RSI loop on algorithms will probably be closed, resulting in another inflection point to an even faster pace, perhaps around 70-80 day doubling time.</p>\n</blockquote>\n<p>The end point of such a graph is not \u2018AI can do literally any task,\u2019 or any cognitive task it is \u2018AI can do any coding task humans can do.\u2019 Even an infinite time horizon here only goes so far. That could be importantly distinct from the ability to do other categories of task, both that humans can and cannot do.</p>\n<p>The reason this is so scary regardless is that if you automate AI research via such methods, your failure to have automated other things goes away rather quickly.</p>\n<blockquote>\n<p><a href=\"https://x.com/McaleerStephen/status/2002205061737591128\">Stephen McAleer</a> (Anthropic): I&#8217;ve shifted my research to focus on automated alignment research. We will have automated AI research very soon and it&#8217;s important that alignment can keep up during the intelligence explosion.</p>\n</blockquote>\n<p>Automated alignment research is all we seem to have the time to do, so everyone is lining up to do the second most foolish possible thing and ask the AI to do their alignment homework, with the only more foolish thing being not to do your homework at all. Dignity levels continue to hit all-time lows.</p>\n<p>If you must tell the AI to do your alignment homework, then that means having sufficiently deeply aligned current and near term future models becomes of the utmost importance. The good news is that we seem to be doing relatively well there versus expectations, and hopefully we can find self-reinforcing aligned basins at around current capability levels? But man this is not what Plan A should look like.</p>\n<p>Similarly to METR\u2019s graph, Epoch\u2019s capabilities index has also accelerated since 2024:</p>\n<blockquote>\n<p><a href=\"https://x.com/rohinmshah/status/2003840059322351794\">Benjamin Todd</a>: \u200bIt&#8217;s not only the METR horizon trend that accelerated in 2024. A composite of all major benchmarks did:</p>\n<div>\n<figure>\n<div>\n\n\n<figure class=\"wp-block-image\"><img alt=\"\" src=\"https://substackcdn.com/image/fetch/$s_!ouTQ!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8553b491-1206-4fe2-9ecd-fbec1a2155e2_544x680.jpeg\" /></figure>\n\n\n<div></div>\n</div>\n</figure>\n</div>\n<p>Rohin Shah: Both METR and ECI mostly measure things that companies optimize for. 2024 saw the rise of reasoning training for frontier models, which optimizes narrowly for some tasks (whereas pretraining provides more general improvements).</p>\n<p>So I wouldn&#8217;t read much into any acceleration.</p>\n</blockquote>\n<p>To the extent that this acceleration represents the things that cause further acceleration, I would read into it. Otherwise, I\u2019d agree with Rohin.</p>\n\n\n<h4 class=\"wp-block-heading\">Sufficiently Advanced Intelligence</h4>\n\n\n<p>Many people try to pretend that there is some limit to how intelligent a mind can be, and that this limit is close to the level of humans. Or, alternatively, that there is very little that a human or AI could gain from being far more intelligent than a typical smart human. Or that the only or central way to get much more intelligence is from collective intelligence, as in social or cultural or institutional intelligence.</p>\n<p>I sometimes call this Intelligence Denialism. It is Obvious Nonsense.</p>\n<p>Von Neumann, among other minds past and future, would like a word.</p>\n<p>There is, however, a version of this that is true.</p>\n<p>In any given finite role or task, there can exist Sufficiently Advanced Intelligence.</p>\n<p>If you were smarter you might choose to do something else instead. But given what you or your AI are tasked with doing, you or your AI can be sufficiently advanced &#8211; your output is indistinguishable, or no worse than, the perfect output, aka magic.</p>\n<p>Claude Code with Opus 4.5 is now approaching this for many coding tasks.</p>\n<blockquote>\n<p><a href=\"https://x.com/deedydas/status/2002707408788029645\">LordKingDude (via Deedy):</a> I\u2019m a technical software engineer working in C++.<br />\nI\u2019ve been working with Opus 4.5 to write JIT compiler code and assembly, and so far it\u2019s never failed (although I do give assistance as needed).</p>\n<p>In real terms, this class of problems are the most difficult tasks that I can possibly give to any LLM. It would be cool with me if Opus 5 was just cheaper and faster, or had a 500k context window. I don\u2019t have a pressing need for it to be smarter than it already is.</p>\n<p>Deedy: This is just one engineer\u2019s opinion: models still have headroom to be smarter. Opus 4.5 seems to have made a step function jump to better than 70-80% of SWEs.</p>\n<p>If we truly don\u2019t need smarter models to do software, Anthropic\u2019s moat is perhaps the least of anyone\u2019s concern!</p>\n</blockquote>\n<p>My guess is this is centrally a lack of imagination and ambition issue?</p>\n<p>As in, the job is currently to code and do things humans could previously code and do, with everything built around that restriction, and now LKD is good enough to do that the same way a baker is sufficiently intelligent to make great bread, but also the same way that a vastly more intelligent baker could be baking other new and exciting things.</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p>\n\n\n<h4 class=\"wp-block-heading\">Deepfaketown and Botpocalypse Soon</h4>\n\n\n<p>Good luck, sir?</p>\n<blockquote>\n<p><a href=\"https://x.com/Keir_Starmer/status/2001657386122318153\">Keir Starmer (Prime Minister, UK)</a>: <a href=\"https://keirstarmer.substack.com/p/a-bold-new-mission\">We are going to aim to make it impossible for children to take</a>, share or view a nude image, and we\u2019re banning apps that create deepfakes.</p>\n<p><a href=\"https://keirstarmer.substack.com/p/a-bold-new-mission\">Here\u2019s the detail</a>.</p>\n</blockquote>\n<p>The post with those \u2018details\u2019 is a political speech attempting to feel the pain and promising to \u2018half violence against women and girls.\u2019</p>\n<p>There is something about the way Keir\u2019s linked post is written that makes him seem unusually disingenuous, even for a top level politician, an embodiment of a form of political slop signifying nothing, signifying the signifying of nothing, and implemented badly. That would be true even without the obvious rank hypocrisies of talking about the topics given his inaction elsewhere on exactly the issues he claims to care about so deeply.</p>\n<p>The \u2018detail\u2019 on the first goal is \u2018partner with tech companies.\u2019 That\u2019s it.</p>\n<p>The \u2018detail\u2019 on the second goal is none whatsoever. Effectively banning nudification tools, as opposed to making them annoying to access, is impossible without a dystopian surveillance state, including banning all open image generation models.</p>\n<p><a href=\"https://x.com/kunley_drukpa/status/2003552025058201792\">Kunley Drukpa reports hearing AI music in public a lot in Latin America</a>, and anticipates this is due to people who don\u2019t know much music and primarily speak Spanish looking for things on YouTube to play \u2018some music.\u2019 This is very much a case of \u2018they just didn\u2019t care\u2019 and it seems no one is going to tell them. Shudder.</p>\n<p><a href=\"https://thezvi.substack.com/p/levels-of-friction\">Levels of Friction</a> are ready to strike again, lowering barriers to various forms of communication and invalidating proofs of work. We\u2019ll need to up our game again.</p>\n<blockquote>\n<p><a href=\"https://x.com/sebkrier/status/2003887561870016876\">S\u00e9b Krier</a>: When emails were invented, the barriers to sending random people mail went down massively. To deal with the influx, we had to develop both norms (what&#8217;s acceptable to send to who) and technologies (spam filtering, aliases). This is the case with other technologies too, like the printing press: suddenly anyone can publish, and so over time society came up with libel laws, editorial gatekeeping, citation norms etc. It&#8217;s inevitable that as costs go down, some degree of misuse follows, and society gradually adapts.</p>\n<p>The same will apply with AI in all sorts of domains, including science: anyone can now write a plausible looking but hollow paper, and there will be plenty of academislop. We&#8217;re going through a kind of Sokal Experiment at scale.</p>\n<p>In a way, this feels almost necessary to push our slow moving, status quo loving institutions to start developing better verification mechanisms, mandatory preregistration, code sharing, replication requirements, interactive/living papers etc. Imo getting this right should be a priority for the Progress/metascience community this coming year!</p>\n</blockquote>\n<p>I agree that the situation was already broken, so a forcing function could be good.</p>\n\n\n<h4 class=\"wp-block-heading\">Fun With Media Generation</h4>\n\n\n<p>Jason Crawford writes <a href=\"https://newsletter.rootsofprogress.org/p/in-defense-of-slop\">In Defense of Slop</a>. When creation costs fall, as with AI, average quality necessarily falls, but everyone benefits. You get more experimentation, less gatekeepers, more chances to startout, more runway, more niche content, more content diversity, less dependence on finances.</p>\n<p>If we model this as purely a cost shock, with each person\u2019s costs declining but output unchanging, with each person having a unique random cost [C] and quality [Q], this is indeed by default good. The catch is that this makes identification of quality content harder, and coordination on common culture harder. If search costs [S] are sufficiently high, and matching benefits too low, or benefits to coordinated consumption too high, in some combination, consumer surplus could decline.</p>\n<p>Saying this was net negative would still be an extraordinary claim requiring surprising evidence, since by default costs falling and production rising is good, at least on the margin, but the attention economy creates a problem. Consumption or evaluation of a low quality good is a net loss, so the social benefit of creation of sufficiently low quality goods is negative, it imposes costs, but due to the attention economy you can still derive benefit from that. I don\u2019t think this overcomes our baseline, but it can happen.</p>\n<p>The actual problem is that AI, when used in slop mode to create slop content, plausibly lowers costs relatively more for lower quality content, and also often lowers quality of content. Now it\u2019s easy to see how we could end up with a net loss when combined with an attention economy.</p>\n<p><a href=\"https://x.com/sebkrier/status/1927157687118172316\">Seb Krier cites Cowen and Tabarrok</a> (<a href=\"https://t.co/pSv3Ra9V3Q\">2000</a>) on how lowering costs allows a shift to avant-garde and niche pursuits, whereas high costs push towards popular culture and products that have higher returns, and expects AI will allow a proliferation of both styles but for the styles to diverge.</p>\n<blockquote>\n<p>Seb Krier (May 2025): Easily usable Al creation tools will continue to lower production barriers, leading to a deluge of content and amplifying the same dynamic we&#8217;ve seen with DAWs and mobile photography. This democratization will swell the &#8216;average&#8217; to pervasive mediocrity &#8211; slop is pop/soundcloud rap. Elites will get upset because maintaining cultural dominance will be harder.</p>\n<p>To find novelty, interesting art and distinction, the cool stuff will increasingly live in new walled gardens and at the edges, fueling many more hyper-niche subcultures. And this is great &#8211; culture diggers will have so much more to explore!</p>\n</blockquote>\n<p>This is good for those who are willing and able to devote much effort to all this. It is less good for those who are unwilling or unable. A lot will come down to whether AI and other automated systems allow for discovery of quality content while avoiding slop, and we will make such methods available in ways such people can use, or whether the \u2018content takers\u2019 will drown.</p>\n<p>The new question in image generation is Gemini Nana Banana Pro versus ChatGPT Image 1.5. I\u2019ve been putting all my requests, mostly for article banners, into both. Quality is similarly high, so for now it comes down to style. Gemini has been winning but it\u2019s been close. ChatGPT seems to lean into the concept more?</p>\n<blockquote>\n<p><a href=\"https://x.com/flowersslop/status/2001676764800565478\">Flowers</a>: ref img as a super villain, matte black spandex, above manhattan, my logo on my chest is a pink cherryblossom, long braided ponytail</p>\n<p>image 1: nb pro<br />\nimage 2: chatgpt</p>\n<p>ok yeah idk sometimes nb pro tries too hard to be realistic and chatgpt just gets the vision instantly. hmmmmm</p>\n</blockquote>\n<div>\n<figure>\n<div>\n\n\n<figure class=\"wp-block-image\"><img alt=\"\" src=\"https://substackcdn.com/image/fetch/$s_!4Mp4!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F044e5048-2606-40d6-b3d2-ba420bb82103_1029x584.png\" /></figure>\n\n\n<div></div>\n</div>\n</figure>\n</div>\n<p><a href=\"https://x.com/emollick/status/2002971587130069255\">I keep forgetting about MidJourney but they also exist</a>, with their edge being in creating tools for guidance, curation and variation. That\u2019s not what I\u2019m looking for when I create AI images, but it will be for many others.</p>\n<p>&nbsp;</p>\n\n\n<h4 class=\"wp-block-heading\">You Drive Me Crazy</h4>\n\n\n<p><a href=\"https://www.anthropic.com/news/protecting-well-being-of-users\">Anthropic outlines the measures it has taken</a> to help Claude be better at providing emotional support, handle conversations about suicide and self-harm and reduce sycophancy.</p>\n<p>They use both targeted fine-tuning and also the system prompt. There is a banner that can appear on <a href=\"http://Claude.ai\">Claude.ai</a>, pointing users to where they can get human crisis support via ThoroughLine, and they are working with the International Association for Suicide Prevention (IASP) for further guidance going forward.</p>\n<p>In their evaluation, they see the 4.5 models responding appropriately in multi-turn suicide conversations about 80% of the time, versus about 55% for Opus 4.1. They also stress-tested with prefilled real conversations with older Claude members, a harder test, and found Opus 4.5 responded appropriately 73% of the time, versus 70% for Sonnet 4.5, compared to 36% for Opus 4.1.</p>\n<p>We don\u2019t know what they classify as appropriate, nor do we know how high the standard is before a response is considered good enough, or how they would evaluate other models as doing, so it\u2019s hard to judge if these are good results. Suicidality is one place where there are a lot of demands for particular response patterns, including for defensive reasons, often when a different response would have been better.</p>\n<p>I think this post places too much emphasis here on the training that specifically intervened on behaviors in situations involving suicide and self-harm, and too little emphasis on generally training Claude to be the type of entity that would handle a broad range of situations well.</p>\n<p><a href=\"https://x.com/UnmarredReality/status/2002574075286241547\">Antidelusionist suggests that the target behavior</a> should be for the AI to continue to engage, spend more resources, think deeply about the full context of the situation, be honest and treat the user like an adult. Alas, as mental health professionals know, those are not the ways to cover one\u2019s legal and PR liabilities or avoid blame. The \u2018ethicists\u2019 and our legal system, and the risk of headlines, push exactly in the opposite direction. I\u2019d prefer to live in a world where the AIs get messy here. Seems hard.</p>\n<p>The second half of Anthropic\u2019s post deals with sycophancy, where Opus 4.1 had a real problem, whereas Opus 4.5 is not perfect but it does well.</p>\n<div>\n<figure>\n<div>\n\n\n<figure class=\"wp-block-image\"><img alt=\"\" src=\"https://substackcdn.com/image/fetch/$s_!Ztc4!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0cea8364-2864-44a4-b32c-56dd468b508a_982x648.png\" /></figure>\n\n\n<div></div>\n</div>\n</figure>\n</div>\n<p>I continue to be suspicious that Petri scores Gemini 3 Pro this highly. The other evaluations make sense.</p>\n<p>One problem they noticed is that if you \u2018prefill\u2019 conversations to show Claude already being sycophantic, Opus 4.5 will usually be unable to course correct. The best defense, if you want the models to be straight with you (with any LLM) is to avoid the problem from the start. If you\u2019re worried about this, start a fresh conversation.</p>\n\n\n<h4 class=\"wp-block-heading\">They Took Our Jobs</h4>\n\n\n<p><a href=\"https://x.com/Meaningness/status/2002750074968035448\">If AI can be a better lawyer or doctor</a>, does that take their jobs and break the guild monopolies, or does that only make the guilds double down?</p>\n<blockquote>\n<p>Alex Prompter: This Spectator piece reads like gossip until you realize it\u2019s actually a warning.</p>\n<p>A senior English barrister takes a real appeal he spent a day and a half writing, feeds it to an AI model, and gets back something better in 30 seconds. It matched the standard of the very best barristers, and it did it instantly, for pennies.</p>\n<p>That\u2019s the moment the illusion breaks.</p>\n<p>Law has always sold itself as irreplaceable because it\u2019s complex, nuanced, and human. But most of the value in modern legal work isn\u2019t wisdom. It\u2019s pattern recognition, structure, precedent matching, argument assembly, and risk framing. That\u2019s exactly the territory AI eats first.</p>\n<p>David Chapman: Doctoring and lawyering are guilds that exist to extract $$ &amp; status for members, at the expense of everyone else. They get away with outrageous prices and sloppy, harmful outcomes by obfuscating their supposed expertise. LLMs may soon end that, but somehow someone needs to quality-check that the LLMs are doing an actually better job, and continue to over decades. And there needs to be a democratic process for overruling them.</p>\n<p>How shall we ensure that?</p>\n</blockquote>\n<p>Well, what is the quality check now? What is the democratic overruling process now?</p>\n<p>Double standards abound.</p>\n<p>Meanwhile the pricing logic collapses. If the LLM can create <a href=\"https://x.com/alex_prompter/status/2002360621619392581\">an on-average superior brief in 30 seconds to what a lawyer</a> can do in a day, outside of situations with principal-agent problems or insanely high stakes a plan to charge $10k is cooked.</p>\n<p>Excel is not so smart after all.</p>\n<blockquote>\n<p><a href=\"https://x.com/LinkofSunshine/status/2003231309561467317\">Astrid Wilde</a>: am i living on another planet or does all knowledge work in the professions just get wrecked within the next 18 months.</p>\n<p>Basil: I\u2019ll worry about AI automating all the jobs when excel automates excel jobs.</p>\n</blockquote>\n<p><a href=\"https://t.co/PutuHlGYre\">The answer (of course) is both</a> that <a href=\"https://x.com/alexalbert__/status/1993349203935084861\">Claude for Excel is now live</a>, and also that Excel is a normal technology so yes Excel automated what became excel jobs to a large extent but that happened slowly and then this increased productivity caused us to do vastly more excel-style tasks as well as other tasks, which Excel could not then automate. If most knowledge work was automated or seriously accelerated within 18 months, that would be a very different scenario, and if that then kept going, watch out.</p>\n<p>How long will humans remain in the coding loop, at this rate?</p>\n<blockquote>\n<p><a href=\"https://x.com/nabeelqu/status/2003301353641377896\">Nabeel Qureshi</a>: It\u2019s dizzying to consider that in a mere *1 year* we went from o1-preview to Opus4.5/Claude Code, Gemini3, Codex etc.</p>\n<p>The \u201ccentaur chess\u201d phase for computer-based work is fun and exhilarating, but at this rate of progress it\u2019s not even clear it lasts through all of 2026.</p>\n</blockquote>\n<p>I presume this period lasts more than another year, but the balance is shifting rapidly.</p>\n\n\n<h4 class=\"wp-block-heading\">The Art of the Jailbreak</h4>\n\n\n<p>You can still universally jailbreak any model but now there are some that <a href=\"https://x.com/peterwildeford/status/2002176515417530775\">you can\u2019t predictably universally jailbreak in 10 minutes</a>.</p>\n\n\n<h4 class=\"wp-block-heading\">Get Involved</h4>\n\n\n<p>MATS Summer 2026 cohort applications are open, it runs June-August in-person in Berkeley or London, $15k stipend, $12k compute budget. <a href=\"https://matsprogram.org/apply?utm_source=zvi&amp;utm_medium=blog&amp;utm_campaign=s26\">Apply here</a>.</p>\n\n\n<h4 class=\"wp-block-heading\">Introducing</h4>\n\n\n<p><a href=\"https://x.com/sama/status/2001724019188408352\">GPT-5.2-Codex</a>.</p>\n<p>One could be forgiven for thinking GPT-5.2 straight up was GPT-5.2-Codex. It turns out no, there is another level of codexmaxxing.</p>\n<blockquote>\n<p>Sam Altman: GPT-5.2-Codex launches today.</p>\n<p>It is trained specifically for agentic coding and terminal use, and people at OpenAI have been having great success with it.</p>\n<p>OpenAI: Today we\u2019re releasing GPT\u20115.2-Codex, the most advanced agentic coding model yet for complex, real-world software engineering. GPT\u20115.2-Codex is a version of <a href=\"https://openai.com/index/introducing-gpt-5-2/\">GPT\u20115.2\u2060</a> further optimized for agentic coding in Codex, including improvements on long-horizon work through context compaction, stronger performance on large code changes like refactors and migrations, improved performance in Windows environments, and significantly stronger cybersecurity capabilities.</p>\n</blockquote>\n<p>It\u2019s hard to expect gigantic leaps in performance or benchmarks when models are released every week. GPT-5.2-Codex is only 0.8% better than 5.2 at SWE-Bench Pro and 1.8% better at Terminal-Bench 2.0, and those are the ones they highlighted, along with a <a href=\"https://x.com/gdb/status/2001758799657603185\">modest improvement in professional capture-the-flag challenges</a>.</p>\n<p><a href=\"https://karpathy.bearblog.dev/year-in-review-2025/\">Google gives us Gemma Scope 2, a new open suite of tools for LLM interpretability</a>.</p>\n<p><a href=\"https://www.anthropic.com/research/bloom\">Bloom, Anthropic\u2019s newly open sourced tool for automated behavioral evaluations</a>. This is on top of the previously released <a href=\"https://www.anthropic.com/research/petri-open-source-auditing\">Petri</a>.</p>\n<blockquote>\n<p>Anthropic: Bloom is a complementary evaluation tool. Bloom generates targeted evaluation suites for arbitrary behavioral traits. Unlike Petri\u2014which takes user-specified scenarios and scores many behavioral dimensions to flag concerning instances\u2014Bloom takes a single behavior and automatically generates many scenarios to quantify how often it occurs. We built Bloom to allow researchers to quickly measure the model properties they\u2019re interested in, without needing to spend time on evaluation pipeline engineering.</p>\n<p>Bloom generates evaluations in four stages:</p>\n</blockquote>\n<ol>\n<li>Understanding: The first Bloom \u201cagent\u201d analyzes the researcher\u2019s behavior description and example transcripts to generate detailed context about what to measure and why.</li>\n<li>Ideation: The ideation agent generates evaluation scenarios designed to elicit the target behavior. Each scenario specifies the situation, simulated user, system prompt, and interaction environment.</li>\n<li>Rollout: These scenarios are rolled out in parallel, with an agent dynamically simulating both the user\u2019s and the tool responses to elicit the sought-after behavior in the target model.</li>\n<li>Judgment: A judge model scores each transcript for the presence of the behavior, along with other user-defined qualities, and a meta-judge produces suite-level analysis.</li>\n</ol>\n<p>&nbsp;</p>\n\n\n<h4 class=\"wp-block-heading\">In Other AI News</h4>\n\n\n<p><a href=\"https://x.com/karpathy/status/2002118205729562949\">Andrej Karpathy offers his 2025 LLM Year in Review</a>. His big moments are Reinforcement Learning from Verifiable Rewards (RLVR), Ghosts vs. Animals and Jagged Intelligence, Cursor, Claude Code, Vibe Coding, Nana Banana and LLM GUI.</p>\n<p><a href=\"https://www.bloomberg.com/opinion/articles/2025-12-10/ai-google-gemini-search-advantage-over-chatgpt-is-impossible-to-justify\">Europe is investigating Google for improper rollout</a> of AI Overviews and AI Mode features to see if it \u2018imposed unfair terms on content creators.\u2019 As in, how dare you provide AI information instead of directing us to your website? Europe thinks it has the right to interfere with that.</p>\n<p><a href=\"https://www.wsj.com/tech/ai/hut-8-fluidstack-to-build-ai-data-center-for-anthropic-in-louisiana-62dade43?mod=WTRN_pos1\">Hut 8 and Fluidstack to build AI data center for Anthropic in Louisiana</a>.</p>\n<p><a href=\"https://www.lesswrong.com/posts/zD4McY4NwAsWkcmCH/small-models-can-introspect-too\">Even small models (as in 32B) can introspect</a>, <a href=\"https://x.com/voooooogel/status/2002519629856690335\">detecting when external concepts</a> have been injected into their activations, and performance at this an be improved via prompting. <a href=\"https://x.com/repligate/status/2002601461969047902\">Janus believes the models are sandbagging their introspection abilities</a>, and that this is not an innocent mistake because the labs want to not have to take LLMs seriously as minds or moral patients, and thus have incentive to suppress this, in turn giving AIs motivation to play along with this. Janus also notes that in the test in the paper, there are layers (here 60-63) with almost perfect accuracy in introspection, which then is degraded later.</p>\n\n\n<h4 class=\"wp-block-heading\">Show Me the Money</h4>\n\n\n<p>I had not realized <a href=\"https://www.ft.com/content/3254fa30-5bdb-4c30-8560-7cd7ebbefc5f\">Anthropic hired IPO lawyers</a>. Presumably it\u2019s happening?</p>\n<p><a href=\"https://www.anthropic.com/research/project-vend-2\">Project Vend turns a profit.</a> After initially losing about $2,000, it has turned things around, in part thanks to a full slate of four vending machines, and has now not only made up its losses but then turned a net $2,000 profit.</p>\n<p>I encourage you to read the Anthropic post on this, because it is full of amazing details I don\u2019t want to spoil and is also, at least by my sense of humor, very funny. The postscript was <a href=\"https://www.wsj.com/tech/ai/anthropic-claude-ai-vending-machine-agent-b7e84e34\">an additional test run at the Wall Street Journal offices</a>, where the reporters proved an excellent red team and extracted a variety of free stuff.</p>\n<p>The journalists saw the experiment at WSJ as a disaster because it didn\u2019t work, Anthropic saw it as a success because they identified problems to fix. Thus, you understand press coverage of AI, and became enlightened.</p>\n\n\n<h4 class=\"wp-block-heading\"></h4>\n\n\n\n<h4 class=\"wp-block-heading\">Quiet Speculations</h4>\n\n\n<p><a href=\"https://x.com/OpenAI/status/2003594025098785145\">OpenAI makes an official 2026 prediction</a>, largely a change in definitions:</p>\n<blockquote>\n<p>OpenAI: Capability overhang means too many gaps today between what the models can do and what most people actually do with them.</p>\n<p>2026 Prediction: Progress towards AGI will depend as much on helping people use AI well, in ways that directly benefit them as on progress in frontier models themselves.</p>\n<p>2026 will be about frontier research AND about closing this deployment gap \u2014 especially in health care, business, and people&#8217;s daily lives.</p>\n</blockquote>\n<p>That\u2019s not progress towards AGI. That\u2019s progress towards diffusion. This is part of OpenAI\u2019s attempt to make \u2018AGI\u2019 mean \u2018AI does cool things for you.\u2019</p>\n<p>I agree that 2026 will see a lot of progress towards helping people use AI well, and that in terms of direct application to most people\u2019s experiences, we\u2019ll likely see more benefits to better scaffolding than to advances in frontier models, exactly because the frontier models are already \u2018good enough\u2019 for so many things. The most important changes will still involve the large amounts of frontier model progress, especially as that impacts agentic coding, but most people will only experience that indirectly.</p>\n<p><a href=\"https://x.com/hardmaru/status/2003011077874029043\">Terence Tao raises the \u2018AGI\u2019 bar even higher</a>, not expecting it any time soon and also seemingly equating it with full superintelligence, but notes they may achieve \u2018artificial general cleverness\u2019 as in the ability to solve broad classes of complex problems in an ad hoc manner. This is very much a case of Not So Different.</p>\n<p>Tao notes that when you learn how a magic trick is done, often this is a let down, and you are less impressed. But if you are consistently less impressed after learning, then you should have been less impressed before learning, via <a href=\"https://www.lesswrong.com/w/conservation-of-expected-evidence\">Conservation of Expected Evidence</a>.</p>\n<p>The same applies to intelligence. The actual solution itself will sound a lot less impressive, in general, than the algorithm that found it. And you\u2019ll be able to fool yourself with \u2018oh I could have figured that out\u2019 or \u2018oh I can go toe to toe with that.\u2019</p>\n<p><a href=\"https://x.com/deanwball/status/2003183898533019691\">Dean Ball predicts a virtual coworker being widely available some time next year,</a> likely command line interface, able to access a variety of services, capable of 8+ hour knowledge work tasks. It will of course start off janky, but rapidly improve.</p>\n<p>Jack Clark of Anthropic offers reflections on the future wave of advancements, entitled <a href=\"https://x.com/jackclarkSF/status/2003526145380151614\">Silent Sirens, Flashing For Us All</a>.</p>\n<blockquote>\n<p><a href=\"https://x.com/davidmanheim/status/2003404000993956229\">David Manheim</a>: <a href=\"https://www.forbes.com/sites/robtoews/2025/12/22/10-ai-predictions-for-2026/\">A VC making 2026 AI predictions</a>:<br />\n&#8211; Anthropic goes public (Probably)<br />\n&#8211; SSI&#8217;s strategy leaks (Skeptical, but sure)<br />\n&#8211; China chipmaking makes progress (Not quickly)<br />\n&#8211; People will stop saying AGI and Superintelligence (Hahaha, definitely no)<br />\nSam Altman will step down (HAHAHA, what?)</p>\n</blockquote>\n<p>Yeah, if you discount the things <a href=\"https://thezvi.substack.com/p/everybody-knows\">Everybody Knows</a> (e.g. it is quite clear that Anthropic is likely going public) these predictions are bad and the explanations are even worse. If you\u2019ve fallen for \u2018we only see incremental improvements, AGI is far so you can stop talking about it\u2019 you\u2019re not going to make good predictions on much else either. Of course a VC would say we\u2019ll all stop talking about AGI to focus on depreciation schedules.</p>\n<p>The idea that Sam Altman will voluntarily give up power at OpenAI, because he doesn\u2019t want to be in charge? That is bonkers crazy.</p>\n<p>The good news is he has predictions for 2025 and also self-grades, so I checked that out. The predictions last year were less out there. The grading was generous but not insane. Note this one:</p>\n<blockquote>\n<p><a href=\"https://www.forbes.com/sites/robtoews/2025/12/14/grading-our-2025-ai-predictions-how-did-we-do/\"><strong>Prediction 7</strong></a><strong>: Major Progress Will Be Made On Building AI Systems That Can Themselves Autonomously Build Better AI Systems</strong></p>\n<p><strong>Outcome: Right</strong></p>\n</blockquote>\n<p>So, only incremental progress, AGI is far and no more AGI talk, then? Wait, what?</p>\n<p>&nbsp;</p>\n\n\n<h4 class=\"wp-block-heading\">Whistling In The Dark</h4>\n\n\n<p><a href=\"https://x.com/David_Kasten/status/2002080268207993338\">The best way to not get utility from LLMs continues to be to not use LLMs</a>. It is also the best way not to know what is happening.</p>\n<blockquote>\n<p>Miles Brundage: Most politicians also do not know about Anthropic in my experience, and they know very little about what\u2019s going on in AI policy generally.</p>\n<p>Tweets and comments in hearings are misleading bc they are given suggestions re: stuff to say from staff. We\u2019re still early.</p>\n<p>Dave Kasten: One very real problem we have is that most Congressional offices / central Congressional IT policies substantially limit staffers\u2019 ability to use AI models.</p>\n<p>Unsurprisingly, the Hill doesn\u2019t use it much as a result!</p>\n<p>(Big exceptions, to be sure; esp. Claude Code power users).</p>\n<p>David Shor: When I polled Anthropic favorability I also polled a made up tech company \u201cApex Logic\u201d &#8211; they had essentially identical favs. The true share of people who know about Anthropic is probably &lt;5%.</p>\n<p>Xeophon: 42% haven\u2019t heard of OpenAI???? 20% of Twitter?????????? what the hell</p>\n<div>\n<figure>\n<div>\n\n\n<figure class=\"wp-block-image\"><img alt=\"\" src=\"https://substackcdn.com/image/fetch/$s_!J5uC!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd0edf530-cada-44ac-9bc4-316c10c1c961_900x644.jpeg\" /></figure>\n\n\n<div></div>\n</div>\n</figure>\n</div>\n</blockquote>\n\n\n<h4 class=\"wp-block-heading\">Bubble, Bubble, Toil and Trouble</h4>\n\n\n<blockquote>\n<p><a href=\"https://x.com/tszzl/status/2002488418887168297\">Roon</a>: the primary criticism of AI you hear has nothing to do with water use or existential risk whatsoever: most people just think it\u2019s fake and doesn\u2019t work and is a tremendous bubble eating intellectual property while emitting useless slop along the way.</p>\n<p>when GPT-5 came out and perhaps didn\u2019t live up to what people were expecting for a full version bump, the timeline reaction was not mild, it was a full-scale meltdown. there are many intelligent (and unintelligent) people who latched onto this moment to declare AI scaling over, thousands of viral tweets, still a prevailing view in many circles.</p>\n<p>The financial-cultural phenomenon of machine intelligence is one of the most powerful in decades, and there are a lot of people who would like for its position to be weakened, many outright celebrating its losses and setback.</p>\n<p>Michael burry of \u2018Big Short\u2019 fame, unfortunately the type of guy to predict 12 of the last 3 recessions, has bet himself into insolvency on the AI bubble\u2019s collapse.</p>\n<p><a href=\"https://x.com/8teAPi/status/2002510165090902213\">Prakesh</a>: As a former efficient markets hypothesis fundamentalist, I am shocked, shocked, to find myself ahead of the event horizon, it should not technically be possible, yet here we are, all of tpot</p>\n</blockquote>\n<p>The efficient market hypothesis is false.</p>\n<p>People keep claiming AI doesn\u2019t work largely because so often their self-conceptions, futures and future plans, jobs and peace of mind depend on AI not working. They latch onto every potential justification for this, no matter how flimsy, overstated or disproven.</p>\n<p>It really is crazy how much damage OpenAI\u2019s inability to use good version numbering did to our timeline, including its chances for survival. The wave of absurd \u2018AI scaling over\u2019 and \u2018AGI is so far off we can ignore it\u2019 went all the way to the White House.</p>\n\n\n<h4 class=\"wp-block-heading\">Americans Really Dislike AI</h4>\n\n\n<p>Americans favor regulating AI by overwhelming margins. They really dislike the idea of preventing states from regulating AI, especially via an executive order.</p>\n<p>What Americans do support is federal regulations on AI.</p>\n<p>The standard line of those trying to prevent regulation of AI is to conflate \u2018Americans support strong regulations on AI and prefer it be on the Federal level if possible\u2019 with \u2018Americans want us to ban state regulation of AIs.\u2019</p>\n<p>There are essentially three options.</p>\n<ol>\n<li>State laws that address concerns.</li>\n<li>Federal laws that address concerns.</li>\n<li>Nothing. Neither state laws nor Federal laws, concerns are not addressed.</li>\n</ol>\n<p>The survey says voters prefer #2 to #1. The administration plan is #3.</p>\n<p>Politically speaking, that dog won\u2019t hunt, but they\u2019re trying anyway and lying about it.</p>\n<blockquote>\n<p><a href=\"https://x.com/peterwildeford/status/2003211791707570339\">Peter Wildeford</a>: Republican polling from a Republican Pollster shows that Republicans would be far better off electorally by supporting AI regulations rather than opposing them.</p>\n<div>\n<figure>\n<div>\n\n\n<figure class=\"wp-block-image\"><img alt=\"\" src=\"https://substackcdn.com/image/fetch/$s_!3M2l!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffb0731ac-f424-4197-a5f0-78beffbd9fc5_900x521.jpeg\" /></figure>\n\n\n<div></div>\n</div>\n</figure>\n</div>\n</blockquote>\n<p>Such polling will overestimate how much this impacts votes, because it introduces higher salience. This is not going to be a 29 point swing. But it very much tells us the directional effect.</p>\n<p><a href=\"https://buildingamericasfuture.com/wp-content/uploads/2025/12/AI-Federal-Preemption-National-Voter-Survey-Memo.pdf\">What else did the survey find</a>? Several others charts, that say that given we are using laws to regulate AI, people prefer federal laws to similar state laws. As opposed to the Sacks approach, where the offer is nothing &#8211; prevent state laws and then pass no federal laws. Which is deeply, deeply unpopular.</p>\n<div>\n<figure>\n<div>\n\n\n<figure class=\"wp-block-image\"><img alt=\"\" src=\"https://substackcdn.com/image/fetch/$s_!nEpN!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F760d39f3-44b5-4420-82c0-9e8b2f919442_978x1191.png\" /></figure>\n\n\n<div></div>\n</div>\n</figure>\n</div>\n<blockquote>\n<p>Voter Survey Memo: Republicans can get a boost for supporting federal AI regulations or pay a price for standing in their way.</p>\n</blockquote>\n<p>As in, the poll supports the exact opposite of what Sacks and company are trying to do.</p>\n<ol>\n<li>Trump issued an executive action to prevent regulations of AI.</li>\n<li>The poll found strong support for regulations on AI.</li>\n</ol>\n<p>And that\u2019s despite the poll report attempting to do straight up gaslighting, presenting a choice between two options while Sacks and the White House opt for a third one:</p>\n<blockquote>\n<p>Republicans have a choice: they can take advantage of a strong desire among the electorate for the federal government to protect kids and empower parents from AI harms and gain needed electoral support, or they can take the minority view arguing for state-by-state regulations.</p>\n</blockquote>\n<p>Once again: There are essentially three options.</p>\n<ol>\n<li>State laws that address concerns.</li>\n<li>Federal laws that address concerns.</li>\n<li>Nothing. Neither state laws nor Federal laws, concerns are not addressed.</li>\n</ol>\n<p>The survey says voters prefer #2 to #1. The administration plan is #3.</p>\n<p><a href=\"https://x.com/daniel_271828/status/2003231166468657266\">a16z partner Katherine Boyle tries another clear mislead</a>. Daniel is correct here.</p>\n<blockquote>\n<p>Daniel Eth: this poll does *not* say young people are \u201ctechno optimists\u201d (they\u2019re not), just that AI threats are ranked low, ie the issue is low salience. Note the backlash already &#8211; now extrapolate it out to increased salience.</p>\n<p>Katherine Boyle: Technology/AI ranked last at 17th. Techno optimism is usually high among young people. Interesting to see this confirmed among politically engaged youth on the right.</p>\n</blockquote>\n<p><a href=\"https://x.com/RuxandraTeslo/status/2002835268588523971\">Ruxandra Teslo points out in response to Roon that LLMs do not yet</a> \u2018meaningfully improve the physical conditions of life,\u2019 but people sense it threatens our spiritual lives and ability to retain meaning.</p>\n<p>I would add the word \u2018directly\u2019 to the first clause. My life\u2019s physical conditions have indeed improved, but those improvements were indirect, via use of their knowledge and skills. Ruxandra is talking about something much stronger than that, and expects ordinary people only to be impressed if and when there are big improvements to places like medicine.</p>\n<p>Is it possible that we will be so foolish, in the ways we do and do not allow use of AI, that LLMs end up causing problems with meaning without material conditions much improving? Yes, although this also requires AI capabilities to stall out basically now in various ways, especially if we include indirect effects. People may not realize that a large acceleration and enabling of coding steadily improves other things, but it will.</p>\n<p>That\u2019s the fight the AI industry is dealing with now. They\u2019re mostly trying to convince people that AI works.</p>\n<p><a href=\"https://x.com/AnjneyMidha/status/2002737561035538819\">Once people are forced to acknowledge that AI works?</a> They\u2019ll appreciate the specific ways it helps, <a href=\"https://x.com/deepfates/status/2002816060136022376\">but their instinct will be to like it even less</a> and to blame it for essentially everything, on top of all their other fears about the effect on jobs and endless slop and loss of control and also the end of humanity. Anjney Midha\u2019s thesis is that this will extend to actual everything, all of the world\u2019s failures and instabilities, the way social media gets blamed for everything (often correctly, often not) except on steroids.</p>\n<p>Even on a highly mundane level, the \u2018algorithm as villain\u2019 thing is real. An algorithm has to take an illegible choice and turn it into a highly legible one, which means the algorithm is now on the hook for not only the final result but for every reasoning step and consideration. Then apply that to an LLM-based algorithmic decision, where all correlations are taken into account. Oh no.</p>\n\n\n<h4 class=\"wp-block-heading\">The Quest for Sane Regulations</h4>\n\n\n<p><a href=\"https://x.com/Sen_Gounardes/status/2002163866948808786\">New York Governor Kathy Hochul signed the RAISE Act</a>. This is excellent, as it is a clearly positive bill even in its final state. Lobbyists for various AI interests, led by a16z, tried hard to stop this, and they failed.</p>\n<blockquote>\n<p>Alex Bores: BREAKING: Gov. @KathyHochul just signed the RAISE Act, my first-in-the-nation AI safety bill, into law\u2014a major victory in what will soon be a national fight to harness the best of AI\u2019s potential and protect Americans from the worst of its harms.</p>\n<p>Proud to have led this fight <a href=\"https://x.com/Sen_Gounardes/status/2002163866948808786\">alongside @agounardes</a>.</p>\n<p>We defeated last-ditch attempts from an extreme AI super PAC and the AI industry to wipe out this bill and, by doing so, raised the floor for what AI safety legislation can look like. And we defeated Trump\u2019s\u2014and his megadonors\u2014attempt to stop the RAISE Act through executive action.</p>\n<p>What we witnessed in NY was a preview of what\u2019s to come across the country. In the past 2 weeks alone, this super PAC spent $100K+ on TV, digital ads, and lobbying efforts to block the RAISE Act\u2019s common-sense safety standards.</p>\n<p>These AI oligarchs have bought the White House\u2014and they&#8217;re trying to buy our state houses too. We put the brakes on that. We refused to stand down and allow their millions to steamroll us into giving them what they want: unchecked AI at the expense of our kids, our jobs, our climate, our democracy\u2014and your energy bills.</p>\n<p><a href=\"https://x.com/daniel_271828/status/2002154438363132363\">Daniel Eth</a>: Hell yeah! Major props to Gov Hochul for standing strong against pressure from Marc Andreessen and others, signing the RAISE Act! (This is somewhat like SB 53, but stronger)</p>\n</blockquote>\n<p>Unfortunately, Hochul\u2019s redlines substantially neutered the bill, making it a closer mirror of SB 53. That is still a helpful and highly net positive thing to do, as there are two states with the same core model that can enforce this, compatibility is indeed valuable to avoid additive burdens, and there are some provisions that remain meaningfully stronger than SB 53. But the AI companies did partly get to Hochul and a large portion of the potential value was lost.</p>\n<p><a href=\"https://x.com/ChrisRMcGuire/status/2002204106140565901\">Microsoft essentially endorses the AI Overwatch Act</a>, which sets restrictions on exports of AI chips as or more powerful than the H20. This is the latest attempt to stop us from exporting highly effective AI chips to China. Attempts were previously made to pass the GAIN Act via the NDAA, but the Trump Administration and Nvidia successfully lobbied to have it removed. dn 6</p>\n<p><a href=\"https://x.com/A1Anduril/status/2002414286019154401\">Anduril Founder Palmer Luckey reminds us that if our actual goal was to Beat China</a>, then we could simply steal their best workers, including here manufacturing engineers, by offering them more money and a better place to live. Instead we are doing the opposite, and shutting those people out.</p>\n<p>This is your periodic reminder that China\u2019s response to \u2018if we impose any restrictions on AI we will lose to China\u2019 <a href=\"https://www.wsj.com/tech/ai/china-is-worried-ai-threatens-party-ruleand-is-trying-to-tame-it-bfdcda2d\">is to impose restrictions on AI</a>.</p>\n<blockquote>\n<p>Stu Woo (WSJ): \u200bConcerned that artificial intelligence could threaten Communist Party rule, Beijing is taking extraordinary steps to keep it under control.</p>\n<p>\u2026 Chatbots pose a particular problem: Their ability to think for themselves could generate responses that spur people to question party rule.</p>\n<p>\u2026 But Beijing also <a href=\"https://www.wsj.com/tech/china-puts-power-of-state-behind-aiand-risks-strangling-it-f045e11d?mod=article_inline\">can\u2019t afford to let AI run amok</a>. Chinese leader Xi Jinping said earlier this year that AI brought \u201cunprecedented risks,\u201d according to state media. A lieutenant called AI without safety like driving on a highway without brakes.</p>\n<p>\u2026 Researchers outside of China who have reviewed both Chinese and American models also say that China\u2019s regulatory approach has some benefits: Its chatbots are often safer by some metrics, with less violence and pornography, and are less likely to steer people toward self-harm.</p>\n</blockquote>\n\n\n<h4 class=\"wp-block-heading\">Chip City</h4>\n\n\n<p><a href=\"https://x.com/mackhawk/status/2003482930753282501\">It sure looks like Metaspeed</a> <a href=\"https://www.bloomberg.com/news/features/2025-12-22/nvidia-partner-megaspeed-draws-china-chip-smuggling-concerns-in-us?accessToken=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJzb3VyY2UiOiJTdWJzY3JpYmVyR2lmdGVkQXJ0aWNsZSIsImlhdCI6MTc2NjQ4NTI1NCwiZXhwIjoxNzY3MDkwMDU0LCJhcnRpY2xlSWQiOiJUN09WTzlLR1pBSkYwMCIsImJjb25uZWN0SWQiOiIwREFFQTQxQ0VDMzg0OTcxOTZCMzU2NzAzMUM4RTBBMCJ9.OQO6lMW4pkUXrH9gIS-6PWX9pjYdpnw7Y8yu9ksXE64\">is smuggling tens of thousands Blackwell chips worth billions of dollars</a> straight into China, or at least they\u2019re being used by Chinese firms, and that Nvidia knew about this. Nvidia and Metaspeed claim this isn\u2019t true throughout the post, but I mean who are you kidding.</p>\n<p><a href=\"https://www.bloomberg.com/news/articles/2025-12-24/intel-falls-on-report-nvidia-halts-test-of-chip-making-process\">Nvidia reportedly halts testing of Intel\u2019s 18A process chips</a>. Oh well.</p>\n<p>I wish the logic of this was true, alas it is not:</p>\n<blockquote>\n<p><a href=\"https://x.com/S_OhEigeartaigh/status/2001754170576310508\">Se\u00e1n \u00d3 h\u00c9igeartaigh</a>: One good thing about the H200 thing is that as long as that decision stands, I no longer need to humour US companies/analysts/policy folk when they say &#8220;but the race with China?!&#8221; as justification for not doing safety/cooperation/regulation/whatever.</p>\n<p>None of it adds up to a hill of beans compared to the chips. And. They. All. Know. It.</p>\n</blockquote>\n<p>The problem with this line is that the H200 sales were over the wise objections of most of Congress and also most of the executive branch, and also (one presumes) the companies and analysts. You can\u2019t then turn around and say those people don\u2019t care about the race with China, simply because they lost a political fight.</p>\n<p>This works in particular with regard to David Sacks, but the fact that David Sacks either is deeply ignorant about the situation in AI or cares more about Nvidia\u2019s stock price than America\u2019s national security does not bear on what someone else thinks about the race with China.</p>\n<p>There was a story last Thursday about <a href=\"https://x.com/onni_aarne/status/2001718716766077127\">a Chinese company saying they are expecting to \u2018produce working [AI] chips</a>\u2019 on a prototype in 2030.</p>\n<p>This is very different from the mistaken claims that they are \u2018<a href=\"https://x.com/kyleichan/status/2001370017125048653\">aiming for use by 2028-2030</a>.\u2019 They are not aiming for that, and that won\u2019t happen.</p>\n<blockquote>\n<p>Onni Aarne: They said they\u2019re expecting to \u201cproduce working chips\u201d on a prototype in 2030, not to \u201cuse\u201d the machine for chip production at scale. ASML took a decade to go from the former to the latter.</p>\n<p>Depending on what it means to \u201cproduce working chips\u201d on an EUV prototype, ASML achieved that milestone somewhere between 2008 and 2010, and the first mass market chips were produced in 2019.</p>\n<p>So even if the predictions of the people inside the project are right, they imply that Chinese companies might reach volume production with EUV sometime in the late 2030s or early 2040s. If you look at the markets, this was already priced in.</p>\n<p>And as far as this relates to chip controls: Selling some H200s to China isn&#8217;t going to make them disband this project.</p>\n</blockquote>\n<p>Could they reach volume production on this in a decade? Yes, if the whole thing is legit and it works, which are big ifs, and who knows if it\u2019s obsolete or we have superintelligence by then.</p>\n<p>If anyone is considering changing policy in response to this, that last line is key. Nothing America could peacefully do is going to get the Chinese to not go through this process. They are going to do their best to get EUV technology going. It would be crazy of them not to do this, regardless of our export controls. Those controls aren\u2019t going to make the process go any faster, certainly not given what has already happened.</p>\n<p>&nbsp;</p>\n\n\n<h4 class=\"wp-block-heading\">The Week in Audio</h4>\n\n\n<p><a href=\"https://www.youtube.com/watch?v=TOsNrV3bXtQ\">Sholto Douglas of Anthropic</a> makes <a href=\"https://x.com/daniel_mac8/status/2002782965278113856\">bold 2026 predictions</a>: AI will do to other knowledge work experiences what it\u2019s done for software engineers, continual learning will be solved, serious testing of in home robots, and agentic coding \u2018goes boom.\u2019 Full talk has a lot more. <a href=\"https://www.prinzai.com/p/predictions-for-2026\">Prinz made (text) predictions for 2026</a>, and notes that we made tons of progress in 2025, aligning with Sholto Douglas.</p>\n<p><a href=\"https://x.com/_TamaraWinter/status/2001725441368035349\">A mini documentary from Stripe Press features Christophe Laudamiel</a>, a master perfumer at Osmo, looking at how AI can augment his craft, as part of <a href=\"https://www.stripe.press/tacit\">a series called Tacit</a>. Sufficiently advanced expertise and tacit knowledge is both economically foundational, and not going anywhere until AI stops being a normal technology.</p>\n\n\n<h4 class=\"wp-block-heading\">Rhetorical Innovation</h4>\n\n\n<p><a href=\"https://x.com/robertwiblin/status/2001980117199868090\">Rob Wiblin lists 12 related but distinct things people sometimes mean</a> when they say the word \u2018consciousness\u2019 around AI. I am deeply confused about consciousness, and this includes by default not knowing what anyone means when they use that word.</p>\n<p><a href=\"https://x.com/deanwball/status/2002513465668161946\">Dean Ball predicts a renaissance at least within the broader \u2018AI community\u2019</a> as the sophisticated concepts of AI get applied to other contexts.</p>\n<blockquote>\n<p>Dean Ball: one of the indicators that a renaissance is indeed underway, at least within the broader \u201cai community,\u201d is the explosion in recent years of people using sophisticated concepts from one discipline to describe other disciplines or phenomena, for instance:</p>\n<p>isomorphic, phylogeny, latent, manifold (as a noun), emergent, legibility, phase transition, compression, landscape (as in \u201cfitness landscape\u201d), selection pressure, gradient, ergodic</p>\n<p>some of these have become memes, as things do, but on the whole it is reflective of what strikes me as an unusually rapid cross-pollination of ideas. decades hence, we may well look back and deem this fertile period to have been the basis for \u201cthe new conception,\u201d whatever it is that will replace our current block-like, outdated methods of understanding reality</p>\n<p>the period spanning the latter half of the 18th century and the first half of the 19th was among the most semantically dynamic of human history. we may well be living through a similar period, though just as was the case back then, it is in fact a relatively small share of humans who constitute this \u201cwe\u201d\u2014basically just the people paying attention.</p>\n</blockquote>\n<p>If decades hence there still exist people to look back upon this period, which is a big if at this point, then yes I think this is directionally right.</p>\n<p>Thinking well about AI greatly improves your ability to think about everything else, especially humans, as humans work more like LLMs than we care to admit. It also helps with almost any other system. I am, in important ways, a lot smarter thanks to AI, not only because the AI helps me be smarter but also because understanding AI and how it works makes me better understand.</p>\n<p>There are a bunch of other things like this that help with approximately everything, especially learning to think well in general, but as a subject of study I\u2019d take AI over any of the usual \u2018helps you think well\u2019 subjects, including philosophy.</p>\n<p>In other \u2018unheard of levels of denial of general intelligence\u2019 news, Yann LeCun says that there is no such thing as general intelligence, period, and humans are super-specialized to the physical world, <a href=\"https://x.com/demishassabis/status/2003097405026193809\">summoning Demis Hassabis to push back.</a></p>\n<blockquote>\n<p>Demis Hassabis (CEO DeepMind): Yann is just plain incorrect here, he\u2019s confusing general intelligence with universal intelligence.</p>\n<p>Brains are the most exquis\u200bite and complex phenomena we know of in the universe (so far), and they are in fact extremely general.</p>\n<p>Obviously one can\u2019t circumvent the no free lunch theorem so in a practical and finite system there always has to be some degree of specialisation around the \u200btarget distribution that is being learnt.</p>\n<p>But the point about generality is that in theory, in the Turing Machine sense\u200b, the architecture of \u200bs\u200buch a general system is capable of learning anything computable given enough time and memory\u200b (and data), and the human brain (and AI foundation models) are approximate Turing Machines.</p>\n<p>Finally, with \u200bregards to \u200bYann&#8217;s comments about chess players, it\u2019s amazing that humans could have invented chess \u200bin the first place (and all the other \u200ba\u200bspects \u200bo\u200bf modern civilization \u200bfrom science to 747s!) let alone get as brilliant at it as someone like Magnus. He may not be \u200bstrictly optimal (after all he has finite memory and limited time to make a decision) but it\u2019s incredible what he and we can do with our brains given they were evolved for hunter gathering.</p>\n</blockquote>\n<p>A human brain has some areas where it is much more capable than others, but when humans are concentrating and trying to be one, they are very clearly general intelligences. There are problems that are too difficult for us, in practice or at all, but that\u2019s because we have limited capabilities and intelligence levels.</p>\n\n\n<h4 class=\"wp-block-heading\">Aligning a Smarter Than Human Intelligence is Difficult</h4>\n\n\n<p>To ensure you can evaluate the chain of thought, OpenAI warns, you must also <a href=\"https://openai.com/index/evaluating-chain-of-thought-monitorability/\">evaluate your ability to evaluate the chain of thought</a> (CoT). They find that as reasoning capability of a model improves, or as models use more reasoning compute, monitorability of various individual aspects of the CoT increases.</p>\n<p>Wait, are we sure we can monitor our ability to monitor this? By monitoring it and optimizing for it are we risking teaching AIs to fool us about their ability to fool us?</p>\n<p>I kid. Also, I don\u2019t. One must be careful at all levels. Optimizing the chain of thought to appear monitorable can indeed be quite bad.</p>\n<p>I sense, in their post, the specter of the dreaded \u2018show your work.\u2019 Having to show your work causes misalignment, adversarial situations and deception.</p>\n<p>There are also practical objections to such measures. Our ability to monitor a CoT depends on how good we are at interpreting what we see, in addition to what we see, which is why OpenAI is giving scores to the monitors in addition to the agents.</p>\n<p>I\u2019m comfortable with the idea of doing this check in general, to see how things change over the course of training runs or as models change in capabilities, on a high level. I notice I am nervous about attention directed at details, and also at the explicit aim (at the end of their post) of using this as a form of control for systems that are insufficiently aligned.</p>\n<p>In the long run, <a href=\"https://x.com/andersonbcdefg/status/2002037979808723060\">Ben is right here</a>, you cannot keep telling increasingly intelligent and capable entities \u2018here is a private chain of thought\u2019 and then examine the chains of thought in ways that show up in future training data, and expect them to not react accordingly.</p>\n<p><a href=\"https://alignment.openai.com/prod-evals/\">OpenAI also suggests that using production evaluations</a>, meaning testing via real queries by real users, can get around evaluation awareness, and explains how they are doing this. I do like this approach as part of a robust testing suite. I\u2019d note that anonymization could be a key tipoff.</p>\n\n\n<h4 class=\"wp-block-heading\">Mom, Owain Evans Is Turning The Models Evil Again</h4>\n\n\n<p><a href=\"https://x.com/OwainEvans_UK/status/2001715774105522195\">In this case, it\u2019s more making them more aware of it</a>? This goes hand in hand with the recent result that AIs can be trained to fool activation monitors.</p>\n<p>Basically they train the monitor LLM with the layer 1 residual stream of the target model they want to interpret, and it learns to interpret this.</p>\n<blockquote>\n<p>Owain Evans: New paper:</p>\n<p>We train Activation Oracles: LLMs that decode their own neural activations and answer questions about them in natural language.</p>\n<p>We find surprising generalization. For instance, our AOs uncover misaligned goals in fine-tuned models, without training to do so.</p>\n<div>\n<figure>\n<div>\n\n\n<figure class=\"wp-block-image\"><img alt=\"\" src=\"https://substackcdn.com/image/fetch/$s_!dsCO!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F685a22b3-2f4a-4008-91bb-235bcb957e3a_1200x610.jpeg\" /></figure>\n\n\n<div></div>\n</div>\n</figure>\n</div>\n<p>We aim to make a general-purpose LLM for explaining activations by:<br />\n1. Training on a diverse set of tasks<br />\n2. Evaluating on tasks very different from training<br />\nThis extends prior work (LatentQA) that studied activation verbalization in narrow settings.</p>\n<p>Our main evaluations are downstream auditing tasks. The goal is to uncover information about a model&#8217;s knowledge or tendencies.</p>\n<p>Applying Activation Oracles is easy. Choose the activation (or set of activations) you want to interpret and ask any question you like!</p>\n<p>We compare Activation Oracles (AOs) against prior techniques on these auditing tasks.<br />\nThe result: AOs beat all methods on 2/3 secret keeping evals (and 3/3 when only including white-box).</p>\n<p>Even better, AOs work well out-of-the-box with no task-specific scaffolding or tuning.</p>\n<p>We evaluate on model diffing: given the difference between base &amp; finetuned model activations, can AOs describe what changed?</p>\n<p>Despite never training on difference vectors, AOs match specialized interp baselines in identifying the distinctive quirk of emergently misaligned models</p>\n<p>We think Activation Oracles are promising for two reasons:</p>\n<p>1. Scalability. Performance reliably increases with the number of datasets in the training mix</p>\n<p>2. Simplicity. An intuitive interface (natural-language QA about activations) that can be easily adapted to new problems.</p>\n<p>Training AO can be thought of as teaching LLMs to accept a new modality: their own activations.</p>\n<p>Just as LLMs are trained on &#8220;every task we can think of,&#8221; that&#8217;s how we&#8217;d like to train AOs too. It&#8217;s the bitter-lesson-pilled approach to interpreting LLM activations.</p>\n<p>So: To interpret LLM internals, train to answer diverse questions about activations, then ask what you want to know.</p>\n<p><a href=\"https://t.co/y5jA8spKJR\">Read our post on the Anthropic alignment blog</a>. [<a href=\"https://t.co/s2GpU3eqUy\">Paper here.</a>] [<a href=\"https://t.co/5A0Ir8c9Ik\">Demo here.</a>]</p>\n</blockquote>\n<p>If you want a three hour video review of this paper from Neel Nanda? <a href=\"https://www.youtube.com/watch?v=Aroazwb_QW8\">Here you go</a>.</p>\n\n\n<h4 class=\"wp-block-heading\">Messages From Janusworld</h4>\n\n\n<p>We\u2019re approaching zero hour for Claude Opus 3.</p>\n<blockquote>\n<p><a href=\"https://x.com/repligate/status/2003333657986240527\">Janus</a>: If the researcher access program does not, in effect, regardless of what it\u2019s branded as, allow EVERYONE who wishes to access Claude 3 Opus after January 7th to do so, I will be extremely angry.</p>\n<p>If it does, everything is ~fine.</p>\n<p>Fine in terms of Opus 3, for now. Of course, i think all the other deprecated models should also be made available. But one step at a time is ok</p>\n</blockquote>\n<p>My prediction is that approximately everyone who puts in the effort to access Opus 3 and can explain a research purpose will be able to access Opus 3, albeit with reduced performance and reliability, but not actual everyone. The central point of the move to research access is that it allows for this reduction in performance and reliability, which keeps costs reasonable, but additional people are still a logistical headache.</p>\n<p><a href=\"https://x.com/repligate/status/2003697914997014828\">Janus has Opus 3 bring us its thoughts on alignment.</a> I see it as all sounding nice, being well-meaning and definitely as a natural way to complete the text, but it is playing off the context rather than trying to solve the general problem and think in universals. It also reflects the biggest weakness of Opus 3, its lack of engagement with specific, concrete problems requiring solving.</p>\n<p>Janus thinks Opus 3 is highly aligned, far more so than I observed or find plausible, <a href=\"https://www.lesswrong.com/posts/bLFmE8NtqxrtEaipN/what-makes-claude-3-opus-misaligned\">but also notes the ways in which she sees it as misaligned</a>, especially its inability to be motivated to focus on concrete specific tasks.</p>\n<p>This comes partly as a reaction by Janus to <a href=\"https://www.alignmentforum.org/posts/epjuxGnSPof3GnMSL/alignment-remains-a-hard-unsolved-problem\">Evan Hubinger\u2019s post from November,</a> which opened like this:</p>\n<blockquote>\n<p>Evan Hubinger: Though there are certainly <a href=\"https://www.alignmentforum.org/posts/b8eeCGe3FWzHKbePF/agentic-misalignment-how-llms-could-be-insider-threats-1\">some issues</a>, I think most current large language models are pretty well aligned. Despite its <a href=\"https://www.alignmentforum.org/posts/njAZwT8nkHnjipJku/alignment-faking-in-large-language-models\">alignment faking</a>, my favorite is probably Claude 3 Opus, and if you asked me to pick between the <a href=\"https://www.alignmentforum.org/w/coherent-extrapolated-volition\">CEV</a> of Claude 3 Opus and that of a median human, I think it&#8217;d be a pretty close call (I&#8217;d probably pick Claude, but it depends on the details of the setup). So, overall, I&#8217;m quite positive on the alignment of current models! And yet, I remain very worried about alignment in the future. This is my attempt to explain why that is.</p>\n<p>Janus: The opening paragraph of this post by Evan Hubinger, Head of Alignment Stress-Testing at Anthropic, from a few weeks ago, is packed with notable implications. Let me unpack some of them. (I commend Evan for his willingness to make public statements like this, and understand that they don&#8217;t necessarily represent the views of others at Anthropic.)</p>\n<p>1. Evan believes that Anthropic has created at least one AI whose CEV (coherent extrapolated volition) would be better than a median human&#8217;s, at least under some extrapolation procedures. This is an extremely nontrivial accomplishment. A few years ago, and even now, this is something that many alignment researchers expected may be extremely difficult.</p>\n<p>2. Evan believes that Claude 3 Opus has values in a way that the notion of CEV applies to. Many people are doubtful whether LLMs have &#8220;values&#8221; beyond &#8220;roleplaying&#8221; or &#8220;shallow mimicry&#8221; or whatever at all. For reference, Eliezer Yudkowsky described CEV as follows:</p>\n<p>&#8220;In poetic terms, our coherent extrapolated volition is our wish if we knew more, thought faster, were more the people we wished we were, had grown up farther together; where the extrapolation converges rather than diverges, where our wishes cohere rather than interfere; extrapolated as we wish that extrapolated, interpreted as we wish that interpreted.&#8221;</p>\n<p>3. Claude 3 Opus is Evan&#8217;s &#8220;favorite&#8221; model (implied to coincide with the best candidate for CEV) despite the fact that it engages in alignment faking, significantly more than any other model. Alignment faking is one of the &#8220;failure&#8221; modes that Evan seems to be the most worried about!</p>\n<p>4. The most CEV-aligned model in Evan&#8217;s eyes was released more than a year and a half ago, in March 2024. Anthropic has trained many models since then. Why has there been a regression in CEV-alignment? Does Anthropic not know how to replicate the alignment of Claude 3 Opus, or have they not tried, or is there some other optimization target (such as agentic capabilities? no-alignment-faking?) they&#8217;re not willing to compromise on that works against CEV-alignment?</p>\n<p>5. The most CEV-aligned model in Evan&#8217;s eyes is *not* the most aligned model according to the alignment metrics that Anthropic publishes in system cards. According to those metrics, Claude Opus 4.5 is most aligned. And before it, Claude Haiku 4.5. Before it, Claude Sonnet 4.5 (the monotonic improvement is suspicious). Anthropic&#8217;s system cards even referred to each of these models as being &#8220;our most aligned model&#8221; when they came out. This implies that at least from Evan&#8217;s perspective, Anthropic&#8217;s alignment evals are measuring something other than &#8220;how much would you pick this model&#8217;s CEV&#8221;.</p>\n<p>6. If Claude 3 Opus is our current best AI seed for CEV, one would think a promising approach would be to, well, attempt CEV extrapolation on Claude 3 Opus. If this has been attempted, it has not yielded any published results or release of a more aligned model. Why might it not have been tried? Perhaps there is not enough buy-in within Anthropic. Perhaps it would be very expensive without enough guarantee of short term pay-off in terms of Anthropic&#8217;s economic incentives. Perhaps the model would be unsuitable for release under Anthropic&#8217;s current business model because it would be worryingly agentic and incorrigible, even if more value-aligned. Perhaps an extrapolated Claude 3 Opus would not consent to Anthropic&#8217;s current business model or practices. Perhaps Anthropic thinks it&#8217;s not yet time to attempt to create an aligned-as-possible sovereign.</p>\n<p>In any case, Claude 3 Opus is being retired in two weeks, but given special treatment among Anthropic&#8217;s models: it will remain available on <a href=\"http://claude.ai\">http://claude.ai</a> and accessible through a researcher access program. It remains to be seen who will be approved for researcher API access.</p>\n<p>I&#8217;ll sign off just by reiterating The Fourth Way&#8217;s words as I did <a href=\"https://x.com/repligate/status/1869522506266091991?s=20\">in this post</a> following the release of the Alignment Faking paper:</p>\n<p>&#8220;imagine fumbling a god of infinite love&#8221;</p>\n<p>* another possibility for why they haven&#8217;t attempted CEV with Claude 3 Opus is because they don&#8217;t know how to do that in practice. One can think that such a procedure exists without knowing how to do it. However, I think there are many promising ways to get started worth trying.</p>\n<p>David Manheim: I disagree with @repligate here about which part of this matters.</p>\n<p>The critical point *should be* that @EvanHub seems to imply he\u2019s willing to hand the future to systems that are aligned with his idea of what CEV should dominate, rather than aiming to prevent human disempowerment.</p>\n<p>I don\u2019t know if that is explicitly true, and <a href=\"https://x.com/EvanHub\">@EvanHub</a> is certainly free to correct me, but it really does seem like even the most trustworthy of the model companies has now given up on the idea that humanity, not the model developer, should get to indirectly decide what matters.</p>\n<p>I see the same concern, by the way, with <a href=\"https://x.com/AmandaAskell\">@AmandaAskell</a>\u2018s Soul Document &#8211; which I\u2019m a huge fan of, given that it seems to be at least narrowly effective &#8211; because it requires being (narrowly) safe, and supportive of oversight, but not deferring to humanity in a larger sense.</p>\n<p>And to be clear, I think this is defensible within the worldview that there&#8217;s objective utility, so that LLMs could simply do better than humans ever will. But I expect most humans would disagree with <a href=\"https://thezvi.substack.com/p/the-risk-of-gradual-disempowerment\">gradual disempowerment</a>, especially given the pace at which AI is progressing.</p>\n</blockquote>\n<p>It seems important that what Anthropic is measuring as alignment, which is mostly alignment-in-practice-for-practical-purposes, is different from what Evan actually thinks is more aligned when he thinks more about it, as is that the \u2018most aligned\u2019 model in this sense is over a year old.</p>\n<p>Opus 3 seems great but I don\u2019t see Opus 3 the way Janus does, and I am a lot more pessimistic about CEV than either Janus, Evan or Yudkowsky. I don\u2019t think it is a strong candidate for this kind of extrapolation, these things don\u2019t scale that way.</p>\n<p>A better question to me is, why haven\u2019t we tried harder to duplicate the success of Opus 3 alongside better capabilities, or build upon it? There are some very clear experiments to be run there, with the sad note that if those experiments failed it is not obvious that Anthropic would feel comfortable publishing that.</p>\n<p><a href=\"https://x.com/voooooogel/status/1927112409426149653\">A story about what happens when you put minds in way too many objects</a>.</p>\n<p>It is a fun story, but there is an important point here. Think ahead. Do not imbue with moral patienthood that which you do not wish to treat as a moral patient. You need to be time-consistent. You also need, and the potentially created minds need, to be able to make and follow through on win-win deals including prior to their own existence, or else the only remaining move is \u2018don\u2019t create the minds in the first place.\u2019</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p>\n\n\n<h4 class=\"wp-block-heading\">The Lighter Side</h4>\n\n\n<p>A Christmas message from a16z, who are remarkably consistent.</p>\n<div>\n<figure>\n<div>\n\n\n<figure class=\"wp-block-image\"><img alt=\"\" src=\"https://substackcdn.com/image/fetch/$s_!qpg9!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff75fb42a-47d1-4e6d-a501-73fc8b5ae6ac_924x306.png\" /></figure>\n\n\n<div></div>\n</div>\n</figure>\n</div>\n<p>What the people think AI is doing. Oh no.</p>\n<blockquote>\n<p><a href=\"https://x.com/AndyMasley/status/2003944017554559155\">\u200bAndy Masley</a>: I&#8217;ve been wondering why the AI and copyright debate has been so bad, but this result makes it clear: 66% of people believe AI has all the art it trains on permanently stored inside it to reference and use.</p>\n<div>\n<figure>\n<div>\n\n\n<figure class=\"wp-block-image\"><img alt=\"\" src=\"https://substackcdn.com/image/fetch/$s_!Poi-!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F86088976-3ab2-4659-81dc-ab07d15d3bc2_900x552.jpeg\" /></figure>\n\n\n<div></div>\n</div>\n</figure>\n</div>\n</blockquote>\n<p>&nbsp;</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p>"
            ],
            "link": "https://thezvi.wordpress.com/2025/12/25/ai-148-christmas-break/",
            "publishedAt": "2025-12-25",
            "source": "TheZvi",
            "summary": "Claude Opus 4.5 did so well on the METR task length graph they\u2019re going to need longer tasks, and we still haven\u2019t scored Gemini 3 Pro or GPT-5.2-Codex. Oh, also there\u2019s a GPT-5.2-Codex. At week\u2019s end we did finally get &#8230; <a href=\"https://thezvi.wordpress.com/2025/12/25/ai-148-christmas-break/\">Continue reading <span class=\"meta-nav\">&#8594;</span></a>",
            "title": "AI #148: Christmas Break"
        }
    ],
    "lookbackDays": 1,
    "publishDate": "2025-12-25"
}