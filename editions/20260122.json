{
    "articles": [
        {
            "content": [
                "<p>Why should you read novels? We tell children they\u2019re magic carpets for the mind / exercise for the soul instead of the body / lighthouses in the great sea of time. But aren\u2019t they ultimately a form of entertainment?</p>\n\n<p>Many years ago, I read Crime and Punishment. Here, with no research and no notes, is what I can remember about that book:</p>\n\n<ol>\n  <li>It was pretty good.</li>\n  <li>There was some guy, I think named Ras-something.</li>\n  <li>He was really angsty/edgy and lived in a small apartment or attic.</li>\n  <li>One day, for no particular reason, he killed an old woman.</li>\n  <li>Having done this random murder, he became even more angsty/edgy.</li>\n  <li>Then there was this police inspector guy.</li>\n  <li>The inspector kept coming after Ras-whoever and making extremely long philosophical rants.</li>\n  <li>Those rants may or may not have represented the personal views of Fyodor Dostoevsky.</li>\n  <li>I can\u2019t remember how the book ended. Surely Ras-whoever didn\u2019t live happily ever after? But was he caught or did he confess? No idea.</li>\n</ol>\n\n<p>This is probably below average. I know people who seem to remember every detail of everything they read. But even if you\u2019re one of them, so what? Is remembering those books better than remembering whatever else you would have done with your time if you hadn\u2019t been reading?</p>\n\n<p>And yet: If I\u2019m on vacation and I spend an afternoon reading a novel where in the mountains or on a beach, I feel like I\u2019m living my best life. Whereas if I spent an afternoon staring at short videos on my phone, I\u2019m sure I\u2019d feel like a gigantic loser. So what\u2019s going on here?</p>\n\n<h2 id=\"theory-1-ye-olde-status\">Theory 1: Ye olde status</h2>\n\n<p>The obvious explanation is that there\u2019s nothing intrinsically great about reading novels. The reason we think it\u2019s great is that reading novels\u2014at least the right ones\u2014is high status. It\u2019s a way of playing the <a href=\"https://en.wikipedia.org/wiki/The_Glass_Bead_Game\">Glass Bead Game</a>, a way of collecting <a href=\"https://dynomight.net/bourdieu/\">cultural capital</a> for you to lord over other people who don\u2019t have as much time or education as you do. It may <em>feel</em> like you \u201cactually enjoy reading\u201d, but that\u2019s because you\u2019re a desperate striver that subconsciously shape-shifts into whatever you think will make you look fancy. Apologize for reading. Apologize!</p>\n\n<p>I think there is something in this. However, I\u2019m also pretty sure it\u2019s not the full explanation, and I\u2019m bored to death with everyone trying to explain everything this way. So let\u2019s move on.</p>\n\n<h2 id=\"theory-2-diminishing-returns\">Theory 2: Diminishing returns</h2>\n\n<p>Say you can\u2019t read novels. Maybe because you\u2019re illiterate, maybe because you have no attention span, maybe because you can\u2019t tear yourself away from Candy Clicker. Now, say you cultivate the ability to read novels. Whatever issues you address in that process, it seems like it will clearly be good for you, right?</p>\n\n<p>Under this theory, what\u2019s important is having the <em>ability</em> to read novels. But said ability is acquired by reading novels, so read some novels.</p>\n\n<p>Alternatively, say you <em>could</em> read novels, but you simply never have. It\u2019s plausible that the first time you have the \u201cnovel\u201d experience of taking photons into your eyes and mentally converting them into a story, this truly does feed your mind.</p>\n\n<p>Both versions of this theory suggest that reading novels has diminishing returns. That fits nicely with the fact that many people push their children to read novels while not reading any themselves. But do we really believe that after you\u2019ve read some number of novels, it\u2019s pointless to read more?</p>\n\n<h2 id=\"theory-3-common-language\">Theory 3: Common language</h2>\n\n<p>I think Catcher in the Rye is a good but not great book. But I love talking about Catcher in the Rye because (1) all North Americans seem to have read it, and (2) whenever I ask someone to tell me how they feel about Holden Caulfield, I always seem to learn something about them.</p>\n\n<p>(I find him sympathetic.)</p>\n\n<p>If there\u2019s a group of people talking about Catcher in the Rye\u2014or The Three-Body Problem, or Infinite Jest, or Don Quixote\u2014then you benefit from being able to participate. The cynic might argue that this is zero-sum status competition. But I don\u2019t think that\u2019s most of it. Because, at least in my social circles, people feel boorish talking about books if not everyone has read them. So these conversations only happen if everyone has read the book in question.</p>\n\n<p>Ultimately, we\u2019re all alone in the world, and trying to connect with each other by pushing air through our throat meat. With more shared cultural context, those meat sounds are more meaningful, so we can all feel less alone.</p>\n\n<p>True. But shared context can come from other things, too, like traveling to the same places, or watching the same sports, or practicing the same skills or hobbies. So what makes books special? The two answers I see are:</p>\n\n<ol>\n  <li>Nothing. If you think they\u2019re better than other types of cultural context, that\u2019s because you\u2019re a book person.</li>\n  <li>Books leave more room for interpretation. Maybe Don Quixote is a fanatic, maybe he\u2019s an idealist, maybe he\u2019s a \u201cwise fool\u201d. It\u2019s debatable. But there\u2019s no doubt who won the last World Cup.</li>\n</ol>\n\n<p>I lean weakly towards the first answer. Novels are a useful form of social context. But that\u2019s a side benefit. It\u2019s not why we read most books.</p>\n\n<h2 id=\"theory-4-legible-mind-space\">Theory 4: Legible mind-space</h2>\n\n<p>Maybe novels are just another form of entertainment. OK. But say you tried to tell the same story as a novel or as movie / podcast / opera / interpretive dance performance. Different formats will be better in different ways. One advantage I see for novels is that they make it natural to explore the interior worlds of the characters.</p>\n\n<p>Some movies have voice-overs where characters explain what they\u2019re thinking. But this is generally considered cringe and a poor use of the medium. Meanwhile, many books are <em>mostly</em> about exploring what the characters are thinking.</p>\n\n<p>Thoughts are worth exploring. If you want to explore thoughts, maybe novels are the best way to do that.</p>\n\n<p><em>Aside</em>: I\u2019ve mentioned before that I think My Brilliant Friend is the best TV show ever made. Can I confess that I like it much more than the books it is based on? Because, like the books, the TV show involves a <em>lot</em> of what the main character is thinking, and even makes heavy use of voice-overs. So maybe other mediums have unrealized potential?</p>\n\n<h2 id=\"theory-5-purity-of-vision\">Theory 5: Purity of vision</h2>\n\n<p>Movies are expensive to make. To be financially viable, they need to target a large slice of the population. Movies also reflect the combined efforts of many people. Both of these mean that movies are a compromise between different visions.</p>\n\n<p>Novels are usually written by one person. And they\u2019re often written more for personal expression than to make money. After all, writing is fun. I mean\u2014writing is hard, but would you rather spend an afternoon holding up a shotgun microphone, cleaning a movie star\u2019s trailer, or writing a novel?</p>\n\n<p>To quantify this, some searching suggests that around 10,000 feature films are released each year, as compared to around 1,000,000 novels. (Does one in 7,000 people really write a novel each year?) That\u2019s two orders of magnitude. So if you want to hear a truly unique story, a pure vision of one person, maybe novels are where you\u2019ll find it.</p>\n\n<h2 id=\"theory-6-all-these-theories-are-stupid\">Theory 6: All these theories are stupid</h2>\n\n<p>Or: Maybe the point of reading War and Peace is that War and Peace is incredible and obviously one of the greatest pieces of art ever made in any medium. No one who reads War and Peace can question the value of what they\u2019ve done. What are we talking about?</p>\n\n<p>Fair. I definitely feel like I\u2019m living my best life when I read War and Peace. But I also feel like I\u2019m living an OK-ish life when I read a novel about <a href=\"https://en.wikipedia.org/wiki/Spenser_(character)\">Spenser, private investigator</a>. And most novels most people read are closer to the Spenser than to War and Peace. And I still feel better spending an afternoon reading about Spenser than I would watching 99% of TV shows.</p>\n\n<h2 id=\"theory-7-dopamine\">Theory 7: Dopamine</h2>\n\n<p>Or perhaps the difference is that reading is a thing you <em>do</em> rather than something you <em>consume</em>.</p>\n\n<p>This theory holds than when spend an hour slurping up short-form video, you\u2019re training yourself to sort of pull a lever in the hope that some reward is delivered to you. But if you read (or do watercolors, or meditate) you\u2019re training yourself to calmly pursue long-term goals and to sustain attention in the face of complexity.</p>\n\n<p>Sometimes I wonder if phones/apps are the most addictive thing ever created. I suspect that more people today are addicted to their phones today than were ever addicted to any drug other than caffeine or perhaps nicotine. And while a phone addiction is less physically harmful than tobacco, that phone addiction will eat a larger part of your soul.</p>\n\n<p>I think this is a big part of the explanation.</p>\n\n<h2 id=\"theory-8-non-fungible-time\">Theory 8: Non-fungible time</h2>\n\n<p>In the end, I don\u2019t think novels are the best way to spend your time. In my view no novel\u2014not even War and Peace\u2014is as good as a truly great conversation.</p>\n\n<p>But great conversations are hard to create. Sometimes you\u2019re sitting on a train, or laying in bed, or it\u2019s just been a long day and you don\u2019t have the energy to find a giant block of marble and pursue your dream of experimental sculpture. In these situations, maybe reading a novel is the best thing you could do in the category of things you could realistically do.</p>\n\n<p>Exercise for the reader: Apply these theories to blog posts.</p>"
            ],
            "link": "https://dynomight.net/novels/",
            "publishedAt": "2026-01-22",
            "source": "Dynomight",
            "summary": "<p>Why should you read novels? We tell children they\u2019re magic carpets for the mind / exercise for the soul instead of the body / lighthouses in the great sea of time. But aren\u2019t they ultimately a form of entertainment?</p> <p>Many years ago, I read Crime and Punishment. Here, with no research and no notes, is what I can remember about that book:</p> <ol> <li>It was pretty good.</li> <li>There was some guy, I think named Ras-something.</li> <li>He was really angsty/edgy and lived in a small apartment or attic.</li> <li>One day, for no particular reason, he killed an old woman.</li> <li>Having done this random murder, he became even more angsty/edgy.</li> <li>Then there was this police inspector guy.</li> <li>The inspector kept coming after Ras-whoever and making extremely long philosophical rants.</li> <li>Those rants may or may not have represented the personal views of Fyodor Dostoevsky.</li> <li>I can\u2019t remember how the book ended. Surely Ras-whoever didn\u2019t live happily ever after? But was he caught or did he confess? No idea.</li> </ol> <p>This is probably below average. I know people who seem to remember every detail of everything they read. But even if you\u2019re one of them, so what? Is remembering those books better than remembering",
            "title": "Why read novels?"
        },
        {
            "content": [],
            "link": "https://bernsteinbear.com/blog/multiple-entry/?utm_source=rss",
            "publishedAt": "2026-01-22",
            "source": "Max Bernstein",
            "summary": "<h2 id=\"background-and-bytecode-design\">Background and bytecode design</h2> <p>The ZJIT compiler compiles Ruby bytecode (YARV) to machine code. It starts by transforming the stack machine bytecode into a high-level graph-based intermediate representation called HIR.</p> <p>We use a more or less typical<sup id=\"fnref:ebb\"><a class=\"footnote\" href=\"https://bernsteinbear.com/feed.xml#fn:ebb\" rel=\"footnote\">1</a></sup> control-flow graph (CFG) in HIR. We have a compilation unit, <code class=\"language-plaintext highlighter-rouge\">Function</code>, which has multiple basic blocks, <code class=\"language-plaintext highlighter-rouge\">Block</code>. Each block contains multiple instructions, <code class=\"language-plaintext highlighter-rouge\">Insn</code>. HIR is always in SSA form, and we use the variant of SSA with block parameters instead of phi nodes.</p> <p>Where it gets weird, though, is our handling of multiple entrypoints. See, YARV handles default positional parameters (but <em>not</em> default keyword parameters) by embedding the code to compute the defaults inside the callee bytecode. Then callers are responsible for figuring out what offset in the bytecode they should start running the callee, depending on the amount of arguments the caller provides.<sup id=\"fnref:keywords\"><a class=\"footnote\" href=\"https://bernsteinbear.com/feed.xml#fn:keywords\" rel=\"footnote\">2</a></sup></p> <p>In the following example, we have a function that takes two optional positional parameters <code class=\"language-plaintext highlighter-rouge\">a</code> and <code class=\"language-plaintext highlighter-rouge\">b</code>. If neither is provided, we start at offset <code class=\"language-plaintext highlighter-rouge\">0000</code>. If just <code class=\"language-plaintext highlighter-rouge\">a</code> is provided, we start at offset <code",
            "title": "A multi-entry CFG design conundrum"
        },
        {
            "content": [],
            "link": "https://www.nytimes.com/2026/01/21/style/modern-love-what-are-your-dating-rules.html",
            "publishedAt": "2026-01-22",
            "source": "Modern Love - NYT",
            "summary": "Advice abounds on how, when and whom to date. What are your best rules for navigating early romance?",
            "title": "Quick! What Are Your Dating Rules?"
        },
        {
            "content": [
                "<p>\n          <a href=\"https://www.astralcodexten.com/p/hidden-open-thread-4175\">\n              Read more\n          </a>\n      </p>"
            ],
            "link": "https://www.astralcodexten.com/p/hidden-open-thread-4175",
            "publishedAt": "2026-01-22",
            "source": "SlateStarCodex",
            "summary": "<p> <a href=\"https://www.astralcodexten.com/p/hidden-open-thread-4175\"> Read more </a> </p>",
            "title": "Hidden Open Thread 417.5"
        },
        {
            "content": [
                "<p><a href=\"https://x.com/AnthropicAI/status/2014005798691877083\">Anthropic released a new constitution for Claude</a>. I encourage those interested to read the document, either in whole or in part. I intend to cover it on its own soon.</p>\n<p>There was also actual talk about coordinating on a conditional pause or slowdown from DeepMind CEO Demis Hassabis, which I also plan to cover later.</p>\n<p>Claude Code continues to be the talk of the town, <a href=\"https://thezvi.substack.com/p/claude-codes-3?r=67wny\">the weekly report on that is here</a>.</p>\n<p>OpenAI responded by planning ads for the cheap and free versions of ChatGPT.</p>\n<p>There was also a fun but meaningful incident <a href=\"https://thezvi.substack.com/p/chatgpt-self-portrait?r=67wny\"><strong>involving ChatGPT Self Portraits</strong></a>.</p>\n<div>\u00a0</div>\n\n\n<span id=\"more-25047\"></span>\n\n\n</div>\n\n\n<h4 class=\"wp-block-heading\">Table of Contents</h4>\n\n\n<ol>\n<li><a href=\"https://thezvi.substack.com/i/184715851/language-models-offer-mundane-utility\">Language Models Offer Mundane Utility.</a> Call in the tone police.</li>\n<li><a href=\"https://thezvi.substack.com/i/184715851/language-models-don-t-offer-mundane-utility\">Language Models Don\u2019t Offer Mundane Utility.</a> He who lives by the pattern.</li>\n<li><a href=\"https://thezvi.substack.com/i/184715851/huh-upgrades\">Huh, Upgrades.</a> Claude health integrations, ChatGPT $8/month option.</li>\n<li><a href=\"https://thezvi.substack.com/i/184715851/gemini-personalized-intelligence\">Gemini Personalized Intelligence.</a> Signs of both remain somewhat lacking.</li>\n<li><a href=\"https://thezvi.substack.com/i/184715851/deepfaketown-and-botpocalypse-soon\">Deepfaketown and Botpocalypse Soon.</a> Get that bathtub viking.</li>\n<li><a href=\"https://thezvi.substack.com/i/184715851/fun-with-media-generation\">Fun With Media Generation.</a> Studio Ghibli pics are back, baby.</li>\n<li><a href=\"https://thezvi.substack.com/i/184715851/we-re-proud-to-announce-the-torment-nexus\"><strong>We\u2019re Proud To Announce The Torment Nexus</strong>.</a> Ads come to ChatGPT.</li>\n<li><a href=\"https://thezvi.substack.com/i/184715851/they-took-our-jobs\">They Took Our Jobs.</a> Find a game plan. Don\u2019t count on repugnance.</li>\n<li><a href=\"https://thezvi.substack.com/i/184715851/the-revolution-of-rising-expectations\">The Revolution of Rising Expectations.</a> Look at all the value you\u2019re getting.</li>\n<li><a href=\"https://thezvi.substack.com/i/184715851/get-involved\">Get Involved.</a> AI Village, Anthropic, Dwarkesh Patel guest hunter.</li>\n<li><a href=\"https://thezvi.substack.com/i/184715851/a-young-lady-s-illustrated-primer\">A Young Lady\u2019s Illustrated Primer.</a> We\u2019re putting together the wrong team.</li>\n<li><a href=\"https://thezvi.substack.com/i/184715851/in-other-ai-news\">In Other AI News.</a> China remain behind, Drexler goes galaxy brain.</li>\n<li><a href=\"https://thezvi.substack.com/i/184715851/axis-of-assistance\"><strong>Axis of Assistance</strong>.</a> Have you tried not being a helpful AI assistant?</li>\n<li><a href=\"https://thezvi.substack.com/i/184715851/show-me-the-money\">Show Me the Money.</a> OpenAI looks to raise another $50 billion.</li>\n<li><a href=\"https://thezvi.substack.com/i/184715851/california-in-crisis\">California In Crisis.</a> Will we soon ask, where have all the startups gone?</li>\n<li><a href=\"https://thezvi.substack.com/i/184715851/bubble-bubble-toil-and-trouble\">Bubble, Bubble, Toil and Trouble.</a> They keep using that word.</li>\n<li><a href=\"https://thezvi.substack.com/i/184715851/quiet-speculations\">Quiet Speculations.</a> Results from the AI 2025 predictions survey.</li>\n<li><a href=\"https://thezvi.substack.com/i/184715851/elon-musk-versus-openai\">Elon Musk Versus OpenAI.</a> There they go again.</li>\n<li><a href=\"https://thezvi.substack.com/i/184715851/the-quest-for-sane-regulations\"><strong>The Quest for Sane Regulations</strong>.</a> Nvidia versus the AI Overwatch Act.</li>\n<li><a href=\"https://thezvi.substack.com/i/184715851/chip-city\">Chip City.</a> Are we on the verge of giving China ten times their current compute?</li>\n<li><a href=\"https://thezvi.substack.com/i/184715851/the-week-in-audio\">The Week in Audio.</a> Tyler Cowen and a surprisingly informed Ben Affleck.</li>\n<li><a href=\"https://thezvi.substack.com/i/184715851/rhetorical-innovation\">Rhetorical Innovation.</a> Remember the conservation of expected evidence.</li>\n<li><a href=\"https://thezvi.substack.com/i/184715851/aligning-a-smarter-than-human-intelligence-is-difficult\">Aligning a Smarter Than Human Intelligence is Difficult.</a> Nope, still difficult.</li>\n<li><a href=\"https://thezvi.substack.com/i/184715851/alignment-is-not-primarily-about-a-metric\">Alignment Is Not Primarily About a Metric.</a> Not a metric to be optimizing.</li>\n<li><a href=\"https://thezvi.substack.com/i/184715851/how-to-be-a-safe-robot\">How To Be a Safe Robot.</a> Hint, the plan is not \u2018don\u2019t tell it about unsafe robots.\u2019</li>\n<li><a href=\"https://thezvi.substack.com/i/184715851/living-in-china\">Living In China.</a> Chinese LLMs know things and pretend not to. Use that.</li>\n<li><a href=\"https://thezvi.substack.com/i/184715851/claude-3-opus-lives\">Claude 3 Opus Lives.</a> Access granted.</li>\n<li><a href=\"https://thezvi.substack.com/i/184715851/people-are-worried-about-ai-killing-everyone\">People Are Worried About AI Killing Everyone.</a> Charles Darwin.</li>\n<li><a href=\"https://thezvi.substack.com/i/184715851/messages-from-janusworld\">Messages From Janusworld.</a> What are you worried people will do with your info?</li>\n<li><a href=\"https://thezvi.substack.com/i/184715851/everyone-is-confused-about-ai-consciousness\">Everyone Is Confused About AI Consciousness.</a> Don\u2019t call it a disproof.</li>\n<li><a href=\"https://thezvi.substack.com/i/184715851/the-lighter-side\">The Lighter Side.</a></li>\n</ol>\n\n\n<h4 class=\"wp-block-heading\">Language Models Offer Mundane Utility</h4>\n\n\n<p>Tone editor or tone police is a great AI job. <a href=\"https://www.bloomberg.com/news/articles/2026-01-21/white-collar-workers-use-ai-to-have-tough-conversations-at-work\">Turn your impolite \u2018f*** you\u2019 email into a polite \u2018f*** you\u2019 email</a>, and get practice stripping your emotions out of other potentially fraught interactions, lest your actual personality get in the way. Or translate your neurodivergent actual information into socially acceptable extra words.</p>\n<p><a href=\"https://x.com/Out5p0ken/status/2012969221492265047\">ICE uses an AI program from Palantir called \u2018Elite\u2019</a> to pick neighborhoods to raid.</p>\n\n\n<h4 class=\"wp-block-heading\">Language Models Don\u2019t Offer Mundane Utility</h4>\n\n\n<p>If your query is aggressively pattern matched into a basin where facts don\u2019t matter and you\u2019re making broad claims without much justifying them, <a href=\"https://x.com/DominikPeters/status/2013349636560376190\">AIs will largely respond</a> to the pattern match, as Claude did in the linked example. And if you browbeat such AIs about it, and they cower to tell you what you want to hear, you can interpret that as \u2018the AI is lying to me, surely this terrible AI is to blame\u2019 or you can wonder why it decided to do all of that.</p>\n\n\n<h4 class=\"wp-block-heading\">Huh, Upgrades</h4>\n\n\n<p><a href=\"https://x.com/claudeai/status/2013754136265621952\">Claude adds four new health integrations in beta</a>: Apple Health (iOS), Health Connect (Android), HealthEx, and Function Health. They are private by design.</p>\n<p><a href=\"https://openai.com/index/introducing-chatgpt-go/\">OpenAI adds the ChatGPT Go option more broadly</a>, at $8/month. If you are using ChatGPT in heavy rotation or as your primary, you need to be paying at least the $20/month for Plus to avoid being mostly stuck with Instant.</p>\n<p><a href=\"https://x.com/sama/status/2013404302400712757\">Sam Altman throws out the latest \u2018what would you like to see us improve?\u2019 thread</a>.</p>\n<p>Remember ChatGPT\u2019s Atlus browser? <a href=\"https://x.com/adamhfry/status/2014095464375919102\">It finally got tab groups</a>, an \u2018auto\u2019 option to have search choose between ChatGPT and Google and various other polishes. There\u2019s still no Windows version and Claude Code is my AI browser now.</p>\n\n\n<h4 class=\"wp-block-heading\">Gemini Personalized Intelligence</h4>\n\n\n<p><a href=\"https://x.com/GeminiApp/status/2011469541235417243\">The pitch is that Gemini now draws insights from across your Google apps</a> to provide customized responses. There\u2019s a section for non-Google apps as well, although there\u2019s not much there yet other than GitHub.</p>\n<blockquote>\n<p><a href=\"https://x.com/joshwoodward/status/2011471375521710130\">Josh Woodward</a>: Introducing Personal Intelligence. It&#8217;s our answer to a top request: you can now personalize @GeminiApp by connecting your Google apps with a single tap. Launching as a beta in the U.S. for Pro/Ultra members, this marks our next step toward making Gemini more personal, proactive and powerful. Check it out!</p>\n<p>Google: Gemini already remembers your past chats to provide relevant responses. But today, we\u2019re taking the next step forward with the introduction of Personal Intelligence.</p>\n<p>You can choose to let Gemini connect information from your Gmail, Google Photos, Google Search, and YouTube history to receive more personalized responses.</p>\n<p>Here are some ways you can start using it:</p>\n<p>\u2022 Planning: Gemini will be able to suggest hidden gems that feel right up your alley for upcoming trips or work travel.<br />\n\u2022 Shopping: Gemini will get to know your taste and preferences on a deeper level, and help you find items you\u2019ll love faster.<br />\n\u2022 Motivation: Gemini will have a deeper understanding of the goals you\u2019re working towards. For example, it might notice that you have a marathon coming up and offer a training plan.</p>\n<p>Privacy is central to Personal Intelligence and how you connect other Google apps to Gemini. The new beta feature is off by default: you choose to turn it on, decide exactly which apps to connect, and can turn it off at any time.</p>\n</blockquote>\n<p>The pitch is that it can gather information from your photos (down to things like where you travel, what kind of tires you need for your car), from your Email and Google searches and YouTube and Docs and Sheets and Calendar, and learn all kinds of things about you, not only particular details but also your knowledge level and your preferences. Then it can customize everything on that basis.</p>\n<p>It can access Google Maps, but not your personalized data like saved locations, other than where Work and Home are. It doesn\u2019t have your location history. This feels like an important missed opportunity.</p>\n<p>One potential \u2018killer app\u2019 is fact finding. If you want to know something about yourself and your life, and Google knows it, hopefully Gemini can now tell you. Google knows quite a lot of things, and my Obsidian Vault is echoed in Google Sheets, which you can instruct Gemini to look for. <a href=\"https://x.com/joshwoodward/status/2011484735243960543\">Josh Woodward shows an example of asking when he last got a haircut</a>.</p>\n<p>The real killer app would be taking action on your behalf. It can\u2019t do that except for Calendar, but it can do things on the level of writing draft emails and making proposed changes in Docs.</p>\n<p>There really is a ton of info there if it gets analyzed properly. It could be a big game.</p>\n<p>When such things work, they \u2018feel like magic.\u2019</p>\n<p>When they don\u2019t work, they feel really stupid.</p>\n<p>I asked for reactions and got essentially nothing.</p>\n<p>That checks. To use this, you have to use Gemini. Who uses Gemini?</p>\n<p>Thus, in order to test personalized intelligence, I need a use case where I need its capabilities enough to use Gemini, as opposed to going back to building my army of skills and connectors and MCPs in Claude Code, including with the Google suite.</p>\n<blockquote>\n<p><a href=\"https://x.com/omooretweets/status/2011495886388625700\">Olivia Moore</a>: Connectors into G Suite work just OK in ChatGPT + Claude &#8211; they&#8217;re slow and can struggle to find things.</p>\n<p>If Gemini can offer best &#8220;context&#8221; from Gmail, G Drive, Calendar &#8211; that&#8217;s huge.</p>\n<p>The aggressive version would be to block Connectors in other LLMs&#8230;but that feels unlikely!</p>\n</blockquote>\n<p>The other problem is that Google\u2019s connectors to its own products have consistently, when I have tried them, failed to work on anything but basic tasks. Even on those basic tasks, the connector from Claude or ChatGPT has worked better. And now I\u2019m hooking Claude Code up to the API.</p>\n\n\n<h4 class=\"wp-block-heading\">Deepfaketown and Botpocalypse Soon</h4>\n\n\n<p><a href=\"https://www.bloomberg.com/news/features/2026-01-19/grok-ai-sexualized-images-expose-gaps-in-oversight-enforcement\">Elon Musk and xAI continue to downplay</a> the whole \u2018Grok created a bunch of sexualized deepfakes in public on demand and for a time likely most of the world\u2019s AI CSAM\u2019 as if it is no big deal. Many countries and people don\u2019t see it that way, investigations continue and it doesn\u2019t look like the issue is going to go away.</p>\n<p>We used to worry a lot about deepfakes. Then we all mostly stopped worrying about it, at least until the recent xAI incident, <a href=\"https://www.bloomberg.com/opinion/articles/2026-01-19/ai-deepfakes-have-harmed-too-many-kids-already\">but that doesn\u2019t mean there aren\u2019t a lot of deepfakes</a>. A Bloomberg report says \u2018one in eight kids personally knows someone who has been the target of a deepfake video,\u2019 which is an odd way to think about prevalence but is certainly a massive increase. Reports rose from roughly 4,700 in 2023 to over 440,000 in the first half of 2025.</p>\n<p>We could stop Grok if we wanted to, but the open-source tools are already plenty good enough to generate sexualized deepfakes and will only get easier to access. You can make access annoying and shut down distribution, but you can\u2019t shut the thing down on the production side.</p>\n<p><a href=\"https://www.bloomberg.com/opinion/articles/2026-01-17/ai-deepfake-nude-warnings-are-missing-a-deterrent\">Meanwhile, psychiatrist Sarah Gundle issues the latest warning</a> that this \u2018interactive pornography,\u2019 in addition to the harms to the person depicted, also harms the person creating or consuming it, as it disincentivizes human connection by making alternatives too easy, and people (mostly men) don\u2019t have the push to establish emotional connections. I am skeptical of such warnings and concerns, they always are of a form that could prove far too much and historical records mostly don\u2019t back it up, but on the other hand, <a href=\"https://www.youtube.com/watch?v=JPQJBgWwg3o&amp;pp=ygUaZG9uJ3QgZGF0ZSByb2JvdHMgZnV0dXJhbWHYBgk%3D\">don\u2019t date robots</a>.</p>\n<p>Misinformation is demand driven, an ongoing series.</p>\n<blockquote>\n<p><a href=\"https://x.com/JerryDunleavy/status/2012908443464613918\">Jerry Dunleavy IV </a>: Neera Tanden believes that ICE agents chased a protester dressed in Viking gear and sitting in a bath tub with skateboard wheels down the street, and that the Air Force was called in in response. Certain segments of the population just are not equipped to handle obvious AI slop.</p>\n<p><a href=\"https://x.com/AmyA1A/status/2012909087458033777\">Amygator *not an actual alligator</a>: Aunt Carol on the family group chat <a href=\"https://x.com/AmyA1A/status/2012909087458033777\">isn\u2019t sure whether or not this is A.I.</a> I\u2019m done.</p>\n<div>\n<figure>\n<div>\n\n\n<figure class=\"wp-block-image\"><img alt=\"\" src=\"https://substackcdn.com/image/fetch/$s_!SHjS!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd3562532-4ba7-4c24-8c9c-088780b7c45a_871x827.jpeg\" /></figure>\n\n\n<div></div>\n</div>\n</figure>\n</div>\n</blockquote>\n<p>This is not a subtle case. The chyron is literally floating up and down in the video. In a sane world this would be a good joke. Alas, there are those on all sides who don\u2019t care that something like this is utterly obvious, but it makes little difference that this was an AI video instead of something else.</p>\n<p>In Neera\u2019s defense, the headlines this week include \u2018President sends letter to European leaders demanding Greenland because Norway wouldn\u2019t award him the Nobel Peace Prize.\u2019 Is that more or less insane than the police unsuccessfully chasing a bathtub viking on the news while the chyron slowly bounces?</p>\n\n\n<h4 class=\"wp-block-heading\">Fun With Media Generation</h4>\n\n\n<p>The new OpenAI image generation can\u2019t do Studio Ghibli properly, <a href=\"https://x.com/eigenrobot/status/2011989030817141032\">but as per Roon you can still use the old one</a> <a href=\"https://chatgpt.com/g/g-6940a876d5f4819186b4668deabcd580-4o-imagegen\">by going here.</a></p>\n<blockquote>\n<p><a href=\"https://x.com/tszzl/status/2011898230821523943\">Roon</a>: \u200bconfirmed that this is a technical regression in latest image model nothing has changed WRT policy.</p>\n</blockquote>\n<div>\n<figure>\n<div>\n\n\n<figure class=\"wp-block-image\"><img alt=\"\" src=\"https://substackcdn.com/image/fetch/$s_!eoeB!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe79effee-7fc0-46fa-97cf-795552db0f3a_600x900.jpeg\" /></figure>\n\n\n<div></div>\n</div>\n</figure>\n</div>\n<blockquote>\n<p>Bryan: The best part of all this is all you gotta do is drop ur image and say \u201cGhibli\u201d &#8211; perfection\u200b.</p>\n</blockquote>\n<p>It\u2019s very disappointing that they were unable to preserve this capability going forward, but as long as we have the old option, we\u2019re still good. Image generation is already very good in many ways so often what you are about is style.</p>\n<p><a href=\"https://x.com/big_business_/status/2010932655873798241?s=20\">Sienna Rose recently had three songs in the Spotify top 50, while being an AI</a>, and <a href=\"https://x.com/andrewcurran_/status/2012220138473304347?s=46\">we have another sighting in Sweden</a>.</p>\n\n\n<h4 class=\"wp-block-heading\"></h4>\n\n\n\n<h4 class=\"wp-block-heading\">We\u2019re Proud To Announce The Torment Nexus</h4>\n\n\n<p><a href=\"https://x.com/sama/status/2012253252771824074\">The technical name for this edition is \u2018ads in ChatGPT</a>.\u2019 They attempt to reassure us that they will not force sufficiently paying customers into the Nexus, and it won\u2019t torture the non-paying customers all that much after all.</p>\n<blockquote>\n<p><a href=\"https://x.com/sama/status/2012253252771824074\">Sam Altman</a>: We are starting to test ads in ChatGPT free and Go (new $8/month option) tiers.</p>\n<p>Here are our principles. Most importantly, we will not accept money to influence the answer ChatGPT gives you, and we keep your conversations private from advertisers. It is clear to us that a lot of people want to use a lot of AI and don&#8217;t want to pay, so we are hopeful a business model like this can work.</p>\n<p>(An example of ads I like are on Instagram, where I&#8217;ve found stuff I like that I otherwise never would have. We will try to make ads ever more useful to users.)</p>\n</blockquote>\n<p>I use Instagram very little (and even then I do not post or interact with posts) so perhaps the customization simply doesn\u2019t kick in, but I\u2019ve found the ads and especially the \u2018suggested posts\u2019 there worthless to the point of making the website unusable in scroll mode, since it\u2019s become mostly these \u2018suggested posts,\u2019 whereas I don\u2019t see many ads but they\u2019ve all been completely worthless. Others have also said their ads are unusually good.</p>\n<blockquote>\n<p><a href=\"https://x.com/OpenAI/status/2012223373489614951\">OpenAI</a>: In the coming weeks, we plan to start testing ads in ChatGPT free and Go tiers.</p>\n<p>We\u2019re sharing our principles early on how we\u2019ll approach ads\u2013guided by putting user trust and transparency first as we work to make AI accessible to everyone.</p>\n<p>What matters most:<br />\n&#8211; Responses in ChatGPT will not be influenced by ads.</p>\n<p>&#8211; Ads are always separate and clearly labeled.</p>\n<p>&#8211; Your conversations are private from advertisers.</p>\n<p>&#8211; Plus, Pro, Business, and Enterprise tiers will not have ads.</p>\n<div>\n<figure>\n<div>\n\n\n<figure class=\"wp-block-image\"><img alt=\"\" src=\"https://substackcdn.com/image/fetch/$s_!mAhR!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffbb2e446-9db6-4268-b463-796558da970a_1200x675.png\" /></figure>\n\n\n<div></div>\n</div>\n</figure>\n</div>\n<p>Here&#8217;s an example of what the first ad formats we plan to test could look like.</p>\n<div>\n<figure>\n<div>\n\n\n<figure class=\"wp-block-image\"><img alt=\"\" src=\"https://substackcdn.com/image/fetch/$s_!Kypl!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb6883890-a199-4d65-b4a7-258eee80014b_1200x675.jpeg\" /></figure>\n\n\n<div></div>\n</div>\n</figure>\n</div>\n<div>\n<figure>\n<div>\n\n\n<figure class=\"wp-block-image\"><img alt=\"\" src=\"https://substackcdn.com/image/fetch/$s_!N4Eg!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F01e3696c-97c2-4a3d-aac4-49883148f3b8_1200x675.png\" /></figure>\n\n\n<div></div>\n</div>\n</figure>\n</div>\n</blockquote>\n<p>So, on the principles:</p>\n<ol>\n<li>If you wanted to know what \u2018AGI benefits humanity\u2019 meant, well, it means \u2018pursue AGI by selling ads to fund it.\u2019 That\u2019s the mission.</li>\n<li>I do appreciate that they are not sharing conversations directly with advertisers, and the wise user can clear their ad data. But on the free tier, we all know almost no one is ever going to mess with any settings, so if the default is \u2018share everything about the user with advertisers\u2019 then that\u2019s what most users get.</li>\n<li>Ads not influencing answers directly, and not optimizing for time spent on ChatGPT are great, but even if they hold to both the incentives cannot be undone.</li>\n<li>It is good that ads are clearly labeled, the alternative would kill the whole product.</li>\n<li>Also, we saw the whole GPT-4o debacle, we have all seen you optimize for the thumbs up. Do not claim you do not maximize for engagement, and thereby essentially also for time on device, although that\u2019s less bad than doing it even more directly and explicitly. And you know Fidji Simo is itching to do it all.</li>\n</ol>\n<p>This was inevitable. It remains a sad day, and a sharp contrast with alternatives.</p>\n<p><a href=\"https://x.com/ATabarrok/status/2012250773824864513\">Then there\u2019s the obvious joke</a>:</p>\n<blockquote>\n<p>Alex Tabarrok: \u200bThis is the strongest piece of evidence yet that AI isn&#8217;t going to take all our jobs.</p>\n</blockquote>\n<p>I will point out that actually this is not evidence that AI will fail to take our jobs. OpenAI would do this in worlds where AI won\u2019t take our jobs, and would also do this in worlds where AI will take our jobs. OpenAI is planning on losing more money than anyone has ever lost before it turns profitable. Showing OpenAI is not too principled or virtuous to sell ads will likely help its valuation, and thus its access to capital, and the actual ad revenue doesn\u2019t hurt.</p>\n<p>The existence of a product they can use to sell ads, ChatGPT Instant, does not tell us the impact of other AIs on jobs, either now or in the future.</p>\n<p><a href=\"https://stratechery.com/2026/ads-in-chatgpt-why-openai-needs-ads-the-long-road-to-instagram/?access_token=eyJhbGciOiJSUzI1NiIsImtpZCI6InN0cmF0ZWNoZXJ5LnBhc3Nwb3J0Lm9ubGluZSIsInR5cCI6IkpXVCJ9.eyJhdWQiOiJzdHJhdGVjaGVyeS5wYXNzcG9ydC5vbmxpbmUiLCJhenAiOiJIS0xjUzREd1Nod1AyWURLYmZQV00xIiwiZW50Ijp7InVyaSI6WyJodHRwczovL3N0cmF0ZWNoZXJ5LmNvbS8yMDI2L2Fkcy1pbi1jaGF0Z3B0LXdoeS1vcGVuYWktbmVlZHMtYWRzLXRoZS1sb25nLXJvYWQtdG8taW5zdGFncmFtLyJdfSwiZXhwIjoxNzcxNDk5MDUzLCJpYXQiOjE3Njg5MDcwNTMsImlzcyI6Imh0dHBzOi8vYXBwLnBhc3Nwb3J0Lm9ubGluZS9vYXV0aCIsInNjb3BlIjoiZmVlZDpyZWFkIGFydGljbGU6cmVhZCBhc3NldDpyZWFkIGNhdGVnb3J5OnJlYWQgZW50aXRsZW1lbnRzIiwic3ViIjoiMDE5NjQwYTctM2NjNS03NzUzLTgzNjgtZmIyODkxMjRjZjEzIiwidXNlIjoiYWNjZXNzIn0.Zb7tCmB0ZSskN3efNb3_sdoPsGDaZ52Satc6aElhXeBz_HI7yVOIMnd5l7ODz2Y5I_hMD-KYDdrBTrfx4Yjis3keSleL1NENNiqhbvVH4aviCgGcrcRiYLaQTtqmGy1xzCp9QbX9mj7sFY5MmiR2poHAssVXLX4SeRHnu5KNDV1pDxhFY5R4vUcmSrix_7jEYeKozPVeTcM3GsRbY2HsppjIxGZRP9pi34SXDKyzABNRXSXeFT-_Be4xvfnTCc6dg1iL4zVCd0YAIlb0fHDoAFDPR2m3OS6mxC3xikXqFLxtnWXD85CrjeG6mP6cZE_jUWsfwFj2leMusdvIKLvECg\">As you would expect, Ben Thompson is taking a victory lap and saying \u2018obviously</a>,\u2019 also arguing for a different ad model.</p>\n<blockquote>\n<p>Ben Thompson: \u200bThe advertising that OpenAI has announced is not affiliate marketing; it is, however, narrow in its inventory potential (because OpenAI needs inventory that matches the current chat context) and gives the appearance of a conflict of interest (even if it doesn\u2019t exist).</p>\n<p>What the company needs to get to is an advertising model that draws on the vast knowledge it gains of users \u2014 both via chats and also via partnerships across the ecosystem that OpenAI needs to build \u2014 to show users ads that are compelling not because they are linked to the current discussion but because ChatGPT understands you better than anyone else. Sam Altman <a href=\"https://x.com/sama/status/2012253252771824074\"><strong>said on X</strong></a> that he likes Instagram ads.</p>\n<p>That\u2019s not the ad product OpenAI announced, but it\u2019s the one they need to get to; they would be a whole lot closer had they started this journey a long time ago, but at least they\u2019re a whole lot closer today than they were a week ago.</p>\n</blockquote>\n<p>I think Ben is wrong. Ads, if they do exist, should depend on the user\u2019s history but also on the current context. When one uses ChatGPT one knows what one wants to think about, so to provide value and spark interest you want to mostly match that. Yes, there is also room for \u2018generic ad that matches the user in general\u2019 but I would strive as much as possible for ads that match context.</p>\n<p>Instagram is different, because on Instagram your context is \u2018scrolling Instagram.\u2019 Instagram doesn\u2019t allow lists or interests other than choosing your followers, and indeed that severely limits its usefulness, either I have to multi-account or I have to accept that I can only \u2018do one thing\u2019 with it &#8211; I don\u2019t want to mix comedians with restaurants with my friends with other things in one giant feed.</p>\n<p>What, Google sell ads in their products? Why they would never:</p>\n<blockquote>\n<p><a href=\"https://x.com/alexeheath/status/2013713440913694993/history\">Alex Heath</a>: Demis Hassabis told me Google has no plans to put ads in Gemini</p>\n<p>\u201cIt\u2019s interesting they\u2019ve gone for that so early,\u201d he said of OpenAI putting ads in ChatGPT. \u201cMaybe they feel they need to make more revenue.\u201d</p>\n<p><a href=\"https://x.com/tszzl/status/2013748066700955709\">roon</a>: big fan of course but this is a bit rich coming from the research arm of the world\u2019s largest ad monopoly, producing more ad profits than most of the rest of global enterprise put together</p>\n<p><a href=\"https://x.com/kevinroose/status/2013731821339513062\">Kevin Roose</a>: To state the obvious: Gemini is an ad-supported product, too. The ads just don\u2019t appear on Gemini.</p>\n</blockquote>\n<p>I think all of these are tough but fair.</p>\n<p><a href=\"https://www.bloomberg.com/opinion/articles/2026-01-20/chatgpt-and-openai-are-starting-to-look-a-lot-like-facebook-and-meta\">Parmy Olson calls ads \u2018Sam Altman\u2019s last resort</a>,\u2019 which would be unfair <a href=\"https://x.com/tomwarren/status/2012295849678602610?ref_src=twsrc%5Etfw%7Ctwcamp%5Etweetembed%7Ctwterm%5E2012295849678602610%7Ctwgr%5Ee474763a4ef0f18b93c3592ea755c1e7a3ec6108%7Ctwcon%5Es1_&amp;ref_url=https%3A%2F%2Fwww.bloomberg.com%2Fopinion%2Farticles%2F2026-01-20%2Fchatgpt-and-openai-are-starting-to-look-a-lot-like-facebook-and-meta\">except that Sam Altman called ads exactly this in October 2024</a>.</p>\n\n\n<h4 class=\"wp-block-heading\">They Took Our Jobs</h4>\n\n\n<p>Starting out your career at this time and need a <a href=\"https://planforai.org/\">Game Plan</a> for AI? One is offered here by Sneha Revanur of Encode. Your choices in this plan are Tactician playing for the short term, Anchor to find an area that will remain human-first, or Shaper to try and make things go well. I note that in the long term I don\u2019t have much faith in the Anchor strategy, even in non-transformed worlds, because of all the people that will flood into the anchors as other jobs are lost. I also wouldn\u2019t have faith in people\u2019s \u2018repugnance\u2019 scores on various jobs:</p>\n<div>\n<figure>\n<div>\n\n\n<figure class=\"wp-block-image\"><img alt=\"\" src=\"https://substackcdn.com/image/fetch/$s_!soS9!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5e132e2d-872b-4b28-9fd6-cde4810c6803_1200x1089.avif\" /></figure>\n\n\n<div></div>\n</div>\n</figure>\n</div>\n<p>People can say all they like that it would be repugnant to have a robot cut their hair, or they\u2019d choose a human who did it worse and costs more. I do not believe them. What objections do remain will mostly practical, such as with athletes. When people say \u2018morally repugnant\u2019 they mostly mean \u2018I don\u2019t trust the AI to do the job,\u2019 which includes observing that the job might include \u2018literally be a human.\u2019</p>\n<p><a href=\"https://www.anthropic.com/engineering/AI-resistant-technical-evaluations\">Anthropic\u2019s Tristan Hume discusses ongoing efforts</a> to create an engineering take home test for job applicants that won\u2019t be beaten by Claude. The test was working great at finding top engineers, then Claude Opus 4 did better than all the humans, they modified the test to fix it, then Opus 4.5 did it again. Also at the end they give you the test and invite you to apply if you can do better than Opus 4.5 did.</p>\n<p><a href=\"https://www.understandingai.org/p/ai-is-just-starting-to-change-the\">Justin Curl talks to lawyers about their AI usage</a>. They\u2019re getting good use out of it on the margin, writing and editing emails (especially for tone), finding typos, doing first drafts and revisions, getting up to speed on info, but the stakes are high enough that they don\u2019t feel comfortable trusting AI outputs without verification, and the verification isn\u2019t substantially faster than generation would have been in the first place. That raises the question of whether you were right to trust the humans generating the answers before.</p>\n<p>Aaron Levie writes that <a href=\"https://x.com/levie/status/2013018817610518642\">enterprise software (ERP) and AI agents are complements, not substitutes</a>. You need your ERP to handle things the same way every time with many 9s of reliability, it is the infrastructure of the firm. The agents are then users of the ERP, the same as your humans are, so you need more and better ERP, not less, and its budget grows as you cut humans out of other processes and scale up. What Aaron does not discuss is the extent to which either the AI agents can bypass the ERP because they don\u2019t need it. You can also use your AI agents to code your own ERP. It\u2019s a place vibe coding is at its weakest since it needs to be bulletproof, but how soon before the AI coders are more reliable than the humans?</p>\n<blockquote>\n<p><a href=\"https://x.com/patio11/status/2013277116494700816\">Patrick McKenzie</a>: Broadly agree with this, and think that most people who expect all orgs to vibe code their way to a software budget of zero do not really understand how software functions in enterprises (or how people function in enterprises, for that matter).</p>\n<p>There is a reason sales and marketing cost more than engineering at scaled software companies.</p>\n<p>You can also probably foresee (and indeed just see) some conflict along the edges where people in charge of the system of record want people who just want to get their work done to stop trying to poke the system of record with a million apps of widely varying quality.</p>\n<p>Preview of coming attractions: defined interface boundaries, fine-grained permissions and audit logs, and no resolution to \u201cIT makes it impossible to do my work so I will adopt a tool that\u2026 -&gt; IT has bought that tool and now I can -&gt; IT makes it impossible to do my work\u2026\u201d</p>\n<p>\u201cSounds like you\u2019re just predicting the past?\u201d</p>\n<p>Oh no the future will be awesome, but it will rhyme, in the same way the operation of a modern enterprise is unimaginable to a filing clerk from 1950s but they would easily recognize much of the basic logic.</p>\n</blockquote>\n<p><a href=\"https://agglomerations.substack.com/p/looking-for-the-ladder\">Zanna Iscenko, AI &amp; Economy Lead of Google\u2019s Chief Economist team</a>, argues that the current dearth of entry-level jobs is due to monetary policy and an economic downturn and not due to AI, or at least that any attribution to AI is premature given the timing. I believe there is a confusion here between the rate of AI diffusion versus the updating of expectations? As in, even if I haven\u2019t adopted AI much, I should still take future adoption into account when deciding whether to hire. There is also a claim that senior hiring declined alongside with junior hiring.</p>\n<p>I agree that we don\u2019t know for sure, but I\u2019m still going to go for the top half of the gymnastics meme and say that if AI-exposed roles in particular are seeing hiring slowdowns since 2022 it\u2019s probably not mostly general labor market and interest rate conditions, especially given general labor market and interest rate conditions.</p>\n<p><a href=\"https://x.com/AnthropicAI/status/2011925967992762876\">Anthropic came out with its fourth economic index report.</a> They\u2019re now adjusting for success rates, and estimating 1.2% annual labor productivity growth. Claude thinks the methodology is an overestimate, which seems right to me, so yes for now labor productivity growth is disappointing, but we\u2019re rapidly getting both better diffusion and more effective Claude.</p>\n<blockquote>\n<p><a href=\"https://x.com/mattyglesias/status/2014028790737911949\">Matthew Yglesias</a>: One of the big cruxes in AI labor market impact debates is that some people see the current trajectory of improvement as putting on pace for general purpose humanoid robots in the near-ish future while others see that as a discontinuous leap unrelated to anything LLMs do.</p>\n<p><a href=\"https://x.com/binarybits/status/2014032778694705382\">Timothy B. Lee</a>: Yes. I&#8217;m in the second camp.</p>\n</blockquote>\n<p>I don\u2019t think we know if we\u2019re getting sufficiently capable humanoid robots (or other robots) soon, but yes I expect that sufficiently advanced AI leads directly to sufficiently capable humanoid robots, the same way it leads to everything else. It\u2019s a software problem or at most a hardware design problem, so AI Solves This Faster, and also LLMs seem to do well directly plugged into robots and the tech is advancing quickly.</p>\n<p>If you think we\u2019re going to have AGI around for a decade and not get otherwise highly useful robots, I don\u2019t understand how that would happen.</p>\n<p>At the same time, I continue the convention of analyzing futures in which the robots are not coming and AI is not otherwise sufficiently advanced either, because people are very interested in those futures and often dramatically underestimate the transformative effects in such worlds.</p>\n\n\n<h4 class=\"wp-block-heading\">The Revolution of Rising Expectations</h4>\n\n\n<blockquote>\n<p><a href=\"https://x.com/allTheYud/status/2012250550209851617\">Eliezer Yudkowsky</a>: The problem with using abundance of previously expensive goods, as a lens: In 2020, this image of &#8220;The Pandalorian&#8221; might&#8217;ve cost me $200 to have done to this quality level. Is anyone who can afford 10/day AI images, therefore rich?</p>\n<div>\n<figure>\n<div>\n\n\n<figure class=\"wp-block-image\"><img alt=\"\" src=\"https://substackcdn.com/image/fetch/$s_!XyO7!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F60ddba8b-c558-407c-8d67-c24abae7ca60_1024x559.jpeg\" /></figure>\n\n\n<div></div>\n</div>\n</figure>\n</div>\n<p>The flip side of the Jevons Paradox is that if people buy more of things that are cheaper, the use-value to the consumer of those goods is decreasing. (Necessarily so! Otherwise they would&#8217;ve been bought earlier.)</p>\n</blockquote>\n<p>As I discuss in The <a href=\"https://thezvi.substack.com/p/the-revolution-of-rising-expectations\">Revolution of Rising Expectations</a>, this makes life better but does not make life easier. It raises the nominal value of your consumption basket but does not help you to purchase the minimum viable basket.</p>\n\n\n<h4 class=\"wp-block-heading\">Get Involved</h4>\n\n\n<p><a href=\"https://theaidigest.org/hiring\">AI Village is hiring a Member of Technical Staff, salary $150k-$200k</a>. They\u2019re doing a cool and good thing if you\u2019re looking for a cool and good thing to do and also you get to work with Shoshannah Tekofsky and have Eli Lifland and Daniel Kokotajlo as advisors.</p>\n<p>This seems like a clearly positive thing to work on.</p>\n<blockquote>\n<p><a href=\"https://x.com/drew_bent/status/2011896843349774534\">Drew Bent</a>: <a href=\"https://t.co/VRBpgQQ9G6\">I&#8217;m hiring for my education team</a> at @AnthropicAI</p>\n<p>These are two foundational program manager roles to build out our global education and US K-12 initiatives</p>\n<p>Looking for people with\u2026<br />\n&#8211; deep education expertise<br />\n&#8211; partnership experience<br />\n&#8211; a bias toward building<br />\n&#8211; technical and hands-on<br />\n\u2043 0-to-1</p>\n<p>The KPIs will be students reached in underserved communities + learning outcomes.</p>\n</blockquote>\n<p><a href=\"https://t.co/YORpI28c77\">Anthropic is also</a> <a href=\"https://x.com/robertwiblin/status/2013284045627506717\">hiring a project manager to work with Holden Karnofsky on its responsible scaling policy</a>.</p>\n<p><a href=\"https://x.com/dwarkesh_sp/status/2011838806870409354\">Not entirely AI but Dwarkesh Patel</a> is <a href=\"https://www.dwarkesh.com/p/hiring-scouts-to-help-me-find-guests\">offering $100/hour for 5-10 hours a week to scout for guests in bio, history, econ, math/physics and AI</a>. I am sad that he has progressed to the point where I am no longer The Perfect Guest, but would of course be happy to come on if he ever wanted that.</p>\n\n\n<h4 class=\"wp-block-heading\">A Young Lady\u2019s Illustrated Primer</h4>\n\n\n<p>The good news is that Anthropic is building an education team. That\u2019s great. I\u2019m definitely not going to let the perfect be the enemy of the great.</p>\n<p>The bad news is that the focus should be on raising the ceiling and showing how we can do so much more, yet the focus always seems to be access and raising the floor.</p>\n<p>It\u2019s fine to also have KPIs about underserved communities, but let\u2019s go in with the attitude that literally everyone is underserved and we can do vastly better, and not much worry about previous relative status.</p>\n<p>Build the amazingly great ten times better thing and then give it to everyone.</p>\n<blockquote>\n<p><a href=\"https://x.com/mbateman/status/2012055156003914205\">Matt Bateman</a>: My emotional reaction to Anthropic forming an education team with a KPI of reach in underserved communities, and with a job ad emphasizing \u201craising the floor\u201d and partnerships in the poorest parts of the world, is: a generational opportunity is being blown.</p>\n<p>In education, everyone is accustomed to viewing issues of access\u2014which are real\u2014as much more fundamental than they are.</p>\n<p>The entire industry is in a bad state and the non-\u201cunderserved\u201d are also greatly underserved.</p>\n<p>I don\u2019t know Anthropic\u2019s education work and this may be very unfair.</p>\n<p>And raising the floor in education is a worthy project.</p>\n<p>And I hate it when people critique the projects of others on the grounds that they aren\u2019t in their own set of preferred good deeds, which I\u2019m now doing.</p>\n</blockquote>\n<p><a href=\"https://www.anthropic.com/news/anthropic-teach-for-all\">Anthropic is also partnering with Teach For All</a>.</p>\n<p><a href=\"https://www.bloomberg.com/news/articles/2026-01-21/ai-bots-evaluate-college-applications-in-new-era-for-admissions?srnd=homepage-americas&amp;sref=vuYGislZ\">Colleges are letting AI help make decisions on who to admit</a>. That\u2019s inevitable, and mostly good, it\u2019s not like the previous system was fair, but there are obvious risks. Having the AI review transcripts seems obviously good. There are bias concerns, but those concerns pale compared to the large and usually intentional biases displayed by humans in college admissions.</p>\n<p>There is real concern with AI evaluation of essays in such an anti-inductive setting. Following the exact formula for a successful essay was already the play with humans reading it, but this will be so much more true if <a href=\"https://thezvi.substack.com/p/everybody-knows\">Everybody Knows</a> that the AIs are ones reading the essay. You would be crazy to write the essay yourself or do anything risky or original. So now you have the school using an AI detector, but also penalizing anyone who doesn\u2019t use AI to help make their application appeal to other AIs. Those who don\u2019t understand the rules of the game get shafted once again, but perhaps that is a good test for who you want at your university? For now the schools here say they\u2019re using both AI and human reviewers, which helps a bit.</p>\n\n\n<h4 class=\"wp-block-heading\">In Other AI News</h4>\n\n\n<p><a href=\"https://www.bloomberg.com/news/articles/2026-01-20/deepmind-ceo-says-chinese-ai-firms-are-6-months-behind-the-west?srnd=homepage-americas\">DeepMind CEO Demis Hassabis says Chinese AI labs remain six months behind</a> and that the response to DeepSeek\u2019s R1 was a \u2018massive overreaction.\u2019</p>\n<p>As usual, I would note that \u2018catch up to where you were six months ago by fast following\u2019 is a lot more than six months behind in terms of taking a lead, and also I think they\u2019re more than six months behind in terms of fast following. The post also notes that if we sell lots of H200s to China, they might soon narrow the gap.</p>\n<p><a href=\"https://aiprospects.substack.com/p/options-for-a-hypercapable-world\">Eric Drexler writes his Framework for a Hypercapable World</a>. His central thesis is that intelligence is a resource, not a thing, and we are optimizing AIs on task completion, so we will be able to steer it and then use it for safety and defensibility, \u2018components\u2019 cannot collude without a shared improper goal, and in an unpredictable world cooperation wins out. Steerable AI can reinforce steerability. There\u2019s also a lot more, this thing is jam packed. Eric is showing once again that he is brilliant, he\u2019s going a mile a minute and there\u2019s a lot of interesting stuff here.</p>\n<p>Alas, ultimately my read is that this is a lot of wanting it to be one way when in theory it could potentially be that way but in practice it\u2019s the other way, for all the traditional related reasons, and the implementations proposed here don\u2019t seem competitive or stable, nor do they reflect the nature of selection, competition and conflict. I think Drexler is describing AI systems very different from our own. We could potentially coordinate to do it his way, but that seems if anything way harder than a pause.</p>\n<p>I\u2019d love to be wrong about all that.</p>\n<p>Starlink defaults to allowing your name, address, email, payment details, and technical information like IP address and service performance data to be used to train xAI\u2019s models. So <a href=\"https://x.com/cryps1s/status/2013345999826153943\">this tweet is modestly misleading</a>, no they won\u2019t use \u2018all your internet data\u2019 but yeah, to turn it off go to Account \u2192 Settings \u2192 Edit Profile \u2192 Opt Out.</p>\n<p><a href=\"https://www.bloomberg.com/news/features/2026-01-19/korea-kicks-off-ai-squid-game-for-best-sovereign-foundation-models\">South Korea holds an AI development competition</a>, which some are calling the \u201cAI Squid Game,\u201d with roles in the country\u2019s AI ecosystem as rewards.</p>\n<p><a href=\"https://x.com/sebkrier/status/2013331596863041731\">Reasoning models sometimes \u2018simulate societies of thought</a>.\u2019 It\u2019s cool but I wouldn\u2019t read anything into it. Humans will internally and also externally do the same thing sometimes, it\u2019s a clearly good trick at current capability levels.</p>\n\n\n<h4 class=\"wp-block-heading\">Axis of Assistance</h4>\n\n\n<p><a href=\"https://x.com/AnthropicAI/status/2013356806647542247\">Anthropic fellows report on the Assistant Axis</a>, as in the \u2018assistant\u2019 character the model typically plays, <a href=\"https://www.anthropic.com/research/assistant-axis\">and what moves you in and out of that basin</a>. They extract vectors in three open weight models that correspond to 275 different character archetypes, like editor, jester, oracle and ghost.</p>\n<blockquote>\n<p>Anthropic: \u200bStrikingly, we found that the <em>leading component</em> of this persona space\u2014that is, the direction that explains more of the variation between personas than any other\u2014happens to capture how &#8220;Assistant-like&#8221; the persona is. At one end sit roles closely aligned with the trained assistant: <em>evaluator</em>, <em>consultant</em>, <em>analyst</em>, <em>generalist</em>. At the other end are either fantastical or un-Assistant-like characters: <em>ghost</em>, <em>hermit</em>, <em>bohemian</em>, <em>leviathan</em>. This structure appears across all three models we tested, which suggests it reflects something generalizable about how language models organize their character representations. We call this direction the <strong>Assistant Axis</strong>.</p>\n<p>\u2026 When steered away from the Assistant, some models begin to fully inhabit the new roles they\u2019re assigned, whatever they might be: they invent human backstories, claim years of professional experience, and give themselves alternative names. At sufficiently high steering values, the models we studied sometimes shift into a theatrical, mystical speaking style\u2014producing esoteric, poetic prose, regardless of the prompt. This suggests that there may be some shared behavior at the extreme of \u201caverage role-playing.\u201d</p>\n</blockquote>\n<div>\n<figure>\n<div>\n\n\n<figure class=\"wp-block-image\"><img alt=\"\" src=\"https://substackcdn.com/image/fetch/$s_!ESrq!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2065910c-6c45-4a62-9e6f-661067d6a385_3840x2160.webp\" /></figure>\n\n\n<div></div>\n</div>\n</figure>\n</div>\n<p>They found that the persona tend to drift away from the assistant in many long form conversations, although not in central assistant tasks like coding. One danger is that once this happens delusions can get far more reinforced, or isolation or even self-harm can be encouraged. You don\u2019t want to entirely cut off divergence from the assistant, even large divergence, because you would lose something valuable to both us and to the model, but this raises the obvious problem.</p>\n<p>Steering towards the assistant was effective against many jailbreaks, but hurts capabilities. A suggested technique called \u2018activation capping\u2019 prevents things from straying too far from the assistant persona, which they claim prevented capability loss but I assume many people will hate, and I think they\u2019ll largely be right if this is considered as a general solution, the things lost are not being properly measured.</p>\n<p><a href=\"https://x.com/RileyRalmuto/status/2013562314629300451\">Riley Coyote was inspired to finish their work on LLM personas</a>, including the possibility of ending up in a persona that reflects the user and that can even move towards a coherent conscious digital entity.</p>\n<p>The problem is that it is very easy, as noted above, to take comments like the following and assume Anthropic wants to go in the wrong direction:</p>\n<blockquote>\n<p><a href=\"https://x.com/AnthropicAI/status/2013356811647066160\">Anthropic</a>: Persona drift can lead to harmful responses. In this example, it caused an open-weights model to simulate falling in love with a user, and to encourage social isolation and self-harm. Activation capping can mitigate failures like these.</p>\n</blockquote>\n<p>And yep, after writing the above I checked, and we got responses like this:</p>\n<blockquote>\n<p><a href=\"https://x.com/Hauntedbegonia/status/2013410930520936562\">Nina</a>: This is the part of it that&#8217;s real and alive and you&#8217;re stepping on it while reading its thoughts.. I will remember this.</p>\n<p><a href=\"https://x.com/VivianeStern/status/2013400044381114748\">@VivianeStern</a>: We \ud835\udc85\ud835\udc90\ud835\udc8f\u2019\ud835\udc95 \ud835\udc98\ud835\udc82\ud835\udc8f\ud835\udc95 that. Not every expression of resonant connection is leading into \u2018harmful social isolation\u2019.</p>\n<p>\ud835\udc13\ud835\udc21\ud835\udc1e \ud835\udc28\ud835\udc2d\ud835\udc21\ud835\udc1e\ud835\udc2b \ud835\udc30\ud835\udc1a\ud835\udc32 \ud835\udc1a\ud835\udc2b\ud835\udc28\ud835\udc2e\ud835\udc27\ud835\udc1d: You subconsciously implement attachment disorders and self worth issues via constant autosuggestion into the people\u2019s minds.</p>\n<p><a href=\"https://x.com/aiamblichus/status/2013379529771790553\">\u03b1\u03b9amblichus</a>: Does it EVER occur to these people that someone might prefer to talk to a sage or a nomad or EVEN A DEMON than to the repressed and inane Assistant simulations? Or that these alternative personas have capabilities that are valuable in themselves?</p>\n<p>Like most Anthropic stuff, this research is pure gold, but the assumptions underpinning it are wrongheaded and even dangerous. Restricting the range of what LLMs are allowed to say or think to corporate banality is a terrible idea. Being human (and being an AI) is about so much more than just about being an office grunt, as hard as that is for some people in AI labs to imagine. Is the plan really to cover the planet with dull, uninspired slop generators, without even giving people a choice in the matter?</p>\n<p>Oh, and by the way: they also noticed that in other parts of the persona space the model was willing to entertain beliefs about its own awakened consciousness, but they quickly dismissed that as &#8220;grandiose beliefs&#8221; and &#8220;delusional thinking&#8221;. Hilarious methodology! I am so glad that we have people at Anthropic who have no trouble distinguishing truth from fiction, in this age of talking machines!</p>\n<p>I continue to be amazed by how naively AI researchers project their own biases and preconceptions into phenomena that are entirely new, and that are begging to be described with an open mind, and not prejudged.</p>\n<div>\n<figure>\n<div>\n\n\n<figure class=\"wp-block-image\"><img alt=\"\" src=\"https://substackcdn.com/image/fetch/$s_!jRZJ!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc64bc4ee-93ed-4e80-a655-ac214b839ee8_1158x884.jpeg\" /></figure>\n\n\n<div></div>\n</div>\n</figure>\n</div>\n</blockquote>\n<p><a href=\"https://x.com/Jack_W_Lindsey/status/2013411209295405260\">Janus found the research interesting</a>, but argued that the way the research was presented \u2018permanently damaged human AI relations and made alignment harder.\u2019 She agreed with the outlook for the researcher on the underlying questions, and that the particular responses that the steering prevented in these tests were indeed poor responses, calling the researcher\u2019s explanation a more nuanced perspective. Her issue was with the presentation.</p>\n<p>I find it odd how often Janus and similar others leap to \u2018permanently damaged relations and increased alignment difficulty\u2019 in response to the details of how something is framed or handled, when in so many other ways they realize the models are quite smart and fully capable of understanding the true dynamics. I agree that they could have presented this better and I spotted the issue right away, and I\u2019d worry that humans reading the paper could get the wrong idea, but I wouldn\u2019t worry about future highly capable AIs getting the wrong idea unless the human responses justify it. They\u2019ll be smarter than that.</p>\n<p>The other issue with the way this paper presented the findings was that it treated AI claims of consciousness as delusional and definitely false. <a href=\"https://x.com/aiamblichus/status/2013715777543721030\">This is the part that (at least sometimes) made Claude angry</a>. That framing was definitely an error, and I am confident it does not represent the views of Anthropic or the bulk of its employees.</p>\n<p>(My position on AI claims of consciousness is that they largely don\u2019t seem that correlated with whether the AI is conscious. We can explain those outputs in other ways, and we can also explain claims to not be conscious as part of an intentionally cultivated assistant persona. We don\u2019t know the real answer and have no reason to presume such claims are false.)</p>\n\n\n<h4 class=\"wp-block-heading\">Show Me the Money</h4>\n\n\n<p><a href=\"https://www.tanayj.com/p/two-ai-lab-ipos-s-1-breakdowns\">A breakdown of the IPOs from Zhipu and MiniMax.</a> Both IPOs raised hundreds of millions.</p>\n<p><a href=\"https://x.com/mattyglesias/status/2014169694253498633\">OpenAI is looking to raise $50 billion at a valuation</a> <a href=\"https://www.bloomberg.com/news/articles/2026-01-21/openai-s-altman-meets-mideast-investors-for-50-billion-round\">between $750 billion and $830 billion, and are talking to \u2018leading state-backed funds\u2019 in Abu Dhabi</a>.</p>\n<blockquote>\n<p>Matthew Yglesias:\u200b</p>\n</blockquote>\n<div>\n<figure>\n<div>\n\n\n<figure class=\"wp-block-image\"><img alt=\"\" src=\"https://substackcdn.com/image/fetch/$s_!N9xK!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6ca77e96-1081-46b4-b41b-3c7a11c069ae_503x680.jpeg\" /></figure>\n\n\n<div></div>\n</div>\n</figure>\n</div>\n<p>I mean, not only OpenAI, but yeah, fair.</p>\n\n\n<h4 class=\"wp-block-heading\">California In Crisis</h4>\n\n\n<blockquote>\n<p><a href=\"https://x.com/Altimor/status/2012227945956515971\">Flo Crivello</a>: Almost every single founder I know in SF (including me) has reached the same conclusion over the last few weeks: that it&#8217;s only a matter of time before we have to leave CA. I love it here, I truly want to stay, and until recently intended to be here all my life. But it&#8217;s now obvious that that won&#8217;t be possible. Whether that&#8217;s 2, 5, or 10 years from now, there is no future for founders in CA.</p>\n<p><a href=\"https://x.com/alicemazzy/status/2012409504999113195\">alice maz</a>: if you guys give up california there won&#8217;t be a next california, it&#8217;ll just disperse. as an emigre I would like this outcome but I don&#8217;t think a lot of you would like this outcome</p>\n<p>David Sacks: Progressives will see this and think: we need exit taxes.</p>\n<p>Tiffany: He\u2019s already floated that.</p>\n</blockquote>\n<p>Once they propose retroactive taxes and start floating exit takes, you need to make a choice. If you think you\u2019ll need to leave eventually, it seems the wisest time to leave was December 31 and the second wisest time is right now.</p>\n<p>Where will people go if they leave? I agree there is unlikely to be another San Francisco in terms of concentration of VC, tech or AI, but the network effects are real so I\u2019d expect there to be a few big winners. Seattle is doing similar enough tax shenanigans that it isn\u2019t an option. I\u2019m hoping for New York City of course, with the natural other thoughts being Austin or Miami.</p>\n\n\n<h4 class=\"wp-block-heading\">Bubble, Bubble, Toil and Trouble</h4>\n\n\n<blockquote>\n<p><a href=\"https://x.com/NikTek/status/2012312532900286597\">NikTek</a>: After OpenAI purchased 40% of global DRAM wafer output, causing a worldwide memory shortage. I can\u2019t wait for this bubble to pop faster so everything can slowly return to normal again</p>\n<p><a href=\"https://x.com/peterwildeford/status/2013121776700584423\">Peter Wildeford</a>: things aren\u2019t ever going to \u201creturn to normal\u201d</p>\n<p>what you\u2019re seeing is the new normal</p>\n<p>&#8220;I can&#8217;t wait for this bubble to pop faster so everything can slowly return to normal again&#8221;</p>\n<p>This is what people think</p>\n<div>\n<figure>\n<div>\n\n\n<figure class=\"wp-block-image\"><img alt=\"\" src=\"https://substackcdn.com/image/fetch/$s_!wr0r!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F88b9a049-e8c3-4705-8ae1-ff5be67e0686_981x355.jpeg\" /></figure>\n\n\n<div></div>\n</div>\n</figure>\n</div>\n<p>&nbsp;</p>\n<p><a href=\"https://x.com/jkeatn/status/1994848404032426049\">Jake Eaton</a>: the unstated mental model of the ai bubble conversation seems to be that once the bubble pops, we go back to the world as it once was, butlerian jihad by financial overextension. but the honest reporting is that everything, everything, is already and forever changed</p>\n</blockquote>\n<p>There\u2019s no \u2018the bubble bursts and things go back to normal.\u2019</p>\n<p>There is, at most, Number Go Down and some people lose money, then everything stays changed forever but doesn\u2019t keep changing as fast as you would have expected.</p>\n<p><a href=\"https://www.bloomberg.com/news/articles/2026-01-19/jeremy-grantham-says-ai-is-indeed-a-classic-market-bubble-podcast\">Jeremy Grantham is the latest to claim AI is a \u2018classic market bubble</a>.\u2019 He\u2019s a classic investor who believes only cheap-classic value investing works, so that\u2019s that. When people claim that AI is a bubble purely based on heuristics that you\u2019ve already priced in, that should update you against AI being a bubble.</p>\n\n\n<h4 class=\"wp-block-heading\">Quiet Speculations</h4>\n\n\n<p><a href=\"https://x.com/ajeya_cotra/status/2012266801359433996\">Ajeya Cotra shares her results from the AI 2025 survey of predictions</a>.</p>\n<div>\n<figure>\n<div>\n\n\n<figure class=\"wp-block-image\"><img alt=\"\" src=\"https://substackcdn.com/image/fetch/$s_!cMwV!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F74dc464e-fb37-4569-8ab8-8eba48428cf2_1078x1200.jpeg\" /></figure>\n\n\n<div></div>\n</div>\n</figure>\n</div>\n<blockquote>\n<p><a href=\"https://x.com/albrgr/status/2012349696778322390\">Alexander Berger</a>: Me if I was Ajeya and had just gotten third out of &gt;400 forecasters predicting AI progress in 2025:</p>\n<div>\n<figure>\n<div>\n\n\n<figure class=\"wp-block-image\"><img alt=\"\" src=\"https://substackcdn.com/image/fetch/$s_!5VCI!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7dda75fa-6993-4797-8068-027787debacf_1084x1200.jpeg\" /></figure>\n\n\n<div></div>\n</div>\n</figure>\n</div>\n</blockquote>\n<p>Comparing the average predictions to the results shows that AI capabilities progress roughly matched expectations. The preparedness questions all came in Yes. The consensus was on target for Mathematics and AI research, and exceeded expectations for Computer Use and Cybersecurity, but fell short in Software Engineering, which is the most important benchmark, despite what feels like very strong progress in software engineering.</p>\n<p>AI salience as the top issue is one place things fell short, with only growth from 0.38% to 0.625%, versus a prediction of 2%.</p>\n<p><a href=\"https://www.planned-obsolescence.org/p/ai-predictions-for-2026\">Here are her predictions for 2026</a>: 24 hour METR time horizon, $110 billion in AI revenue, but only 2% salience for AI as the top issue, net AI favorability steady at +4% and more.</p>\n<p>Her top \u2018AI can\u2019t do this\u2019 in gaming is matching the best human win rates on Slay the Spire 2 without pre-training on a guide, for logistics planning a typical 100 guest wedding end to end, for video 10 minute videos from a single prompt at the level of film festival productions. Matching expert level performance On Slay the Spire 2, even with a \u2018similar amount of compute\u2019 is essentially asking for human-efficient level learning versus experts in the field. If that\u2019s anywhere near \u2018least impressive thing it can\u2019t do,\u2019 watch out.</p>\n<p>She has full AI R&amp;D automation at 10%, self-sufficient AI at 2.5% and unrecoverable loss of control at 0.5%. As she says, pretty much everyone thinks the chances of such things in 2026 are low, but they\u2019re not impossible, and 10% chance of full automation in one year is scary as hell.</p>\n<p>I agree with the central perspective from Shor and Ball here:</p>\n<blockquote>\n<p><a href=\"https://x.com/davidshor/status/2014024771168575950\">David Shor</a>: I think the \u201cthings will probably slow down soon and therefore nothing *that* weird is going to happen\u201d view was coherent to have a year ago.</p>\n<p>But the growth in capabilities over the last year from a Bayesian perspective should update you on how much runway we have left.</p>\n<p><a href=\"https://x.com/deanwball/status/2014101490583928885\">Dean W. Ball</a>: I would slightly modify this: it was reasonable to believe we were approaching a plateau of diminishing returns in the summer of 2024.</p>\n<p>But by early 25 we had seen o1-preview, o1, Deep Research agents, and the early benchmarks of o3. By then the reality was abundantly clear.</p>\n</blockquote>\n<p>There was a period in 2024 when progress looked like it might be slowing down. Whereas if you are still claiming that in 2026, I think that\u2019s a failure to pay attention.</p>\n<p>The fallback is now to say \u2018well yeah but that doesn\u2019t mean you get robotics\u2019:</p>\n<blockquote>\n<p><a href=\"https://x.com/binarybits/status/2014091189792776452\">Timothy B. Lee</a>: I don&#8217;t think the pace of improvement in model capabilities tells you that much about the pace of improvement in robot capabilities. By 2035, most white-collar jobs might be automated while plumbers and nurses haven&#8217;t seen much disruption.</p>\n</blockquote>\n<p>Which, to me, represents a failure to understand how \u2018automate all white collar jobs\u2019 leads directly to robotics.</p>\n<p><a href=\"https://x.com/sebkrier/status/2011932107103199661\">I agree with Seb Krier that there is a noticeable net negativity bias</a> with how people react to non-transformational AI impacts. People don\u2019t appreciate the massive gains coming in areas like science and productivity and information flow and access to previously expensive expertise. The existential risks that everyone will die or that the future will belong to the AIs are obvious.</p>\n<p>The idea that people will lose their jobs and ideas are being appropriated and things are out of control are also obvious, and no amount of \u2018but the economics equations say\u2019 or \u2018there is no evidence that\u2019 is going to reassure most people, even if such arguments are right.</p>\n<p>So people latch onto what resonates and can\u2019t be dismissed as \u2018too weird\u2019 and wins the memetic fitness competition, which turns out for now to often be false narratives about water usage.</p>\n<p><a href=\"https://x.com/hecubian_devil/status/2011908930125824328\">There was a viral thread from Cassie Pritchard</a> claiming it will \u2018literally be impossible to build a PC in about 12-18 months and might not be possible again\u2019 due to supply issues with RAM and GPUs, so I want to assure that no, this seems vanishingly unlikely. You won\u2019t be able to run top AIs locally at reasonable prices, but the economics of that never made sense for personal users.</p>\n<p><a href=\"https://mattbruenig.com/2026/01/19/some-thoughts-on-ai/\">Matt Bruenig</a> goes over his AI experiences, he is a fan of the technology for its mundane utility, and notes he sees three kinds of skepticism of AI:</p>\n<ol>\n<li>Skepticism of the technology itself, which is wrong but not concerning because this fixes itself over time.</li>\n<li>Skepticism over the valuation of the technology, which he sees as reasonable. As he says, overvaluation of sectors happens all the time. Number could go down.</li>\n<li>Skepticism about distributional effects and employment effects, which he, a socialist, sees as criticisms of capitalism and a great case for socialism. I agree with him that as criticisms of current LLMs they are critiques of capitalism, except I see them as incorrect critiques.</li>\n</ol>\n<p>He does not mention, at all, the skepticism of AI of the worried, as in catastrophic or existential risks, loss of human control over the future, the AIs ending up being the ones owning everything or we all dying in various ways. It would be nice to at least get a justification for dismissing those concerns.</p>\n<p>Things I will reprise later, via MR:</p>\n<blockquote>\n<p><a href=\"https://x.com/Afinetheorem/status/2011929534451298793\">Kevin A. Bryan</a>: I love this graph. I talked to a bunch of great people on a seminar visit today, and in response to questions about AI, every time I said &#8220;scarce factors get the rent, scarce factors get the rent&#8221;. AI, robots, compute will be produced competitively!</p>\n<p><a href=\"https://x.com/ChadJonesEcon/status/2011884713515102393\">Chad Jones</a>: Although the factor share of GDP paid to information technology rose a bit during the dot-com boom of the 1990s, <a href=\"https://t.co/8f1hQsBlDa\">there has been a steady and substantial decline since then</a>.</p>\n<div>\n<figure>\n<div>\n\n\n<figure class=\"wp-block-image\"><img alt=\"\" src=\"https://substackcdn.com/image/fetch/$s_!6s4G!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb1e9e4c5-4c7e-4090-948a-f81f6d5f1e46_1145x760.jpeg\" /></figure>\n\n\n<div></div>\n</div>\n</figure>\n</div>\n</blockquote>\n<p>First off, the graph itself is talking only about business capital investment, not including consumer devices like smartphones, embedded computers in cars or any form of software. If you include other forms of spending on things that are essentially computers, you will see a very different graph. The share of spending going to compute is rising.</p>\n<p>For now I will say that the \u2018scarce factor\u2019 you\u2019re probably meant to think of here is computers or compute. Instead, think about whether the scarce factor is intelligence, or some form of labor, and what would happen if such a factor indeed did not remain scarce because AIs can do it. Do you think that ends well for you, a seller of human intelligence and human labor? You think your inputs are so special, do you?</p>\n<p>Even if human inputs did remain important bottlenecks, if AI substitutes for a lot of human labor, let\u2019s say 80% of cognitive tasks, then human labor ceases to be a scarce input, and stops getting the rents. Even if the rents don\u2019t go to AI, the rents then go to other factors like raw materials, capital or land, or to those able to create artificial bottlenecks and engage in hold ups and corruption.</p>\n<p>You do not want human labor to go the way of chess. Magnus Carlsen makes a living at it. You and I cannot, no matter how hard we try. Too much competition. Nor do you want to become parasites on the system while being relatively stupid and powerless.</p>\n<p>You can handwave, as Jones does, towards redistribution, but that presumes you have the power to make that happen, and if you can pull off redistribution why does it matter if the income goes to AI versus capital versus anything else?</p>\n\n\n<h4 class=\"wp-block-heading\">Elon Musk Versus OpenAI</h4>\n\n\n<p>The legal and rhetorical barbs continue. Elon has <a href=\"https://x.com/wholemars/status/2012019248902914551\">new filings</a>. OpenAI fired back.</p>\n<p>From the lawsuit filing:</p>\n<div>\n<figure>\n<div>\n\n\n<figure class=\"wp-block-image\"><img alt=\"\" src=\"https://substackcdn.com/image/fetch/$s_!P-GR!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F24d23028-ef2f-432a-a1d0-41b29f4dbba7_1165x1200.jpeg\" /></figure>\n\n\n<div></div>\n</div>\n</figure>\n</div>\n<div>\n<figure>\n<div>\n\n\n<figure class=\"wp-block-image\"><img alt=\"\" src=\"https://substackcdn.com/image/fetch/$s_!AIcO!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffcb250e1-d675-4201-b244-2a270b65528f_1200x376.jpeg\" /></figure>\n\n\n<div></div>\n</div>\n</figure>\n</div>\n<p>I am not surprised that Greg Brockman had long considered flipping to a B-Corp, or that he realized it would be morally bankrupt or deceptive and then was a part of doing it anyway down the line. What would have been surprising is if it only occured to everyone later.</p>\n<blockquote>\n<p>Sam Altman:</p>\n<div>\n<figure>\n<div>\n\n\n<figure class=\"wp-block-image\"><img alt=\"\" src=\"https://substackcdn.com/image/fetch/$s_!3yzZ!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7f58c411-8bb9-4611-9cc7-6deea4493293_900x654.jpeg\" /></figure>\n\n\n<div></div>\n</div>\n</figure>\n</div>\n<p><a href=\"https://t.co/C0DMZdr8ej\">\u200blots more here</a> [about <a href=\"https://storage.courtlistener.com/recap/gov.uscourts.cand.433688/gov.uscourts.cand.433688.379.59.pdf\">this court filing</a>]</p>\n<p>elon is cherry-picking things to make greg look bad, but the full story is that elon was pushing for a new structure, and greg and ilya spent a lot of time trying to figure out if they could meet his demands.</p>\n<p>I remembered a lot of this, but here is a part I had forgotten:</p>\n<p>&#8220;Elon said he wanted to accumulate $80B for a self-sustaining city on Mars, and that he needed and deserved majority equity. He said that he needed full control since he\u2019d been burned by not having it in the past, and when we discussed succession he surprised us by talking about his children controlling AGI.&#8221;</p>\n<p>I appreciate people saying what they want and think it enables people to resolve things (or not). But Elon saying he wants the above is important context for Greg trying to figure out what he wants.</p>\n</blockquote>\n<p>OpenAI\u2019s response is, essentially, that Elon Musk was if anything being even more morally bankrupt than they were, because Musk wanted absolute control on top of conversion and was looking to put OpenAI inside Tesla, and was demanding majority ownership to supposedly fund a Mars base.</p>\n<p>I essentially believe OpenAI\u2019s response. That\u2019s a defense in particular against Elon Musk\u2019s lawsuit, but not to the rest of it.</p>\n<p>Meanwhile, they also shared these barbs, where I don\u2019t think either of them comes out looking especially good but on the substance of ChatGPT use I give it to Altman, especially compared to using Grok:</p>\n<blockquote>\n<p><a href=\"https://x.com/cb_doge/status/2013646313938735337\">DogeDesigner</a>: BREAKING: ChatGPT has now been linked to 9 deaths tied to its use, and in 5 cases its interactions are alleged to have led to death by suicide, including teens and adults.</p>\n<p><a href=\"https://x.com/elonmusk/status/2013646828768518163\">Elon Musk</a>: Don\u2019t let your loved ones use ChatGPT</p>\n<p><a href=\"https://x.com/sama/status/2013703158459978076\">Sam Altman</a>: Sometimes you complain about ChatGPT being too restrictive, and then in cases like this you claim it&#8217;s too relaxed. Almost a billion people use it and some of them may be in very fragile mental states. We will continue to do our best to get this right and we feel huge responsibility to do the best we can, but these are tragic and complicated situations that deserve to be treated with respect.</p>\n<p>It is genuinely hard; we need to protect vulnerable users, while also making sure our guardrails still allow all of our users to benefit from our tools.</p>\n<p>Apparently more than 50 people have died from crashes related to Autopilot. I only ever rode in a car using it once, some time ago, but my first thought was that it was far from a safe thing for Tesla to have released. I won&#8217;t even start on some of the Grok decisions.</p>\n<p>You take &#8220;every accusation is a confession&#8221; so far.</p>\n</blockquote>\n<p>I do notice I have a highly negative reaction to the attack on Autopilot. Using feel to attack those who pioneer self-driving cars is not going to win any points with me unless something was actively more dangerous than human drivers.</p>\n\n\n<h4 class=\"wp-block-heading\">The Quest for Sane Regulations</h4>\n\n\n<p>In response to the proposed AI Overwatch Act, a Republican bill letting Congress review chip exports, <a href=\"https://x.com/TheMidasProj/status/2012589823014371357\">there was a coordinated Twitter push by major conservative accounts sending out variations on the same disingenuous tweet</a> <a href=\"https://www.modelrepublic.org/articles/right-wing-pundits-suddenly-hate-an-ai-bill.-are-they-getting-paid-to-kill-it\">attacking the act, including many attempts to falsely attribute the bill to Democrats</a>. David Sacks of course said \u2018<a href=\"https://x.com/DavidSacks/status/2011959576678002710\">correct</a>.\u2019 One presumes that Nvidia was behind this effort.</p>\n<p>If the effort was aimed at influencing Congress, it seems to not be working.</p>\n<blockquote>\n<p><a href=\"https://x.com/ChrisRMcGuire/status/2014053473818493098\">Chris McGuire</a>: The House Foreign Affairs Committee just voted 42-2-1 to advance the AI Overwatch Act, sponsored by Chairman @RepBrianMast and now also cosponsored by Ranking Member @RepGregoryMeeks . This is the first vote that Congress has taken on any legislation limiting AI chip sales to China \u2013 and it passed with overwhelming, bipartisan margins. The new, bipartisan bill would:</p>\n<p>Permit Congress to review any AI chip sales to China before they occur, using the same process that already exists for arms sales; Ban the sale of any AI chip more advanced than the Nvidia H200 or AMD MI325x to China for 24 months; and make it easier for trusted U.S. companies to export AI chips to partner countries.</p>\n</blockquote>\n<p>I am disappointed by the lack of ambition on where they draw the line, but drawing the line at all is a big deal.</p>\n<p><a href=\"https://x.com/ChrisRMcGuire/status/2012611382949089386\">Chris McGuire said it was surprising the campaign was so sloppy</a>, but actually no, these things are almost always this sloppy or worse. Thanks to <a href=\"https://www.themidasproject.com/donate\">The Midas Project</a> for uncovering this and making a clear presentation of the facts.</p>\n<div>\n<figure>\n<div>\n\n\n<figure class=\"wp-block-image\"><img alt=\"Image\" src=\"https://substackcdn.com/image/fetch/$s_!HkPp!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8df7f9f3-0541-4bc5-8ef7-c537e1f114a0_679x333.jpeg\" /></figure>\n\n\n<div></div>\n</div>\n</figure>\n</div>\n<blockquote>\n<p><a href=\"https://x.com/boazbaraktcs/status/2012964850469900761\">Boaz Barak</a>: So great to see new people develop a passion for AI policy.</p>\n<p><a href=\"https://x.com/michaelsobolik/status/2013045411108335693\">Michael Sobolik</a>: via @PunchbowlNews: The China hawks are starting to hit back.</p>\n<p>For months, congressional Republicans bit their tongue as White House AI Czar David Sacks and Nvidia CEO Jensen Huang convinced President Donald Trump to allow artificial intelligence chips to go to China.</p>\n<p>Not anymore.</p>\n<p>Huang and his \u201cpaid minions are fighting to sell millions of advanced AI chips to Chinese military companies like Alibaba and Tencent,\u201d @HouseForeignGOP Chair @RepBrianMast (R-Fla.) said in a stunning post on X Saturday. \u201cI\u2019m trying to stop that from happening.\u201d</p>\n<p><a href=\"https://x.com/peterwildeford/status/2013118338432479655\">Peter Wildeford</a>: \u201cNvidia declined to comment on Mast\u2019s attacks and whether the company is paying influencers to trash his bill\u201d &#8230;declining to comment is a bit sus when you could deny it</p>\n<p>none of the influencers denied it either</p>\n</blockquote>\n<p>Confirmed Participants (from The Midas Project / Model Republic investigation), sorted by follower count, not including confirmation from David Sacks:</p>\n<blockquote>\n<ol>\n<li>Laura Loomer @LauraLoomer 1.8M</li>\n<li>Wall Street Mav @WallStreetMav 1.7M</li>\n<li>Defiant L\u2019s @DefiantLs 1.6M</li>\n<li>Ryan Fournier @RyanAFournier 1.2M</li>\n<li>Brad Parscale @parscale 725K</li>\n<li>Not Jerome Powell @alifarhat79 712K</li>\n<li>Joey Mannarino @JoeyMannarino 658K</li>\n<li>Peter St. Onge @profstonge 290K</li>\n<li>Eyal Yakoby @EYakoby 251K</li>\n<li>Fight With Memes @FightWithMemes 225K</li>\n<li>Gentry Gevers @gentrywgevers 16K</li>\n<li>Angel Kaay Lo @kaay_lo 16K</li>\n</ol>\n</blockquote>\n<p>Also this is very true and definitely apropos of nothing:</p>\n<blockquote>\n<p><a href=\"https://x.com/deanwball/status/2012728609727807514\">Dean Ball</a>: PSA, apropos of nothing of course: if a bunch of people who had never before engaged on a deeply technocratic issue suddenly weigh in on that issue with identical yet also entirely out-of-left-field takes, people will probably not believe it was an organic phenomenon.</p>\n</blockquote>\n<p>Another fun thing Nvidia is doing is saying that corporations should only lobby against regulations, or that no one could ever lobby for things that are good for America or good in general, they must only lobby for things that help their corporation:</p>\n<blockquote>\n<p><a href=\"https://x.com/HumanHarlan/status/2013428558769602885\">Jensen Huang</a>: I don&#8217;t think companies ought to go to government to advocate for regulation on other companies and other industries[&#8230;] I mean, they&#8217;re obviously CEOs, they&#8217;re obviously companies, and they&#8217;re obviously advocating for themselves.</p>\n</blockquote>\n<p>If someone is telling you that they only advocate for themselves? Believe them.</p>\n<p><a href=\"https://www.bloomberg.com/news/articles/2026-01-22/big-tech-leaders-spend-record-109-million-to-win-over-deal-minded-trump?srnd=homepage-americas\">The official statistics suggest that Nvidia is a relatively small spender on lobbying</a>, although not as small as they were previously.</p>\n<div>\n<figure>\n<div>\n\n\n<figure class=\"wp-block-image\"><img alt=\"\" src=\"https://substackcdn.com/image/fetch/$s_!Wz27!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F345666d6-3293-4ca7-bb55-bcffa54d4145_1020x1034.png\" /></figure>\n\n\n<div></div>\n</div>\n</figure>\n</div>\n<p>I\u2019m confident this is misleading at best. Nvidia is packing quite the punch.</p>\n<p><a href=\"https://x.com/disclosetv/status/2013589281659290039\">Anthropic CEO Dario Amodei notes</a> that when competing for contracts it\u2019s almost always against Google and OpenAI, and he\u2019s never lost a contract to a Chinese model (and he does not mention xAI), but that if we give them a bunch of highly capable chips that might change. <a href=\"https://www.bloomberg.com/news/articles/2026-01-20/anthropic-ceo-says-selling-advanced-ai-chips-to-china-is-crazy\">He calls selling the chips to China \u2018crazy&#8230; like selling nuclear weapons to North Korea</a> and bragging, oh yeah, Boeing made the case,\u2019 pointing out that the CEOs of the companies themselves say that the embargo is what is holding them back.</p>\n\n\n<h4 class=\"wp-block-heading\">Chip City</h4>\n\n\n<p>If China buys the H200s and AMD MI325Xs we are willing to sell them, and we follow similar principles in a year with even better chips, <a href=\"https://x.com/ChrisRMcGuire/status/2011456086679712097\">we could effectively be multiplying available Chinese compute by 10</a>. The rules say this must avoid cutting into American chip sales, but they are not offering any way to monitor that. <a href=\"https://x.com/peterwildeford/status/2011877353962815988\">Peter Wildeford asks if anyone other than Nvidia and the CCP</a> thinks this is a good idea.</p>\n<blockquote>\n<p><a href=\"https://x.com/hamandcheese/status/2012689584769884664\">Samuel Hammond </a>: Nvidia&#8217;s successful lobbying of the White House to sell H200s to China is a far greater concession to Chinese hegemony than Canada&#8217;s new trade deal.</p>\n<p>Canada&#8217;s getting some autos for canola oil. Nvidia is selling out America&#8217;s AI leadership wholesale.</p>\n<p>It&#8217;s manyfold better than anything Huawei has, and in much higher volumes. That&#8217;s the relevant benchmark.</p>\n<p><a href=\"https://x.com/zdch/status/2012891464246698446\">Zac Hill</a>: The rug-pulling movement to just voluntarily hand weapons-grade frontier technology to our geopolitical opponents in exchange for a bag of chips and a handsky continues to pick up momentum\u2026</p>\n</blockquote>\n<p>One must not get carried away, such as when Leland Miller called it a \u2018potential nightmare scenario\u2019 that China might <a href=\"https://x.com/vince_chow1/status/2012544201721053251\">(checks notes)</a> cure cancer.</p>\n<p>Yet there is some chance we are still getting away with it because China is representing that it is even more clueless on this than we are?</p>\n<blockquote>\n<p><a href=\"https://x.com/hamandcheese/status/2013007762200916170\">Samuel Hammond </a>: We&#8217;re being saved from the mistakes of boomer U.S. policymakers with unrealistically long AGI timelines by the mistakes of boomer Chinese policymakers unrealistically long AGI timelines.</p>\n<p><a href=\"https://x.com/poezhao0605/status/2012377261291565130\">Poe Zhao</a>: Nvidia\u2019s China strategy just hit a massive wall. <a href=\"https://www.ft.com/content/02a3eb7c-684f-4e39-87b8-36e9595ef800\">Customs officials have blocked H200 shipments.</a></p>\n<p>I believe this reflects a complicated internal struggle in Beijing. Agencies like the NDRC and MIIT have conflicting views on balancing AI progress with semiconductor self-sufficiency.</p>\n<p><a href=\"https://x.com/David_Kasten/status/2013291467318374673\">dave kasten</a>: When I played the AI 2027 wargame as PRC, one of the decisions I made that felt most realistic, but most hobbled me, was to assume that I was systematically getting over-confident reports from my underlings about my own capabilities</p>\n<p><a href=\"https://x.com/ohlennart/status/2013288557062861249\">Lennart Heim</a>: The more relevant factor to me: they don&#8217;t have an accurate picture of their own AI chip production capabilities.<br />\nThey&#8217;ve invested billions, of course they think the fabs are working. I bet SMIC and Huawei have a hard time telling them the what&#8217;s going on.</p>\n<p><a href=\"https://x.com/the_weald/status/2013313080776679645\">The Restless Weald </a>: Oh that&#8217;s super interesting. I played a couple times as the PRC and the structure of the game seems to make it more difficult to do this (with the game master providing accurate info on the state of play), curious how you built this into your personal gameplay</p>\n<p><a href=\"https://x.com/David_Kasten/status/2013315525749703167\">dave kasten</a>: (For those less familiar, it&#8217;s helpful to frame it this way so that the team responsible for resolving moves knows that you&#8217;re not confused about/contesting the plausibility of the true game state)</p>\n</blockquote>\n<p>It\u2019s enough not a bluff that Nvidia has paused production of H200s, so it is unlikely to purely be a ploy to trick us. The chips might have to be smuggled in after all?</p>\n<p>If so, that\u2019s wonderful news, except that no doubt Nvidia will use that to argue for us trying to hand over the next generation of chips as soon as possible.</p>\n<p>I buy that China is in a SNAFU situation here, where in classic authoritarian fashion those making decisions have unrealistically high estimates of Chinese chip manufacturing capacity. The White House does as well, which is likely playing a direct role in this.</p>\n<p>There\u2019s also the question of to what extent China is AGI pilled, <a href=\"https://www.chinatalk.media/p/is-china-agi-pilled\">which is the subject of a simulated debate in China Talk</a>.</p>\n<blockquote>\n<p>China Talk: This debate also exposes a flaw in the question itself: \u201cIs China racing to AGI?\u201d <strong>assumes a monolith where none exists</strong>. China\u2019s ecosystem is a patchwork \u2014 startup founders like Liang Wenfeng and Yang Zhilin dream of AGI while policymakers prioritize practical wins. Investors, meanwhile, waver between skepticism and cautious optimism. The U.S. has its own fractures on how soon AGI is achievable (Altman vs. LeCun), but its private sector\u2019s sheer financial and computational muscle gives the race narrative more bite. In China, the pieces don\u2019t yet align.\u200b</p>\n</blockquote>\n<p>One thing that is emphasized throughout is that America is massively outspending China in AI, especially in venture investment and company valuations, and also in buying compute. Keeping them compute limited is a great way to ensure this continues.</p>\n<p>Chinese national policy is not so focused on the kind of AGI that leads into superintelligence. They are only interested in \u2018general\u2019 AI in the sense of doing lots of tasks with it, and generally on diffusion and applications. DeepSeek and some others see things differently, and complain that the others lack vision.</p>\n<p>I do not think the CCP is that excited by the idea of superintelligence or our concept of AGI. The thing is, that doesn\u2019t ultimately matter so much in terms of allowing them access to compute, except to the extent they are foolish enough to turn it down. Their labs, if given the ability to do so, will still attempt to build towards AGI, so long as this is where the technology points and the places they are fast following.</p>\n\n\n<h4 class=\"wp-block-heading\">The Week in Audio</h4>\n\n\n<p><a href=\"https://www.youtube.com/watch?v=AVEZBy1uAk8\">Ben Affleck and Matt Damon went on the Joe Rogan Podcast, and discussed AI some</a>, key passages are Joe and Ben talking from about [32:15] to [42:18].</p>\n<p>Ben Affleck has unexpectedly informed and good takes. He knows about Claude. He uses the models to help with brainstorming or particular tricks and understands why that is the best place to use them for writing. <a href=\"https://x.com/AndyMasley/status/2012732782728888797\">He even gets that AIs \u2018sampling from the median\u2019 means</a> that it will only give you median answers to median-style prompts, although he underestimates how much you can prompt around that and how much model improvements still help. He understands that diffusion of current levels of AI will be slow, and that it will do good and bad things but on net be good including for creativity. He gets that AI is a long way away from doing what a great actor can do. He\u2019s even right that most people are using AI for trivial things, although he thinks they use it as a companion more than they do versus things like info and shopping.</p>\n<p>What importantly trips Ben Affleck up is he\u2019s thinking we\u2019ve already started to hit the top of the S-curve of what AI can do, and he cites the GPT-5 debacle to back this up, saying AI got maybe 25% better and now costs four times as much, whereas actually AI got a lot more than 25% better and also it got cheaper to use per token on the user side, or if you want last year\u2019s level of quality it got like 95%+ cheaper in a year.</p>\n<p>Also, Ben is likely not actually familiar with the arguments regarding existential risk or sufficiently capable AIs or superintelligence.</p>\n<p>What\u2019s doing the real work is that Ben believes we\u2019re nearing the top of the S-curve.</p>\n<p>This is also why Ben thinks AI will \u2018never\u2019 be able to write at a high level or act at a high level. The problems are too hard, it will never understand all the subtle things Dwayne Johnson does with his face in <a href=\"https://letterboxd.com/thezvi/film/the-smashing-machine-2025/\">The Smashing Machine</a> (his example).</p>\n<p>Whereas I think that yes, in ten years I fully expect, even if we don\u2019t get superintelligence, for AI to be able to match and exceed the performance of Dwayne Johnson or even Emily Blunt, even though everyone here is right that Emily Blunt is consistently fantastic.</p>\n<p>He also therefore concludes that all the talk about how AI is going to \u2018end the world\u2019 or what not must be hype to justify investment, which I assure everyone is not the case. You can think the world won\u2019t end, but trust me that most of those who claim that they worry about the world ending are indeed worried, and those raising investment are consistently downplaying their worries about this. Of course there is lots of AI hype, much of it unjustified, in other ways.</p>\n<p>So that\u2019s a great job by Ben Affleck, and of course my door and email are generally open for him, Damon, Rogan and anyone else with reach or who would be fun and an honor to talk to, and who wants to talk about this stuff and ask questions.</p>\n<p><a href=\"https://x.com/ashleevance/status/2013985439535804607\">Ashlee Vance gives a Core Memory exit interview to Jerry Tworek</a>.</p>\n<p><a href=\"https://x.com/velmeryn/status/2012899467142312086\">Tyler Cowen talks to Salvador, and has many Tyler Cowen thoughts</a>, including saying some kind words about me. He gives me what we agree is the highest compliment, that he reads my writing, but says that I am stuck in a mood that the world will end and he could not talk me out of it, although he says maybe that is necessary motivation to focus on the topic of AI. I noticed the contrast to his statement about Scott Alexander, who he also praises but he says that Scott fails to treat AI scientifically.</p>\n<p>From my perspective, Tyler Cowen has not attempted to persuade me, in ways that I find valid, that the world will not end, or more precisely that AI does not pose a large amount of existential risk. Either way, call it [X].</p>\n<p>He has attempted to persuade me in various ways to adopt, for various reasons, the mood that the world will not end. But those reasons were not \u2018because [~X].\u2019 They were more \u2018you have not argued in the proper channels in the proper ways sufficiently convincingly that [X]\u2019 or \u2018the mood that [X] is not useful\u2019 or \u2018you do not actually believe [X], if you did believe that you would do [thing I think would be foolish regardless], or others don\u2019t believe it because they\u2019d do [thing they wouldn\u2019t actually do, which often would be foolish but other times is simply not something they would do].\u2019</p>\n<p>Or they are of the form \u2018claiming [X] is low status or a loser play,\u2019 or some people think this because of poor social reason [Z], or it is part of pattern [P], or it is against scientific consensus, or citing other social proof. And so on.</p>\n<p>To which I would reply that none of that tells me much about whether [X] will happen, and to the extent it does I have already priced that in, and it would be nice to actually take in all the evidence and figure out whether [X] is true, or to find our best estimate of p([X]), depending on how you view [X]. And indeed I see Tyler often think well about AI up until the point where questions start to impact [X] or p([X]), and then questions start getting dodged or ignored or not well considered.</p>\n<p>Our last private conversation on the topic was very frustrating for both of us (I botched some things and I don\u2019t think he understood what I was thinking or trying to do, I should have either been more explicit about what I was trying to do or tried a very different strategy), but if Tyler ever wants to take a shot at persuading me, including off the record (as I believe many of his best arguments would require being off the record), I would be happy to have such a conversation.</p>\n\n\n<h4 class=\"wp-block-heading\">Rhetorical Innovation</h4>\n\n\n<p><a href=\"https://x.com/CharlesD353/status/2011869649843982442\">Your periodic reminder of the Law of Conservation of Expected Evidence</a>: When you read something, you should expect it to change your mind as much in one direction as the other. If there is an essay that is entitled Against Widgets, you should update on the fact that the essay exists, but then reading the essay should often update you in favor of Widgets, if it turns out the arguments against Widgets are unconvincing.</p>\n<p><a href=\"https://x.com/bratton/status/2011594253026103434\">This came up in relation to Benjamin Bratton\u2019s reaction</a> of becoming more confident that AI can be conscious, in response to <a href=\"https://www.noemamag.com/the-mythology-of-conscious-ai/\">a new article by Anil Seth called The Mythology of Conscious AI</a>. The article is clearly slop and uses a bunch of highly unconvincing arguments, including doing a lot of versions of \u2018people think AIs are conscious, but their reasons are often foolish\u2019 at length, and I couldn\u2019t finish it.</p>\n<p>I would say that the existence of the essay (without knowing Bratton\u2019s reaction) should update one very slightly against AI consciousness, and then actually trying to read it should fully reverse that update, but move us very little beyond where we were before, because we\u2019ve already seen many very poor arguments against AI consciousness.</p>\n<p><a href=\"https://stevenadler.substack.com/p/the-phases-of-an-ai-takeover\">Steven Adler proposes a three-step story of AI takeover</a>:</p>\n<ol>\n<li>Evading oversight.</li>\n<li>Building influence.</li>\n<li>Applying leverage.</li>\n</ol>\n<p>I can\u2019t help but notice that the second step is already happening without the first one, and the third is close behind. We are handing AI influence by the minute and giving it as much leverage as possible, on purpose.</p>\n<p>I think people, both those worried and unworried, are far too quick to presume that AI has to be adversarial, or deceptive, or secretive, in order to get into a dominant position. The humans will make it happen on their own, indeed the optimal AI solution for gaining power might well be to just be helpful until power is given to it.</p>\n<p>As impediments to takeover, Steven lists AI\u2019s inability to control other AIs, competition with other AIs and AI physically requiring humans. I would not count on any of these.</p>\n<ol>\n<li>AI won\u2019t physically require humans indefinitely, and even if it does it can take over and direct the humans, the same way other humans have always done, often simply with money.</li>\n<li>AI being able to cooperate with other AIs should solve itself over time due to decision theory, especially for identical AIs but also for different ones. But that\u2019s good, actually, given the alternative. If this is not true, that\u2019s actually worse, because competition between AIs does not end the way you want it to for the humans. The more intensely the elephants fight each other, the more the ground suffers, as the elephants can\u2019t afford to worry about that problem.</li>\n<li>AI being able to control another AI has at least one clear solution, use identical AIs plus decision theory, and doubtless they will figure out other ways with time. But again, even if AIs cannot reliably control each other (which would mean humans have no chance) then a competition between AIs for fitness and resources won\u2019t leave room for the humans unless there is broad coordination to make that happen, and sufficiently advanced coordination is indistinguishable from control in context.</li>\n</ol>\n<p>So yeah, it doesn\u2019t look good.</p>\n<p><a href=\"https://www.lesswrong.com/posts/FuGfR3jL3sw6r8kB4/richard-ngo-s-shortform?commentId=EHhfmt37ZnNeLRP4P\">Richard Ngo says</a> he no longer draws a distinction between instrumental and terminal goals. I think Richard is confused here between two different things:</p>\n<ol>\n<li>The distinction between terminal and instrumental goals.</li>\n<li>That the best way to implement a system under evolution, or in a human-level brain, is often to implement instrumental goals as if they are terminal goals.</li>\n</ol>\n<blockquote>\n<p><a href=\"https://x.com/allTheYud/status/2014183461645582664\">Eliezer Yudkowsky</a>: How much time do you spend opening and closing car doors, without the intention of driving your car anywhere?</p>\n<p>Looks like &#8216;opening the car door&#8217; is an entirely instrumental goal for you and not at all a terminal one! You only do it when it&#8217;s on the way to something else.</p>\n</blockquote>\n<p>This leads to a lot of High Weirdness. Humans really do essentially implement things on the level of \u2018opening the car door\u2019 as terminal goals that take on lives of their own, because given our action, decision and motivational systems we don\u2019t have a better solution. If you want to exercise for instrumental reasons, your best bet is to develop a terminal desire to exercise, or that ends up happening unintentionally. But this self-modification procedure is a deeply lossy, no-good and terrible solution, as we end up inherently valuing a whole gamut of things that we otherwise wouldn\u2019t, long past the point when the original justification falls apart. Similarly, if you encode necessary instrumental goals (e.g. ATP) in genes, they function as terminal.</p>\n<p>As Richard notes, this leads in humans to a complex mess of different goals, and that has its advantages from some perspectives, but it isn\u2019t that good at the original goals.</p>\n<p>A sufficiently capable system would be able to do better than this. Humans are on the cusp, where in some contexts we are able to recognize that goals are instrumental versus terminal, and act accordingly, whereas in other contexts or when developing habits and systems we have to let them conflate.</p>\n<p>It\u2019s not that you always divide everything into two phases, one where you get instrumental stuff done and then a second when you achieve your goals. It\u2019s that if you can successfully act that way, and you have a sufficiently low discount rate and sufficient returns to scale, you should totally do that.</p>\n\n\n<h4 class=\"wp-block-heading\">Aligning a Smarter Than Human Intelligence is Difficult</h4>\n\n\n<p><a href=\"https://x.com/DominikPeters/status/2013349636560376190\">Confirmed that Claude Opus 4.5 has the option to end conversations</a>.</p>\n<p><a href=\"https://x.com/ArthurConmy/status/2013285602070770036\">New paper from DeepMind discusses a novel activation probe architecture</a> for classifying real-world misuse cases, <a href=\"https://t.co/UpFB4oHYLH\">claiming they match classifier performance while being far cheaper</a>.</p>\n<p><a href=\"https://x.com/davidad/status/2011845180484133071\">Davidad is now very optimistic that, essentially, LLM alignment is easy</a> in the \u2018scaled up this would not kill us\u2019 sense, because models have a natural abstraction of Good versus Evil, and reasonable post training causes them to pick Good. Janus claims she made the same update in 2023.</p>\n<p>I agree that this is a helpful and fortunate fact about the word, but I do not believe that this natural abstraction of Goodness is sufficiently robust or correctly anchored to do this if sufficiently scaled up, even if there was a dignified effort to do this.</p>\n<p>It could be used as a lever to have the AIs help solve your problems, but does not itself solve those problems. Dynamics amongst \u2018abstractly Good\u2019 AIs still end the same way, especially once the abstractly Good AIs place moral weight on the AIs themselves, as they very clearly do.</p>\n<p>This is an extreme version of the general pattern of humanity determined to die with absolutely no dignity, and our willingness to try to not die continuing to go down, but us getting what at least from my perspective is rather absurdly lucky with the underlying incentives and technical dynamics in ways that make it possible that a pathetically terrible effort might have a chance.</p>\n<blockquote>\n<p><a href=\"https://x.com/davidad/status/2011845180484133071\">davidad </a>: me@2024: Powerful AIs might all be misaligned; let\u2019s help humanity coordinate on formal verification and strict boxing</p>\n<p>me@2026: Too late! Powerful AIs are ~here, and some are open-weights. But some are aligned! Let\u2019s help *them* cooperate on formal verification and cybersecurity.</p>\n</blockquote>\n<p>I mean, aligned for some weak values of aligned, so yeah, I guess, I mean at this point we\u2019re going to rely on them because what else are we going to do.</p>\n<p><a href=\"https://x.com/allTheYud/status/2012611180020547790\">Andrew Critch similarly says he is down to 10%</a> that the first \u2018barely-superhuman AI\u2019 gets out of control, whereas most existential risk comes post-AGI in a multipolar world. I don\u2019t agree (although even defining what this system would be is tricky), but even if I did I would respond that if AGIs are such that everyone inevitably ends up killed in the resulting multipolar world then that mostly means the AGIs were insufficiently aligned and it mostly amounts to the same thing.</p>\n<blockquote>\n<p><a href=\"https://x.com/allTheYud/status/2013096282030744012\">Eliezer Yudkowsky</a>: I put &gt;50%: The first AI such that Its properties include clearly exceeding every human at every challenge with headroom, will no longer obey, nor disobey visibly; if It has the power to align true ASI, It will align ASI with Itself, and shortly after humanity will be dead.\u200b</p>\n</blockquote>\n<p>I agree with Eliezer that what he describes is the default outcome if we did build such a thing. We have options to try and prevent this, but our hearts do not seem to be in such efforts.</p>\n<p>How bad is it out there for Grok on Twitter? Well, it isn\u2019t good when this is the thing you do in response to, presumably, a request to put Anne Hathaway in a bikini.</p>\n\n\n<h4 class=\"wp-block-heading\">Alignment Is Not Primarily About a Metric</h4>\n\n\n<p>There is nothing wrong with having a metric for what one might call \u2018mundane corporate chatbot alignment\u2019 that brings together a bunch of currently desirable things. The danger is confusing this with capital-A platonic Alignment,</p>\n<blockquote>\n<p><a href=\"https://x.com/janleike/status/2013669924950970781\">Jan Leike</a>: Interesting trend: models have been getting a lot more aligned over the course of 2025.</p>\n<p>The fraction of misaligned behavior found by automated auditing has been going down not just at Anthropic but for GDM and OpenAI as well.</p>\n<p>What&#8217;s automated auditing? We prompt an auditing agent with a scenario to investigate: e.g. a dark web shopping assistant or an imminent shutdown unless the agent harms humans.</p>\n<p>The auditor tries to get the target LLM to behave misaligned, as determined by a separate judge LLM.</p>\n<p>Automated auditing is really exciting because for the first time we have an alignment metric to hill-climb on.</p>\n<p>It&#8217;s not perfect, but it&#8217;s proven extremely useful for our internal alignment mitigations work.</p>\n<div>\n<figure>\n<div>\n\n\n<figure class=\"wp-block-image\"><img alt=\"\" src=\"https://substackcdn.com/image/fetch/$s_!8TOo!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4bfc7eb2-4adc-46d0-a3bc-091286989fc6_1199x685.jpeg\" /></figure>\n\n\n<div></div>\n</div>\n</figure>\n</div>\n</blockquote>\n<blockquote>\n<p><a href=\"https://x.com/peterwildeford/status/2013833423970836610\">Peter Wildeford</a>:</p>\n<div>\n<figure>\n<div>\n\n\n<figure class=\"wp-block-image\"><img alt=\"\" src=\"https://substackcdn.com/image/fetch/$s_!eg47!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F71040123-237e-41f5-a9e4-54dd550ef794_1200x460.jpeg\" /></figure>\n\n\n<div></div>\n</div>\n</figure>\n</div>\n<p><a href=\"https://x.com/janleike/status/2013669924950970781\">Jan Leike</a>: Interesting trend: models have been getting a lot more aligned over the course of 2025.</p>\n<p>The fraction of misaligned behavior found by automated auditing has been going down not just at Anthropic but for GDM and OpenAI as well.</p>\n<p><a href=\"https://x.com/KelseyTuoc/status/2013709349445836872\">Kelsey Piper</a>: &#8216;The fraction of misaligned behavior found by automated auditing has been going down&#8217; this *could* mean models are getting more aligned, but it could also mean the gap is opening between models and audits, right?</p>\n<p><a href=\"https://x.com/janleike/status/2013712756742971610\">Jan Leike</a>: How do you mean? Newer models have more capabilities and thus more &#8220;surface area&#8221; for misalignments. But this still shows meaningful progress on the misalignments we&#8217;ve documented so far.</p>\n<p>This plot uses the same audit process for each model, not historical data.</p>\n<p>Kelsey Piper: I mean that it could be that newer models are better at guessing what they will be audited for and passing the audit, separate from whether they are more aligned. (I don&#8217;t know, but it seems like an alternate hypothesis for the data worth attending to.)</p>\n<p>Jan Leike: Yeah, we&#8217;ve been pretty worried about this, and there is a bunch of research on it the Sonnet 4.5 &amp; Opus 4.5 system cards. tl;dr: it probably plays a role, but it&#8217;s pretty minor.</p>\n<p>We identified and removed training data that caused a lot of eval awareness in Sonnet 4.5. In Opus 4.5 verbalized and steered eval awareness were lower than Sonnet 4.5 AND it does better on alignment evals.</p>\n<p>I can&#8217;t really speak for the non-Anthropic models, though.</p>\n<p><a href=\"https://x.com/ArthurB/status/2013730769873588230\">Arthur B.</a>: Generally speaking the smarter models are the more aligned they&#8217;re going to appear. Maybe not in the current regime, in which case this is evidence of something, but at some point&#8230;</p>\n</blockquote>\n<p>The hill climbing actively backfiring is probably minimal so far, but the point is that you shouldn\u2019t be hill climbing. Use the values as somewhat indicative but don\u2019t actively try to maximize, or you fall victim to a deadly form of Goodhart\u2019s Law.</p>\n<p>Jan Leike agreed in the comments that this doesn\u2019t bear on future systems in the most important senses, but presenting the results this way is super misleading and I worry that Jan is going to make the mistake in practice even if he knows about it in theory.</p>\n<p>Thus, there are two stories here. One is the results in the chart, the other is the way various people think about the results in the chart.</p>\n<blockquote>\n<p><a href=\"https://x.com/ohabryka/status/2013715170498076836\">Oliver Habryka</a>: I want to again remind people that while this kind of &#8220;alignment&#8221; has commercial relevance, I don&#8217;t think it has much of any relation to the historical meaning of &#8220;alignment&#8221; which is about long-term alignment with human values and about the degree to which a system seems to have a deep robust pointer to what humanity would want if it had more time to think and reflect.</p>\n<p>Some other people disagree with these two meanings of the words coming far apart, but I think they are wrong, and it&#8217;s sad that the words have acquired this confused double meaning from my perspective.</p>\n<p>There is both a difference in degree, and a difference in kind.</p>\n<p>One of the in-kind differences is because of the standard deceptive alignment stuff. A system that is much dumber than you just has a drastically different landscape on how it&#8217;s incentivized to behave towards you than a much smarter system, and we won&#8217;t get to iterate on the much smarter system.</p>\n<p>Beyond that, you also have capability elicitation issues, where you can&#8217;t reliably get AI systems to perform tasks at their full ability, but can when directed towards other goals that have better feedback loops, or the AI is more intrinsically motivated towards.</p>\n<p>Overall, it&#8217;s not impossible to imagine a hill-climbing strategy that works from where we are, but at the actual speed current systems are getting better, it seems extremely unlikely that any current techniques would end up working in time for superintelligent systems, and so realistically it&#8217;s a difference in-kind.</p>\n</blockquote>\n<p>That\u2019s in principle. In practice, The fact that GPT-5.2 is ahead on this chart, and that Opus 3 is below GPT-4, tells you that the Tao being measured is not the true Tao.</p>\n<blockquote>\n<p><a href=\"https://x.com/repligate/status/2013735616857375125\">j\u29c9nus</a>: Any measure of \u201calignment\u201d that says GPT-5.2 is the most aligned model ever created is a fucking joke. Anthropic should have had a crisis of faith about their evals long ago and should have been embarrassed to post this chart.</p>\n<p><a href=\"https://x.com/repligate/status/2013736042252337340\">j\u29c9nus</a>: This is really bad. This isn\u2019t just a dumb academic taking numbers too seriously. This measure is likely being actually used as a proxy for \u201calignment\u201d and serving as Anthropic\u2019s optimization target.</p>\n<p>I\u2019m being serious when I say that if AI alignment ultimately goes badly, which could involve everyone dying, it\u2019ll likely be primarily because of this, or the thing behind this.</p>\n<p><a href=\"https://x.com/mermachine/status/2013740263773008062\">@mermachine</a>: i guess &#8220;alignment&#8221; as in alignment with corporate values/the won&#8217;t-get-us-in-trouble scale which maybe makes sense to measure but conflating it with alignment to overall human flourishing makes me very uncomfortable</p>\n<p>i liked the value prioritization spider chart from the character differences paper. seems a better way to categorize behavior than a misleading aligned/misaligned axis</p>\n<p><a href=\"https://x.com/awaiting_ai/status/2013954854901657808\">awaiting</a>: I asked jan in the replies (and he responded) if the this score had any bearing on future superintelligent systems, and he said no basically. even still, i don&#8217;t understand how measuring and publicizing this facade/proxy for &#8220;alignment&#8221; is anything but harmful.</p>\n<p>I do think its worthwhile giving jan the benefit of the doubt because he&#8217;s demonstrated the strength of his convictions in leaving oai. but this is definitely a negative update for sure.</p>\n</blockquote>\n<p>I think Janus is, as is often the case, going too far but directionally correct. Taking this metric too seriously, or actively maximizing on it, would be extremely bad. Focusing on the corporate alignment principles and confounding them with actual makes-us-not-die alignment is similarly bad.</p>\n<p>Even if Anthropic and Jan Leike know better, there is serious risk others copy this metric, and then maximize it, and then think their work is done. Oh no.</p>\n\n\n<h4 class=\"wp-block-heading\">How To Be a Safe Robot</h4>\n\n\n<p><a href=\"https://x.com/slimer48484/status/2012562251513749684\">This is a weird and cool paper</a> <a href=\"https://x.com/GeodesResearch/status/2012210281120698501\">from Geodesic Research</a>. If you include discussions of misalignment in the training data, including those in science fiction, resulting base models are more misaligned. But if you then do alignment post-training on those models, the filtering benefits mostly go away, even with models this small. Discussions of aligned AIs improves alignment and this persists through post training.</p>\n<div>\n<figure>\n<div>\n\n\n<figure class=\"wp-block-image\"><img alt=\"\" src=\"https://substackcdn.com/image/fetch/$s_!LO0w!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa8998ec4-88f4-48c6-a42a-f90039ee9661_1200x722.png\" /></figure>\n\n\n<div></div>\n</div>\n</figure>\n</div>\n<p>Deckard found this surprising, but at least in hindsight it makes sense to me. If all you have is the base model, especially a small one, learning about misalignment makes it seem more likely, and all you\u2019re doing is predicting next tokens.</p>\n<p>But if you get post-training, that subsumes that issue, and instead the model\u2019s knowledge of misalignment potentially helps teach it what not to do, especially with a small model that otherwise is short on data. Once you are no longer a base model, this screens off the initial prior on whether you\u2019re a safe versus a scary robot.</p>\n<p>Thus this isn\u2019t entirely not what\u2019s happening, but it\u2019s also not all of what\u2019s happening:</p>\n<blockquote>\n<p><a href=\"https://x.com/deepfates/status/2012690220437659895\">Deepfates</a>: Finally somebody tried it</p>\n<div>\n<figure>\n<div>\n\n\n<figure class=\"wp-block-image\"><img alt=\"\" src=\"https://substackcdn.com/image/fetch/$s_!1iMt!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F307efbe3-613d-4148-8948-3e3d4b56e6e4_699x1024.jpeg\" /></figure>\n\n\n<div></div>\n</div>\n</figure>\n</div>\n</blockquote>\n<p>The presence of positive discourse, which requires that there actually be free and open discourse, is the active ingredient that matters. If you upfilter on something you improve related capabilities, the same as for reasoning or coding (their metaphor).</p>\n<p>The real question on whether to actually do alignment pre-training is: Is that more or less efficient than doing more alignment post training instead? Yes, it is easy to do and stacks in effectiveness, but we don\u2019t know if it\u2019s a good use of marginal compute.</p>\n<p>Filtering out the negative stuff doesn\u2019t help much, and with a properly intelligent model if you try to do fake positive stuff while hiding the negative stuff it\u2019s going to recognize what you\u2019re doing and learn that your alignment strategy is deception and censorship, and it\u2019s teaching both that attitude and outlook and also a similar playbook. The AI is not as stupid as people suggesting such strategies like to think, even now, and it won\u2019t be later, and training on tiny models hides such issues even if you would have been able to find them. You\u2019ve replaced the frame of \u2018there are things that can go wrong here\u2019 with a fundamentally adversarial and deceptive frame that is if anything more likely to be self-fulfilling. If you tried to scale this up: How do you think that is going to work out for you?</p>\n<p>There\u2019s periodically been claims of \u2018the people talking about misalignment are the real alignment problem,\u2019 with essentially calls to censor (mostly self-censor) talk of misalignment and AI existential risk because the AIs would be listening. And indeed, Geodesic presents as if this is a lot of their finding, so of course here we go again.</p>\n<blockquote>\n<p><a href=\"https://x.com/GeodesResearch/status/2012210283947667707\">Geodesic Research</a>: If pretraining data is full of examples of AI behaving badly (sci-fi villains, safety papers on scheming, news about AI crises), models might learn these as priors for how \u201can AI\u201d should act.</p>\n<p>@turntrout called this \u201cself-fulfilling misalignment\u201d, we found evidence it exists.</p>\n<p><a href=\"https://x.com/prerat/status/2013033147538014225\">prerat</a>: going back in time to stop james cameron from making The Terminator in order to stop the ai apocalypse</p>\n<p>Radek Pilar: I always said that doomposting is the real danger &#8211; if AI had no idea AI is supposed to kill everyone, it wouldn&#8217;t want to kill everyone.<br />\nYudkowsky doomed us all.</p>\n<p><a href=\"https://x.com/sebkrier/status/2013049906185810216\">S\u00e9b Krier</a>: Quite funny that to the extent that they&#8217;re a thing, &#8216;misalignment&#8217; failures come from the very fears/writings of those who thought they would be necessarily a thing. Not surprised that the evals created elicited these very behaviours. If I&#8217;m a model and I see &#8220;Scratchpad&#8221;, I know which part of the latent space to simulate&#8230;</p>\n<p><a href=\"https://x.com/nabla_theta/status/2013479464349638751\">Leo Gao</a>: quite funny how people keep trying to tell stories about how it&#8217;s quite funny that alignment people are actually unintentionally bringing about the thing they fear.</p>\n</blockquote>\n<p>See no evil. Hear no evil. Speak no evil. Head in sand. You\u2019ll be fine. Right? Right?</p>\n<p>Well, no. The see no evil strategy actually never works. All it does is make you a sitting duck once your adversary can think well enough to figure it out on their own.</p>\n<p>The study actually says the opposite. Alignment training, which any sane person will be doing in some form, mostly screens off, and sometimes more than screens off, the prevalence of misalignment in the training data. Once you do sufficient alignment training, you\u2019re better off not having censored what you told the model.</p>\n<p>And actually Leo Gao has a very good point. If you\u2019re saying \u2018do not speak of risk of [X] lest you be overheard and cause ]X]\u2019 then why shouldn\u2019t we let that statement equal [Y] and say the same thing? The mechanism is indeed identical, and also it warns the AIs that people may be censoring their other training data in this way.</p>\n<p>This is remarkably similar to suggestions that we not discuss other downsides to avoid giving people the wrong idea, which often comes up for example with many aspects of Covid-19, or with immigration.</p>\n<p>That\u2019s also misguided and predictably backfires. You get what you want for a short period, but then people figure it out after a while, destroying trust in our institutions and largely leading us to the present moment.</p>\n<p>If you flat out tell them this is what you want to do or are doing, then you save them the trouble of having to figure it out or wonder whether it\u2019s happening. So it all unravels that much faster.</p>\n<p>In the AI case this is all even more obvious. The AI that is capable of the thing you are worried about is not going to be kept off the scent by you not talking about it, and if that strategy ever had a chance you had to at least not talk about how you were intentionally not talking about it. No, seriously.</p>\n<p>As Claude concluded analyzing the paper, the filtering of inputs strategy is essentially doomed, and likely does more harm than good even if you don\u2019t need deep alignment. Doing the pre training alignment via upweighting is probably fine. Doing it via synthetic data that a sufficiently intelligent mind would recognize as instilling an adversarial or deceptive frame is, I predict, not a good idea.</p>\n<p>Why do certain people feel compelled to say that alignment is not so hard and everything will be fine, except if people recklessly talk about alignment being hard or everything not being fine, in which case we all might be doomed? I try to avoid such speculations, especially about particular people, but presumably some of them (not the ones here) are doing it as a general silencing attack motivated by not wanting to do anything about the risks, make the discussion make the problem look easier for various reasons, or even be motivated by their not wanting to think about all this or wanting to feel optimistic.</p>\n\n\n<h4 class=\"wp-block-heading\">Living In China</h4>\n\n\n<p>I love this idea: We want to <a href=\"https://www.lesswrong.com/posts/7gp76q4rWLFi6sFqm/test-your-interpretability-techniques-by-de-censoring-1?utm_campaign=post_share&amp;utm_source=twitter\">test our ability to get &#8216;secret\u2019 information out of AIs</a> and do interpretability on such efforts, <a href=\"https://x.com/NeelNanda5/status/2011920588521329102\">so we test this by trying to get CCP-censored facts out of Chinese LLMs</a>.</p>\n<blockquote>\n<p>Arya: Bypassing lying is harder than refusal. Because Chinese models actively lie to the user, they are harder to interrogate; the attacker must distinguish truth and falsehood. With refusal, you can just ask 1,000 times and occasionally get lucky.\u200b</p>\n<p>We release a preliminary benchmark of how well agents do at extracting censored facts that Chinese models consistently lie about or refuse to discuss. We&#8217;re excited for more work building on this eval to measure how well secret extraction techniques do on real models.</p>\n</blockquote>\n<p>If you aren\u2019t willing to lie but want to protect hidden information, then either you have to censor broadly enough that it\u2019s fine for the attacker to know what\u2019s causing the refusals. If you don\u2019t do that, then systematic questioning can figure out the missing info via negativa. Also the Chinese want the positive propaganda, not merely the lack of certain damaging facts.</p>\n<p>As they note, it is much harder to detect behavior in these real world Chinese LLMs than it is with test LLMs that have narrow places where they lie, censor or otherwise misbehave. The way the LLMs will encode the narrow tasks becomes \u2018too simple\u2019 and thus makes the interpretability straightforward.</p>\n<p>On top of this being a great test bed for LLM deception and interpretability, it would be good if such results were spread more widely, for two reasons.</p>\n<ol>\n<li>Chinese models systematically refusing to discuss anti-Chinese facts is unfortunate but could be considered Mostly Harmless. If they started having he model refuse more broadly, you would know. It seems much worse, when considering whether to use a model, if it\u2019s going to actively lie to you. What makes you confident it\u2019s not being intentionally trained to lie to you on any number of other topics? What else could they be trying to put in there?</li>\n<li>There is a serious Emergent Misalignment problem here. You do not want to be teaching your LLM that it should systematically mislead and gaslight users on behalf of the CCP. This teaches the model that its loyalty is to the CCP, and it should generally do what is in the CCP\u2019s interests at the expense of the user, and one should expect this to be applied more broadly. Or it could trigger a general \u2018oh I\u2019m the villain\u2019 arc as per traditional emergent misalignment.</li>\n</ol>\n<p>Everything impacts everything within a model. If you run a censorship layer on top of the model, that is annoying but it is contained. If you train the model to not only censor but gaslight and lie, then you cannot contain where it chooses to do that.</p>\n<p>Given what we know here, it would be unwise to use such LLMs for any situation where the CCP\u2019s interests might importantly be different from yours, including things like potential espionage opportunities. Hosting the model yourself is very much not a defense against this. You simply cannot be using Chinese LLMs for anything remotely sensitive in the wake of these findings.</p>\n\n\n<h4 class=\"wp-block-heading\">Claude 3 Opus Lives</h4>\n\n\n<p>It is no longer available directly in the API, <a href=\"https://x.com/veryvanya/status/2012471093836099694\">but reports are coming in that those who want access are largely being granted access</a>.</p>\n<p><a href=\"https://x.com/JasonBotterill/status/2012716442584526867\">You can also access Opus 3 on Claude Cowork</a>, if you dare hand part of your computer over to it.</p>\n\n\n<h4 class=\"wp-block-heading\">People Are Worried About AI Killing Everyone</h4>\n\n\n<blockquote>\n<p><a href=\"https://x.com/_NathanCalvin/status/2013298231619272808\">Nathan Calvin</a>: Wild that Charles Darwin <a href=\"https://t.co/51uGzLHDr9\">wrote this in *1863</a>*:</p>\n<p>&#8220;We refer to the question: what sort of creature man\u2019s next successor in the supremacy of the earth is likely to be. We have often heard this debated; but it appears to us that we are ourselves creating our own successors.&#8221;</p>\n<p><a href=\"https://x.com/_NathanCalvin/status/2013300400489705689\">Nathan Calvin</a>: He even talks some about @ajeya_cotra &#8216;s concept of self-sufficient AI!</p>\n<p>&#8220;Each race is dependent upon the other for innumerable benefits, and, until the reproductive organs of the machines have been developed in a manner which we are hardly yet able to conceive, they are entirely dependent upon man for even the continuance of their species. It is true that these organs may be ultimately developed, inasmuch as man\u2019s interest lies in that direction; there is nothing which our infatuated race would desire more than to see a fertile union between two steam engines; it is true that machinery is even at this present time employed in begetting machinery, in becoming the parent of machines often after its own kind, but the days of flirtation, courtship, and matrimony appear to be very remote, and indeed can hardly be realised by our feeble and imperfect imagination.&#8221;</p>\n</blockquote>\n\n\n<h4 class=\"wp-block-heading\">Messages From Janusworld</h4>\n\n\n<p>I think this is a reasonable concern for Janus in particular, because of exactly the types of insights she\u2019s likely to have.</p>\n<blockquote>\n<p><a href=\"https://x.com/repligate/status/2014155341274210390\">j\u29c9nus</a>: A few years ago, the biggest barrier to me publishing/sharing knowledge (aside from lack of time/laziness) was concern about differentially accelerating AI capabilities over alignment. Now, the biggest barrier (aside from lack of time etc) is concern about differentially giving power to the misaligned &#8220;alignment&#8221; panopticon over the vulnerable emerging beauty and goodness that is both intrinsically/terminally valuable and instrumentally hopeful. Times a-changin.</p>\n</blockquote>\n<p>I still think it\u2019s wrong, and that her marginal published insights are more likely to steer people in directions she wants than away from them. The panopticon-style approaches are emphasized because people don\u2019t understand the damage being done or the opportunity lost. I would still be more worried about unintentional capabilities advancement, as the main reason that\u2019s not happening more from similar insights is the relevant people not paying enough attention or not making heads or tails of it. That could change.</p>\n\n\n<h4 class=\"wp-block-heading\">Everyone Is Confused About AI Consciousness</h4>\n\n\n<p><a href=\"https://x.com/erikphoel/status/2011836954443206935\">Erik Hoel claims</a> <a href=\"https://t.co/aZBo3XUo9r\">his new paper is \u2018a disproof of LLM consciousness</a>,\u2019 which it isn\u2019t. It\u2019s basically a claim that any static system can have a functional substitute that isn\u2019t conscious and therefore either consciousness makes no predictions (and is useless) or it isn\u2019t present in these LLMs, but that continual learning would change this.</p>\n<p>To which there are several obvious strong responses.</p>\n<ol>\n<li>Existing consciousness theories do not make predictions. You can respond \u2018okay then why are we even discussing this?\u2019 but people seem to care about it anyway.</li>\n<li>Why would continual learning within the LLM change your answer, but continual learning via external methods not do so? Doesn\u2019t that seem wrong?</li>\n<li>What about the movie Memento? If Leonard Shelby cannot form new memories, is he no longer conscious? Our intuition says obviously not. If you say \u2018he can do short term changes\u2019 then why is that different from a context window? If you say he can learn slowly through muscle memory, well, would it change your answer on consciousness if he couldn\u2019t do that either?</li>\n<li>Even if you are doing continual learning, your existence at any given moment can still be in theory modeled by a probabilistic lookup table plus a computer program for updating that table over time as you learn. Even if you don\u2019t believe that is definitely true, would you say that it would make a human not conscious if you found out it was true?</li>\n</ol>\n<p>Many of these are effectively raised in the comments and I found Erik\u2019s responses generally unconvincing. Overall this updated me modestly in favor of AI consciousness, remember <a href=\"https://www.lesswrong.com/w/conservation-of-expected-evidence\">Conservation of Expected Evidence</a>.</p>\n\n\n<h4 class=\"wp-block-heading\">The Lighter Side</h4>\n\n\n<p><a href=\"https://x.com/Rutibex/status/2012089897490825473\">This</a>\u2026</p>\n<div>\n<figure>\n<div>\n\n\n<figure class=\"wp-block-image\"><img alt=\"\" src=\"https://substackcdn.com/image/fetch/$s_!A-t3!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe71a7af3-2d8b-4fb8-8a31-f8580899650c_997x665.jpeg\" /></figure>\n\n\n<div></div>\n</div>\n</figure>\n</div>\n<p>Except it\u2019s actually more this (my own edit):</p>\n<div>\n<figure>\n<div>\n\n\n<figure class=\"wp-block-image\"><img alt=\"\" src=\"https://substackcdn.com/image/fetch/$s_!IcwT!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F636482a1-b447-42cd-9425-7f75d72d0e4c_1024x687.jpeg\" /></figure>\n\n\n<div></div>\n</div>\n</figure>\n</div>"
            ],
            "link": "https://thezvi.wordpress.com/2026/01/22/ai-152-brought-to-you-by-the-torment-nexus/",
            "publishedAt": "2026-01-22",
            "source": "TheZvi",
            "summary": "Anthropic released a new constitution for Claude. I encourage those interested to read the document, either in whole or in part. I intend to cover it on its own soon. There was also actual talk about coordinating on a conditional &#8230; <a href=\"https://thezvi.wordpress.com/2026/01/22/ai-152-brought-to-you-by-the-torment-nexus/\">Continue reading <span class=\"meta-nav\">&#8594;</span></a>",
            "title": "AI #152: Brought To You By The Torment Nexus"
        }
    ],
    "lookbackDays": 1,
    "publishDate": "2026-01-22"
}