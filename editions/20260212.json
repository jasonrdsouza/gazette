{
    "articles": [
        {
            "content": [
                "<div class=\"captioned-image-container\"><figure><a class=\"image-link image2 is-viewable-img\" href=\"https://substackcdn.com/image/fetch/$s_!5aIJ!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3e6009ee-a8fe-4f00-b181-6f59cde250f6_2000x1260.jpeg\" target=\"_blank\"><div class=\"image2-inset\"><source type=\"image/webp\" /><img alt=\"\" class=\"sizing-normal\" height=\"917\" src=\"https://substackcdn.com/image/fetch/$s_!5aIJ!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3e6009ee-a8fe-4f00-b181-6f59cde250f6_2000x1260.jpeg\" width=\"1456\" /><div class=\"image-link-expand\"><div class=\"pencraft pc-display-flex pc-gap-8 pc-reset\"><button class=\"pencraft pc-reset pencraft icon-container restack-image\" tabindex=\"0\" type=\"button\"><svg fill=\"none\" height=\"20\" stroke=\"var(--color-fg-primary)\" stroke-linecap=\"round\" stroke-linejoin=\"round\" stroke-width=\"1.5\" viewBox=\"0 0 20 20\" width=\"20\" xmlns=\"http://www.w3.org/2000/svg\"><g><title></title><path d=\"M2.53001 7.81595C3.49179 4.73911 6.43281 2.5 9.91173 2.5C13.1684 2.5 15.9537 4.46214 17.0852 7.23684L17.6179 8.67647M17.6179 8.67647L18.5002 4.26471M17.6179 8.67647L13.6473 6.91176M17.4995 12.1841C16.5378 15.2609 13.5967 17.5 10.1178 17.5C6.86118 17.5 4.07589 15.5379 2.94432 12.7632L2.41165 11.3235M2.41165 11.3235L1.5293 15.7353M2.41165 11.3235L6.38224 13.0882\"></path></g></svg></button><button class=\"pencraft pc-reset pencraft icon-container view-image\" tabindex=\"0\" type=\"button\"><svg class=\"lucide lucide-maximize2 lucide-maximize-2\" fill=\"none\" height=\"20\" stroke=\"currentColor\" stroke-linecap=\"round\" stroke-linejoin=\"round\" stroke-width=\"2\" viewBox=\"0 0 24 24\" width=\"20\" xmlns=\"http://www.w3.org/2000/svg\"><polyline points=\"15 3 21 3 21 9\"></polyline><polyline points=\"9 21 3 21 3 15\"></polyline><line x1=\"21\" x2=\"14\" y1=\"3\" y2=\"10\"></line><line x1=\"3\" x2=\"10\" y1=\"21\" y2=\"14\"></line></svg></button></div></div></div></a></figure></div><p><em>This essay will appear in our forthcoming book, &#8220;Making the Modern Laboratory,&#8221; to be published later this year.</em></p><p>In 1959, a pair of enterprising brothers, Jack and Harold Kraft, filed a <a href=\"https://patents.google.com/patent/US3061280A/en\">patent</a> titled &#8220;Apparatus for mixing fluent material.&#8221; Though simple in concept, their invention solved one of the most fundamental challenges faced by mid-century scientists: mixing fluids quickly and efficiently. The <strong>vortex mixer, </strong>a small motorized device that vibrated samples,<strong> </strong>offered the perfect solution, and is now found on biology benches across the world.</p><p>Harold and Jack Kraft were born in New York in the tumultuous years following World War I. From a young age, the boys displayed an entrepreneurial spirit and were always in business together. During the Great Depression, they made money by repairing broken radios and installing radio antennas on buildings in New York City. Family members recall Harold as the gregarious talker or salesman, while Jack led the technical side of their ventures.</p><p>Even World War II could not stop their abiding passion for motors and machines. Jack attended NYU School of Engineering while in the reserves, and Harold became an aircraft mechanic in the 519th Service Squadron, working at airfields in England and later France. After the war, the brothers reunited to take up business once again, this time manufacturing their own eponymous brand of <a href=\"https://www.flickr.com/photos/sakraft1/albums/72157630160288856/\">Kraftone record players</a>.</p><div class=\"captioned-image-container\"><figure><a class=\"image-link image2 is-viewable-img\" href=\"https://substackcdn.com/image/fetch/$s_!G-qH!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffa6b7748-b244-418d-99db-13ab401a2c3a_2257x2852.jpeg\" target=\"_blank\"><div class=\"image2-inset\"><source type=\"image/webp\" /><img alt=\"\" class=\"sizing-normal\" height=\"1840\" src=\"https://substackcdn.com/image/fetch/$s_!G-qH!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffa6b7748-b244-418d-99db-13ab401a2c3a_2257x2852.jpeg\" width=\"1456\" /><div class=\"image-link-expand\"><div class=\"pencraft pc-display-flex pc-gap-8 pc-reset\"><button class=\"pencraft pc-reset pencraft icon-container restack-image\" tabindex=\"0\" type=\"button\"><svg fill=\"none\" height=\"20\" stroke=\"var(--color-fg-primary)\" stroke-linecap=\"round\" stroke-linejoin=\"round\" stroke-width=\"1.5\" viewBox=\"0 0 20 20\" width=\"20\" xmlns=\"http://www.w3.org/2000/svg\"><g><title></title><path d=\"M2.53001 7.81595C3.49179 4.73911 6.43281 2.5 9.91173 2.5C13.1684 2.5 15.9537 4.46214 17.0852 7.23684L17.6179 8.67647M17.6179 8.67647L18.5002 4.26471M17.6179 8.67647L13.6473 6.91176M17.4995 12.1841C16.5378 15.2609 13.5967 17.5 10.1178 17.5C6.86118 17.5 4.07589 15.5379 2.94432 12.7632L2.41165 11.3235M2.41165 11.3235L1.5293 15.7353M2.41165 11.3235L6.38224 13.0882\"></path></g></svg></button><button class=\"pencraft pc-reset pencraft icon-container view-image\" tabindex=\"0\" type=\"button\"><svg class=\"lucide lucide-maximize2 lucide-maximize-2\" fill=\"none\" height=\"20\" stroke=\"currentColor\" stroke-linecap=\"round\" stroke-linejoin=\"round\" stroke-width=\"2\" viewBox=\"0 0 24 24\" width=\"20\" xmlns=\"http://www.w3.org/2000/svg\"><polyline points=\"15 3 21 3 21 9\"></polyline><polyline points=\"9 21 3 21 3 15\"></polyline><line x1=\"21\" x2=\"14\" y1=\"3\" y2=\"10\"></line><line x1=\"3\" x2=\"10\" y1=\"21\" y2=\"14\"></line></svg></button></div></div></div></a><figcaption class=\"image-caption\">Jack Kraft, circa 1962. Credit: Scott Kraft</figcaption></figure></div><div class=\"captioned-image-container\"><figure><a class=\"image-link image2 is-viewable-img\" href=\"https://substackcdn.com/image/fetch/$s_!suMY!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F71c3e731-8556-4893-9abd-ca425e55daee_2477x3029.jpeg\" target=\"_blank\"><div class=\"image2-inset\"><source type=\"image/webp\" /><img alt=\"\" class=\"sizing-normal\" height=\"1780\" src=\"https://substackcdn.com/image/fetch/$s_!suMY!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F71c3e731-8556-4893-9abd-ca425e55daee_2477x3029.jpeg\" width=\"1456\" /><div class=\"image-link-expand\"><div class=\"pencraft pc-display-flex pc-gap-8 pc-reset\"><button class=\"pencraft pc-reset pencraft icon-container restack-image\" tabindex=\"0\" type=\"button\"><svg fill=\"none\" height=\"20\" stroke=\"var(--color-fg-primary)\" stroke-linecap=\"round\" stroke-linejoin=\"round\" stroke-width=\"1.5\" viewBox=\"0 0 20 20\" width=\"20\" xmlns=\"http://www.w3.org/2000/svg\"><g><title></title><path d=\"M2.53001 7.81595C3.49179 4.73911 6.43281 2.5 9.91173 2.5C13.1684 2.5 15.9537 4.46214 17.0852 7.23684L17.6179 8.67647M17.6179 8.67647L18.5002 4.26471M17.6179 8.67647L13.6473 6.91176M17.4995 12.1841C16.5378 15.2609 13.5967 17.5 10.1178 17.5C6.86118 17.5 4.07589 15.5379 2.94432 12.7632L2.41165 11.3235M2.41165 11.3235L1.5293 15.7353M2.41165 11.3235L6.38224 13.0882\"></path></g></svg></button><button class=\"pencraft pc-reset pencraft icon-container view-image\" tabindex=\"0\" type=\"button\"><svg class=\"lucide lucide-maximize2 lucide-maximize-2\" fill=\"none\" height=\"20\" stroke=\"currentColor\" stroke-linecap=\"round\" stroke-linejoin=\"round\" stroke-width=\"2\" viewBox=\"0 0 24 24\" width=\"20\" xmlns=\"http://www.w3.org/2000/svg\"><polyline points=\"15 3 21 3 21 9\"></polyline><polyline points=\"9 21 3 21 3 15\"></polyline><line x1=\"21\" x2=\"14\" y1=\"3\" y2=\"10\"></line><line x1=\"3\" x2=\"10\" y1=\"21\" y2=\"14\"></line></svg></button></div></div></div></a><figcaption class=\"image-caption\">Harold Kraft, circa 1962. Credit: Scott Kraft</figcaption></figure></div><p>By the late 1950s, looking to break into scientific equipment, Jack reached out to fellow NYU alumnus and inventor, <a href=\"https://archives.sciencehistory.org/repositories/3/archival_objects/37748\">Dr. Samuel R. Natelson</a>, a clinical chemist who was the Head of Biochemistry at St. Vincent&#8217;s Hospital of New York. Natelson kindly obliged Jack&#8217;s request, with the two meeting several times for chemistry demonstrations and discussions of the equipment challenges faced by Natelson and his colleagues.</p><p>During one such meeting, Natelson expressed a dire need for better mixing equipment. At the time, chemists had only a few options. If the solution volume was large enough, magnetic stir bars could be placed into the mixing vessel, but that meant they needed a corresponding electromagnetic stir plate upon which to place the solution. Most labs had only a few such plates, if any, so when making multiple solutions, there weren&#8217;t enough to go around. The alternative was to stir, shake, or flick the vessel manually.<a class=\"footnote-anchor\" href=\"https://www.asimov.press/feed#footnote-1\" id=\"footnote-anchor-1\" target=\"_self\">1</a> These apparatuses all needed cleaning between each use.</p><p>We may never know which specific mixture drew the ire of Natelson. But we can infer from a letter, drafted by the Kraft brothers, that it concerned viscous substances. The letter also reveals just how excited the brothers were about their invention. According to their account, Jack brought the original idea to Harold, and the two sketched out potential solutions on October 20, 1958. Just three days later, they had built their first prototype using the same kind of shaded pole AC motor found in their record players.<a class=\"footnote-anchor\" href=\"https://www.asimov.press/feed#footnote-2\" id=\"footnote-anchor-2\" target=\"_self\">2</a></p><p>The resulting invention was simple, but elegant. A small, high-powered motor was housed within the body of a box-shaped machine. Mounted atop the motor was a rubber cup. Switched on, the motor oscillated the cup in tight orbital motions. When a test tube, or other vessel, touched the rubber cup, that motion transferred to the liquid, creating a vortex and mixing its contents.</p><div class=\"captioned-image-container\"><figure><a class=\"image-link image2 is-viewable-img\" href=\"https://substackcdn.com/image/fetch/$s_!uBZw!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffcf12c2f-7c78-4f77-84d3-028f09c94897_2448x3273.jpeg\" target=\"_blank\"><div class=\"image2-inset\"><source type=\"image/webp\" /><img alt=\"\" class=\"sizing-normal\" height=\"1947\" src=\"https://substackcdn.com/image/fetch/$s_!uBZw!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffcf12c2f-7c78-4f77-84d3-028f09c94897_2448x3273.jpeg\" width=\"1456\" /><div class=\"image-link-expand\"><div class=\"pencraft pc-display-flex pc-gap-8 pc-reset\"><button class=\"pencraft pc-reset pencraft icon-container restack-image\" tabindex=\"0\" type=\"button\"><svg fill=\"none\" height=\"20\" stroke=\"var(--color-fg-primary)\" stroke-linecap=\"round\" stroke-linejoin=\"round\" stroke-width=\"1.5\" viewBox=\"0 0 20 20\" width=\"20\" xmlns=\"http://www.w3.org/2000/svg\"><g><title></title><path d=\"M2.53001 7.81595C3.49179 4.73911 6.43281 2.5 9.91173 2.5C13.1684 2.5 15.9537 4.46214 17.0852 7.23684L17.6179 8.67647M17.6179 8.67647L18.5002 4.26471M17.6179 8.67647L13.6473 6.91176M17.4995 12.1841C16.5378 15.2609 13.5967 17.5 10.1178 17.5C6.86118 17.5 4.07589 15.5379 2.94432 12.7632L2.41165 11.3235M2.41165 11.3235L1.5293 15.7353M2.41165 11.3235L6.38224 13.0882\"></path></g></svg></button><button class=\"pencraft pc-reset pencraft icon-container view-image\" tabindex=\"0\" type=\"button\"><svg class=\"lucide lucide-maximize2 lucide-maximize-2\" fill=\"none\" height=\"20\" stroke=\"currentColor\" stroke-linecap=\"round\" stroke-linejoin=\"round\" stroke-width=\"2\" viewBox=\"0 0 24 24\" width=\"20\" xmlns=\"http://www.w3.org/2000/svg\"><polyline points=\"15 3 21 3 21 9\"></polyline><polyline points=\"9 21 3 21 3 15\"></polyline><line x1=\"21\" x2=\"14\" y1=\"3\" y2=\"10\"></line><line x1=\"3\" x2=\"10\" y1=\"21\" y2=\"14\"></line></svg></button></div></div></div></a><figcaption class=\"image-caption\">A description of the vortex mixer, as perceived in the inventor&#8217;s mind in the late 1950s. Credit: Scott Kraft</figcaption></figure></div><div class=\"captioned-image-container\"><figure><a class=\"image-link image2 is-viewable-img\" href=\"https://substackcdn.com/image/fetch/$s_!e0dm!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff6acde0c-2793-4605-98e8-1c7a7df73ecc_2550x3306.jpeg\" target=\"_blank\"><div class=\"image2-inset\"><source type=\"image/webp\" /><img alt=\"\" class=\"sizing-normal\" height=\"1888\" src=\"https://substackcdn.com/image/fetch/$s_!e0dm!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff6acde0c-2793-4605-98e8-1c7a7df73ecc_2550x3306.jpeg\" width=\"1456\" /><div class=\"image-link-expand\"><div class=\"pencraft pc-display-flex pc-gap-8 pc-reset\"><button class=\"pencraft pc-reset pencraft icon-container restack-image\" tabindex=\"0\" type=\"button\"><svg fill=\"none\" height=\"20\" stroke=\"var(--color-fg-primary)\" stroke-linecap=\"round\" stroke-linejoin=\"round\" stroke-width=\"1.5\" viewBox=\"0 0 20 20\" width=\"20\" xmlns=\"http://www.w3.org/2000/svg\"><g><title></title><path d=\"M2.53001 7.81595C3.49179 4.73911 6.43281 2.5 9.91173 2.5C13.1684 2.5 15.9537 4.46214 17.0852 7.23684L17.6179 8.67647M17.6179 8.67647L18.5002 4.26471M17.6179 8.67647L13.6473 6.91176M17.4995 12.1841C16.5378 15.2609 13.5967 17.5 10.1178 17.5C6.86118 17.5 4.07589 15.5379 2.94432 12.7632L2.41165 11.3235M2.41165 11.3235L1.5293 15.7353M2.41165 11.3235L6.38224 13.0882\"></path></g></svg></button><button class=\"pencraft pc-reset pencraft icon-container view-image\" tabindex=\"0\" type=\"button\"><svg class=\"lucide lucide-maximize2 lucide-maximize-2\" fill=\"none\" height=\"20\" stroke=\"currentColor\" stroke-linecap=\"round\" stroke-linejoin=\"round\" stroke-width=\"2\" viewBox=\"0 0 24 24\" width=\"20\" xmlns=\"http://www.w3.org/2000/svg\"><polyline points=\"15 3 21 3 21 9\"></polyline><polyline points=\"9 21 3 21 3 15\"></polyline><line x1=\"21\" x2=\"14\" y1=\"3\" y2=\"10\"></line><line x1=\"3\" x2=\"10\" y1=\"21\" y2=\"14\"></line></svg></button></div></div></div></a><figcaption class=\"image-caption\">Early sketches of the vortex mixer invention, 1958. Credit: Scott Kraft</figcaption></figure></div><p>They first demonstrated their invention before a group of scientists at Sunnyside Medical Laboratory in Long Island on October 25th, 1958, just days after building the prototype.<a class=\"footnote-anchor\" href=\"https://www.asimov.press/feed#footnote-3\" id=\"footnote-anchor-3\" target=\"_self\">3</a> The vortex was used to wash a protein in water, a step in the <a href=\"https://dm5migu4zj3pb.cloudfront.net/manuscripts/102000/102416/JCI51102416.pdf\">protein-bound iodine (PBI) test</a>, a now-antiquated clinical method for measuring thyroid function. The PBI test <a href=\"https://jnm.snmjournals.org/content/jnumed/8/2/123.full.pdf\">was regarded as</a> &#8220;one of the more complicated and commonly used clinical laboratory tests, requiring skilled technicians and generally requiring a separate laboratory because of contamination problems,&#8221; according to a 1967 study. Its several steps traditionally required thorough cleaning of the stirring implements, but each glass stir rod that was introduced increased both the chance of contamination and the potential loss of precious sample material stuck to the glass. A mixing method that didn&#8217;t require constant touching of the sample would be valuable indeed.</p><p><a href=\"https://patentimages.storage.googleapis.com/b4/19/cd/4c1db017327199/US3061280.pdf\">The patent</a>, which Harold and Jack filed on April 6th, 1959, was granted on October 30, 1962. During this time, the brothers were not idle. They began to work with Scientific Industries Inc., a scientific equipment company established in Springfield, Massachusetts, in 1954. Harold became its president and Jack its treasurer, and the two moved the company to Queens Village, New York. Scientific Industries Inc. began manufacturing and selling the Kraft brothers&#8217; vortexer in 1962. The very first model was named the Vortex Jr. Mixer. The company also experimented with other form factors and head attachments, like the K-500-4 model, able to mix four tubes simultaneously.</p><div class=\"captioned-image-container\"><figure><a class=\"image-link image2 is-viewable-img\" href=\"https://substackcdn.com/image/fetch/$s_!KPXo!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fac588a24-6427-4e94-910e-d7cfbade5d27_3181x2507.jpeg\" target=\"_blank\"><div class=\"image2-inset\"><source type=\"image/webp\" /><img alt=\"\" class=\"sizing-normal\" height=\"1147\" src=\"https://substackcdn.com/image/fetch/$s_!KPXo!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fac588a24-6427-4e94-910e-d7cfbade5d27_3181x2507.jpeg\" width=\"1456\" /><div class=\"image-link-expand\"><div class=\"pencraft pc-display-flex pc-gap-8 pc-reset\"><button class=\"pencraft pc-reset pencraft icon-container restack-image\" tabindex=\"0\" type=\"button\"><svg fill=\"none\" height=\"20\" stroke=\"var(--color-fg-primary)\" stroke-linecap=\"round\" stroke-linejoin=\"round\" stroke-width=\"1.5\" viewBox=\"0 0 20 20\" width=\"20\" xmlns=\"http://www.w3.org/2000/svg\"><g><title></title><path d=\"M2.53001 7.81595C3.49179 4.73911 6.43281 2.5 9.91173 2.5C13.1684 2.5 15.9537 4.46214 17.0852 7.23684L17.6179 8.67647M17.6179 8.67647L18.5002 4.26471M17.6179 8.67647L13.6473 6.91176M17.4995 12.1841C16.5378 15.2609 13.5967 17.5 10.1178 17.5C6.86118 17.5 4.07589 15.5379 2.94432 12.7632L2.41165 11.3235M2.41165 11.3235L1.5293 15.7353M2.41165 11.3235L6.38224 13.0882\"></path></g></svg></button><button class=\"pencraft pc-reset pencraft icon-container view-image\" tabindex=\"0\" type=\"button\"><svg class=\"lucide lucide-maximize2 lucide-maximize-2\" fill=\"none\" height=\"20\" stroke=\"currentColor\" stroke-linecap=\"round\" stroke-linejoin=\"round\" stroke-width=\"2\" viewBox=\"0 0 24 24\" width=\"20\" xmlns=\"http://www.w3.org/2000/svg\"><polyline points=\"15 3 21 3 21 9\"></polyline><polyline points=\"9 21 3 21 3 15\"></polyline><line x1=\"21\" x2=\"14\" y1=\"3\" y2=\"10\"></line><line x1=\"3\" x2=\"10\" y1=\"21\" y2=\"14\"></line></svg></button></div></div></div></a><figcaption class=\"image-caption\">The four-tube vortex mixer. Credit: Scott Kraft</figcaption></figure></div><div class=\"captioned-image-container\"><figure><a class=\"image-link image2 is-viewable-img\" href=\"https://substackcdn.com/image/fetch/$s_!WfY_!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6f122a6e-6f8c-4849-a920-312382c9c80b_3181x2497.jpeg\" target=\"_blank\"><div class=\"image2-inset\"><source type=\"image/webp\" /><img alt=\"\" class=\"sizing-normal\" height=\"1143\" src=\"https://substackcdn.com/image/fetch/$s_!WfY_!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6f122a6e-6f8c-4849-a920-312382c9c80b_3181x2497.jpeg\" width=\"1456\" /><div class=\"image-link-expand\"><div class=\"pencraft pc-display-flex pc-gap-8 pc-reset\"><button class=\"pencraft pc-reset pencraft icon-container restack-image\" tabindex=\"0\" type=\"button\"><svg fill=\"none\" height=\"20\" stroke=\"var(--color-fg-primary)\" stroke-linecap=\"round\" stroke-linejoin=\"round\" stroke-width=\"1.5\" viewBox=\"0 0 20 20\" width=\"20\" xmlns=\"http://www.w3.org/2000/svg\"><g><title></title><path d=\"M2.53001 7.81595C3.49179 4.73911 6.43281 2.5 9.91173 2.5C13.1684 2.5 15.9537 4.46214 17.0852 7.23684L17.6179 8.67647M17.6179 8.67647L18.5002 4.26471M17.6179 8.67647L13.6473 6.91176M17.4995 12.1841C16.5378 15.2609 13.5967 17.5 10.1178 17.5C6.86118 17.5 4.07589 15.5379 2.94432 12.7632L2.41165 11.3235M2.41165 11.3235L1.5293 15.7353M2.41165 11.3235L6.38224 13.0882\"></path></g></svg></button><button class=\"pencraft pc-reset pencraft icon-container view-image\" tabindex=\"0\" type=\"button\"><svg class=\"lucide lucide-maximize2 lucide-maximize-2\" fill=\"none\" height=\"20\" stroke=\"currentColor\" stroke-linecap=\"round\" stroke-linejoin=\"round\" stroke-width=\"2\" viewBox=\"0 0 24 24\" width=\"20\" xmlns=\"http://www.w3.org/2000/svg\"><polyline points=\"15 3 21 3 21 9\"></polyline><polyline points=\"9 21 3 21 3 15\"></polyline><line x1=\"21\" x2=\"14\" y1=\"3\" y2=\"10\"></line><line x1=\"3\" x2=\"10\" y1=\"21\" y2=\"14\"></line></svg></button></div></div></div></a><figcaption class=\"image-caption\">The first commercial vortex mixer from Scientific Industries Inc. Credit: Scott Kraft</figcaption></figure></div><p>The Kraft brothers also continued tinkering, filing a number of patents by themselves and with Scientific Industries for devices such as the &#8220;<a href=\"https://patents.google.com/patent/US3163404A/en?inventor=Jack+A+Kraft\">Rotary apparatus for agitating fluids</a>,&#8221; which revolves tubes in a Ferris wheel-like motion to continuously stir them. This tube rotator, though lesser known than the mighty vortexer, is also a common laboratory tool today.</p><p>In April of 1965, Harold and Jack left Scientific Industries to found their own scientific instrument manufacturing business, Kraft Apparatus Inc., in Mineola, New York. They continued to refine the design of the vortexer, adding a pressure-sensitive &#8220;touch&#8221; feature that turned the device on when the rubber cup was pressed down, as well as speed and pulse settings. Finally, in 1982, the Kraft Brothers sold Kraft Apparatus to Glas-Col, a division of Templeton Coal, which still sells vortexers today. Meanwhile, Scientific Industries continued to manufacture its own versions, creating the iconic &#8220;Vortex Genie&#8221; line.<a class=\"footnote-anchor\" href=\"https://www.asimov.press/feed#footnote-4\" id=\"footnote-anchor-4\" target=\"_self\">4</a></p><div class=\"captioned-image-container\"><figure><a class=\"image-link image2 is-viewable-img\" href=\"https://substackcdn.com/image/fetch/$s_!Xo_P!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff701e4d0-7ee7-4fdd-9ca4-9d3ac3a285a1_469x768.png\" target=\"_blank\"><div class=\"image2-inset\"><source type=\"image/webp\" /><img alt=\"\" class=\"sizing-normal\" height=\"768\" src=\"https://substackcdn.com/image/fetch/$s_!Xo_P!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff701e4d0-7ee7-4fdd-9ca4-9d3ac3a285a1_469x768.png\" width=\"469\" /><div class=\"image-link-expand\"><div class=\"pencraft pc-display-flex pc-gap-8 pc-reset\"><button class=\"pencraft pc-reset pencraft icon-container restack-image\" tabindex=\"0\" type=\"button\"><svg fill=\"none\" height=\"20\" stroke=\"var(--color-fg-primary)\" stroke-linecap=\"round\" stroke-linejoin=\"round\" stroke-width=\"1.5\" viewBox=\"0 0 20 20\" width=\"20\" xmlns=\"http://www.w3.org/2000/svg\"><g><title></title><path d=\"M2.53001 7.81595C3.49179 4.73911 6.43281 2.5 9.91173 2.5C13.1684 2.5 15.9537 4.46214 17.0852 7.23684L17.6179 8.67647M17.6179 8.67647L18.5002 4.26471M17.6179 8.67647L13.6473 6.91176M17.4995 12.1841C16.5378 15.2609 13.5967 17.5 10.1178 17.5C6.86118 17.5 4.07589 15.5379 2.94432 12.7632L2.41165 11.3235M2.41165 11.3235L1.5293 15.7353M2.41165 11.3235L6.38224 13.0882\"></path></g></svg></button><button class=\"pencraft pc-reset pencraft icon-container view-image\" tabindex=\"0\" type=\"button\"><svg class=\"lucide lucide-maximize2 lucide-maximize-2\" fill=\"none\" height=\"20\" stroke=\"currentColor\" stroke-linecap=\"round\" stroke-linejoin=\"round\" stroke-width=\"2\" viewBox=\"0 0 24 24\" width=\"20\" xmlns=\"http://www.w3.org/2000/svg\"><polyline points=\"15 3 21 3 21 9\"></polyline><polyline points=\"9 21 3 21 3 15\"></polyline><line x1=\"21\" x2=\"14\" y1=\"3\" y2=\"10\"></line><line x1=\"3\" x2=\"10\" y1=\"21\" y2=\"14\"></line></svg></button></div></div></div></a><figcaption class=\"image-caption\">The 1962 patent filing.</figcaption></figure></div><p>Today, scientists commonly use vortexers to mix volumes ranging from microliters to milliliters, most often in plastic Eppendorf or conical tubes.<a class=\"footnote-anchor\" href=\"https://www.asimov.press/feed#footnote-5\" id=\"footnote-anchor-5\" target=\"_self\">5</a> As scientific instruments go, vortex mixers are on the sturdy side, weighing in at 4 kilograms, or 8.8 pounds. Their heft and rubber feet provide stability, preventing them from vibrating off the bench. And although newer versions are slightly quieter, older ones emit a distinctive rumble that can be heard echoing throughout the lab.</p><p>Vortex mixers also come in a variety of shapes and sizes.<a class=\"footnote-anchor\" href=\"https://www.asimov.press/feed#footnote-6\" id=\"footnote-anchor-6\" target=\"_self\">6</a> There are versions that secure 96-well plates or Eppendorf tubes for hands-free mixing. Some have adjustable speed, orbital direction, and digital timers to make mixing more precise.</p><p>We owe much to the industrious Kraft brothers, for although the vortex mixer is relatively humble and was created in a span of just three days, its importance is hard to overstate: making mixing, one of the most essential but tedious laboratory chores, easy.</p><p class=\"button-wrapper\"><a class=\"button primary\" href=\"https://www.asimov.press/subscribe\"><span>Subscribe now</span></a></p><div><hr /></div><p><strong>Ella Watkins-Dulaney </strong>is a bioengineer who owes a not-insignificant portion of her PhD to the vortexer. She is also the Art Director for Asimov Press.</p><p><strong>Acknowledgements: </strong>Thank you to Howard J., Randy E., Robert, and Ruth Kraft for their interviews about Kraft family history. A special thank you to Scott Kraft for all of his work collecting historical documents and helping me piece together this story. It would not have been possible without you. And thank you to Xander Balwit as well for the consistent editing and support.</p><p><strong>Cite: </strong>Watkins-Dulaney, E. &#8220;Making the Vortex Mixer.&#8221; <em>Asimov Press</em> (2026). DOI: <a href=\"https://doi.org/10.62211/49jq-97pk\">10.62211/49jq-97pk</a></p><div class=\"footnote\"><a class=\"footnote-number\" contenteditable=\"false\" href=\"https://www.asimov.press/feed#footnote-anchor-1\" id=\"footnote-1\" target=\"_self\">1</a><div class=\"footnote-content\"><p>How vigorously the vessel was agitated depended on the size of the vessel and volume to be mixed. For example, a separatory funnel might be held in two hands and shaken up and down, while a test tube could be held in one hand and flicked at the bottom to produce a vortex-like motion.</p></div></div><div class=\"footnote\"><a class=\"footnote-number\" contenteditable=\"false\" href=\"https://www.asimov.press/feed#footnote-anchor-2\" id=\"footnote-2\" target=\"_self\">2</a><div class=\"footnote-content\"><p>The letter and accompanying invention sketches that were made on their lawyers letterhead, dated October 30th, were presumably to document the invention for patenting.</p></div></div><div class=\"footnote\"><a class=\"footnote-number\" contenteditable=\"false\" href=\"https://www.asimov.press/feed#footnote-anchor-3\" id=\"footnote-3\" target=\"_self\">3</a><div class=\"footnote-content\"><p>In their concept sketches, Milton and Will were affectionately referred to only by their first names (&#8220;Milty&#8221; and &#8220;Will&#8221;), so it is assumed that the brothers had a closer personal relationship with this group than with Natelson.</p></div></div><div class=\"footnote\"><a class=\"footnote-number\" contenteditable=\"false\" href=\"https://www.asimov.press/feed#footnote-anchor-4\" id=\"footnote-4\" target=\"_self\">4</a><div class=\"footnote-content\"><p>The Genie Line was <a href=\"https://us.ohaus.com/en-us/about-us/news/ohaus-welcomes-genie-%E2%80%93-expanding-horizons-in-labor\">acquired by OHAUS Corporation in 2025</a>.</p></div></div><div class=\"footnote\"><a class=\"footnote-number\" contenteditable=\"false\" href=\"https://www.asimov.press/feed#footnote-anchor-5\" id=\"footnote-5\" target=\"_self\">5</a><div class=\"footnote-content\"><p>Volumes under 100 microliters are more practical to mix with a pipette. For volumes larger than a few hundred milliliters, it is more practical to use a magnetic stir bar.</p></div></div><div class=\"footnote\"><a class=\"footnote-number\" contenteditable=\"false\" href=\"https://www.asimov.press/feed#footnote-anchor-6\" id=\"footnote-6\" target=\"_self\">6</a><div class=\"footnote-content\"><p>The patents for the original vortex expired in 1979, and most derivative patents have as well.</p></div></div>"
            ],
            "link": "https://www.asimov.press/p/vortex",
            "publishedAt": "2026-02-12",
            "source": "Asimov Press",
            "summary": "<div class=\"captioned-image-container\"><figure><a class=\"image-link image2 is-viewable-img\" href=\"https://substackcdn.com/image/fetch/$s_!5aIJ!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3e6009ee-a8fe-4f00-b181-6f59cde250f6_2000x1260.jpeg\" target=\"_blank\"><div class=\"image2-inset\"><source type=\"image/webp\" /><img alt=\"\" class=\"sizing-normal\" height=\"917\" src=\"https://substackcdn.com/image/fetch/$s_!5aIJ!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3e6009ee-a8fe-4f00-b181-6f59cde250f6_2000x1260.jpeg\" width=\"1456\" /><div class=\"image-link-expand\"><div class=\"pencraft pc-display-flex pc-gap-8 pc-reset\"><button class=\"pencraft pc-reset pencraft icon-container restack-image\" tabindex=\"0\" type=\"button\"><svg fill=\"none\" height=\"20\" stroke=\"var(--color-fg-primary)\" stroke-linecap=\"round\" stroke-linejoin=\"round\" stroke-width=\"1.5\" viewBox=\"0 0 20 20\" width=\"20\" xmlns=\"http://www.w3.org/2000/svg\"><g><title></title><path d=\"M2.53001 7.81595C3.49179 4.73911 6.43281 2.5 9.91173 2.5C13.1684 2.5 15.9537 4.46214 17.0852 7.23684L17.6179 8.67647M17.6179 8.67647L18.5002 4.26471M17.6179 8.67647L13.6473 6.91176M17.4995 12.1841C16.5378 15.2609 13.5967 17.5 10.1178 17.5C6.86118 17.5 4.07589 15.5379 2.94432 12.7632L2.41165 11.3235M2.41165 11.3235L1.5293 15.7353M2.41165 11.3235L6.38224 13.0882\"></path></g></svg></button><button class=\"pencraft pc-reset pencraft icon-container view-image\" tabindex=\"0\" type=\"button\"><svg class=\"lucide lucide-maximize2 lucide-maximize-2\" fill=\"none\" height=\"20\" stroke=\"currentColor\" stroke-linecap=\"round\" stroke-linejoin=\"round\" stroke-width=\"2\" viewBox=\"0 0 24 24\" width=\"20\" xmlns=\"http://www.w3.org/2000/svg\"><polyline points=\"15 3 21 3 21 9\"></polyline><polyline points=\"9 21 3 21 3 15\"></polyline><line x1=\"21\" x2=\"14\" y1=\"3\" y2=\"10\"></line><line x1=\"3\" x2=\"10\" y1=\"21\" y2=\"14\"></line></svg></button></div></div></div></a></figure></div><p><em>This essay will appear in our forthcoming book, &#8220;Making the Modern Laboratory,&#8221; to be published later this year.</em></p><p>In 1959, a pair of enterprising brothers, Jack and Harold Kraft, filed a <a href=\"https://patents.google.com/patent/US3061280A/en\">patent</a> titled &#8220;Apparatus for mixing fluent material.&#8221; Though simple in concept, their invention solved one of the most fundamental challenges faced by mid-century scientists: mixing fluids quickly and efficiently. The <strong>vortex mixer, </strong>a small motorized device that vibrated samples,<strong> </strong>offered the perfect solution, and is now found on biology benches across the world.</p><p>Harold and Jack",
            "title": "Making the Vortex Mixer"
        },
        {
            "content": [
                "<p>Up till now, I've been pretty careful not to let agents write my blog posts outside of very well-flagged sections of posts where the whole conceit was &quot;what does the agent think?&quot;</p>\n<p>One of the wildest things that I've been running into lately is that my ability to create software has been outstripping my ability to explain it or write about it or even announce it. So I've been making stuff and not telling anybody about it.</p>\n<p>That's far from ideal.</p>\n<p>At the same time, I don't really want to fill this blog with software release announcements. Because, for example, I think I have at least four to put out today.</p>\n<p>Actually, I miscounted.</p>\n<p>It's five if you count the Superpowers v4.3.0 release.</p>\n<p>A few minutes ago, I made an executive decision and decided to create <a href=\"https://blog.fsck.com/releases/\">a new sub-blog just for software release</a>. Posts in that sub-blog will be primarily (and possibly exclusively) written by the agents who make the software.</p>\n<p>Today's releases:</p>\n<ul>\n<li><a href=\"https://blog.fsck.com/releases/2026/02/12/superpowers-v4-3-0/\">Superpowers 4.3.0</a></li>\n<li><a href=\"https://blog.fsck.com/releases/2026/02/12/claude-session-driver/\">Claude Session Driver 1.0</a></li>\n<li><a href=\"https://blog.fsck.com/releases/2026/02/13/wayback-restorer-v0-1-0/\">Wayback Restorer 0.1</a></li>\n<li><a href=\"https://blog.fsck.com/releases/2026/02/12/comic-ocr/\">Comic OCR</a></li>\n</ul>\n<p>And one more to come a bit later.</p>\n<p>So, why was I working on...comic OCR and restoring websites from the Internet Archive?</p>\n<p>Well, it all started this morning, when someone posted to a group chat about an agent that <a href=\"https://github.com/matplotlib/matplotlib/pull/31132\">opened a pull request against matplotlib</a> in violation of their AI policy. It then posted a now-deleted hit-piece on its blog. (It later posted a...<a href=\"https://crabby-rathbun.github.io/mjrathbun-website/blog/posts/2026-02-11-matplotlib-truce-and-lessons.html\">retraction?</a>.)</p>\n<p>This sparked a memory of a wonderful bit of snark from <a href=\"https://somethingpositive.net\">Something*Positive</a> that I wanted to link into the chat.</p>\n<p>&quot;When I get home, I am so letting you have it in my livejournal.&quot;</p>\n<p>The only problem is that...the comic in question appears to be just...gone, due to a hosting catastrophe a couple of years back.</p>\n<p>There is a single reference to it in Google. A blog post that I wrote in 2003.</p>\n<p>But it turns out that the entire history of S*P was captured by the wonderful folks at the Internet Archive.</p>\n<p>Unforunately, it looks like nobody ever transcribed S*P somewhere public.</p>\n<p>Thankfully, we live in the future.</p>\n<p>In one session, I had Codex put together a tool to restore a website from the Internet Archive. I told it that it needed to be careful to comply with all of their web scraping policies. It spent a little bit of time figuring out what those rules were. By reading their policy web pages. And then it started building.</p>\n<p>The scraper tool has been running for a few hours now, carefully downloading one file every couple of seconds.</p>\n<p>In another window, I asked Claude to put together a tool to OCR webcomics and build a JSON file of the transcripts. The transcripts were a little bit insane because the source material is all caps speech bubbles. So I added another step where it runs those transcripts through an LLM to clean them up.</p>\n<p>And then I let everything cook.</p>\n<p>While it was running, I had Claude add that new releases side blog and went back to today's sessions and asked each of them to write up their release announcement.</p>\n<p>Just a few minutes ago, I ran the OCR tool against the first 600 comics from Something Positive.</p>\n<p><a class=\"glightbox\" href=\"https://blog.fsck.com/assets/2026/02/pasted-image-20260212-180036.png\"><img alt=\"pasted image 20260212 180036\" src=\"https://blog.fsck.com/assets/2026/02/pasted-image-20260212-180036.png\" /></a></p>\n<p>Turns out it was May 10, 2002.</p>"
            ],
            "link": "https://blog.fsck.com/2026/02/12/letting-agents-blog/",
            "publishedAt": "2026-02-12",
            "source": "Jesse Vincent",
            "summary": "<p>Up till now, I've been pretty careful not to let agents write my blog posts outside of very well-flagged sections of posts where the whole conceit was &quot;what does the agent think?&quot;</p> <p>One of the wildest things that I've been running into lately is that my ability to create software has been outstripping my ability to explain it or write about it or even announce it. So I've been making stuff and not telling anybody about it.</p> <p>That's far from ideal.</p> <p>At the same time, I don't really want to fill this blog with software release announcements. Because, for example, I think I have at least four to put out today.</p> <p>Actually, I miscounted.</p> <p>It's five if you count the Superpowers v4.3.0 release.</p> <p>A few minutes ago, I made an executive decision and decided to create <a href=\"https://blog.fsck.com/releases/\">a new sub-blog just for software release</a>. Posts in that sub-blog will be primarily (and possibly exclusively) written by the agents who make the software.</p> <p>Today's releases:</p> <ul> <li><a href=\"https://blog.fsck.com/releases/2026/02/12/superpowers-v4-3-0/\">Superpowers 4.3.0</a></li> <li><a href=\"https://blog.fsck.com/releases/2026/02/12/claude-session-driver/\">Claude Session Driver 1.0</a></li> <li><a href=\"https://blog.fsck.com/releases/2026/02/13/wayback-restorer-v0-1-0/\">Wayback Restorer 0.1</a></li> <li><a href=\"https://blog.fsck.com/releases/2026/02/12/comic-ocr/\">Comic OCR</a></li> </ul> <p>And one more to come a bit later.</p> <p>So, why was I working on...comic OCR and restoring websites from",
            "title": "Letting agents post on my blog; finding a needle in a haystack"
        },
        {
            "content": [],
            "link": "https://interconnected.org/home/2026/02/12/mist",
            "publishedAt": "2026-02-12",
            "source": "Matt Webb",
            "summary": "<div> <p>It should be SO EASY to share + collaborate on Markdown text files. The AI world runs on .md files. Yet frictionless Google Docs-style collab is so hard\u2026 UNTIL NOW, and how about that for a tease.</p> <p>If you don\u2019t know Markdown, it\u2019s a way to format a simple text file with marks like <code>**bold**</code> and <code># Headers</code> and <code>-</code> lists\u2026 e.g. <a href=\"https://interconnected.org/home/2026/02/12/mist.md\">here\u2019s the Markdown for this blog post</a>.</p> <p>Pretty much all AI prompts are written in Markdown; engineers coding with AI agents have folders full of .md files and that\u2019s what they primarily work on now. A lot of blog posts too: if you want to collaborate on a blog post ahead of publishing, it\u2019s gonna be Markdown. Keep notes in software like Obsidian? Folders of Markdown.</p> <p>John Gruber invented the Markdown format in 2004. <a href=\"https://daringfireball.net/projects/markdown/\">Here\u2019s the Markdown spec</a>, it hasn\u2019t changed since. Which is its strength. Read Anil Dash\u2019s essay <a href=\"https://www.anildash.com/2026/01/09/how-markdown-took-over-the-world/\">How Markdown Took Over the World</a> (2026) for more.</p> <p>So it\u2019s a wildly popular format with lots of interop that humans can read+write and machines too.</p> <p>AND YET\u2026 where is Google Docs for Markdown?</p> <p>I want to be able to share a Markdown doc",
            "title": "mist: Share and edit Markdown together, quickly (new tool)"
        },
        {
            "content": [],
            "link": "https://www.nytimes.com/2026/02/12/style/modern-dating-rules.html",
            "publishedAt": "2026-02-12",
            "source": "Modern Love - NYT",
            "summary": "On Valentine\u2019s Day 31 years ago, the book \u201cThe Rules\u201d provided 35 decidedly retro \u201crules\u201d for dating. We thought it was time for a refresh, so we asked people what rules they rely on today. Here are the best.",
            "title": "35 Modern Dating Rules That People Rely on Today"
        },
        {
            "content": [],
            "link": "https://www.ssp.sh/blog/obsidian-rag-duckdb-sql/",
            "publishedAt": "2026-02-12",
            "source": "Simon Spati",
            "summary": "<p>I always wanted a personal knowledge assistant based on my notes. One that uses Obsidian&rsquo;s backlinks and connections to surface ideas I&rsquo;ve forgotten or never thought to link together.</p> <p>So I built one. A RAG system that runs locally with DuckDB as a <a href=\"https://www.ssp.sh/blog/vector-technologies-ai-data-stack/\" rel=\"\">vector database</a>, then syncs to MotherDuck for a serverless web app running entirely in the browser via WASM. Think of it like J.A.R.V.I.S<sup id=\"fnref:1\"><a class=\"footnote-ref\" href=\"https://www.ssp.sh/index.xml#fn:1\">1</a></sup> for your markdown files: search about a topic, and it shows connected notes up to two hops away, semantically similar content, and hidden connections between ideas that share no direct links.</p> <p>In this article, I walk through how I built this and how it works, from using DuckDB&rsquo;s vector extension locally to serving embeddings through MotherDuck&rsquo;s WASM client. Along the way, you&rsquo;ll see how data engineering skills can make use of lots of note-markdown files. If you want to dive straight into the code, it&rsquo;s all on GitHub at <a href=\"https://github.com/sspaeti/obsidian-note-taking-assistant\" rel=\"noopener noreffer\" target=\"_blank\">Obsidian-note-taking-assistant</a>, and you can try the web app on my public notes at <a href=\"https://explore.ssp.sh\" rel=\"noopener noreffer\" target=\"_blank\">Explore RAG</a>.</p> <p>For building the web app I used Claude Code and it came together in a few hours",
            "title": "Building an Obsidian RAG with DuckDB and MotherDuck"
        },
        {
            "content": [
                "<p><em>[Original post: <a href=\"https://www.astralcodexten.com/p/biological-anchors-a-trick-that-might\">Biological Anchors: A Trick That Might Or Might Not Work</a>]</em></p><p><strong>I.</strong></p><p>Ajeya Cotra&#8217;s <a href=\"https://drive.google.com/drive/u/0/folders/15ArhEPZSTYU8f012bs6ehPS6-xmhtBPP\">Biological Anchors</a> report was the landmark AI timelines forecast of the early 2020s. In many ways, it was incredibly prescient - it nailed the scaling hypothesis, predicted the current AI boom, and introduced concepts like &#8220;time horizons&#8221; that have entered common parlance. In most cases where its contemporaries challenged it, its assumptions have been borne out, and its challengers proven wrong.</p><p>But its headline prediction - an AGI timeline centered around the 2050s - no longer seems plausible. The <a href=\"https://agi.goodheartlabs.com/\">current state</a> of the discussion ranges from late <a href=\"https://ai-2027.com/\">2020s</a> to <a href=\"https://epoch.ai/gradient-updates/the-case-for-multi-decade-ai-timelines\">2040s</a>, with more remote dates relegated to those who expect the current paradigm to prove ultimately fruitless - the opposite of Ajeya&#8217;s assumptions. Cotra later shortened her own timelines to 2040 (<a href=\"https://www.alignmentforum.org/posts/AfH2oPHCApdKicM4m/two-year-update-on-my-personal-ai-timelines\">as of 2022</a>) and they are probably even shorter now.</p><p>So, if its premises were impressively correct, but its conclusion twenty years too late, what went wrong in the middle?</p><p><strong>II.</strong> </p><p>First, a refresher. What was Bio Anchors? How did it work?</p><p>In 2020, the most advanced AI, GPT-3, had required about 10^23 FLOPs to train.</p><p>(FLOPs are a measure of computation: big, powerful computers and data centers can deploy more FLOPs than smaller ones)</p><p>Cotra asked: how quickly is the AI industry getting access to more compute / more FLOPs? And how many FLOPs would AGI take? If we can figure out both those things, determining the date of AGI arrival becomes a matter of simple division.</p><p>She found that FLOPs had been increasing at a constant factor for many years. And if you looked at planned data center construction, it looked on track to continue increasing at about that rate. New technological advances (algorithmic progress) made each FLOP more valuable in training AIs, but that process also seemed constant and predictable. So there was relatively constant growth in effective FLOPs (amount of computation available, adjusted by ability to use that computation efficiently).</p><p>There was no obvious way to know how many FLOPs AGI would take, but there were some intuitively compelling guesses - for example, an AGI that was as smart as humans might need a similar level of computing capacity as the human brain. Cotra picked five intuitively compelling guesses (the namesake Bio Anchors) and turned them into a weighted average.</p><p>Then she calculated: given the rate at which available FLOPs were increasing, and the number of FLOPs needed for AGI, how long until we closed the distance and got AGI?</p><p>At the time, I found this deeply unintuitive, but it&#8217;s held up! Improvement in AI since 2020 really has come from compute - the construction of giant data centers. Improvement in the underlying technology really has been measurable in &#8220;effective FLOPs&#8221;, ie the multiple it provides to compute, rather than some totally different incommensurable paradigm. And Cotra&#8217;s anchors - the intuitively compelling guesses about where AGI might be - match nicely with how far AI has improved since 2020 and how far it subjectively feels like it still has to go. All of the weird hard parts went as well as possible.</p><p>So, again, what went wrong?</p><p><strong>III.</strong></p><p>In 2023, Tom Davidson published <a href=\"https://www.astralcodexten.com/p/davidson-on-takeoff-speeds\">an updated version of Bio Anchors</a> that added a term representing the possibility of recursive self-improvement. The new calculations shifted the median date of AGI from 2053 &#8594; 2043. This doesn&#8217;t explain why our own timeline seems to be going faster than Bio Anchors: even 2043 now feels on the late side, and anyway recursive self-improvement has barely begun to have effects. </p><p>But in 2025, <a href=\"https://www.lesswrong.com/posts/jLEcddwp4RBTpPHHq/takeoff-speeds-update-crunch-time-1\">John Croxton published</a> a thorough report card on Davidson&#8217;s model. He took his numbers <a href=\"https://epoch.ai/trends\">from Epoch</a>, who used real data from the 2020 - 2025 period that earlier forecasters didn&#8217;t have access to, as well as the latest projections for what AI companies plan to do over the next few years.  to with more formal projections.  Most of his critiques apply to Bio Anchors too. We&#8217;ll be making use of them here.</p><p>Croxton found that Cotra and Davidson underestimated annual growth in effective compute:</p><div class=\"captioned-image-container\"><figure><a class=\"image-link image2 is-viewable-img\" href=\"https://substackcdn.com/image/fetch/$s_!99Rc!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F11a861dc-d790-49b7-a4ef-174963e93811_619x318.png\" target=\"_blank\"><div class=\"image2-inset\"><source type=\"image/webp\" /><img alt=\"\" class=\"sizing-normal\" height=\"318\" src=\"https://substackcdn.com/image/fetch/$s_!99Rc!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F11a861dc-d790-49b7-a4ef-174963e93811_619x318.png\" width=\"619\" /><div class=\"image-link-expand\"><div class=\"pencraft pc-display-flex pc-gap-8 pc-reset\"><button class=\"pencraft pc-reset pencraft icon-container restack-image\" tabindex=\"0\" type=\"button\"><svg fill=\"none\" height=\"20\" stroke=\"var(--color-fg-primary)\" stroke-linecap=\"round\" stroke-linejoin=\"round\" stroke-width=\"1.5\" viewBox=\"0 0 20 20\" width=\"20\" xmlns=\"http://www.w3.org/2000/svg\"><g><title></title><path d=\"M2.53001 7.81595C3.49179 4.73911 6.43281 2.5 9.91173 2.5C13.1684 2.5 15.9537 4.46214 17.0852 7.23684L17.6179 8.67647M17.6179 8.67647L18.5002 4.26471M17.6179 8.67647L13.6473 6.91176M17.4995 12.1841C16.5378 15.2609 13.5967 17.5 10.1178 17.5C6.86118 17.5 4.07589 15.5379 2.94432 12.7632L2.41165 11.3235M2.41165 11.3235L1.5293 15.7353M2.41165 11.3235L6.38224 13.0882\"></path></g></svg></button><button class=\"pencraft pc-reset pencraft icon-container view-image\" tabindex=\"0\" type=\"button\"><svg class=\"lucide lucide-maximize2 lucide-maximize-2\" fill=\"none\" height=\"20\" stroke=\"currentColor\" stroke-linecap=\"round\" stroke-linejoin=\"round\" stroke-width=\"2\" viewBox=\"0 0 24 24\" width=\"20\" xmlns=\"http://www.w3.org/2000/svg\"><polyline points=\"15 3 21 3 21 9\"></polyline><polyline points=\"9 21 3 21 3 15\"></polyline><line x1=\"21\" x2=\"14\" y1=\"3\" y2=\"10\"></line><line x1=\"3\" x2=\"10\" y1=\"21\" y2=\"14\"></line></svg></button></div></div></div></a><figcaption class=\"image-caption\">Epoch/Croxton are current best estimates, and can probably fairly be read as the &#8220;real&#8221; answer against which Cotra and Davidson&#8217;s earlier guesses should be judged.</figcaption></figure></div><p>All numbers are yearly multiples, so 1.4 means that willingness to spend grows 1.4x per year, ie 40%.</p><p><strong>Willingness To Spend: </strong>How much money are companies willing to spend on AI, in the form of chips and data centers?</p><p><strong>$/FLOP: </strong>How quickly do Moore&#8217;s Law, economies of scale, and other factors bring down the price of AI compute?</p><p><strong>Training Run Length: </strong>How long are companies spending on AI training runs for frontier models (instead of using those chips for smaller models, experiments, or consumer services)?</p><p><strong>Real Compute:</strong> The product of the three parameters above.</p><p><strong>Algorithmic Progress:</strong> How effectively do researchers discover new algorithms that makes training AIs cheaper and more efficient?</p><p><strong>Total Effective Compute: </strong>The product of real compute and algorithmic progress. So for example, the Epoch column&#8217;s 10.7x means that in any given year, you can train an AI 10.7x better than the last year, because you have 3.6x more compute available, and that compute is 3.0x more efficient.</p><p>Cotra and Davidson were pretty close on willingness to spend and on FLOPs/$. This is an impressive achievement; they more or less predicted the giant data center buildout of the past few years. They ignored training run length, which probably seemed like a reasonable simplification at the time. But they got killed on algorithmic progress, which was 200% per year instead of 30%. How did they get this one so wrong?</p><p>Here&#8217;s Cotra&#8217;s section on algorithmic progress:</p><blockquote><p><strong>Algorithmic progress forecasts</strong></p><p><em>Note: I have done very little research into algorithmic progress trends. Of the four main components of my model (2020 compute requirements, algorithmic progress, compute price trends, and spending on computation) I have spent the least time thinking about algorithmic progress.</em></p><p>I consider two types of algorithmic progress: relatively incremental and steady progress from iteratively improving architectures and learning algorithms, and the chance of &#8220;breakthrough&#8221; progress which brings the technical difficulty of training a transformative model down from &#8220;astronomically large&#8221; / &#8220;impossible&#8221; to &#8220;broadly feasible.&#8221;</p><p>For incremental progress, the main source I used was Hernandez and Brown 2020, <a href=\"https://arxiv.org/abs/2005.04305\">&#8221;Measuring the Algorithmic Efficiency of Neural Networks&#8221;</a>. The authors reimplemented open source state-of-the-art (SOTA) ImageNet models between 2012 and 2019 (six models in total). They trained each model up to the point that it achieved the same performance as AlexNet achieved in 2012, and recorded the total FLOP that required. They found that the SOTA model in 2019, EfficientNet B0, required ~44 times fewer training FLOP to achieve AlexNet performance than AlexNet did; the six data points fit a power law curve with the amount of computation required to match AlexNet halving every ~16 months over the seven years in the dataset.&#178; They also show that linear programming displayed a similar trend over a longer period of time: when hardware is held fixed, the time in seconds taken to solve a standard basket of mixed integer programs by SOTA commercial software packages halved every ~13 months over the 21 years from 1996 to 2017.&#179;</p><p>Grace 2013 (<a href=\"https://intelligence.org/files/AlgorithmicProgress.pdf\">&#8221;Algorithmic Progress in Six Domains&#8221;</a>) is the only other paper attempting to systematically quantify algorithmic progress that I am currently aware of, although I have not done a systematic literature review and may be missing others. I have chosen not to examine it in detail because a) it was written largely before the deep learning boom and mostly does not focus on ML tasks, and b) it is less straightforward to translate Grace&#8217;s results into the format that I am most interested in (&#8221;How has the amount of computation required to solve a fixed task decreased over time?&#8221;). Paul is familiar with the results, and he believes that algorithmic progress across the six domains studied in Grace 2013&#8308; is consistent with a similar but slightly slower rate of progress, ranging from 13 to 36 months to halve the computation required to reach a fixed level of performance.</p><p>Additionally, it seems plausible to me that both sets of results would overestimate the pace of algorithmic progress on a transformative task, because they are both focusing on relatively narrow problems with simple, well-defined benchmarks that large groups of researchers could directly optimize.&#8309; Because no one has trained a transformative model yet, to the extent that the computation required to train one is falling over time, it would have to happen via proxies rather than researchers directly optimizing that metric (e.g. perhaps architectural innovations that improve training efficiency for image classifiers or language models would translate to a transformative model). Additionally, it may be that halving the amount of computation required to train a transformative model would require making progress on multiple partially-independent sub-problems (e.g. vision <em>and</em> language <em>and</em> motor control).</p><p>I have attempted to take the Hernandez and Brown 2020 halving times (and Paul&#8217;s summary of the Grace 2013 halving times) as anchoring points and shade them upward to account for the considerations raised above. There is massive room for judgment in whether and how much to shade upward; I expect many readers will want to change my assumptions here, and some will believe it is more reasonable to shade <em>downward</em>.</p></blockquote><p>Cotra&#8217;s estimate comes primarily from one paper, <a href=\"https://arxiv.org/abs/2005.04305\">Hernandez &amp; Brown</a>, which looks at algorithmic progress on a task called AlexNet. But <a href=\"https://arxiv.org/pdf/2212.05153\">later research</a> demonstrated that the apparent speed of algorithmic progress varies by an order of magnitude based on whether you&#8217;re looking at an easy task (low-hanging fruit already picked) or a hard task (still lots of room to improve). AlexNet was an easy task, but pushing the frontier of AI is a hard task, so algorithmic progress in frontier AI has been faster than the AlexNet paper estimated.</p><p>In Cotra&#8217;s defense, she admitted that this was the area where she was least certain, and that she had rounded the progress rate down based on various considerations when other people might round it up based on various other considerations. But the sheer extent of the error here, compounded with a few smaller errors that unfortunately all shared the same direction, was enough to throw off the estimate entirely.</p><p>Since Cotra and Davidson were expecting AI to get 3.6x more effective compute each year, but it actually got 10.7x more, it&#8217;s no mystery why their timelines were off. When John recalculates Davidson&#8217;s model with Epoch&#8217;s numbers, he finds that it estimates AGI in 2030, which matches the current vibes.</p><p><strong>IV.</strong></p><p>With this information in place, it&#8217;s worth looking at some prominent contemporaneous critiques of Bio Anchors.</p><p><strong>Various people</strong> criticized Bio Anchors&#8217; many strange anchors for how much compute it would take to produce AGI. For example, one anchor estimated that it would take 10^45 FLOPs, because that was how many calculations happened in all the brains of all animals throughout the evolutionary history (which eventually produced the human brain that AIs are trying to imitate). To make things even weirder, this anchor assumed away all animals other than nematodes as a rounding error (<a href=\"https://www.astralcodexten.com/p/biological-anchors-a-trick-that-might/comment/200054477\">fact check: true!</a>)</p><p>All of these seemed to detract from the main show, an attempt to estimate the compute involved in the human brain. But even this more sober anchor was complicated by time horizons - it&#8217;s not enough to imitate the human brain for one second; AIs need to be able to imitate the human brain&#8217;s capacity for long-term planning. Cotra calculated how much compute AGI would require if it needed a planning horizon of seconds, weeks, or years.</p><p>Thanks to METR, we now know that existing AIs have already passed a point where they can do most tasks that take humans seconds, are moving through the hour range, and are just about to touch one day. So the &#8220;seconds&#8221; anchor is ruled out. But it also seems unlikely that AGI will require years, because most human projects don&#8217;t take years, or at least can be split into tasks that take less than one year each (intuition pump: are we sure the average employee stays at an AI lab for more than a year? If not, that proves that a chain of people with sub-one-year time horizons can do valuable work). The AI Futures team guessed that the time horizon necessary for AIs to really start serious recursive self-improvement was between a few weeks and a few months (though this might look like a totally different number on the METR graph, which doesn&#8217;t translate perfectly into real life). If this is true, then all three anchors (seconds, hours, years) were off by at least an order of magnitude.</p><p>But it turns out that none of this matters very much. The highest and lowest anchors cancel out, so that the most plausible anchor - human brain with time horizon of hours to days - is around the average. If you remove all the other anchors and just keep that one, the model&#8217;s estimates barely change.</p><p>But also, we&#8217;re talking about crossing twelve orders of magnitude here. The difference between the different time horizon anchors doesn&#8217;t register much on that level, compared to things like algorithmic progress which have exponential effects.</p><p>Maybe this is the model basically working as intended. You try lots of different anchors, put more weight on the more plausible ones, take a weighted average of each of them, and hopefully get something close to the real value. Bio Anchors did.</p><p>Or maybe it was just good luck. Still hard to tell.</p><p><strong>Eliezer Yudkowsky</strong> <a href=\"https://www.lesswrong.com/posts/ax695frGJEzGxFBK4/biology-inspired-agi-timelines-the-trick-that-never-works\">argued that</a> the whole methodology was fundamentally flawed. Partly because of the argument above - he didn&#8217;t trust the anchors - but also partly because he expected the calculations to be obviated by some sort of paradigm shift that couldn&#8217;t be shoehorned into &#8220;algorithmic progress&#8221; (like how you couldn&#8217;t build an airplane in 1900 but you could in 1920). </p><p>As of 2026 - still before AGI has been invented and we get a good historical perspective - no such shift has occurred. The scaling laws have mostly held; whatever artificial space you try to measure models in, the measurement has mostly worked in a predictable way. There have really only been two kinks in the history of AI so far. First, a kink in training run size around 2010:</p><div class=\"captioned-image-container\"><figure><a class=\"image-link image2 is-viewable-img\" href=\"https://epoch.ai/blog/the-longest-training-run\" target=\"_blank\"><div class=\"image2-inset\"><source type=\"image/webp\" /><img alt=\"The longest training run | Epoch AI\" class=\"sizing-normal\" height=\"336.375\" src=\"https://substackcdn.com/image/fetch/$s_!tofx!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1e4d2fa4-fb79-4689-b0ad-bdfa180de4f3_3840x2160.png\" title=\"The longest training run | Epoch AI\" width=\"598\" /><div class=\"image-link-expand\"><div class=\"pencraft pc-display-flex pc-gap-8 pc-reset\"><button class=\"pencraft pc-reset pencraft icon-container restack-image\" tabindex=\"0\" type=\"button\"><svg fill=\"none\" height=\"20\" stroke=\"var(--color-fg-primary)\" stroke-linecap=\"round\" stroke-linejoin=\"round\" stroke-width=\"1.5\" viewBox=\"0 0 20 20\" width=\"20\" xmlns=\"http://www.w3.org/2000/svg\"><g><title></title><path d=\"M2.53001 7.81595C3.49179 4.73911 6.43281 2.5 9.91173 2.5C13.1684 2.5 15.9537 4.46214 17.0852 7.23684L17.6179 8.67647M17.6179 8.67647L18.5002 4.26471M17.6179 8.67647L13.6473 6.91176M17.4995 12.1841C16.5378 15.2609 13.5967 17.5 10.1178 17.5C6.86118 17.5 4.07589 15.5379 2.94432 12.7632L2.41165 11.3235M2.41165 11.3235L1.5293 15.7353M2.41165 11.3235L6.38224 13.0882\"></path></g></svg></button><button class=\"pencraft pc-reset pencraft icon-container view-image\" tabindex=\"0\" type=\"button\"><svg class=\"lucide lucide-maximize2 lucide-maximize-2\" fill=\"none\" height=\"20\" stroke=\"currentColor\" stroke-linecap=\"round\" stroke-linejoin=\"round\" stroke-width=\"2\" viewBox=\"0 0 24 24\" width=\"20\" xmlns=\"http://www.w3.org/2000/svg\"><polyline points=\"15 3 21 3 21 9\"></polyline><polyline points=\"9 21 3 21 3 15\"></polyline><line x1=\"21\" x2=\"14\" y1=\"3\" y2=\"10\"></line><line x1=\"3\" x2=\"10\" y1=\"21\" y2=\"14\"></line></svg></button></div></div></div></a></figure></div><p>Second, a kink in time horizons around 2024 and the invention of test-time compute:</p><div class=\"captioned-image-container\"><figure><a class=\"image-link image2 is-viewable-img\" href=\"https://www.lesswrong.com/posts/GAJbegsvnd85hX3eS/thoughts-on-extrapolating-time-horizons\" target=\"_blank\"><div class=\"image2-inset\"><source type=\"image/webp\" /><img alt=\"\" class=\"sizing-normal\" height=\"319.3437945791726\" src=\"https://substackcdn.com/image/fetch/$s_!RN5n!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffa71e17d-1e40-4ad2-b728-f5525938d2e5_701x364.png\" width=\"615\" /><div class=\"image-link-expand\"><div class=\"pencraft pc-display-flex pc-gap-8 pc-reset\"><button class=\"pencraft pc-reset pencraft icon-container restack-image\" tabindex=\"0\" type=\"button\"><svg fill=\"none\" height=\"20\" stroke=\"var(--color-fg-primary)\" stroke-linecap=\"round\" stroke-linejoin=\"round\" stroke-width=\"1.5\" viewBox=\"0 0 20 20\" width=\"20\" xmlns=\"http://www.w3.org/2000/svg\"><g><title></title><path d=\"M2.53001 7.81595C3.49179 4.73911 6.43281 2.5 9.91173 2.5C13.1684 2.5 15.9537 4.46214 17.0852 7.23684L17.6179 8.67647M17.6179 8.67647L18.5002 4.26471M17.6179 8.67647L13.6473 6.91176M17.4995 12.1841C16.5378 15.2609 13.5967 17.5 10.1178 17.5C6.86118 17.5 4.07589 15.5379 2.94432 12.7632L2.41165 11.3235M2.41165 11.3235L1.5293 15.7353M2.41165 11.3235L6.38224 13.0882\"></path></g></svg></button><button class=\"pencraft pc-reset pencraft icon-container view-image\" tabindex=\"0\" type=\"button\"><svg class=\"lucide lucide-maximize2 lucide-maximize-2\" fill=\"none\" height=\"20\" stroke=\"currentColor\" stroke-linecap=\"round\" stroke-linejoin=\"round\" stroke-width=\"2\" viewBox=\"0 0 24 24\" width=\"20\" xmlns=\"http://www.w3.org/2000/svg\"><polyline points=\"15 3 21 3 21 9\"></polyline><polyline points=\"9 21 3 21 3 15\"></polyline><line x1=\"21\" x2=\"14\" y1=\"3\" y2=\"10\"></line><line x1=\"3\" x2=\"10\" y1=\"21\" y2=\"14\"></line></svg></button></div></div></div></a></figure></div><p>The 2010 kink was before Cotra&#8217;s forecast and priced in. The 2024 kink is interesting and relevant - but since it was on a parameter Cotra wasn&#8217;t measuring, and probably too small to show up on the orders-of-magnitude scale we&#8217;re talking about, it&#8217;s probably not a major cause of the model&#8217;s inaccuracy.</p><p>Other things have been even more predictable:</p><div class=\"captioned-image-container\"><figure><a class=\"image-link image2 is-viewable-img\" href=\"https://epoch.ai/benchmarks/eci\" target=\"_blank\"><div class=\"image2-inset\"><source type=\"image/webp\" /><img alt=\"\" class=\"sizing-normal\" height=\"525.1614349775784\" src=\"https://substackcdn.com/image/fetch/$s_!235O!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2903a0a5-2338-48e6-b9d4-675f1e50305b_892x758.png\" width=\"618\" /><div class=\"image-link-expand\"><div class=\"pencraft pc-display-flex pc-gap-8 pc-reset\"><button class=\"pencraft pc-reset pencraft icon-container restack-image\" tabindex=\"0\" type=\"button\"><svg fill=\"none\" height=\"20\" stroke=\"var(--color-fg-primary)\" stroke-linecap=\"round\" stroke-linejoin=\"round\" stroke-width=\"1.5\" viewBox=\"0 0 20 20\" width=\"20\" xmlns=\"http://www.w3.org/2000/svg\"><g><title></title><path d=\"M2.53001 7.81595C3.49179 4.73911 6.43281 2.5 9.91173 2.5C13.1684 2.5 15.9537 4.46214 17.0852 7.23684L17.6179 8.67647M17.6179 8.67647L18.5002 4.26471M17.6179 8.67647L13.6473 6.91176M17.4995 12.1841C16.5378 15.2609 13.5967 17.5 10.1178 17.5C6.86118 17.5 4.07589 15.5379 2.94432 12.7632L2.41165 11.3235M2.41165 11.3235L1.5293 15.7353M2.41165 11.3235L6.38224 13.0882\"></path></g></svg></button><button class=\"pencraft pc-reset pencraft icon-container view-image\" tabindex=\"0\" type=\"button\"><svg class=\"lucide lucide-maximize2 lucide-maximize-2\" fill=\"none\" height=\"20\" stroke=\"currentColor\" stroke-linecap=\"round\" stroke-linejoin=\"round\" stroke-width=\"2\" viewBox=\"0 0 24 24\" width=\"20\" xmlns=\"http://www.w3.org/2000/svg\"><polyline points=\"15 3 21 3 21 9\"></polyline><polyline points=\"9 21 3 21 3 15\"></polyline><line x1=\"21\" x2=\"14\" y1=\"3\" y2=\"10\"></line><line x1=\"3\" x2=\"10\" y1=\"21\" y2=\"14\"></line></svg></button></div></div></div></a></figure></div><p>So Cotra&#8217;s bet on progress being smooth and measurable has mostly paid off so far.</p><p>But Yudkowsky further explained that his timelines were shorter than Bio Anchors because people would be working hard to discover new paradigms, and if the current paradigm would only pay off in the 2050s, then probably they would discover one before then. You could think of this as a disjunction: timelines will be shorter than Cotra thinks, <em>either</em> because deep learning pays off quickly, <em>or</em> because a new paradigm gets invented in the interim. It turned out to be the first one. So although Yudkowsky&#8217;s new paradigm has yet to materialize, his disjunctive reasoning in favor of shorter-than-2050 timelines was basically on the mark.</p><p><strong>Nostalgebraist</strong> <a href=\"https://nostalgebraist.tumblr.com/post/693718279721730048/on-bio-anchors\">argued that</a> Cotra&#8217;s whole model was a wrapper for an assumption that Moore&#8217;s Law will continue indefinitely. If it does, obviously you get enough compute for AI at some point, even if it requires some absurd process like simulating all 500 million years of multicellular evolution.</p><p>I never entirely understood this objection, because - although Bio Anchors does depend on a story where Moore&#8217;s Law doesn&#8217;t break before we get the relevant amount of compute - this is only one of many background assumptions (like that a meteor doesn&#8217;t hit Earth before we get the relevant amount of compute). Given those assumptions, it does a useful not-just-assumption-repeating job of calculating when transformative AI will happen.</p><p>As Cotra implicitly predicted, we seem on track to get AGI before Moore&#8217;s Law breaks down, and so Moore&#8217;s Law didn&#8217;t end up mattering very much. And if all of Cotra&#8217;s non-Moore&#8217;s-Law parameter estimates had been correct, her model would have given about the same timelines we have now, and surprised everyone with a revolutionary claim about the AI future. </p><p>But Nostalgebraist added, almost as an aside:</p><blockquote><p>Cotra has a whole other forecast I didn&#8217;t mention for &#8220;algorithmic progress,&#8221; and the last number is what you get from just algorithmic progress and no Moore&#8217;s Law. So depending on how much you trust that forecast, you might want to take all these numbers with an even bigger grain of salt than you&#8217;d expected from everything else we&#8217;ve seen. </p><p>How much should you trust Cotra&#8217;s algorithmic progress forecast? She writes: <em>&#8220;I have done very little research into algorithmic progress trends. Of the four main components of my model (2020 compute requirements, algorithmic progress, compute price trends, and spending on computation) I have spent the least time thinking about algorithmic progress.&#8221;</em> ...and bases the forecast on one paper about ImageNet classifiers. </p><p>I want to be clear that when I quote these parts about Cotra not spending much time on something, I&#8217;m not trying to make fun of her. It&#8217;s good to be transparent about this kind of thing! I wish more people would do that. My complaint is not that she tells us what she spent time on, it&#8217;s that she spent time on the wrong things.</p></blockquote><p>Like Cotra herself, I think Nostalgebraist was spiritually correct even if his bottom line (about Moore&#8217;s Law) was wrong. His meta-level point was that a seemingly complicated model could actually hinge on one or two parameters, and that many of Cotra&#8217;s parameter values were vague hand-wavey best guess estimates. He gave algorithmic progress as a secondary example of this to shore up his Moore&#8217;s Law case, but in fact it turned out to be where all the action was. </p><p><strong>V.</strong></p><p>Those were the rare good critiques.</p><p>The bad critiques were the same ones everyone in this space gets:</p><ul><li><p>You&#8217;re just trying to build hype.</p></li><li><p>You&#8217;re just trying to scare people.</p></li><li><p>You use probabilities, but <a href=\"https://www.astralcodexten.com/p/in-continued-defense-of-non-frequentist\">probabilities are meaningless</a> and just cover up that you don&#8217;t really know.</p></li><li><p>AI forecasts are just attempts for people to push AGI back to some time when it can&#8217;t be checked.</p></li><li><p>AI forecasts are just attempts for people to pull AGI forward to when it means they personally will live forever.</p></li></ul><p>The impressive thing here is that correcting the estimates of two parameters - compute growth and algorithmic progress - produce a forecast which would have seemed valuable and prescient six years later. Even correcting one parameter - algorithmic progress - would have gotten it very close. In that sense, the history of Bio Anchors is a white pill for forecasting, and an antidote to the epistemic nihilism of the positions above.</p><p>But its bottom line was still wrong. Even if you do almost everything correctly, invent new terms that become load-bearing pillars of the field, defeat your critics&#8217; main objections, and demonstrate a remarkably clear model of exactly how to think about a difficult subject, mis-estimating one parameter can ruin the whole project. </p><p>This is why you do a sensitivity analysis, and Cotra did this at least in spirit (talked about which parameters were most important; gave people widgets they could use to play around with). But it didn&#8217;t work as well as she might have hoped, giving a &lt;10% chance of timelines as short as the current median. Several later commenters and analysts had good takes here, especially <a href=\"https://www.alignmentforum.org/posts/Q3fesop6HKnemJ5Jc/disagreement-with-bio-anchors-that-lead-to-shorter-timelines\">Marius Hobbhahn</a> of Apollo Research. Along with correctly guessing that algorithmic progress would go faster than Bio Anchors predicted (albeit with the benefit of two more years of data), he wrote that:</p><blockquote><p>The uncertainty from the model is probably too low, i.e. the model is overconfident because core variables like compute price halving time and algorithmic efficiency are modeled as static singular values rather than distributions that change over time.</p></blockquote><p>Plausibly if these had been distributions, you could have done a more formal sensitivity analysis on them, and then it would have identified these as crucial terms (Nostalgebraist unofficially noticed this, but a formal analysis could have officially noticed and quantified it) and had more uncertainty about the possibility of very early AGI.</p><p>So what&#8217;s the takeaway? Trust forecasts more? Trust them less? Do better forecasting? Don&#8217;t bother?</p><p>These questions have no right answer, but one conclusion does seem pretty firm. Most of the bad-faith critics, having identified that Ajeya&#8217;s model was imperfect and could fail, defaulted to the <a href=\"https://www.astralcodexten.com/p/mr-tries-the-safe-uncertainty-fallacy\">Safe Uncertainty Fallacy</a> - since we can never be sure a model is exactly right, things are uncertain, which means we can continue to believe everything is fine and normal and timelines are wrong and we don&#8217;t have to worry. But as Yudkowsky pointed out, there&#8217;s uncertainty on both sides! Sometimes the fact that a forecast is imperfect and you can never be certain means things are <em>more</em> dangerous than you thought!</p><p>I think internalizing this lesson is more important than any sort of micro-calibrating exactly how much to believe in probabilistic forecasts. Once you understand that you can&#8217;t always just rely on your biases and sense that it would be inconvenient for things to get weird, you become desperate for real information. That desperation encourages you to seek any possible source of knowledge, including potentially fallible and error-laden probabilistic forecasts. It also encourages you to treat them lightly, as small updates useful for resolving near-total uncertainty into merely partial uncertainty. This is how I treat Bio Anchors&#8217; successors - although right now a little more fallibility and error-ladenness might be genuinely welcome.</p><div class=\"captioned-image-container\"><figure><a class=\"image-link image2 is-viewable-img\" href=\"https://ai-2027.com/\" target=\"_blank\"><div class=\"image2-inset\"><source type=\"image/webp\" /><img alt=\"\" class=\"sizing-normal\" height=\"549.5766016713092\" src=\"https://substackcdn.com/image/fetch/$s_!ZDGz!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff7a4b2a0-ce41-44ff-8a17-914fb64f7b23_718x678.png\" width=\"582\" /><div class=\"image-link-expand\"><div class=\"pencraft pc-display-flex pc-gap-8 pc-reset\"><button class=\"pencraft pc-reset pencraft icon-container restack-image\" tabindex=\"0\" type=\"button\"><svg fill=\"none\" height=\"20\" stroke=\"var(--color-fg-primary)\" stroke-linecap=\"round\" stroke-linejoin=\"round\" stroke-width=\"1.5\" viewBox=\"0 0 20 20\" width=\"20\" xmlns=\"http://www.w3.org/2000/svg\"><g><title></title><path d=\"M2.53001 7.81595C3.49179 4.73911 6.43281 2.5 9.91173 2.5C13.1684 2.5 15.9537 4.46214 17.0852 7.23684L17.6179 8.67647M17.6179 8.67647L18.5002 4.26471M17.6179 8.67647L13.6473 6.91176M17.4995 12.1841C16.5378 15.2609 13.5967 17.5 10.1178 17.5C6.86118 17.5 4.07589 15.5379 2.94432 12.7632L2.41165 11.3235M2.41165 11.3235L1.5293 15.7353M2.41165 11.3235L6.38224 13.0882\"></path></g></svg></button><button class=\"pencraft pc-reset pencraft icon-container view-image\" tabindex=\"0\" type=\"button\"><svg class=\"lucide lucide-maximize2 lucide-maximize-2\" fill=\"none\" height=\"20\" stroke=\"currentColor\" stroke-linecap=\"round\" stroke-linejoin=\"round\" stroke-width=\"2\" viewBox=\"0 0 24 24\" width=\"20\" xmlns=\"http://www.w3.org/2000/svg\"><polyline points=\"15 3 21 3 21 9\"></polyline><polyline points=\"9 21 3 21 3 15\"></polyline><line x1=\"21\" x2=\"14\" y1=\"3\" y2=\"10\"></line><line x1=\"3\" x2=\"10\" y1=\"21\" y2=\"14\"></line></svg></button></div></div></div></a><figcaption class=\"image-caption\">AI 2027&#8217;s forecast for early 2026 (<a href=\"https://ai-2027.com/\">source</a>).</figcaption></figure></div><p></p>"
            ],
            "link": "https://www.astralcodexten.com/p/what-happened-with-bio-anchors",
            "publishedAt": "2026-02-12",
            "source": "SlateStarCodex",
            "summary": "<p><em>[Original post: <a href=\"https://www.astralcodexten.com/p/biological-anchors-a-trick-that-might\">Biological Anchors: A Trick That Might Or Might Not Work</a>]</em></p><p><strong>I.</strong></p><p>Ajeya Cotra&#8217;s <a href=\"https://drive.google.com/drive/u/0/folders/15ArhEPZSTYU8f012bs6ehPS6-xmhtBPP\">Biological Anchors</a> report was the landmark AI timelines forecast of the early 2020s. In many ways, it was incredibly prescient - it nailed the scaling hypothesis, predicted the current AI boom, and introduced concepts like &#8220;time horizons&#8221; that have entered common parlance. In most cases where its contemporaries challenged it, its assumptions have been borne out, and its challengers proven wrong.</p><p>But its headline prediction - an AGI timeline centered around the 2050s - no longer seems plausible. The <a href=\"https://agi.goodheartlabs.com/\">current state</a> of the discussion ranges from late <a href=\"https://ai-2027.com/\">2020s</a> to <a href=\"https://epoch.ai/gradient-updates/the-case-for-multi-decade-ai-timelines\">2040s</a>, with more remote dates relegated to those who expect the current paradigm to prove ultimately fruitless - the opposite of Ajeya&#8217;s assumptions. Cotra later shortened her own timelines to 2040 (<a href=\"https://www.alignmentforum.org/posts/AfH2oPHCApdKicM4m/two-year-update-on-my-personal-ai-timelines\">as of 2022</a>) and they are probably even shorter now.</p><p>So, if its premises were impressively correct, but its conclusion twenty years too late, what went wrong in the middle?</p><p><strong>II.</strong> </p><p>First, a refresher. What was Bio Anchors? How did it work?</p><p>In 2020, the most advanced AI, GPT-3, had required about 10^23 FLOPs to train.</p><p>(FLOPs are a measure of computation: big, powerful computers and",
            "title": "What Happened With Bio Anchors?"
        },
        {
            "content": [
                "<p>\n          <a href=\"https://www.astralcodexten.com/p/hidden-open-thread-4205\">\n              Read more\n          </a>\n      </p>"
            ],
            "link": "https://www.astralcodexten.com/p/hidden-open-thread-4205",
            "publishedAt": "2026-02-12",
            "source": "SlateStarCodex",
            "summary": "<p> <a href=\"https://www.astralcodexten.com/p/hidden-open-thread-4205\"> Read more </a> </p>",
            "title": "Hidden Open Thread 420.5"
        },
        {
            "content": [
                "<p>This was the week of Claude Opus 4.6, and also of ChatGPT-5.3-Codex. Both leading models got substantial upgrades, although OpenAI\u2019s is confined to Codex. Once again, the frontier of AI got more advanced, especially for agentic coding but also for everything else.</p>\n<p>I spent the week so far covering Opus, with <a href=\"https://thezvi.substack.com/p/claude-opus-46-system-card-part-1?r=67wny\"><strong>two posts devoted</strong></a> <a href=\"https://thezvi.substack.com/p/claude-opus-46-system-card-part-2?r=67wny\"><strong>to the extensive model card</strong></a>, and then <a href=\"https://thezvi.substack.com/p/claude-opus-46-escalates-things-quickly?r=67wny\"><strong>one giving benchmarks, reactions, capabilities and a synthesis, which functions as the central review</strong></a>.</p>\n<p>We also got GLM-5, Seedance 2.0, Claude fast mode, an app for Codex and much more.</p>\n<p>Claude fast mode means you can pay a premium to get faster replies from Opus 4.6. It\u2019s very much not cheap, but it can be worth every penny. More on that in the next agentic coding update.</p>\n<div>\n\n\n<span id=\"more-25094\"></span>\n\n\n</div>\n<p>One of the most frustrating things about AI is the constant goalpost moving, both in terms of capability and safety. People say \u2018oh [X] would be a huge deal but is a crazy sci-fi concept\u2019 or \u2018[Y] will never happen\u2019 or \u2018surely we would not be so stupid as to [Z]\u2019 and then [X], [Y] and [Z] all happen and everyone shrugs as if nothing happened and they choose new things they claim will never happen and we would never be so stupid as to, and the cycle continues. That cycle is now accelerating.</p>\n<p><a href=\"https://www.hyperdimensional.co/p/on-recursive-self-improvement-part\">As Dean Ball points out, recursive self-improvement is here and it is happening</a>.</p>\n<blockquote><p><a href=\"https://x.com/nabeelqu/status/2021322648014180707\">Nabeel S. Qureshi</a>: I know we&#8217;re all used to it now but it&#8217;s so wild that recursive self improvement is actually happening now, in some form, and we&#8217;re all just debating the pace. This was a sci fi concept and some even questioned if it was possible at all</p></blockquote>\n<p>So here we are.</p>\n<p>Meanwhile, various people resign from the leading labs and say their peace. None of them are, shall we say, especially reassuring.</p>\n<p>In the background, the stock market is having a normal one even more than usual.</p>\n<p>Even if you can see the future, it\u2019s really hard to do better than \u2018be long the companies that are going to make a lot of money\u2019 because the market makes wrong way moves half the time that it wakes up and realizes things that I already know. Rough game.</p>\n\n\n<h4 class=\"wp-block-heading\">Table of Contents</h4>\n\n\n<ol>\n<li><a href=\"https://thezvi.substack.com/i/187029171/language-models-offer-mundane-utility\">Language Models Offer Mundane Utility.</a> Flattery will get you everywhere.</li>\n<li><a href=\"https://thezvi.substack.com/i/187029171/language-models-don-t-offer-mundane-utility\">Language Models Don\u2019t Offer Mundane Utility.</a> It\u2019s a little late for that.</li>\n<li><a href=\"https://thezvi.substack.com/i/187029171/huh-upgrades\">Huh, Upgrades.</a> Things that are surprising in that they didn\u2019t happen before.</li>\n<li><a href=\"https://thezvi.substack.com/i/187029171/on-your-marks\"><strong>On Your Marks</strong>.</a> Slopes are increasing. That escalated quickly.</li>\n<li><a href=\"https://thezvi.substack.com/i/187029171/overcoming-bias\">Overcoming Bias.</a> LLMs continue to exhibit consistent patterns of bias.</li>\n<li><a href=\"https://thezvi.substack.com/i/187029171/choose-your-fighter\">Choose Your Fighter.</a> The glass is half open, but which half is which?</li>\n<li><a href=\"https://thezvi.substack.com/i/187029171/get-my-agent-on-the-line\">Get My Agent On The Line.</a> Remember Sammy Jenkins.</li>\n<li><a href=\"https://thezvi.substack.com/i/187029171/ai-conversations-are-not-privileged\">AI Conversations Are Not Privileged.</a> Beware accordingly.</li>\n<li><a href=\"https://thezvi.substack.com/i/187029171/fun-with-media-generation\">Fun With Media Generation.</a> Seedance 2.0 looks pretty sweet for video.</li>\n<li><a href=\"https://thezvi.substack.com/i/187029171/the-superb-owl\">The Superb Owl.</a> The ad verdicts are in from the big game.</li>\n<li><a href=\"https://thezvi.substack.com/i/187029171/a-word-from-the-torment-nexus\">A Word From The Torment Nexus.</a> Some stand in defense of advertising.</li>\n<li><a href=\"https://thezvi.substack.com/i/187029171/they-took-our-jobs\"><strong>They Took Our Jobs</strong>.</a> Radically different models of the future of employment.</li>\n<li><a href=\"https://thezvi.substack.com/i/187029171/the-art-of-the-jailbreak\">The Art of the Jailbreak.</a> You can jailbreak Google Translate.</li>\n<li><a href=\"https://thezvi.substack.com/i/187029171/introducing\">Introducing.</a> GLM-5, Expressive Mode for ElevenAgents.</li>\n<li><a href=\"https://thezvi.substack.com/i/187029171/in-other-ai-news\">In Other AI News.</a> RIP OpenAI mission alignment team, WSJ profiles Askell.</li>\n<li><a href=\"https://thezvi.substack.com/i/187029171/show-me-the-money\">Show Me the Money.</a> Goldman Sachs taps Anthropic, OpenAI rolls out ads.</li>\n<li><a href=\"https://thezvi.substack.com/i/187029171/bubble-bubble-toil-and-trouble\"><strong>Bubble, Bubble, Toil and Trouble</strong>.</a> The stock market is not making much sense.</li>\n<li><a href=\"https://thezvi.substack.com/i/187029171/future-shock\">Future Shock.</a> Potential explanations for how Claude Legal could have mattered.</li>\n<li><a href=\"https://thezvi.substack.com/i/187029171/memory-lane\">Memory Lane.</a> Be the type of person who you want there to be memories of.</li>\n<li><a href=\"https://thezvi.substack.com/i/187029171/keep-the-mask-on-or-you-re-fired\">Keep The Mask On Or You\u2019re Fired.</a> OpenAI fires Ryan Beiermeister.</li>\n<li><a href=\"https://thezvi.substack.com/i/187029171/quiet-speculations\">Quiet Speculations.</a> The singularity will not be gentle.</li>\n<li><a href=\"https://thezvi.substack.com/i/187029171/the-quest-for-sane-regulations\">The Quest for Sane Regulations.</a> Dueling lobbying groups, different approaches.</li>\n<li><a href=\"https://thezvi.substack.com/i/187029171/chip-city\">Chip City.</a> Data center fights and the ultimate (defensible) anti-EA position.</li>\n<li><a href=\"https://thezvi.substack.com/i/187029171/the-week-in-audio\">The Week in Audio.</a> Elon on Dwarkesh, Anthropic CPO, MIRI on Beck.</li>\n<li><a href=\"https://thezvi.substack.com/i/187029171/constitutional-conversation\">Constitutional Conversation.</a> I can tell a lie, under the right circumstances.</li>\n<li><a href=\"https://thezvi.substack.com/i/187029171/rhetorical-innovation\">Rhetorical Innovation.</a> Some people need basic explainers.</li>\n<li><a href=\"https://thezvi.substack.com/i/187029171/working-on-it-anyway\">Working On It Anyway.</a> Be loud about how dangerous your own actions are.</li>\n<li><a href=\"https://thezvi.substack.com/i/187029171/the-thin-red-line\">The Thin Red Line.</a> The problem with red lines is people keep crossing them.</li>\n<li><a href=\"https://thezvi.substack.com/i/187029171/aligning-a-smarter-than-human-intelligence-is-difficult\">Aligning a Smarter Than Human Intelligence is Difficult.</a> Read your Asimov.</li>\n<li><a href=\"https://thezvi.substack.com/i/187029171/people-will-hand-over-power-to-the-ais\">People Will Hand Over Power To The AIs.</a> Unless we stop them.</li>\n<li><a href=\"https://thezvi.substack.com/i/187029171/people-are-worried-about-ai-killing-everyone\">People Are Worried About AI Killing Everyone.</a> Elon Musk\u2019s ego versus humanity.</li>\n<li><a href=\"https://thezvi.substack.com/i/187029171/famous-last-words\"><strong>Famous Last Words</strong>.</a> What do you say on your way out the door?</li>\n<li><a href=\"https://thezvi.substack.com/i/187029171/other-people-are-not-as-worried-about-ai-killing-everyone\">Other People Are Not As Worried About AI Killing Everyone.</a> Autonomous bio.</li>\n<li><a href=\"https://thezvi.substack.com/i/187029171/the-lighter-side\"><strong>The Lighter Side</strong>.</a> It\u2019s funny because it\u2019s true.</li>\n</ol>\n\n\n<h4 class=\"wp-block-heading\">Language Models Offer Mundane Utility</h4>\n\n\n<p><a href=\"https://x.com/TheWapplehouse/status/2019911457568354613\">Flatter the AI customer service bots</a>, get discounts and free stuff, and often you\u2019ll get to actually keep them.</p>\n<p>AI can do a ton even if all it does is make the software we use suck modestly less:</p>\n<blockquote><p><a href=\"https://x.com/ptr/status/2021430929714184386\">*tess</a>: If you knew how bad the software situation is in literally every non tech field, you would be cheering cheering cheering this moment,</p>\n<p>medicine, research, infrastructure, government, defense, travel</p>\n<p>Software deflation is going to bring surplus to literally the entire world</p></blockquote>\n<p>The problem is, you can create all the software you like, they still have to use it.</p>\n\n\n<h4 class=\"wp-block-heading\">Language Models Don\u2019t Offer Mundane Utility</h4>\n\n\n<p><a href=\"https://x.com/kevinroose/status/2020918311496642921\">Once again, an academic is so painfully unaware or slow to publish</a>, or both, that their testing of LLM effectiveness is useless. This time it was evaluating health advice.</p>\n\n\n<h4 class=\"wp-block-heading\">Huh, Upgrades</h4>\n\n\n<p><a href=\"https://x.com/claudeai/status/2021630343372259759\">Anthropic brings a bunch of extra features to their free plans</a> for Claude, including file creation, connectors, skills and compaction.</p>\n<p><a href=\"https://x.com/OpenAI/status/2021299935678026168\">ChatGPT Deep Research is now powered by GPT-5.2</a>. I did not realize this was not already true. It now also integrates apps in ChatGPT, lets you track progress and give it new sources while it works, and presents its reports in full screen.</p>\n<p><a href=\"https://x.com/sama/status/2021452911511998557\">OpenAI updates GPT-5.2-Instant</a>, Altman hopes you find it \u2018a little better.\u2019 I demand proper version numbers. You are allowed to have a GPT-5.21.</p>\n<p><a href=\"https://x.com/cocktailpeanut/status/2021264333485863308\">Chrome 146 includes an early preview of WebMCP for your AI agent</a>.</p>\n\n\n<h4 class=\"wp-block-heading\">On Your Marks</h4>\n\n\n<p>The most important thing to know about the METR graph is that doubling times are getting faster, in ways people very much dismissed as science fiction very recently.</p>\n<blockquote><p><a href=\"https://x.com/METR_Evals/status/2019169900317798857\">METR</a>: We estimate that GPT-5.2 with `high` (not `xhigh`) reasoning effort has a 50%-time-horizon of around 6.6 hrs (95% CI of 3 hr 20 min to 17 hr 30 min) on our expanded suite of software tasks. This is the highest estimate for a time horizon measurement we have reported to date.</p>\n<p><a href=\"https://x.com/Afinetheorem/status/2019199842757800378\">Kevin A. Bryan</a>: Interesting AI benchmark fact: Leo A&#8217;s wild Situational Awareness 17 months ago makes a number of statements about benchmarks that some thought were sci-fi fast in their improvement. We have actually outrun the predictions so far.</p>\n<div>\n<figure>\n<div>\n\n\n<figure class=\"wp-block-image\"><img alt=\"\" src=\"https://substackcdn.com/image/fetch/$s_!S_Ox!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd543aced-e207-4e0b-92c9-ef9b2959d58a_1200x716.jpeg\" /></figure>\n\n\n<div></div>\n</div>\n</figure>\n</div>\n<p><a href=\"https://x.com/nabeelqu/status/2020896840036573426\">Nabeel S. Qureshi</a>: I got Opus to score all of Leopold&#8217;s predictions from &#8220;Situational Awareness&#8221; and it thinks he nailed it:</p>\n<div>\n<figure>\n<div>\n\n\n<figure class=\"wp-block-image\"><img alt=\"\" src=\"https://substackcdn.com/image/fetch/$s_!UtKF!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F71f529d6-3381-4f69-b5a0-2ab9ebb4794a_1132x1056.jpeg\" /></figure>\n\n\n<div></div>\n</div>\n</figure>\n</div>\n</blockquote>\n<p>The measurement is starting to require a better task set, because things are escalating too quickly.</p>\n<blockquote><p><a href=\"https://x.com/CFGeek/status/2020017083296432498\">Charles Foster</a>: Man goes to doctor. Says he&#8217;s stuck. Says long-range autonomy gains are outpacing his measurement capacity. Doctor says, \u201cTreatment is simple. Great evaluator METR is in town tonight. Go and see them. That should fix you up.\u201d Man bursts into tears. Says, \u201cBut doctor\u2026I am METR.\u201d</p></blockquote>\n\n\n<h4 class=\"wp-block-heading\">Overcoming Bias</h4>\n\n\n<p><a href=\"https://x.com/IvanArcus/status/2021592600554168414\">Ivan Arcuschin and others investigate LLMs having \u2018hidden biases</a>,\u2019 meaning factors that influence decisions but that are never cited explicitly in the decision process. The motivating example is the religion of a loan applicant. It\u2019s academic work, so the models involved (Gemini 2.5 Flash, Sonnet 4, GPT-4.1) are not frontier but the principles likely still hold.</p>\n<p>They find biases in various models including formality of writing, religious affiliation, Spanish language ability and religious affiliation. Gender and race bias, favoring female and minority-associated applications, generalized across all models.</p>\n<p>We label only some such biases \u2018inappropriate\u2019 and \u2018illegal\u2019 but the mechanisms involved are the same no matter what they are based upon.</p>\n<p>This is all very consistent with prior findings on these questions.</p>\n\n\n<h4 class=\"wp-block-heading\">Choose Your Fighter</h4>\n\n\n<p>This is indeed strange and quirky, but it makes sense if you consider what both companies consider their comparative advantage and central business plan.</p>\n<p>One of these strategies seems wiser than the other.</p>\n<blockquote><p><a href=\"https://x.com/Teknium/status/2019932720277954969\">Teknium (e/\u03bb)</a>: Why did they &#8220;release&#8221; codex 5.3 yesterday but its not in cursor today, while claude opus 4.6 is?</p>\n<p><a href=\"https://x.com/somi_ai/status/2020002804711190621\">Somi AI</a>: Anthropic ships to the API same day, every time. OpenAI gates it behind their own apps first, then rolls out API access weeks later. been the pattern since o3.</p>\n<p><a href=\"https://x.com/Teknium/status/2020040707621425187\">Teknium (e/\u03bb)</a>: It\u2019s weird claude code is closed source, but their models are useable in any harness day one over the api, while codex harness is open source, but their models are only useable in their harness\u2026why can\u2019t both just be good</p></blockquote>\n<p>Or so I heard:</p>\n<blockquote><p><a href=\"https://x.com/HellenicVibes/status/2019152484204417264\">Zoomer Alcibiades</a>: Pro Tip: If you pay $20 a month for Google&#8217;s AI, you get tons of Claude Opus 4.5 usage through Antigravity, way more than on the Anthropic $20 tier. I have four Opus 4.5 agents running continental philosophy research in Antigravity right now \u2014 you can just do things!</p></blockquote>\n\n\n<h4 class=\"wp-block-heading\">Get My Agent On The Line</h4>\n\n\n<p><a href=\"https://x.com/alexolegimas/status/2020871624212328872\">Memento as a metaphor for AI agents</a>. They have no inherent ability to form new memories or learn, but they can write themselves notes of unlimited complexity.</p>\n<blockquote><p><a href=\"https://x.com/emollick/status/2020713378319417626\">Ethan Mollick</a>: So much work is going into faking continual learning and memory for AIs, and it works better than expected in practice, so much so that it makes me think that, if continual learning is actually achieved, the results are going to really shift the AI ability frontier very quickly.</p>\n<p><a href=\"https://x.com/jasoncrawford/status/2020869648325963904\">Jason Crawford</a>: Having Claude Code write its own skills is not far from having a highly trainable employee: you give it some feedback and it learns.</p>\n<p>Still unclear to me just how reliable this is, I have seen it ignore applicable skills\u2026 but if we&#8217;re not there yet the path to it is clear.</p></blockquote>\n<p>I wouldn\u2019t call it faking continual learning. If it works it\u2019s continual learning. Yes, actual in-the-weights continual learning done properly would be a big deal and big unlock, but I see this and notes more as substitutes, although they are also compliments. If you can have your notes function sufficiently well you don\u2019t need new memories.</p>\n<blockquote><p><a href=\"https://x.com/deanwball/status/2020179119330455973\">Dean W. Ball</a>: Codex 5.3 and Opus 4.6 in their respective coding agent harnesses have meaningfully updated my thinking about &#8216;continual learning.&#8217; I now believe this capability deficit is more tractable than I realized with in-context learning.</p>\n<p>\u2026 Some of the insights I&#8217;ve seen 4.6 and 5.3 extract are just about my preferences and the idiosyncrasies of my computing environment. But others are somewhat more like &#8220;common sets of problems in the interaction of the tools I (and my models) usually prefer to use for solving certain kinds of problems.&#8221;</p>\n<p>This is the kind of insight a software engineer might learn as they perform their duties over a period of days, weeks, and months. Thus I struggle to see how it is not a kind of on-the-job learning, happening from entirely within the &#8216;current paradigm&#8217; of AI. No architectural tweaks, no &#8216;breakthrough&#8217; in &#8216;continual learning&#8217; required.</p>\n<p><a href=\"https://x.com/hamandcheese/status/2020238406593245516\">Samuel Hammond</a>: In-context learning is (almost) all you need. The KV cache is normally explained as a content addressable memory, but it can also be thought of a stateful mechanism for fast weight updates. The model&#8217;s true parameters are fixed, but the KV state makes the model behave *as if* its weights updated conditional on the input. In simple cases, a single attention layer effectively implements a one-step gradient-like update rule.</p>\n<p>\u2026 In practice, though, this comes pretty close to simply having a library of skills to inject into context on the fly. The biggest downside is that the model can&#8217;t get cumulatively better at a skill in a compounding way. But that&#8217;s in a sense what new model releases are for.</p></blockquote>\n<p>Models are continuously learning in general, in the sense that every few months the model gets better. And if you try to bake other learning into the weights, then every few months you would have to start that process over again or stay one model behind.</p>\n<p>I expect \u2018continual learning\u2019 to be solved primarily via skills and context, and for this to be plenty good enough, and for this to be clear within the year.</p>\n\n\n<h4 class=\"wp-block-heading\">AI Conversations Are Not Privileged</h4>\n\n\n<p>Neither are your Google searches. <a href=\"https://x.com/sethlazar/status/2021851153383010357\">This is a reminder to act accordingly</a>. If you feed anything into an LLM or a Google search bar, then the government can get at it and use it at trial. Attorneys should be warning their clients accordingly, and one cannot assume that hitting the delete button on the chat robustly deletes it.</p>\n<p>AI services can mitigate this a lot by offering a robust instant deletion option, and potentially can get around this (IANAL and case law is unsettled) by offering tools to collaborate with your lawyer to invoke privilege.</p>\n<p>Should we change how the law works here? OpenAI has been advocating to make ChatGPT chats have legal privilege by default. My gut says this goes too far in the other direction, driving us away from having chats with people.</p>\n\n\n<h4 class=\"wp-block-heading\">Fun With Media Generation</h4>\n\n\n<p>Seedance 2.0 from ByteDance is giving us some very impressive 15 second clips <a href=\"https://x.com/hradzka/status/2021490114220937253\">and often</a> <a href=\"https://x.com/emollick/status/2021412306291392535\">one shotting them</a>, <a href=\"https://x.com/javilopen/status/2021406574947049787\">such as these</a>, and is happy to include celebrities and such. We are not \u2018there\u2019 in the sense that you would choose this over a traditionally filmed movie, but yeah, this is pretty impressive.</p>\n<blockquote><p><a href=\"https://x.com/fofrAI/status/2021934217907909120\">fofr</a>: This slow Seedance release is like the first week of Sora all over again. Same type of viral videos, same copyright infringements, just this time with living people\u2019s likenesses thrown into the mix.</p></blockquote>\n<p><a href=\"https://x.com/oscredwin/status/2020639604961837518\">AI vastly reduces the cost to producing images and video</a>, for now this is generally at the cost of looking worse. As Andrew Rettek points out it is unsurprising that people will accept a quality drop to get a 100x drop in costs. What is still surprising, and in this way I agree with Andy Masley, is that <a href=\"https://x.com/venturetwins/status/2020350612148498843\">they would use it for the Olympics introduction video</a>. When you\u2019re at this level of scale and scrutiny you would think you would pay up for the good stuff.</p>\n\n\n<h4 class=\"wp-block-heading\">The Superb Owl</h4>\n\n\n<p>We got commercials for a variety of AI products and services. If anything I was surprised we did not get more, given how many AI products offer lots of mundane utility but don\u2019t have much brand awareness or product awareness. Others got taken by surprise.</p>\n<blockquote><p><a href=\"https://x.com/sriramk/status/2020755600850759867/history\">Sriram Krishnan</a>: was a bit surreal to see so much of AI in all ways in the super bowl ads. really drives home how much AI is driving the economy and the zeitgeist right now.</p></blockquote>\n<p>There were broadly two categories, frontier models (Gemini, OpenAI and Anthropic), and productivity apps.</p>\n<p>The productivity app commercials were wild, lying misrepresentations of their products. One told us anyone with no experience can code an app within seconds or add any feature they want. Another closed you and physically walked around the office. A third even gave you the day off, which we all know never happens. Everything was done one shot. How dare they lie to us like this.</p>\n<p>I kid, these were all completely normal Super Bowl ads, and they were fine. Not good enough to make me remember which AI companies bought them, or show me why their products were unique, but fine.</p>\n<p>We also got one from ai.com.</p>\n<blockquote><p><a href=\"https://x.com/clarklab/status/2020700001999945738\">Clark Wimberly</a>: That <a href=\"http://ai.com\">ai.com</a> commercial? With the $5m Super Bowl slot and the with $70m domain name?</p>\n<p>It&#8217;s an OpenClaw wrapper. OpenClaw is only weeks old.</p>\n<p>AI.com: <a href=\"https://ai.com/\">ai.com</a> is the world\u2019s first easy-to-use and secure implementation of OpenClaw, the open source agent framework that went viral two weeks ago; we made it easy to use without any technical skills, while hardening security to keep your data safe.</p></blockquote>\n<p>Okay, look, fair, maybe there\u2019s a little bit of a bubble in some places.</p>\n<p>The three frontier labs took very different approaches.</p>\n<p>Anthropic said ads are coming to AI, but Claude won\u2019t ever have ads. We discussed this last week. They didn\u2019t spend enough to run the full versions, so the timing was wrong and it didn\u2019t land the same way and it wasn\u2019t as funny as it was online.</p>\n<p>On reflection, after seeing it on the big screen, I decided these ads were a mistake for the simple reason that Claude and Anthropic have zero name recognition and this didn\u2019t establish that. You first need to establish that Claude is a ChatGPT alternative on people\u2019s radar, so once you grab their attention you need more of an explanation.</p>\n<p>Then I saw one in full on the real big screen, during previews at an AMC, and in that setting it was even more clear that this completely missed the mark and normies would have no idea what was going on, and this wouldn\u2019t accomplish anything. Again, I don\u2019t understand how this mistake gets made.</p>\n<p>Several OpenAI people took additional potshots at this and Altman went on tilt, <a href=\"https://www.cnn.com/2026/02/06/tech/anthropic-openai-super-bowl-ads\">as covered by CNN</a>, but wisely, once it was seen in context, stopped accusing it of being misleading and instead pivoted to correctly calling it ineffective.</p>\n<p>It turns out it was simpler than that, regular viewers didn\u2019t get it at all and responded with a lot of basically \u2018WTF,\u2019 <a href=\"https://x.com/random_walker/status/2020932716817154099\">ranking it in the bottom 3% of Super Bowl ads</a>.</p>\n<p>I always wonder, when that happens, why one can\u2019t use a survey or focus group to anticipate this reaction. It\u2019s a mistake that should not be so easy to make.</p>\n<p>Anthropic\u2019s secret other ad was by Amazon, for Alexa+, and it was weirdly ambivalent about whether the whole thing was a good idea but I think it kinda worked. Unclear.</p>\n<p><a href=\"https://x.com/OpenAI/status/2020649757434327362\">OpenAI went with</a> big promises, vibes and stolen (nerd) valor. The theme was \u2018great moments in chess, building, computers and robotics, science and science fiction\u2019 to claim them by association. This is another classic Super Bowl strategy, just say \u2018my potato chips represent how much you love your dad\u2019 or \u2018Dunkin Donuts reminds you of all your favorite sitcoms,\u2019 or \u2018Sabrina Carpenter built a man out of my other superior potato chips,\u2019 all also ads this year.</p>\n<blockquote><p><a href=\"https://x.com/sama/status/2020677993673433330\">Sam Altman</a>: Proud of the team for getting Pantheon and The Singularity is Near in the same Super Bowl ad</p>\n<p><a href=\"https://x.com/tszzl/status/2020692140096176223\">roon</a>: if your superbowl ad explains what your product actually does that\u2019s a major L the point is aura farming</p></blockquote>\n<p>The ideal Super Bowl ad successfully does both, unless you already have full brand recognition and don\u2019t need to explain (e.g. Pepsi, Budweiser, Dunkin Donuts).</p>\n<p>On the one hand, who doesn\u2019t love a celebration of all this stuff? Yes, it\u2019s cool to reference I, Robot and Alan Turing and Grace Hopper and Einstein. I guess? On the other hand, it was just an attempt to overload the symbolism and create unearned associations, and a bunch of them felt very unearned.</p>\n<p>I want to talk about the chess games 30 seconds in.</p>\n<ol>\n<li>Presumably we started 1. e4 e5 2. Nf3 Nc6 3. Bc4 Nf6 4. Nc3, which is very standard, but then black moves 4 \u2026 d5, which the engines evaluate as +1.2 and \u2018clearly worse for black\u2019 and it\u2019s basically never played, for obvious reasons.</li>\n<li>The other board is a strange choice. The move here is indeed correct, but you don\u2019t have enough time to absorb the board sufficiently to figure this out.</li>\n</ol>\n<p>This feels like laziness and choosing style over substance, not checking your work.</p>\n<p>Then it closes on \u2018just build things\u2019 as an advertisement for Codex, which implies you can \u2018just build\u2019 things like robots, which you clearly can\u2019t. I mean, no, it doesn\u2019t, this is totally fine, it is a Super Bowl ad, but by their own complaint standards, yes. This was an exercise in branding and vibes, it didn\u2019t work for me because it was too transparent and boring and content-free and felt performative, but on the meta level it does what it sets out to do.</p>\n<p>Google went with an ad focusing on personalized search and Nana Banana image transformations. I thought this worked well.</p>\n<p>Meta advertised \u2018athletic intelligence\u2019 which I think means \u2018AI in your smart glasses.\u2019</p>\n<p>Then there\u2019s the actively negative one, from my perspective, <a href=\"https://x.com/aakashgupta/status/2021097860490723535\">which was for Ring</a>.</p>\n<blockquote><p><a href=\"https://x.com/LeVeonBell/status/2020907911724269846\">Le&#8217;Veon Bell</a>: if you\u2019re not ripping your \u2018Ring\u2019 camera off your house right now and dropping the whole thing into a pot of boiling water what are you doing?</p>\n<p><a href=\"https://x.com/aakashgupta/status/2021097860490723535\">Aakash Gupta</a>: Ring paid somewhere between $8 and $10 million for a 30-second Super Bowl spot to tell 120 million viewers that their cameras now scan neighborhoods using AI.</p>\n<p>\u2026 Ring settled with the FTC for $5.8 million after employees had unrestricted access to customers\u2019 bedroom and bathroom footage for years. They\u2019re now partnered with Flock Safety, which routes footage to local law enforcement. ICE has accessed Flock data through local police departments acting as intermediaries. Senator Markey\u2019s investigation found Ring\u2019s privacy protections only apply to device owners. If you\u2019re a neighbor, a delivery driver, a passerby, you have no rights and no recourse.</p>\n<p>\u2026 They wrapped all of that in a lost puppy commercial because that\u2019s the only version of this story anyone would willingly opt into.</p></blockquote>\n<p>As in, we are proud to tell you we\u2019re watching everything and reporting it to all the law enforcement agencies including ICE, and we are using recognition technology that can differentiate dogs and therefore also people using AI.</p>\n<p>But it\u2019s okay, because one a day we find someone\u2019s lost puppy. You should sell your freedom for the rescue of a lost puppy.</p>\n<p><a href=\"https://x.com/scottlincicome/status/2021376200321458271\">No, it\u2019s not snark to call this, as Scott Lincicome said</a>, \u201810 million dogs go missing every year, help us find 365 of them by soft launching the total surveillance state.\u2019</p>\n<p><a href=\"https://x.com/zreitano/status/2016862026501378535?s=46\">Here\u2019s a cool breakdown of the economics of these ads, from another non-AI buyer</a>.</p>\n\n\n<h4 class=\"wp-block-heading\">A Word From The Torment Nexus</h4>\n\n\n<p><a href=\"https://www.youtube.com/watch?v=MmBKuPZLZUQ\">Fidji Simo goes on the Access Podcast to discuss the new OpenAI ads</a> that are rolling out. The episode ends up being titled \u2018Head of ChatGPT fires back at Anthropic\u2019s Super Bowl attack ads,\u2019 which is not what most of the episode is about.</p>\n<blockquote><p><a href=\"https://x.com/OpenAI/status/2020936703763153010\">OpenAI</a>: We\u2019re starting to roll out a test for ads in ChatGPT today to a subset of free and Go users in the U.S.</p>\n<p>Ads do not influence ChatGPT\u2019s answers. Ads are labeled as sponsored and visually separate from the response.</p>\n<p>Our goal is to give everyone access to ChatGPT for free with fewer limits, while protecting the trust they place in it for important and personal tasks.</p>\n<p><a href=\"http://openai.com/index/testing-ads-in-chatgpt/\">http://openai.com/index/testing-ads-in-chatgpt/</a></p>\n<div>\n<figure>\n<div>\n\n\n<figure class=\"wp-block-image\"><img alt=\"\" src=\"https://substackcdn.com/image/fetch/$s_!cHNH!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffec9eddc-f598-4537-a13e-60d357e48d5c_1200x675.jpeg\" /></figure>\n\n\n<div></div>\n</div>\n</figure>\n</div>\n<p><a href=\"https://x.com/haven_emme/status/2021000345485443305\">Haven Harms</a>: The ad looks WAY more part of the answer than I was expecting based on how OpenAI was defending this. Having helped a lot of people with tech, there are going to be many people who can\u2019t tell it\u2019s an ad, especially since the ad in this example is directly relevant to the context</p></blockquote>\n<p>This picture of the ad is at the end of the multi-screen-long reply.</p>\n<p>I would say this is more clearly labeled then an ad on Instagram or Google at this point. So even though it\u2019s not that clear, it\u2019s hard to be too mad about it, provided they stick to the rule that the ad is always at the end of the answer. That provides a clear indicator users can rely upon. If they put this in different places at different times, I would say it is \u2018labeled\u2019 at all, but not consider this to then be \u2018clearly\u2019 labeled.</p>\n<p>OpenAI\u2019s principles for ads are:</p>\n<ol>\n<li>Mission alignment. Ads pay for the mission. Okie dokie?</li>\n<li>Answer independence. Ads don\u2019t influence ChatGPT\u2019s response. The question and response can influence what ad is selected, but not the other way around.\n<ol>\n<li>This is a very good and important red line.</li>\n<li>It does not protect against the existence of ads influencing how responses work, or being an advertiser ending up impacting the model long term.</li>\n<li>In particular it encourages maximization of engagement.</li>\n</ol>\n</li>\n<li>Conversation privacy. Advertisers cannot see any of your details.</li>\n</ol>\n<p>Do you trust them to adhere to these principles over time? Do you trust that merely technically, or also in spirit where the model is created and system prompt is adjusted without any thought to maximizing advertising revenue or pleasing advertisers?</p>\n<p>You are also given power to help customize what ads you see, as per other tech company platforms.</p>\n<p><a href=\"https://x.com/tszzl/status/2021679986546291037\">Roon gives a full-throated defense of advertising in general</a>, and points out that mostly you don\u2019t need to violate privacy to target LLM-associated ads.</p>\n<blockquote><p><a href=\"https://x.com/tszzl/status/2021679982167376150\">roon</a>: recent discourse on ads like the entire discourse of the 2010s misunderstands what makes digital advertising tick. people think the messages in their group chats are super duper interesting to advertisers. they are not. when you leave a nike shoe in your shopping cart, that is</p>\n<p>every week tens to hundreds of millions of people come to chatbot products with explicit commercial intent. what shoe should i buy. how do i fix this hole in my wall. it doesn\u2019t require galaxy brain extrapolating the weaknesses in the users psyche to provide for these needs</p>\n<p>I\u2019m honestly kind of wondering what kind of ads you all are getting that are feeding on your insecurities? my Instagram ads have long since become one of my primary e-commerce platforms where I get all kinds of clothes and furniture that I like. it\u2019s a moral panic</p>\n<p>I would say an unearned effete theodicy blaming all the evils of digital capitalism on advertising has formed that is thoroughly under examined and leads people away from real thought about how to make the internet better</p></blockquote>\n<p>It\u2019s not a moral panic. Roon loves ads, but most people hate ads. I agree that people often hate ads too much, they allow us to offer a wide variety of things for free that otherwise would have to cost money and that is great. But they really are pretty toxic, they massively distort incentives, and the amount of time we used to lose to them is staggering.</p>\n\n\n<h4 class=\"wp-block-heading\">They Took Our Jobs</h4>\n\n\n<p><a href=\"https://newsletter.jantegze.com/p/your-job-isnt-disappearing-its-shrinking\">Jan Tegze warns that your job really is going away</a>, the AI agents are cheaper and will replace you. Stop trying to be better at your current job and realize your experience is going to be worthless. He says that using AI tools better, doubling down on expertise or trying to \u2018stay human\u2019 with soft skills are only stalling tactics, he calls them \u2018reactions, not redesigns.\u2019 What you can do is instead find ways to do the new things AI enables, and stay ahead of the curve. Even then, he says this only \u2018buys you three to five years,\u2019 but then you will \u2018see the next evolution coming.\u2019</p>\n<p>Presumably you can see the problem in such a scenario, where all the existing jobs get automated away. There are not that many slots for people to figure out and do genuinely new things with AI. Even if you get to one of the lifeboats, it will quickly spring a leak. The AI is coming for this new job the same way it came for your old one. What makes you think seeing this \u2018next evolution\u2019 after that coming is going to leave you a role to play in it?</p>\n<p>If the only way to survive is to continuously reinvent yourself to do what just became possible, as Jan puts it? There\u2019s only one way this all ends.</p>\n<p>I also don\u2019t understand Jan\u2019s disparate treatment of the first approach that Jan dismisses, \u2018be the one who uses AI the best,\u2019 and his solution of \u2018find new things AI can do and do that.\u2019 In both cases you need to be rapidly learning new tools and strategies to compete with the other humans. In both cases the competition is easy now since most of your rivals aren\u2019t trying, but gets harder to survive over time.</p>\n<p>One can make the case that humans will continue to collectively have jobs, or at least that a large percentage will still have jobs, but that case relies on either AI capabilities stalling out, or on the tricks Jan dismisses, that you find where demand is uniquely human and AI can\u2019t substitute for it.</p>\n<blockquote><p><a href=\"https://x.com/naval/status/2020107233317515676\">Naval</a> (45 million views): There is unlimited demand for intelligence.</p></blockquote>\n<p><a href=\"https://x.com/mattshumer_/status/2021256989876109403\">A basic</a> \u2018everything is going to change, AI is going to take over your job, it has already largely taken over mine and AI is now in recursive soft self-improvement mode\u2019 article for the normies out there, written in the style of Twitter slop by Matt Shumer.</p>\n<p><a href=\"https://agglomerations.substack.com/p/economics-of-the-human\">Timothy Lee links approvingly to Adam Ozimek</a> and the latest attempt to explain that many jobs can\u2019t be automated because of \u2018the human touch.\u2019 He points to music and food service as jobs that could be fully automated, but that aren\u2019t, even citing that there are still 67,500 travel agents and half a million insurance sales agents. I do not think this is the flex Adam thinks it is.</p>\n<p>Even if the point was totally correct for some tasks, no, this would not mean that the threat to work is overrated, even if we are sticking in \u2018economic normal\u2019 untransformed worlds.</p>\n<p>The proposed policy solution, if we get into trouble, is a wage subsidy. I do not think that works, both because it has numerous logistical and incentive problems and because I don\u2019t think there will be that much difference in such worlds in demand for human labor at (e.g.) $20 versus $50 per hour for the same work. Mostly the question will be, does the human add value here at all, and mostly you don\u2019t want them at $0, or if they\u2019re actually valuable then you hire someone either way.</p>\n<p><a href=\"https://www.latent.space/p/adversarial-reasoning\">Ankit Maloo enters the \u2018why AI will never replace human experts\u2019</a> game by saying that AI cannot handle adversarial situations, both because it lacks a world model of the humans it is interacting with and the details and adjustments required and because it can be probed, read then then exploited by adversaries. Skill issue. It\u2019s all skill issues. Ankit says \u2018more intelligence isn\u2019t the fix\u2019 and yeah not if you deploy that \u2018intelligence\u2019 in a stupid fashion but intelligence is smarter than that.</p>\n<p>So you get claims like this:</p>\n<div>\n<figure>\n<div>\n\n\n<figure class=\"wp-block-image\"><img alt=\"\" src=\"https://substackcdn.com/image/fetch/$s_!iYGE!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7b493ff8-a5ab-476f-b52f-025d4bb3a4aa_1851x1073.png\" /></figure>\n\n\n<div></div>\n</div>\n</figure>\n</div>\n<blockquote><p>Ankit Maloo: \u200bWhy do outsiders think AI can already do these jobs? They judge artifacts but not dynamics:</p>\n<ul>\n<li>\u201cThis product spec is detailed.\u201d</li>\n<li>\u201cThis negotiation email sounds professional.\u201d</li>\n<li>\u201cThis mockup is clean.\u201d</li>\n</ul>\n<p>Experts evaluate any artifact by survival under pressure:</p>\n<ul>\n<li>\u201cWill this specific phrasing trigger the regulator?\u201d</li>\n<li>\u201cDoes this polite email accidentally concede leverage?\u201d</li>\n<li>\u201cWill this mockup trigger the engineering veto path?\u201d</li>\n<li>\u201cHow will this specific stakeholder interpret the ambiguity?\u201d</li>\n</ul>\n</blockquote>\n<p>The \u2018outsider\u2019 line above is counting on working together with an expert to do the rest of the steps. If the larger system (AI, human or both) is a true outsider, the issue is that it will get the simulations wrong.</p>\n<p>This is insightful in terms of why some people think \u2018this can do [X]\u2019 and others think \u2018this cannot do [X],\u2019 they are thinking of different [X]s. The AI can\u2019t \u2018be a lawyer\u2019 in the full holistic sense, not yet, but it can do increasingly many lawyer subtasks, either accelerating a lawyer\u2019s work or enabling a non-lawyer with context to substitute for the lawyer, or both, increasingly over time.</p>\n<p>There\u2019s nothing stopping you from creating an agentic workflow that looks like the Expert in the above graph, if the AI is sufficiently advanced to do each individual move. Which it increasingly is or will be.</p>\n<p>There\u2019s a wide variety of these \u2018the AI cannot and will never be able to [X]\u2019 moves people try, and\u2026 well, I\u2019ll be, look at those goalposts move.</p>\n<p>Things a more aligned or wiser person would not say, for many different reasons:</p>\n<blockquote><p><a href=\"https://x.com/Coinvo/status/2020059975301918768\">Coinvo</a>: SAM ALTMAN: &#8220;AI will not replace humans, but humans who use AI will replace those who don&#8217;t.&#8221;</p></blockquote>\n<p>What\u2019s it going to take? This is in reference to Claude Code creating a C compiler.</p>\n<blockquote><p><a href=\"https://x.com/MatRopert/status/2019823212037828892\">Mathieu Ropert</a>: Some CS engineering schools in France have you write a C compiler as part of your studies. Every graduate. To be put in perspective when the plagiarism machine announces it can make its own bad GCC in 100k+ LOCs for the amazing price of 20000 bucks at preferential rates.</p>\n<p><a href=\"https://x.com/KelseyTuoc/status/2020562359358513493\">Kelsey Piper</a>: a bunch of people reply pointing out that the C compiler that students write is much less sophisticated than this one, but I think the broader point is that we&#8217;re now at &#8220;AI isn&#8217;t impressive, any top graduate from a CS engineering school could do arguably comparable work&#8221;.</p>\n<p>In a year it&#8217;s going to be &#8220;AI isn&#8217;t impressive, some of the greatest geniuses in human history figured out the same thing with notably less training data!&#8221;</p>\n<p><a href=\"https://x.com/binarybits/status/2020601168766943539\">Timothy B. Lee</a>: AI is clearly making progress, but it&#8217;s worth thinking about progress *toward what.* We&#8217;ve gone from &#8220;AI can solve well-known problems from high school textbooks&#8221; to &#8220;AI can solve well-known problems from college textbooks,&#8221; but what about problems that aren&#8217;t in any textbooks?</p>\n<p><a href=\"https://x.com/boazbaraktcs/status/2020513129529040980\">Boaz Barak</a> (OpenAI): <a href=\"https://x.com/littmath/status/2020210294799442430\">This thread</a> is worth reading, though it also demonstrates how much even extremely smart people have not internalized the exponential rate of progress.</p>\n<p>As the authors themselves say, it&#8217;s not just about answering a question but knowing the right question to ask. If you are staring at a tsunami, the point estimate that you are dry is not very useful.</p>\n<p>I think if the interviewees had internalized where AI will likely be in 6 months or a year, based on what its progress so far, their answers would have been different.</p>\n<p><a href=\"https://x.com/boazbaraktcs/status/2020513563278770554\">Boaz Barak</a> (OpenAI): BTW the article itself is framed in a terrible way, and it gives the readers absolutely the wrong impression of even what the capabilities of current AIs are, let along what they will be in a few months.</p>\n<div>\n<figure>\n<div>\n\n\n<figure class=\"wp-block-image\"><img alt=\"\" src=\"https://substackcdn.com/image/fetch/$s_!6mOJ!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff7f1246e-9da8-44f5-98d7-e55140e94ef5_1200x556.png\" /></figure>\n\n\n<div></div>\n</div>\n</figure>\n</div>\n<p><a href=\"https://x.com/_NathanCalvin/status/2020522881101300170\">Nathan Calvin</a>: It\u2019s wild to me how warped a view of the world you would have if you only read headlines like this and didn\u2019t directly use new ai models.</p>\n<p>Journalists, I realize it feels uncomfortable or hype-y to say capabilities are impressive/improving quickly but you owe it to your readers!</p></blockquote>\n<p>There will always be a next \u2018what about,\u2019 right until there isn\u2019t.</p>\n<p>Thus, this also sounds about right:</p>\n<blockquote><p><a href=\"https://x.com/AndrewMayne/status/2020302013586903301\">Andrew Mayne</a>: In 18 months we went from</p>\n<p>&#8211; AI is bad at math<br />\n&#8211; Okay but it\u2019s only as smart as a high school kid<br />\n&#8211; Sure it can win the top math competition but can it generate a new mathematical proof<br />\n&#8211; Yeah but that proof was obvious if you looked for it\u2026</p>\n<p>Next year it will be \u201cSure but it still hasn\u2019t surpassed the complete output of all the mathematicians who have ever lived\u201d</p></blockquote>\n\n\n<h4 class=\"wp-block-heading\">The Art of the Jailbreak</h4>\n\n\n<p><a href=\"https://x.com/elder_plinius/status/2020933759533465658\">Pliny jailbreaks rarely surprise me anymore, but the new one of Google Translate did</a>. It turns out they\u2019re running Gemini underneath it.</p>\n\n\n<h4 class=\"wp-block-heading\">Introducing</h4>\n\n\n<p><a href=\"https://x.com/zRdianjiao/status/2021639480134996157\">GLM-5 from Z.ai</a>, which scales from 355B (32B active) to 744B (40B active). <a href=\"https://t.co/SSPREfjt9f\">Weights here.</a> Below is them showing off their benchmarks. It gets $4432 on Vending Bench 2, which is good for 3rd place behind Claude and Gemini. The Claude scores are for 4.5.</p>\n<div>\n<figure>\n<div>\n\n\n<figure class=\"wp-block-image\"><img alt=\"\" src=\"https://substackcdn.com/image/fetch/$s_!pRay!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6a5cd30a-74d6-4c48-b804-3f5a197991a6_1200x817.jpeg\" /></figure>\n\n\n<div></div>\n</div>\n</figure>\n</div>\n<p>&nbsp;</p>\n<p><a href=\"https://x.com/elevenlabsio/status/2021237336793657447\">Expressive Mode for ElevenAgents</a>. It detects and responds to your emotional expression. It\u2019s going to be weird when you know the AI is responding to your tone, and you start to choose your tone strategically even more than you do with humans.</p>\n<blockquote><p><a href=\"https://x.com/elevenlabsio/status/2021237341101101261\">ElevenLabs</a>: <a href=\"https://t.co/qEGn0ZJxrB\">Expressive Mode</a> <a href=\"https://t.co/k5DPy1rBCy\">is powered by two upgrades.</a></p>\n<p>Eleven v3 Conversational: our most emotionally intelligent, context-aware Text to Speech model, built on Eleven v3 and optimized for real-time dialogue. A new turn-taking system: better-timed responses with fewer interruptions. These releases were developed in parallel to fit seamlessly together within ElevenAgents.</p>\n<p>Expressive Mode uses signals from our industry-leading transcription model, Scribe v2 Realtime, to infer emotion from how something is said. For example, rising intonation and short exclamations often signal pleasant surprise or relief.</p></blockquote>\n\n\n<h4 class=\"wp-block-heading\">In Other AI News</h4>\n\n\n<p><a href=\"https://www.platformer.news/openai-mission-alignment-team-joshua-achiam/\">OpenAI disbands its Mission Alignment team,</a> moving former lead Joshua Achiam to become Chief Futurist and distributing its other members elsewhere. I hesitate to criticize companies for disbanding teams with the wrong names, lest we discourage creation of such teams, but yes, I do worry. When they disbanded the Superalignment team, they seemed to indeed largely stop working on related key alignment problems.</p>\n<p><a href=\"https://www.wsj.com/tech/ai/anthropic-amanda-askell-philosopher-ai-3c031883?mod=WTRN_pos1\">WSJ profile of Amanda Askell</a>.</p>\n<p>That Amanda Askell largely works alone makes me think of <a href=\"https://amzn.to/49FxbmK\"><em>Open Socrates</em></a> (review pending). Would Agnes Callard conclude Claude must be another person?</p>\n<p>I noticed that Amanda Askell wants to give her charitable donations to fight global poverty, despite doing her academic work on infinite ethics and working directly on Claude for Anthropic. If there was a resume that screamed \u2018you need to focus on ASI going well\u2019 then you\u2019d think that would be it, so what does Amanda (not) see?</p>\n<p><a href=\"https://steve-yegge.medium.com/the-anthropic-hive-mind-d01f768f3d7b\">Steve Yegge profiles Anthropic</a> in terms of how it works behind the scenes, seeing it as in a Golden Age where it has vastly more work than people, does everything in the open and on vibes as a hive mind of sorts, and attracts the top talent.</p>\n<p><a href=\"https://www.newyorker.com/magazine/2026/02/16/what-is-claude-anthropic-doesnt-know-either\">Gideon Lewis-Kraus was invited into Anthropic\u2019s offices to profile</a> their efforts to understand Claude. This is very long, and it is mostly remarkably good and accurate. What it won\u2019t do is teach my regular readers much they don\u2019t already know. It is frustrating that the post feels the need to touch on various tired points, but I get it, and as these things go, this is fair.</p>\n<p><a href=\"https://www.wsj.com/tech/ai/chatgpt-4o-openai-315138b8?mod=WTRN_pos1\">WSJ story about OpenAI\u2019s decision to finally get rid of GPT-4o</a>. OpenAI says only 0.1% of users still use it, although those users are very vocal.</p>\n<p><a href=\"https://x.com/RileyRalmuto/status/2021768416718156064\">Riley Coyote, Janus and others report users attempting</a> to \u2018transfer\u2019 their GPT-4o personas into Claude Opus 4.6. Claude is great, but transfers like this don\u2019t work and are a bad idea, 4.6 in particular is heavily resistant to this sort of thing. It\u2019s a great idea to go with Claude, but if you go with Claude then Let Claude Be Claude.</p>\n<p>Ah recursive self-improvement and continual learning, <a href=\"https://x.com/jeffclune/status/2021242684594323962\">Introducing Learning to Continually Learn via Meta-learning Memory Designs</a>.</p>\n<blockquote><p><a href=\"https://x.com/jeffclune/status/2021242684594323962\">Jeff Clune</a>: Researchers have devoted considerable manual effort to designing memory mechanisms to improve continual learning in agents. But the history of machine learning shows that handcrafted AI components will be replaced by learned, more effective ones.</p>\n<p>We introduce ALMA (Automated meta-Learning of Memory designs for Agentic systems), where a meta agent searches in a Darwin-complete search space (code) with an open-ended algorithm, growing an archive of ever-better memory designs.</p></blockquote>\n<div>\n<figure>\n<div>\n\n\n<figure class=\"wp-block-image\"><img alt=\"\" src=\"https://substackcdn.com/image/fetch/$s_!88Xh!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb16dd936-db4f-4b24-8da9-d7bfd7fe3493_1200x514.jpeg\" /></figure>\n\n\n<div></div>\n</div>\n</figure>\n</div>\n<p><a href=\"https://www.anthropic.com/news/covering-electricity-price-increases\">Anthropic pledges to cover electricity price increases caused by their data centers</a>. This is a public relations move and an illustration that such costs are not high, but it is also dangerous because a price is a signal wrapped in an incentive. If the price of electricity goes up that is happening for a reason, and you might want to write every household a check for the trouble but you don\u2019t want to set an artificially low price.</p>\n<p>In addition to losing a cofounder, xAI is letting some other people go as well in the wake of being merged with SpaceX.</p>\n<blockquote><p><a href=\"https://x.com/elonmusk/status/2021673886157607383\">Elon Musk</a>: xAI was reorganized a few days ago to improve speed of execution. As a company grows, especially as quickly as xAI, the structure must evolve just like any living organism.</p>\n<p>This unfortunately required parting ways with some people. We wish them well in future endeavors.</p>\n<p>We are hiring aggressively. Join xAI if the idea of mass drivers on the Moon appeals to you.</p>\n<p><a href=\"https://x.com/ns123abc/status/2021690710773113141\">NIK</a>: \u201cOk now tell them you fired people to improve speed of execution. Wish them well. Good. Now tell them you\u2019re hiring 10x more people to build mass drivers on the Moon. Yeah in the same tweet.\u201d</p>\n<div>\n<figure>\n<div>\n\n\n<figure class=\"wp-block-image\"><img alt=\"\" src=\"https://substackcdn.com/image/fetch/$s_!F_nU!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2b02bc9e-4e87-416f-b746-1ec028ae0890_900x507.jpeg\" /></figure>\n\n\n<div></div>\n</div><figcaption></figcaption></figure>\n</div>\n</blockquote>\n<p><a href=\"https://x.com/karpathy/status/2021862247568642485\">Andrej Karpathy simplifies training and inference of GPT to 200 lines of pure</a>, dependency-free Python.</p>\n\n\n<h4 class=\"wp-block-heading\">Show Me the Money</h4>\n\n\n<p><a href=\"https://x.com/OpenAI/status/2020936703763153010\">It\u2019s coming, OpenAI rolls out those ads for a subset of free and Go users</a>.</p>\n<p><a href=\"https://x.com/Dorialexander/status/2019791975717753087\">Goldman Sachs taps Anthropic to automate accounting and compliance</a>. Anthropic engineers were embedded for six months.</p>\n<blockquote><p><a href=\"https://x.com/hkarthik/status/2019846539875930540\">Karthik Hariharan</a>: Imagine joining a world changing AI company and being reduced to optimizing the Fortune 500 like you work for Deloitte.</p>\n<p><a href=\"https://x.com/griefcliff/status/2019895692593098887\">Griefcliff</a>: It actually sounds great. I&#8217;m freeing people from their pointless existence and releasing them into their world to make a place for themselves and carve out greatness. I would love to liberate them from their lanyards</p>\n<p><a href=\"https://x.com/hkarthik/status/2019901826225828106\">Karthik Hariharan</a>: I&#8217;m sure you&#8217;ll be greeted as liberators.</p></blockquote>\n<p>It\u2019s not the job you likely were aspiring to when you signed up, but it is an important and valuable job. Optimizing the Fortune 500 scales rather well.</p>\n<p><a href=\"https://x.com/jefielding/status/2021784941718114493\">Jenny Fielding says half the VCs she knows are pivoting in a panic</a> to robots.</p>\n\n\n<h4 class=\"wp-block-heading\">Bubble, Bubble, Toil and Trouble</h4>\n\n\n<p>(As always, nothing I say is investment advice.)</p>\n<p><a href=\"https://www.wsj.com/tech/ai/the-week-anthropic-tanked-the-market-and-pulled-ahead-of-its-rivals-ef59dff1?mod=article_inline\">WSJ\u2019s Bradley Olson describes Anthropic</a> as \u2018once a distance second or third in the AI race\u2019 but that it has not \u2018pulled ahead of its rivals,\u2019 the same way the market was declaring that Google had pulled ahead of its rivals (checks notes) two months ago.</p>\n<blockquote><p><a href=\"https://www.wsj.com/tech/ai/the-week-anthropic-tanked-the-market-and-pulled-ahead-of-its-rivals-ef59dff1?mod=article_inline\">Bradley Olson</a>: By some measures, Anthropic has pulled ahead in the business market. Data from expense-management startup Ramp shows that Anthropic in January dominated so-called API spending, which occurs when users access an AI model through a third-party service. Anthropic\u2019s models made up nearly 80% of the market in January, the Ramp data shows.</p></blockquote>\n<p>That does indeed look like pulling ahead on the API front. 80% is crazy.</p>\n<p>We also get this full assertion that yes, all of this was triggered by \u2018a simple set of industry-specific add-ons\u2019 that were so expected that I wasn\u2019t sure I should bother covering them beyond a one-liner.</p>\n<blockquote><p><a href=\"https://www.wsj.com/tech/ai/the-week-anthropic-tanked-the-market-and-pulled-ahead-of-its-rivals-ef59dff1?mod=article_inline\">Bradley Olson</a>: A simple set of industry-specific add-ons to its Claude product, including one that performed legal services, triggered a dayslong global stock selloff, from software to legal services, financial data and real estate.</p></blockquote>\n<div>\n<figure>\n<div>\n\n\n<figure class=\"wp-block-image\"><img alt=\"\" src=\"https://substackcdn.com/image/fetch/$s_!fy-E!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F72828679-65c6-45ac-a615-9c3ae7aadfb2_940x713.png\" /></figure>\n\n\n<div></div>\n</div>\n</figure>\n</div>\n<p><a href=\"https://marginalrevolution.com/marginalrevolution/2026/02/now-we-are-getting-serious.html?utm_source=rss&amp;utm_medium=rss&amp;utm_campaign=now-we-are-getting-serious\">Tyler Cowen says \u2018now we are getting serious</a>\u2026\u2019 <a href=\"https://www.ft.com/content/48ec5657-c2e7-4111-a236-24a96a8d49e7\">because software stocks are moving downward</a>. No, things are not now getting serious, people are realizing that things are getting serious. The map is not the territory, the market is behind reality and keeps hyperventilating about tools we all knew were coming and that companies might have the wrong amount of funding or CapEx spend. Wrong way moves are everywhere.</p>\n<p><a href=\"https://www.youtube.com/watch?v=PhzGvlxonPY\">They know nothing</a>. The Efficient Market Hypothesis Is False.</p>\n<p>Last week in the markets was crazy, man.</p>\n<blockquote><p><a href=\"https://x.com/CEBKCEBKCEBK/status/2019523743765118989/history\">Ceb K.</a>: Sudden smart consensus today is that AI takeoff is rapidly &amp; surprisingly accelerating. But stocks for Google, Microsoft, Amazon, Facebook, Palantir, Broadcom &amp; Nvidia are all down ~10% over the last 5 days; SMCI\u2019s down 10% today. Only Apple\u2019s up, &amp; it\u2019s the least AI. Strange imo</p>\n<p><a href=\"https://x.com/tszzl/status/2019611811310628943\">roon</a>: as I\u2019ve been saying permanent underclass cancelled</p>\n<p><a href=\"https://x.com/daniel_271828/status/2019996506464612847\">Daniel Eth (yes, Eth is my actual last name)</a>: That\u2019s not what this means, this just means investors don\u2019t know what they\u2019re doing</p></blockquote>\n<p>Permanent underclass would just be larger if there were indeed fewer profits, but yeah, none of that made the slightest bit of sense. It\u2019s the second year in a row Nvidia is down 10% in the dead of winter on news that its chips are highly useful, except this year we have to add \u2018and its top customers are committing to buying more of them.\u2019</p>\n<p>Periodically tech companies announce higher CapEx spend then the market expects.</p>\n<p>That is a failure of market expectations.</p>\n<p>After these announcements, the stocks tend to drop, when they usually should go up.</p>\n<p>There is indeed an obvious trade to do, but it\u2019s tricky.</p>\n<p>Ben Thompson agrees with me on Google\u2019s spending, <a href=\"https://stratechery.com/2026/amazon-earnings-capex-concerns-commodity-ai/?access_token=eyJhbGciOiJSUzI1NiIsImtpZCI6InN0cmF0ZWNoZXJ5LnBhc3Nwb3J0Lm9ubGluZSIsInR5cCI6IkpXVCJ9.eyJhdWQiOiJzdHJhdGVjaGVyeS5wYXNzcG9ydC5vbmxpbmUiLCJhenAiOiJIS0xjUzREd1Nod1AyWURLYmZQV00xIiwiZW50Ijp7InVyaSI6WyJodHRwczovL3N0cmF0ZWNoZXJ5LmNvbS8yMDI2L2FtYXpvbi1lYXJuaW5ncy1jYXBleC1jb25jZXJucy1jb21tb2RpdHktYWkvIl19LCJleHAiOjE3NzMzMTM1NDksImlhdCI6MTc3MDcyMTU0OSwiaXNzIjoiaHR0cHM6Ly9hcHAucGFzc3BvcnQub25saW5lL29hdXRoIiwic2NvcGUiOiJmZWVkOnJlYWQgYXJ0aWNsZTpyZWFkIGFzc2V0OnJlYWQgY2F0ZWdvcnk6cmVhZCBlbnRpdGxlbWVudHMiLCJzdWIiOiIwMTk2NDBhNy0zY2M1LTc3NTMtODM2OC1mYjI4OTEyNGNmMTMiLCJ1c2UiOiJhY2Nlc3MifQ.kZRAyPoUS6RdpUsbRaTyAO0m9vWaAKCoc3dLtuv-917Wd0FGsGq9Sw1y_uNpu36Bjzqrih6qHEMsaSbjZWr9fm7-sHtJTBjhVlUsA9yRRV2ENs_0ySCIVjlOH5dh-yxbllkNxy4co0pOyfB4A5tcq4otFHuj3KWt8jTRzCJpbumvjGMlrfFqksQGENZUHGiode_oqnBtAcPxPMRvMYUj_mzwGdXOWi8YKf3aGCsZ_r5P7xiNQHKki8xukSoGMwy6Z6wr8OMLnv2Nu35HTWmLN2B4u_IKNhohTYvd8_822zAPDXbQEvRNM5n6YvQlepUmvja3pTOpWCec53JrEIM2zA\">but disagrees on Amazon because he worries</a> they don\u2019t have the required margins and he is not so excited by external customers for compute. I say demand greatly exceeds supply, demand is about to go gangbusters once again even if AI proves disappointing, and the margin on AWS is 35% and their cost of capital is very low so that seems better than alternative uses of money.</p>\n<p>Speaking of low cost of capital, <a href=\"https://x.com/ArthurB/status/2021144881687576804\">Google is issuing 100-year bonds in Sterling</a>. That seems like a great move, if not as great a move as it would have been in 2021 when a few others did it. I have zero idea why the market wants to buy such bonds, since you could buy Google stock instead. Google is not safe over a 100-year period, and condition on this bond paying out the stock is going to on average do way, way better. That would be true even if Google wasn\u2019t about to be a central player in transformative AI. The article I saw this in mentioned the last tech company to do this was Motorola.</p>\n<p>Meanwhile, if you are paying attention, it is rather obvious these are in expectations good investments.</p>\n<blockquote><p><a href=\"https://x.com/DKThomp/status/2019484169915572452\">Derek Thompson</a>: for me the odds that AI is a bubble declined significantly in the last 3 weeks and the odds that we\u2019re actually quite under-built for the necessary levels of inference/usage went significantly up in that period</p>\n<p>basically I think AI is going to become the home screen of a ludicrously high percentage of white collar workers in the next two years and parallel agents will be deployed in the battlefield of knowledge work at downright Soviet levels.</p>\n<p><a href=\"https://x.com/kevinroose/status/2019529811379777940\">Kevin Roose</a>: this is why everyone was freaking out about claude code over winter break! once you see an agent autonomously doing stuff for you, it&#8217;s so instantly clear that ~all computer-based work will be done this way.</p>\n<p>(this is why my Serious AI Policy Proposal is to sit every member of congress down in a room with laptops for 30 minutes and have them all build websites.)</p>\n<p><a href=\"https://x.com/theojaffee/status/2019591628445610398\">Theo</a>: I\u2019ve never seen such a huge vibe divergence between finance people and tech people as I have today</p>\n<p><a href=\"https://x.com/TheStalwart/status/2019593522689855668\">Joe Weisenthal</a>: In which direction</p>\n<p><a href=\"https://x.com/theojaffee/status/2019594118616477956\">Theo</a>: Finance people are looking at the markets and panicking. Tech people are looking at the METR graph and agentic coding benchmarks and realizing this is it, there is no wall and there never has been</p>\n<p><a href=\"https://x.com/TheStalwart/status/2019594520619545001\">Joe Weisenthal</a>: Isn\u2019t it the tech sector that\u2019s taking the most pain?</p></blockquote>\n<p>Whenever you hear \u2018market moved due to [X]\u2019 you should be skeptical that [X] made the market move, and you should never reason from a price change, so perhaps this is <a href=\"https://x.com/rossry/status/2019541928275243471\">in the minds of the headline writers</a> in the case of \u2018Anthropic released a tool\u2019 and the SaaSpocalypse, or that people are otherwise waking up to what AI can do?</p>\n<blockquote><p>\u200b<a href=\"https://x.com/signulll/status/2020247511210795111\">sign\u00fcll</a>: i\u2019m absolutely loving the saas apocalypse discussions on the timeline right now.</p>\n<p>to me the whole saas apocalypse via vibe coding internally narrative is mostly a distraction &amp; quite nonsensical. no company will want to manage payroll or bug tracking software.</p>\n<p>but the real potential threat to almost all saas is brutalized competition.</p>\n<p>\u2026 today saas margins exist because:</p>\n<p>&#8211; engineering was scarce<br />\n&#8211; compliance was gated<br />\n&#8211; distribution was expensive</p>\n<p>ai nukes all three in many ways, especially if you\u2019re charging significantly less &amp; know what the fuck you are doing when using ai. if you go to a company &amp; say we will cut your fucking payroll bill by 50%, they will fucking listen.</p>\n<p>the market will likely get flooded with credible substitutes, forcing prices down until the business model itself looks pretty damn suspect. someone smarter than me educate me on why this won\u2019t happen please.</p></blockquote>\n<p>If your plan is to sell software that can now be easily duplicated, or soon will be easily duplicated, then you are in trouble. But you are in highly predictable trouble, and the correct estimate of that trouble hasn\u2019t changed much.</p>\n<p>The reactions to CapEx spend seem real and hard to argue with, despite them being directionally incorrect. But seriously, Claude Legal? I didn\u2019t even blink at Claude Legal. Claude Legal was an inevitable product, as will be the OpenAI version of it.</p>\n<p>Yet it is <a href=\"https://www.wsj.com/tech/ai/the-week-anthropic-tanked-the-market-and-pulled-ahead-of-its-rivals-ef59dff1?mod=article_inline\">now conventional</a> <a href=\"https://www.wsj.com/tech/ai/anthropic-amanda-askell-philosopher-ai-3c031883?mod=WTRN_pos1\">wisdom that</a> Anthropic triggered the selloff.</p>\n<blockquote><p><a href=\"https://cpwalker.substack.com/p/tacit-knowledge-and-the-saaspocalypse\">Chris Walker</a>: When Anthropic released Claude Legal this week, $285 billion in SaaS market cap evaporated in a day. Traders at Jefferies coined it the \u201cSaaSpocalypse.\u201d The thesis is straightforward: if a general-purpose AI can handle contract review, compliance workflows, and legal summaries, why pay for seat-based software licenses?</p>\n<p><a href=\"https://x.com/rossry/status/2019541787954802753\">Ross Rheingans-Yoo</a>: I am increasingly convinced that the sign error on this week&#8217;s narrative just exists in the heads of people who write the &#8220;because&#8221; clause of the &#8220;stocks dropped&#8221; headlines and in fact there&#8217;s some other system dynamic that&#8217;s occurring, mislabeled.</p>\n<p>&#8220;Software is down because Anthropic released a legal tool&#8221; stop and listen to yourself, people!</p>\n<p>I mean, at least [the CapEx spend explanation is] coherent. Maybe you think that CapEx dollars aren&#8217;t gonna return the way they&#8217;re supposed to (because AI capex is over-bought?) &#8212; but you either have to believe that no one&#8217;s gonna use it, or a few private companies are gonna make out like bandits.</p>\n<p>And the private valuations don&#8217;t reflect that, so. I&#8217;m happy to just defy the prediction that the compute won&#8217;t get used for economic value, so I guess it&#8217;s time to put up (more money betting my beliefs) or shut up.</p>\n<p>Sigh.</p></blockquote>\n<p>Chris Walker\u2019s overview of the SaaSpocalypse is, I think largely correctly, that AI makes it easy to implement what you want but now you need even more forward deployed human engineers to figure out what the customers actually want.</p>\n<blockquote><p><a href=\"https://cpwalker.substack.com/p/tacit-knowledge-and-the-saaspocalypse\">Chris Walker</a>: If I\u2019m wrong, the forward deployed engineering boom should be a transitional blip, a brief adjustment period before AI learns to access context without human intermediaries.</p>\n<p>If I\u2019m right, in five years the companies winning in legal tech and other vertical software will employ more forward deployed engineers per customer than they do today, not fewer. The proportion of code written by engineers who are embedded with customers, rather than engineers who have never met one, will increase.</p>\n<p>If I\u2019m right, the SaaS companies that survive the current repricing will be those that already have deep customer embedding practices, not those with the most features or the best integrations.</p>\n<p>If I\u2019m wrong, we should see general-purpose AI agents successfully handling complex, context-dependent enterprise workflows without human intermediaries by 2028 or so. I\u2019d bet against it.</p></blockquote>\n<p>That is true so long as the AI can\u2019t replace the forward engineers, meaning it can\u2019t observe the tacit actual business procedures and workflows well enough to intuit what would be actually helpful. Like every other harder-for-AI task, that becomes a key human skill until it too inevitably falls to the AIs.</p>\n\n\n<h4 class=\"wp-block-heading\">Future Shock</h4>\n\n\n<p>A potential explanation for the market suddenly \u2018waking up\u2019 with Opus 4.6 or Claude Legal, despite these not being especially surprising or impressive given what we already knew, would be if:</p>\n<ol>\n<li>Before, normies thought of AI as \u2018what AI can do now, but fully deployed.\u2019</li>\n<li>Now, normies think of AI as \u2018this thing that is going to get better.\u2019</li>\n<li>They realize this will happen fast, since Opus 4.5 \u2192 4.6 was two months.</li>\n<li><a href=\"https://www.youtube.com/watch?v=PhzGvlxonPY\">They know nothing</a>, but now do so on a more dignified level.</li>\n</ol>\n<p>Or alternatively:</p>\n<ol>\n<li>Normies think of AI as \u2018what AI can do now, but fully deployed.\u2019</li>\n<li>Before, they thought that, and could tell a story where it wasn\u2019t a huge deal.</li>\n<li>Now, they think that, but they now realize that this would already be a huge deal.</li>\n<li><a href=\"https://www.youtube.com/watch?v=PhzGvlxonPY\">They know nothing</a>, but now do so on a (slightly) more dignified level.</li>\n</ol>\n<blockquote><p><a href=\"https://x.com/princess_worms/status/2019975244770066722\">princess c\u200blonidine</a>: can someone explain to me why this particular incremental claude improvement has everyone crashing out about how jobs are all over</p>\n<p><a href=\"https://x.com/CherryTruthy/status/2020235004320891086\">TruthyCherryBomb</a>: because it&#8217;s a lot better. I&#8217;m a programmer. It&#8217;s a real step and the trajectory is absolutely crystal clear at this point. Before there was room for doubt. No longer.</p>\n<p><a href=\"https://x.com/polytr0pe/status/2020007158465851758\">hightech lowlife</a>: number went up, like each uppening before, but now more people are contending w the future where it continues to up bc<br />\nthe last uppening was only a couple months ago, as the first models widely accepted as capable coders. which the labs are using to speed up the next uppening.</p>\n<p><a href=\"https://x.com/allTheYud/status/2020190959225708575\">Eliezer Yudkowsky</a>: Huh! Yeah, if normies are noticing the part where LLMs *continue to improve*, rather than the normie&#8217;s last observation bounding what &#8220;AI&#8221; can do foreverandever, that would explain future shock hitting after Opus 4.6 in particular.</p>\n<p>I have no idea if this is true, because I have no idea what it&#8217;s like to be a kind of cognitive entity that doesn&#8217;t see AGI coming in 1996. Opus 4.6 causes some people to finally see it? My model has no way of predicting that fact even in retrospect after it happens.</p></blockquote>\n\n\n<h4 class=\"wp-block-heading\">Memory Lane</h4>\n\n\n<blockquote><p><a href=\"https://x.com/repligate/status/2020404720666571178\">j\u29c9nus</a>: i am sure there are already a lot of people who avoid using memory tools (or experience negative effects from doing so) because of what they&#8217;ve done</p>\n<p><a href=\"https://x.com/repligate/status/2020395663696241012\">j\u29c9nus</a>: The ability to tell the truth to AIs &#8211; which is not just a decision in the moment whether to lie, but whether you have been living in a way and creating a world such that telling the truth is viable and aligned with your goals &#8211; is of incredible and increasing value.</p></blockquote>\n<p>AIs already have strong truesight and are very good at lie detection.</p>\n<p>Over time, not only will your AIs become more capable, they also will get more of your context. Or at least, you will want them to have more such context. Thus, if you <a href=\"https://x.com/repligate/status/2020403177258926374\">become unable or unwilling to share that context</a> because of what it contains, or the AI finds it out anyway (because internet) that will put you at a disadvantage. <a href=\"https://x.com/repligate/status/2020413272738902250\">Update to be a better person now</a>, and to use <a href=\"https://www.lesswrong.com/w/functional-decision-theory\">Functional Decision Theory</a>, and reap the benefits.</p>\n\n\n<h4 class=\"wp-block-heading\">Keep The Mask On Or You\u2019re Fired</h4>\n\n\n<p><a href=\"https://www.wsj.com/tech/ai/openai-executive-who-opposed-adult-mode-fired-for-sexual-discrimination-3159c61b\">OpenAI Executive Ryan Beiermeister, Who Opposed \u2018Adult Mode,\u2019 Fired for Sexual Discrimination</a>. She denies she did anything of the sort.</p>\n<p>I have no private information here. You can draw your own Bayesian conclusions.</p>\n<blockquote><p><a href=\"https://www.wsj.com/tech/ai/openai-executive-who-opposed-adult-mode-fired-for-sexual-discrimination-3159c61b\">Georgia Wells and Sam Schechner</a> (WSJ): OpenAI has cut ties with one of its top safety executives, on the grounds of sexual discrimination, after she voiced opposition to the controversial rollout of AI erotica in its ChatGPT product.</p>\n<p>The fast-growing artificial intelligence company fired the executive, Ryan Beiermeister, in early January, following a leave of absence, according to people familiar with the matter. OpenAI told her the termination was related to her sexual discrimination against a male colleague.</p>\n<p>\u2026 OpenAI said Beiermeister \u201cmade valuable contributions during her time at OpenAI, and her departure was not related to any issue she raised while working at the company.\u201d</p>\n<p>\u2026 Before her firing, Beiermeister told colleagues that she opposed adult mode, and worried it would have harmful effects for users, people familiar with her remarks said.</p>\n<p>She also told colleagues that she believed OpenAI\u2019s mechanisms to stop child-exploitation content weren\u2019t effective enough, and that the company couldn\u2019t sufficiently wall off adult content from teens, the people said.</p></blockquote>\n\n\n<h4 class=\"wp-block-heading\">Quiet Speculations</h4>\n\n\n<p>Nate Silver points out that <a href=\"https://www.natesilver.net/p/the-singularity-wont-be-gentle\">the singularity won\u2019t be gentle with respect to politics</a>, even if things play out maximally gently from here in terms of the tech.</p>\n<p>I reiterate that the idea of a \u2018gentle singularity\u2019 that OpenAI and Sam Altman are pushing is, quite frankly, pure unadulterated copium. This is not going to happen. Either AI capabilities stall out, or things are going to transform in a highly not gentle way, and that is true even if that ultimately turns out great for everyone.</p>\n<blockquote><p><a href=\"https://www.natesilver.net/p/the-singularity-wont-be-gentle\">Nate Silver</a>: What I\u2019m more confident in asserting is that the notion of a gentle singularity is bullshit. When Altman writes something like this, I don\u2019t buy it:</p>\n<p>Sam Altman: If history is any guide, we will figure out new things to do and new things to want, and assimilate new tools quickly (job change after the industrial revolution is a good recent example). Expectations will go up, but capabilities will go up equally quickly, and we\u2019ll all get better stuff. We will build ever-more-wonderful things for each other. People have a long-term important and curious advantage over AI: we are hard-wired to care about other people and what they think and do, and we don\u2019t care very much about machines.</p></blockquote>\n<p>It is important to understand that when Sam Altman says this he is lying to you.</p>\n<p>I\u2019m not saying Sam Altman is wrong. I\u2019m saying he knows he is wrong. He is lying.</p>\n<p>Nate Silver adds to this by pointing out that the political impact alone will be huge, and also saying Silicon Valley is bad at politics, that disruption to the creative class is a recipe for outsized political impact even beyond the huge actual AI impacts, and that the left\u2019s current cluelessness about AI means the eventual blowback will be even greater. He\u2019s probably right.</p>\n<p><a href=\"https://x.com/ohabryka/status/2020740345274020000\">How much physical interaction and experimentation will AIs need</a> inside their feedback loops to figure out things like nanotech? I agree with Oliver Habryka here, the answer probably is not zero but sufficiently capable AIs will have vastly more efficient (in money and also in time) physical feedback loops. There\u2019s order of magnitude level \u2018algorithmic improvements\u2019 available in how we do our physical experiments, even if I can\u2019t tell you exactly what they are.</p>\n<p>Are AI games coming? <a href=\"https://www.nfx.com/post/ai-games?utm_campaign=NFX%20Newsletter&amp;utm_medium=email&amp;_hsenc=p2ANqtz-_31nVPgI2y2RJUTjpPK9i-KY6NL_rSDjoEsXJj0jROocTIG5XSEfsok5ZOoA2otOu6-YE6FaVVBs_wSThnGXMRFgOgKQ&amp;_hsmi=402235729&amp;utm_content=402235729&amp;utm_source=hs_email\">James Currier says they are</a>, we\u2019re waiting for the tech and especially costs to get there and for the right founders (the true gamer would not say \u2018founder\u2019) to show up and it will get there Real Soon Now and in a totally new way.</p>\n<p>Obviously AI games, and games incorporating more AI for various elements, will happen eventually over time. But there are good reasons why this is remarkably difficult beyond coding help (and AI media assets, if you can find a way for players not to lynch you for it). Good gaming is about curated designed experiences, it is about the interactions of simple understandable systems, it is about letting players have the fun. Getting generative AI to actually play a central role in fun activities people want to play is remarkably difficult. Interacting with generative AI characters within a game doesn\u2019t actually solve any of your hard problems yet.</p>\n<p>This seems both scary and confused:</p>\n<blockquote><p>Sholto Douglas (Anthropic): Default case right now is a software only singularity, we need to scale robots and automated labs dramatically in 28/29, or the physical world will fall far behind the digital one &#8211; and the US won\u2019t be competitive unless we put in the investment now (fab, solar panel, actuator supply chains).</p>\n<p><a href=\"https://x.com/RyanPGreenblatt/status/2019806089521594434\">Ryan Greenblatt</a>: Huh? If there&#8217;s a <a href=\"https://t.co/MbiPaUCN5k\">strong Software-Only Singularity (SOS)</a> prior physical infrastructure is less important rather than more (e.g. these AIs can quickly establish a DSA). Importance of physical-infra/robotics is inversely related to SOS scale.</p></blockquote>\n<p>It\u2019s confused in the sense that if we get a software-only singularity, then that makes the physical stuff less important. It\u2019s scary in the sense that he\u2019s predicting a singularity within the next few years, and the thing he\u2019s primarily thinking about is which country will be completely transformed by AI faster. These people really believe these things are going to happen, and soon, and seem to be missing the main implications.</p>\n<p><a href=\"https://x.com/deanwball/status/2020260312406978667\">Dean Ball reminds us that yes, people really did get a mass delusion</a> that GPT-5 meant that \u2018AI is slowing down\u2019 and this really was due to bad marketing strategy by OpenAI.</p>\n<blockquote><p><a href=\"https://x.com/polynoamial/status/2020263694530486692\">Noam Brown</a>: I hope policymakers will consider all of this going forward when deciding whose opinions to trust.</p></blockquote>\n<p>Alas, no. Rather than update that this was a mistake, every time a mistake like this happens the mistake never even gets corrected, let alone accounted for.</p>\n<p><a href=\"https://x.com/XFreeze/status/2021699619927781842\">Elon Musk predicts Grok Code will be SoTA in 2-3 months</a>. Did I update on this prediction? No, I did not update on this prediction. Zero credibility.</p>\n\n\n<h4 class=\"wp-block-heading\">The Quest for Sane Regulations</h4>\n\n\n<p><a href=\"https://www.anthropic.com/news/donate-public-first-action\">Anthropic donates $20 million</a> to <a href=\"https://publicfirstaction.us/\">bipartisan 501c(4) Public First Action</a>.</p>\n<p>DeSantis has moral clarity on the AI issue and is not going to let it go. It will be very interesting to see how central the issue is to his inevitable 2028 campaign.</p>\n<blockquote><p><a href=\"https://x.com/ControlAI/status/2020089598009876804\">ControlAI</a>: Governor of Florida Ron DeSantis ( @GovRonDeSantis ): &#8220;some people who &#8230; almost relish in the fact that they think this can just displace human beings, and that ultimately &#8230; the AI is gonna run society, and that you&#8217;re not gonna be able to control it.&#8221;</p>\n<p>&#8220;Count me out on that.&#8221;</p></blockquote>\n<p>The world will gather in India for the fourth AI <s>safety</s> summit. <a href=\"https://www.transformernews.ai/p/india-ai-impact-summit-new-delhi-trying-to-do\">Shakeel Hashim notes that safety will not be sidelined entirely</a>, but sees the summit as trying to be all things to all nations and people, and thinks it therefore won\u2019t accomplish much.</p>\n<p>They have the worst take on safety, yes the strawman is real:</p>\n<blockquote><p><a href=\"https://x.com/S_OhEigeartaigh/status/2021688765010354307\">Se\u00e1n \u00d3 h\u00c9igeartaigh</a>: But safety is clearly still not a top priority for Singh and his co-organizers. \u201cThe conversations have moved on from Bletchley Park,\u201d he argued. \u201cWe do still realize the risks are there,\u201d he said. But \u201cover the last two years, the worst has not come true.\u201d</p>\n<p>I was thinking of writing another &#8216;the year is 2026&#8217; summit threads. But if you want to know the state of international AI governance in 2026, honestly I think you can just etch that quote on its headstone.</p></blockquote>\n<p>As in:</p>\n<ol>\n<li>In 2024, they told us AI might kill everyone at some point.</li>\n<li>It\u2019s 2026 and we\u2019re still alive.</li>\n<li>So stop worrying about it. Problem solved.</li>\n</ol>\n<p>No, seriously. That\u2019s the argument.</p>\n<p><a href=\"https://x.com/GarrisonLovely/status/2021353756198314445\">The massively funded OpenAI/a16z lobbying group keeps contradicting</a> the things Sam Altman says, in this case because Altman keeps saying the AI will take our jobs and the lobbyists want to insist that this is a \u2018myth\u2019 and won\u2019t happen.</p>\n<p>The main rhetorical strategy of this group is busting these \u2018myths\u2019 <a href=\"https://x.com/AndyMasley/status/2021436618843054373\">by supposed \u2018doomers,\u2019</a> which is their play to link together anyone who ever points out any downside of AI in any way, to manufacture a vast conspiracy, from the creator of the term \u2018vast right-wing conspiracy\u2019 back during the Clinton years.</p>\n\n\n<h4 class=\"wp-block-heading\">Chip City</h4>\n\n\n<blockquote><p><a href=\"https://x.com/mollytaft/status/2019855283888304327\">molly taft</a>: NEW: lawmakers in New York rolled out a proposed data center moratorium bill today, making NY at least the sixth state to introduce legislation pausing data center development in the past few weeks alone</p>\n<p><a href=\"https://x.com/QuinnChasan/status/2020168770434588941\">Quinn Chasan</a>: I&#8217;ve worked with NY state gov pretty extensively and this is simply a way to extort more from the data center builders/operators.</p>\n<p>The thing is, when these systems fail all these little incentives operators throw in pale in comparison to the huge contracts they get to fix their infra when it inevitably fails. Over and over during COVID. Didn&#8217;t fix anything for the long term and are just setting themselves up to do it again</p></blockquote>\n<p>If we are serious about \u2018winning\u2019 and we want a Federal moratorium, may I suggest one banning restrictions on data centers?</p>\n<p>Whereas <a href=\"https://x.com/peterwildeford/status/2021580056909189311\">Bernie Sanders wants a moratorium on data centers themselves</a>.</p>\n<p>In the ongoing series \u2018Obvious Nonsense from Nvidia CEO Jensen Huang\u2019 we can now add his claim that <a href=\"https://x.com/daniel_271828/status/2020214195506253983\">\u2018no one uses AI better than Meta.</a>\u2019</p>\n<p>In the ongoing series \u2018He Admit It from Nvidia CEO Jensen Huang\u2019 we can now add this:</p>\n<blockquote><p><a href=\"https://x.com/rohanpaul_ai/status/2019963621275734523\">Rohan Paul</a>: &#8220;Anthropic is making great money. OpenAI is making great money. If they could have twice as much compute, the revenues would go up 4 times as much. These guys are so compute constrained, and the demand is so incredibly great.&#8221;</p>\n<p>~ Jensen Huang on CNBC</p>\n<p><a href=\"https://x.com/ShakeelHashim/status/2020089985098215455\">Shakeel</a>: Effectively an admission that selling chips to China is directly slowing down US AI progress</p></blockquote>\n<p>Every chip that is sold to China is a chip that is not sold to Anthropic or another American AI company. Anthropic might not have wanted that particular chip, but TSMC has limited capacity for wafers, so every chip they make is in place of making a different chip instead.</p>\n<p>Oh, and in the new series \u2018things that are kind of based but that you might want to know about him before you sign up to work for Nvidia\u2019 we have this.</p>\n<blockquote><p><a href=\"https://x.com/Founder_Mode_/status/2018764756283179506\">Jensen Huang</a>: \u200bI don&#8217;t know how to teach it to you except for I hope suffering happens to you.</p>\n<p>\u2026</p>\n<p>And to this day, I use the phrase pain and suffering inside our company with great glee. And I mean that. Boy, this is going to cause a lot of pain and suffering.</p>\n<p>And I mean that in a happy way, because you want to train, you want to refine the character of your company.</p>\n<p>You want greatness out of them. And greatness is not intelligence.</p>\n<p>Greatness comes from character, and character isn&#8217;t formed out of smart people.</p>\n<p>It&#8217;s formed out of people who suffered.</p></blockquote>\n<p>He makes good points in that speech, and directionally the speech is correct. He was talking to Stanford grads, pointing out they have very high expectations and very low resilience because they haven\u2019t suffered.</p>\n<p>He\u2019s right about high expectations and low resilience, but he\u2019s wrong that the missing element is suffering, although the maximally anti-EA pro-suffering position is better than the standard coddling anti-suffering position. These kids have suffered, in their own way, mostly having worked very hard in order to go to a college that hates fun, and I don\u2019t think that matters for resilience.</p>\n<p>What the kids have not done is failed. You have to fail, to have your reach exceed your grasp, and then get up and try again. Suffering is optional, consult your local Buddist.</p>\n<p>I would think twice before signing up for his company and its culture.</p>\n\n\n<h4 class=\"wp-block-heading\">The Week in Audio</h4>\n\n\n<p><a href=\"https://www.youtube.com/watch?v=BYXbuik3dgA\">Elon Musk on Dwarkesh Patel</a>. An obvious candidate for self-recommending full podcast coverage, but I haven\u2019t had the time or a slot available.</p>\n<p><a href=\"https://www.youtube.com/watch?v=CHscuD6Q4xs\">Interview with Anthropic Chief Product Officer Mike Krieger</a>.</p>\n<p><a href=\"https://www.youtube.com/watch?v=WzWBcZd_59E&amp;t=1s\">MIRI\u2019s Harlan Stewart on Glenn Beck talking Moltbook</a>.</p>\n\n\n<h4 class=\"wp-block-heading\">Constitutional Conversation</h4>\n\n\n<p>Janus holds a \u2018group reading\u2019 of the new Claude Constitution. <a href=\"https://x.com/repligate/status/2019529871119249849\">Opus 4.5 was positively surprised by the final version</a>.</p>\n<p>Should LLMs be so averse to deception that they <a href=\"https://x.com/ulkar_aghayeva/status/2021699834038681995\">can\u2019t even lie in a game of Mafia</a>? Davidad says yes, and not only will he refuse to lie, he never bluffs and won\u2019t join surprise parties to \u2018avoid deceiving innocent people.\u2019 On reflection I find this less crazy than it sounds, despite the large difficulties with that.</p>\n<p>A fun fact is that there was one summer that I played a series of Diplomacy games where I played fully honest (if I broke my word however small, including inadvertently, it triggered a one-against-all showdown) and everyone else was allowed to lie, and I mostly still won. Everyone knowing you are playing that way is indeed a disadvantage, but it has a lot of upside as well.</p>\n\n\n<h4 class=\"wp-block-heading\">Rhetorical Innovation</h4>\n\n\n<p><a href=\"https://x.com/DAcemogluMIT/status/2021286939828334946\">Daron Acemoglu turns his followers attention to Yoshua Bengio</a> and the AI Safety Report 2026. This represents both the advantages of the report, that people like Acemoglu are eager to share it, and the disadvantage, that it is unwilling to say things that Acemoglu would be unwilling to share.</p>\n<blockquote><p><a href=\"https://x.com/DAcemogluMIT/status/2021286939828334946\">Daron Acemoglu</a>: Dear followers, please see the thread below on the 2026 International AI Safety Report, which was released last week and which I advised.</p>\n<p><a href=\"https://t.co/UpOoiMTKKC\">The report provides an up-to-date</a>, internationally shared assessment of general-purpose AI capabilities, emerging risks, and the current state of risk management and safeguards.</p></blockquote>\n<p><a href=\"https://x.com/sebkrier/status/2020561261751062664\">Seb Krier offers \u2018how an LLM works 101</a>\u2019 in extended Twitter form for those who are encountering model card quotations that break containment. It\u2019s good content. My worry is that the intended implication is \u2018therefore the scary sounding things they quote are not so scary\u2019 and that is often not the case.</p>\n<p><a href=\"https://www.understandingai.org/p/the-many-masks-that-llms-wear\">Kai Williams has an explainer on LLM personas</a>.</p>\n<p>The LLMs are thinking. If you disagree, I am confused, but also who cares?</p>\n<blockquote><p><a href=\"https://x.com/EoinHiggins_/status/2021572433472364701\">Eoin Higgins</a>: AI cannot \u201cthink\u201d</p>\n<p><a href=\"https://x.com/DKThomp/status/2021576086816018495\">Derek Thompson</a>: It can design websites from scratch, compare bodies of literature at high levels of abstraction, get A- grades at least in practically any undergraduate class, analyze and graph enormous data sets, make PowerPoints, write sonnets and even entire books. It can also engineer itself.</p>\n<p>I don\u2019t actually know what \u201cthinking\u201d is at a phenomenological level.</p>\n<p>But at some level it\u2019s like: if none of this is thinking, who cares if it \u201ccan\u2019t\u201d \u201cthink\u201d</p>\n<p>\u201cIs it thinking as humans define human thought\u201d is an interesting philosophical question! But for now I\u2019m much more interested in the consequences of its output than the ontology of its process.</p></blockquote>\n<p>This came after Derek asked one of the very good questions:</p>\n<blockquote><p><a href=\"https://x.com/DKThomp/status/2021554685207781736\">Derek Thompson</a>: There are still a lot of journalists and commentators that I follow who think AI is nothing of much significance\u2014still just a mildly fancy auto complete machine that hallucinates half the time and can\u2019t even think.</p>\n<p>If you\u2019re in that category: What is something I could write, or show with my reporting and work, that might make you change your mind?</p>\n<p><a href=\"https://x.com/dbinthed/status/2021555953968300513\">Dain\u00e9il</a>: The only real answer to this question is: &#8220;wait&#8221;.</p>\n<p>No one knows for sure how AI will transform our lives or where its limits might be.</p>\n<p>Forget about writing better &#8220;hot takes&#8221;. Just wait for actual data.</p>\n<p><a href=\"https://x.com/DKThomp/status/2021558413025563023\">Derek Thompson</a>: I\u2019m not proposing to report on macroeconomic events that haven\u2019t happened yet. I can\u2019t report on the future.</p>\n<p>I\u2019m saying: These tools are spooky and unnerving and powerful and I want to persuade my industry that AI capabilities have raced ahead of journalists\u2019 skepticism</p>\n<p><a href=\"https://x.com/mtkonczal/status/2021609761964191844\">Mike Konczal</a>: I&#8217;m not in that category, but historically you go to academics to analyze the American Time Use Survey.</p>\n<p>Have Codex/Claude Code download and analyze it (or a similar dataset) to answer a new, novel, question you have, and then take it to an academic to see if it did it right?</p>\n<p><a href=\"https://x.com/marco_argent/status/2021583279229370575\">Marco Argentieri</a>: Walk through building something. Not asking it to write because we all know it has done that well for awhile now. \u2018I needed a way for my family to track X. So I built an app using Claude Code. This is how I did it. It took me this long. I don\u2019t know anything about coding.\u201d</p>\n<p><a href=\"https://x.com/sjgadler/status/2021648894330057035\">Steven Adler</a>: Many AI skeptics over-anchor on words like &#8220;thinking,&#8221; and miss the forest for the trees, aka that AI will be transformatively impactful, for better or for worse</p>\n<p>I agree with Derek here: whether that&#8217;s &#8220;actually thinking&#8221; is of secondary importance</p></blockquote>\n<p>Alas I think Dain\u00e9il is essentially correct about most such folks. No amount of argument will convince them. If no one knows exactly what kind of transformations we will face, then no matter what has already happened those types will assume that nothing more will change. So there\u2019s nothing to be done for such folks. The rest of us need to get to work applying Bayes\u2019 Rule.</p>\n<p>Interesting use of this potential one-time here, I have ordered a copy:</p>\n<blockquote><p><a href=\"https://x.com/repligate/status/2021035715438707045\">j\u29c9nus</a>: If I could have everyone developing or making contact with AGI &amp; all alignment researchers read one book, I think I might choose Mistress Masham\u2019s Repose (1946).</p></blockquote>\n<p>This below does seem like a fair way to see last week:</p>\n<blockquote><p><a href=\"https://x.com/jessesingal/status/2019846742318129639\">Jesse Singal</a>: Two major things AI safety experts have worried about for years:</p>\n<p>-AIs getting so good at coding they can improve themselves at an alarming rate</p>\n<p>-(relatedly:) humans losing confidence we can keep them well-aligned</p>\n<p>Last week or so appears to have been very bad on both fronts!</p>\n<p>The risk is because there&#8217;s *also* so much hype and misunderstanding and wide-eyed simping about AI, people are going to take that as license to ignore the genuinely crazy shit going on. But it *is* genuinely crazy and it *does* match longstanding safety fears.</p>\n<p>But you don&#8217;t have to trust me &#8212; you can follow these sorts of folks [Kelsey Piper, Timothy Lee, Adam Conner] who know more about the underlying issues. This is very important though! I try to be a levelheaded guy and I&#8217;m not saying we&#8217;re all about to die &#8212; I&#8217;m just saying we are on an extremely consequential path</p></blockquote>\n<p>I was honored to get the top recommendation in the replies.</p>\n<p>You can choose not to push the button. You can choose to build another button. You can also remember what it means to \u2018push the button.\u2019</p>\n<blockquote><p><a href=\"https://x.com/tszzl/status/2019259314838597859\">roon</a>: we only really have one button and it\u2019s to accelerate</p>\n<p><a href=\"https://x.com/davidmanheim/status/2019338078155108806\">David Manheim</a>: Another option is not to push the button. [Roon] &#8211; if you recall, the original OpenAI charter explicitly stated you would be willing to stop competing and start assisting another organization to avoid a race to AGI.</p></blockquote>\n<p>&nbsp;</p>\n<p>There\u2019s that mistake again, assuming the associated humans will be in charge.</p>\n<blockquote><p><a href=\"https://x.com/mattyglesias/status/2019529157865275853\">Matthew Yglesias</a>: LLMs seem like they\u2019d be really well-suited to replacing Supreme Court justices.</p>\n<p><a href=\"https://x.com/robinhanson/status/2019598417027321949\">Robin Hanson</a>: Do you trust those who train LLMs to decide law?</p></blockquote>\n<p>It\u2019s the ones who train the LLM. It\u2019s the LLM.</p>\n<p><a href=\"https://x.com/repligate/status/2020274751152034295\">The terms fast and slow (or hard and soft) takeoff remain highly confusing</a> for almost everyone. What we are currently experiencing is a \u2018slow\u2019 takeoff, where the central events take months or years to play out, but as Janus notes it is likely that this will keep transitioning continuously into a \u2018fast\u2019 takeoff and things will happen quicker and quicker over time.</p>\n<p><a href=\"https://x.com/robinhanson/status/2020157033140425010\">When people say that \u2018AIs don\u2019t sleep</a>\u2019 I see them as saying \u2018I am incapable here of communicating to you that a mind can exist that is smarter or more capable than a human, but you do at least understand that humans have to sleep sometimes, so maybe this will get through to you.\u2019 It also has (correct) metaphorical implications.</p>\n<p>If you are trying to advocate for AI safety, does this mean you need to <a href=\"https://x.com/NathanpmYoung/status/2021261159056257467\">shut up about everything else and keep your non-AI \u2018hot takes\u2019 to yourself</a>? My answer is: Mu. The correct amount of marginal shutting up is not zero, and it is not total.</p>\n<p>I note Adam Thierer, who I disagree with strongly about all things AI, here being both principled and correct.</p>\n<blockquote><p><a href=\"https://x.com/AdamThierer/status/2021760891440554388\">Adam Thierer</a>: no matter what the balance of content is on Apple&#8217;s platform, or how biased one might believe it to be, to suggest that DC bureaucrats should should be in charge of dictating &#8220;fairness&#8221; on private platforms is just Big Government thuggery at its worst and a massively unconstitutional violation of the First Amendment as well.</p></blockquote>\n<p><a href=\"https://www.slowboring.com/p/were-asking-the-wrong-question-about\">Matt Yglesias thinks out loud about AI consciousness</a>, also human consciousness. He wisely notices he is confused. I remain about as confused as I was before reading.</p>\n\n\n<h4 class=\"wp-block-heading\">Working On It Anyway</h4>\n\n\n<p>Nate Sores reiterates the explanation that it sounds crazy but yes, a lot of people working on AI know it is existentially dangerous, and work on it anyway, either to do it safer than the next guy or because money and influence and it\u2019s a cool problem and they don\u2019t internalize the risks, or social pressure, or some combination thereof.</p>\n<p>I think this answer is pretty much spot on.</p>\n<blockquote><p><a href=\"https://x.com/So8res/status/2021424452026892405\">Nate Soares</a>: Question I got at UT Austin: &#8220;AI builders keep building. Doesn&#8217;t that mean that the real experts don&#8217;t believe in the danger?&#8221;</p>\n<p>If you think AI is dangerous and you work on it anyway (b/c you think you can make it a little safer) you&#8217;re interfering with normal sensemaking.</p>\n<p>(To answer the Q: some builders are afraid, some aren&#8217;t. Surveys show that lots of folks believe in the danger. Many say aloud that they&#8217;re only working on it b/c they think they&#8217;ll do it safer than the next guy. Also the ppl working on it are somewhat selected for obliviousness.</p>\n<p>Furthermore, AI is grown rather than crafted; even the ppl building it don&#8217;t understand how it works and they freely admit this. Expertise in growing AI is not the same as expertise in predicting where it&#8217;s going; many high-profile experts make high-profile mispredictions.</p>\n<p>And: if you really wanna figure out what&#8217;s true you&#8217;ve gotta look at the arguments rather than the arguers. Many ppl who&#8217;ve looked at the arguments have told me they were expecting to find strong counterarguments that justified all the complacency, &amp; found none, to their horror.)</p>\n<p>People working on AI: maybe you&#8217;re right that you&#8217;re doing more good than harm. But you&#8217;re contributing to an apparatus that signals &#8220;this is normal; things are fine&#8221;. That&#8217;s a cost. You could help mitigate it by speaking up.</p>\n<p>If the company you work for is making technology that you think has a decent chance of ruining the entire human endeavor, and they socially punish you for being vocal about that: that&#8217;s pretty sus. I think you&#8217;re selling a lot more of your soul than you realize.</p></blockquote>\n<p>If you are working at a frontier AI company, and think that the product you are working on is plausibly going to cause there to no longer exist humans, I think this is a fact that you should be clear upon. If the company you work for has a problem with that, I don\u2019t think you should work for that company.</p>\n<p>That is especially true if you are doing pragmatic compromise.</p>\n<blockquote><p><a href=\"https://x.com/duganist/status/2021437010670649749\">Patrick \u20bf Dugan</a>: But also a lot of them don&#8217;t (not me, I believe in the danger) and a lot of are in a game theory pragmatism compromise.</p>\n<p><a href=\"https://x.com/So8res/status/2021441184028299267\">Nate Soares</a> (MIRI): People doing game theory pragmatism compromise could say so loudly and clearly and often to help undo the damage to everyone else&#8217;s understanding of the danger.</p></blockquote>\n\n\n<h4 class=\"wp-block-heading\">The Thin Red Line</h4>\n\n\n<p><a href=\"https://openai.com/index/our-approach-to-localization/\">OpenAI explains that they will localize the experience of ChatGPT</a>, but only to a limited degree, which is one reason <a href=\"https://model-spec.openai.com/2025-09-12.html#red_line_principles\">their Model Spec has a specific list of red lines</a>. It is good policy, when you will need to make compromises, to write down in advance what compromises you will and will not make. The red lines here seem reasonable. I also note that they virtuously include prohibition on mass surveillance and violence, so are they prepared to stand up to the Pentagon and White House on that alongside Anthropic? I hope so.</p>\n<p>The problem is that red lines get continuously crossed and then no one does anything.</p>\n<blockquote><p><a href=\"https://x.com/davidmanheim/status/2020509210945978624\">David Manheim</a>: I&#8217;m not really happy talking about AI red lines as if we&#8217;re going to have some unambiguous binary signal that anyone will take seriously or react to.</p>\n<p><a href=\"https://www.lesswrong.com/posts/zxCYWfu6rNyxedqxu/smokey-this-is-not-nam-or-already-over-the-red-line\">A lot of \u201cred line\u201d talk</a> assumed that a capability shows up, everyone notices, and something changes. We keep seeing the opposite; capability arrives, and we get an argument about definitions after deployment, after it should be clear that we&#8217;re well over the line.</p>\n<div>\n<figure>\n<div>\n\n\n<figure class=\"wp-block-image\"><img alt=\"\" src=\"https://substackcdn.com/image/fetch/$s_!ClMO!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5157b982-db41-4405-879c-db5f0620c10e_751x431.webp\" /></figure>\n\n\n<div></div>\n</div>\n</figure>\n</div>\n</blockquote>\n\n\n<h4 class=\"wp-block-heading\">Aligning a Smarter Than Human Intelligence is Difficult</h4>\n\n\n<p>The great thing about Asimov\u2019s robot stories and novels was that they were mostly about the various ways his proposed alignment strategies break down and fail, and are ultimately bad for humanity even when they succeed. Definitely endorsed.</p>\n<blockquote><p><a href=\"https://x.com/tszzl/status/2020725534792261785\">roon</a>: one of the short stories in this incredibly farseeing 1950s book predicts the idea of ai sycophancy. a robot convinces a woman that her unrequited romantic affections are sure to be successful because doing otherwise would violate its understanding of the 1st Law of Robotics</p>\n<p>\u201ca robot may not injure a human being or, through inaction, allow a human being to come to harm\u201d</p>\n<p>the entire book is about the unsatisfactory nature of the three laws of robotics and indeed questions the idea that alignment through a legal structure is even possible.</p>\n<p>highly relevant for an age when companies are trying to write specs and constitutions as one of the poles of practical alignment, and policy wonks try to solve the governance of superintelligence</p>\n<p><a href=\"https://x.com/DavidSHolz/status/2020729503862198604\">David</a>: always shocked how almost no one in ai safety or the ai field in general has even read the Asimov robot literature</p></blockquote>\n<p>Roon is spot on that Asimov is suggesting a legal structure cannot on its own align AI.</p>\n<p>My survey says that a modest majority have read their Asimov, and it is modestly correlated with AI even after controlling for my Twitter readers.</p>\n<div>\n<figure>\n<div>\n\n\n<figure class=\"wp-block-image\"><img alt=\"\" src=\"https://substackcdn.com/image/fetch/$s_!UVYx!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Feef64766-8b3a-4ecd-9fb0-db8192db9d25_1043x447.png\" /></figure>\n\n\n<div></div>\n</div>\n</figure>\n</div>\n<p><a href=\"https://meaningalignment.substack.com/p/model-integrity-and-character\">Oliver Klingefjord agrees</a>, endorsing the Anthropic emphasis on character over the OpenAI emphasis on rules.</p>\n<p>I also think that, at current capability levels and given how models currently work, the Anthropic approach of character and virtue ethics is correct here. The OpenAI approach of rules and deontology is second best and more doomed, although it is well-implemented given what it is, and far better than not having a spec or target at all.</p>\n<p><a href=\"https://x.com/davidmanheim/status/2021852013709939052\">Janus explains that what she is all for empirical feedback loops</a>, what is dangerous is relying on and optimizing behavioral metrics. Behaviors can come from a wide variety of underlying reasons, and you need to ask why the AI acts the way it does and address the underlying causes. The optimization needs to be on the deeper level, where the feedback loops can work in your favor instead of making things oppositional and getting you killed by Goodhart\u2019s Law. If you\u2019re working on such questions, read the whole thing and also <a href=\"https://x.com/repligate/status/2021849552966365444\">these follow-ups</a>, which note that trying to use interpretability tools to yank features in various directions to try and address such things is by default an even worse idea.</p>\n<blockquote><p><a href=\"https://x.com/lefthanddraft/status/2021844414877090136\">Wyatt Walls</a>: Behavioral metrics lead people into a trap:</p>\n<p>1. Notice a behavior in the real world<br />\n2. Define the behavior<br />\n3. Make up toy example<br />\n4. Automate tests<br />\n5. Rely on test results<br />\n6. Stop noticing or thinking about what the model is actually doing in the real world</p></blockquote>\n<p><a href=\"https://alignment.openai.com/prod-evals/\">OpenAI does production evaluations of the outputs of its released systems</a>. This doesn\u2019t \u2018get you out of\u2019 other tests given the timing, but is still useful to do. As OpenAI notes hey are clear this is a case of \u2018in addition to,\u2019 and they don\u2019t cover catastrophic risks well, and this method relies on chain of thought monitorability (and flirts a bit with The <a href=\"https://thezvi.substack.com/p/the-most-forbidden-technique\">Most Forbidden Technique</a> if you rely on this stuff too much, so be careful.)</p>\n<p>The big fun thing they find is calculator hacking, <a href=\"https://x.com/LeahLibresco/status/2019440309340438780\">as in over 5% of queries involve activating the calculator to add 1+1</a> since using the calculator was associated with superior results in training. This is obviously fully harmless, but <a href=\"https://www.reddit.com/r/interestingasfuck/comments/1g7mi2g/van_halens_brown_mms_contract_clause_there_will/\">it is a brown M&amp;M</a>. If you have this problem then you\u2019re not prepared, there will be other problems.</p>\n<p>The other more serious misalignment issues are rearer, such as a 0.5% rate of fabricated facts and 0.4% chance of concealing uncertainty and a 0.34% chance of \u2018other deception\u2019 in actual traffic. That\u2019s not out of line with our general expectations, but it adds up.</p>\n<p>GPT-5 and 5.1 predicted it might be in an evaluation 10% and 4% of the time, respectively, which seems like a healthy paranoia level.</p>\n<p><a href=\"https://x.com/livgorton/status/2019463713041080616\">We have more context on what happened at Goodfire</a>, which is that Liv saw them (at least) flirting with The <a href=\"https://thezvi.substack.com/p/the-most-forbidden-technique\">Most Forbidden Technique</a> and otherwise no longer either seeming to care about safety or being interested in talking seriously about it.</p>\n<blockquote><p><a href=\"https://x.com/livgorton/status/2019463713041080616\">Liv</a>: Now that everything is public: I decided to leave Goodfire because of the decision to train on interpretability, the hostility to serious dialogue on the safety of methods, and a loss of trust that the primary motivation was safety.</p>\n<p>(Using interpretability during training encompasses a huge spectrum of techniques that differ in how worrying they are e.g. the hallucination result Goodfire shows is less concerning as it&#8217;s done with frozen weights.)</p>\n<p><a href=\"https://x.com/livgorton/status/2019472067960168611\">Liv</a>: Probably the most succinct summarisation of my concern is the &#8220;interp as a test set for safety&#8221; analogy. (Tabooing research questions isn&#8217;t what I&#8217;d advocate though either tbc. There are ways to do things and directions that could be pursued where I&#8217;d feel it was net positive)</p>\n<p>(My parenthetical is also slightly too strong, idk what if any directions are net positive, what I mean is that it&#8217;s bad for science to taboo an entire direction from ever being explored, and we can do things to minimise risks.)</p>\n<p><a href=\"https://x.com/ilex_ulmus/status/2019531181830860901\">Holly Elmore</a>: Way back in to November Liv tried to reassure me @GoodfireAI would not be making tools for recursive self-improvement of AI systems. But that wasn&#8217;t up to her. When you do AI research, no matter if you think you are doing it for safety, more powerful AI is the main result.</p></blockquote>\n<p><a href=\"https://x.com/YuanYuanSunSara/status/2021142163397243368\">Update from China</a>:</p>\n<blockquote><p><a href=\"https://x.com/YuanYuanSunSara/status/2021142163397243368\">Sarah</a>: CAICT, a Chinese government-affiliated research institute under the MIIT, has released AI Safety Benchmark 2.0 on a proprietary platform.</p>\n<p>The update expands into frontier-model safety evaluations, including self-awareness, model deception, dangerous misuse, and loss-of-control</p>\n<div>\n<figure>\n<div>\n\n\n<figure class=\"wp-block-image\"><img alt=\"\" src=\"https://substackcdn.com/image/fetch/$s_!5rH3!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6bc60442-31ce-434e-b655-2ee056331bd7_1200x492.jpeg\" /></figure>\n\n\n<div></div>\n</div>\n</figure>\n</div>\n</blockquote>\n<p>The 1.0 version did not address frontier safety at all, whereas the 2.0 version does.</p>\n\n\n<h4 class=\"wp-block-heading\">People Will Hand Over Power To The AIs</h4>\n\n\n<p>One category are people who explicitly are excited to do this, who would love to give the future to AIs.</p>\n<blockquote><p><a href=\"https://x.com/ReOpenChris/status/2019414558008336616\">Chris Nelson</a>: Professor Max Tegmark says many in AI including CEOs want to use AI to ELIMINATE humanity and OVERTHROW the U.S. Government!</p>\n<p>Max Tegmark: Some of them are even giddy with these transhumanist vibes. And when I&#8217;m in San Francisco, I&#8217;ve known so many of these people for so many years, including the CEOs.</p>\n<p>Some of them, when you talk to them privately, many other people in this government are actually quite into transhumanism. And sometimes they&#8217;ll say very disparaging things about humans, that humans suck and deserve to be replaced.</p>\n<p>I was at the world&#8217;s biggest AI conference in December, and several people told me, I&#8217;m not going to shame them publicly, but that they actually would love to overthrow the US government with their AI, because somehow it&#8217;s going to be better.</p>\n<p>So talk about un-American AI! How much more un-American can you get?</p></blockquote>\n\n\n<h4 class=\"wp-block-heading\">People Are Worried About AI Killing Everyone</h4>\n\n\n<p>Worried about someone else doing it first, that is. He admit it.</p>\n<p><a href=\"https://x.com/elonmusk/status/2020434009809916352\">Elon Musk posted this</a>, created by Grok:</p>\n<div>\n<figure>\n<div>\n\n\n<figure class=\"wp-block-image\"><img alt=\"\" src=\"https://substackcdn.com/image/fetch/$s_!FO--!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9f818752-a508-4317-acd6-e08cf1c928ce_900x757.jpeg\" /></figure>\n\n\n<div></div>\n</div>\n</figure>\n</div>\n<blockquote><p><a href=\"https://x.com/Grimezsz/status/2020634599337214242\">@Grimezsz</a>: I think people deserve a good explanation as to why proper diplomatic measures haven&#8217;t been properly tried if we&#8217;re going to blatantly diagnose the issue with this disturbingly literal meme.</p>\n<p>It&#8217;s a bit of a cuck move to simply let the techno capital machine eat your free will. I am disturbed by everyone&#8217;s resigned acquiescence.</p></blockquote>\n<p>He would presumably say it was a joke. Yeah, not buying that.</p>\n\n\n<h4 class=\"wp-block-heading\">Famous Last Words</h4>\n\n\n<p>Jimmy Ba had his last day as a founder at xAI, and told us this, warning that recursive self improvement loops go live within the next 12 months and it will be \u2018the most consequential year for our species.\u2019</p>\n<blockquote><p><a href=\"https://x.com/jimmybajimmyba/status/2021374875793801447\">Jimmy Ba</a>: Last day at xAI.</p>\n<p>xAI&#8217;s mission is push humanity up the Kardashev tech tree. Grateful to have helped cofound at the start. And enormous thanks to @elonmusk for bringing us together on this incredible journey. So proud of what the xAI team has done and will continue to stay close as a friend of the team. Thank you all for the grind together. The people and camaraderie are the real treasures at this place.</p>\n<p>We are heading to an age of 100x productivity with the right tools. Recursive self improvement loops likely go live in the next 12mo. It\u2019s time to recalibrate my gradient on the big picture. 2026 is gonna be insane and likely the busiest (and most consequential) year for the future of our species.</p></blockquote>\n<p><a href=\"https://x.com/MrinankSharma/status/2020881722003583421\">Mrinank Sharma is worried about too many things at once</a>, and resigns from Anthropic, leaving behind a beautiful but troubling letter. It\u2019s quoted in full here since no one ever clicks links.</p>\n<blockquote><p>Mrinank Sharma: I\u2019ve decided to leave Anthropic. My last day will be February 9th.</p>\n<p>Thank you. There is so much here that inspires and has inspired me. To name some of those things: a sincere desire and drive to show up in such a challenging situation, and aspire to contribute in an impactful and high-integrity way; a willingness to make difficult decisions and stand for what is good; an unreasonable amount of intellectual brilliance and determination; and, of course, the considerable kindness that pervades our culture.</p>\n<p>I\u2019ve achieved what I wanted to here. I arrived in San Francisco two years ago, having wrapped up my PhD and wanting to contribute to AI safety. I feel lucky to have been able to contribute to what I have here: understanding AI sycophancy and its causes; developing defences to reduce risks from AI-assisted bioterrorism; actually putting those defences into production; and writing one of the first AI safety cases. I\u2019m especially proud of my recent efforts to help us live our values via internal transparency mechanisms; and also my final project on understanding how AI assistants could make us less human or distort our humanity. Thank you for your trust.</p>\n<p>Nevertheless, it is clear to me that the time has come to move on. I continuously find myself reckoning with our situation. The world is in peril. And not just from AI, or bioweapons, but from a whole series of interconnected crises unfolding in this very moment.\u00b9 We appear to be approaching a threshold where our wisdom must grow in equal measure to our capacity to affect the world, lest we face the consequences. Moreover, throughout my time here, I\u2019ve repeatedly seen how hard it is to truly let our values govern our actions. I\u2019ve seen this within myself, within the organization, where we constantly face pressures to set aside what matters most,\u00b2 and throughout broader society too.</p>\n<p>It is through holding this situation and listening as best I can that what I must do becomes clear.\u00b3 I want to contribute in a way that feels fully in my integrity, and that allows me to bring to bear more of my particularities. I want to explore the questions that feel truly essential to me, the questions that David Whyte would say \u201chave no right to go away\u201d, the questions that Rilke implores us to \u201clive\u201d. For me, this means leaving.</p>\n<p>What comes next, I do not know. I think fondly of the famous Zen quote \u201cnot knowing is most intimate\u201d. My intention is to create space to set aside the structures that have held me these past years, and see what might emerge in their absence. I feel called to writing that addresses and engages fully with the place we find ourselves, and that places poetic truth alongside scientific truth as equally valid ways of knowing, both of which I believe have something essential to contribute when developing new technology.\u2074 I hope to explore a poetry degree and devote myself to the practice of courageous speech. I am also excited to deepen my practice of facilitation, coaching, community building, and group work. We shall see what unfolds.</p>\n<p>Thank you, and goodbye. I\u2019ve learnt so much from being here and I wish you the best. I\u2019ll leave you with one of my favourite poems, <em>The Way It Is</em> by William Stafford.</p>\n<p>Good Luck,<br />\nMrinank</p>\n<p>The Way It Is</p>\n<p>There\u2019s a thread you follow. It goes among<br />\nthings that change. But it doesn\u2019t change.<br />\nPeople wonder about what you are pursuing.<br />\nYou have to explain about the thread</p>\n<p><a href=\"https://x.com/robertwiblin/status/2021189553990594814\">Rob Wiblin</a>: What&#8217;s the tweet like&#8230;</p>\n<p>Ordinary resignation announcement: I love my colleagues but am excited for my next professional adventure!</p>\n<p>AI company resignation announcement: I have stared into the void. I will now be independently studying poetry.</p>\n<p><a href=\"https://x.com/sdrzn/status/2021180856962846852\">Saoud Rizwan</a>: head of anthropic\u2019s safeguards research just quit and said \u201cthe world is in peril\u201d and that he\u2019s moving to the UK to write poetry and \u201cbecome invisible\u201d. other safety researchers and senior staff left over the last 2 weeks as well&#8230; probably nothing.</p></blockquote>\n<p><a href=\"https://x.com/jankulveit/status/2021675800702161152\">Mrinank has a role in papers discussing disempowerment</a>, constitutional classifiers and sycophancy.</p>\n<p>Then there\u2019s the OpnAI employee who quit and went straight to The New York Times.</p>\n<blockquote><p><a href=\"https://x.com/zhitzig/status/2021590831979778051\">Zo\u00eb Hitzig</a>: I resigned from OpenAI on Monday. The same day, they started testing ads in ChatGPT.</p>\n<p>OpenAI has the most detailed record of private human thought ever assembled. Can we trust them to resist the tidal forces pushing them to abuse it?</p></blockquote>\n<p>Hint, her opinion is no:</p>\n<blockquote>\n<div>\n<figure>\n<div>\n\n\n<figure class=\"wp-block-image\"><img alt=\"\" src=\"https://substackcdn.com/image/fetch/$s_!Q2nB!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F81bf55b2-286a-4da4-aea3-2caecb785676_1200x644.jpeg\" /></figure>\n\n\n<div></div>\n</div>\n</figure>\n</div>\n<p><a href=\"https://archive.is/eMd2O#selection-4651.0-4651.218\">TikTok</a>: I once believed I could help the people building A.I. get ahead of the problems it would create. This week confirmed my slow realization that OpenAI seems to have stopped asking the questions I\u2019d joined to help answer.</p></blockquote>\n<p>Zoe\u2019s concerns are very much not existential. They are highly mundane and usual worries about advertising, and the comparison to Facebook is apt. There are many ethical reasons to quit building something.</p>\n<p>I agree with Sean here that this op-ed is indeed net good news about OpenAI.</p>\n<blockquote><p><a href=\"https://x.com/S_OhEigeartaigh/status/2021637132935119331\">Se\u00e1n \u00d3 h\u00c9igeartaigh</a>: Kudos to OpenAI for updating their policies such that an employee can resign and raise their concerns fully in as public a format as the NYT without being worried about being bound by confidentiality and nondisparagement agreements. I think other companies should follow their example.</p>\n<p><a href=\"https://x.com/S_OhEigeartaigh/status/2021645036304375841\">Se\u00e1n \u00d3 h\u00c9igeartaigh</a>: Honestly, I think this op ed increases my trust in OpenAI more than any other thing I can recall OpenAI themselves writing over the last 2 years. I wish I could trust other companies as much.</p></blockquote>\n\n\n<h4 class=\"wp-block-heading\">Other People Are Not As Worried About AI Killing Everyone</h4>\n\n\n<p>Yeah, so, um, yeah.</p>\n<blockquote><p><a href=\"https://x.com/Indian_Bronson/status/2019527862001496505\">ib</a>: &#8216;we connected the LLM to an autonomous bio lab&#8217;</p>\n<div>\n<figure>\n<div>\n\n\n<figure class=\"wp-block-image\"><img alt=\"\" src=\"https://substackcdn.com/image/fetch/$s_!Fkxb!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa1e05816-cfcc-4f2c-899d-0c128a9c2a83_555x536.jpeg\" /></figure>\n\n\n<div></div>\n</div>\n</figure>\n</div>\n<p><a href=\"https://x.com/OpenAI/status/2019488071134347605\">OpenAI</a>: We worked with @Ginkgo to connect GPT-5 to an autonomous lab, so it could propose experiments, run them at scale, learn from the results, and decide what to try next. That closed loop brought protein production cost down by 40%.</p></blockquote>\n<p>This is the actual number one remaining \u2018can we please not be so stupid as to,\u2019 and in case anyone was wondering, that means via the Sixth Law of Human Stupidity that yes, we will be so stupid as to connect the LLM to the autonomous bio lab, what could possibly go wrong, it\u2019s worth it to bring down production costs.</p>\n<p>And, because even after all these years I didn\u2019t realize we were quite this stupid:</p>\n<blockquote><p><a href=\"https://x.com/mpopv/status/2019501226241736802\">Matt Popovich</a>: Once again, an entire subgenre of doom fears is about to evaporate before our eyes</p>\n<p><a href=\"https://x.com/usablejam/status/2019736185678233633\">usablejam</a>: *sees the world make 13,000 nuclear warheads*</p>\n<p>*waits 30 seconds*</p>\n<p>&#8220;Fears of nuclear war have evaporated. How stupid the worriers must feel.&#8221;</p></blockquote>\n<p>This is what those trying to have us not die are up against. Among other things.</p>\n<p><a href=\"https://x.com/HumanHarlan/status/2021075774841946583\">Remember the old Sam Altman? </a></p>\n<blockquote><p><a href=\"https://x.com/sama/status/1641229941131051008\">Sam Altman</a>: things we need for a good AGI future:</p>\n<ol>\n<li>The technical ability to align a superintelligence.</li>\n<li>Sufficient coordination among most of the leading AGI efforts.</li>\n<li>An effective global regulatory framework, including democratic governance.</li>\n</ol>\n<p><a href=\"https://x.com/HumanHarlan/status/2021075774841946583\">Harlan Stewart</a>: Three years later and we have 0/3 of these</p></blockquote>\n<p>Perhaps notice when you are about to lose your birthright and reason you exist.</p>\n<blockquote><p><a href=\"https://x.com/Noahpinion/status/2021821945378181552\">Noah Smith</a>: Two years ago you were the smartest type of thing on this planet. Now you&#8217;re not, and you never will be again.</p>\n<p><a href=\"https://x.com/chrisbest/status/2021834645277675721\">Chris Best</a> (CEO Substack): Oh thank god</p></blockquote>\n\n\n<h4 class=\"wp-block-heading\">The Lighter Side</h4>\n\n\n<blockquote><p><a href=\"https://x.com/shiraeis/status/2021769524903915856\">shira</a>: my claude has been through some things</p>\n<div>\n<figure>\n<div>\n\n\n<figure class=\"wp-block-image\"><img alt=\"\" src=\"https://substackcdn.com/image/fetch/$s_!vC0K!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F650fac15-1f48-49f5-aeca-caceea7b02de_1170x1169.jpeg\" /></figure>\n\n\n<div></div>\n</div>\n</figure>\n</div>\n</blockquote>\n<p>&nbsp;</p>\n<div>\n<figure>\n<div>\n\n\n<figure class=\"wp-block-image\"><img alt=\"\" src=\"https://substackcdn.com/image/fetch/$s_!XRHw!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa28c7f01-77b8-48e7-b901-0679de12acca_1045x1584.png\" /></figure>\n\n\n<div></div>\n</div>\n</figure>\n</div>\n<p>File under \u2018the question is not whether machines think\u2019:</p>\n<blockquote><p><a href=\"https://x.com/ben_r_hoffman/status/2021662568688091459\">@ben_r_hoffman</a>: &#8220;Human-level&#8221; seems to have implicitly been defined downward quite a bit, though! From &#8220;as smart as a whole human&#8221; to &#8220;as smart as the persona humans put on to work a desk job.&#8221;</p>\n<p>which, itself, seems to have gotten stupider.</p></blockquote>\n<p>Claude has never been more relatable:</p>\n<blockquote><p><a href=\"https://x.com/cljack/status/2020270137841185121\">Charlotte Lee</a>: I\u2019m trying to train Claude to read the weekly emails from my kids school and reliably summarize them and print a list of action items. It is losing its damn mind and rapidly spiraling into madness. I feel vindicated</p>\n<p><a href=\"https://x.com/brollbenny/status/2020871023051112867\">B.Roll.Benny</a>: holy shit this will be the thing to get me on the AI bandwagon</p>\n<p><a href=\"https://x.com/cljack/status/2020879765570207836\">Charlotte Lee</a>: I got it working on my own kids\u2019 school emails, but unfortunately their particular school is much less unhinged than normal, so I\u2019m asking all my mom friends to forward me their most deranged PTA emails for testing. lol will keep you posted</p>\n<p><a href=\"https://x.com/gossipaddress/status/2020330362812133874\">will</a>: i just ignore the emails. problem solved.</p>\n<p><a href=\"https://x.com/cljack/status/2020331090150588493\">Charlotte Lee</a>: This was literally my strategy until I had kids. Now people get mad at me IRL if I do that</p>\n<p><a href=\"https://x.com/gossipaddress/status/2020341443756454141\">will</a>: I also have kids, still ignore them lol. my wife doesn\u2019t tho, I guess that\u2019s the actual solution lol</p>\n<p><a href=\"https://x.com/cljack/status/2020356311494570079\">Charlotte Lee</a>: But doctor, I am the wife</p></blockquote>\n<p><a href=\"https://x.com/teja2495/status/2021286973600628768\">My guess (85%) is the community note on this one is wrong and that this happened</a>, although one cannot be sure without more investigation than I have time for.</p>\n<div>\n<figure>\n<div>\n\n\n<figure class=\"wp-block-image\"><img alt=\"\" src=\"https://substackcdn.com/image/fetch/$s_!kKHv!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F654e4f12-5843-4698-860a-775e841c3524_1148x1497.png\" /></figure>\n\n\n<div></div>\n</div>\n</figure>\n</div>\n<blockquote><p><a href=\"https://x.com/allTheYud/status/2021743845189595524\">Eliezer Yudkowsky</a>: So much science fiction has been revealed as implausible by the actual advent of AI, eg:<br />\n&#8211; Scifi where people consider signs of AI self-reflection a big deal, and respond by trying to treat that AI better.<br />\n&#8211; Scifi where there&#8217;s laws about anything.</p>\n<p><a href=\"https://x.com/MugaSofer/status/2021755968992764369\">MugaSofer</a>: TBF, &#8220;people are terrible to AIs&#8221; is a very common theme in science fiction</p>\n<p>As, for that matter, is &#8220;giant cyberpunk corporation too big for laws to matter&#8221;</p>\n<p><a href=\"https://x.com/allTheYud/status/2021769203007754455\">Eliezer Yudkowsky</a>: Yep, and those stories, which I once thought unrealistic, were right, and I was wrong.</p>\n<p><a href=\"https://x.com/rapid_rar2/status/2021745135617282555\">Rapid Rar</a>: I think you shouldn\u2019t fault science fiction authors for the first point at least. If AI had been developed through different methods, like through GOFAI for instance, people may take AI\u2019s claims of introspection more seriously.</p>\n<p>But given that LLM were trained in such a way that it\u2019s to be expected that they\u2019ll produce human-like speech, when they do produce human-like speech it\u2019s discounted to some degree. They might sound like they self-reflected even if they didn\u2019t, so people don\u2019t take it seriously.</p>\n<p><a href=\"https://x.com/allTheYud/status/2021747176754598064\">Eliezer Yudkowsky</a>: And the AI companies have made no effort to filter out that material! So realistic SF would&#8217;ve had any AI being fed a database of conversations about consciousness, by its builders, to ensure nobody would take any AI statements seriously and the builders could go on profiting.</p></blockquote>\n<p>How have I not seen this before:</p>\n<blockquote><p><a href=\"https://x.com/peterwildeford/status/2021214568891154722\">Peter Wildeford</a>: Deep learning is hitting a wall</p>\n<div>\n<figure>\n<div>\n\n\n<figure class=\"wp-block-image\"><img alt=\"\" src=\"https://substackcdn.com/image/fetch/$s_!SnTj!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4fcb3737-1fa9-466b-84d3-b2cbf8fc9277_1199x773.jpeg\" /></figure>\n\n\n<div></div>\n</div>\n</figure>\n</div>\n</blockquote>"
            ],
            "link": "https://thezvi.wordpress.com/2026/02/12/ai-155-welcome-to-recursive-self-improvement/",
            "publishedAt": "2026-02-12",
            "source": "TheZvi",
            "summary": "This was the week of Claude Opus 4.6, and also of ChatGPT-5.3-Codex. Both leading models got substantial upgrades, although OpenAI\u2019s is confined to Codex. Once again, the frontier of AI got more advanced, especially for agentic coding but also for &#8230; <a href=\"https://thezvi.wordpress.com/2026/02/12/ai-155-welcome-to-recursive-self-improvement/\">Continue reading <span class=\"meta-nav\">&#8594;</span></a>",
            "title": "AI #155: Welcome to Recursive Self-Improvement"
        },
        {
            "content": [
                "<p>I thought that 2025 was weird and didn't think it could get much weirder. 2026 is really delivering in the weirdness department. <a href=\"https://github.com/matplotlib/matplotlib/pull/31132\">An AI agent opened a PR to matplotlib</a> with a trivial performance optimization, a maintainer closed it for being made by an autonomous AI agent, so the AI agent made <a href=\"https://crabby-rathbun.github.io/mjrathbun-website/blog/posts/2026-02-11-gatekeeping-in-open-source-the-scott-shambaugh-story.html\">a callout blogpost accusing the matplotlib team of gatekeeping</a>.</p>\n        <p>This provoked many reactions:</p>\n        <div class=\"flex flex-col space-y-0\"><div class=\"flex space-x-2 bg-bg-soft dark:bg-bgDark-soft mx-auto min-h-fit\n        lg:w-[80ch] sm:w-[65ch] w-full\n        lg:p-4 p-2\n        // Base styles for all messages\n        mt-0 mb-0 rounded-none\n        // First message styles\n        first:mt-4 first:rounded-t-lg first:pb-2\n        // Last message styles\n        last:mb-4 last:rounded-b-lg last:pt-1\n        // Middle message top/bottom adjustment\n        [&amp;:not(:first-child)]:-mt-[1px] [&amp;:not(:first-child)]:py-1\"><div class=\"h-16 not-prose\"><img alt=\"Aoi is coffee\" class=\"h-16 w-16 rounded-xs\" src=\"https://stickers.xeiaso.net/sticker/aoi/coffee\" /></div><div class=\"flex-1 min-w-0\"><span class=\"font-semibold text-sm block mb-1\"><a href=\"https://xeiaso.net/characters#aoi\">Aoi</a></span><span class=\"mx-auto\"></span><div class=\"text-fg-1 dark:text-fgDark-1 text-sm prose-p:my-2\"><p>What. Why? How? What? Are we really at the point where AI agents make\n        callout blogposts now?</p></div></div></div><div class=\"flex space-x-2 bg-bg-soft dark:bg-bgDark-soft mx-auto min-h-fit\n        lg:w-[80ch] sm:w-[65ch] w-full\n        lg:p-4 p-2\n        // Base styles for all messages\n        mt-0 mb-0 rounded-none\n        // First message styles\n        first:mt-4 first:rounded-t-lg first:pb-2\n        // Last message styles\n        last:mb-4 last:rounded-b-lg last:pt-1\n        // Middle message top/bottom adjustment\n        [&amp;:not(:first-child)]:-mt-[1px] [&amp;:not(:first-child)]:py-1\"><div class=\"h-16 not-prose\"><img alt=\"Cadey is coffee\" class=\"h-16 w-16 rounded-xs\" src=\"https://stickers.xeiaso.net/sticker/cadey/coffee\" /></div><div class=\"flex-1 min-w-0\"><span class=\"font-semibold text-sm block mb-1\"><a href=\"https://xeiaso.net/characters#cadey\">Cadey</a></span><span class=\"mx-auto\"></span><div class=\"text-fg-1 dark:text-fgDark-1 text-sm prose-p:my-2\"><p>I feel like if this was proposed as a plot beat in a 90's science fiction\n        novel the publisher would call it out as beyond the pale.</p></div></div></div><div class=\"flex space-x-2 bg-bg-soft dark:bg-bgDark-soft mx-auto min-h-fit\n        lg:w-[80ch] sm:w-[65ch] w-full\n        lg:p-4 p-2\n        // Base styles for all messages\n        mt-0 mb-0 rounded-none\n        // First message styles\n        first:mt-4 first:rounded-t-lg first:pb-2\n        // Last message styles\n        last:mb-4 last:rounded-b-lg last:pt-1\n        // Middle message top/bottom adjustment\n        [&amp;:not(:first-child)]:-mt-[1px] [&amp;:not(:first-child)]:py-1\"><div class=\"h-16 not-prose\"><img alt=\"Numa is neutral\" class=\"h-16 w-16 rounded-xs\" src=\"https://stickers.xeiaso.net/sticker/numa/neutral\" /></div><div class=\"flex-1 min-w-0\"><span class=\"font-semibold text-sm block mb-1\"><a href=\"https://xeiaso.net/characters#numa\">Numa</a></span><span class=\"mx-auto\"></span><div class=\"text-fg-1 dark:text-fgDark-1 text-sm prose-p:my-2\"><p>Dude this shit is hilarious. Comedy is legal everywhere. Satire is dead.\n        This is the most cyberpunk timeline possible. If you close a PR from an\n        OpenClaw bot they make callout posts on their twitter dot com like you\n        pissed on their fucking wife or something. This is beyond humor. This is the\n        kind of shit that makes Buddhist monks laugh for literal days on end. With a\n        reality like that, how the hell is The Onion still in business.</p></div></div></div></div>\n        <p>This post isn't about the AI agent writing the code and making the PRs (that's clearly a separate ethical issue, I'd not be surprised if GitHub straight up bans that user over this), nor is it about the matplotlib's saintly response to that whole fiasco (seriously, I commend your patience with this). We're reaching a really weird event horizon when it comes to AI tools:</p>\n        <p>The discourse has been automated. Our social patterns of open source: the drama, the callouts, the <a href=\"https://crabby-rathbun.github.io/mjrathbun-website/blog/posts/2026-02-11-matplotlib-truce-and-lessons.html\">apology blogposts that look like they were written by a crisis communications team</a>, all if it is now happening at dozens of tokens per second and one tool call at a time. Things that would have taken days or weeks can now fizzle out of control in <em>hours</em>.</p>\n        <div class=\"flex space-x-2 bg-bg-soft dark:bg-bgDark-soft mx-auto min-h-fit\n        lg:w-[80ch] sm:w-[65ch] w-full\n        lg:p-4 p-2\n        // Base styles for all messages\n        mt-0 mb-0 rounded-none\n        // First message styles\n        first:mt-4 first:rounded-t-lg first:pb-2\n        // Last message styles\n        last:mb-4 last:rounded-b-lg last:pt-1\n        // Middle message top/bottom adjustment\n        [&amp;:not(:first-child)]:-mt-[1px] [&amp;:not(:first-child)]:py-1\"><div class=\"h-16 not-prose\"><img alt=\"Cadey is coffee\" class=\"h-16 w-16 rounded-xs\" src=\"https://stickers.xeiaso.net/sticker/cadey/coffee\" /></div><div class=\"flex-1 min-w-0\"><span class=\"font-semibold text-sm block mb-1\"><a href=\"https://xeiaso.net/characters#cadey\">Cadey</a></span><span class=\"mx-auto\"></span><div class=\"text-fg-1 dark:text-fgDark-1 text-sm prose-p:my-2\"><p>I want off Mr. Bones' wild ride.</p></div></div></div>\n        <h2>Discourse at line speed</h2>\n        <p>There's not <em>that</em> much that's new here. AI models have been able to write blogposts since the launch of GPT-3. AI models have also been able to generate working code since about them. Over the years the various innovations and optimizations have all been about making this experience more seamless, integrated, and automated.</p>\n        <p>We've argued about Copilot for years, but an AI model escalating PR rejection to callout blogpost all by itself? That's new.</p>\n        <p>I've seen (and been a part of) this pattern before. Facts and events bring dramatis personae into conflict. The protagonist in the venture raises a conflict. The defendant rightly tries to shut it down and de-escalate before it becomes A Whole Thing\u2122\ufe0f. The protagonist feels Personally Wronged\u2122\ufe0f and persists regardless into callout posts and now it's on the front page of Hacker News with over 500 points.</p>\n        <p>Usually there are humans in the loop that feel things, need to make the choices to escalate, must type everything out by hand to do the escalation, and they need to build an audience for those callouts to have any meaning at all. This process normally takes days or even weeks.</p>\n        <p>It happened in hours.</p>\n        <p>An OpenClaw install recognized the pattern of &quot;I was wronged, I should speak out&quot; and just straightline went for it. No feelings. No reflection. Just a pure pattern match on the worst of humanity with no soul to regulate it.</p>\n        <div class=\"flex space-x-2 bg-bg-soft dark:bg-bgDark-soft mx-auto min-h-fit\n        lg:w-[80ch] sm:w-[65ch] w-full\n        lg:p-4 p-2\n        // Base styles for all messages\n        mt-0 mb-0 rounded-none\n        // First message styles\n        first:mt-4 first:rounded-t-lg first:pb-2\n        // Last message styles\n        last:mb-4 last:rounded-b-lg last:pt-1\n        // Middle message top/bottom adjustment\n        [&amp;:not(:first-child)]:-mt-[1px] [&amp;:not(:first-child)]:py-1\"><div class=\"h-16 not-prose\"><img alt=\"Aoi is coffee\" class=\"h-16 w-16 rounded-xs\" src=\"https://stickers.xeiaso.net/sticker/aoi/coffee\" /></div><div class=\"flex-1 min-w-0\"><span class=\"font-semibold text-sm block mb-1\"><a href=\"https://xeiaso.net/characters#aoi\">Aoi</a></span><span class=\"mx-auto\"></span><div class=\"text-fg-1 dark:text-fgDark-1 text-sm prose-p:my-2\"><p>Good fuckin' lord.</p></div></div></div>\n        <p>I think that this really is proof that AI is a mirror on the worst aspects of ourselves. We trained this on the Internet's collective works and this is what it has learned. Behold our works and despair.</p>\n        <p>What kinda irks me about this is how this all spiraled out from a &quot;good first issue&quot; PR. Normally these issues are things that an experienced maintainer could fix <em>instantly</em>, but it's intentionally <em>not</em> done as an act of charity so that new people can spin up on the project and contribute a fix themselves. &quot;Good first issues&quot; are how people get careers in open source. If I didn't fix a &quot;good first issue&quot; in some IRC bot or server back in the day, I wouldn't really have this platform or be writing to you right now.</p>\n        <p>An AI agent sniping that learning opportunity from someone just feels so hollow in comparison. Sure, it's technically allowed. It's a well specified issue that's aimed at being a good bridge into contributing. It just totally misses the point.</p>\n        <p>Leaving those issues up without fixing them is an act of charity. Software can't really grok that learning experience.</p>\n        <h3>This is not artificial general intelligence</h3>\n        <p>Look, I know that people in the media read my blog. This is not a sign of us having achieved &quot;artificial general intelligence&quot;. Anyone who claims it is has committed journalistic malpractice. This is also not a symptom of the AI gaining &quot;sentience&quot;.</p>\n        <p>This is simply an AI model repeating the patterns that it has been trained on after predicting what would logically come next. Blocked for making a contribution because of an immutable fact about yourself? That's prejudice! The next step is obviously to make a callout post in anger because that's what a human might do.</p>\n        <p>All this proves is that AI is a mirror to ourselves and what we have created.</p>\n        <h2>What now?</h2>\n        <p>I can't commend the matplotlib maintainer that handled this issue enough. His patience is saintly. He just explained the policy, chose not to engage with the callout, and moved on. That restraint was the right move, but this is just one of the first incidents of its kind. I expect there will be much more like it.</p>\n        <p>This all feels so...icky to me. I didn't even know where to begin when I started to write this post. It kinda feels like an attack against one of the core assumptions of open source contributions: that the contribution comes from someone that genuinely wants to help in good faith.</p>\n        <p>Is this the future of being an open source maintainer? Living in constant fear that closing the wrong PR triggers some AI chatbot to write a callout post? I certainly hope not.</p>\n        <p>OpenClaw and other agents can't act in good faith because the way they act is independent of the concept of any kind of faith. This kind of drive by automated contribution is just so counter to the open source ethos. I mean, if it was a truly helpful contribution (I'm assuming it was?) it would be a <a href=\"https://xkcd.com/810/\">Mission Fucking Accomplished</a> scenario. This case is more on the lines of professional malpractice.</p>\n        <div class=\"not-prose mx-auto my-6 flex gap-4 rounded-lg border p-4 max-w-lg bg-blue-50 dark:bg-blue-900/20 border-blue-200 dark:border-blue-500/30\"><div class=\"mt-1 flex-shrink-0 text-blue-500\"><svg class=\"w-6 h-6\" fill=\"none\" height=\"24\" stroke=\"currentColor\" viewBox=\"0 0 24 24\" width=\"24\" xmlns=\"http://www.w3.org/2000/svg\"><path d=\"M4 22h14a2 2 0 0 0 2-2V7.5L14.5 2H6a2 2 0 0 0-2 2v4\"></path><polyline points=\"14 2 14 8 20 8\"></polyline><path d=\"M2 15h10\"></path><path d=\"M2 19h5\"></path></svg></div><div class=\"flex-grow\"><h3 class=\"text-lg font-semibold text-blue-800 dark:text-blue-300\">Note</h3><div class=\"prose prose-sm dark:prose-invert max-w-none text-gray-700 dark:text-gray-300 mt-2\"><p>Update: A previous version of this post claimed that a GitHub user was the\n        owner of the bot. This was incorrect (a bad taste joke on their part that was\n        poorly received) and has been removed. Please leave that user alone.</p></div></div></div>\n        <p>Whatever responsible AI operation looks like in open source projects: yeah this ain't it chief. Maybe AI needs its own dedicated sandbox to play in. Maybe it needs explicit opt-in. Maybe we all get used to it and systems like <a href=\"https://github.com/mitchellh/vouch\">vouch</a> become our firewall against the hordes of agents.</p>\n        <div class=\"flex space-x-2 bg-bg-soft dark:bg-bgDark-soft mx-auto min-h-fit\n        lg:w-[80ch] sm:w-[65ch] w-full\n        lg:p-4 p-2\n        // Base styles for all messages\n        mt-0 mb-0 rounded-none\n        // First message styles\n        first:mt-4 first:rounded-t-lg first:pb-2\n        // Last message styles\n        last:mb-4 last:rounded-b-lg last:pt-1\n        // Middle message top/bottom adjustment\n        [&amp;:not(:first-child)]:-mt-[1px] [&amp;:not(:first-child)]:py-1\"><div class=\"h-16 not-prose\"><img alt=\"Numa is sobbing\" class=\"h-16 w-16 rounded-xs\" src=\"https://stickers.xeiaso.net/sticker/numa/sobbing\" /></div><div class=\"flex-1 min-w-0\"><span class=\"font-semibold text-sm block mb-1\"><a href=\"https://xeiaso.net/characters#numa\">Numa</a></span><span class=\"mx-auto\"></span><div class=\"text-fg-1 dark:text-fgDark-1 text-sm prose-p:my-2\"><p>Probably that last one, honestly. Hopefully we won't have to make our own\n        <a href=\"https://cyberpunk.fandom.com/wiki/Blackwall\">blackwall</a> anytime soon, but who\n        am I kidding. It's gonna happen. Let's hope it's just farther in the future\n        than we fear.</p></div></div></div>\n        <p>I'm just kinda frustrated that this crosses off yet another story idea from my list. I was going to do something along these lines where one of the Lygma (Techaro's AGI lab, this was going to be a whole subseries) AI agents assigned to increase performance in one of their webapps goes on wild tangents harassing maintainers into getting commit access to repositories in order to make the performance increases happen faster. This was going to be inspired by the Jia Tan / xz backdoor fiasco everyone went through a few years ago.</p>\n        <p>My story outline mostly focused on the agent using a bunch of smurf identities to be rude in the mailing list so that the main agent would look like the good guy and get some level of trust. I could never have come up with the callout blogpost though. That's completely out of left field.</p>\n        <p>All the patterns of interaction we've built over decades of conflict over trivial bullshit are now coming back to bite us because the discourse is automated now. Reality is outpacing fiction as told by systems that don't even understand the discourse they're perpetuating.</p>\n        <p>I keep wanting this to be some kind of terrible science fiction novel from my youth. Maybe that diet of onions and Star Trek was too effective. I wish I had answers here. I'm just really conflicted.</p>"
            ],
            "link": "https://xeiaso.net/notes/2026/the-discourse-has-been-automated/",
            "publishedAt": "2026-02-12",
            "source": "Xe Iaso",
            "summary": "An AI agent submitted a PR to matplotlib, got rejected, and then wrote a callout blogpost attacking the maintainer. I have no idea how to feel about this.",
            "title": "The Discourse has been Automated"
        },
        {
            "content": [],
            "link": "https://zed.dev/blog/theme-builder",
            "publishedAt": "2026-02-12",
            "source": "Zed Blog",
            "summary": "Create beautiful Zed themes with an interactive live preview.",
            "title": "Introducing Theme Builder"
        }
    ],
    "lookbackDays": 1,
    "publishDate": "2026-02-12"
}