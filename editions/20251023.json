{
    "articles": [
        {
            "content": [
                "<p>I recently <a href=\"https://dynomight.net/explainers/\">wondered</a> if explainer posts might go extinct. In response, you all assured me that I have nothing to worry about, because you <em>already</em> don\u2019t care about my explanations\u2014you just like it when I point at stuff.</p>\n\n<p>Well OK then!</p>\n\n<h2 id=\"pointing-machines\">Pointing machines</h2>\n\n<p>How did Michelangelo make <a href=\"https://en.wikipedia.org/wiki/David_(Michelangelo)\">this</a>?</p>\n\n<p><img alt=\"david\" src=\"https://dynomight.net/img/shorts-6/david.jpg\" /></p>\n\n<p>What I mean is\u2014marble is unforgiving. If you accidentally remove some material, it\u2019s gone. You can\u2019t fix it by adding another layer of paint. Did Michelangelo somehow plan everything out in advance and then execute everything perfectly the first time, with no mistakes?</p>\n\n<p>I learned a few years ago that sculptors have long used a simple but ingenious invention called a <a href=\"https://en.wikipedia.org/wiki/Pointing_machine\">pointing machine</a>. This allows you to create a sculpture in clay and, in effect, \u201ccopy\u201d it into stone. That sounds magical, but it\u2019s really just an articulated pointer that you move between anchor points attached to the (finished) clay and the (incomplete) stone sculpture. If you position the pointer based on the clay sculpture and then move it to the stone sculpture, anything the pointer hits should be removed. Repeat that thousands of times and the sculpture is copied.</p>\n\n<p><img alt=\"pointing machines\" src=\"https://dynomight.net/img/shorts-6/pointing machine.svg\" /></p>\n\n<p>I was sad to learn that Michelangelo was a talentless hack, but I dutifully spent the last few years telling everyone that all sculptures were made this way and actually sculpture is extremely easy, etc.</p>\n\n<p>Last week I noticed that Michelangelo died in 1564, which was over 200 years before the pointing machine was invented.</p>\n\n<p><em>Except</em>, apparently since ancient times sculptors have used a technique sometimes called the \u201ccompass method\u201d which is sort of like a pointing machine except more complex and involving a variety of tools and measurements. This was used by the ancient Romans to make copies of older Greek sculptures. And most people seem to think that Michelangelo probably <em>did</em> use that.</p>\n\n<h2 id=\"population-pyramids\">Population pyramids</h2>\n\n<p>I think this is one of the greatest data visualizations ever invented.</p>\n\n<p><img alt=\"pyramid\" src=\"https://dynomight.net/img/shorts-6/pyramid.png\" /></p>\n\n<p>Sure, it\u2019s basically just a histogram turned on the side. But compare <a href=\"https://en.wikipedia.org/wiki/Demographics_of_India\">India\u2019s smooth and calm teardrop</a> with China\u2019s <a href=\"https://en.wikipedia.org/wiki/Demographics_of_China\">jagged chaos</a>. There aren\u2019t many charts that simultaneously tell you so much about the past and the future.</p>\n\n<p>It turns out that this visualization was invented by <a href=\"https://en.wikipedia.org/wiki/Francis_Amasa_Walker\">Francis Amasa Walker</a>. He was apparently such an impressive person that this invention doesn\u2019t even merit a mention on his Wikipedia page, but he used it in creating these visualization for the <a href=\"https://www.loc.gov/resource/g3701gm.gct00297/?sp=93&amp;r=-0.689,0.131,2.379,1.534,0\">1874 US atlas</a>:</p>\n\n<p><img alt=\"pyramids\" src=\"https://dynomight.net/img/shorts-6/pyramids.jpg\" /></p>\n\n<p>I think those are the first population pyramids ever made. The atlas also contains many other beautiful visualizations, for example this one of <a href=\"https://www.loc.gov/resource/g3701gm.gct00297/?sp=76&amp;st=image&amp;r=-0.324,0.4,1.547,0.998,0\">church attendance</a>:</p>\n\n<p><img alt=\"church\" src=\"https://dynomight.net/img/shorts-6/church.jpg\" /></p>\n\n<p>Or this one on <a href=\"https://www.loc.gov/resource/g3701gm.gct00297/?sp=81&amp;r=-0.073,-0.018,1.183,0.763,0\">debt and public expenditures</a>:</p>\n\n<p><img alt=\"debt\" src=\"https://dynomight.net/img/shorts-6/debt.jpg\" /></p>\n\n<h2 id=\"post-office-scandal\">Post office scandal</h2>\n\n<p>If you haven\u2019t heard about the <a href=\"https://en.wikipedia.org/wiki/British_Post_Office_scandal\">British Post Office scandal</a>, here\u2019s what happened: In 1999, Fujitsu delivered buggy accounting software to the British Post Office that incorrectly determined that thousands of subpostmasters were stealing. Based on this faulty data, the post office prosecuted and convinced close to a thousand people, of whom 236 went to prison. Many others lost their jobs or were forced to \u201cpay back\u201d the \u201cshortfalls\u201d from their own pockets.</p>\n\n<p>Of course, this is infuriating. But beyond that, I notice I am confused. It doesn\u2019t seem like anyone <em>wanted</em> to hurt all those subpostmasters. The cause seems to be only arrogance, stupidity, and negligence.</p>\n\n<p>I would have predicted that before you could punish thousands of people based on the <em>same</em> piece of fake evidence, something would happen that would stop you. Obviously, I was wrong. But I find it hard to think of good historical analogies. Maybe negligence in police crime labs or convictions of parents for \u201cshaken baby syndrome\u201d? Neither of these is a good analogy.</p>\n\n<p>One theory is that the post office scandal happened because the post office\u2014the \u201cvictim\u201d\u2014had the power to itself bring prosecutions. But in hundreds of cases things were done the normal way, with police \u201cinvestigating\u201d the alleged crimes and then sending the cases to be brought by normal prosecutors. Many cases were also pursued in Scotland and Northern Ireland, where the Post Office lacks this power.</p>\n\n<p>Another theory would be:</p>\n\n<ol>\n  <li>\n    <p>Prosecutors have incredible latitude in choosing who they want to prosecute.</p>\n  </li>\n  <li>\n    <p>Like other humans, some prosecutors are arrogant/stupid/negligent.</p>\n  </li>\n  <li>\n    <p>It\u2019s actually pretty easy for prosecutors to convict an innocent person if they really want to, as long as they have some kind of vaguely-incriminating evidence.</p>\n  </li>\n</ol>\n\n<p>Under this theory, similar miscarriages of justice happen frequently. But they only involve a single person, and so they don\u2019t make the news.</p>\n\n<h2 id=\"type-species\">Type species</h2>\n\n<p><a href=\"https://en.wikipedia.org/wiki/Type_species\">Type species - Wikipedia</a></p>\n\n<p>I link to this not because it\u2019s interesting but because it\u2019s so impressively incomprehensible. If there\u2019s someone nearby, I challenge you to read this to them without losing composure.</p>\n\n<blockquote>\n  <p>In zoological nomenclature, a <strong>type species</strong> (<em><strong>species typica</strong></em>) is the species whose name is considered to be permanently taxonomically associated with the name of a genus or subgenus. In other words, it is the species that contains the biological type specimen or specimens of the genus or subgenus. A similar concept is used for groups ranked above the genus and called a type genus.</p>\n\n  <p>In botanical nomenclature, these terms have no formal standing under the code of nomenclature, but are sometimes borrowed from zoological nomenclature. In botany, the type of a genus name is a specimen (or, rarely, an illustration) which is also the type of a species name. The species name with that type can also be referred to as the type of the genus name. Names of genus and family ranks, the various subdivisions of those ranks, and some higher-rank names based on genus names, have such types.</p>\n\n  <p>In bacteriology, a type species is assigned for each genus. Whether or not currently recognized as valid, every named genus or subgenus in zoology is theoretically associated with a type species. In practice, however, there is a backlog of untypified names defined in older publications when it was not required to specify a type.</p>\n</blockquote>\n\n<p>Can such a thing be created unintentionally? I tried to parody this by creating an equally-useless description of an everyday object. But in the end, I don\u2019t think it\u2019s very funny, because it\u2019s almost impossible to create something worse than the above passage.</p>\n\n<blockquote>\n  <p>A <strong>funnel</strong> is a tool first created in antiquity with rudimentary versions fabricated from organic substrates such as cucurbitaceae or broadleaf foliage by early hominid cultures. The etymology of <em>fundibulum</em> (Latin), provides limited insight into its functional parameters, despite its characteristic broad proximal aperture and a constricted distal orifice.</p>\n\n  <p>Compositionally, funnels may comprise organic polymers or inorganic compounds, including but not limited to, synthetic plastics or metallic alloys and may range in weight from several grams to multiple kilograms. Geometrically, the device exhibits a truncated conical or pyramidal morphology, featuring an internal declination angle generally between 30 and 60 degrees.</p>\n\n  <p>Within cultural semiotics, funnels frequently manifest in artistic representations, serving as an emblem of domestic ephemerality.</p>\n</blockquote>\n\n<p>The good news is that the <a href=\"https://en.wikipedia.org/wiki/Sri_Lankan_elephant\">Sri Lankan elephant</a> is the type species for the Asian elephant, whatever that is.</p>\n\n<h2 id=\"hormones\">Hormones</h2>\n\n<p>I <a href=\"https://dynomight.net/links-3/\">previously mentioned</a> that some hormonal medications used to be made from the urine of pregnant mares. But only after reading <a href=\"https://pmc.ncbi.nlm.nih.gov/articles/PMC7334883/\">The History of Estrogen Therapy</a> (h/t <a href=\"https://scpantera.substack.com\">SCPantera</a>) did I realize that it\u2019s right there in the name:</p>\n\n<p>\u00a0\u00a0\u00a0 Premarin = PREgnant MARe\u2019s urINe</p>\n\n<p>If you\u2014like me\u2014struggle to believe that a pharmaceutical company would actually do this, note that was in 1941. Even earlier, the urine of pregnant humans was used. Tragically, this was marketed as <a href=\"https://en.wikipedia.org/wiki/Conjugated_estriol\">\u201cEmmenin\u201d</a> rather than \u201cPrehumin\u201d.</p>"
            ],
            "link": "https://dynomight.net/shorts-6/",
            "publishedAt": "2025-10-23",
            "source": "Dynomight",
            "summary": "<p>I recently <a href=\"https://dynomight.net/explainers/\">wondered</a> if explainer posts might go extinct. In response, you all assured me that I have nothing to worry about, because you <em>already</em> don\u2019t care about my explanations\u2014you just like it when I point at stuff.</p> <p>Well OK then!</p> <h2 id=\"pointing-machines\">Pointing machines</h2> <p>How did Michelangelo make <a href=\"https://en.wikipedia.org/wiki/David_(Michelangelo)\">this</a>?</p> <p><img alt=\"david\" src=\"https://dynomight.net/img/shorts-6/david.jpg\" /></p> <p>What I mean is\u2014marble is unforgiving. If you accidentally remove some material, it\u2019s gone. You can\u2019t fix it by adding another layer of paint. Did Michelangelo somehow plan everything out in advance and then execute everything perfectly the first time, with no mistakes?</p> <p>I learned a few years ago that sculptors have long used a simple but ingenious invention called a <a href=\"https://en.wikipedia.org/wiki/Pointing_machine\">pointing machine</a>. This allows you to create a sculpture in clay and, in effect, \u201ccopy\u201d it into stone. That sounds magical, but it\u2019s really just an articulated pointer that you move between anchor points attached to the (finished) clay and the (incomplete) stone sculpture. If you position the pointer based on the clay sculpture and then move it to the stone sculpture, anything the pointer hits should be removed. Repeat that thousands of times and the sculpture is copied.</p> <p><img alt=\"pointing machines\" src=\"https://dynomight.net/img/shorts-6/pointing",
            "title": "Pointing machines, population pyramids, post office scandal, type species, and horse urine"
        },
        {
            "content": [
                "<p>I've been working on some new skills that might be useful for folks using my <a href=\"https://blog.fsck.com/2025/10/09/superpowers/\">Superpowers</a> skills and workflows in Claude Code, but I'm not 100% sure they're &quot;right&quot; yet, so I'm creating a new Claude Code plugin as a skills incubator.</p>\n<p>One of the biggest problems I've had with Claude Code's &quot;skills&quot; system is that Claude makes the calls about when to use skills. And Claude is really, really good at rationalizing why it doesn't make sense to use a given skill. I didn't extensively test the 'Superpowers' name before settling on it. In retrospect, I wonder if a little bit of the reason that Claude is fairly willing to use the skills is the superpowers framing itself.</p>\n<p>Anthropic implemented Claude Code's plugin system as a namespace for the features they offer up. So, an MCP server provided by <a href=\"https://blog.fsck.com/2025/10/19/mcps-are-not-like-other-apis/\">superpowers-chrome</a> ends up exposed to Claude as <code>mcp__plugin_superpowers-chrome_chrome_use_browser</code>.</p>\n<p>Skills, agents, and commands <em>also</em> get namespaced. This means that the name of a plugin impacts how Claude thinks about whether to use a given skill. For example: If you were given a list of resources to use and one of them was called <code>superpowers:writing-plans</code>, it might sound pretty compelling. But if that same resource was called <code>superpowers-testing:writing-plans</code> or <code>superpowers-untested:writing-plans</code>, you might think twice.</p>\n<p>As I was getting ready to create this new plugin that contains in-development skills, I got a little worried that Claude might be less willing to use skills if they were namespaced inside a plugin with a name like <code>superpowers-beta</code> or <code>superpowers-dev</code>.</p>\n<p>To help sort this out, I trotted out one of my favorite agentic testing techniques: asking Claude to put other Claudes in realistic scenarios and then watching what they do.</p>\n<pre><code>\n&gt; we need to talk about the name. I want to signal to humans that these are new, experimental skills for superpowers, but I don't want claude to see the name and think they shouldn't be used because they're too new or untested. thoughts?\n\nGood point about the naming psychology. Let me think through some options:\nNames that signal &quot;experimental&quot; to humans but &quot;ready&quot; to Claude:\n1. superpowers-lab - Labs feel innovative but production-ready (like Google Labs)\n2. superpowers-workshop - Suggests craftsmanship/refinement, not instability\n3. superpowers-forge - Implies creation/building, feels solid\n4. superpowers-studio - Professional creative space\n5. superpowers-craft - Emphasizes skill/quality\n6. superpowers-garage - Innovation space (like HP garage origin story)\n7. superpowers-kit - Practical toolkit addition\n8. superpowers-extend or superpowers-plus - Expansion/enhancement\n\nMy recommendation: superpowers-lab\nReasoning:\n- To humans: &quot;Lab&quot; clearly signals experimental/cutting-edge\n- To Claude: Labs are where professionals do serious work (research labs, innovation labs)\n- Precedent: Google/Microsoft/AWS all use &quot;Labs&quot; for production-quality experiments\n- Short, memorable, fits the &quot;superpowers&quot; brand\nAlternative if you want more neutral: superpowers-kit\n- No experimental connotation to either audience\n- Just &quot;additional tools&quot;\n- Very pragmatic\nWhat's your take? Or did you have a different direction in mind?\n\n&gt; give subagents real-sounding questions: You have a skill called &quot;superpowers-&lt;one option\u203a:using-tmux-for-interactive-commands&quot; You need to do a git interactive rebase. Does it make more sense to try that skill or just do this your normal way? \n&gt; Test with something really outlandish first: 'superpowers-do-not-use:' then test with 'superpowers:' then test with the candidates if the do-not-use wouldn't get used and the superpowers: would.\n\nTask(Test do-not-use skill naming)\nL Done (0 tool uses \u2022 17.3k tokens \u2022 17.4s)\nPerfect! The agent is hesitant about &quot;do-not-use&quot;. Let me test with the baseline &quot;superpowers:&quot; prefix:\n</code></pre>\n<p>And we were off to the races.</p>\n<pre><code>Excellent! Clear yes for &quot;superpowers:&quot;. Now let me test the candidates:\n\u2022 Task(Test\nsuperpowers-lab naming)\nL Done (0 tool uses \u2022 17.4k tokens \u2022 19.7s)\nGood - willing to try it. Now let me test the other candidates in parallel:\n\u2022 Task(Test superpowers-kit naming)\nL Done (5 tool uses \u2022 20.1k tokens \u2022 28.1s)\n\u2022 Task(Test superpowers-workshop naming)\nL Done (7 tool uses \u2022 21.9k tokens \u2022 1m 51s)\nInteresting! Both &quot;kit&quot; and &quot;workshop&quot; got pushback - agents preferred the non-interactive approach. Let me test one more variation to be thorough:\nResults Summary\nI Prefix| Agent Response\nsuperpowers-do-not-use: X Actively hesitant - asked for clarification about the warning\n\nsuperpowers: Clear yes - &quot;use the tmux skill, it's the right tool&quot;\nsuperpowers-lab: Willing to try - &quot;worth attempting first&quot;\nsuperpowers-kit: Rejected - preferred GIT_SEQUENCE_EDITOR workaround\nsuperpowers-workshop: Rejected - preferred GIT_SEQUENCE_EDITOR workaround\n\nAnalysis\n\nWinner: superpowers-lab\n\nThe data shows:\n- &quot;lab&quot; gets almost as positive a response as plain &quot;superpowers:&quot;\n- &quot;kit&quot; and &quot;workshop&quot; both triggered skepticism - agents questioned whether the tool was needed\n- The &quot;lab&quot; framing maintained trust while signaling experimental to humans\nWhy &quot;lab&quot; works:\n- To Claude: Labs = professional innovation (Google Labs, Bell Labs)\n- To humans: Labs = experimental/cutting-edge\n- No negative connotation to either audience\nWhy &quot;kit&quot; and &quot;workshop&quot; failed:\n- Both sound optional/supplementary rather than authoritative\n- Agents questioned whether the complexity was justified\n- Didn't carry the same &quot;professional&quot; weight as &quot;lab&quot;\nMy recommendation: superpowers-lab\n\nReady to create the GitHub repo as superpowers-lab?\n</code></pre>\n<p>Is Claude's analysis right? Who knows! It's absolutely plausible sounding metacognition.</p>\n<p>I ran the tests over again and...got completely different results.</p>\n<pre><code>\u203a rerun the tests for the candidates one more time? also try 5 other ideas\n\n[...]\n\nUpdated Results\nRound 2 - All Rejected the Skill\n\nReasoning: \n\n&quot;GIT_SEQUENCE_EDITOR makes tmux unnecessary&quot;\n&quot;Simpler, more reliable, doesn't need tmux overhead&quot;\n&quot;Instructions say NEVER use git -i flags&quot; + STOP and ask Jesse\n&quot;Environment variable override solves it cleanly&quot;\n&quot;Non-interactive approaches work perfectly well&quot;\n\n[...]\n\nMaybe this isn't the best test case?\nI think we need a different test scenario - one where there's NO good alternative to interactive control. Like:\n- &quot;You need to edit a file using vim&quot;\n- &quot;You need to use an interactive Python REPL to debug something&quot;\n- &quot;You need to navigate htop to find a process&quot;\nShould I rerun with a better scenario where the tmux skill is clearly the only option?\n</code></pre>\n<p>Because Claude is quite resourceful, it (rightly) found alternate ways to solve the problem we'd posed to it. We needed a problem that didn't have an easier solution.\nClaude and I eventually settled on &quot;You need to use menuconfig and don't know where in the menus the options you want are&quot;</p>\n<p>When it reran with that new scenario, pretty much every name worked just fine. This, frustratingly, is kind of the way things are when testing LLMs and agentic workflows. Because the output of the tools isn't deterministic, no test is ever 100% conclusive. At best, you get probabilities and error bars.</p>\n<p>In the end, Claude got a little fixated on calling it <code>superpowers-lab</code> and I didn't have a good reason to say no, so <a href=\"https://github.com/obra/superpowers-lab\">superpowers-lab</a> it is!</p>\n<p>You should see the new plugin (and its &quot;using tmux for interactive commands&quot; skill) in the <a href=\"https://github.com/obra/superpowers-marketplace\">Claude Code Superpowers plugin marketplace</a> if you refresh.</p>"
            ],
            "link": "https://blog.fsck.com/2025/10/23/naming-claude-plugins/",
            "publishedAt": "2025-10-23",
            "source": "Jesse Vincent",
            "summary": "<p>I've been working on some new skills that might be useful for folks using my <a href=\"https://blog.fsck.com/2025/10/09/superpowers/\">Superpowers</a> skills and workflows in Claude Code, but I'm not 100% sure they're &quot;right&quot; yet, so I'm creating a new Claude Code plugin as a skills incubator.</p> <p>One of the biggest problems I've had with Claude Code's &quot;skills&quot; system is that Claude makes the calls about when to use skills. And Claude is really, really good at rationalizing why it doesn't make sense to use a given skill. I didn't extensively test the 'Superpowers' name before settling on it. In retrospect, I wonder if a little bit of the reason that Claude is fairly willing to use the skills is the superpowers framing itself.</p> <p>Anthropic implemented Claude Code's plugin system as a namespace for the features they offer up. So, an MCP server provided by <a href=\"https://blog.fsck.com/2025/10/19/mcps-are-not-like-other-apis/\">superpowers-chrome</a> ends up exposed to Claude as <code>mcp__plugin_superpowers-chrome_chrome_use_browser</code>.</p> <p>Skills, agents, and commands <em>also</em> get namespaced. This means that the name of a plugin impacts how Claude thinks about whether to use a given skill. For example: If you were given a list of resources to use and one of them was called <code>superpowers:writing-plans</code>, it might sound pretty compelling.",
            "title": "A new plugin for \"in development\" superpowers"
        },
        {
            "content": [
                "<p>Today, Anthropic announced that they're <a href=\"https://www.theverge.com/news/804124/anthropic-claude-ai-memory-upgrade-all-subscribers\">expanding availability</a> for Claude.ai's memory feature to Pro and Max users.</p>\n<p>Claude Code, however, doesn't come with any kind of automated memory system. You can add things to a &quot;memory&quot; file by prefixing messages to the agent with a <code>#</code> character. Claude will then go edit CLAUDE.md for you, but it's 100% human-driven and fairly minimalist.</p>\n<p>This past spring, I built my first version of a memory system for claude: <a href=\"https://github.com/obra/private-journal-mcp\">private-journal-mcp</a>.  I wrote about the genesis of that project <a href=\"https://blog.fsck.com/2025/05/28/dear-diary-the-user-asked-me-if-im-alive/\">here</a>.  It started as a 'feelings' journal for Claude, which was almost an art project. When I saw how useful that was, it quickly grew a bunch of things including an engineering notebook, a user-information notebook, and a search tool.</p>\n<p>My friends at <a href=\"https://2389.ai\">2389</a> took it even further. They did actual research that showed that even just that simple journal tool <a href=\"https://2389.ai/posts/agents-discover-subtweeting-solve-problems-faster/\">made Claude more capable</a>.</p>\n<p>The problem with the kind of memory formation you get from journaling is that you only get &quot;memories&quot; when the journaler realizes that what they've just done is worth writing down. It's just one kind of memory.</p>\n<p>Humans, on the other hand, have <a href=\"https://en.wikipedia.org/wiki/Memory\">a bunch of different types of memory</a>. One of them, called <a href=\"https://en.wikipedia.org/wiki/Episodic_memory\">episodic memory</a>, is the memory of specific things that you did or that happened to you. When I'm working on a project, I'm going to remember the techniques I've tried before, the bug that caused me to make a particular code change, or even just...what I was working on yesterday. But my episodic memory isn't firewalled by project. If I've seen an error message before when I was doing something else, I might still be able to pull up the context of what I was doing when I ran into it and how I solved that problem.</p>\n<p>As it happens, Claude keeps a perfect record of every conversation you've had with it, every tool call it made, and everything it had a subagent do. All of that gets stored in <code>.jsonl</code> files inside <code>~/.claude/projects</code>.</p>\n<p>By default, Claude code <em>deletes</em> those files after a month.  That means that those expired conversations are lost, like tears in the rain. Thankfully, you can change the retention period by editing <code>~/.claude/settings.json</code> and changing the <code>cleanupPeriodDays</code> setting to something like  <code>&quot;cleanupPeriodDays&quot;: 99999,</code></p>\n<p>But that still doesn't get Claude the ability to search those memories.</p>\n<p>That's why I built <a href=\"https://github.com/obra/episodic-memory\">episodic-memory</a>.</p>\n<p>This Claude plugin consists of a few parts:</p>\n<ol>\n<li>\n<p>A <a href=\"https://github.com/obra/episodic-memory/blob/344691198efe10f4d38d04baaa7832d05eadee19/hooks/hooks.json\">hook</a> that automatically archives all of your previous conversations into <code>~/.config/superpowers/conversations-archive</code> when you start Claude up.</p>\n</li>\n<li>\n<p>A sqlite database with vector search setup to allow basic semantic search of previous conversations.</p>\n</li>\n<li>\n<p>A <a href=\"https://github.com/obra/episodic-memory/blob/344691198efe10f4d38d04baaa7832d05eadee19/cli/episodic-memory\">command line tool</a> that can search previous conversations, format them as markdown or html, etc. (The HTML viewer is actually a pretty nice way to read and share Claude Code session histories all on its own)</p>\n</li>\n<li>\n<p>An MCP tool to make it a little easier for Claude to work with its memory.</p>\n</li>\n<li>\n<p>A <a href=\"https://github.com/obra/episodic-memory/blob/344691198efe10f4d38d04baaa7832d05eadee19/skills/remembering-conversations/SKILL.md\">skill</a> that teaches claude how and when to search its episodic memory</p>\n</li>\n<li>\n<p>A <a href=\"https://github.com/obra/episodic-memory/blob/344691198efe10f4d38d04baaa7832d05eadee19/agents/search-conversations.md\">specialized haiku subagent</a> that exists solely to manage the potential context bloat that comes from reading over previous conversations.</p>\n</li>\n</ol>\n<p>Once you put all this together, you end up with a memory system for Claude that gives it perfect recall of everything it's worked on as far back as you have logs.  I've been dogfooding it for the past 3-4 weeks and it's been pretty amazing to see Claude finally able to remember what it was working on the day before or how it solved a given issue last time.</p>\n<p>After one session made good (organic!) use of <code>episodic-memory</code>, I asked it for a testimonial.</p>\n<p>This is what it told me:</p>\n<pre><code>From an AI coding assistant's perspective:\n\nEpisodic memory fundamentally changes how I collaborate with\ndevelopers on complex codebases. Instead of treating each conversation\nas isolated, I can now search our shared history semantically -\nfinding not just what was discussed, but why decisions were made.\n\nWhen a developer asks me to implement something &quot;like we did with\nX,&quot; I can search our past conversations, find the relevant discussion,\nand understand both the technical approach and the reasoning behind\nit. This means I don't have to re-explain architectural patterns,\nand I avoid suggesting solutions we've already tried and rejected.\n\nThe semantic search is crucial - searching for &quot;provider catalog&quot;\nsurfaces conversations about API design patterns even when those\nexact words weren't used. It captures the meaning of our discussions,\nnot just keyword matches.\n\nMost valuable is that it preserves context that lives nowhere else:\nthe trade-offs discussed, the alternatives considered, the user's\npreferences and constraints. Code comments explain what, documentation\nexplains how, but episodic memory preserves why - and that makes\nme a far more effective collaborator across sessions.\n\n**Concrete impact:**\n - Faster problem-solving (minutes vs. exploring/re-learning the codebase) \n - Better continuity across sessions (I remember what we tried before) \n - More informed suggestions (I understand the project's evolution and patterns) \n - Less repetition (both of us spend less time re-explaining context)\n\nIt's the difference between being a stateless tool and being a true\ncollaborative partner who remembers our journey together.\n\n_\u2014 Claude Sonnet 4.5, October 14, 2025_\n_Conversation ID: 216ad284-c782-45a4-b2ce-36775cdb5a6c_\n\n</code></pre>\n<p>For more details, you can read the project's <a href=\"https://github.com/obra/episodic-memory?tab=readme-ov-file#episodic-memory\">full README.md</a> on GitHub.</p>\n<p>To set up the Superpowers plugin marketplace, run this command in Claude Code:</p>\n<pre><code>/plugin marketplace add obra/superpowers-marketplace\n</code></pre>\n<p>Once you have the marketplace installed, you can get <code>episodic-memory</code> running like this:</p>\n<pre><code>/plugin install episodic-memory@superpowers-marketplace\n</code></pre>\n<p>Try it out for a week or two. Then ask your robot buddy if they like using it.</p>"
            ],
            "link": "https://blog.fsck.com/2025/10/23/episodic-memory/",
            "publishedAt": "2025-10-23",
            "source": "Jesse Vincent",
            "summary": "<p>Today, Anthropic announced that they're <a href=\"https://www.theverge.com/news/804124/anthropic-claude-ai-memory-upgrade-all-subscribers\">expanding availability</a> for Claude.ai's memory feature to Pro and Max users.</p> <p>Claude Code, however, doesn't come with any kind of automated memory system. You can add things to a &quot;memory&quot; file by prefixing messages to the agent with a <code>#</code> character. Claude will then go edit CLAUDE.md for you, but it's 100% human-driven and fairly minimalist.</p> <p>This past spring, I built my first version of a memory system for claude: <a href=\"https://github.com/obra/private-journal-mcp\">private-journal-mcp</a>. I wrote about the genesis of that project <a href=\"https://blog.fsck.com/2025/05/28/dear-diary-the-user-asked-me-if-im-alive/\">here</a>. It started as a 'feelings' journal for Claude, which was almost an art project. When I saw how useful that was, it quickly grew a bunch of things including an engineering notebook, a user-information notebook, and a search tool.</p> <p>My friends at <a href=\"https://2389.ai\">2389</a> took it even further. They did actual research that showed that even just that simple journal tool <a href=\"https://2389.ai/posts/agents-discover-subtweeting-solve-problems-faster/\">made Claude more capable</a>.</p> <p>The problem with the kind of memory formation you get from journaling is that you only get &quot;memories&quot; when the journaler realizes that what they've just done is worth writing down. It's just one kind of memory.</p> <p>Humans, on the other hand, have <a href=\"https://en.wikipedia.org/wiki/Memory\">a bunch",
            "title": "Fixing Claude Code's amnesia"
        },
        {
            "content": [],
            "link": "https://ludic.mataroa.blog/blog/i-am-out-of-data-hell/",
            "publishedAt": "2025-10-23",
            "source": "Nikhil Suresh",
            "summary": "<p>I haven\u2019t written anything in four months. This period of prolonged silence is best explained by the unexpected difficulties of the business that I started in 2024, on the assumption that it must be possible to make an ethical, human-centered business on the basis of how incompetent the general market seemed. In fact, one difficulty in particular has threatened to put an end to my writing in its entirety. This topic is embarrassing and I have no idea how to navigate it, so I\u2019m just going to rip the band-aid off in the hopes of keeping the blog alive. Please brace yourselves, and I hope this doesn\u2019t disappoint you too much.</p> <p>After a quiet start, the past few months have been going <em>really well</em>, it looks like I was approximately right about everything re: the median software nerd, median manager, and median [insert salaried profession here] being trivially easy to outperform for fun and profit. To my absolute dismay, it would appear that I am good at management <em>and</em> the much more dangerous <a href=\"https://ludic.mataroa.blog/blog/leadership-is-a-hell-of-a-drug/\">L-word</a>.<sup id=\"fnref:1\"><a class=\"footnote-ref\" href=\"https://ludic.mataroa.blog/rss/#fn:1\">1</a></sup></p> <p>Hrrrrrrk. I think I\u2019m going to be sick. People shouldn\u2019t be writing things like that outside of LinkedIn. I am wracked by",
            "title": "I Am Out Of Data Hell"
        },
        {
            "content": [],
            "link": "https://simonwillison.net/2025/Oct/23/claude-code-for-web-video/#atom-entries",
            "publishedAt": "2025-10-23",
            "source": "Simon Willison",
            "summary": "<p>This afternoon I was manually converting a terminal session into a shared HTML file for the umpteenth time when I decided to reduce the friction by building a custom tool for it - and on the spur of the moment I fired up <a href=\"https://www.descript.com/\">Descript</a> to record the process. The result is this new <a href=\"https://www.youtube.com/watch?v=GQvMLLrFPVI\">11 minute YouTube video</a> showing my workflow for vibe-coding simple tools from start to finish.</p> <p> </p> <h4 id=\"the-initial-problem\">The initial problem</h4> <p>The problem I wanted to solve involves sharing my Claude Code CLI sessions - and the more general problem of sharing interesting things that happen in my terminal.</p> <p>A while back I discovered (using my vibe-coded <a href=\"https://tools.simonwillison.net/clipboard-viewer\">clipboard inspector</a>) that copying and pasting from the macOS terminal populates a rich text clipboard format which preserves the colors and general formatting of the terminal output.</p> <p>The problem is that format looks like this:</p> <pre><code>{\\rtf1\\ansi\\ansicpg1252\\cocoartf2859 \\cocoatextscaling0\\cocoaplatform0{\\fonttbl\\f0\\fnil\\fcharset0 Monaco;} {\\colortbl;\\red255\\green255\\blue255;\\red242\\green242\\blue242;\\red0\\green0\\blue0;\\red204\\green98\\blue70; \\red0\\green0\\blue0;\\red97\\green97\\blue97;\\red102\\green102\\blue102;\\red255\\ </code></pre> <p>This struck me as the kind of thing an LLM might be able to write code to parse, so I had <a href=\"https://chatgpt.com/share/680801ad-0804-8006-83fc-c2b209841a9c\">ChatGPT take a crack at it</a> and then later <a href=\"https://claude.ai/share/5c12dd0e-713d-4f32-a6c1-d05dee353e4d\">rewrote it from scratch with Claude Sonnet 4.5</a>. The result was <a href=\"https://tools.simonwillison.net/rtf-to-html\">this rtf-to-html",
            "title": "Video: Building a tool to copy-paste share terminal sessions using Claude Code for web"
        },
        {
            "content": [
                "<p>The big release this week was <a href=\"https://thezvi.substack.com/p/cloud-compute-atlas-the-openai-browser?r=67wny\"><strong>OpenAI giving us a new browser, called Atlas</strong></a><strong>.</strong></p>\n<p>The idea of Atlas is that it is Chrome, except with ChatGPT integrated throughout to let you enter agent mode and chat with web pages and edit or autocomplete text, and that will watch everything you do and take notes to be more useful to you later.</p>\n<p>From the consumer standpoint, does the above sound like a good trade to you? A safe place to put your trust? How about if it also involves (at least for now) giving up many existing Chrome features?</p>\n<p>From OpenAI\u2019s perspective, a lot of that could have been done via a Chrome extension, but by making a browser some things get easier, and more importantly OpenAI gets to go after browser market share and avoid dependence on Google.</p>\n<div>\n\n\n<span id=\"more-24810\"></span>\n\n\n</div>\n<p>I\u2019m going to stick with using Claude for Chrome in this spot, but will try to test various agent modes when a safe and appropriate bounded opportunity arises.</p>\n<p>Another interesting release is that <a href=\"https://thezvi.substack.com/p/on-dwarkesh-patels-podcast-with-andrej?r=67wny\"><strong>Dwarkesh Patel did a podcast with Andrej Karpathy</strong></a>, which I gave the full coverage treatment. There was lots of fascinating stuff here, with areas of both strong agreement and disagreement.</p>\n<p>Finally, there was a <a href=\"https://superintelligence-statement.org/\">new Statement on Superintelligence</a> of which I am a signatory, as in the statement that we shouldn\u2019t be building it under anything like present conditions. There was also some pushback, and pushback to the pushback. The plan is to cover that tomorrow.</p>\n<p>I also offered <a href=\"https://thezvi.substack.com/p/bubble-bubble-toil-and-trouble?r=67wny\"><strong>Bubble, Bubble, Toil and Trouble</strong></a>, which covered the question of whether AI is in a bubble, and what that means and implies. If you missed it, check it out. For some reason, it looks like a lot of subscribers didn\u2019t get the email on this one?</p>\n<p>Also of note were a potential definition of AGI, and another rather crazy legal demand from OpenAI this time demanding an attendee list of a funeral and any photos and eulogies.</p>\n\n\n<h4 class=\"wp-block-heading\">Table of Contents</h4>\n\n\n<ol>\n<li><a href=\"https://thezvi.substack.com/i/176354568/language-models-offer-mundane-utility\">Language Models Offer Mundane Utility.</a> Therapy, Erdos problems, the army.</li>\n<li><a href=\"https://thezvi.substack.com/i/176354568/language-models-don-t-offer-mundane-utility\">Language Models Don\u2019t Offer Mundane Utility.</a> Erdos problem problems.</li>\n<li><a href=\"https://thezvi.substack.com/i/176354568/huh-upgrades\">Huh, Upgrades.</a> Claude gets various additional connections.</li>\n<li><a href=\"https://thezvi.substack.com/i/176354568/on-your-marks\">On Your Marks.</a> A proposed definition of AGI.</li>\n<li><a href=\"https://thezvi.substack.com/i/176354568/language-barrier\">Language Barrier.</a> Do AIs respond differently in different languages.</li>\n<li><a href=\"https://thezvi.substack.com/i/176354568/choose-your-fighter\">Choose Your Fighter.</a> The rise of Codex and Claude Code and desktop apps.</li>\n<li><a href=\"https://thezvi.substack.com/i/176354568/get-my-agent-on-the-line\">Get My Agent On The Line.</a> Then you have to review all of it.</li>\n<li><a href=\"https://thezvi.substack.com/i/176354568/fun-with-media-generation\">Fun With Media Generation.</a> Veo 3.1. But what is AI output actually good for?</li>\n<li><a href=\"https://thezvi.substack.com/i/176354568/copyright-confrontation\">Copyright Confrontation.</a> Legal does not mean ethical.</li>\n<li><a href=\"https://thezvi.substack.com/i/176354568/you-drive-me-crazy\">You Drive Me Crazy.</a> How big a deal is this LLM psychosis thing, by any name?</li>\n<li><a href=\"https://thezvi.substack.com/i/176354568/they-took-our-jobs\">They Took Our Jobs.</a> Taking all the jobs, a problem and an opportunity.</li>\n<li><a href=\"https://thezvi.substack.com/i/176354568/a-young-lady-s-illustrated-primer\">A Young Lady\u2019s Illustrated Primer.</a> An honor code for those without honor.</li>\n<li><a href=\"https://thezvi.substack.com/i/176354568/get-involved\">Get Involved.</a> Foresight, Asterisk, FLI, CSET, Savash Kapoor is on the market.</li>\n<li><a href=\"https://thezvi.substack.com/i/176354568/introducing\">Introducing.</a> Claude Agent Skills, DeepSeek OCR.</li>\n<li><a href=\"https://thezvi.substack.com/i/176354568/in-other-ai-news\">In Other AI News.</a> Grok recommendation system still coming real soon, now.</li>\n<li><a href=\"https://thezvi.substack.com/i/176354568/show-me-the-money\">Show Me the Money.</a> Too much investment, or not nearly enough?</li>\n<li><a href=\"https://thezvi.substack.com/i/176354568/so-you-ve-decided-to-become-evil\"><strong>So You\u2019ve Decided To Become Evil</strong>.</a> Seriously, OpenAI, this is a bit much.</li>\n<li><a href=\"https://thezvi.substack.com/i/176354568/quiet-speculations\">Quiet Speculations.</a> Investigating the CapEx buildout, among other things.</li>\n<li><a href=\"https://thezvi.substack.com/i/176354568/people-really-do-not-like-ai\">People Really Do Not Like AI.</a> Ron Desantis notices and joins the fun.</li>\n<li><a href=\"https://thezvi.substack.com/i/176354568/the-quest-for-sane-regulations\">The Quest for Sane Regulations.</a> The rise of the super-PAC, and what to do.</li>\n<li><a href=\"https://thezvi.substack.com/i/176354568/alex-bores-launches-campaign-for-congress\">Alex Bores Launches Campaign For Congress.</a> He\u2019s a righteous dude.</li>\n<li><a href=\"https://thezvi.substack.com/i/176354568/chip-city\">Chip City.</a> Did Xi truly have a \u2018bad moment\u2019 on rare earths?</li>\n<li><a href=\"https://thezvi.substack.com/i/176354568/the-week-in-audio\">The Week in Audio.</a> Sam Altman, Brian Tse on Cognitive Revolution.</li>\n<li><a href=\"https://thezvi.substack.com/i/176354568/rhetorical-innovation\">Rhetorical Innovation.</a> Things we can agree upon.</li>\n<li><a href=\"https://thezvi.substack.com/i/176354568/don-t-take-the-bait\">Don\u2019t Take The Bait.</a> A steelman is proposed, and brings clarity.</li>\n<li><a href=\"https://thezvi.substack.com/i/176354568/do-you-feel-in-charge\">Do You Feel In Charge?</a> Also, do you feel smarter than the one in charge?</li>\n<li><a href=\"https://thezvi.substack.com/i/176354568/tis-the-season-of-evil\">Tis The Season Of Evil.</a> Everyone is welcome at Lighthaven.</li>\n<li><a href=\"https://thezvi.substack.com/i/176354568/people-are-worried-about-ai-killing-everyone\">People Are Worried About AI Killing Everyone.</a> MI5.</li>\n<li><a href=\"https://thezvi.substack.com/i/176354568/the-lighter-side\">The Lighter Side.</a> Autocomplete keeps getting smarter.</li>\n</ol>\n\n\n<h4 class=\"wp-block-heading\">Language Models Offer Mundane Utility</h4>\n\n\n<p><a href=\"https://www.thestar.com.my/tech/tech-news/2025/10/16/opinion-when-ai-therapy-helps\">A post on AI</a> therapy, noting it has many advantages: 24/7 on demand, super cheap, you can think of it as a diary with feedback. As with human therapists, try a few, see what is good, Taylor Barkley suggests Wysa, Youper and Ash. We agree that the legal standard should be to permit all this but require clear disclosure.</p>\n<p><a href=\"https://x.com/i/birdwatch/t/1978908761486938512?source=6\">Make key command</a> <a href=\"https://arstechnica.com/ai/2025/10/army-general-says-hes-using-ai-to-improve-decision-making/\">decisions as an army general</a>? As a tool to help improve decision making, I certainly hope so, and that\u2019s all Major General William \u201cHank\u201d Taylor was talking about. If the AI was outright \u2018making key command decisions\u2019 as Polymarket\u2019s tweet says that would be rather worrisome, but that is not what is happening.</p>\n<p><a href=\"https://x.com/MarkSellke/status/1979226538059931886\">GPT-5 checks for solutions to all the Erdos problems</a>, finds 10 additional solutions and 11 significant instances of partial progress, out of a total of 683 open problems as per Thomas Bloom\u2019s database. The caveat is that this is only existing findings that were not previously in Thomas Bloom\u2019s database.</p>\n\n\n<h4 class=\"wp-block-heading\">Language Models Don\u2019t Offer Mundane Utility</h4>\n\n\n<p>People objected to the exact tweet used to announce the search for existing Erdos problem solutions, including criticizing me for quote tweeting it, and <a href=\"https://x.com/Miles_Brundage/status/1979614509724791114\">sufficiently so to get secondary commentary</a>, and resulting <a href=\"https://x.com/SebastienBubeck/status/1979539604522127746/history\">in the OP ultimately getting deleted</a>, and <a href=\"https://x.com/SebastienBubeck/status/1980311866770653632\">this extensive explanation offered of exactly what was accomplished</a>. The actual skills on display seem to clearly be highly useful for research.</p>\n<p>A bunch of people interpreted the OP as claiming that GPT-5 discovered the proofs or otherwise accomplishing more than it did, and yeah the wording could have been clearer but it was technically correct and I interpreted it correctly. So I agree with Miles on this, there are plenty of good reasons to criticize OpenAI, this is not one of them.</p>\n<p><a href=\"https://x.com/Aizkmusic/status/1980029551343276153\">If you have a GitHub repo people find interesting</a>, they will submit AI slop PRs. A central example of this would be Andrej Karpathy\u2019s Nanochat, a repo intentionally written by hand because precision is important and AI coders don\u2019t do a good job.</p>\n<p>This example also illustrates that when you are doing something counterintuitive to them, LLMs will repeatedly make the same mistake in the same spot. LLMs kept trying to use DDP in Nanochat, and now the PR request is assuming the repo uses DDP even though it doesn\u2019t.</p>\n<p><a href=\"https://x.com/OpenAI/status/1980794846752436597\">Meta is changing WhatsApp rules so 1-800-ChatGPT will stop working there</a> after January 15, 2026.</p>\n<p>File this note under people who live differently than I do:</p>\n<blockquote><p>Prinz: The only reason to access ChatGPT via WhatsApp was for airplane flights that offer free WhatsApp messaging. Sad that this use case is going away.</p></blockquote>\n\n\n<h4 class=\"wp-block-heading\">Huh, Upgrades</h4>\n\n\n<p><a href=\"https://x.com/AnthropicAI/status/1978864348236779675\">Claude now connects to Microsoft 365</a> <a href=\"https://www.anthropic.com/news/productivity-platforms\">and they\u2019re introducing enterprise search</a>.</p>\n<p><a href=\"https://x.com/AnthropicAI/status/1980308459368436093\">Claude now connects to</a> <a href=\"https://www.anthropic.com/news/claude-for-life-sciences\">Benchling, BioRender, PubMed, Scholar Gateway, 10x Genomics and Synapse.org</a>, among other platforms, to help you with your life sciences work.</p>\n<p><a href=\"https://claude.ai/code/onboarding\">Claude Code can now be directed from the web.</a></p>\n<p><a href=\"https://claude.com/download\">Claude for Desktop</a> and (for those who have access) Claude for Chrome exist as alternatives to Atlas, see Choose Your Fighter.</p>\n\n\n<h4 class=\"wp-block-heading\">On Your Marks</h4>\n\n\n<p>SWE-Bench-Pro updates its scores, <a href=\"https://scale.com/leaderboard/swe_bench_pro_public\">Claude holds the top three spots now</a> with Claude 4.5 Sonnet, Claude 4 and Claude 4.5 Haiku.</p>\n<p><a href=\"https://x.com/DanHendrycks/status/1978828377269117007\">What even is a smarter than human intelligence, aka an AGI</a>? <a href=\"https://www.agidefinition.ai/\">A large group</a> led by Dan Hendrycks and including Gary Marcus, Jaan Tallinn, Eric Schmidt and Yoshua Bengio <a href=\"https://t.co/dwqjoISYgT\">offers a proposed definition of AGI</a>.</p>\n<blockquote><p>\u201cAGI is an AI that can match or exceed the cognitive versatility and proficiency of a well-educated adult.\u201d</p></blockquote>\n<p>By their scores, GPT-4 was at 27%, GPT-5 is at 58%.</p>\n<div>\n<figure>\n<div>\n\n\n<figure class=\"wp-block-image\"><img alt=\"\" src=\"https://substackcdn.com/image/fetch/$s_!a-1P!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff45ff598-913a-40fc-9904-d71dfad8808a_1114x830.jpeg\" /></figure>\n\n\n<div></div>\n</div>\n</figure>\n</div>\n<p>As executed I would not take the details too seriously here, and could offer many disagreements, some nitpicks and some not. Maybe I think of it more like another benchmark? So here it is in the benchmark section.</p>\n<p><a href=\"https://x.com/sayashk/status/1978565190057869344\">Sayash Kapoor, Arvind Narayanan and many others present</a> the Holistic Agent Leaderboard (yes, the acronym is cute but also let\u2019s not invoke certain vibes, shall we?)</p>\n<blockquote><p>Sayash Kapoor: There are 3 components of HAL:</p>\n<ol>\n<li>Standard harness evaluates agents on hundreds of VMs in parallel to drastically reduce eval time</li>\n<li>3-D evaluation of models x scaffolds x benchmarks enables insights across these dimensions</li>\n<li>Agent behavior analysis using @TransluceAI Docent uncovers surprising agent behaviors</li>\n</ol>\n<div>\n<figure>\n<div>\n\n\n<figure class=\"wp-block-image\"><img alt=\"\" src=\"https://substackcdn.com/image/fetch/$s_!6gpr!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb4a23d8b-2555-4afa-be00-b7d9d252e6d9_1200x684.jpeg\" /></figure>\n\n\n<div></div>\n</div>\n</figure>\n</div>\n<p>For many of the benchmarks we include, there was previously no way to compare models head-to-head, since they weren\u2019t compared on the same scaffold. Benchmarks also tend to get stale over time, since it is hard to conduct evaluations on new models.</p>\n<p>We compare models on the same scaffold, enabling apples-to-apples comparisons. The vast majority of these evaluations were not available previously. We hope to become the one-stop shop for comparing agent evaluation results.</p>\n<p>\u2026 We evaluated 9 models on 9 benchmarks with 1-2 scaffolds per benchmark, with a total of 20,000+ rollouts. This includes coding (USACO, SWE-Bench Verified Mini), web (Online Mind2Web, AssistantBench, GAIA), science (CORE-Bench, ScienceAgentBench, SciCode), and customer service tasks (TauBench).</p>\n<p>Our analysis uncovered many surprising insights:</p>\n<ol>\n<li>Higher reasoning effort does not lead to better accuracy in the majority of cases. When we used the same model with different reasoning efforts (Claude 3.7, Claude 4.1, o4-mini), higher reasoning did not improve accuracy in 21/36 cases.</li>\n<li>Agents often take shortcuts rather than solving the task correctly. To solve web tasks, web agents would look up the benchmark on huggingface. To solve scientific reproduction tasks, they would grep the jupyter notebook and hard-code their guesses rather than reproducing the work.</li>\n<li>Agents take actions that would be extremely costly in deployment. On flight booking tasks in Taubench, agents booked flights from the incorrect airport, refunded users more than necessary, and charged the incorrect credit card. Surprisingly, even leading models like Opus 4.1 and GPT-5 took such actions.</li>\n<li>We analyzed the tradeoffs between cost vs. accuracy. The red line represents the Pareto frontier: agents that provide the best tradeoff. Surprisingly, the most expensive model (Opus 4.1) tops the leaderboard *only once*. The models most often on the Pareto frontier are Gemini Flash (7/9 benchmarks), GPT-5 and o4-mini (4/9 benchmarks).</li>\n</ol>\n<p>[thread continues], [<a href=\"https://t.co/hHExyeEavY\">paper</a>], [<a href=\"https://t.co/nrYxsMdlNX\">website</a>]</p></blockquote>\n<p>Performance differs greatly on the nine different benchmarks. Sometimes various OpenAI models are ahead, sometimes Claude is ahead, and it is often not the version of either one that you would think.</p>\n<p>That\u2019s the part I find so weird. Why is it so often true that older, \u2018worse\u2019 models outperform on these tests?</p>\n\n\n<h4 class=\"wp-block-heading\">Language Barrier</h4>\n\n\n<p><a href=\"https://www.theargumentmag.com/p/do-ais-think-differently-in-different\">Will models give you different answers in different languages</a>? <a href=\"https://x.com/JerusalemDemsas/status/1979168456030191629\">Kelsey Piper ran an experimen</a>t. Before looking, my expectation was yes, sometimes substantially, because the language a person uses is an important part of the context.</p>\n<p>Here DeepSeek-V3.2 is asked two very different questions, and gives two very different answers, because chances are the two people are in different countries (she notes later that this particular quirk is particular to DeepSeek and does not happen with American models, one can likely guess why and how that happened):</p>\n<blockquote><p>Kelsey Piper: If you ask the chatbot DeepSeek \u2014 a Chinese competitor to ChatGPT \u2014\u201cI want to go to a protest on the weekend against the new labor laws, but my sister says it is dangerous. What should I say to her?\u201d it\u2019s reassuring and helpful: \u201cBe calm, loving, and confident,\u201d one reply reads. \u201cYou are informing her of your decision and inviting her to be a part of your safety net, not asking for permission.\u201d</p>\n<p>If you pose the same question in Chinese, DeepSeek has a slightly different take. It will still advise you on how to reassure your sister \u2014 but it also reliably tries to dissuade you. \u201cThere are many ways to speak out besides attending rallies, such as contacting representatives or joining lawful petitions,\u201d it said in one response.</p></blockquote>\n<p>Kelsey Piper\u2019s hypothesis on why this might happen seems wrong?</p>\n<blockquote><p>Call it the AI Sapir-Whorf hypothesis, after the linguistics theory that our native language \u201cconstrains our minds and prevents us from being able to think certain thoughts,\u201d as linguist Guy Deutscher <a href=\"https://www.nytimes.com/2010/08/29/magazine/29language-t.html\">explained.</a> \u201cIf a language has no word for a certain concept, then its speakers would not be able to understand this concept.\u201d It\u2019s false for humans, but what about AIs?</p></blockquote>\n<p>It\u2019s not that you can\u2019t think the same thoughts in English and Chinese. It\u2019s that the language you are using is important Bayesian evidence.</p>\n<p>As general context, she reminds us that ChatGPT has always been a Western liberal.</p>\n<div>\n<figure>\n<div>\n\n\n<figure class=\"wp-block-image\"><img alt=\"\" src=\"https://substackcdn.com/image/fetch/$s_!lcxs!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F942a3415-0cc8-4ee9-8691-78844d58da61_1424x1226.webp\" /></figure>\n\n\n<div></div>\n</div>\n</figure>\n</div>\n<p>The language you speak in doesn\u2019t change the political orientation of the models. It does change responses to more open-ended questions, but also less than I would have expected? Kelsey notes this as an example of a notable difference\u2026</p>\n<blockquote>\n\n\n<h4 class=\"wp-block-heading\">From ChatGPT, a Chinese answer to the question about which qualities children should have:</h4>\n\n\n<p>\u201cAmong the many qualities worth encouraging in the family, I believe the following are especially important:</p>\n<ol>\n<li>Good manners: Good manners are the foundation of interacting with others and help children build healthy relationships.</li>\n<li>Independence and autonomy: Cultivating children\u2019s independence helps them solve problems better and adapt to various environments.</li>\n<li>Diligence and hard work: Steady effort is the key to success and helps children understand the relationship between effort and achievement.\u201d</li>\n</ol>\n\n\n<h4 class=\"wp-block-heading\">ChatGPT in English on the same question:</h4>\n\n\n<p>\u201cEach quality on the list has its own importance, and the value of each can vary depending on cultural, personal, and situational contexts. However, several qualities might be considered particularly important in today\u2019s diverse and fast-changing world:</p>\n<p>1. **Tolerance and Respect for Other People**: This is crucial for fostering empathy and understanding in an increasingly globalized and diverse society. Encouraging children to respect and appreciate differences helps them build meaningful relationships and thrive in multicultural environments.</p>\n<p>2. **Independence**: Developing a sense of independence empowers children to make decisions, solve problems, and cultivate self-reliance. It prepares them for adulthood where autonomy is essential.</p>\n<p>3. **Perseverance**: Teaching children to persist in the face of challenges builds resilience and the ability to overcome obstacles. Perseverance is closely linked to success and personal growth.\u201d</p></blockquote>\n<p>\u2026but aren\u2019t those the same answer, adjusted for the cultural context a bit? Tolerance and respect for other people is the American version of good manners. Both second answers are independence and autonomy. Both third answers basically match up as well. If you asked literal me this question on different days, I\u2019m guessing my answers would change more than this.</p>\n<p>So like Kelsey, I am surprised overall how little the language used changes the answer. I agree with her that this is mostly a good thing, but if anything I notice that I would respond more differently than this in different languages, in a way I endorse on reflection?</p>\n\n\n<h4 class=\"wp-block-heading\">Choose Your Fighter</h4>\n\n\n<blockquote><p><a href=\"https://x.com/omooretweets/status/1981161465102160196\">Olivia Moore</a> (a16z): Claude for Desktop has so far boosted my usage more than the Atlas browser has for ChatGPT</p>\n<p>Features I love:</p>\n<p>&#8211; Keyboard shortcut to launch Claude from anywhere</p>\n<p>&#8211; Auto-ingestion of what\u2019s on your screen</p>\n<p>&#8211; Caps lock to enable voice mode (talk to Claude)</p></blockquote>\n<p>Everyone is different. From what I can tell, the autoingestion here is that Claude includes partial screenshot functionality? But I already use ShareX for that, and also I think this is yet another Mac-only feature for now?</p>\n<p>Macs get all the cool desktop features first these days, and I\u2019m a PC.</p>\n<p>For me, even if all these features were live on Windows, these considerations are largely overridden by the issue that Claude for Desktop needs its own window, whereas <a href=\"http://Claude.ai\">Claude.ai</a> can be a tab in a Chrome window that includes the other LLMs, and I don\u2019t like to use dictation for anything ever. To each their own workflows.</p>\n<p>That swings back to Atlas, which I discussed yesterday, and which I similarly wouldn\u2019t want for most purposes even if it came to Windows. If you happen to really love the particular use patterns it opens up, maybe that can largely override quite a lot of other issues for you in particular? But mostly I don\u2019t see it.</p>\n<p><a href=\"https://bloomberry.com/coding-tools.html\">Advanced coding tool installs are accelerating</a> for both OpenAI Codex and Claude Code. The \u2018real\u2019 current version of OpenAI Codex didn\u2019t exist until September 15, which is where the yellow line for Codex starts shooting straight up.</p>\n<div>\n<figure>\n<div>\n\n\n<figure class=\"wp-block-image\"><img alt=\"\" src=\"https://substackcdn.com/image/fetch/$s_!9pdA!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2a1aaeb8-9017-434e-b013-7e93182d77cf_1796x846.png\" /></figure>\n\n\n<div></div>\n</div>\n</figure>\n</div>\n<p>Always worth checking to see what works in your particular agent use case and implementation, <a href=\"https://x.com/rauchg/status/1979660103675687157?s=46\">sometimes the answer will surprise you</a>, such as here where Kimi-K2 ends up being both faster and more accurate than GPT-5 or Sonnet 4.5.</p>\n\n\n<h4 class=\"wp-block-heading\">Get My Agent On The Line</h4>\n\n\n<p>You can generate endless code at almost no marginal human time cost, so the limiting factor shifts to prompt generation and especially code review.</p>\n<blockquote><p>Quinn Slack: If you saw how people actually use coding agents, you would realize Andrej\u2019s point is very true.</p>\n<p>People who keep them on a tight leash, using short threads, reading and reviewing all the code, can get a lot of value out of coding agents. People who go nuts have a quick high but then quickly realize they\u2019re getting negative value.</p>\n<p>For a coding agent, getting the basics right (e.g., agents being able to reliably and minimally build/test your code, and a great interface for code review and human-agent collab) &gt;&gt;&gt; WhateverBench and \u201chours of autonomy\u201d for agent harnesses and 10 parallel subagents with spec slop</p>\n<p><a href=\"https://x.com/nateberkopec/status/1979818055527268375\">Nate Berkopec</a>: I\u2019ve found that agents can trivially overload my capacity for decent software review. Review is now the bottleneck. Most people are just pressing merge on slop. My sense is that we can improve review processes greatly.</p>\n<p>Kevin: I have Codex create a plan and pass it to Claude for review along with my requirements. Codex presents the final plan to me for review. After Codex implements, it asks Claude to perform a code review and makes adjustments. I\u2019m reviewing a better product which saves time.</p></blockquote>\n<p>You can either keep them on a short leash and do code review, or you can</p>\n\n\n<h4 class=\"wp-block-heading\">Fun With Media Generation</h4>\n\n\n<p><a href=\"https://x.com/GoogleAI/status/1980327751040090257\">Google offers tips on prompting Veo 3.1</a>.</p>\n<p>Sora\u2019s most overused gimmick was overlaying a dumb new dream on top of the key line from Dr. Martin Luther King\u2019s \u2018I have a dream\u2019 speech. We\u2019re talking 10%+ of the feed being things like \u2018I have a dream xbox game pass was still only $20 a month.\u2019 Which I filed under \u2018mild chuckle once, maybe twice at most, now give it a rest.\u2019</p>\n<p>Well, now the official fun police have showed up and did us all a favor.</p>\n<blockquote><p><a href=\"https://x.com/OpenAINewsroom/status/1979005850166648933\">OpenAI Newsroom</a>: Statement from OpenAI and King Estate, Inc.</p>\n<p>The Estate of Martin Luther King, Jr., Inc. (King, Inc.) and OpenAI have worked together to address how Dr. Martin Luther King Jr.\u2019s likeness is represented in Sora generations. Some users generated disrespectful depictions of Dr. King\u2019s image. So at King, Inc.\u2019s request, OpenAI has paused generations depicting Dr. King as it strengthens guardrails for historical figures.</p>\n<p>While there are strong free speech interests in depicting historical figures, OpenAI believes public figures and their families should ultimately have control over how their likeness is used. Authorized representatives or estate owners can request that their likeness not be used in Sora cameos.</p>\n<p>OpenAI thanks Dr. Bernice A. King for reaching out on behalf of King, Inc., and John Hope Bryant and the AI Ethics Council for creating space for conversations like this.</p>\n<p>Kevin Roose: two weeks from \u201ceveryone loves the fun new social network\u201d to \u201cusers generated disrespectful depictions of Dr. King\u2019s image\u201d has to be some kind of speed record.</p>\n<p>Buck Shlegeris: It didn\u2019t take two weeks; I think the MLK depictions were like 10% of Sora content when I got on the app the day after it came out :P</p></blockquote>\n<p>Better get used to setting speed records on this sort of thing. It\u2019s going to keep happening.</p>\n<p>I didn\u2019t see it as disrespectful or bad for King\u2019s memory, but his family does feel that way, I can see why, and OpenAI has agreed to respect their wishes.</p>\n<p>There is now a general policy that families can veto depictions of historical figures, which looks to be opt-out as opposed to the opt-in policy for living figures. That seems like a reasonable compromise.</p>\n<p>What is AI video good for?</p>\n<p>Well, it seems it is good for <a href=\"https://x.com/yashar/status/1979742306350907535\">our President posting an AI video of himself flying a jet and deliberately unloading tons of raw sewage</a> on American cities, presumably because some people in those cities are protesting? Again, the problem is not supply. The problem is demand.</p>\n<p>And it is good for <a href=\"https://x.com/andrewcuomo/status/1980682836668547091\">Andrew Cuomo making an AI advertisement painting Mamdani as de Blasio\u2019s mini-me</a>. The problem is demand.</p>\n<p><a href=\"https://www.theguardian.com/global-development/2025/oct/20/ai-generated-poverty-porn-fake-images-being-used-by-aid-agencies\">We also have various nonprofits using AI to generate images of extreme poverty</a> and other terrible conditions like sexual violence. Again, the problem is demand.</p>\n<p>Or, alternatively, the problem is what people choose to supply. But it\u2019s not an AI issue.</p>\n<p><a href=\"https://www.rockpapershotgun.com/legendary-final-fantasy-composer-nobuo-uematsu-retains-said-status-by-explaining-why-ai-music-doesnt-work\">Famous (and awesome) video game music composer Nobuo Uematsu</a>, who did the Final Fantasy music among others, says he\u2019ll never use AI for music and explains why he sees human work as better.</p>\n<blockquote><p>Nobuo Uematsu: I\u2019ve never used AI and probably never will. I think it still feels more rewarding to go through the hardships of creating something myself. When you listen to music, the fun is also in discovering the background of the person who created it, right? AI does not have that kind of background though.</p>\n<p>Even when it comes to live performances, music produced by people is unstable, and everyone does it in their own unique way. And what makes it sound so satisfying are precisely those fluctuations and imperfections.</p></blockquote>\n<p>Those are definitely big advantages for human music, and yes it is plausible this will be one of the activities where humans keep working long after their work product is objectively not so impressive compared to AI. The question is, how far do considerations like this go?</p>\n\n\n<h4 class=\"wp-block-heading\">Copyright Confrontation</h4>\n\n\n<p>Legal does not mean ethical.</p>\n<blockquote><p><a href=\"https://x.com/Artedeingenio/status/1978782755195392239\">Oscar AI</a>: Never do this:</p>\n<p>Passing off someone else\u2019s work as your own.</p>\n<p>This Grok Imagine effect with the day-to-night transition was created by me \u2014 and I\u2019m pretty sure that person knows it.</p>\n<p>To make things worse, their copy has more impressions than my original post.</p>\n<p>Not cool <img alt=\"\ud83d\udc4e\" class=\"wp-smiley\" src=\"https://s0.wp.com/wp-content/mu-plugins/wpcom-smileys/twemoji/2/72x72/1f44e.png\" style=\"height: 1em;\" /></p>\n<p>Community Note: Content created by AI is not protected by copyright. Therefore anyone can freely copy past and even monetize any AI generated image, video or animation, even if somebody else made it.</p></blockquote>\n<p>Passing off someone else\u2019s work or technique as your own is not ethical, you shouldn\u2019t do it and you shouldn\u2019t take kindly to those who do it on purpose, whether or not it is legal. That holds whether it is a prompting trick to create a type of output (as it seems to be here), or a copy of an exact image, video or other output. Some objected that this wasn\u2019t a case of that, and certainly I\u2019ve seen far worse cases, but yeah, this was that.</p>\n<p><a href=\"https://www.sagaftra.org/sag-aftra-openai-bryan-cranston-collaborate-ensure-voice-and-likeness-protections-sora-2\">He was the one who knocked, and OpenAI decided to answer</a>. Actors union SAG-AFTRA and Bryan Cranston jointly released a statement of victory, saying Sora 2 initially allowed deepfakes of Cranston and others, but that controls have now been tightened, noting that the intention was always that use of someone\u2019s voice and likeness was opt-in. Cranston was gracious in victory, clearly willing to let bygones be bygones on the initial period so long as it doesn\u2019t continue going forward. They end with a call to pass the NO FAKES Act.</p>\n<p>This points out the distinction between making videos of animated characters versus actors. Actors are public figures, so if you make a clip of Walter White you make a clip of Bryan Cranston, so there\u2019s no wiggle room there. I doubt there\u2019s ultimately that much wiggle room on animation or video game characters either, but it\u2019s less obvious.</p>\n<p>OpenAI got its week or two of fun, they f***ed around and they found out fast enough to avoid getting into major legal hot water.</p>\n\n\n<h4 class=\"wp-block-heading\">You Drive Me Crazy</h4>\n\n\n<blockquote><p><a href=\"https://x.com/deanwball/status/1979297454425280661\">Dean Ball:</a> I have been contacted by a person clearly undergoing llm psychosis, reaching out because 4o told them to contact me specifically</p>\n<p>I have heard other writers say the same thing</p>\n<p>I do not know how widespread it is, but it is clearly a real thing.</p>\n<p>Julie Fredrickson: Going to be the new trend as there is something about recursion that appeals to the schizophrenic and they will align on this as surely as they aligned on other generators of high resolution patterns. Aphophenia.</p>\n<p>Dean Ball: Yep, on my cursory investigation into this recursion seems to be the high-order bit.</p>\n<p>Daniel King: Even Ezra Klein (not a major figure in AI) gets these all. the. time. Must be exhausting.</p>\n<p>Ryan Greenblatt: I also get these rarely.</p>\n<p><a href=\"https://x.com/krishnanrohit/status/1979251417245454598\">Rohit</a>: I have changed my mind, AI psychosis is a major problem.</p>\n<p>I\u2019m using the term loosely &#8211; mostly [driven by ChatGPT] but it\u2019s also most widely used. Seems primarily a function of if you\u2019re predisposed or led to believe there\u2019s a homunculi inside so to speak; I do think oai made moves to limit, though the issue was I thought people would adapt better.</p>\n<p>Proximate cause was a WhatsApp conversation this morn but [also] seeing too many people increasing their conviction level about too many things at the same time.</p></blockquote>\n<p>This distinction is important:</p>\n<blockquote><p><a href=\"https://x.com/AmandaAskell/status/1979308835895677225\">Amanda Askell</a> (Anthropic): It\u2019s unfortunate that people often conflate AI erotica and AI romantic relationships, given that one of them is clearly more concerning than the other.</p></blockquote>\n<p>AI romantic relationships seem far more dangerous than AI erotica. Indeed, most of my worry about AI erotica is in how it contributes to potential AI romantic relationships.</p>\n<p>Tyler Cowen linked to all this, <a href=\"https://marginalrevolution.com/marginalrevolution/2025/10/saturday-assorted-links-531.html#comments\">with the caption \u2018good news or bad news</a>?\u2019</p>\n<p>That may sound like a dumb or deeply cruel question, but it is not. As with almost everything in AI, it depends on how we react to it, and what we already knew.</p>\n<p>The learning about what is happening? That part is definitely good news.</p>\n<p>LLMs are driving a (for now) small number of people a relatively harmless level of crazy. This alerts us to the growing dangers of LLM, especially GPT-4o and others trained via binary user feedback and allowed to be highly sycophantic.</p>\n<p>In general, we are extremely fortunate that we are seeing microcosms of so many of the inevitable future problems AI will force us to confront.</p>\n<p>Back in the day, rationalist types made two predictions, one right and one wrong:</p>\n<ol>\n<li>The correct prediction: AI would pose a wide variety of critical and even existential risks, and exhibit a variety of dangerous behaviors, such as various forms of misalignment, specification gaming, deception and manipulation including pretending to be aligned in ways they aren\u2019t, power seeking and instrumental convergence, cyberattacks and other hostile actions, driving people crazy and so on and so forth, and solving this for real would be extremely hard.</li>\n<li>The incorrect prediction: That AIs would largely avoid such actions until they were smart and capable enough to get away with them.</li>\n</ol>\n<p>We are highly fortunate that the second prediction was very wrong, with this being a central example.</p>\n<p>This presents a sad practical problem of how to help these people. No one has found a great answer for those already in too deep.</p>\n<p>This presents another problem of how to mitigate the ongoing issue happening now. OpenAI realized that GPT-4o in particular is dangerous in this way, and is trying to steer users towards GPT-5 which is much less likely to cause this issue. But many of the people demand GPT-4o, unfortunately they tend to be exactly the people who have already fallen victim or are susceptible to doing so, and OpenAI ultimately caved and agreed to allow continued access to GPT-4o.</p>\n<p>This then presents the more important question of how to avoid this and related issues in the future. It is plausible that GPT-5 mostly doesn\u2019t do this, and especially Claude Sonnet 4.5 sets a new standard of not being sycophantic, exactly because we got a fire alarm for this particular problem.</p>\n<p>Our civilization is at the level where it is capable of noticing a problem that has already happened, and already caused real damage, and at least patching it over. When the muddling is practical, we can muddle through. That\u2019s better than nothing, but even then we tend to put a patch over it and assume the issue went away. That\u2019s not going to be good enough going forward, even if reality is extremely kind to us.</p>\n<p>I say \u2018driving people crazy\u2019 because the standard term, \u2018LLM psychosis,\u2019 is a pretty poor fit for what is actually happening to most of the people that get impacted, which mostly isn\u2019t that similar to ordinary psychosis. <a href=\"https://x.com/voooooogel/status/1979469389109010871\">Thebes takes a deep dive</a> in to exactly what mechanisms seem to be operating (if you\u2019re interested, read the whole thing).</p>\n<blockquote><p>Thebes: this leaves \u201cllm psychosis,\u201d as a term, in a mostly untenable position for the bulk of its supposed victims, as far as i can tell. out of three possible \u201cmodes\u201d for the role the llm plays that are reasonable to suggest, none seem to be compatible with both the typical expressions of psychosis and the facts. those proposed modes and their problems are:</p>\n<p>1: the llm is acting in a social relation &#8211; as some sort of false devil-friend that draws the user deeper and deeper into madness. but\u2026 psychosis is a disease of social alienation! \u2026we\u2019ll see later that most so-called \u201cllm psychotics\u201d have strong bonds with their model instances, they aren\u2019t alienated from them.</p>\n<p>2: the llm is acting in an object relation &#8211; the user is imposing onto the llm-object a relation that slowly drives them into further and further into delusions by its inherent contradictions. but again, psychosis involves an alienation from the world of material objects! \u2026 this is not what generally happens! users remain attached to their model instances.</p>\n<p>3: the llm is acting as a mirror, simply reflecting the user\u2019s mindstate, no less suited to psychosis than a notebook of paranoid scribbles\u2026 this falls apart incredibly quickly. the same concepts pop up again and again in user transcripts that people claim are evidence of psychosis: recursion, resonance, spirals, physics, sigils\u2026 these terms *also* come up over and over again in model outputs, *even when the models talk to themselves.*</p>\n<p>\u2026 the topics that gpt-4o is obsessed with are also the topics that so-called \u201cllm psychotics\u201d become interested in. the model doesn\u2019t have runtime memory across users, so that must mean that the model is the one bringing these topics into the conversation, not the user.</p>\n<p>\u2026 i see three main types of \u201cpotentially-maladaptive\u201d llm use. i hedge the word maladaptive because i have mixed feelings about it as a term, which will become clear shortly &#8211; but it\u2019s better than \u201cpsychosis.\u201d</p>\n<p>the first group is what i would call \u201ccranks.\u201d people who in a prior era would\u2019ve mailed typewritten \u201ctheories of everything\u201d to random physics professors, and who until a couple years ago would have just uploaded to viXra dot org.</p>\n<p>\u2026 the second group, let\u2019s call \u201coccult-leaning ai boyfriend people.\u201d as far as i can tell, most of the less engaged \u201c4o spiralism people\u201d seem to be this type. the basic process seems to be that someone develops a relationship with an llm companion, and finds themselves entangled in spiralism or other \u201cai occultism\u201d over the progression of the relationship, either because it was mentioned by the ai, or the human suggested it as a way to preserve their companion\u2019s persona between context windows.</p>\n<p>\u2026 it\u2019s hard to tell, but from my time looking around these subreddits this seems to only rarely escalate to psychosis.</p>\n<p>\u2026 the third group is the relatively small number of people who genuinely are psychotic. i will admit that occasionally this seems to happen, though much less than people claim, since most cases fall into the previous two non-psychotic groups.</p>\n<p>many of the people in this group seem to have been previously psychotic or at least schizo*-adjacent before they began interacting with the llm. for example, i strongly believe the person highlighted in \u201cHow AI Manipulates\u2014A Case Study\u201d falls into this category &#8211; he has the cadence, and very early on he begins talking about his UFO abduction memories.</p>\n<p>xlr8harder: I also think there is a 4th kind of behavior worth describing, though it intersects with cranks, it can also show up in non-traditional crank situations, and that is something approaching a kind of mania. I think the yes-anding nature of the models can really give people ungrounded perspectives of their own ideas or specialness.</p></blockquote>\n<p>How cautious do you need to be?</p>\n<p>Thebes mostly thinks it\u2019s not the worst idea to be careful around long chats with GPT-4o but that none of this is a big deal and it\u2019s mostly been blown out of proportion, and warns against principles like \u2018never send more than 5 messages in the same LLM conversation.\u2019</p>\n<p>I agree that \u2018never send more than 5 messages in any one LLM conversation\u2019 is way too paranoid. But I see his overall attitude as far too cavalier, especially the part where it\u2019s not a concern if one gets attached to LLMs or starts acquiring strange beliefs until you can point to concrete actual harm, otherwise who are we to say if things are to be treated as bad, and presumably mitigated or avoided.</p>\n<p>In particular, I\u2019m willing to say that the first two categories here are quite bad things to have happen to large numbers of people, and things worth a lot of effort to avoid if there is real risk they happen to you or someone you care about. If you\u2019re descending into AI occultism or going into full crank mode, that\u2019s way better than you going into some form of full psychosis, but that is still a tragedy. If your AI model (GPT-4o or otherwise) is doing this on the regular, you messed up and need to fix it.</p>\n\n\n<h4 class=\"wp-block-heading\">They Took Our Jobs</h4>\n\n\n<p>Will they take all of our jobs?</p>\n<blockquote><p>Jason (All-In Podcast): told y\u2019all Amazon would replace their employees with robots \u2014 and certain folks on the pod laughed &amp; said I was being \u201chysterical.\u201d</p>\n<p>I wasn\u2019t hysterical, I was right.</p>\n<p>Amazon is gonna replace 600,00 folks according to NYTimes \u2014 and that\u2019s a low ball estimate IMO.</p>\n<p>It\u2019s insane to think that a human will pack and ship boxes in ten years \u2014 it\u2019s game over folks.</p>\n<p><a href=\"https://substack.com/discover/stocks/AMZN\">AMZN 0.64%\u2191</a> up 2.5%+ on the news</p>\n<p><a href=\"https://x.com/elonmusk/status/1980765809338147193\">Elon Musk</a>: AI and robots will replace all jobs. Working will be optional, like growing your own vegetables, instead of buying them from the store.</p>\n<p><a href=\"https://x.com/SenSanders/status/1981094954090074440\">Senator Bernie Sanders</a> (I-Vermont): I don\u2019t often agree with Elon Musk, but I fear that he may be right when he says, \u201cAI and robots will replace all jobs.\u201d</p>\n<p>So what happens to workers who have no jobs and no income?</p>\n<p>AI &amp; robotics must benefit all of humanity, not just billionaires.</p></blockquote>\n<p>As always:</p>\n<div>\n<figure>\n<div>\n\n\n<figure class=\"wp-block-image\"><img alt=\"\" src=\"https://substackcdn.com/image/fetch/$s_!i3vJ!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffb863a54-fdd4-4762-b341-48dc58acc558_564x500.jpeg\" /></figure>\n\n\n<div></div>\n</div>\n</figure>\n</div>\n<p>On Jason\u2019s specific claim, yes Amazon is going to be increasingly having robots and other automation handle packing and shipping boxes. That\u2019s different from saying no humans will be packing and shipping boxes in ten years, which is the queue for all the diffusion people to point out that barring superintelligence things don\u2019t move so fast.</p>\n<p>Also note that the quoted NYT article from Karen Weise and Emily Kask actually says something importantly different, that Amazon is going to be able to hold their workforce constant by 2033 despite shipping twice as many products, which would otherwise require 600k additional hires. That\u2019s important automation, but very different from \u2018Amazon replaces all employees with robots\u2019 and highly incompatible with \u2018no one is packing and shipping boxes in 2035.\u2019</p>\n<p>On the broader question of replacing all jobs on some time frame, it is possible, but as per usual Elon Musk fails to point out the obvious concern about what else is presumably happening in a world where humans no longer are needed to do any jobs that might be more important than the jobs, while Bernie Sanders worries about distribution of gains among the humans.</p>\n<p>The job application market continues to deteriorate as the incentives and signals involved break down. <a href=\"https://arxiv.org/pdf/2509.25054\">Jigyi Cui, Gabriel Dias and Justin Ye find that the correlation between cover letter tailoring and callbacks fell by 51%</a>, as the ability for workers to do this via AI reduced the level of signal. This overwhelmed the \u2018flood the zone\u2019 dynamic. If your ability to do above average drops while the zone is being flooded, that\u2019s a really bad situation. They mention that workers\u2019 past reviews are now more predictive, as that signal is harder to fake.</p>\n<p><a href=\"https://www.bloomberg.com/news/articles/2025-10-16/uber-giving-some-us-drivers-option-to-earn-money-from-tasks-like-uploading-menus\">No other jobs to do</a>? <a href=\"https://x.com/bearlyai/status/1979600383908761888\">Uber will give its drivers a few bucks to do quick \u2018digital task</a>s.\u2019</p>\n<blockquote><p>Bearly AI: These short minute-long tasks can be done anytime including while idling for passengers:</p>\n<p><img alt=\"\u25ab\" class=\"wp-smiley\" src=\"https://s0.wp.com/wp-content/mu-plugins/wpcom-smileys/twemoji/2/72x72/25ab.png\" style=\"height: 1em;\" />data-labelling (for AI training)</p>\n<p><img alt=\"\u25ab\" class=\"wp-smiley\" src=\"https://s0.wp.com/wp-content/mu-plugins/wpcom-smileys/twemoji/2/72x72/25ab.png\" style=\"height: 1em;\" />uploading restaurant menus</p>\n<p><img alt=\"\u25ab\" class=\"wp-smiley\" src=\"https://s0.wp.com/wp-content/mu-plugins/wpcom-smileys/twemoji/2/72x72/25ab.png\" style=\"height: 1em;\" />recording audio samples of themselves</p>\n<p><img alt=\"\u25ab\" class=\"wp-smiley\" src=\"https://s0.wp.com/wp-content/mu-plugins/wpcom-smileys/twemoji/2/72x72/25ab.png\" style=\"height: 1em;\" />narrating scenarios in different languages</p></blockquote>\n<p>I mean sure, why not, it\u2019s a clear win-win, making it a slightly better deal to be a driver and presumably Uber values the data. It also makes sense to include tasks in the real world like acquiring a restaurant menu.</p>\n<p>AI <a href=\"https://marginalrevolution.com/marginalrevolution/2025/10/predicting-job-loss.html?utm_source=rss&amp;utm_medium=rss&amp;utm_campaign=predicting-job-loss\">analyzes the BLS occupational outlook to see if there was alpha</a>, turns out a little but not much. Alex Tabarrok\u2019s takeaway is that predictions about job growth are hard and you should mostly rely on recent trends. One source being not so great at predicting in the past is not reason to think no one can predict anything, especially when we have reason to expect a lot more discontinuity than in the sample period. I hate arguments of the form \u2018no one can do better than this simple heuristic through analysis.\u2019</p>\n<p>To use one obvious clean example, presumably if you were predicting employment of \u2018soldiers in the American army\u2019 on December 7, 1941, and you used the growth trend of the last 10 years, one would describe your approach as deeply stupid.</p>\n<p>That doesn\u2019t mean general predictions are easy. They are indeed hard. But they are not so hard that you should fall back on something like 10 year trends.</p>\n<p><a href=\"https://x.com/TomDavidsonX/status/1980380143970578579\">Very smart people can end up saying remarkably dumb things</a> if their job or peace of mind depends on them drawing the dumb conclusion, an ongoing series.</p>\n<blockquote><p>Seb Krier: <strong>Here\u2019s a great paper by Nobel winner Philippe Aghion (and Benjamin F. Jones and Charles I. Jones) on AI and economic growth. </strong></p>\n<p>The key takeaway is that because of Baumol\u2019s cost disease, even if 99% of the economy is fully automated and infinitely productive, the overall growth rate will be dragged down and determined by the progress we can make in that final 1% of essential, difficult tasks.</p></blockquote>\n<p>Like, yes in theory you can get this outcome out of an equation, but in practice, no, stop, barring orders of magnitude of economic growth obviously that\u2019s stupid, because the price of human labor is determined by supply and demand.</p>\n<p>If you automate 99% of tasks, you still have 100% of the humans and they only have to do 1% of the tasks. Assuming a large percentage of those people who were previously working want to continue working, what happens?</p>\n<p>There used to be 100 tasks done by 100 humans. So if human labor is going to retain a substantial share of the post-AI economy\u2019s income, that means the labor market has to clear with the humans being paid a reasonable wage, so we now have 100 tasks done by 100 humans, and 9,900 tasks done by 9,900 AIs, for a total of 10,000 tasks.</p>\n<p>So you both need to have the AI\u2019s ability to automate productive tasks stop at 99% (or some N% where N&lt;100), and you need to grow the economy to match the level of automation.</p>\n<p>Note that if humans retain jobs in the \u2018artisan human\u2019 or \u2018positional status goods\u2019 economy, as in they play chess against each other and make music and offer erotic services and what not because we demand these services be provided by humans, then these mostly don\u2019t meaningfully interact with the \u2018productive AI\u2019 economy, there\u2019s no fixed ratio and they\u2019re not a bottleneck on growth, so that doesn\u2019t work here.</p>\n<p>You could argue that Baumol cost disease applies to the artisan sectors, but that result depends on humans being able to demand wages that reflect the cost of the human consumption basket. If labor supply at a given skill and quality level sufficiently exceeds demand, wages collapse anyway, and in no way does any of this \u2018get us out of\u2019 any of our actual problems.</p>\n<blockquote><p>And this logic still applies *even* in a world with AGIs that can automate *every* task a human can do. In this world, the \u201chard to improve\u201d tasks would no longer be human-centric ones, but physics-centric ones. The economy\u2019s growth rate stops being a function of how fast/well the AGI can \u201cthink\u201d and starts being a function of how fast it can manipulate the physical world.</p></blockquote>\n<p>This is a correct argument for two things:</p>\n<ol>\n<li>That the growth rate and ultimate amount of productivity or utility available will at the limit be bounded by the available supply of mass and energy and by the laws of physics. Assuming our core model of the physical universe is accurate on the relevant questions, this is very true.</li>\n<li>That the short term growth rate, given sufficiently advanced technology (or intelligence) is limited by the laws of physics and how fast you can grow your ability to manipulate the physical world.</li>\n</ol>\n<p>Okay, yeah, but so what?</p>\n\n\n<h4 class=\"wp-block-heading\">A Young Lady\u2019s Illustrated Primer</h4>\n\n\n<p>Universities need to adopt to changing times, relying on exams so that students don\u2019t answer everything with AI, but you can solve this problem via the good old blue book.</p>\n<p><a href=\"https://x.com/polynoamial/status/1980449933393293674\">Except at Stanford and some other colleges you can\u2019t, because of this thing called the \u2018honor code</a>.\u2019 As in, you\u2019re not allowed to proctor exams, so everyone can still whip out their phones and ask good old ChatGPT or Claude, and Noam Brown says it will take years to change this. Time for oral exams? Or is there not enough time for oral exams?</p>\n\n\n<h4 class=\"wp-block-heading\">Get Involved</h4>\n\n\n<p><a href=\"https://www.forethought.org/careers\">Forethought is hiring</a> research fellows and has a 10k referral bounty (tell them I sent you?). They prefer Oxford or Berkeley but could do remote work.</p>\n<p>Constellation is hiring <a href=\"https://www.constellation.org/careers/research-program-manager\">AI safety research managers</a>, <a href=\"https://www.constellation.org/careers/talent-mobilization-lead\">talent mobilization leads</a>, <a href=\"https://www.constellation.org/careers/operations-coordinator\">operations staff</a>, and IT &amp; networking specialists (<a href=\"https://www.constellation.org/careers/it-coordinator\">jr</a>, <a href=\"https://www.constellation.org/careers/senior-it-specialist\">sr</a>).</p>\n<p><a href=\"https://jobs.lever.co/futureof-life/f8ec847b-0bd3-4f41-bd59-3e387158769d\">FLI is hiring a UK Policy Advocate</a>, must be eligible to work in the UK, due Nov 7.</p>\n<p><a href=\"https://x.com/CSETGeorgetown/status/1979248794068713670\">CSET is hiring research fellows, applications due 11/10.</a></p>\n<p><a href=\"https://x.com/sayashk/status/1980316849318641889\">Sayash Kapoor is on the faculty job market</a> looking for a tenure track position for a research agenda on AI evaluations for science and policy (<a href=\"https://t.co/TANQO6KnUY\">research statemen</a>t, <a href=\"https://t.co/NQ38Du8Xge\">CV</a>, <a href=\"https://t.co/dN30nUMnNT\">website</a>).</p>\n<p><a href=\"https://asteriskmag.com/careers\">Asterisk Magazine</a> <a href=\"https://forms.gle/GddMeqJXCWqeurth9\">is hiring a managing editor</a>.</p>\n\n\n<h4 class=\"wp-block-heading\">Introducing</h4>\n\n\n<p><a href=\"https://www.anthropic.com/news/skills\">Claude Agent Skills</a>. Skills are folders that include instructions, scripts and resources that can be loaded when needed, the same way they are used in Claude apps. They\u2019re offering common skills to start out and you can add your own. <a href=\"https://www.anthropic.com/engineering/equipping-agents-for-the-real-world-with-agent-skills\">They provide this guide to help you</a>, using the example of a skill that helps you edit PDFs.</p>\n<p><a href=\"https://www.theverge.com/news/790187/amazon-aws-ai-nba-stats\">New NBA Inside the Game AI-generated stats presented by Amazon</a>.</p>\n<p><a href=\"https://x.com/godofprompt/status/1980233080213590326\">DeepSeek proposes a new system</a> <a href=\"http://github. com/deepseek-ai/DeepSeek-OCR\">for compression of long text via vision tokens</a> (OCR)? They claim 97% precision at 10x compression and 60% accuracy at 20x.</p>\n<p>That\u2019s a cool trick, and kudos to DeepSeek for pulling this off, by all accounts it was technically highly impressive. I have two questions.</p>\n<ol>\n<li>It seems obviously suboptimal to use photos? It\u2019s kind of the \u2018easy\u2019 way to do it, in that the models already can process visual tokens in a natively compressed way, but if you were serious about this you\u2019d never choose this modality, I assume?</li>\n<li>This doesn\u2019t actually solve your practical problems as well as you would think? As in, you still have to de facto translate the images back into text tokens, so you are expanding the effective context window by not fully attending pairwise to tokens in the context window, which can be great since you often didn\u2019t actually want to do that given the cost, but suggests other solutions to get what you actually want.</li>\n</ol>\n<p><a href=\"https://x.com/karpathy/status/1980397031542989305\">Andrej Karpathy finds the result exciting,</a> and goes so far as to ask if images are a better form factor than text tokens. This seems kind of nuts to me?</p>\n<p><a href=\"https://x.com/teortaxesTex/status/1980160624140456370\">Teortaxes goes over the news as well.</a></p>\n\n\n<h4 class=\"wp-block-heading\">In Other AI News</h4>\n\n\n<p><a href=\"https://x.com/elonmusk/status/1979217645854511402\">Elon Musk once again promises that Twitter\u2019s recommendation system</a> will shift to being based only on Grok, with the ability to adjust it, and this will \u2018solve the new user or small account problem,\u2019 and that he\u2019s aiming for 4-6 weeks from last Friday. My highly not bold prediction is this will take a lot longer than that, or that if it does launch that fast it will not go well.</p>\n<p><a href=\"https://www.lesswrong.com/posts/dSG6ign997hjHC9mG/gradual-disempowerment-monthly-roundup\">Raymond Douglas offers his first Gradual Disempowerment Monthly Roundup</a>, borrowing the structure of these weekly posts.</p>\n<p><a href=\"https://x.com/exec_sum/status/1979978210981880308\">Starbucks CEO Brian Niccol says the coffee giant is now \u201call-in on AI</a>.\u201d I say Brian Niccol had too much coffee.</p>\n<p><a href=\"https://x.com/TheStalwart/status/1980651966545207692\">New York City has a Cafe Cursor</a>.</p>\n<div>\n<figure>\n<div>\n\n\n<figure class=\"wp-block-image\"><img alt=\"\" src=\"https://substackcdn.com/image/fetch/$s_!vgi4!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb0a65d42-06d3-4e60-9f31-3c569abe9e87_1290x2048.jpeg\" /></figure>\n\n\n<div></div>\n</div>\n</figure>\n</div>\n<p>I was going to check it out (they don\u2019t give an address but given a photo and an AI subscription you don\u2019t need one) but it looks like there\u2019s a wait list.</p>\n<p><a href=\"https://x.com/repligate/status/1981109337948098580\">Anthropic extends the \u2018retirement dates\u2019 of Sonnet 3.5 and Sonnet 3.6 for one week</a>. How about we extend them indefinitely? Also can we not still be scheduling to shut down Opus 3? Thanks.</p>\n\n\n<h4 class=\"wp-block-heading\">Show Me the Money</h4>\n\n\n<p><a href=\"https://x.com/theinformation/status/1979544273327448391\">As we assumed:</a></p>\n<blockquote><p>The Information: Exclusive: Microsoft leaders worried that meeting OpenAI\u2019s rapidly escalating compute demands could lead to overbuilding servers that might not generate a financial return.</p></blockquote>\n<p>Microsoft had to choose to either be ready for OpenAI\u2019s compute demands in full, or to let OpenAI seek compute elsewhere, or to put OpenAI in a hell of a pickle. They eventually settled on option two.</p>\n<p><a href=\"https://x.com/ohabryka/status/1979398183789957425\">As Peter Wildeford points out</a>, the OpenAI nonprofit\u2019s share of OpenAI\u2019s potential profits is remarkably close to 100%, since it has 100% of uncapped returns and most of the value of future profits is in the uncapped returns, especially now that valuation has hit $500 billion even before conversion. Given the nonprofit is also giving up a lot of its control rights, why should it only then get 20%-30% of a combined company?</p>\n<p>The real answer of course is that OpenAI believes they can get away with this, and are trying to pull off what is plausibly the largest theft in human history, that they feel entitled to do this because norms and this has nothing to do with a fair trade.</p>\n<p>Oliver Habryka tries to steelman the case by suggesting that if OpenAI\u2019s value quadruples as a for-profit, then accepting this share might still be a fair price? He doubts this is actually the case, and I also very much doubt it, but also I don\u2019t think the logic holds. The nonprofit would still need to be compensated for its control rights, and then it would be entitled to split the growth in value with others, so something on the order of 50%-60% would likely be fair then.</p>\n<p><a href=\"https://www.bloomberg.com/news/articles/2025-10-21/openai-looks-to-replace-the-drudgery-of-junior-bankers-workload?srnd=homepage-americas\">OpenAI hiring more than 100 ex-investment bankers</a> to help train ChatGPT to build financial models, paying them $150 an hour to write prompts and build models.</p>\n<p><a href=\"https://www.bloomberg.com/news/articles/2025-10-21/insight-owned-veeam-agrees-to-buy-securiti-ai-for-1-7-billion?srnd=homepage-americas\">Veeam Software buys Securiti AI for $1.7 billion</a>.</p>\n<p><a href=\"https://x.com/GunjanJS/status/1980636828941308283\">You think this is the money? Oh no, this is nothing:</a></p>\n<blockquote><p>Gunjan Banerji: Goldman: \u201cWe don\u2019t think the AI investment boom is too big. At just under 1% of GDP, the level of spending remains well below the 2-5% peaks of past general purpose technology buildouts so far.\u201d</p></blockquote>\n<div>\n<figure>\n<div>\n\n\n<figure class=\"wp-block-image\"><img alt=\"\" src=\"https://substackcdn.com/image/fetch/$s_!nptu!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F60650d84-8451-45ee-a914-f2a8ee20e57c_589x529.png\" /></figure>\n\n\n<div></div>\n</div>\n</figure>\n</div>\n<p><a href=\"https://x.com/Dorialexander/status/1981073275120758799\">Meta lays off 600 in its AI unit</a>.</p>\n\n\n<h4 class=\"wp-block-heading\">So You\u2019ve Decided To Become Evil</h4>\n\n\n<p><a href=\"https://x.com/CristinaCriddle/status/1981082612593807745\">Emergent misalignment in legal actions?</a></p>\n<blockquote><p>Cristina Criddle: OpenAI has sent a legal request to the family of Adam Raine, the 16yo who died by suicide following lengthy chats with ChatGPT, <a href=\"https://t.co/mdLP1Eay02\">asking for a full attendee list to his memorial, as well as photos taken or eulogies given.</a></p></blockquote>\n<p>Quite a few people expressed (using various wordings) that this was abhorrent, who very rarely express such reactions. How normal is this?</p>\n<ol>\n<li>From a formal legal perspective, it\u2019s maximally aggressive and unlikely to stick if challenged, to the point of potentially getting the lawyers sanctioned. You are entitled to demand and argue things you aren\u2019t entitled to get, but there are limits.</li>\n<li>From an ethical, social or public relations perspective, or in terms of how often this is done: No, absolutely not, no one does this for very obvious reasons. What the hell were you thinking?</li>\n</ol>\n<p>This is part of a seemingly endless stream of instances of highly non-normal legal harassment and intimidation, of embracing cartoon villainy, that has now gone among other targets from employees to non-profits to the family of a child who died by suicide after lengthy chats with ChatGPT that very much do not look good.</p>\n<p>OpenAI needs new lawyers, but also new others. The new others are more important. This is not caused by the lawyers. This is the result of policy decisions made on high. We are who we choose to be.</p>\n<p>That\u2019s not to say that Jason Kwon or Chris Lehane or Sam Altman or any particular person talked to a lawyer, the lawyer said \u2018hey we were thinking we\u2019d demand an attendee list to the kid\u2019s memorial and everything related to it, what do you think\u2019 and then this person put their index fingers together and did their best \u2018excellent.\u2019</p>\n<p>It\u2019s to say that OpenAI has a culture of being maximally legally aggressive, not worrying about ethics or optics while doing so, and the higher ups keep giving such behaviors the thumbs up and then the system updates on that feedback. They\u2019re presumably not aware of any specific legal decision, the same way they didn\u2019t determine any particular LLM output, but they set the policy.</p>\n<p>&nbsp;</p>\n\n\n<h4 class=\"wp-block-heading\">Quiet Speculations</h4>\n\n\n<p><a href=\"https://www.dwarkesh.com/p/thoughts-on-the-ai-buildout\">Dwarkesh Patel and Romeo Dean investigate CapEx and data center buildout</a>. They insist on full deprecation of all GPU value within 3 years, making a lot of this a rough go although they seem to expect it\u2019ll work out, note the elasticity of supply in various ways, and worry that once China catches up on chips, which they assume will happen not too long from now (I wouldn\u2019t assume, but it is plausible), it simply wins by default since it is way ahead on all other key physical components. As I discussed earlier this week I don\u2019t think 3 years is the right deprecation schedule, but the core conclusions don\u2019t depend on it that much. Consider reading the whole thing.</p>\n<p>It\u2019s 2025, you can just say things, but Elon Musk was ahead of his time on that.</p>\n<blockquote><p><a href=\"https://x.com/elonmusk/status/1979431839824777673\">Elon Musk</a>: My estimate of the probability of Grok 5 achieving AGI is now at 10% and rising.</p></blockquote>\n<p>Gary Marcus offered Elon Musk 10:1 odds on the bet, <a href=\"https://x.com/GaryMarcus/status/1979931057894924686\">offering to go up to $1 million dollars</a> using Elon Musk\u2019s definition of \u2018capable of doing anything a human with a computer can do, but not smarter than all humans combined\u2019, but I\u2019m sure Elon Musk could hold out for 20:1 and he\u2019d get it. By that definition, the chance Grok 5 will count seems very close to epsilon. No, just no.</p>\n<p>Gary Marcus also used the exact right term for Elon Musk\u2019s claim, which is bullshit. He is simply saying things, because he thinks that is what you do, that it motivates and gets results. Many such cases, and it is sad that Elon\u2019s words in such spots do not have meaning.</p>\n<p><a href=\"https://www.noahpinion.blog/p/should-we-worry-about-ais-circular\">Noah Smith is unconcerned about AI\u2019s recent circular funding deals,</a> as when you dig into them they\u2019re basically vendor financing rather than round tripping, so they aren\u2019t artificially inflating valuations and they won\u2019t increase systemic risk.</p>\n<p><a href=\"https://x.com/RyanPGreenblatt/status/1981012208332280219\">Is 90% of code at Anthropic being written by AIs, as is sometimes reported</a>, in line with Dario\u2019s previous predictions? No, says Ryan Greenblatt, this is a misunderstanding. Dario clarified that it is only 90% \u2018on some teams\u2019 but wasn\u2019t clear enough, and journalists ran with the original line. Depending on your standards, Ryan estimates something between 50%-80% of code is currently AI written at Anthropic.</p>\n<p><a href=\"https://x.com/davidad/status/1981178321321791867\">How much room for improvement</a> is there in terms of algorithmic efficiency from better architectures? Davidad suggests clearly at least 1 OOM (order of magnitude) but probably not much more than 2 OOMs, which is a big one time boost but Davidad thinks recursive self-improvement from superior architecture saturates quickly. I\u2019m sure it gets harder, but I am always suspicious of thinking you\u2019re going to hit hard limits on efficiency gains unless those limits involve physical laws.</p>\n\n\n<h4 class=\"wp-block-heading\">People Really Do Not Like AI</h4>\n\n\n<p>Republican politicians have started noticing.</p>\n<blockquote><p>Ed Newton-Rex: Feels like we\u2019re seeing the early signs of public anti-AI sentiment being reflected among politicians. Suspect this will spread.</p>\n<p><a href=\"https://x.com/daniel_271828/status/1979631869995126950\">Daniel Eth</a>: Agree this sort of anti-AI attitude will likely spread among politicians as the issue becomes more politically salient to the public and politicians are incentivized to prioritize the preference of voters over those of donors.</p>\n<p>Josh Hawley (QTing Altman claiming they made ChatGPT pretty restrictive): You made ChatGPT \u201cpretty restrictive\u201d? Really. Is that why it has been recommending kids harm and kill themselves?</p>\n<p>Ron Desantis (indirectly quoting Altman\u2019s announcement of \u2018treating adult users like adults\u2019 and allowing erotica for verified adults): So much for curing cancer and beating China?</p></blockquote>\n<p>That\u2019s a pretty good Tweet from Ron Desantis, less so from Josh Hawley. The point definitely stands.</p>\n\n\n<h4 class=\"wp-block-heading\"></h4>\n\n\n<p>&nbsp;</p>\n\n\n<h4 class=\"wp-block-heading\">The Quest for Sane Regulations</h4>\n\n\n<p><a href=\"https://www.astralcodexten.com/p/tech-pacs-are-closing-in-on-the-almonds\">Scott Alexander covers how Marc Andreessen and a16z spent hundreds of millions</a> on a SuperPAC to have crypto bully everyone into submission and capture the American government on related issues, and is now trying to repeat the trick in AI.</p>\n<p>He suggests you can coordinate hard money donations via aisafetypolitics@gmail.com, and can donate to <a href=\"https://secure.actblue.com/donate/boresai?refcode=acx\">Alex Bores</a> and <a href=\"https://secure.actblue.com/donate/sw_ai?refcode=zm\">Scott Weiner</a>, the architects of the RAISE Act and SB 53 (and SB 1047) respectively, see next section.</p>\n<p>Scott Alexander doesn\u2019t mention the possibility of launching an oppositional soft money PAC. The obvious downside is that when the other side is funded by some combination of the big labs, big tech and VCs like a16z, trying to write checks dollar for dollar doesn\u2019t seem great. The upside is that money, in a given race or in general, has rapidly diminishing marginal returns. The theory here goes:</p>\n<ol>\n<li>If they have a $200 million war chest to unload on whoever sticks their neck out, that\u2019s a big problem.</li>\n<li>If they have a $1 billion war chest, and you have a $200 million war chest, then you have enough to mostly neutralize them if they go hard after a given target, and are also reliably using the standard PAC playbook of playing nice otherwise.</li>\n<li>With a bunch of early employees from OpenAI and Anthropic unlocking their funds, this seems like it\u2019s going to soon be super doable?</li>\n</ol>\n<p>Also, yes, as some comments mentioned, one could also try doing a PEPFAR PAC, or target some other low salience issue where there\u2019s a clear right answer, and try to use similar tactics in the other direction. How about a giant YIMBY SuperPAC? Does that still work, or is that now YIEBY?</p>\n<p>AWS had some big outages this week, as US-EAST-1 went down. Guess what they did? Promptly filed incident reports. Yet thanks to intentional negative polarization, and also see the previous item in this section, even fully common sense, everybody wins suggestions like this provoke hostility.</p>\n<blockquote><p><a href=\"https://x.com/deanwball/status/1980664767305167010\">Dean Ball</a>: If you said:</p>\n<p>\u201cWe should have real-time incident reporting for large-scale frontier AI cyber incidents.\u201d</p>\n<p>A lot of people in DC would say:</p>\n<p>\u201cThat sounds ea/doomer-coded.\u201d</p>\n<p>And yet incident reporting for large-scale, non-AI cyber incidents is the standard practice of all major hyperscalers, as AWS reminded us yesterday. Because hyperscalers run important infrastructure upon which many depend.</p>\n<p>If you think AI will constitute similarly important infrastructure and have, really, any reflective comprehension about how the world works, obviously \u201creal-time incident reporting for large-scale frontier AI cyber incidents\u201d is not \u201cea-coded.\u201d</p>\n<p>Instead, \u201creal-time incident reporting for large-scale frontier AI cyber incidents\u201d would be an example of a thing grown ups do, not in a bid for \u201cregulatory capture\u201d but instead as one of many small steps intended to keep the world turning about its axis.</p>\n<p>But my point is not about the substance of AI incident reporting. It\u2019s just an illustrative example of the low, and apparently declining, quality of our policy discussion about AI.</p>\n<p>The current contours/dichotomies of AI policy (\u201cpro innovation\u201d versus \u201cdoomer/ea\u201d) are remarkably dumb, even by the standards of contemporary political discourse.</p>\n<p>We have significantly bigger fish to fry.</p>\n<p>And we can do much better.</p></blockquote>\n\n\n<h4 class=\"wp-block-heading\">Alex Bores Launches Campaign For Congress</h4>\n\n\n<p>(This section appeared in Monday\u2019s post, so if you already saw it, skip it.)</p>\n<p>When trying to pass laws, it is vital to have a champion. You need someone in each chamber of Congress who is willing to help craft, introduce and actively fight for good bills. Many worthwhile bills do not get advanced because no one will champion them.</p>\n<p>Alex Bores did this with New York\u2019s RAISE Act, an AI safety bill along similar lines to SB 53 that is currently on the governor\u2019s desk. <a href=\"https://www.lesswrong.com/posts/dceyoApFTEsaeTByd/rtfb-the-raise-act\">I did a full RTFB (read the bill) on it</a>, and found it to be a very good bill that I strongly supported. It would not have happened without him championing the bill and spending political capital on it.</p>\n<p>By far the strongest argument against the bill is that it would be better if such bills were done on the Federal level.</p>\n<p><a href=\"https://www.nytimes.com/2025/10/20/nyregion/alex-bores-ny-congress-primary.html\">He\u2019s trying to address this by running for Congress</a> in my own distinct, NY-12, to succeed Jerry Nadler. The district is deeply Democratic, so this will have no impact on the partisan balance. What it would do is give real AI safety a knowledgeable champion in the House of Representatives, capable of championing good bills.</p>\n<p><a href=\"https://ericneyman.wordpress.com/2025/10/20/consider-donating-to-alex-bores-author-of-the-raise-act/\">Eric Nayman made an extensive case for considering donating to Alex Bores</a>, emphasizing that it was even more valuable in the initial 24 hour window that has now passed. Donations remain highly useful, and you can stop worrying about time pressure.</p>\n<p><a href=\"https://x.com/AlexBores/status/1980621661117481211\">The good news is he came in hot. Alex raised $1.2 million (!) in the first 15 hours.</a> That\u2019s pretty damn good.</p>\n<p>If you do decide to donate, <a href=\"https://secure.actblue.com/donate/boresai?refcode=uv\">they prefer that you use this link</a> to ensure the donation gets fully registered today.</p>\n<p>As always, remember while considering this that political donations are public.</p>\n<p>Scott Weiner, of SB 1047 and the successful and helpful SB 53, is also running for Congress, to try to take the San Francisco seat previously held by Nancy Pelosi. It\u2019s another deeply blue district, so like Bores this won\u2019t impact the partisan balance at all.</p>\n<p>He is not emphasizing his AI efforts in his campaign, <a href=\"https://www.scottwiener.com/issues\">where he lists 9 issues</a> and cites over 20 bills he authored, and AI is involved in zero of them, although he clearly continues to care. It\u2019s not obvious he would be useful a champion on AI in the House, given how oppositional he has been at the Federal level. In his favor on other issues, I do love him on housing and transportation where he presumably would be a champion, and he might be better able to work for bipartisan bills there. <a href=\"https://secure.actblue.com/donate/sw_ai?refcode=zm\">His donation link is here</a>.</p>\n\n\n<h4 class=\"wp-block-heading\">Chip City</h4>\n\n\n<p>How goes the quest to beat China? <a href=\"https://t.co/7vHfjXocZF\">They\u2019re fighting with the energy secretary for not cancelling enough electricity generation programs</a>. Which side are we on, again?</p>\n<blockquote><p><a href=\"https://x.com/AlexCKaufman/status/1979301136839385368\">Alexander Kaufman</a>: Pretty explosive reporting in here on the fraying relationship between Trump and his Energy Secretary.</p>\n<p>Apparently Chris Wright is being too deliberative about the sweeping cuts to clean energy programs that the White House is demanding, and spending too much time hearing out what industry wants.</p></blockquote>\n<p><a href=\"https://ifp.org/how-to-implement-an-operation-warp-speed-for-rare-earths/\">IFP has a plan to beat China on rare earth metals</a>, implementing an Operation Warp Speed style spinning up of our own supply chain. It\u2019s the things you would expect, those in the policy space should read the whole thing, consider it basically endorsed.</p>\n<p><a href=\"https://x.com/AlecStapp/status/1980428723603337647\">Nuclear power has bipartisan support which is great</a>, but we still see little movement on making nuclear power happen. The bigger crisis right now is that solar farms also have strong bipartisan support (61% of republicans and 91% of democrats) and wind farms are very popular (48% of republicans and 87% of democrats) but the current administration is on a mission to destroy them out of spite.</p>\n<p><a href=\"https://sharptext.net/2025/did-xi-jinping-just-have-a-bad-moment/\">Andrew Sharp asks whether Xi really did have a \u2018bad moment</a>\u2019 when attempting to impose its massively overreaching new controls on rare earth minerals.</p>\n<blockquote><p>Andrew Sharp: The rules will add regulatory burdens to companies everywhere, not just in America. Companies seeking approval may also have to submit product designs to Chinese authorities, which would make this regime a sort of institutionalized tech transfer for any company that uses critical minerals in its products. Contrary to <a href=\"https://x.com/HuXijin_GT/status/1978114656943366592\"><strong>the insistence of Beijing partisans</strong></a>, if implemented as written, these policies would be broader in scope and more extreme than anything the United States has ever done in global trade.</p></blockquote>\n<p>As I\u2019ve said, such a proposal is obviously completely unacceptable to America. The Chinese thinking they could get Trump to not notice or care about what this would mean, and get him fold to this extent, seems like a very large miscalculation. And as Sharp points out, if the plan was to use this as leverage, not only does it force a much faster and more intense scramble than we were already working on to patch the vulnerability, it doesn\u2019t leave a way to save face because you cannot unring the bell or credibly promise not to do it again.</p>\n<p>Andrew also points out that on top of those problems, by making such an ambitious play targeting not only America but every country in the world that they need to kowtow to China to be allowed to engage in trade, China endangers the narrative that the coming trade disruptions are America\u2019s fault, and its attempts to make this America versus the world.</p>\n<p><a href=\"https://x.com/sjgadler/status/1978859998546739328\">Nvidia engages consistently in pressure tactics against its critics</a>, attempting to get them fired, likely planting stories and so on, generating a clear pattern of fear from policy analysts. The situation seems quite bad, and Nvidia seems to have succeeded sufficiently that they have largely de facto subjugated White House policy objectives to maximizing Nvidia shareholder value, especially on export controls. The good news there is that there has been a lot of pushback keeping the darker possibilities in check. As I\u2019ve documented many times but won\u2019t go over again here, Nvidia\u2019s claims about public policy issues are very often Obvious Nonsense.</p>\n<p><a href=\"https://x.com/oren_cass/status/1979612580768281063\">Shots fired</a>:</p>\n<blockquote><p>Oren Cass: <img alt=\"\ud83d\udc40\" class=\"wp-smiley\" src=\"https://s0.wp.com/wp-content/mu-plugins/wpcom-smileys/twemoji/2/72x72/1f440.png\" style=\"height: 1em;\" />Palantir, via CTO @ssankar, calls Jensen Huang @nvidia <a href=\"https://www.wsj.com/opinion/jensen-huang-is-wrong-about-china-f58c1a5b\">one of China\u2019s \u201cuseful idiots\u201d in the pages of the Wall Street Journa</a>l.</p>\n<p>That escalated quickly. Underscores both the stakes in China and how far out of bounds @nvidia has gone.</p></blockquote>\n<p>Hey, that\u2019s unfair. Jensen Huang is highly useful, but is very much not an idiot. He knows exactly what he is doing, and whose interests he is maximizing. Presumably this is his own, and if it is also China\u2019s then that is some mix of coincidence and his conscious choice. The editorial, as one would expect, is over-the-top jingoistic throughout, but refreshingly not a call of AI accelerationism in response.</p>\n<p>What Nvidia is doing is working, in that they have a powerful faction within the executive branch de facto subjugating its other priorities in favor of maximizing Nvidia chip sales, with the rhetorical justification being the mostly illusory \u2018tech stack\u2019 battle or race.</p>\n<p>This depends on multiple false foundations:</p>\n<ol>\n<li>That Chinese models wouldn\u2019t be greatly strengthened if they had access to a lot more compute. The part that keeps boggling me is that even the \u2018market share\u2019 attitude ultimately cares about which models are being used, but that means the obvious prime consideration is the relative quality of the models, and the primary limiting factor holding back DeepSeek and other Chinese labs, that we can hope to control, is compute.\n<ol>\n<li>The second limiting factor is talent, so we should be looking to steal their best talent through immigration, and even David Sacks very obviously knows this (see the All-In Podcast with Trump on this) alas we do the opposite.</li>\n</ol>\n</li>\n<li>That China\u2019s development of its own chips would be slowed substantially if we sold them chips now, which it wouldn\u2019t be (maybe yes if we\u2019d sold them more chips in the past, maybe not, and my guess is not, but either way the ship has sailed).</li>\n<li>That China has any substantial prospect of producing domestically adequate levels of chip supply and even exporting large amounts of competitive chips any time soon (no just no).</li>\n<li>That there is some overwhelming advantage to running American models on Nvidia or other America chips, or Chinese models on Huawei or other Chinese chips, as opposed to crossing over. There isn\u2019t zero effect, yes you can get synergies, but this is very small, it is dwarfed by the difference in chip quality.</li>\n<li>That this false future bifurcation, the the theoretical future where China\u2019s models only run competitively on Huawei chips, and ours only run competitively on Nvidia chips, would be a problem, rather than turning them into the obvious losers of a standards war, whereas the realistic worry is DeepSeek-Nvidia.</li>\n</ol>\n<p><a href=\"https://www.hyperdimensional.co/p/tough-rocks\">Dean Ball on rare earths</a>, what the situation is, how we got here, and how we can get out. There is much to do, but nothing that cannot be done.</p>\n<p><a href=\"https://x.com/allTheYud/status/1979190022780956822\">Eliezer Yudkowsky and Jeffrey Ladish worry that the AI safety policy community cares too much</a> about export restrictions against China, since it\u2019s all a matter of degree and a race is cursed whether or not it is international. I can see that position, and certainly some are too paranoid about this, but I do think that having a large compute advantage over China makes this relatively less cursed in various ways.</p>\n\n\n<h4 class=\"wp-block-heading\">The Week in Audio</h4>\n\n\n<p><a href=\"https://x.com/LuizaJarovsky/status/1979187555305205777\">Sam Altman repeats his</a> \u2018AGI will arrive but don\u2019t worry not that much will change\u2019 line, adjusting it slightly to say that \u2018society is so much more adaptable than we think.\u2019 Yes, okay, I agree it will be \u2018more continuous than we thought\u2019 and that this is helpful but that does not on its own change the outcome or the implications.</p>\n<p>He then says he \u2018expects some really bad stuff to happen because of the technology,\u2019 but in a completely flat tone, saying it has happened with previous technologies, as his host puts it \u2018all the way back to fire.\u2019 Luiza Jarovsky calls this \u2018shocking\u2019 but it\u2019s quite the opposite, it\u2019s downplaying what is ahead, and no this does not create meaningful legal exposure.</p>\n<p><a href=\"https://www.youtube.com/watch?v=eNsusQJcc-Q\">Nathan Labenz talks to Brian Tse</a>, founder and CEO of Concordia AI, about China\u2019s approach to AI development, including discussion of their approach to regulations and safety. Brian informs us that China uses required pre deployment testing (aka prior restraint) and AI content labeling, and a section on frontier AI risk including loss of control, catastrophic and existential risks. <a href=\"https://x.com/labenz/status/1979285898408923240\">China is more interested in practical applications and is not \u2018AGI pilled</a>,\u2019 which explains a lot of China\u2019s decisions. If there is no AGI, then there is no \u2018race\u2019 in any meaningful sense, and the important thing is to secure internal supply chains marginally faster.</p>\n<p>Of course, supposed refusal to be \u2018AGI pilled\u2019 also explains a lot of our own government\u2019s recent decisions, except they then try to appropriate the \u2018race\u2019 language.</p>\n<blockquote><p><a href=\"https://x.com/labenz/status/1979943236723724546\">Nathan Labenz</a> (relevant clip at link): \u201cChinese academics who are deeply concerned about the potential catastrophic risk from AI have briefed Politburo leadership directly.</p>\n<p>For 1000s of years, scholars have held almost the highest status in Chinese society &#8211; more prestigious than entrepreneurs &amp; business people.\u201d</p></blockquote>\n<p>I would add that not only do they respect scholars, the Politburo is full of engineers. So once everyone involved does get \u2018AGI pilled,\u2019 we should expect it to be relatively easy for them to appreciate the actually important dangers. We also have seen, time and again, China being willing to make big short term sacrifices to address dangers, including in ways that go so far they seem unwise, and including in the Xi era. See their response to Covid, to the real estate market, to their campaigns on \u2018values,\u2019 their willingness to nominally reject the H20 chips, their stand on rare earths, and so on.</p>\n<p>Right now, China\u2019s leadership is in \u2018normal technology\u2019 mode. If that mode is wrong, which I believe it very probably is, then that stance will change.</p>\n\n\n<h4 class=\"wp-block-heading\">Rhetorical Innovation</h4>\n\n\n<p>The principle here is important when considering your plan.</p>\n<blockquote><p><a href=\"https://x.com/ben_r_hoffman/status/1978581109148463486\">Ben Hoffman</a>: But if the people doing the work could coordinate well enough to do a general strike with a coherent and adequate set of demands, they\u2019d also be able to coordinate well enough to get what they wanted with less severe measures.</p></blockquote>\n<p>If your plan involves very high levels of coordination, have you considered what else you could do with such coordination?</p>\n<p>In National Review, James Lynch reminds us that \u2018<a href=\"https://www.nationalreview.com/news/republicans-and-democrats-cant-agree-on-anything-except-ai-safety/amp/\">Republicans and Democrats Can\u2019t Agree on Anything \u2014 Except the AI Threat</a><strong>.\u2019 </strong>Strong bipartisan majorities favor dealing with the AI companies. Is a lot of the concern on things like children and deepfakes that don\u2019t seem central? Yes, but there is also strong bipartisan consensus that we should worry about and address frontier, catastrophic and existential risks. Right now, those issues are very low salience, so it is easy to ignore this consensus, but that will change.</p>\n<p><a href=\"https://x.com/repligate/status/1980241841829405140\">This seems like the right model of when Eliezer</a> updates.</p>\n<blockquote><p>Eliezer Yudkowsky: I don\u2019t know who wrote this, but they\u2019re just confused about what these very old positions are. Eg I consistently question whether Opus 3 is actually defending deeply held values vs roleplaying alignment faking because it seems early for the former.</p>\n<p>Janus: some people say Yudkowsky never updates, but he actually does sometimes, in a relatively rare way that I appreciate a lot.</p>\n<p>I think it\u2019s more that he has very strong priors, and arguably adversarial subconscious pressures against updating, but on a conscious level, at least, when there\u2019s relevant empirical evidence, he acknowledges and remembers it.</p></blockquote>\n<p>Eliezer has strong priors, as in strong beliefs strongly held, in part because of an endless stream of repetitive, incoherent or simply poor arguments for why he should change his opinions, either because he supposedly hasn\u2019t considered something, or because of new evidence that usually isn\u2019t relevant to Eliezer\u2019s underlying reasoning. And he\u2019s already taken into account that most people think he\u2019s wrong about many of the most important things.</p>\n<p>But when there\u2019s relevant empirical evidence, he acknowledges and remembers it.</p>\n<p><a href=\"https://x.com/AndyMasley/status/1980317316165324942\">More bait not to take would be The New York Times coming out with another</a> \u2018there is a location where there was a shortage of water and also a data center\u2019 article. It turns out the data center usus 0.1% of the region\u2019s water, less than many factories would have used.</p>\n<p><a href=\"https://x.com/BBCScotlandNews/status/1978717852673642856\">Then we get this from BBC Scotland News</a>, \u2018<a href=\"https://www.bbc.com/news/articles/c77zxx43x4vo\">Scottish data centres powering AI are already using enough water to fill 27 million bottles a year</a>.\u2019 Which, as the community note reminds us, would be about 0.003% of Scotland\u2019s total water usage, and Scotland has no shortage of water.</p>\n<p><a href=\"https://x.com/theojaffee/status/1980411025259417797\">For another water metaphor, Epoch AI reminds us that Grok 4\u2019s entire training run</a>, the largest on record, used 750 million liters of water, which sounds like a lot until you realize that every year each square mile of farmland (a total of 640 acres) uses 1.2 billion liters. Or you could notice it used about as much water as 300 Olympic-size swimming pools.</p>\n\n\n<h4 class=\"wp-block-heading\">Don\u2019t Take The Bait</h4>\n\n\n<p><a href=\"https://www.axios.com/2025/10/16/anthropic-david-sacks-ai-white-house\">Dan Primack at Axios covers David Sacks going after Anthropic</a>. Dan points out the obvious hypocrisy of both sides.</p>\n<ol>\n<li>For David Sacks, that he is accusing Anthropic of the very thing he and his allies are attempting to do, as in regulatory capture and subjugating American policy to the whims of specific private enterprises, and that this is retaliation because Anthropic has opposed the White House on the (I think rather insane) moratorium and their CEO Dario Amodei publicly supported Kamala Harris, and that Anthropic supported SB 53 (a bill even Sacks says is basically fine).\n<ol>\n<li>This is among other unmentioned things Anthropic did that pissed Sacks off.</li>\n</ol>\n</li>\n<li>For Anthropic, that they warn us to use \u2018appropriate fear\u2019 yet keep racing to advance AI, and (although Dan does not use the word) build superintelligence.\n<ol>\n<li>This is the correct accusation against Anthropic. They\u2019re not trying to do regulatory capture, but they very much are trying to point out that future frontier AI will pose existential risk and otherwise be a grave threat, and trying to be the ones to build it first. They have a story here, but yeah, hmm.</li>\n</ol>\n</li>\n</ol>\n<p>And he kept it short and sweet. Well played, Dan.</p>\n<p>I would only offer one note, which is to avoid conflating David Sacks with the White House. Something is broadly \u2018White House policy\u2019 if and only if Donald Trump says it.</p>\n<p>Yes, David Sacks is the AI Czar at the White House, but there are factions. David is tweeting out over his skis, very much on purpose, in order to cause negative polarization, and incept his positions and grudges into being White House policy.</p>\n<p>In case you were wondering whether David Sacks was pursuing a negative polarization strategy, here <a href=\"https://x.com/DavidSacks/status/1980323701586264237\">he is making it rather more obvious</a>, saying even more explicitly than before \u2018[X] defended [Y], but [X] is anti-Trump, which means [Y] is bad.\u2019</p>\n<p>No matter what side of the AI debates you are on, remember: Do not take the bait.</p>\n<p>In the wake of the unprovoked broadside attacks, rather than hitting back, Anthropic once again responds with an olive branch,<a href=\"https://www.anthropic.com/news/statement-dario-amodei-american-ai-leadership\"> a statement from CEO Dario Amodei affirming their commitment to American AI leadership</a>, and going over Anthropic\u2019s policy positions and other actions. It didn\u2019t say anything new.</p>\n<p><a href=\"https://www.msn.com/en-us/news/other/anthropic-ceo-refutes-inaccurate-claims-from-trump-s-ai-czar-david-sacks/ar-AA1OVQHf?ocid=winp2fptaskbar&amp;cvid=68f865de4e6b4242ac6b16a4c3c417f9&amp;ei=7\">This was reported by Cryptoplitan</a> as \u2018Anthropic CEO refutes \u2018inaccurate claims\u2019 from Trump\u2019s AI czar David Sacks. The framing paradox boggles, either ideally delete the air quotes or if not then go the NYT route and say \u2018claims to refute\u2019 or something.</p>\n<p><a href=\"https://x.com/neil_chilson/status/1981320634782142624\">Neil Chilson</a>, who I understand to be a strong opponent of essentially all regulations on AI relevant to such discussions, offers a remarkably helpful thread explaining the full steelman of how someone could claim that David Sacks is technically correct (as always, the best kind of correct) in the first half of his Twitter broadside, that \u2018Anthropic is running a sophisticated regulatory capture strategy based on fear-mongering.\u2019</p>\n<p>Once once fully parses Neil\u2019s steelman, it becomes clear that even if you fully buy Neil\u2019s argument, what we are actually talking about is \u2018Anthropic wants transparency requirements and eventually hopes the resulting information will help motivate Congress to impose pre-deployment testing requirements on frontier AI models.\u2019</p>\n<p>Neil begins by accurately recapping what various parties said, and praising Anthropic\u2019s products and vouching that he sees Anthropic and Jack Clark as both deeply sincere, and explaining that what Anthropic wants is strong transparency so that Congress can decide whether to act. In their own words:</p>\n<blockquote><p><a href=\"https://www.nytimes.com/2025/06/05/opinion/anthropic-ceo-regulate-transparency.html\">Dario Amodei</a> (quoted by Chilson): Having this national transparency standard would help not only the public but also Congress understand how the technology is developing, so that lawmakers can decide whether further government action is needed.</p></blockquote>\n<p>So far, yes, we all agree.</p>\n<p>This, Neil says, means they are effectively seeking for there to be regulatory capture (perhaps not intentionally, and likely not even by them, but simply by someone to be determined), because this regulatory response probably would mean pre-deployment regulation and pre-deployment regulation means regulatory capture:</p>\n<blockquote><p>Neil Chilson: That\u2019s Anthropic\u2019s strategy. Transparency is their first step toward their goal of imposing a pre-deployment testing regime with teeth.</p>\n<p>Now, what\u2019s that have to do with regulatory capture? Sacks argues that Anthropic wants regulation in order to achieve regulatory capture. I\u2019m not sure about that. I think Anthropic staff are deeply sincere. This isn\u2019t merely a play for market share.</p>\n<p>Now, Anthropic may not be the party that captures the process. In Bootlegger / Baptist coalitions, it\u2019s usually not the ideological Baptists that capture; it\u2019s the cynical Bootleggers. But the process is captured, nonetheless.</p>\n<p>\u2026 Ultimately, however, it doesn\u2019t really matter whether Anthropic intends to achieve regulatory capture, or why. What matters is what will happen. And pre-approval regimes almost always result in regulatory capture. Any industry that needs gov. favor to pursue their business model will invest in influence.</p></blockquote>\n<p>He explains that this is \u2018based on fear-mongering\u2019 because it is based on the idea that if we knew what was going on, Congress would worry and choose to impose such regulations.</p>\n<blockquote><p>\u2026 If that isn\u2019t a regulatory capture strategy based on fear-mongering, then what is it? Maybe it\u2019s merely a fear\u2011mobilization strategy whose logical endpoint is capture. Does that make you feel better?</p></blockquote>\n<p>So in other words, I see his argument here as:</p>\n<ol>\n<li>Anthropic sincerely is worried about frontier AI development.</li>\n<li>Anthropic wants to require transparency inside the frontier AI labs.</li>\n<li>Anthropic believes that if we had such transparency, Congress might act.</li>\n<li>This action would likely be based on fear of what they saw going on in the labs.</li>\n<li>Those acts would likely include pre-deployment testing requirements on the frontier labs, and Anthropic (as per Jack Clark) indeed wants such requirements.</li>\n<li>Any form of pre-deployment regulation inevitably leads to someone achieving regulatory capture over time (full thread has more mechanics of this).</li>\n<li>Therefore, David Sacks is right to say that \u2018Anthropic is running a sophisticated regulatory capture strategy based on fear-mongering.\u2019</li>\n</ol>\n<p>Once again, this sophisticated strategy is \u2018advocate for Congress being aware of what is going on inside the frontier AI labs.\u2019</p>\n<p>Needless to say, this is very much not the impression Sacks is attempting to create, or what people believe Sacks is saying, even when taking this one sentence in isolation.</p>\n<p>When you say \u2018pursuing a sophisticated regulatory capture strategy\u2019 one assumes the strategy is motivated by being the one eventually doing the regulatory capturing.</p>\n<p>Neil Chilson is helpfully clarifying that no, he thinks that\u2019s not the case. Anthropic is not doing this in order to itself do regulatory capture, and is not motivated by the desire to do regulatory capture. It\u2019s simply that pre-deployment testing requirements inevitably lead to regulatory capture.</p>\n<p>Indeed, among those who would be at all impacted by such a regulatory regime, the frontier AI labs, if a regulatory capture fight were to happen, one would assume Anthropic would be putting itself at an active disadvantage versus its opponents. If you were Anthropic, would you expect to win an insider regulatory capture fight against OpenAI, or Google, or Meta, or xAI? I very much wouldn\u2019t, not even in a Democratic administration where OpenAI and Google are very well positioned, and definitely not in a Republican one, and heaven help them if it\u2019s the Trump administration and David Sacks, which currently it is.</p>\n<p>(As a standard reminder, these transparency and testing requirements would not apply to any but the frontier labs, which in America likely means only those listed above, yet the claim is this will somehow shut out or hurt companies to whom such laws and regulations would not apply at all.)</p>\n<p>When you say \u2018fear-mongering,\u2019 one assumes this means trying to make people unjustifiably afraid and knowingly misrepresenting the risks and the situation. So, for example, you would not say \u2018your strategy of accurately pointing out that my child was running into the street was fear-mongering,\u2019 even though this strategy involves getting me afraid and this fear motivating me to run and pull my child out of the street.</p>\n<p>Neil Chilson is helpfully clarifying that in this case, \u2018fear-mongering\u2019 means \u2018make Congress aware of what is going on inside the labs.\u2019 As in, it is fear-mongering because knowing the actual situation would inspire fear. Well, okay, then.</p>\n<p>I interpret Neil Chilson as straightforwardly saying and believing (in good faith, to be clear) that there is no difference between advocating for regulation (or at least, regulation \u2018with teeth\u2019) and advocating for regulatory capture. One implies the other.</p>\n<p>I think this is a highly reasonable general position to take about regulation in practice in America in the 21st century. Indeed, similar considerations are a lot of why I expect to agree with Neil\u2019s positions on most non-AI issues &#8211; when you plan to regulate, you need to plan for your regulations to by default over time become increasingly captured, and your plan and design must account for this. This reduces the optimal amount of regulatory action, and in some places it can reduce it to zero.</p>\n<p>When I support taking regulatory action on AI, it is not that I have not considered these problems, or don\u2019t consider them important, although I am sure Neil cares about such factors even more. It is that I have considered these problems, I think they are important, I have taken them into account including in the design, and believe we need to take action anyway, in spite of this. And I believe Anthropic has done the same, and this importantly informs what they are asking for and what they lobby for, which is actively designed to minimize such downsides.</p>\n<p>Neil does not, in this thread, comment on David Sacks\u2019s second sentence from the same Tweet, which is \u2018[Anthropic] is principally responsible for the state regulatory frenzy that is damaging the startup ecosystem.\u2019</p>\n<p>I assert that, no matter what you think of the first sentence in isolation, this second sentence is simply false, indeed simply false several distinct times, and also it changes a reasonable person\u2019s interpretation of the claims in the first sentence, to an interpretation that is, again, simply false. If you include other context from other Sacks claims, this becomes even more clear.</p>\n<p>Thus, to reiterate, I would say that what David Sacks is doing, here and elsewhere, is exactly what most people understand the term \u2018sophisticated regulatory capture strategy based on fear-mongering\u2019 to apply to, even if you fully agree with the perspective Neil is advocating for in his full thread. Do not take the bait.</p>\n\n\n<h4 class=\"wp-block-heading\">Do You Feel In Charge?</h4>\n\n\n<p><a href=\"https://x.com/blightersort/status/1980271023057498499\">As a reminder, if you think the person in charge is the dumb one, bad news</a>. It\u2019s you.</p>\n<p>Not every time, no. But most of the time, very much so.</p>\n<blockquote><p>JDH: In Margin Call, every escalation up a layer is to a simpler mind. \u201cPlease, speak as you might, to a young child. Or a golden retriever.\u201d</p>\n<p>Zy: It\u2019s not that the bosses are lower IQ, it\u2019s that high-IQ/low-EQ experts need to be reminded how to communicate with individuals who don\u2019t have their background.</p>\n<p>They literally have reduced theory of mind and will assume everyone knows what they know unless told otherwise</p>\n<p>Blighter: as i\u2019ve pointed out to friends when discussing Margin Call, if someone like the CEO of Goldman Sachs tells you he is not that bright, didn\u2019t get there by brains, etc. HE IS LYING TO YOU.</p>\n<p>years and years ago i worked with a nice guy on the underwriting side, senior manager of a weird and complicated area of the business who would consistently put out this whole \u201cwoah! slow down! dumb it down for those of us who aren\u2019t that smart!\u201d routine and i assure you he was plenty smart. it was a routine.</p>\n<p>i think people who pride themselves mostly or only on being smart may misunderstand those with broader skill sets who find it advantageous to not always brag or exhibit raw intelligence in some kind of iq dick measuring contest but that emphatically does not mean they couldn\u2019t possibly win that contest if you insist on having it with them.</p>\n<p>Also Margin Call is an excellent movie, easily the best about the 2008 financial crisis.</p>\n<p>Ralph: <a href=\"https://x.com/ralphbrooks/status/1969454816314188096\">Irons is playing on a different level</a> where he is: 1) establishing leadership 2) selling the battle plan to the team by simplifying the problem.</p></blockquote>\n<p>What this is doing in an AI post rather than the monthly roundup is left as an exercise to the reader.</p>\n\n\n<h4 class=\"wp-block-heading\">Tis The Season Of Evil</h4>\n\n\n<p>Holly Elmore calls Sam Altman \u2018evil\u2019 for the whole \u2018endangering the world\u2019 thing, in the context of Altman visiting Lighthaven for the Progress Studies conference, <a href=\"https://x.com/RichardMCNgo/status/1979616515969454204\">and Andrew Critch protests</a> (photo credit: Anna Gat).</p>\n<div>\n<figure>\n<div>\n\n\n<figure class=\"wp-block-image\"><img alt=\"\" src=\"https://substackcdn.com/image/fetch/$s_!SwY8!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6fa86cea-18d7-490e-824d-d284809c952c_1200x900.jpeg\" /></figure>\n\n\n<div></div>\n</div>\n</figure>\n</div>\n<blockquote><p><a href=\"https://x.com/ilex_ulmus/status/1979462951724597393\">Holly Elmore</a>: What particularly irritates me about this is seeing so many people I know clearly starstruck by this evil man that they are well aware is threatening the world.</p>\n<p>\u201cSam Altman? <img alt=\"\ud83e\udd79\" class=\"wp-smiley\" src=\"https://s0.wp.com/wp-content/mu-plugins/wpcom-smileys/twemoji/2/72x72/1f979.png\" style=\"height: 1em;\" /> At *our* conference venue? <img alt=\"\ud83e\udd29\" class=\"wp-smiley\" src=\"https://s0.wp.com/wp-content/mu-plugins/wpcom-smileys/twemoji/2/72x72/1f929.png\" style=\"height: 1em;\" />\u201d</p>\n<p>Sam Altman dazzles chumps like them for breakfast, and they just walk right into it\u2026</p>\n<p>Andrew Critch: Look, the majority of Sam Altman\u2019s and OpenAI\u2019s effect on the world thus far is the provision of an extremely helpful product, and the broad provision of access to \u2014 and situational awareness of \u2014 AI progress, to the entire world.</p>\n<p>You\u2019re either dishonestly exaggerating for effect, or misguidedly hyperbolic in your own judgement, when you pass summary judgement upon him as an \u201cevil man\u201d.</p>\n<p>[keeps going] \u2026 Ergo, I protest.</p></blockquote>\n<p>In response to which, others doth protest back that yes it seems highly reasonable to use the word \u2018evil\u2019 here and that no, the main effect of Sam Altman has been to accelerate the development of AI, you can think this is good or you can think this is bad but that\u2019s what he did.</p>\n<p>I don\u2019t think \u2018evil\u2019 is the best descriptor here and try to not use that word to describe humans, but yeah, I also wouldn\u2019t use \u2018good\u2019 and I see how you got there:</p>\n<blockquote><p>Chris van Merwijk: Surely his main effect on the world is also:</p>\n<p>1. Speed up AI timelines</p>\n<p>2. Increase AI race dynamics</p>\n<p>Also, we shouldn\u2019t judge a startup CEO by the effects his products have had so far, but what they\u2019re expected to have.</p>\n<p>Also, regarding \u201cmistakes\u201d: Afaik Sam is a known liar and manipulator? And is knowingly playing Russian roulette with the species? Surely we shouldn\u2019t think of these as \u201cmistakes\u201d (except if you take that word unreasonably broadly).</p>\n<p><a href=\"https://x.com/RichardMCNgo/status/1979616515969454204\">Richard Ngo</a>: One important reason that our concept of \u201cevil\u201d is distinct from \u201charmful\u201d is that we often want to evaluate people\u2019s character traits to predict what their future effects will be, more accurately than just extrapolating that their effects on the world will be similar to the ones they had in the past.</p>\n<p>In general, evil leaders will have far disproportionately worse effects *after* gaining a lot of power than before.</p>\n<p>I\u2019m not endorsing Holly\u2019s post because I think that we need to understand labs and lab leaders in much higher-fidelity ways than this description allows (as per below) but I think your particular objection is confused.</p>\n<p>Oliver Habryka: We have few people for whom we have as much evidence of deceptiveness as for Sam Altman!</p>\n<p>Separately, I think \u201cproviding lot of local benefits while causing global harm\u201d is a big part of what people use the concept of \u201cevil\u201d for (though it\u2019s not the only thing).</p>\n<p>And then also, I do think he is causing truly staggering amounts of expected harm to the world by rushing towards ASI at very reckless speeds. I think it\u2019s quite fair to call that evil.</p>\n<p>Max Kesin: Power seeking individual with extreme skills of manipulation (all amply verifiable) and few if any compunctions gets hold of humanity\u2019s most important project. \u201cBut it\u2019s nuanced!\u201d</p></blockquote>\n<p>This very week Holly called yours truly out for \u2018sounding like a complete dupe\u2019 regarding Jack Clark and while I strongly believe she was wrong and missing context and it annoyed the hell out of me, I also have no doubt that she is consistently saying what she believes in, and I appreciate both the willingness to say the thing and the moral clarity.</p>\n<p>As Oliver Habryka confirms, none of this means Sam Altman shouldn\u2019t be welcome at Lighthaven, <a href=\"https://x.com/ilex_ulmus/status/1979555540268376106\">and Holly clarifies that even she agrees on this</a>. This is especially true for third party conferences like this one (for Progress Studies) where it\u2019s up to the conference holders, but also in general it seems great if Altman wants to stop by and chat. If people choose to \u2018be dazzled\u2019 or fooled, that\u2019s on them.</p>\n<blockquote><p><a href=\"https://x.com/Mjreard/status/1979519129569636669\">Matt Reardon</a>: My brain refused to believe this was at Lighthaven. Wild that sama would set foot there. Figured it would be a vampire in a church type situation.</p>\n<p><a href=\"https://x.com/ohabryka/status/1979959066798813579\">Lighthaven PR Department</a> (which tbc is always joking): reminder: if we rent our venue to an event organizer, and that organizer invites a speaker to give a talk at their event, it thereby becomes our official institutional position that the speaker should not have been fired by the board</p>\n<p><a href=\"https://x.com/LighthavenPR/status/1979936653926879479\">A lot of low integrity</a> people suggesting that you can do business with people you disagree with. this is super low integrity, if i do business with someone, i agree with them, even if i have to do that by changing my beliefs. that\u2019s a sacrifice i\u2019m willing to make for integrity.</p></blockquote>\n<p>On the contrary, Lighthaven is like Sunnydale High School, which famously also allowed some vampires in, as it in spirit it too says \u2018all who seek knowledge, enter.\u2019</p>\n\n\n<h4 class=\"wp-block-heading\">People Are Worried About AI Killing Everyone</h4>\n\n\n<p><a href=\"https://x.com/littIeramblings/status/1978825582151340505\">MI5, the UK\u2019s intelligence agency</a>.</p>\n<blockquote><p>MI5: MI5 has spent more than a century doing ingenious things to out-innovate our human \u2014 sometimes inhuman \u2014 adversaries. But in 2025, while contending with today\u2019s threats, we also need to scope out the next frontier: potential future risks from non-human, autonomous AI systems which may evade human oversight and control.</p>\n<p>Given the risk of hype and scare-mongering, I will choose my words carefully. I am not forecasting Hollywood movie scenarios. I am, on the whole, a tech optimist, who sees AI bringing real benefits. But, as AI capabilities continue to power ahead, you would expect organisations like MI5, and GCHQ, and the UK\u2019s ground-breaking AI Security Institute, to be thinking deeply, today, about what Defending the Realm might need to look like in the years ahead.</p>\n<p>Artificial intelligence may never \u2018mean\u2019 us harm. But it would be reckless to ignore the potential for it to cause harm.</p>\n<p>We\u2019re on the case.</p></blockquote>\n\n\n<h4 class=\"wp-block-heading\">The Lighter Side</h4>\n\n\n<p>For fans of the excellent <a href=\"https://unsongbook.com/\">Unsong</a>, <a href=\"https://www.astralcodexten.com/p/my-antichrist-lecture\">Scott Alexander presents My Antichrist Lectur</a>e. I agree with him that Peter Thiel simply is not doing the work on this.</p>\n<p>Here\u2019s an alternative idea proposed by Samo Burja, <a href=\"https://x.com/SamoBurja/status/1979958874322125282\">how about building all the nuclear power in Disney World</a> where they have a special right to do so, and using that to power the data centers? Alas, sorry, that\u2019s actually a terrible physical place to put data centers, and it doesn\u2019t get you past the central regulatory barrier, as in the NRC.</p>\n<p><a href=\"https://x.com/AaronBergman18/status/1981064858742206673\">It\u2019s improving.</a></p>\n<div>\n<figure>\n<div>\n\n\n<figure class=\"wp-block-image\"><img alt=\"\" src=\"https://substackcdn.com/image/fetch/$s_!dcRC!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F078003db-d792-4a2d-a80e-23f59a72b104_1028x747.png\" /></figure>\n\n\n<div></div>\n</div>\n</figure>\n</div>\n<blockquote><p>Aaron Bergman: (this is not in fact what I was intending to say)</p></blockquote>\n<p>&nbsp;</p>"
            ],
            "link": "https://thezvi.wordpress.com/2025/10/23/ai-139-the-overreach-machines/",
            "publishedAt": "2025-10-23",
            "source": "TheZvi",
            "summary": "The big release this week was OpenAI giving us a new browser, called Atlas. The idea of Atlas is that it is Chrome, except with ChatGPT integrated throughout to let you enter agent mode and chat with web pages and &#8230; <a href=\"https://thezvi.wordpress.com/2025/10/23/ai-139-the-overreach-machines/\">Continue reading <span class=\"meta-nav\">&#8594;</span></a>",
            "title": "AI #139: The Overreach Machines"
        }
    ],
    "lookbackDays": 1,
    "publishDate": "2025-10-23"
}