{
    "articles": [
        {
            "content": [
                "<p>I download a lot of videos from YouTube, and <a href=\"https://github.com/yt-dlp/yt-dlp\">yt-dlp</a> is my tool of choice.\nSometimes I download videos as a one-off, but more often I\u2019m downloading videos in a project \u2013 my bookmarks, my collection of TV clips, or my social media scrapbook.</p>\n\n<p>I\u2019ve noticed myself writing similar logic in each project \u2013 finding the downloaded files, converting them to MP4, getting the channel information, and so on.\nWhen you write the same thing multiple times, it\u2019s a sign you should extract it into a shared tool \u2013 so that\u2019s what I\u2019ve done.</p>\n\n<p><a href=\"https://github.com/alexwlchan/yt-dlp_alexwlchan\">yt-dlp_alexwlchan</a> is a script that calls yt-dlp with my preferred options, in particular:</p>\n\n<ul>\n  <li>Download the highest-quality video, thumbnail, and subtitles</li>\n  <li>Save the video as MP4 and the thumbnail as a JPEG</li>\n  <li>Get some information about the video (like title and description) and the channel (like the name and avatar)</li>\n</ul>\n\n<p>All this is presented in a CLI command which prints a JSON object that other projects can parse.\nHere\u2019s an example:</p>\n\n<pre class=\"language-console\"><code><span class=\"gp\">$</span><span class=\"w\"> </span>yt-dlp_alexwlchan.py <span class=\"s2\">\"https://www.youtube.com/watch?v=TUQaGhPdlxs\"</span>\n<span class=\"go\">{\n  \"id\": \"TUQaGhPdlxs\",\n  \"url\": \"https://www.youtube.com/watch?v=TUQaGhPdlxs\",\n  \"title\": \"\\\"new york city, manhattan, people\\\" - Free Public Domain Video\",\n  \"description\": \"All videos uploaded to this channel are in the Public Domain: Free for use by anyone for any purpose without restriction. #PublicDomain\",\n  \"date_uploaded\": \"2022-03-25T01:10:38Z\",\n  \"video_path\": \"\\uff02new york city, manhattan, people\\uff02 - Free Public Domain Video [TUQaGhPdlxs].mp4\",\n  \"thumbnail_path\": \"\\uff02new york city, manhattan, people\\uff02 - Free Public Domain Video [TUQaGhPdlxs].jpg\",\n  \"subtitle_path\": null,\n  \"channel\": {\n    \"id\": \"UCDeqps8f3hoHm6DHJoseDlg\",\n    \"name\": \"Public Domain Archive\",\n    \"url\": \"https://www.youtube.com/channel/UCDeqps8f3hoHm6DHJoseDlg\",\n    \"avatar_url\": \"https://yt3.googleusercontent.com/ytc/AIdro_kbeCfc5KrnLmdASZQ9u649IxrxEUXsUaxdSUR_jA_4SZQ=s0\"\n  },\n  \"site\": \"youtube\"\n}\n</span></code></pre>\n<p>Rather than using the yt-dlp CLI, I\u2019m using the Python interface.\nI can <a href=\"https://github.com/yt-dlp/yt-dlp?tab=readme-ov-file#embedding-yt-dlp\">import the <code>YouTubeDL</code> class</a> and pass it some options, then pull out the important fields from the response.\nThe library is very flexible, and <a href=\"https://github.com/yt-dlp/yt-dlp/blob/5513036104ed9710f624c537fb3644b07a0680db/yt_dlp/YoutubeDL.py#L198-L577\">the options are well-documented</a>.</p>\n\n<p>This is similar to my <a href=\"https://alexwlchan.net/2024/create-thumbnail/\"><code>create_thumbnail</code> tool</a>.\nI only have to define <a href=\"https://alexwlchan.net/2025/create-thumbnail-is-exif-aware/\">my preferred behaviour</a> once, then other code can call it as an external script.</p>\n\n<p>I have ideas for changes I might make in the future, like tidying up filenames or supporting more sites, but I\u2019m pretty happy with this first pass.\nAll the code is in <a href=\"https://github.com/alexwlchan/yt-dlp_alexwlchan\">my yt-dlp_alexwlchan GitHub repo</a>.</p>\n\n<p>This script is based on my preferences, so you probably don\u2019t want to use it directly \u2013 but if you use yt-dlp a lot, it could be a helpful starting point for writing your own script.</p>\n\n<p>Even if you don\u2019t use yt-dlp, the idea still applies: when you find yourself copy-pasting configuration and options, turn it into a standalone tool.\nIt keeps your projects cleaner and more consistent, and your future self will thnak you for it.</p>\n\n\n    <p>[If the formatting of this post looks odd in your feed reader, <a href=\"https://alexwlchan.net/2025/yt-dlp-wrapper/?ref=rss\">visit the original article</a>]</p>"
            ],
            "link": "https://alexwlchan.net/2025/yt-dlp-wrapper/?ref=rss",
            "publishedAt": "2025-10-07",
            "source": "Alex Chan",
            "summary": "I've written a new script which calls yt-dlp with my preferred options, so I don't have to copy my configuration across different projects.",
            "title": "Creating a personal wrapper around yt-dlp"
        },
        {
            "content": [
                "<div class=\"trix-content\">\n  <div>We're fed an endless stream of consternation over AI slop these days. The content apocalypse is nigh! It'll rot your brain! Okay, sure, maybe, but have you seen the kind of content sludge that perfectly ordinary humans are capable of producing? It's thrice as tragic.</div><div><br />The web is full of it. Garbage writing and brain-dead shorts. Content mills pumping out nonsense pages and gagging videos to appease whatever the high priests of SEO now think they've divined will please Lord Google or Master TikTok.&nbsp;<br /><br /></div><div>It's been infecting websites everywhere with \"calls to action\", \"white paper available upon sign up\", and \"10 ways to supercharge your productivity\". Links stuffed into every crevice to juice rankings, capture \"most searched for\" keywords, and convert, convert, convert.</div><div><br />It's an affront to humanity to make sentient beings do this work. Turning human potential, creativity, and ingenuity into content sludge is a process no more dignified than turning pink slime into chicken nuggets.<br /><br /></div><div>I'll take AI slop over human sludge any day. Let the little robots barf up tokens to unlock the next basis point of incremental conversion. Better them than us, I say. This is exactly the soul-crushing, creative drudgery that machines were made to munch through without complaint.<br /><br /></div><div>But couldn't we do without sludge or slop, you say? Sure, right after we reach a shared state of nirvana. As soon as the average 4.5 hours of screen-on time is turned into real reading, real making, real pursuits. So that'll happen exactly never.<br /><br /></div><div>Case in point: the most important attribute of a phone for most people is still the battery life. These little content slop and sludge faucets can already spew out nearly an entire day's worth of nonstop eyeball junk, and yet you crave more. More! MORE!</div><div><br />So stop whining about the AI slop. You're already steeped in human sludge. And the door to exit both was always there. But you're not going to open it, are you?</div>\n</div>"
            ],
            "link": "https://world.hey.com/dhh/give-me-ai-slop-over-human-sludge-any-day-8c4b747d",
            "publishedAt": "2025-10-07",
            "source": "DHH",
            "summary": "<div class=\"trix-content\"> <div>We're fed an endless stream of consternation over AI slop these days. The content apocalypse is nigh! It'll rot your brain! Okay, sure, maybe, but have you seen the kind of content sludge that perfectly ordinary humans are capable of producing? It's thrice as tragic.</div><div><br />The web is full of it. Garbage writing and brain-dead shorts. Content mills pumping out nonsense pages and gagging videos to appease whatever the high priests of SEO now think they've divined will please Lord Google or Master TikTok.&nbsp;<br /><br /></div><div>It's been infecting websites everywhere with \"calls to action\", \"white paper available upon sign up\", and \"10 ways to supercharge your productivity\". Links stuffed into every crevice to juice rankings, capture \"most searched for\" keywords, and convert, convert, convert.</div><div><br />It's an affront to humanity to make sentient beings do this work. Turning human potential, creativity, and ingenuity into content sludge is a process no more dignified than turning pink slime into chicken nuggets.<br /><br /></div><div>I'll take AI slop over human sludge any day. Let the little robots barf up tokens to unlock the next basis point of incremental conversion. Better them than us, I say. This is exactly the soul-crushing, creative drudgery that",
            "title": "Give me AI slop over human sludge any day"
        },
        {
            "content": [],
            "link": "https://harper.blog/notes/2025-10-07_2fb321738a0d_hung-out-with-some-good-friend/",
            "publishedAt": "2025-10-07",
            "source": "Harper Reed",
            "summary": "<p>Hung out with some good friends this weekend!</p> <figure> <img alt=\"image_1.jpg\" height=\"1200\" src=\"https://harper.blog/notes/2025-10-07_2fb321738a0d_hung-out-with-some-good-friend/image_1.jpg\" width=\"1800\" /> </figure> <figure> <img alt=\"image_2.jpg\" height=\"1800\" src=\"https://harper.blog/notes/2025-10-07_2fb321738a0d_hung-out-with-some-good-friend/image_2.jpg\" width=\"1800\" /> </figure> <hr /> <p>Thank you for using RSS. I appreciate you. <a href=\"mailto:harper&#64;modest.com\">Email me</a></p>",
            "title": "Note #290"
        },
        {
            "content": [
                "<div class=\"trix-content\">\n  <div>In some cases, design is what something looks like.<br /><br />In other cases, design is how something works.<br /><br />But the most interesting designs to me are when design changes your behavior. Even the smallest details can change how someone interacts with something.<br /><br />Take the power reserve indicator on the A. Lange &amp; S\u00f6hne Lange 1 watch. The power reserve indicator indicates how much \"power\" (wind) is left. It's pictured below on the right side of the dial. It starts with AUF (\"up\") and ends with AB (\"down\"). A fully wound Lange 1 (indicator up at AUF) will give you about 72 hours before the watch fully runs out of power, stops, and must be wound again. It moves down as the watch runs until you're out of power. Wind it again to fill it back up.<br /><br />  <figure class=\"attachment attachment--preview attachment--lightboxable attachment--jpg\">\n      <a href=\"https://world.hey.com/jason/49baf157/blobs/eyJfcmFpbHMiOnsiZGF0YSI6MjI5MTUzNTg5OSwicHVyIjoiYmxvYl9pZCJ9fQ--8a5e070419da19277ebc2c0dd73503ff4d80e44e845643d361781b2106d55814/a-lange-sohne-lange-1-pink-gold-191.032-power-reserve-indication.jpg?disposition=attachment\" title=\"Download a-lange-sohne-lange-1-pink-gold-191.032-power-reserve-indication.jpg\">\n        <img alt=\"a-lange-sohne-lange-1-pink-gold-191.032-power-reserve-indication.jpg\" src=\"https://world.hey.com/jason/49baf157/representations/eyJfcmFpbHMiOnsiZGF0YSI6MjI5MTUzNTg5OSwicHVyIjoiYmxvYl9pZCJ9fQ--8a5e070419da19277ebc2c0dd73503ff4d80e44e845643d361781b2106d55814/eyJfcmFpbHMiOnsiZGF0YSI6eyJmb3JtYXQiOiJqcGciLCJyZXNpemVfdG9fbGltaXQiOlszODQwLDI1NjBdLCJxdWFsaXR5Ijo2MCwibG9hZGVyIjp7InBhZ2UiOm51bGx9LCJjb2FsZXNjZSI6dHJ1ZX0sInB1ciI6InZhcmlhdGlvbiJ9fQ--b3779d742b3242a2a5284869a45b2a113e0c177f0450c29f0baca1ee780f6604/a-lange-sohne-lange-1-pink-gold-191.032-power-reserve-indication.jpg\" />\n</a>\n  </figure><br /><br />Simple enough, right? An indicator and a scale for fully wound through unwound. Just like a car's fuel gauge. You have full through empty, with a few ticks in between to indicate 3/4 or 1/4 tank left, and typically a red zone at the end saying you really need to fill this thing up soon or you're going to be stranded.<br /><br />However, all is not as it seems on the Lange 1. There's something very clever going on here to <em>change your behavior</em>.<br /><br />First you'll notice five triangles between AUF and AUB. They aren't equally spaced. At first you might think it looks like each is about a quarter of the scale, and then the last two at the bottom would be like the red zone on your fuel gage.<br /><br />But no. The indicator follows a non-linear progression downwards. It doesn't sweep from top to bottom evenly over time. It's actually accelerated early.<br /><br />When fully wound, It takes just a day for the indicator to drop down two markers to the halfway point. From there, it takes a day each to hit the lower two markers. This makes it look like it's unwinding faster than it is because the indicator covers more distance in that first 24 hours. If the spacing were uniform, and the indicator was linear, the owner might not feel the need to wind it until the power reserve was nearly fully depleted. Then you might have a dead watch when you pick it up the next morning.<br />  <figure class=\"attachment attachment--preview attachment--lightboxable attachment--png\">\n      <a href=\"https://world.hey.com/jason/49baf157/blobs/eyJfcmFpbHMiOnsiZGF0YSI6MjI5MTU2Mjk0NywicHVyIjoiYmxvYl9pZCJ9fQ--09a33414b685c89a8af23fa6c72a2e093ca7d87cf41b1b99a5945859e86acbb0/Note%20Oct%207,%202025.png?disposition=attachment\" title=\"Download Note Oct 7, 2025.png\">\n        <img alt=\"Note Oct 7, 2025.png\" src=\"https://world.hey.com/jason/49baf157/representations/eyJfcmFpbHMiOnsiZGF0YSI6MjI5MTU2Mjk0NywicHVyIjoiYmxvYl9pZCJ9fQ--09a33414b685c89a8af23fa6c72a2e093ca7d87cf41b1b99a5945859e86acbb0/eyJfcmFpbHMiOnsiZGF0YSI6eyJmb3JtYXQiOiJwbmciLCJyZXNpemVfdG9fbGltaXQiOlszODQwLDI1NjBdLCJxdWFsaXR5Ijo2MCwibG9hZGVyIjp7InBhZ2UiOm51bGx9LCJjb2FsZXNjZSI6dHJ1ZX0sInB1ciI6InZhcmlhdGlvbiJ9fQ--7edc7b21f6fad97fa22412618822c4d19725431f296c7ce47dc174b61535d27c/Note%20Oct%207,%202025.png\" />\n</a>\n  </figure><br />So what's the net effect of this tiny little design detail that the owner may not even understand? Well, it looks like the watch is already half-way out of power after the first day, so i<em>t encourages the owner to wind the watch more frequently</em>. To keep it closer to topped off, even when it's not necessary. This helps prevents the watch from running out of power, losing time, and, ultimately, stopping. A stopped watch may be right twice a day, but it's rarely at the times you want.<br /><br />Small detail, material behavior change. Well considered, well executed, well done.<br /><br /></div><div>-Jason</div>\n</div>"
            ],
            "link": "https://world.hey.com/jason/when-design-drives-behavior-49baf157",
            "publishedAt": "2025-10-07",
            "source": "Jason Fried",
            "summary": "<div class=\"trix-content\"> <div>In some cases, design is what something looks like.<br /><br />In other cases, design is how something works.<br /><br />But the most interesting designs to me are when design changes your behavior. Even the smallest details can change how someone interacts with something.<br /><br />Take the power reserve indicator on the A. Lange &amp; S\u00f6hne Lange 1 watch. The power reserve indicator indicates how much \"power\" (wind) is left. It's pictured below on the right side of the dial. It starts with AUF (\"up\") and ends with AB (\"down\"). A fully wound Lange 1 (indicator up at AUF) will give you about 72 hours before the watch fully runs out of power, stops, and must be wound again. It moves down as the watch runs until you're out of power. Wind it again to fill it back up.<br /><br /> <figure class=\"attachment attachment--preview attachment--lightboxable attachment--jpg\"> <a href=\"https://world.hey.com/jason/49baf157/blobs/eyJfcmFpbHMiOnsiZGF0YSI6MjI5MTUzNTg5OSwicHVyIjoiYmxvYl9pZCJ9fQ--8a5e070419da19277ebc2c0dd73503ff4d80e44e845643d361781b2106d55814/a-lange-sohne-lange-1-pink-gold-191.032-power-reserve-indication.jpg?disposition=attachment\" title=\"Download a-lange-sohne-lange-1-pink-gold-191.032-power-reserve-indication.jpg\"> <img alt=\"a-lange-sohne-lange-1-pink-gold-191.032-power-reserve-indication.jpg\" src=\"https://world.hey.com/jason/49baf157/representations/eyJfcmFpbHMiOnsiZGF0YSI6MjI5MTUzNTg5OSwicHVyIjoiYmxvYl9pZCJ9fQ--8a5e070419da19277ebc2c0dd73503ff4d80e44e845643d361781b2106d55814/eyJfcmFpbHMiOnsiZGF0YSI6eyJmb3JtYXQiOiJqcGciLCJyZXNpemVfdG9fbGltaXQiOlszODQwLDI1NjBdLCJxdWFsaXR5Ijo2MCwibG9hZGVyIjp7InBhZ2UiOm51bGx9LCJjb2FsZXNjZSI6dHJ1ZX0sInB1ciI6InZhcmlhdGlvbiJ9fQ--b3779d742b3242a2a5284869a45b2a113e0c177f0450c29f0baca1ee780f6604/a-lange-sohne-lange-1-pink-gold-191.032-power-reserve-indication.jpg\" /> </a> </figure><br /><br />Simple enough, right? An indicator and a scale for fully wound through unwound. Just like a car's fuel gauge. You have full through empty, with a few ticks in between to indicate 3/4 or 1/4 tank left, and typically a red zone",
            "title": "When design drives behavior"
        },
        {
            "content": [],
            "link": "https://simonwillison.net/2025/Oct/7/vibe-engineering/#atom-entries",
            "publishedAt": "2025-10-07",
            "source": "Simon Willison",
            "summary": "<p>I feel like <strong>vibe coding</strong> is <a href=\"https://simonwillison.net/2025/Mar/19/vibe-coding/\">pretty well established now</a> as covering the fast, loose and irresponsible way of building software with AI - entirely prompt-driven, and with no attention paid to how the code actually works. This leaves us with a terminology gap: what should we call the other end of the spectrum, where seasoned professionals accelerate their work with LLMs while staying proudly and confidently accountable for the software they produce?</p> <p>I propose we call this <strong>vibe engineering</strong>, with my tongue only partially in my cheek.</p> <p>One of the lesser spoken truths of working productively with LLMs as a software engineer on non-toy-projects is that it's <em>difficult</em>. There's a lot of depth to understanding how to use the tools, there are plenty of traps to avoid, and the pace at which they can churn out working code raises the bar for what the human participant can and should be contributing.</p> <p>The rise of <strong>coding agents</strong> - tools like <a href=\"https://www.claude.com/product/claude-code\">Claude Code</a> (released February 2025), OpenAI's <a href=\"https://github.com/openai/codex\">Codex CLI</a> (April) and <a href=\"https://github.com/google-gemini/gemini-cli\">Gemini CLI</a> (June) that can iterate on code, actively testing and modifying it until it achieves a specified goal, has dramatically increased the usefulness of LLMs",
            "title": "Vibe engineering"
        },
        {
            "content": [
                "<p>The odds are against you and the situation is grim.</p>\n<p>Your scrappy band are the only ones facing down a growing wave of powerful inhuman entities with alien minds and mysterious goals. The government is denying that anything could possibly be happening and actively working to shut down the few people trying things that might help. Your thoughts, no matter what you think could not harm you, inevitably choose the form of the destructor. You knew it was going to get bad, but this is so much worse.</p>\n<p>You have an idea. You\u2019ll cross the streams. Because there is a very small chance that you will survive. You\u2019re in love with this plan. You\u2019re excited to be a part of it.</p>\n<div>\n\n\n<span id=\"more-24769\"></span>\n\n\n</div>\n<p>Welcome to the always excellent <a href=\"https://www.lighthaven.space/\">Lighthaven</a> venue for The Curve, Season 2, a conference I had the pleasure to attend this past weekend.</p>\n<p>Where the accelerationists and the worried come together to mostly get along and coordinate on the same things, because the rest of the world has gone blind and mad. In some ways technical solutions seem relatively promising, shifting us from \u2018might be actually impossible\u2019 levels of impossible to Shut Up And Do The Impossible levels of impossible, all you have to do is beat the game on impossible difficulty level. As a speed run. On your first try. Good luck.</p>\n<p>The action space has become severely constrained. Between the actual and perceived threats from China, the total political ascendence of Nvidia in particular and anti-regulatory big tech in general, and the setting in of more and more severe race conditions and the increasing dependence of the entire economy on AI capex investments, it\u2019s all we can do to try to only shoot ourselves in the foot and not aim directly for the head.</p>\n<p>Last year we were debating tradeoffs. This year, aside from the share price of Nvidia, as long as you are an American who likes humans considering things that might pass? On the margin, there are essentially no tradeoffs. It\u2019s better versus worse.</p>\n<p>That doesn\u2019t invalidate the thesis of <a href=\"https://www.google.com/aclk?sa=L&amp;pf=1&amp;ai=DChsSEwib99C1x5KQAxWRaUcBHQl0K24YACICCAEQABoCcXU&amp;co=1&amp;ase=2&amp;gclid=Cj0KCQjw9JLHBhC-ARIsAK4PhcqiqfsQqV5wjmDoCvNlL_f4ndFmLyPKyldVRixTy92R8rsLCWZwaZsaAuHnEALw_wcB&amp;cid=CAASQ-RoLj79NPg3H77DYh_XmI1E1cDwNIacC8xx4RvslVpc_1kIgGL0o2n1Pi6KBuYufUah7opJ6K4DN-d4k6NLg5Tc-2c&amp;cce=2&amp;category=acrcp_v1_32&amp;sig=AOD64_177znfRXezHro1Dp5DaleSWjTofw&amp;q&amp;nis=4&amp;adurl=https://ifanyonebuildsit.com/?ref%3Dgoogle-ad%26gad_source%3D1%26gad_campaignid%3D22720293810%26gbraid%3D0AAAAA_oFpEZlMePcWbJ_sIGA8iwbX-K8f%26gclid%3DCj0KCQjw9JLHBhC-ARIsAK4PhcqiqfsQqV5wjmDoCvNlL_f4ndFmLyPKyldVRixTy92R8rsLCWZwaZsaAuHnEALw_wcB&amp;ved=2ahUKEwjTrcq1x5KQAxXwFVkFHZfKJqEQ0Qx6BAgQEAE\"><em>If Anyone Builds It, Everyone Dies</em></a> or the implications down the line. At some point we will probably either need to do impactful international coordination or other interventions that involved large tradeoffs, or humanity loses control over the future or worse. That implication exists in every reasonable sketch of the future I have seen in which AI does not end up a \u2018normal technology.\u2019 So one must look forward towards that, as well.</p>\n<p>You can also look at it as Year 1 of the curve was billed (although I don\u2019t use the d word) as \u2018doomers vs. accelerationists\u2019 and now as Nathan Lambert says <a href=\"https://www.interconnects.ai/p/thoughts-on-the-curve\">it was DC and SF types</a>, like when the early season villains and heroes are now all working together as the stakes get raised and the new Big Bad shows up, then you do it again until everything is cancelled.</p>\n\n\n<h4 class=\"wp-block-heading\">Overall Impressions</h4>\n\n\n<p>The Curve was a great experience. The average quality of attendees was outstanding. I would have been happy to talk to a large fraction of them 1-on-1 for a long time, and there were a number that I\u2019m sad I missed. Lots of worthy sessions lost out to other plans.</p>\n<p>As Anton put it, every (substantive) conversation I had made me feel smarter. There was opportunity everywhere, everyone was cooperative and seeking to figure things out, and everyone stayed on point.</p>\n<p>To the many people who came up to me to thank me for my work, you\u2019re very welcome. I appreciate it every time and find it motivating.</p>\n\n\n<h4 class=\"wp-block-heading\">The Inside View</h4>\n\n\n<p>What did people at the conference think about some issues?</p>\n<p>We have charts.</p>\n<p>Where is <a href=\"https://thezvi.substack.com/p/ai-and-the-technological-richter\">AI on the technological richter scale</a>?</p>\n<div>\n<figure>\n<div>\n\n\n<figure class=\"wp-block-image\"><img alt=\"\" src=\"https://substackcdn.com/image/fetch/$s_!CNnx!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6eb8b7f1-7f56-4144-8414-7ddabdea091f_450x600.jpeg\" /></figure>\n\n\n<div></div>\n</div>\n</figure>\n</div>\n<p>There are dozens of votes here. Only one person put this as low as a high 8, which is the range of automobiles, electricity and the internet. A handful put it with fire, the wheel, agriculture and the printing press. Then most said this is similar to the rise of the human species, a full transformation. A few said it is a bigger deal than that.</p>\n<p>If you were situationally aware enough to show up, you are aware of the situation.</p>\n<p>These are median predictions, so the full distribution will have a longer tail, but this seems reasonable to me. The default is 10, that AI is going to be a highly non-normal technology on the level of the importance of humans, but there\u2019s a decent chance it will \u2018only\u2019 be a 9 on the level of agriculture or fire, and some chance it disappoints and ends up Only Internet Big.</p>\n<p>Last year, people would often claim AI wouldn\u2019t even be Internet Big. We are rapidly approaching the point where that is not a position you can offer with a straight face.</p>\n<p>How did people expect this to play out?</p>\n<div>\n<figure>\n<div>\n\n\n<figure class=\"wp-block-image\"><img alt=\"\" src=\"https://substackcdn.com/image/fetch/$s_!B67x!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9e21e877-3918-4f54-8f2a-144706ebe62d_600x450.jpeg\" /></figure>\n\n\n<div></div>\n</div>\n</figure>\n</div>\n<p>That\u2019s hard to read, so the centers of the distributions are, note that there was clearly a clustering effect:</p>\n<ol>\n<li>90% of code is written by AI by ~2028.</li>\n<li>90% of human remote work can be done more cheaply by AI by ~2031.</li>\n<li>Most cars on America\u2019s roads lack human drivers by ~2041.</li>\n<li>AI makes Nobel Prize worthy discovery by ~2032.</li>\n<li>First one-person $1 billion company by 2026.</li>\n<li>First year of &gt;10% GDP growth by ~2038 (but 3 votes for never).</li>\n</ol>\n<div>\n<figure>\n<div>\n\n\n<figure class=\"wp-block-image\"><img alt=\"\" src=\"https://substackcdn.com/image/fetch/$s_!GTyI!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7d75172e-520c-4f5b-b57b-a27af092ddd0_600x450.jpeg\" /></figure>\n\n\n<div></div>\n</div>\n</figure>\n</div>\n<ol>\n<li>People estimate 15%-50% current speedup at AI labs from AI coding.</li>\n<li>When AI is fully automated, disagreement over how good their research taste will be, but median is roughly as good as the median current AI worker.</li>\n<li>If we replaced each human with an AI version of themselves that was the same except 30x faster with 30 copies, but we only had access to similar levels of compute, we\u2019d get maybe a 12x speedup in progress.</li>\n</ol>\n<p>What are people worried or excited about? A lot of different things, from \u2018everyone lives\u2019 to \u2018concentration of power,\u2019 \u2018everyone dies\u2019 and especially \u2018loss of control\u2019 which have the most +1s on their respective sides. Others are excited to cure their ADD or simply worried everything will suck.</p>\n<div>\n<figure>\n<div>\n\n\n<figure class=\"wp-block-image\"><img alt=\"\" src=\"https://substackcdn.com/image/fetch/$s_!E7lN!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F84d9ad1a-8987-47b7-b953-ca32a4563669_4000x3000.jpeg\" /></figure>\n\n\n<div></div>\n</div>\n</figure>\n</div>\n<p>Which kind of things going wrong worries people most, misalignment or misuse?</p>\n<p>Why not both? Pretty much everyone said both.</p>\n<div>\n<figure>\n<div>\n\n\n<figure class=\"wp-block-image\"><img alt=\"\" src=\"https://substackcdn.com/image/fetch/$s_!-i2Z!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8d9a5519-4d24-432f-9966-ed0336f25cd4_4000x3000.jpeg\" /></figure>\n\n\n<div></div>\n</div>\n</figure>\n</div>\n<p>Finally, who is this nice man with my new favorite IYKYK t-shirt?</p>\n<p>(I mean, he has a name tag, it\u2019s OpenAI\u2019s Boaz Barak)</p>\n<div>\n<figure>\n<div>\n\n\n<figure class=\"wp-block-image\"><img alt=\"\" src=\"https://substackcdn.com/image/fetch/$s_!65B-!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6e329c64-560f-46dc-8984-884aaa4ac70a_450x600.jpeg\" /></figure>\n\n\n<div></div>\n</div>\n</figure>\n</div>\n\n\n<h4 class=\"wp-block-heading\">Track Trouble</h4>\n\n\n<p>The central problem at every conference is fear of missing out. Opportunity costs. There are many paths, even when talking to a particular person. You must choose.</p>\n<p>That goes double at a conference like The Curve. The quality of the people there was off the charts and the schedule forced hard choices between sessions. There were entire other conferences I could have productively experienced. I also probably could have usefully done a lot more prep work.</p>\n<p>I could of course have hosted a session, which I chose not to do this time around. I\u2019m sure there were various topics I could have done that people would have liked, but I was happy for the break, and it\u2019s not like there\u2019s a shortage of my content out there.</p>\n<p>My strategy is mostly to not actively plan my conference experiences, instead responding to opportunity. I think this is directionally correct but I overplay it, and should have (for example) looked at the list of who was going to be there.</p>\n<p>What were the different tracks or groups of discussions and sessions I ended up in?</p>\n<ol>\n<li>Technical alignment discussions. I had the opportunity to discuss safety and alignment work with a number of those working on such issues at Anthropic, DeepMind and even xAI. I missed OpenAI this time around, but they were there. This always felt exciting, enlightening and fun. I still get imposter syndrome every time people in such conversations take me and my takes and ideas seriously. Conditions are in many ways horribly terrible but everyone is on the same team and some things seem promising. I felt progress was made. My technical concrete pitch to Anthropic included (among other things) both particular experimental suggestions and also a request that they sustain access to Sonnet 3.5 and 3.6.\n<ol>\n<li>It wouldn\u2019t make sense to go into the technical questions here.</li>\n</ol>\n</li>\n<li>Future projecting. I went to talks by Joshua Achiam and Helen Toner about what future capabilities and worlds might look like. Jack Clark\u2019s closing talk was centrally this but touched on other things.</li>\n<li>AI policy discussions. These felt valuable and enlightening in both directions, but were infuriating and depressing throughout. People on the ground in Washington kept giving us variations on \u2018it\u2019s worse than you know,\u2019 which it usually is. So now you know. Others seemed not to appreciate how bad things had gotten. I was often pointing out that people\u2019s proposals implied some sort of international treaty and form of widespread compute surveillance, had zero chance of actually causing us not to die, or sometimes both. At other times, I was pointing out that things literally wouldn\u2019t work on the level of \u2018do the object level goal\u2019 let alone make us win. Or we were trying to figure out what was sufficiently completely costless and not even a tiny bit weird or complex that one could propose that might actually do anything meaningful. Or simply observing other perspectives.\n<ol>\n<li>In particular, different people maintained different players were relatively powerful, but I came away from various discussions more convinced than ever that for now White House policy and rhetoric on AI can be modeled as fully captured by Nvidia, although constrained in some ways by congressional Republicans and some members of the MAGA movement. This is pretty much a worst case scenario. If we were captured by OpenAI or other AI labs that wouldn\u2019t be great but at least their interests and America are mostly aligned.</li>\n</ol>\n</li>\n<li>Nonprofit funding discussions. I\u2019d just come out of the latest Survival and Flourishing Fund round, various players seemed happy to talk and strategize, and it seems likely that very large amounts of money will be unlocked soon as OpenAI and Anthropic employees with increasingly valuable equity become liquid. The value of helping steer this seems crazy high, but the stakes on everything seem crazy high.\n<ol>\n<li>One particular worry is that a lot of this money could effectively get captured by various existing players, especially the existing EA/OP ecosystem, in ways that would very much be a shame.</li>\n<li>Another is simply that a bunch of relatively uninformed money could overwhelm incentives, contaminate various relationships and dynamics, introduce parasitic entry, drop average quality a lot, and so on.</li>\n<li>Or everyone involved could end up with a huge time sink and/or end up not deploying the funds.</li>\n<li>So there\u2019s lots to do. But it\u2019s all tricky, and trying to gain visible influence over the direction of funds is a very good way to get your own social relationships and epistemics very quickly compromised, also it can quickly eat up infinite time, so I\u2019m hesitant to get too involved or involved in the wrong ways.</li>\n</ol>\n</li>\n</ol>\n<p>What other tracks did I actively choose not to participate in?</p>\n<p>There were of course AI timelines discussions, but I did my best to avoid them except when they were directly relevant to a concrete strategic question. At one point someone in a 4-person conversation I was mostly observing said \u2018let\u2019s change the subject, can we argue about AI timelines\u2019 and I outright said \u2018no\u2019 but was overruled, and after a bit I walked away. For those who don\u2019t follow these debates, many of the more aggressive timelines have gotten longer over the course of 2025, with people who expected crazy to happen in 2027 or 2028 now not expecting crazy for several more years, but there are those who still mostly hold firm to a faster schedule.</p>\n<p>There were a number of talks about AI that assumed it was mysteriously a \u2018normal technology.\u2019 There were various sessions on economics projections, or otherwise taking place with the assumption that AI would not cause things to change much, except for whatever particular effect people were discussing. How would we \u2018strengthen our democracy\u2019 when people had these neat AI tools, or avoid concentration of power risks? What about the risk of They Took Our Jobs? What about our privacy? How would we ensure everyone or every nation has fair access?</p>\n<p>These discussions almost always silently assume that AI capability \u2018hits a wall\u2019 some place not very far from where it is now and then everything moves super slowly. Achiam\u2019s talk had elements of this, and I went because he\u2019s OpenAI\u2019s Head of Mission Alignment so knowing how he thinks about this seemed super valuable.</p>\n<p>To the extent I interacted with this it felt like smart people thinking about a potential world almost certainly very different from our own. Fascinating, can create useful intuition pumps, but that\u2019s probably not what\u2019s going to happen. If nothing else was going on, sure, count me in.</p>\n<p>But also all the talk of \u2018bottlenecks\u2019 therefore 0.5% or 1% GDP growth boost per year tops has already been overtaken purely by capex spending and I cannot remember a single economist or other GDP growth skeptic acknowledging that this already made their projections wrong and updating reasonably.</p>\n<p>There was <a href=\"https://thezvi.substack.com/i/151975026/asi-a-scenario\">an AI 2027 style tabletop exercise</a> again this year, which I recommend doing if you haven\u2019t done it before, except this time I wasn\u2019t aware it was happening, and also by now I\u2019ve done it a number of times.</p>\n<p>There were of course debates directly about doom, but remarkably little and I had no interest. It felt like everyone was either acknowledging existential risk enough that there wasn\u2019t much value of information in going further, or sufficiently blind they were in \u2018normal technology\u2019 mode. At some point people get too high level to think building smarter than human minds is a safe proposition.</p>\n\n\n<h4 class=\"wp-block-heading\">Let\u2019s Talk</h4>\n\n\n\n<h4 class=\"wp-block-heading\">Jagged Alliance</h4>\n\n\n<p>Helen Toner gave a talk on taking AI jaggedness seriously. What would it mean if AIs kept getting increasingly better and superhuman at many tasks, while remaining terrible at other tasks, or at least relatively highly terrible compared to humans? How does the order of capabilities impact how things unfold? Even if we get superhuman coding and start to get big improvements in other areas as a result, that won\u2019t make their ability profile similar to humans.</p>\n<p>I agree with Helen that such jaggedness is mostly good news and potentially could buy us substantial time for various transitions. However, it\u2019s not clear to me that this jaggedness does that much for that long, AI is (I am projecting) not going to stall out in the lagging areas or stay subhuman in key areas for as much calendar time as one might hope.</p>\n<p>A fun suggestion was to imagine LLMs talking about how jagged human capabilities are. Look how dumb we are in some ways while being smart in others. I do think in a meaningful sense LLMs and other current AIs are \u2018more jagged\u2019 than humans in practice, because humans have continual learning and the ability to patch the situation and also route the physical world around our idiocy where they\u2019re being importantly dumb. So we\u2019re super dumb, but we try to not let it get in the way.</p>\n<blockquote><p><a href=\"https://x.com/neil_chilson/status/1974539655577264261\">Neil Chilson:</a> Great talk by @hlntnr about the jaggedness of AI, why it is likely to continue, and why it matters. Love this slide and her point that while many AI forecasters use smooth curves, a better metaphor is the chaotic transitions in fluid heating.</p>\n<div>\n<figure>\n<div>\n\n\n<figure class=\"wp-block-image\"><img alt=\"\" src=\"https://substackcdn.com/image/fetch/$s_!dNNc!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F22dac514-ada6-43f6-a0d9-216d0c02318f_1536x2048.jpeg\" /></figure>\n\n\n<div></div>\n</div>\n</figure>\n</div>\n<p>\u201cJaggedness\u201d being the uneven ability of AI to do tasks that seem about equally difficult to humans.</p>\n<p>Occurs to me I should have shared the \u201cwhy this matters\u201d slide, which was the most thought provoking one to me:</p>\n<div>\n<figure>\n<div>\n\n\n<figure class=\"wp-block-image\"><img alt=\"\" src=\"https://substackcdn.com/image/fetch/$s_!qkyF!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1f4ca6c3-fb91-4d17-a472-cfd114727cbf_1199x725.jpeg\" /></figure>\n\n\n<div></div>\n</div>\n</figure>\n</div>\n<p>&nbsp;</p></blockquote>\n<p>I am seriously considering talking about time to \u2018crazy\u2019 going forward, and whether that is a net helpful thing to say.</p>\n<p>The curves definitely be too smooth. It\u2019s hard to properly adjust for that. But I think the fluid dynamics metaphor, while gorgeous, makes the opposite mistake.</p>\n\n\n<h4 class=\"wp-block-heading\">More Teachers\u2019 Dirty Looks</h4>\n\n\n<p>I watched a talk by Randi Weingarten about how she and other teachers are advocating and viewing AI around issues in education. One big surprise is that she says they don\u2019t worry or care much about AI \u2018cheating\u2019 or doing work via ChatGPT, there are ways around that, especially \u2018project based learning that is relevant,\u2019 and the key thing is that education is all about human interactions. To her ChatGPT is a fine tool, although things like Character.ai are terrible, and she strongly opposes phones in schools for the right reasons and I agree with that.</p>\n<p>She said teachers need latitude to \u2018change with the times\u2019 but usually aren\u2019t given it, they need permission to change anything and if anything goes wrong they\u2019re fired (although there are the other stories we hear that teachers often can\u2019t be fired almost no matter what in many cases?). I do sympathize here. A lot needs to change.</p>\n<p>Why is education about human interactions? This wasn\u2019t explained. I always thought education was about learning things, I mostly didn\u2019t learn things through human interaction, I mostly didn\u2019t learn things in school via meaningful human interaction, and to the extent I learned things via meaningful human interaction it mostly wasn\u2019t in school. As usual when education professionals talk about education I don\u2019t get the sense they want children to learn things, or that they care about children being imprisoned and bored with their time wasted for huge portions of many days, but care about something else entirely? It\u2019s not clear what her actual objection to Alpha School (which she of course confirmed she hates) was other than decentering teachers, or what concretely was supposedly going wrong there? Frankly it sounded suspiciously like a call to protect jobs.</p>\n<p>If anything, her talk seemed to be a damning indictment of our entire system of schools and education. She presents vocational education as state of the art and with the times, and cited an example of a high school with a sub-50% graduation rate going to 100% graduation rate and 182 of 186 students getting a \u2018certification\u2019 from future farmers of America after one such program. Aside from the obvious \u2018why do you need a certificate to be a farmer\u2019 and also \u2018why would you choose farmer in 2025\u2019 this is saying kids should spend vastly less time in school? Many other such implications were there throughout.</p>\n<p>Her group calls for \u2018guardrails\u2019 and \u2018accountability\u2019 on AI, worries about things like privacy, misinformation and understanding \u2018the algorithms\u2019 or the dangers to democracy, and points to declines in male non-college earnings,</p>\n\n\n<h4 class=\"wp-block-heading\">The View Inside The White House</h4>\n\n\n<p>There was a Chatham House discussion of executive branch AI policy in America where all involved were being diplomatic and careful. There\u2019s a lot of continuity between the Biden approach to AI and much of the Trump approach, there\u2019s a lot of individual good things going on, and it was predicted that CAISI would have a large role going forward, lots of optimism and good detail.</p>\n<p>It seems reasonable to say that the Trump administration\u2019s first few months of AI policy were unexpectedly good, and the AI Action Plan was unexpectedly good. Then there are the other things that happened.</p>\n<p>Thus the session included some polite versions of \u2018what the hell are we doing?\u2019 that was at most slightly beneath the surface. As a central example, one person observed that if America \u2018loses on AI,\u2019 it would likely be because we did one or more of failing to (1) provide the necessary electrical power, (2) failed to bring in the top AI talent or (3) sold away our chip advantage. They didn\u2019t say, but I will note here, that current American policy seems determined to screw up all three of these? We are cancelling solar, wind and battery projects all over, we are restricting our ability to acquire talent, and we are seriously debating selling Blackwell chips directly to China.</p>\n\n\n<h4 class=\"wp-block-heading\">Assume The Future AIs Be Scheming</h4>\n\n\n<p>I was sad that going to that talk ruled out watching Buck Shlegeris debate Timothy Lee about whether keeping AI agents under control will be hard, as I expected that session to both be extremely funny (and one sided) and also plausibly enlightening in navigating such arguments, but that\u2019s how conferences go. I did then get to see Buck discuss mitigating insider threats from scheming AIs, in which he explained some of the ways in which dealing with scheming AIs that are smarter than you is very hard. I\u2019d go farther and say that in the types of scenarios Buck is discussing there it\u2019s not going to work out for you. If the AIs be smarter than you and also scheming against you and you try to use them for important stuff anyway you lose.</p>\n<p>That doesn\u2019t mean do zero attempts to mitigate this but at some point the whole effort is counterproductive as it creates context that creates what it is worried about, without giving you much chance of winning.</p>\n\n\n<h4 class=\"wp-block-heading\">Interlude</h4>\n\n\n<p>At one point I took a break to get dinner at a nearby restaurant. The only other people there were two women. The discussion included mention of AI 2027 and also that one of them is reading If Anyone Builds It, Everyone Dies.</p>\n<p>Also at one point I saw a movie star I\u2019m a fan of, hanging out and chatting. Cool.</p>\n\n\n<h4 class=\"wp-block-heading\">Eyes On The Mission</h4>\n\n\n<p>Sunday started out with Josh Achiam\u2019s talk (again, he\u2019s Head of Mission Alignment at OpenAI, but his views here were his own) about the challenge of the intelligence age. If it comes out, it\u2019s worth a watch. There were a lot of very good thoughts and considerations here. I later got to have some good talk with him during the afterparty. Like much talk at OpenAI, it also silently ignored various implications of what was being built, and implicitly assumed the relevant capabilities just stopped in any place they would cause bigger issues. The talk acknowledged that it was mostly assuming alignment is solved, which is fine as long as you say that explicitly, we have many different problems to deal with, but other questions also felt assumed away more silently. Josh promises his full essay version will deal with that.</p>\n\n\n<h4 class=\"wp-block-heading\">Putting The Code Into Practice</h4>\n\n\n<p>I got to go to a Chatham House Q&amp;A about the EU Frontier AI Code of Practice, which various people keep reminding me I should write about, and I swear I want to do that as soon as I have some spare time. There was a bunch of info, some of it new to me, and also insight into how those involved think all of this is going to work. I later shared with them my model of how I think the AI companies will respond, in particular the chance they will essentially ignore the law when inconvenient because of lack of sufficient consequences. And I offered suggestions on how to improve impact here. But on the margin, yeah, the law does some good things.</p>\n\n\n<h4 class=\"wp-block-heading\">Missing It</h4>\n\n\n<p>I got into other talks and missed out on one I wanted to see by Joe Allen, about How the MAGA Movement Sees AI. This is a potentially important part of the landscape on AI going forward, as a bunch of MAGA types really dislike AI and are in position to influence the White House.</p>\n<p>As I look over the schedule in hindsight I see a bunch of other stuff I\u2019m sad I missed, but the alternative would have been missing valuable 1-on-1s or other talks.</p>\n\n\n<h4 class=\"wp-block-heading\">Clark Talks About The Frontier</h4>\n\n\n<p>The final talk was Jack Clark giving his perspective on events. This was a great talk, if it does online you should watch it, it gave me a very concrete sense of where he is coming from.</p>\n<p>Jack Clark has high variance. When he\u2019s good, he\u2019s excellent, such as in this talk, including the Q&amp;A, and when he asked Achaim an armor piercing question, or when he\u2019s sticking to his guns on timelines that I think are too short even though it doesn\u2019t seem strategic to do that. At other times, him and the policy team at Anthropic are in some sort of Official Mode where they\u2019re doing a bunch of hedging and making things harder.</p>\n<p>The problem I have with Anthropic\u2019s communications is, essentially, that they are not close to the Pareto Frontier, where the y-axis is something like \u2018Better Public Policy and Epistemics\u2019 and the x-axis can colloquially be called \u2018Avoid Pissing Off The White House.\u2019 I acknowledge there is a tradeoff here, especially since we risk negative polarization, but we need to be strategic, and certain decisions have been de facto poking the bear for little gain, and at other times they hold back for little gain the other way. We gotta be smarter about this.</p>\n\n\n<h4 class=\"wp-block-heading\">Other Perspectives</h4>\n\n\n\n<h4 class=\"wp-block-heading\">Deepfates</h4>\n\n\n<p>They are often very different from mine, or yours.</p>\n<blockquote><p><a href=\"https://x.com/deepfates/status/1974902965892010027\">Deepfates</a>: looks like a lot of people who work on policy and research for aligning AIs to human interests. I\u2019m curious what you think about how humans align to AI.</p>\n<p>my impression so far: people from big labs and people from government, politely probing each other to see which will rule the world. they can\u2019t just out and say it but there\u2019s zerosumness in the air</p>\n<p>Chris Painter: That isn\u2019t my impression of the vibe at the event! Happy to chat.</p></blockquote>\n<p>I was with Chris on this. It very much did not feel zero sum. There did seem to be a lack of appreciation of the \u2018by default the AIs rule the world\u2019 problem, even in a place dedicated largely to this particular problem.</p>\n<blockquote><p><a href=\"https://x.com/deepfates/status/1975305177986179444\">Deepfates</a>: Full review of The Curve: people just want to believe that Anyone is ruling the world. some of them can sense that Singleton power is within reach and they are unable to resist The opportunity. whether by honor or avarice or fear of what others will do with it.</p></blockquote>\n<p>There is that too, that currently no one is ruling the world, and it shows. It also has its advantages.</p>\n<blockquote><p>so most people are just like \u201cuh-oh! what will occur? shouldn\u2019t somebody be talking about this?\u201d which is fine honestly, and a lot of them are doing good research and I enjoy learning about it. The policy stuff is more confusing</p>\n<p>diverse crowd but multiple clusters talking past each other as if the other guys are ontologically evil and no one within earshot could possibly object. and for the most part they don\u2019t actually? people just self-sort by sessions or at most ask pointed questions. parallel worlds.</p></blockquote>\n<p>Yep, parallel worlds, but I never saw anyone say someone else was evil. What, never? Well, hardly ever. And not anyone who actually showed up. Deeply confused and likely to get us all killed? Well, sure, there was more of that, but obviously true, and again not the people present.</p>\n<blockquote><p>things people are concerned about in no order: China. Recursive self-improvement. internal takeover of AI labs by their models. Fascism. Copyright law. The superPACs. Sycophancy. Privacy violations. Rapid unemployment of whole sectors of society. Religious and political backlash, autonomous agents, capabilities. autonomous agents, legal liability. autonomous agents, nightmare nightmare nightmare.</p>\n<p>The fear of the other party, the other company, the other country, the other, the unknown, most of all the alien thing that threatens what it means to be human.</p></blockquote>\n<p>Fascinating to see threatens \u2018what it means to be human\u2019 on that list but not \u2018the ability to keep being human (or alive),\u2019 which I assure Deepfates a bunch of us were indeed very concerned about.</p>\n<blockquote><p>so they want to believe that the world is ruleable, that somebody, anybody, is at the wheel, as we careen into the strangest time in human history.</p>\n<p>and they do Not want it to be the AIs. even as they keep putting decision making power and communication surface on the AIs lol</p></blockquote>\n<p>You can kind of tell here that Deepfates is fine with it being the AIs and indeed is kind of disdainful of anyone who would object to this. As in, they understand what is about to happen, but think this is good, actually (and are indeed working to bring it about). So yeah, some actual strong disagreements were present, but didn\u2019t get discussed.</p>\n<p>I may or may not have seen Deepfates, since I don\u2019t know their actual name, but we presumably didn\u2019t talk, given:</p>\n<blockquote><p>i tried telling people that i work for a rogue AI building technologies to proliferate autonomous agents (among other things). The reaction was polite confusion. It seemed a bit unreal for everyone to be talking about the world ending and doing normal conference behaviors anyway.</p></blockquote>\n<p>Polite confusion is kind of the best you can hope for when someone says that?</p>\n<blockquote><p>Regardless, very interesting event. Good crowd, good talks, plenty of food and caffeinated beverages. Not VC/pitch heavy like a lot of SF things.</p>\n<p>Thanks to Lighthaven for hosting and Golden Gate Institute/Manifund for organizing. Will be curious to see what comes of this.</p></blockquote>\n<p>I definitely appreciated the lack of VC and pitching. I did get pitched once (on a nonprofit thing) but I was happy to take it. Focus was tight throughout.</p>\n\n\n<h4 class=\"wp-block-heading\">Anton</h4>\n\n\n<blockquote><p><a href=\"https://x.com/atroyn/status/1974509920268734725\">Anton</a>: \u201care you with the accelerationist faction?\u201d</p>\n<div>\n<figure>\n<div>\n\n\n<figure class=\"wp-block-image\"><img alt=\"\" src=\"https://substackcdn.com/image/fetch/$s_!elz6!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F91cec20d-4971-42ef-b075-7326d34e7043_1536x2048.jpeg\" /></figure>\n\n\n<div></div>\n</div>\n</figure>\n</div>\n<p>most people here have thought long and hard about ai, every conversation i have \u2014 even with those i vehemently disagree \u2014 feels like it makes me smarter..</p>\n<p>i cant overemphasize how good the vibes are at this event.</p>\n<p>Rob S: Another Lighthaven banger?</p>\n<p>Anton: ANOTHA ONE.</p></blockquote>\n\n\n<h4 class=\"wp-block-heading\">Jack Clark</h4>\n\n\n<p>As I note above, his closing talk was excellent. Otherwise, he seemed to be in the back of many of the same talks I was at. Listening. Gathering intel.</p>\n<blockquote><p><a href=\"https://x.com/jackclarkSF/status/1975037702656663902\">Jack Clark</a> (policy head, Anthropic): I spent a few days at The Curve and I am humbled and overjoyed by the experience &#8211; it is a special event, now in its second year, and I hope they preserve whatever lightning they\u2019ve managed to capture in this particular bottle. It was a privilege to give the closing talk.</p>\n<p>During the Q&amp;A I referenced The New Book, and likely due to the exhilaration of giving the earlier speech I fumbled a word and titled it: If Anyone Reads It, Everyone Dies.</p>\n<p>James Cham: It was such an inspiring (and terrifying) talk!</p></blockquote>\n\n\n<h4 class=\"wp-block-heading\">Roon</h4>\n\n\n<p>I did see Roon at one point but it was late in the day and neither of us had an obvious conversation we wanted to have and he wandered off. He\u2019s low key in person.</p>\n<p>I was very disappointed to realize he did not say \u2018den of inquiry\u2019 here:</p>\n<blockquote><p><a href=\"https://x.com/tszzl/status/1974624231456711123\">Roon</a>: The Curve is insane because a bunch of DC staffers in suits have shown up to Lighthaven, a rationalist den of iniquity that looks like a Kinkade painting.</p>\n<p>Jaime Sevilla: Jokes on you I am not a DC staffer, I just happen to like wearing my suit.</p>\n<p>Neil Chilson: Hey, I ditched the jacket after last night.</p>\n<p>Being Siedoh: i was impressed that your badge just says \u201cRoon\u201d lol.</p></blockquote>\n<p>To be fair, you absolutely wanted a jacket of some kind for the evening portion. That\u2019s why they were giving away sweatshirts. It was still quite weird to see the few people who did wear suits.</p>\n\n\n<h4 class=\"wp-block-heading\">Nathan Lambert</h4>\n\n\n<p><a href=\"https://www.interconnects.ai/p/thoughts-on-the-curve\">Nathan made the opposite of my choice</a>, and spent the weekend centered on timeline debates.</p>\n<blockquote><p>Nathan Lambert: My most striking takeaway is that the AI 2027 sequence of events, from AI models automating research engineers to later automating AI research, and potentially a singularity if your reasoning is so inclined, is becoming a standard by which many debates on AI progress operate under and tinker with.</p>\n<p>It\u2019s good that many people are taking the long term seriously, but there\u2019s a risk in so many people assuming a certain sequence of events is a sure thing and only debating the timeframe by which they arrive.</p></blockquote>\n<p>This feel like the deepfates theory of self-selection within the conference. I observed the opposite, that so many people were denying that any kind of research automation or singularity was going to happen. Usually they didn\u2019t even assert it wasn\u2019t happening, they simply went about discussing futures where it mysteriously didn\u2019t happen, presumably because of reasons, maybe \u2018bottlenecks\u2019 or muttering \u2018normal technology\u2019 or something.</p>\n<p>Within the short timelines and taking AGI (at least somewhat) seriously debate subconference, to the extent I saw it, yes I do think there\u2019s widespread convergence on the automating AI research analysis.</p>\n<p>Whereas Nathan is in the \u2018nope definitely not happening\u2019 camp, it seems, but is helpfully explaining that it is because of bottlenecks in the automation loop.</p>\n<blockquote><p>These long timelines are strongly based on the fact that the category of research engineering is too broad. Some parts of the RE job will be fully automated next year, and more the next. <strong>To check the box of automation the entire role needs to be replaced.</strong></p>\n<p>What is more likely over the next few years, each engineer is doing way more work and the job description evolves substantially. I make this callout on full automation because it is required for the distribution of outcomes that look like a singularity due to the need to remove the human bottleneck for an ever accelerating pace of progress. This is a point to reinforce that I am currently confident in a singularity not happening.</p></blockquote>\n<p>The automation theory is that, as Nathan documents in his writeup, within a few years the existing research engineers (REs) will be unbelievably productive (80%-90% automated) and in some ways RE is already automated, yet that doesn\u2019t allow us to finish the job, and humans continue importantly slowing down the loop because Real Science Is Messy and involves a social marketplace of ideas. Apologies for my glib paraphrasing. It\u2019s possible in theory that these accelerations of progress and partial automations plus our increased scaling are no match for increasing problem difficulty, but it seems unlikely to me.</p>\n<p>It seems far more likely that this kind of projection forgets how much things accelerate in such scenarios. Sure, it will probably be a lot messier than the toy models and straight lines on graphs, it always is, but you\u2019d best start believing in singularities, because you\u2019re in one, if you look at the arc of history.</p>\n\n\n<h4 class=\"wp-block-heading\">The Food</h4>\n\n\n<p>The following is a very minor thing but I enjoy it so here you go.</p>\n<p>All three meals were offered each day buffet style. Quality at these events is generally about as good as buffets get, they know who the good offerings are at this point. I ask for menus in advance so I can choose when to opt out and when to go hard, and which day to do my traditional one trip to a restaurant.</p>\n<p>Also there was some of this:</p>\n<blockquote><p><a href=\"https://x.com/tyler_m_john/status/1974671068544868436\">Tyler John</a>: It\u2019s riddled with contradictions. The neoliberal rationalists allocate vegan and vegetarian food with a central planner rather than allowing demand to determine the supply.</p>\n<p>Rachel: Yeah fwiw this was not a design choice. I hate this. I unfortunately didn\u2019t notice that it was still happening yesterday :/</p>\n<p>Tyler John: Oh on my end it\u2019s only a very minor complaint but I did enjoy the irony.</p>\n<p>Robert Winslow: I had a bad experience with this kind of thing at a conference. They said to save the veggies for the vegetarians. So instead of everyone taking a bit of meat and a bit of veg, everyone at the front of the line took more meat than they wanted, and everyone at the back got none.</p></blockquote>\n<p>You obviously can\u2019t actually let demand determine supply, because you (1) can\u2019t afford the transaction costs of charging on the margin and (2) need to order the food in advance. And there are logistical advantages to putting (at least some of) the vegan and vegetarian food in a distinct area so you don\u2019t risk contamination or put people on lines that waste everyone\u2019s time. If you\u2019re worried about a mistake, you\u2019d rather run out of meat a little early, you\u2019d totally take down the sign (or ignore it) if it was clear the other mistake was happening, and there were still veg options for everyone else.</p>\n<p>If you are confident via law of large numbers plus experience that you know your ratios, and you\u2019ve chosen (and been allowed to choose) wisely, then of course you shouldn\u2019t need anything like this.</p>\n\n\n<h4 class=\"wp-block-heading\"></h4>\n\n\n<p>&nbsp;</p>\n<p>&nbsp;</p>"
            ],
            "link": "https://thezvi.wordpress.com/2025/10/07/bending-the-curve/",
            "publishedAt": "2025-10-07",
            "source": "TheZvi",
            "summary": "The odds are against you and the situation is grim. Your scrappy band are the only ones facing down a growing wave of powerful inhuman entities with alien minds and mysterious goals. The government is denying that anything could possibly &#8230; <a href=\"https://thezvi.wordpress.com/2025/10/07/bending-the-curve/\">Continue reading <span class=\"meta-nav\">&#8594;</span></a>",
            "title": "Bending The Curve"
        },
        {
            "content": [],
            "link": "https://zed.dev/blog/hidden-gems-team-edition-part-1",
            "publishedAt": "2025-10-07",
            "source": "Zed Blog",
            "summary": "Team-favorite workflows and hidden features of Zed.",
            "title": "Hidden Gems: Team Edition Part 1"
        }
    ],
    "lookbackDays": 1,
    "publishDate": "2025-10-07"
}