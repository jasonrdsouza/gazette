{
    "articles": [
        {
            "content": [],
            "link": "https://olano.dev/blog/2666",
            "publishedAt": "2025-09-09",
            "source": "Facundo Olano",
            "summary": "Historias apiladas como fetas de carne en el fierro de un shawarma.",
            "title": "2666"
        },
        {
            "content": [
                "<img alt=\"i ran Claude in a loop for three months, and it created a genz programming language called cursed\" src=\"https://ghuntley.com/content/images/2025/09/universal_upscale_0_dc6aa202-ccbc-4c9d-a60e-7bf677aeb143_0.jpg\" /><p>It&apos;s a strange feeling knowing that you can create anything, and I&apos;m starting to wonder if there&apos;s a seventh stage to the &quot;<a href=\"https://ghuntley.com/ngmi/\" rel=\"noreferrer\">people stages of AI adoption by software developers</a>&quot; </p><figure class=\"kg-card kg-image-card\"><img alt=\"i ran Claude in a loop for three months, and it created a genz programming language called cursed\" class=\"kg-image\" height=\"803\" src=\"https://ghuntley.com/content/images/2025/03/image-17.png\" width=\"2000\" /></figure><p>whereby that seventh stage is essentially this scene in the matrix...</p><figure class=\"kg-card kg-embed-card\"></figure><p>It&apos;s where you deeply understand that &apos;<a href=\"https://ghuntley.com/dothings/\">you can now do anything</a>&apos; and just start doing it because it&apos;s possible and fun, and doing so is faster than explaining yourself. Outcomes speak louder than words. </p><p>There&apos;s a falsehood that AI results in SWE&apos;s skill atrophy, and there&apos;s no learning potential. </p><blockquote>If you&#x2019;re using AI only to &#x201c;do&#x201d; and not &#x201c;learn&#x201d;, you are missing out<br />- <a href=\"https://x.com/davidfowl/status/1910930253608001565?ref=ghuntley.com\">David Fowler</a></blockquote><p>I&apos;ve never written a compiler, yet I&apos;ve always wanted to do one, so I&apos;ve been working on one for the last three months by running Claude in a while true loop (aka &quot;<a href=\"https://ghuntley.com/ralph\" rel=\"noreferrer\">Ralph Wiggum</a>&quot;) with a simple prompt:</p><blockquote>Hey, can you make me a programming language like Golang but all the lexical keywords are swapped so they&apos;re Gen Z slang?</blockquote><p>Why? I really don&apos;t know. But it exists. And it produces compiled programs. During this period, Claude was able to implement anything that Claude desired. </p><p>The programming language is called &quot;cursed&quot;.  It&apos;s cursed in its lexical structure, it&apos;s cursed in how it was built, it&apos;s cursed that this is possible, it&apos;s cursed in how cheap this was, and it&apos;s cursed through how many times I&apos;ve sworn at Claude.</p><figure class=\"kg-card kg-image-card kg-width-full kg-card-hascaption\"><img alt=\"i ran Claude in a loop for three months, and it created a genz programming language called cursed\" class=\"kg-image\" height=\"1144\" src=\"https://ghuntley.com/content/images/2025/09/image-1.png\" width=\"2000\" /><figcaption><span style=\"white-space: pre-wrap;\">https://cursed-lang.org/ </span></figcaption></figure><p>For the last three months, Claude has been running in this loop with a single goal:</p><blockquote>&quot;Produce me a Gen-Z compiler, and you can implement anything you like.&quot;</blockquote><p>It&apos;s now available at:</p><ul><li><a href=\"https://cursed-lang.org/?ref=ghuntley.com\" rel=\"noreferrer\">https://cursed-lang.org/</a> </li><li><a href=\"https://github.com/ghuntley/cursed?ref=ghuntley.com\">https://github.com/ghuntley/cursed</a></li></ul><figure class=\"kg-card kg-bookmark-card kg-card-hascaption\"><a class=\"kg-bookmark-container\" href=\"https://cursed-lang.org/?ref=ghuntley.com\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">the &#x1f480; cursed programming language: programming, but make it gen z</div><div class=\"kg-bookmark-description\"></div><div class=\"kg-bookmark-metadata\"><img alt=\"i ran Claude in a loop for three months, and it created a genz programming language called cursed\" class=\"kg-bookmark-icon\" src=\"https://static.ghost.org/v5.0.0/images/link-icon.svg\" /><span class=\"kg-bookmark-author\">&#x1f480; cursed</span></div></div></a><figcaption><p><span style=\"white-space: pre-wrap;\">the website</span></p></figcaption></figure><figure class=\"kg-card kg-bookmark-card kg-card-hascaption\"><a class=\"kg-bookmark-container\" href=\"https://github.com/ghuntley/cursed?ref=ghuntley.com\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">GitHub - ghuntley/cursed: the &#x1f480; cursed programming language: programming, but make it gen z</div><div class=\"kg-bookmark-description\">the &#x1f480; cursed programming language: programming, but make it gen z - ghuntley/cursed</div><div class=\"kg-bookmark-metadata\"><img alt=\"i ran Claude in a loop for three months, and it created a genz programming language called cursed\" class=\"kg-bookmark-icon\" src=\"https://ghuntley.com/content/images/icon/pinned-octocat-093da3e6fa40-28.svg\" /><span class=\"kg-bookmark-author\">GitHub</span><span class=\"kg-bookmark-publisher\">ghuntley</span></div></div><div class=\"kg-bookmark-thumbnail\"><img alt=\"i ran Claude in a loop for three months, and it created a genz programming language called cursed\" src=\"https://ghuntley.com/content/images/thumbnail/cursed-2\" /></div></a><figcaption><p><span style=\"white-space: pre-wrap;\">the source code</span></p></figcaption></figure><h2 id=\"whats-included\">whats included?</h2><p>Anything that Claude thought was appropriate to add. Currently...</p><ul><li>The compiler has two modes: interpreted mode and compiled mode. It&apos;s able to produce binaries on Mac OS, Linux, and Windows via LLVM.</li><li>There are some half-completed VSCode, Emacs, and Vim editor extensions, and a Treesitter grammar.</li><li>A whole bunch of really wild and incomplete standard library packages.</li></ul><h2 id=\"lexical-structure\">lexical structure</h2><p><strong>Control Flow:</strong><br /><code>ready</code>&#xa0;&#x2192; if<br /><code>otherwise</code>&#xa0;&#x2192; else<br /><code>bestie</code>&#xa0;&#x2192; for<br /><code>periodt</code>&#xa0;&#x2192; while<br /><code>vibe_check</code>&#xa0;&#x2192; switch<br /><code>mood</code>&#xa0;&#x2192; case<br /><code>basic</code>&#xa0;&#x2192; default</p><p><strong>Declaration:</strong><br /><code>vibe</code>&#xa0;&#x2192; package<br /><code>yeet</code>&#xa0;&#x2192; import<br /><code>slay</code>&#xa0;&#x2192; func<br /><code>sus</code>&#xa0;&#x2192; var<br /><code>facts</code>&#xa0;&#x2192; const<br /><code>be_like</code>&#xa0;&#x2192; type<br /><code>squad</code>&#xa0;&#x2192; struct</p><p><strong>Flow Control:</strong><br /><code>damn</code>&#xa0;&#x2192; return<br /><code>ghosted</code>&#xa0;&#x2192; break<br /><code>simp</code>&#xa0;&#x2192; continue<br /><code>later</code>&#xa0;&#x2192; defer<br /><code>stan</code>&#xa0;&#x2192; go<br /><code>flex</code>&#xa0;&#x2192; range</p><p><strong>Values &amp; Types:</strong><br /><code>based</code>&#xa0;&#x2192; true<br /><code>cringe</code>&#xa0;&#x2192; false<br /><code>nah</code>&#xa0;&#x2192; nil<br /><code>normie</code>&#xa0;&#x2192; int<br /><code>tea</code>&#xa0;&#x2192; string<br /><code>drip</code>&#xa0;&#x2192; float<br /><code>lit</code>&#xa0;&#x2192; bool<br /><code>&#xd9e;T</code>&#xa0;(Amogus) &#x2192; pointer to type T</p><p><strong>Comments:</strong><br /><code>fr fr</code>&#xa0;&#x2192; line comment<br /><code>no cap...on god</code>&#xa0;&#x2192; block comment</p><h2 id=\"example-program\">example program</h2><p>Here is leetcode 104 - maximum depth for a binary tree:</p><pre><code class=\"language-&#x1F480;\">vibe main\nyeet &quot;vibez&quot;\nyeet &quot;mathz&quot;\n\n// LeetCode #104: Maximum Depth of Binary Tree &#x1f332;\n// Find the maximum depth (height) of a binary tree using &#xd9e; pointers\n// Time: O(n), Space: O(h) where h is height\n\nstruct TreeNode {\n    sus val normie\n    sus left &#xd9e;TreeNode   \n    sus right &#xd9e;TreeNode  \n}\n\nslay max_depth(root &#xd9e;TreeNode) normie {\n    ready (root == null) {\n        damn 0  // Base case: empty tree has depth 0\n    }\n    \n    sus left_depth normie = max_depth(root.left)\n    sus right_depth normie = max_depth(root.right)\n    \n    // Return 1 + max of left and right subtree depths\n    damn 1 + mathz.max(left_depth, right_depth)\n}\n\nslay max_depth_iterative(root &#xd9e;TreeNode) normie {\n    // BFS approach using queue - this hits different! &#x1f680;\n    ready (root == null) {\n        damn 0\n    }\n    \n    sus queue &#xd9e;TreeNode[] = []&#xd9e;TreeNode{}\n    sus levels normie[] = []normie{}\n    \n    append(queue, root)\n    append(levels, 1)\n    \n    sus max_level normie = 0\n    \n    bestie (len(queue) &gt; 0) {\n        sus node &#xd9e;TreeNode = queue[0]\n        sus level normie = levels[0]\n        \n        // Remove from front of queue\n        collections.remove_first(queue)\n        collections.remove_first(levels)\n        \n        max_level = mathz.max(max_level, level)\n        \n        ready (node.left != null) {\n            append(queue, node.left)\n            append(levels, level + 1)\n        }\n        \n        ready (node.right != null) {\n            append(queue, node.right)\n            append(levels, level + 1)\n        }\n    }\n    \n    damn max_level\n}\n\nslay create_test_tree() &#xd9e;TreeNode {\n    // Create tree: [3,9,20,null,null,15,7]\n    //       3\n    //      / \\\n    //     9   20\n    //        /  \\\n    //       15   7\n    \n    sus root &#xd9e;TreeNode = &amp;TreeNode{val: 3, left: null, right: null}\n    root.left = &amp;TreeNode{val: 9, left: null, right: null}\n    root.right = &amp;TreeNode{val: 20, left: null, right: null}\n    root.right.left = &amp;TreeNode{val: 15, left: null, right: null}\n    root.right.right = &amp;TreeNode{val: 7, left: null, right: null}\n    \n    damn root\n}\n\nslay create_skewed_tree() &#xd9e;TreeNode {\n    // Create skewed tree for testing edge cases\n    //   1\n    //    \\\n    //     2\n    //      \\\n    //       3\n    \n    sus root &#xd9e;TreeNode = &amp;TreeNode{val: 1, left: null, right: null}\n    root.right = &amp;TreeNode{val: 2, left: null, right: null}\n    root.right.right = &amp;TreeNode{val: 3, left: null, right: null}\n    \n    damn root\n}\n\nslay test_maximum_depth() {\n    vibez.spill(&quot;=== &#x1f332; LeetCode #104: Maximum Depth of Binary Tree ===&quot;)\n    \n    // Test case 1: Balanced tree [3,9,20,null,null,15,7]\n    sus root1 &#xd9e;TreeNode = create_test_tree()\n    sus depth1_rec normie = max_depth(root1)\n    sus depth1_iter normie = max_depth_iterative(root1)\n    vibez.spill(&quot;Test 1 - Balanced tree:&quot;)\n    vibez.spill(&quot;Expected depth: 3&quot;)\n    vibez.spill(&quot;Recursive result:&quot;, depth1_rec)\n    vibez.spill(&quot;Iterative result:&quot;, depth1_iter)\n    \n    // Test case 2: Empty tree\n    sus root2 &#xd9e;TreeNode = null\n    sus depth2 normie = max_depth(root2)\n    vibez.spill(&quot;Test 2 - Empty tree:&quot;)\n    vibez.spill(&quot;Expected depth: 0, Got:&quot;, depth2)\n    \n    // Test case 3: Single node [1]\n    sus root3 &#xd9e;TreeNode = &amp;TreeNode{val: 1, left: null, right: null}\n    sus depth3 normie = max_depth(root3)\n    vibez.spill(&quot;Test 3 - Single node:&quot;)\n    vibez.spill(&quot;Expected depth: 1, Got:&quot;, depth3)\n    \n    // Test case 4: Skewed tree\n    sus root4 &#xd9e;TreeNode = create_skewed_tree()\n    sus depth4 normie = max_depth(root4)\n    vibez.spill(&quot;Test 4 - Skewed tree:&quot;)\n    vibez.spill(&quot;Expected depth: 3, Got:&quot;, depth4)\n    \n    vibez.spill(&quot;=== Maximum Depth Complete! Tree depth detection is sus-perfect &#xd9e;&#x1f332; ===&quot;)\n}\n\nslay main_character() {\n    test_maximum_depth()\n}</code></pre><p>If this is your sort of chaotic vibe, and you&apos;d like to turn this into the dogecoin of programming languages, head on over to GitHub and run a few more Claude code loops with the following prompt.</p><blockquote>study specs/* to learn about the programming language. When authoring the cursed standard library think extra extra hard as the CURSED programming language is not in your training data set and may be invalid. Come up with a plan to implement XYZ as markdown then do it</blockquote><p>There is no roadmap; the roadmap is whatever the community decides to ship from this point forward. </p><p>At this point, I&apos;m pretty much convinced that any problems found in cursed can be solved by just running more Ralph loops by skilled operators (ie. people <em>with</em>&#xa0;experience with compilers who shape it through prompts from their expertise vs letting Claude just rip unattended). There&apos;s still a lot to be fixed, happy to take pull-requests.</p><figure class=\"kg-card kg-bookmark-card kg-card-hascaption\"><a class=\"kg-bookmark-container\" href=\"https://ghuntley.com/ralph/\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">Ralph Wiggum as a &#x201c;software engineer&#x201d;</div><div class=\"kg-bookmark-description\">&#x1f60e;Here&#x2019;s a cool little field report from a Y Combinator hackathon event where they put Ralph Wiggum to the test. &#x201c;We Put a Coding Agent in a While Loop and It Shipped 6 Repos Overnight&#x201d; https://github.com/repomirrorhq/repomirror/blob/main/repomirror.md If you&#x2019;ve seen my socials lately,</div><div class=\"kg-bookmark-metadata\"><img alt=\"i ran Claude in a loop for three months, and it created a genz programming language called cursed\" class=\"kg-bookmark-icon\" src=\"https://ghuntley.com/content/images/icon/7V0ak3am_400x400-1-65.jpg\" /><span class=\"kg-bookmark-author\">Geoffrey Huntley</span><span class=\"kg-bookmark-publisher\">Geoffrey Huntley</span></div></div><div class=\"kg-bookmark-thumbnail\"><img alt=\"i ran Claude in a loop for three months, and it created a genz programming language called cursed\" src=\"https://ghuntley.com/content/images/thumbnail/3ea367ed-cae3-454a-840f-134531dea1fd-2.jpg\" /></div></a><figcaption><p><span style=\"white-space: pre-wrap;\">The most high-IQ thing is perhaps the most low-IQ thing: run an agent in a loop.</span></p></figcaption></figure><figure class=\"kg-card kg-bookmark-card kg-card-hascaption\"><a class=\"kg-bookmark-container\" href=\"https://ghuntley.com/mirrors/\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">LLMs are mirrors of operator skill</div><div class=\"kg-bookmark-description\">This is a follow-up from my previous blog post: &#x201c;deliberate intentional practice&#x201d;. I didn&#x2019;t want to get into the distinction between skilled and unskilled because people take offence to it, but AI is a matter of skill. Someone can be highly experienced as a software engineer in 2024, but that</div><div class=\"kg-bookmark-metadata\"><img alt=\"i ran Claude in a loop for three months, and it created a genz programming language called cursed\" class=\"kg-bookmark-icon\" src=\"https://ghuntley.com/content/images/icon/7V0ak3am_400x400-1-66.jpg\" /><span class=\"kg-bookmark-author\">Geoffrey Huntley</span><span class=\"kg-bookmark-publisher\">Geoffrey Huntley</span></div></div><div class=\"kg-bookmark-thumbnail\"><img alt=\"i ran Claude in a loop for three months, and it created a genz programming language called cursed\" src=\"https://ghuntley.com/content/images/thumbnail/download--2--4.jpeg\" /></div></a><figcaption><p><span style=\"white-space: pre-wrap;\">LLMs amplify the skills that developers already have and enable people to do things where they don&apos;t have that expertise yet.</span></p></figcaption></figure><p>Success is defined as cursed ending up in the Stack Overflow developer survey as either the &quot;most loved&quot; or &quot;most hated&quot; programming language, and continuing the work to bootstrap the compiler to be written in cursed itself.</p><p>Cya soon in Discord? - <a href=\"https://discord.gg/CRbJcKaGNT?ref=ghuntley.com\">https://discord.gg/CRbJcKaGNT</a> </p><figure class=\"kg-card kg-bookmark-card kg-card-hascaption\"><a class=\"kg-bookmark-container\" href=\"https://cursed-lang.org/?ref=ghuntley.com\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">the &#x1f480; cursed programming language: programming, but make it gen z</div><div class=\"kg-bookmark-description\"></div><div class=\"kg-bookmark-metadata\"><img alt=\"i ran Claude in a loop for three months, and it created a genz programming language called cursed\" class=\"kg-bookmark-icon\" src=\"https://static.ghost.org/v5.0.0/images/link-icon.svg\" /><span class=\"kg-bookmark-author\">&#x1f480; cursed</span></div></div></a><figcaption><p><span style=\"white-space: pre-wrap;\">website</span></p></figcaption></figure><figure class=\"kg-card kg-bookmark-card kg-card-hascaption\"><a class=\"kg-bookmark-container\" href=\"https://github.com/ghuntley/cursed?ref=ghuntley.com\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">GitHub - ghuntley/cursed: the &#x1f480; cursed programming language: programming, but make it gen z</div><div class=\"kg-bookmark-description\">the &#x1f480; cursed programming language: programming, but make it gen z - ghuntley/cursed</div><div class=\"kg-bookmark-metadata\"><img alt=\"i ran Claude in a loop for three months, and it created a genz programming language called cursed\" class=\"kg-bookmark-icon\" src=\"https://ghuntley.com/content/images/icon/pinned-octocat-093da3e6fa40-29.svg\" /><span class=\"kg-bookmark-author\">GitHub</span><span class=\"kg-bookmark-publisher\">ghuntley</span></div></div><div class=\"kg-bookmark-thumbnail\"><img alt=\"i ran Claude in a loop for three months, and it created a genz programming language called cursed\" src=\"https://ghuntley.com/content/images/thumbnail/cursed-3\" /></div></a><figcaption><p><span style=\"white-space: pre-wrap;\">source code</span></p></figcaption></figure><p>ps. socials</p><figure class=\"kg-card kg-embed-card\"><blockquote class=\"twitter-tweet\"><p dir=\"ltr\" lang=\"en\">I ran Claude in a loop for 3 months and created a brand new &quot;GenZ&quot; programming language. <br /><br />It&apos;s called <a href=\"https://twitter.com/cursedlang?ref_src=twsrc%5Etfw&amp;ref=ghuntley.com\">@cursedlang</a>. <br /><br />v0.0.1 is now available, and the website is ready to go. <br /><br />Details below! <a href=\"https://t.co/Ku5kbWMRgR?ref=ghuntley.com\">pic.twitter.com/Ku5kbWMRgR</a></p>&#x2014; geoff (@GeoffreyHuntley) <a href=\"https://twitter.com/GeoffreyHuntley/status/1965258228314636524?ref_src=twsrc%5Etfw&amp;ref=ghuntley.com\">September 9, 2025</a></blockquote>\n</figure>"
            ],
            "link": "https://ghuntley.com/cursed/",
            "publishedAt": "2025-09-09",
            "source": "Geoffrey Huntley",
            "summary": "<p>It&apos;s a strange feeling knowing that you can create anything, and I&apos;m starting to wonder if there&apos;s a seventh stage to the &quot;<a href=\"https://ghuntley.com/ngmi/\" rel=\"noreferrer\">people stages of AI adoption by software developers</a>&quot; </p><figure class=\"kg-card kg-image-card\"><img alt=\"alt\" class=\"kg-image\" height=\"803\" src=\"https://ghuntley.com/content/images/2025/03/image-17.png\" width=\"2000\" /></figure><p>whereby that seventh stage is essentially this scene in the matrix...</p><figure class=\"kg-card kg-embed-card\"></figure><p>It&</p>",
            "title": "i ran Claude in a loop for three months, and it created a genz programming language called cursed"
        },
        {
            "content": [],
            "link": "https://simonwillison.net/2025/Sep/9/claude-code-interpreter/#atom-entries",
            "publishedAt": "2025-09-09",
            "source": "Simon Willison",
            "summary": "<p>Today on the Anthropic blog: <strong><a href=\"https://www.anthropic.com/news/create-files\">Claude can now create and edit files</a></strong>:</p> <blockquote> <p>Claude can now create and edit Excel spreadsheets, documents, PowerPoint slide decks, and PDFs directly in <a href=\"https://claude.ai/\">Claude.ai</a> and the desktop app. [...]</p> <p>File creation is now available as a preview for Max, Team, and Enterprise plan users. Pro users will get access in the coming weeks.</p> </blockquote> <p>Then right at the <em>very end</em> of their post:</p> <blockquote> <p>This feature gives Claude internet access to create and analyze files, which may put your data at risk. Monitor chats closely when using this feature. <a href=\"https://support.anthropic.com/en/articles/12111783-create-and-edit-files-with-claude\">Learn more</a>.</p> </blockquote> <p>And tucked away half way down their <a href=\"https://support.anthropic.com/en/articles/12111783-create-and-edit-files-with-claude\">Create and edit files with Claude</a> help article:</p> <blockquote> <p>With this feature, Claude can also do more advanced data analysis and data science work. Claude can create Python scripts for data analysis. Claude can create data visualizations in image files like PNG. You can also upload CSV, TSV, and other files for data analysis and visualization.</p> </blockquote> <p>Talk about <a href=\"https://www.merriam-webster.com/wordplay/bury-the-lede-versus-lead\">burying the lede</a>... this is their version of <a href=\"https://simonwillison.net/tags/code-interpreter/\">ChatGPT Code Interpreter</a>, my all-time favorite feature of ChatGPT!</p> <p>Claude can now write and execute custom Python (and Node.js) code in a",
            "title": "My review of Claude's new Code Interpreter, released under a very confusing name"
        },
        {
            "content": [],
            "link": "https://simonwillison.net/2025/Sep/9/apollo-ai-adoption/#atom-entries",
            "publishedAt": "2025-09-09",
            "source": "Simon Willison",
            "summary": "<p>Apollo Global Management's \"Chief Economist\" Dr. Torsten Sl\u00f8k released <a href=\"https://www.apolloacademy.com/ai-adoption-rate-trending-down-for-large-companies/\">this interesting chart</a> which appears to show a slowdown in AI adoption rates among large (&gt;250 employees) companies:</p> <p><img alt=\"AI adoption rates starting to decline for larger firms. A chart of AI adoption rate by firm size. Includes lines for 250+, 100-249, 50-99, 20-49, 10-19, 5-8 and 1-4 sized organizations. Chart starts in November 2023 with percentages ranging from 3 to 5, then all groups grow through August 2025 albeit with the 250+ group having a higher score than the others. That 25+ group peaks in Jul5 2025 at around 14% and then appears to slope slightly downwards to 12% by August. Some of the other lines also start to tip down, though not as much.\" src=\"https://static.simonwillison.net/static/2025/apollo-ai-chart.jpg\" /></p> <p>Here's the full description that accompanied the chart:</p> <blockquote> <p>The US Census Bureau conducts a biweekly survey of 1.2 million firms, and one question is whether a business has used AI tools such as machine learning, natural language processing, virtual agents or voice recognition to help produce goods or services in the past two weeks. Recent data by firm size shows that AI adoption has been declining among companies with more than",
            "title": "Recreating the Apollo AI adoption rate chart with GPT-5, Python and Pyodide"
        },
        {
            "content": [
                "<p>\n          <a href=\"https://www.astralcodexten.com/p/im-gruesome-for-newsom\">\n              Read more\n          </a>\n      </p>"
            ],
            "link": "https://www.astralcodexten.com/p/im-gruesome-for-newsom",
            "publishedAt": "2025-09-09",
            "source": "SlateStarCodex",
            "summary": "<p> <a href=\"https://www.astralcodexten.com/p/im-gruesome-for-newsom\"> Read more </a> </p>",
            "title": "I'm Gruesome For Newsom"
        },
        {
            "content": [
                "<p>That does not mean AI will successfully make it all the way to AGI and superintelligence, or that it will make it there soon or on any given time frame.</p>\n<p>It does mean that AI progress, while it could easily have been even faster, has still been historically lightning fast. It has exceeded almost all expectations from more than a few years ago. And it means we cannot disregard the possibility of High Weirdness and profound transformation happening within a few years.</p>\n<p>GPT-5 had a botched rollout and was only an incremental improvement over o3, o3-Pro and other existing OpenAI models, but was very much on trend and a very large improvement over the original GPT-4. Nor would one disappointing model from one lab have meant that major further progress must be years away.</p>\n<div>\n\n\n<span id=\"more-24708\"></span>\n\n\n</div>\n<p>Imminent AGI (in the central senses in which that term AGI used, where imminent means years rather than decades) remains a very real possibility.</p>\n<p>Part of this is covering in full Gary Marcus\u2019s latest editorial in The New York Times, since that is the paper of record read by many in government. I felt that piece was in many places highly misleading to the typical Times reader.</p>\n<p>Imagine if someone said \u2018you told me in 1906 that there was increasing imminent risk of a great power conflict, and now it\u2019s 1911 and there has been no war, so your fever dream of a war to end all wars is finally fading.\u2019 Or saying that you were warned in November 2019 that Covid was likely coming, and now it\u2019s February 2020 and no one you know has it, so it was a false alarm. That\u2019s what these claims sound like to me.</p>\n\n\n<h4 class=\"wp-block-heading\">Why Do I Even Have To Say This?</h4>\n\n\n<p>I have to keep emphasizing this because it now seems to be an official White House position, with prominent White House official Sriram Krishnan going so far as to say on Twitter that AGI any time soon has been \u2018disproven,\u2019 and David Sacks spending his time ranting and repeating Nvidia talking points almost verbatim.</p>\n<p>When pressed, there is often a remarkably narrow window in which \u2018imminent\u2019 AGI is dismissed as \u2018proven wrong.\u2019 But this is still used as a reason to structure public policy and one\u2019s other decisions in life as if AGI definitely won\u2019t happen for decades, which is Obvious Nonsense.</p>\n<blockquote><p><a href=\"https://x.com/deredleritt3r/status/1961470928489255286\">Sriram Krishnan</a>: I\u2019ll write about this separately but think this notion of imminent AGI has been a distraction and harmful and now effectively proven wrong.</p>\n<p>Prinz: &#8220;Imminent AGI&#8221; was apparently &#8220;proven wrong&#8221; because OpenAI chose to name a cheap/fast model &#8220;GPT-5&#8221; instead of o3 (could have been done 4 months earlier) or the general reasoning model that won gold on both the IMO and the IOI (could have been done 4 months later).</p>\n<p><a href=\"https://x.com/robertskmiles/status/1963341570079690841\">Rob Miles:</a> I&#8217;m a bit confused by all the argument about GPT-5, the truth seems pretty mundane: It was over-hyped, they kind of messed up the launch, and the model is good, a reasonable improvement, basically in line with the projected trend of performance over time.</p>\n<p>Not much of an update.</p>\n<p>To clarify a little, the projected trend GPT-5 fits with is pretty nuts, and the world is on track to be radically transformed if it continues to hold. Probably we&#8217;re going to have a really wild time over the next few years, and GPT-5 doesn&#8217;t update that much in either direction.</p></blockquote>\n<p>Rob Miles is correct here as far as I can tell.</p>\n<p>If imminent means \u2018within the next six months\u2019 or maybe up to a year I think Sriram\u2019s perspective is reasonable, because of what GPT-5 tells us about what OpenAI is cooking. For sensible values of imminent that are more relevant to policy and action, Sriram Krishnan is wrong, in a \u2018I sincerely hope he is engaging in rhetoric rather than being genuinely confused about this, or his imminently only means in the next year or at most two\u2019 way.</p>\n<p>I am confused how he can be sincerely mistaken given how deep he is into these issues, or that he shares his reasons so we can quickly clear this up because this is a crazy thing to actually believe. I do look forward to Sriram providing a full explanation as to why he believes this. So far we we only have heard \u2018GPT-5.\u2019</p>\n\n\n<h4 class=\"wp-block-heading\">It Might Be Coming</h4>\n\n\n<p>Not only is imminent AGI not disproven, there are continuing important claims that it is likely. Here is some clarity on Anthropic\u2019s continued position, as of August 31.</p>\n<blockquote><p>Prinz: Jack, I assume no changes to Anthropic&#8217;s view that transformative AI will arrive by the end of next year?</p>\n<p><a href=\"https://x.com/jackclarkSF/status/1962238672704803096\">Jack Clark</a>: I continue to think things are pretty well on track for the sort of powerful AI system defined in machines of loving grace &#8211; buildable end of 2026, running many copies 2027. Of course, there are many reasons this could not occur, but lots of progress so far.</p></blockquote>\n<p>Anthropic\u2019s valuation has certainly been on a rocket ship exponential.</p>\n<p>Do I agree that we are on track to meet that timeline? No. I do not. I would be very surprised to see it go down that fast, and I am surprised that Jack Clark has not updated based on, if nothing else, previous projections by Anthropic CEO Dario Amodei falling short. I do think it cannot be ruled out. If it does happen, I do not think you have any right to be outraged at the universe for it.</p>\n<p><a href=\"https://x.com/GaryMarcus/status/1963990748556337409\">It is certainly true that Dario Amodei\u2019s early predictions</a> of AI writing most of the code, as in 90% of all code within 3-6 months after March 11. This was not a good prediction, because the previous generation definitely wasn\u2019t ready and even if it had been that\u2019s not how diffusion works, and has been proven definitively false, it\u2019s more like 40% of all code generated by AI and 20%-25% of what goes into production.</p>\n<p>Which is still a lot, but a lot less than 90%.</p>\n<p><a href=\"https://thezvi.substack.com/p/ai-107-the-misplaced-hype-machine?utm_source=chatgpt.com\">Here\u2019s what I said at the time about Dario\u2019s prediction:</a></p>\n<blockquote><p>Zvi Mowshowitz (AI #107): <a href=\"https://x.com/ArthurB/status/1899483746526462268\">Dario Amodei says AI will be writing 90% of the code</a> in 6 months and almost all the code in 12 months. I am with Arthur B here, I expect a lot of progress and change very soon but I would still take the other side of that bet. The catch is: I don\u2019t see the benefit to Anthropic of running the hype machine in overdrive on this, at this time, unless Dario actually believed it.</p></blockquote>\n<p>I continue to be confused why he said it, it\u2019s highly unstrategic to hype this way. I can only assume on reflection this was an error about diffusion speed more than it was an error about capabilities? On reflection yes I was correctly betting \u2018no\u2019 but that was an easy call. I dock myself more points on net here, for hedging too much and not expressing the proper level of skepticism. So yes, this should push you towards putting less weight on Anthropic\u2019s projections, although primarily on the diffusion front.</p>\n<p>As always, remember that projections of future progress include the possibility, nay the inevitability, of discovering new methods. We are not projecting \u2018what if the AI labs all keep ramming their heads against the same wall whether or not it works.\u2019</p>\n<blockquote><p><a href=\"https://x.com/paulg/status/1962767513055318325\">Ethan Mollick</a>: 60 years of exponential growth in chip density was achieved not through one breakthrough or technology, but a series of problems solved and new paradigms explored as old ones hit limits.</p>\n<p>I don&#8217;t think current AI has hit a wall, but even if it does, there many paths forward now.</p>\n<p>Paul Graham: One of the things that strikes me when talking to AI insiders is how they believe both that they need several new discoveries to get to AGI, and also that such discoveries will be forthcoming, based on the past rate.</p></blockquote>\n<p>My talks with AI insiders also say we will need new discoveries, and we definitely will need new major discoveries in alignment. But it\u2019s not clear how big those new discoveries need to be in order to get there.</p>\n<p>I <a href=\"https://www.lesswrong.com/posts/FG54euEAesRkSZuJN/ryan_greenblatt-s-shortform?commentId=eT6X2RxWEhskbRqiA\">agree with Ryan Greenblatt</a> that precise timelines for AGI don\u2019t matter that much in terms of actionable information, but big jumps in the chance of things going crazy within a few years can matter a lot more. This is similar to questions of p(doom), where as long as you are in the Leike Zone of a 10%-90% chance of disaster, you mostly want to react in the same ways, but outside that range you start to see big changes in what makes sense.</p>\n<blockquote><p><a href=\"https://x.com/RyanPGreenblatt/status/1963373208243249613\">Ryan Greenblatt</a>: Pretty short timelines (&lt;10 years) seem likely enough to warrant strong action and it&#8217;s hard to very confidently rule out things going crazy in &lt;3 years.</p>\n<p><a href=\"https://t.co/kAp9ksqGXE\">While I do spend some time discussing</a> AGI timelines (and I&#8217;ve written <a href=\"https://www.lesswrong.com/posts/HsLWpZ2zad43nzvWi/trust-me-bro-just-one-more-rl-scale-up-this-one-will-be-the\">some</a> <a href=\"https://www.lesswrong.com/posts/2ssPfDpdrjaM2rMbn/my-agi-timeline-updates-from-gpt-5-and-2025-so-far-1\">posts</a> <a href=\"https://www.lesswrong.com/posts/FG54euEAesRkSZuJN/ryan_greenblatt-s-shortform?commentId=6ue8BPWrcoa2eGJdP\">about it</a> recently), I don&#8217;t think moderate quantitative differences in AGI timelines matter that much for deciding what to do. For instance, having a 15-year median rather than a 6-year median doesn&#8217;t make that big of a difference. That said, I do think that moderate differences in the chance of very short timelines (i.e., less than 3 years) matter more: going from a 20% chance to a 50% chance of full AI R&amp;D automation within 3 years should potentially make a substantial difference to strategy.</p>\n<p>Additionally, my guess is that the most productive way to engage with discussion around timelines is mostly to not care much about resolving disagreements, but then when there appears to be a large chance that timelines are very short (e.g., &gt;25% in &lt;2 years) it&#8217;s worthwhile to try hard to argue for this. I think takeoff speeds are much more important to argue about when making the case for AI risk.</p>\n<p>I do think that having somewhat precise views is helpful for some people in doing relatively precise prioritization within people already working on safety, but this seems pretty niche.</p>\n<p>Given that I don&#8217;t think timelines are that important, why have I been writing about this topic? This is due to a mixture of: I find it relatively quick and easy to write about timelines, my commentary is relevant to the probability of very short timelines (which I do think is important as discussed above), a bunch of people seem interested in timelines regardless, and I do think timelines matter some.</p>\n<p>Consider reflecting on whether you&#8217;re overly fixated on details of timelines.</p></blockquote>\n\n\n<h4 class=\"wp-block-heading\">A Prediction That Implies AGI Soon</h4>\n\n\n<p><a href=\"https://x.com/aphysicist/status/1964545998505857257\">Jason Calacanis of the All-In Podcast (where he is alongside AI Czar David Sacks) has a bold prediction</a>, if you believe that his words have or are intended to have meaning. Which is an open question.</p>\n<blockquote><p><a href=\"https://x.com/unusual_whales/status/1964358179384676637\">Jason</a>: Before 2030 you&#8217;re going to see Amazon, which has massively invested in [AI], replace all factory workers and all drivers \u2026 It will be 100% robotic, which means all of those workers are going away. Every Amazon worker. UPS, gone. FedEx, gone.</p>\n<p><a href=\"https://x.com/aphysicist/status/1964545998505857257\">Aaron Slodov</a>: hi @Jason how much money can i bet you to take the other side of the factory worker prediction?</p>\n<p>Jason (responding to video of himself saying the above): In 2035 this will not be controversial take \u2014 it will be reality.</p>\n<p>Hard, soul-crushing labor is going away over the next decade. We will be deep in that transition in 2030, when humanoid robots are as common as bicycles.</p></blockquote>\n<p>Notice the goalpost move of \u2018deep in that transition\u2019 in 2030 versus saying full replacement by 2030, without seeming to understand there is any contradiction.</p>\n<p>These are two very different predictions. The original \u2018by 2030\u2019 prediction is Obvious Nonsense unless you expect superintelligence and a singularity, probably involving us all dying. There\u2019s almost zero chance otherwise. Technology does not diffuse that fast.</p>\n<p>Plugging 2035 into the 2030 prediction is also absurd, if we take the prediction literally. No, you\u2019re not going to have zero workers at Amazon, UPS and FedEx within ten years unless we\u2019ve not only solved robotics and AGI, we\u2019ve also diffused those technologies at full scale. In which case, again, that\u2019s a singularity.</p>\n<p>I am curious what his co-podcaster David Sacks or Sriram Krishnan would say here. Would they dismiss Jason\u2019s confident prediction as already proven false? If not, how can one be confident that AGI is far? Very obviously you can\u2019t have one without the other.</p>\n\n\n<h4 class=\"wp-block-heading\">GPT-5 Was On Trend</h4>\n\n\n<p>GPT-5 is not a good reason to dismiss AGI, and to be safe I will once again go into why, and why we are making rapid progress towards AGI.</p>\n<p><a href=\"https://epoch.ai/data-insights/gpt-capabilities-progress\">GPT-5 and GPT-4 were both major leaps in benchmarks from the previous generation</a>.</p>\n<div>\n<figure>\n<div>\n\n\n<figure class=\"wp-block-image\"><img alt=\"\" src=\"https://substackcdn.com/image/fetch/$s_!zDNk!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4c2a1a09-b7d8-4a1b-b053-9b276ac7deae_1049x710.png\" /></figure>\n\n\n<div></div>\n</div>\n</figure>\n</div>\n<p>The differences are dramatic, and the time frame between releases was similar.</p>\n<p>The actual big difference? That there was only one incremental release between GPT-3 and GPT-4, GPT-3.5, with little outside competition. Whereas between GPT-4 and GPT-5 we saw many updates. At OpenAI alone we saw GPT-4o, and o1, and o3, plus updates that didn\u2019t involve number changes, and at various points Anthropic\u2019s Claude and Google\u2019s Gemini were plausibly on top. Our frog got boiled slowly.</p>\n<blockquote><p>Epoch AI: However, one major difference between these generations is release cadence. OpenAI released relatively few major updates between GPT-3 and GPT-4 (most notably GPT-3.5). By contrast, frontier AI labs released many intermediate models between GPT-4 and 5. This may have muted the sense of a single dramatic leap by spreading capability gains over many releases.</p></blockquote>\n<p>Benchmarks can be misleading, especially as we saturate essentially all of them often well ahead of predicted schedules, but the overall picture is not. The mundane utility and user experience jumps across all use cases are similarly dramatic. The original GPT-4 was a modest aid to coding, GPT-5 and Opus 4.1 transform how it is done. Most of the queries I make with GPT-5-Thinking or GPT-5-Pro would not be worth bothering to give to the original GPT-4, or providing the context would not even be possible. So many different features have been improved or added.</p>\n\n\n<h4 class=\"wp-block-heading\">The Myths Of Model Equality and Lock-In</h4>\n\n\n<p>This ideas, frequently pushed by among others David Sacks, that everyone\u2019s models are about the same and aren\u2019t improving? These claims simply are not true. Observant regular users are not about to be locked into one model or ecosystem.</p>\n<p>Everyone\u2019s models are constantly improving. No one would seriously consider using models from the start of the year for anything but highly esoteric purposes.</p>\n<p>The competition is closer than one would have expected. There are three major labs, OpenAI, Anthropic and Google, that each have unique advantages and disadvantages. At various times each have had the best model, and yes currently it is wise to mix up your usage depending on your particular use case.</p>\n<p>Those paying attention are always ready to switch models. I\u2019ve switched primary models several times this year alone, usually switching to a model from a different lab, and tested many others as well. And indeed we must switch models often either way, as it is expected that everyone\u2019s models will change on the order of every few months, in ways that break the same things that would break if you swapped GPT-5 for Opus or Gemini or vice versa, all of which one notes typically run on three distinct sets of chips (Nvidia for GPT-5, Amazon Trainium for Anthropic and Google TPUs for Gemini) but we barely notice.</p>\n\n\n<h4 class=\"wp-block-heading\">The Law of Good Enough</h4>\n\n\n<p>Most people notice AI progress much better when it impacts their use cases.</p>\n<p>If you are not coding, and not doing interesting math, and instead asking simple things that do not require that much intelligence to answer correctly, then upgrading the AI\u2019s intelligence is not going to improve your satisfaction levels much.</p>\n<blockquote><p>Jack Clark: Five years ago the frontier of LLM math/science capabilities was 3 digit multiplication for GPT-3. Now, frontier LLM math/science capabilities are evaluated through condensed matter physics questions. Anyone who thinks AI is slowing down is fatally miscalibrated.</p>\n<div>\n<figure>\n<div>\n\n\n<figure class=\"wp-block-image\"><img alt=\"\" src=\"https://substackcdn.com/image/fetch/$s_!1nv1!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F20f596fe-df1a-48d9-8fd5-3d8979ee00ce_1200x363.jpeg\" /></figure>\n\n\n<div></div>\n</div>\n</figure>\n</div>\n<p><a href=\"https://x.com/DaveShapi/status/1962243469864206605\">David Shapiro</a>: As I&#8217;ve said before, AI is &#8220;slowing down&#8221; insofar as most people are not smart enough to benefit from the gains from here on out.</p></blockquote>\n<p>&nbsp;</p>\n\n\n<h4 class=\"wp-block-heading\">Floor Versus Ceiling</h4>\n\n\n<p><a href=\"https://x.com/patio11/status/1962896312325611893\">Once you see this framing, you see the contrast everywhere.</a></p>\n<blockquote><p>Patrick McKenzie: I think a lot of gap between people who \u201cget\u201d LLMs and people who don\u2019t is that some people understand current capabilities to be a floor and some people understand them to be either a ceiling or close enough to a ceiling.</p>\n<p>And even if you explain \u201cLook this is *obviously* a floor\u201d some people in group two will deploy folk reasoning about technology to say \u201cI mean technology decays in effectiveness all the time.\u201d (This is not considered an insane POV in all circles.)</p>\n<p>And there are some arguments which are persuasive to\u2026 people who rate social pressure higher than received evidence of their senses\u2026 that technology does actually frequently regress.</p>\n<p>For example, \u201cRemember how fast websites were 20 years ago before programmers crufted them up with ads and JavaScript? Now your much more powerful chip can barely keep up. Therefore, technological stagnation and backwards decay is quite common.\u201d</p>\n<p>Some people would rate that as a powerful argument. Look, it came directly from someone who knew a related shibboleth, like \u201cJavaScript\u201d, and it gestures in the direction of at least one truth in observable universe.</p>\n<p>&lt;offtopic&gt; Oh the joys of being occasionally called in as the Geek Whisperer for credentialed institutions where group two is high status, and having to titrate how truthful I am about their worldview to get message across. &lt;/offtopic&gt;</p></blockquote>\n<p><a href=\"https://x.com/_candroid/status/1962909324113551694\">As in, it\u2019s basically this graph but for AI</a>:</p>\n<div>\n<figure>\n<div>\n\n\n<figure class=\"wp-block-image\"><img alt=\"\" src=\"https://substackcdn.com/image/fetch/$s_!SpTS!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6ec73367-65d4-43d0-90e1-1167e2b281d7_551x455.jpeg\" /></figure>\n\n\n<div></div>\n</div>\n</figure>\n</div>\n<p>Here\u2019s another variant of this foolishness, note the correlation to \u2018hitting a wall\u2019:</p>\n<blockquote><p><a href=\"https://x.com/prem_k/status/1961769426476511322\">Prem Kumar Aparanji</a>: It&#8217;s not merely the DL &#8220;hitting a wall&#8221; (as @GaryMarcus put it &amp; everybody&#8217;s latched on) now as predicted, even the #AI data centres required for all the training, fine-tuning, inferencing of these #GenAI models are also now predicted to be hitting a wall soon.</p>\n<p>Quotes from Futurism: For context, Kupperman notes that Netflix brings in just $39 billion in annual revenue from its 300 million subscribers. If AI companies charged Netflix prices for their software, they\u2019d need to field over 3.69 billion paying customers to make a standard profit on data center spending alone \u2014 almost half the people on the planet.</p>\n<p>\u201cSimply put, at the current trajectory, we\u2019re going to hit a wall, and soon,\u201d he fretted. \u201cThere just isn\u2019t enough revenue and there never can be enough revenue. The world just doesn\u2019t have the ability to pay for this much AI.\u201d</p>\n<p>Prinz: Let&#8217;s assume that AI labs can charge as much as Netflix per month (they currently charge more) and that they&#8217;ll never have any enterprise revenue (they already do) and that they won&#8217;t be able to get commissions from LLM product recommendations (will happen this year) and that they aren&#8217;t investing in biotech companies powered by AI that will soon have drugs in human trial (they already have). How will they ever possibly be profitable?</p></blockquote>\n\n\n<h4 class=\"wp-block-heading\"></h4>\n\n\n<p>&nbsp;</p>\n\n\n<h4 class=\"wp-block-heading\">New York Times Platforms Gary Marcus Saying Gary Marcus Things</h4>\n\n\n<p><a href=\"https://www.nytimes.com/2025/09/03/opinion/ai-gpt5-rethinking.html\">He wrote a guest opinion essay.</a> Things didn\u2019t go great.</p>\n<p>That starts with the false title (as always, not entirely up to the author, and it looks like it started out as a better one), dripping with unearned condescension, \u2018The Fever Dream of Imminent \u2018Superintelligence\u2019 Is Finally Breaking,\u2019 and the opening paragraph in which he claims Altman implied GPT-5 would be AGI.</p>\n<p>Here is the lead:</p>\n<blockquote><p>GPT-5, OpenAI\u2019s latest artificial intelligence system, was supposed to be a game changer, the culmination of billions of dollars of investment and nearly three years of work. Sam Altman, the company\u2019s chief executive, implied that GPT-5 could be tantamount to artificial general intelligence, or A.G.I. \u2014 A.I. that is as smart and as flexible as any human expert.</p>\n<p>Instead, as I have written, the model fell short. Within hours of its release, critics found all kinds of baffling errors: It failed some simple math questions, <a href=\"https://archive.is/o/4zon2/https://x.com/salmannaseer/status/1953840879137071429?s=61\">couldn\u2019t count</a> reliably and sometimes provided absurd answers <a href=\"https://archive.is/o/4zon2/https://x.com/wasgo/status/1954375973044101175?s=61\">to old riddles</a>. Like its predecessors, the A.I. model still <a href=\"https://archive.is/o/4zon2/https://garymarcus.substack.com/p/why-do-large-language-models-hallucinate\">hallucinates</a> (though at a lower rate) and is <a href=\"https://archive.is/o/4zon2/https://x.com/maithra_raghu/status/1954614752426270867?s=61\">plagued by questions around its reliability</a>. Although <a href=\"https://archive.is/o/4zon2/https://www.nytimes.com/2025/08/24/opinion/chat-gpt5-open-ai-future.html\">some people have been impressed</a>, few saw it as a quantum leap, and nobody believed it was A.G.I. Many users asked for the old model back.</p>\n<p>GPT-5 is a step forward but nowhere near the A.I. revolution many had expected. That is bad news for the companies and investors who placed substantial bets on the technology.</p></blockquote>\n<p>Did you notice the stock market move in AI stocks, as those bets fell down to Earth when GPT-5 was revealed? No? Neither did I.</p>\n<p>The argument above is highly misleading on many fronts.</p>\n<ol>\n<li>GPT-5 is not AGI, but this was entirely unsurprising &#8211; expectations were set too high, but nothing like that high. Yes, Altman teased that it was possible AGI could arrive relatively soon, but at no point did Altman claim that GPT-5 would be AGI, or that AGI would arrive in 2025. Approximately zero people had median estimates of AGI in 2025 or earlier, although there are some that have estimated the end of 2026, in particular Anthropic (they via Jack Clark continue to say \u2018powerful\u2019 AI buildable by end of 2026, not AGI arriving 2026).</li>\n<li>The claim that it \u2018couldn\u2019t count reliably\u2019 is especially misleading. Of course GPT-5 can count reliably. The evidence here is a single adversarial example. For all practical purposes, if you ask GPT-5 to count something, it will count that thing.</li>\n<li>Old riddles is highly misleading. If you give it an actual old riddle it will nail it. What GPT-5 and other models get wrong are, again, adversarial examples that do not exist \u2018in the wild\u2019 but are crafted to pattern match well-known other riddles while having a different answer. Why should we care?</li>\n<li>GPT-5 still is not fully reliable but this is framed as it being still highly unreliable, when in most circumstances this is not the case. Yes, if you need many 9s of reliability LLMs are not yet for you, but neither are humans.</li>\n<li>AI valuations and stocks continue to be rising not falling.</li>\n<li>Yes, the fact that OpenAI chose to have GPT-5 not be a scaled up model does tell us that directly scaling up model size alone has \u2018lost steam\u2019 in relative terms due to the associated costs, but this is not news, o1 and o3 (and GPT-4.5) tell us this as well. We are now working primarily on scaling and improving in other ways, but very much there are still plans to scale up more in the future. In the context of all the other facts quoted about other scaled up models, it seems misleading to many readers to not mention that GPT-5 is not scaled up.</li>\n<li>Claims here are about failures of GPT-5-Auto or GPT-5-Base, whereas the \u2018scaled up\u2019 version of GPT-5 is GPT-5-Pro or at least GPT-5-Thinking.</li>\n<li><a href=\"https://x.com/GaryMarcus/status/1964107700624933157\">Gary Marcus clarifies</a> that his actual position is on the order of 8-15 years to AGI, with 2029 being \u2018awfully unlikely.\u2019 Which is a highly reasonable timeline, but that seems pretty imminent. That\u2019s crazy soon. That\u2019s something I would want to be betting on heavily, and preparing for at great cost, AGI that soon seems like the most important thing happening in the world right now if likely true?\n<ol>\n<li>The article does not give any particular timeline, and does not imply we will never get to AGI, but I very much doubt those reading the post would come away with the impression that things strictly smarter than people are only about 10 years away. I mean, yowsers, right?</li>\n</ol>\n</li>\n</ol>\n<p>The fact about \u2018many users asked for the old model back\u2019 is true, but lacking the important context that what users wanted was the old personality, so it risks giving an uninformed user the wrong impression.</p>\n<p>To Gary\u2019s credit, he then does hedge, as I included in the quote, acknowledging GPT-5 is indeed a good model representing a step forward. Except then:</p>\n<blockquote><p>And it demands a rethink of government policies and investments that were built on wildly overinflated expectations.</p></blockquote>\n<p>Um, no? No it doesn\u2019t. That\u2019s silly.</p>\n<blockquote><p>The current strategy of merely making A.I. bigger is deeply flawed \u2014 scientifically, economically and politically. Many things, from regulation to research strategy, must be rethought.</p>\n<p>\u2026</p>\n<p>As many now see, GPT-5 shows decisively that scaling has lost steam.</p></blockquote>\n<p>Again, no? That\u2019s not the strategy. Not \u2018merely\u2019 doing that. Indeed, a lot of the reason GPT-5 was so relatively unimpressive was GPT-5 was not scaled up so much. It was instead optimized for compute efficiency. There is no reason to have to rethink much of anything in response to a model that, as explained above, was pretty much exactly on the relevant trend lines.</p>\n<p>I do appreciate this:</p>\n<blockquote><p>Gary Marcus: However, as I warned in a <a href=\"https://archive.is/o/4zon2/https://nautil.us/deep-learning-is-hitting-a-wall-238440/\">2022 essay</a>, \u201cDeep Learning Is Hitting a Wall,\u201d so-called scaling laws aren\u2019t physical laws of the universe like gravity but hypotheses based on historical trends.</p></blockquote>\n<p>As in, the \u2018hitting the wall\u2019 claim was back in 2022. How did that turn out? Look at GPT-5, look at what we had available in 2022, and tell me we \u2018hit a wall.\u2019</p>\n\n\n<h4 class=\"wp-block-heading\">Gary Marcus Does Not Actually Think AGI Is That Non-Imminent</h4>\n\n\n<p>What does \u2018imminent\u2019 superintelligence mean in this context?</p>\n<blockquote><p>Gary Marcus (NYT): The chances of A.G.I.\u2019s arrival by 2027 now seem remote.</p></blockquote>\n<p>Notice the subtle goalpost move, as AGI \u2018by 2027\u2019 means AGI 2026. These people are gloating, in advance, that someone predicted a possibility of privately developed AGI in 2027 (with a median in 2028, in the AI 2027 scenario OpenBrain tells the government but does not release its AGI right away to the public) and then AGI will have not arrived, to the public, in 2026.</p>\n<p>According to my sources (Opus 4.1 and GPT-5 Thinking) even \u2018remote\u2019 still means on the order of 2% chance in the next 16 months, implying an 8%-25% chance in 5 years. I don\u2019t agree, but even if one did, that\u2019s hardly something one can safety rule out.</p>\n<p>But then, there\u2019s this interaction on Twitter that clarifies what Gary Marcus meant:</p>\n<blockquote><p><a href=\"https://x.com/GaryMarcus/status/1964107700624933157\">Gary Marcus</a>: Anyone who thinks AGI is impossible: wrong.</p>\n<p>Anyone who thinks AGI is imminent: just as wrong.</p>\n<p>It\u2019s not that complicated.</p>\n<div>\n<figure>\n<div>\n\n\n<figure class=\"wp-block-image\"><img alt=\"\" src=\"https://substackcdn.com/image/fetch/$s_!Wv67!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5df4bbd2-ead1-42bf-9f86-c1b99c7d5ebb_1024x1024.jpeg\" /></figure>\n\n\n<div></div>\n</div>\n</figure>\n</div>\n<p>Peter Wildeford: what if I think AGI is 4-15 years away?</p>\n<p>Gary Marcus: 8-15 and we might reach an agreement. 4 still seems awfully unlikely to me. to many core cognitive problems aren\u2019t really being addressed, and solutions may take a while to roll once we find the basic insights we are lacking.</p>\n<p>But it\u2019s a fair question.</p></blockquote>\n<p>That\u2019s a highly reasonable position one can take. Awfully unlikely (but thus possible) in four years, likely in 8-15, median timeline of 2036 or so.</p>\n<p>Notice that on the timescale of history, 8-15 years until likely AGI, the most important development in the history of history if and when it happens, seems actually kind of imminent and important? That should demand an aggressive policy response focused on what we are going to do when we get to do that, not be treated as a reason to dismiss this?</p>\n<p>Imagine saying, in 2015, \u2018I think AGI is far away, we\u2019re talking 18-25 years\u2019 and anticipating the looks you would get.</p>\n<p>The rest of the essay is a mix of policy suggestions and research direction suggestions. If indeed he is right about research directions, of which I am skeptical, we would still expect to see rapid progress soon as the labs realize this and pivot.</p>\n\n\n<h4 class=\"wp-block-heading\">Can Versus Will Versus Always, Typical Versus Adversarial</h4>\n\n\n<p>A common tactic among LLM doubters, which was one of the strategies used in the NYT editorial, is to show a counterexample, where a model fails a particular query, and say \u2018model can\u2019t do [X]\u2019 or the classic Colin Fraser line of \u2018yep it\u2019s dumb.\u2019</p>\n<p><a href=\"https://x.com/Teknium1/status/1964874276877881395\">Here\u2019s a chef&#8217;s kiss example I saw on Monday morning</a>:</p>\n<div>\n<figure>\n<div>\n\n\n<figure class=\"wp-block-image\"><img alt=\"\" src=\"https://substackcdn.com/image/fetch/$s_!EcUr!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3ebd448a-90bf-4598-b7b2-ec7c77acfbc1_1037x933.png\" /></figure>\n\n\n<div></div>\n</div>\n</figure>\n</div>\n<p>I mean, that\u2019s very funny, but it is rather obvious how it happened with the strawberries thing all over Twitter and thus the training data, and it tells us very little about overall performance.</p>\n<p>In such situations, we have to differentiate between different procedures, the same as in any other scientific experiment. As in:</p>\n<p>Did you try to make it fail, or try to set it up to succeed? Did you choose an adversarial or a typical example? Did you get this the first time you tried it or did you go looking for a failure? Are you saying it \u2018can\u2019t [X]\u2019 because it can\u2019t ever do [X], because it can\u2019t ever do [X] out of the box, it can\u2019t reliably do [X], or it can\u2019t perfectly do [X], etc?</p>\n<p>If you conflate \u2018I can elicit wrong answers on [X] if I try\u2019 with \u2018it can\u2019t do [X]\u2019 then the typical reader will have a very poor picture.</p>\n<blockquote><p>Daniel Litt (responding to NYT article by Gary Marcus that says \u2018[GPT-5] failed some simple math questions, couldn\u2019t count reliably\u2019): While it\u2019s true one can elicit poor performance on basic math question from frontier models like GPT-5, IMO this kind of thing (in NYTimes) is likely to mislead readers about their math capabilities.</p>\n<p>Derya Unutmaz: AI misinformation at the NYT is at its peak. What a piece of crap \u201cnewspaper\u201d it has become. It\u2019s not even worth mentioning the author of this article-but y\u2019all can guess. Meanwhile, just last night I posted a biological method invented by GPT-5 Pro, &amp; I have so much more coming!</p>\n<p>Ethan Mollick: This is disappointing. Purposefully underselling what models can do is a really bad idea. It is possible to point out that AI is flawed without saying it can&#8217;t do math or count &#8211; it just isn&#8217;t true.</p>\n<p>People need to be realistic about capabilities of models to make good decisions.</p>\n<p>I think the urge to criticize companies for hype blends into a desire to deeply undersell what models are capable of. Cherry-picking errors is a good way of showing odd limitations to an overethusiastic Twitter crowd, but not a good way of making people aware that AI is a real factor.</p>\n<p><a href=\"https://x.com/ShakeelHashim/status/1963182536353280012\">Shakeel</a>: The NYT have published a long piece by Gary Marcus on why GPT-5 shows scaling doesn&#8217;t work anymore. At no point does the piece mention that GPT-5 is not a scaled up model.</p>\n<p>[He highlights the line from the post, \u2018As many now see, GPT-5 shows decisively that scaling has lost steam.\u2019]</p>\n<p><a href=\"https://x.com/tracewoodgrains/status/1963460879187386487\">Tracing Woods</a>: Gary Marcus is a great demonstration of the power of finding a niche and sticking to it</p>\n<p>He had the foresight to set himself up as an &#8220;AI is faltering&#8221; guy well in advance of the technology advancing faster than virtually anyone predicted, and now he&#8217;s the go-to</p>\n<p>The thing I find most impressive about Gary Marcus is the way he accurately predicted AI would scale up to an IMO gold performance and then hit a wall (upcoming).</p></blockquote>\n\n\n<h4 class=\"wp-block-heading\">Counterpoint</h4>\n\n\n<p><a href=\"https://x.com/GaryMarcus/status/1963244799378723163\">Gary Marcus was not happy about these responses</a>, and doubled down on \u2018but you implied it would be scaled up, no takesies backsies.\u2019</p>\n<blockquote><p>Gary Marcus (replying to Shakeel directly): this is intellectually dishonest, at BEST it at least as big as 4.5 which was intended as 5 which was significantly larger than 4 it is surely scaled up compared to 4 which is what i compared it to.</p>\n<p>Shakeel: we know categorically that it is not an OOM scale up vs. GPT-4, so &#8230; no. And there&#8217;s a ton of evidence that it&#8217;s smaller than 4.5.</p>\n<p>Gary Marcus (QTing Shakeel): intellectually dishonest reply to my nytimes article.</p>\n<p>openai implied implied repeatedly that GPT-5 was a scaled up model. it is surely scaled up relative to GPT-4.</p>\n<p>it is possible &#8211; openAI has been closed mouth &#8211; that it is same size as 4.5 but 4.5 itself was surely scaled relative to 4, which is what i was comparing with.</p>\n<p>amazing that after years of discussion of scaling the new reply is to claim 5 wasn\u2019t scaled at all.</p>\n<p>Note that if it wasn\u2019t, contra all the PR, that\u2019s even more reason to think that OpenAI knows damn well that is time for leaning on (neuro)symbolic tools and that scaling has reached diminishing returns.</p>\n<p>JB: It can\u2019t really be same in parameter count as gpt4.5 they really struggled serving that and it was much more expensive on the API to use</p>\n<p>Gary Marcus: so a company valued at $300b that\u2019s raised 10 of billions didn\u2019t have the money to scale anymore even though there whole business plan was scaling? what does that tell you?</p></blockquote>\n<p>I am confused how one can claim Shakeel is being intellectually dishonest. His statement is flat out true. Yes, of course the decision not to scale</p>\n<p>It tells me that they want to scale how much they serve the model and how much they do reasoning at inference time, and that this was the most economical solution for them at the time. JB is right that very, very obviously GPT-4.5 is a bigger model than GPT-5 and it is crazy to not realize this.</p>\n\n\n<h4 class=\"wp-block-heading\">\u2018Superforecasters\u2019 Have Reliably Given Unrealistically Slow AI Projections Without Reasonable Justifications</h4>\n\n\n<p>A post like this would be incomplete if I failed to address superforecasters.</p>\n<p>I\u2019ve been over this several times before, where superforecasters reliably have crazy slow projections for progress and even crazier predictions that when we do make minds smarter than ourselves that is almost certainly not an existential risk.</p>\n<p>My coverage of this started <a href=\"https://thezvi.substack.com/i/117877977/what-even-is-a-superforecaster\">way back in AI #14</a> and <a href=\"https://thezvi.substack.com/i/116505755/other-people-are-not-worried-about-ai-killing-everyone\">AI #9</a> <a href=\"https://thezvi.substack.com/i/139306535/would-you-like-some-volcano-apocalypse-insurance\">regarding</a> existential risk estimates, including <a href=\"https://thezvi.substack.com/i/160780617/phillip-tetlock-calibrates-his-skepticism\">Tetlock\u2019s response to AI 2027</a>. One common theme in such timeline projections is predicting Nothing Ever Happens <a href=\"https://thezvi.substack.com/i/166915480/minimum-viable-model\">even when this particular something has already happened.</a></p>\n<p>Now that the dust settled on models getting IMO Gold in 2025, it is a good time to look back on the fact that domain experts expected less progress in math than we got, and superforecasters expected a lot less, across the board.</p>\n<div>\n<figure>\n<div>\n\n\n<figure class=\"wp-block-image\"><img alt=\"\" src=\"https://substackcdn.com/image/fetch/$s_!97j4!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9ec952ce-0a63-4a56-87c1-1a0e20f1ae8e_1200x600.jpeg\" /></figure>\n\n\n<div></div>\n</div>\n</figure>\n</div>\n<blockquote><p>Forecasting Research Institute: Respondents\u2014especially superforecasters\u2014underestimated AI progress.</p>\n<p>Participants predicted the state-of-the-art accuracy of ML models on the MATH, MMLU, and QuaLITY benchmarks by June 2025.</p>\n<p>Domain experts assigned probabilities of 21.4%, 25%, and 43.5% to the achieved outcomes. Superforecasters assigned even lower probabilities: just 9.3%, 7.2%, and 20.1% respectively.</p>\n<p>The International Mathematical Olympiad results were even more surprising. AI systems achieved gold-level performance at the IMO in July 2025. Superforecasters assigned this outcome just a 2.3% probability. Domain experts put it at 8.6%.</p>\n<p><a href=\"https://x.com/GarrisonLovely/status/1962901888594018763\">Garrison Lovely</a>: <a href=\"https://www.lesswrong.com/posts/sWLLdG6DWJEy3CH7n/imo-challenge-bet-with-eliezer\">This makes Yudkowsky and Paul Christiano&#8217;s predictions of IMO gold by 2025</a> look even more prescient (they also predicted it a ~year before this survey was conducted).</p></blockquote>\n<p>Note that even Yudkowsky and Christiano had only modest probability that the IMO would fall as early as 2025.</p>\n<blockquote><p><a href=\"https://x.com/AndrewCritchPhD/status/1963083316250456210\">Andrew Critch</a>: Yeah sorry forecasting fam, ya gotta learn some AI if you wanna forecast anything, because AI affects everything and if ya don&#8217;t understand it ya forecast it wrong.</p></blockquote>\n<p>Or, as I put it back in the unrelated-to-AI post <a href=\"https://thezvi.substack.com/p/rock-is-strong?utm_source=chatgpt.com\">Rock is Strong</a>:</p>\n<blockquote><p><a href=\"https://www.youtube.com/watch?v=Jp8znvfYbow&amp;ab_channel=TheyMightBeGiants-Topic\">Everybody wants a rock</a>. It\u2019s easy to see why. If all you want is an almost always right answer, there are places where <a href=\"https://astralcodexten.substack.com/p/heuristics-that-almost-always-work\">they almost always work</a>.</p>\n<p>\u2026</p>\n<p>The security guard has an easy to interpret rock because all it has to do is say \u201cNO ROBBERY.\u201d The doctor\u2019s rock is easy too, \u201cYOU\u2019RE FINE, GO HOME.\u201d This one is different, and doesn\u2019t win the competitions even if we agree it\u2019s cheating on tail risks. It\u2019s not a coherent world model.</p>\n<p>Still, on the desk of the best superforecaster is a rock that says \u201cNOTHING EVER CHANGES OR IS INTERESTING\u201d as a reminder not to get overexcited, and to not assign <em>super high </em>probabilities to weird things that seem right to them.</p></blockquote>\n<p>Thus:</p>\n<blockquote><p><a href=\"https://x.com/daniel_271828/status/1963022828292436268\">Daniel Eth</a>: In 2022, superforecasters gave only a 2.3% chance of an AI system achieving an IMO gold by 2025. Yet this wound up happening. AI progress keeps being underestimated by superforecasters.</p>\n<p>I feel like superforecasters are underperforming in AI (in this case even compared to domain experts) because two reference classes are clashing:</p>\n<p>\u2022 steady ~exponential increase in AI</p>\n<p>\u2022 nothing ever happens.</p>\n<p>And for some reason, superforecasters are reaching for the second.</p></blockquote>\n<p>Hindsight is hindsight, and yes you will get a 98th percentile result 2% of the time. But I think at 2.3% for 2025 IMO Gold, you are not serious people.</p>\n<p>That doesn\u2019t mean that being serious people was the wise play here. The incentives might well have been to follow the \u2018nothing ever happens\u2019 rock. We still have to realize this, as we can indeed smell what the rock is cooking.</p>\n\n\n<h4 class=\"wp-block-heading\">What To Expect When You\u2019re Expecting AI Progress</h4>\n\n\n<p>A wide range of potential paths of AI progress are possible. There are a lot of data points that should impact the distribution of outcomes, and one must not overreact to any one development. One should especially not overreact to not being blown away by progress for a span of a few months. Consider your baseline that\u2019s causing that.</p>\n<p>My timelines for hitting various milestones, including various definitions of AGI, involve a lot of uncertainty. I think not having a lot of uncertainty is a mistake.</p>\n<p>I especially think saying either \u2018AGI almost certainly won\u2019t happen within 5 years\u2019 or \u2018AGI almost certainly will happen within 15 years,\u2019 would be a large mistake. There are so many different unknowns involved.</p>\n<p>I can see treating full AGI in 2026 as effectively a Can\u2019t Happen. I don\u2019t think you can extend that even to 2027, although I would lay large odds against it hitting that early.</p>\n<p>A wide range of medians seem reasonable to me. I can see defending a median as early as 2028, or one that extends to 2040 or beyond if you think it is likely that anything remotely like current approaches cannot get there. I have not put a lot of effort into picking my own number since the exact value currently lacks high value of information. If you put a gun to my head for a typical AGI definition I\u2019d pick 2031, but with no \u2018right to be surprised\u2019 if it showed up in 2028 or didn\u2019t show up for a while. Consider the 2031 number loosely held.</p>\n<p>To close out, consider once again: Even if you we agreed with Gary Marcus and said 8-15 years, with median 2036? Take a step back and realize how soon and crazy that is.</p>\n<p>&nbsp;</p>\n\n\n<h4 class=\"wp-block-heading\"></h4>\n\n\n<p>&nbsp;</p>\n<p>&nbsp;</p>"
            ],
            "link": "https://thezvi.wordpress.com/2025/09/09/yes-ai-continues-to-make-rapid-progress-including-towards-agi/",
            "publishedAt": "2025-09-09",
            "source": "TheZvi",
            "summary": "That does not mean AI will successfully make it all the way to AGI and superintelligence, or that it will make it there soon or on any given time frame. It does mean that AI progress, while it could easily &#8230; <a href=\"https://thezvi.wordpress.com/2025/09/09/yes-ai-continues-to-make-rapid-progress-including-towards-agi/\">Continue reading <span class=\"meta-nav\">&#8594;</span></a>",
            "title": "Yes, AI Continues To Make Rapid Progress, Including Towards AGI"
        },
        {
            "content": [
                "<div class=\"flex space-x-2 bg-bg-soft dark:bg-bgDark-soft mx-auto min-h-fit\n        lg:w-[80ch] sm:w-[65ch] w-full\n        lg:p-4 p-2\n        // Base styles for all messages\n        mt-0 mb-0 rounded-none\n        // First message styles\n        first:mt-4 first:rounded-t-lg first:pb-2\n        // Last message styles\n        last:mb-4 last:rounded-b-lg last:pt-1\n        // Middle message top/bottom adjustment\n        [&amp;:not(:first-child)]:-mt-[1px] [&amp;:not(:first-child)]:py-1\"><div class=\"h-16 not-prose\"><img alt=\"Cadey is coffee\" class=\"h-16 w-16 rounded-xs\" src=\"https://stickers.xeiaso.net/sticker/cadey/coffee\" /></div><div class=\"flex-1 min-w-0\"><span class=\"font-semibold text-sm block mb-1\"><a href=\"https://xeiaso.net/characters#cadey\">Cadey</a></span><span class=\"mx-auto\"></span><div class=\"text-fg-1 dark:text-fgDark-1 text-sm prose-p:my-2\"><p>This post and its online comment sections are blame-free zones. We are not\n        blaming anyone for clicking on the phishing link. If you were targeted with\n        such a phishing attack, you'd fall for it too and it's a matter of when not\n        if. Anyone who claims they wouldn't is wrong.</p><br /><p>This is also a bit of a rant.</p></div></div></div>\n        <p>Yesterday <a href=\"https://www.aikido.dev/blog/npm-debug-and-chalk-packages-compromised\">one of the biggest package ecosystems had very popular packages get compromised</a>. We're talking functionality like:</p>\n        <ul>\n        <li>Formatting text with colors for use in the terminal</li>\n        <li>A list of common color names and their RGB values</li>\n        <li>A decorator for functions so you can debug their inputs/outputs as they are run</li>\n        <li>A utility function that determines if its argument can be used like an array</li>\n        </ul>\n        <p>These kinds of dependencies are everywhere and nobody would even think that they could be harmful. Getting code into these packages means that it's almost guaranteed a free path to production deployments. If an open proxy server (a-la Bright Data or other botnets that the credit card network tolerates for some reason), API key stealer, or worse was sent through this chain of extreme luck on the attacker's part, then this would be a completely different story.</p>\n        <p>We all dodged a massive bullet because all the malware did was modify the destination addresses of cryptocurrency payments mediated via online wallets like <a href=\"https://metamask.io/\">MetaMask</a>.</p>\n        <p>As someone adjacent to the online security community, I have a sick sense of appreciation for this attack. This was a really good attack. It started with a phishing email that I'd probably fall for if it struck at the right time:</p>\n        <figure class=\"max-w-3xl mx-auto not-prose w-full undefined\"><a href=\"https://files.xeiaso.net/blog/2025/we-dodged-a-bullet/the-email.jpg\"><source type=\"image/avif\" /><source type=\"image/webp\" /><img src=\"https://files.xeiaso.net/blog/2025/we-dodged-a-bullet/the-email.jpg\" /></a></figure>\n        <p>This is frankly a really good phishing email. Breaking it down:</p>\n        <ul>\n        <li>It greets the user personally with their NPM username. This makes it look personalized, so people are more likely to trust it.</li>\n        <li>People are used to the idea of changing passwords for security. With that in mind, at a glance the idea of changing your two-factor auth credentials &quot;for security reasons&quot; isn't completely unreasonable.</li>\n        <li>NPM has always been kinda weird compared to other open source package repositories, so them requiring something strange like that reads as reasonable.</li>\n        <li>It sets a deadline a few days in the future. This creates a sense of urgency, and when you combine urgency with being rushed by life, you are much more likely to fall for the phishing link.</li>\n        <li>It links to a website (I'm assuming it's on npm.help), and that website is used to get the two-factor credentials somehow and then start publishing new packages with the exploit code.</li>\n        </ul>\n        <p>This is a 10/10 phishing email. Looking at it critically the only part about it that stands out is the domain &quot;npmjs.help&quot; instead of &quot;npmjs.com&quot;. Even then, that wouldn't really stand out to me because I've seen companies use new <a href=\"https://en.wikipedia.org/wiki/Generic_top-level_domain\">generic top level domains</a> to separate out things like the blog at <code>.blog</code> or the docs at <code>.guide</code>, not to mention the <a href=\"https://www.linkedin.com/posts/tokih_heres-your-builders-guide-to-new-activity-7315851665348141057-9oBM\"><code>.new</code> stack</a>.</p>\n        <p>One of my friends <a href=\"https://bsky.app/profile/buttplug.engineer\">qdot</a> also got the phishing email and here's what he had to say:</p>\n        <center><blockquote class=\"bluesky-embed\"><p lang=\"en\"><p>I got the email for it and was like &quot;oh I'll deal with this\n        later&quot;.</p><br /><br /><p>Saved by procrastination!</p></p><p>\u2014 qdot (<a href=\"https://bsky.app/profile/did:plc:sfvpv6dfrug3rnjewn7gyx62?ref_src=embed\">\n        @buttplug.engineer\n        </a>) <a href=\"https://bsky.app/profile/did:plc:sfvpv6dfrug3rnjewn7gyx62/post/3lyds45qxpc2i?ref_src=embed\">September 8, 2025 at 2:04 PM</a></p></blockquote></center>\n        <p>With <a href=\"https://www.npmjs.com/package/is-arrayish\">how</a> <a href=\"https://www.npmjs.com/package/color-string\">widely</a> <a href=\"https://www.npmjs.com/package/color-name\">used</a> these libraries are, this could have been <em>so much worse</em> than it was. I can easily imagine a timeline where this wasn't just a cryptocurrency interceptor. Imagine if something this widely deployed into an ecosystem where automated package bumping triggering production releases is common did API key theft. You'd probably have more OpenAI API keys than you know what you'd do with. You could probably go for years without having to pay for AWS again.</p>\n        <p>It is just maddening to me that a near Jia Tan level chain of malware and phishing was wasted on cryptocurrency interception that won't even run in the majority of places those compromised libraries were actually used. When I was bumping packages around these issues, I found that most of these libraries were used in command line tools.</p>\n        <p>This was an attack obviously targeted towards the Web 3 ecosystem as users of Web 3 tools are used to making payments with their browsers. With my black hat on, I think that the reason they targeted more generic packages instead of Web 3 packages was so that the compromise wouldn't be as noticed by the Web 3 ecosystem. Sure, you'd validate the rigging that helps you interface with Metamask, but you'd never think that it would get monkey-patched by your color value parsing library.</p>\n        <p>One of the important things to take away from this is that every dependency could be malicious. We should take the time to understand the entire dependency tree of our programs, but we aren't given that time. At the end of the day, we still have to ship things.</p>"
            ],
            "link": "https://xeiaso.net/notes/2025/we-dodged-a-bullet/",
            "publishedAt": "2025-09-09",
            "source": "Xe Iaso",
            "summary": "<div class=\"flex space-x-2 bg-bg-soft dark:bg-bgDark-soft mx-auto min-h-fit lg:w-[80ch] sm:w-[65ch] w-full lg:p-4 p-2 // Base styles for all messages mt-0 mb-0 rounded-none // First message styles first:mt-4 first:rounded-t-lg first:pb-2 // Last message styles last:mb-4 last:rounded-b-lg last:pt-1 // Middle message top/bottom adjustment [&amp;:not(:first-child)]:-mt-[1px] [&amp;:not(:first-child)]:py-1\"><div class=\"h-16 not-prose\"><img alt=\"Cadey is coffee\" class=\"h-16 w-16 rounded-xs\" src=\"https://stickers.xeiaso.net/sticker/cadey/coffee\" /></div><div class=\"flex-1 min-w-0\"><span class=\"font-semibold text-sm block mb-1\"><a href=\"https://xeiaso.net/characters#cadey\">Cadey</a></span><span class=\"mx-auto\"></span><div class=\"text-fg-1 dark:text-fgDark-1 text-sm prose-p:my-2\"><p>This post and its online comment sections are blame-free zones. We are not blaming anyone for clicking on the phishing link. If you were targeted with such a phishing attack, you'd fall for it too and it's a matter of when not if. Anyone who claims they wouldn't is wrong.</p><br /><p>This is also a bit of a rant.</p></div></div></div> <p>Yesterday <a href=\"https://www.aikido.dev/blog/npm-debug-and-chalk-packages-compromised\">one of the biggest package ecosystems had very popular packages get compromised</a>. We're talking functionality like:</p> <ul> <li>Formatting text with colors for use in the terminal</li> <li>A list of common color names and their RGB values</li> <li>A decorator for functions so you can debug their inputs/outputs as they are run</li> <li>A utility function that determines if its argument can be used like an array</li> </ul> <p>These kinds of dependencies are everywhere and nobody would even think that",
            "title": "We all dodged a bullet"
        }
    ],
    "lookbackDays": 1,
    "publishDate": "2025-09-09"
}