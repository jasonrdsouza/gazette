{
    "articles": [
        {
            "content": [
                "<header>\n  <h1>Error Codes for Control Flow</h1>\n  <time class=\"meta\" datetime=\"2025-11-06\">Nov 6, 2025</time>\n</header>\n<p>Two ideas today:</p>\n<ul>\n<li>\nDisplaying an error message to the user is a different aspect of error handling than branching\nbased on a specific error condition.\n</li>\n<li>\nIn Zig, error sets are fancy error codes, not poor man\u2019s sum types.\n</li>\n</ul>\n<p>In other words, it\u2019s worth thinking about diagnostic reporting and error handling (in the literal\nsense) separately. There are generally two destinations for any error. An error can be bubbled to an\nisolation boundary and presented to the operator (for example, as an HTTP 500 message, or stderr\noutput). Alternatively, an error can be handled by taking an appropriate recovery action.</p>\n<p>For the first case (reporting), often it is sufficient that an error is an interface that knows how\nto present itself. The catch is that the presentation interface isn\u2019t fixed: HTML output is different\nfrom terminal output. If you know the ultimate destination up front, it usually is simpler to render\nthe error immediately. Otherwise, an error can be made a structured product type to allow (require)\nthe user full control over the presentation (localization of error messages is a good intuition\npump).</p>\n<p>If you need to <em>branch</em> on error to handle it, you generally need a sum type. Curiously though,\nthere\u2019s going to be a finite number of branches up the stack across all call-sites, so, just like a\nlean reporting type might contain only the final presentation, a lean handling type might be just an\nenumeration of all different code-paths \u2014 an error code.</p>\n<p>As usual, Zig\u2019s design is (thought) provocative. The language handles the \u201chandling\u201d part, leaving\nalmost the entirety of reporting to the user. Zig uses type system to fix problems with error codes,\nmostly keeping runtime semantics as is.</p>\n<p>In C, error codes are in-band, and it\u2019s easy to confuse a valid\nresult with an error code (e.g. doing <code>kill(-1)</code> by accident). Zig uses type-checked error unions:\n<span class=\"display\"><code>ReadError!usize</code></span>\nwhich require explicit unpacking with <code>catch</code>. Error codes are easy to ignore by mistake, but,\nbecause the compiler knows which values are errors, Zig requires a special form for ignoring an\nerror: <span class=\"display\"><code>catch {}</code></span></p>\n<p>As a nice touch, while Zig requires explicit discards for all unused values, discarding non-error\nvalue requires a different syntax:</p>\n\n<figure class=\"code-block\">\n\n\n<pre><code><span class=\"line\"><span class=\"hl-keyword\">pub</span> <span class=\"hl-keyword\">fn</span><span class=\"hl-function\"> main</span>() <span class=\"hl-type\">void</span> {</span>\n<span class=\"line\">    _ = can_fail();</span>\n<span class=\"line\">    <span class=\"hl-comment\">// ^ error: error union is discarded</span></span>\n<span class=\"line\"></span>\n<span class=\"line\">    can_fail() <span class=\"hl-keyword\">catch</span> {};</span>\n<span class=\"line\">    <span class=\"hl-comment\">// ^ error: incompatible types: &#x27;u32&#x27; and &#x27;void&#x27;</span></span>\n<span class=\"line\"></span>\n<span class=\"line\">    _ = can_fail() <span class=\"hl-keyword\">catch</span> {};</span>\n<span class=\"line\">    <span class=\"hl-comment\">// Works.</span></span>\n<span class=\"line\">}</span>\n<span class=\"line\"></span>\n<span class=\"line\"><span class=\"hl-keyword\">fn</span><span class=\"hl-function\"> can_fail</span>() <span class=\"hl-operator\">!</span><span class=\"hl-type\">u32</span> {</span>\n<span class=\"line\">    <span class=\"hl-keyword\">return</span> <span class=\"hl-keyword\">error</span>.Nope;</span>\n<span class=\"line\">}</span></code></pre>\n\n</figure>\n<p>This protects from a common error when initially a result of an infallible function is ignored, but\nthen the function grows a failing path, and the error gets silently ignored. That\u2019s\nthe <a href=\"https://docs.rs/powerletters/latest/powerletters/fn.I.html\">I power letter</a>!</p>\n<p>As an aside, <a href=\"https://internals.rust-lang.org/t/use-of-must-use-in-the-standard-library/15425/14\">I used to be\nunsure</a> whether\nits best to annotate specific APIs with <code>#[must_use]</code> or do the opposite and, Swift-style, require\nall return values to be used. My worry was that adding a lot of trivial discards will drown\nload-bearing discards in the noise. After using Zig, I can confidently say that trivial discards\nhappen rarely and are a non-issue (but it certainly helps to separate value- and error-discards\nsyntactically). This doesn\u2019t mean that retrofitting mandatory value usage into existing languages is\na good idea! This drastic of a change usually retroactively invalidates a lot of previously\nreasonable API design choices.</p>\n<p>Zig further leverages the type system to track <em>which</em> errors can be returned by the API:</p>\n\n<figure class=\"code-block\">\n\n\n<pre><code><span class=\"line\"><span class=\"hl-keyword\">pub</span> <span class=\"hl-keyword\">fn</span><span class=\"hl-function\"> readSliceAll</span>(</span>\n<span class=\"line\">    r: <span class=\"hl-operator\">*</span>Reader,</span>\n<span class=\"line\">    buffer: []<span class=\"hl-type\">u8</span>,</span>\n<span class=\"line\">) <span class=\"hl-keyword\">error</span>{ReadFailed, EndOfStream}<span class=\"hl-operator\">!</span><span class=\"hl-type\">void</span> {</span>\n<span class=\"line\">    <span class=\"hl-keyword\">const</span> n = <span class=\"hl-keyword\">try</span> readSliceShort(r, buffer);</span>\n<span class=\"line\">    <span class=\"hl-keyword\">if</span> (n <span class=\"hl-operator\">!=</span> buffer.len) <span class=\"hl-keyword\">return</span> <span class=\"hl-keyword\">error</span>.EndOfStream;</span>\n<span class=\"line\">}</span>\n<span class=\"line\"></span>\n<span class=\"line\"><span class=\"hl-keyword\">pub</span> <span class=\"hl-keyword\">fn</span><span class=\"hl-function\"> readSliceShort</span>(</span>\n<span class=\"line\">    r: <span class=\"hl-operator\">*</span>Reader,</span>\n<span class=\"line\">    buffer: []<span class=\"hl-type\">u8</span>,</span>\n<span class=\"line\">) <span class=\"hl-keyword\">error</span>{ReadFailed}<span class=\"hl-operator\">!</span><span class=\"hl-type\">usize</span> {</span>\n<span class=\"line\">    <span class=\"hl-comment\">// ...</span></span>\n<span class=\"line\">}</span></code></pre>\n\n</figure>\n<p>The tracking works additively (calling two functions unions the error sets) and subtractively (a\nfunction can handle a subset of errors and propagate the rest). Zig also leverages its whole-program\ncompilation model to allow fully inferring the error sets. The closed world model is also what\nallows assigning unambiguous numeric code to symbolic error constants, which in turn allows a\ncatchall <code>anyerror</code> type.</p>\n<p>But the symbolic name is all you get out of the error value. The language doesn\u2019t ship anything\nfirst-class for reporting, and diagnostic information is communicated out of band using diagnostic\nsink pattern:</p>\n\n<figure class=\"code-block\">\n\n\n<pre><code><span class=\"line\"><span class=\"hl-comment\">/// Parses the given slice as ZON.</span></span>\n<span class=\"line\"><span class=\"hl-keyword\">pub</span> <span class=\"hl-keyword\">fn</span><span class=\"hl-function\"> fromSlice</span>(</span>\n<span class=\"line\">    T: <span class=\"hl-type\">type</span>,</span>\n<span class=\"line\">    gpa: Allocator,</span>\n<span class=\"line\">    source: [:<span class=\"hl-numbers\">0</span>]<span class=\"hl-keyword\">const</span> <span class=\"hl-type\">u8</span>,</span>\n<span class=\"line\">    diag: ?<span class=\"hl-operator\">*</span>Diagnostics,</span>\n<span class=\"line\">) <span class=\"hl-keyword\">error</span>{ OutOfMemory, ParseZon }<span class=\"hl-operator\">!</span>T {</span>\n<span class=\"line\">    <span class=\"hl-comment\">// ...</span></span>\n<span class=\"line\">}</span></code></pre>\n\n</figure>\n<p>If the caller wants to handle the error, they pass <code>null</code> sink and <code>switch</code> on the error value. If\nthe caller wants to present the error to the user, they pass in <code>Diagnostics</code> and extract formatted\noutput from that.</p>"
            ],
            "link": "https://matklad.github.io/2025/11/06/error-codes-for-control-flow.html",
            "publishedAt": "2025-11-06",
            "source": "Alex Kladov",
            "summary": "<header> <h1>Error Codes for Control Flow</h1> <time class=\"meta\" datetime=\"2025-11-06\">Nov 6, 2025</time> </header> <p>Two ideas today:</p> <ul> <li> Displaying an error message to the user is a different aspect of error handling than branching based on a specific error condition. </li> <li> In Zig, error sets are fancy error codes, not poor man\u2019s sum types. </li> </ul> <p>In other words, it\u2019s worth thinking about diagnostic reporting and error handling (in the literal sense) separately. There are generally two destinations for any error. An error can be bubbled to an isolation boundary and presented to the operator (for example, as an HTTP 500 message, or stderr output). Alternatively, an error can be handled by taking an appropriate recovery action.</p> <p>For the first case (reporting), often it is sufficient that an error is an interface that knows how to present itself. The catch is that the presentation interface isn\u2019t fixed: HTML output is different from terminal output. If you know the ultimate destination up front, it usually is simpler to render the error immediately. Otherwise, an error can be made a structured product type to allow (require) the user full control over the presentation (localization of error messages is a good intuition pump).</p>",
            "title": "Error Codes for Control Flow"
        },
        {
            "content": [
                "<p>Hi friends,</p>\n<p>In October, Scour ingested <strong>1,042,894 new posts from 14,140 sources</strong>. I was also training for the NYC Marathon (which is why this email comes a few days into November)!</p>\n<p>Last month was all about Interests:</p>\n<h2 id=\"interest-recommendations-everywhere\">\ud83d\udce8 Interest Recommendations Everywhere</h2><p>Your weekly email digest now includes a couple of topic recommendations at the end. And, if you use an RSS reader to consume your Scour feed, you\u2019ll also find interest recommendations in that feed as well.</p>\n<h2 id=\"interest-autocomplete\">\ud83d\udd0e Interest Autocomplete</h2><p>When you add a new interest on the Interests page, you\u2019ll now see a menu of similar topics that you can click to quickly add.</p>\n<h2 id=\"popular-interests-page\">\ud83c\udfc6 Popular Interests Page</h2><p>You can browse the new <a href=\"https://scour.ing/browse/interests/popular\">Popular Interests page</a> to find other topics you might want to add.</p>\n<h2 id=\"infinite-scrolling-is-optional\">\u267e\ufe0f Infinite Scrolling is Optional</h2><p>Infinite scrolling is now optional. You can disable it and switch back to explicit pages on your Settings page. Thanks <a href=\"https://feedback.scour.ing/91\">Tom\u00e1\u0161 Burkert</a> for this suggestion!</p>\n<h2 id=\"goldilocks-problem-with-topic-recommendations\">\ud83d\udc3b Goldilocks Problem with Topic Recommendations</h2><p>Earlier, Scour\u2019s topic recommendations were a little too broad. I tried to fix that and now, as you might have noticed, they\u2019re often too specific. I\u2019m still working on solving this \u201cGoldilocks problem\u201d, so more on this to come!</p>\n<h2 id=\"some-of-my-favorite-posts\">\ud83d\udd16 Some of My Favorite Posts</h2><p>Finally, here were a couple of <a href=\"https://scour.ing/@emschwartz/likes\">my favorite posts</a> that I found on Scour in October:</p>\n<ul>\n<li><a href=\"https://scour.ing/@emschwartz/p/https://huggingface.co/blog/rteb\">Introducing RTEB: A New Standard for Retrieval Evaluation</a></li>\n<li><a href=\"https://scour.ing/@emschwartz/p/https://www.krupadave.com/articles/everything-about-transformers?x=v3\">Everything About Transformers</a></li>\n<li><a href=\"https://scour.ing/@emschwartz/p/https://allvpv.org/turn-off-cursor\">Turn off Cursor, turn on your mind</a></li>\n</ul>\n<p>Happy Scouring!\n- Evan</p>"
            ],
            "link": "https://emschwartz.me/scour-october-update/",
            "publishedAt": "2025-11-06",
            "source": "Evan Schwartz",
            "summary": "<p>Hi friends,</p> <p>In October, Scour ingested <strong>1,042,894 new posts from 14,140 sources</strong>. I was also training for the NYC Marathon (which is why this email comes a few days into November)!</p> <p>Last month was all about Interests:</p> <h2 id=\"interest-recommendations-everywhere\">\ud83d\udce8 Interest Recommendations Everywhere</h2><p>Your weekly email digest now includes a couple of topic recommendations at the end. And, if you use an RSS reader to consume your Scour feed, you\u2019ll also find interest recommendations in that feed as well.</p> <h2 id=\"interest-autocomplete\">\ud83d\udd0e Interest Autocomplete</h2><p>When you add a new interest on the Interests page, you\u2019ll now see a menu of similar topics that you can click to quickly add.</p> <h2 id=\"popular-interests-page\">\ud83c\udfc6 Popular Interests Page</h2><p>You can browse the new <a href=\"https://scour.ing/browse/interests/popular\">Popular Interests page</a> to find other topics you might want to add.</p> <h2 id=\"infinite-scrolling-is-optional\">\u267e\ufe0f Infinite Scrolling is Optional</h2><p>Infinite scrolling is now optional. You can disable it and switch back to explicit pages on your Settings page. Thanks <a href=\"https://feedback.scour.ing/91\">Tom\u00e1\u0161 Burkert</a> for this suggestion!</p> <h2 id=\"goldilocks-problem-with-topic-recommendations\">\ud83d\udc3b Goldilocks Problem with Topic Recommendations</h2><p>Earlier, Scour\u2019s topic recommendations were a little too broad. I tried to fix that and now, as you might have noticed, they\u2019re often too specific. I\u2019m still working on solving this \u201cGoldilocks problem\u201d, so more on",
            "title": "Scour - October Update"
        },
        {
            "content": [
                "<div class=\"lead\"><p>Some concepts are easy to grasp in the abstract. Boiling water: apply heat and wait. Others you really need to try. You only think you understand how a bicycle works, until you learn to ride one.</p>\n</div>\n<p>There are big ideas in computing that are easy to get your head around. The AWS S3 API. It&rsquo;s the most important storage technology of the last 20 years, and it&rsquo;s like boiling water. Other technologies, you need to get your feet on the pedals first.</p>\n\n<p>LLM agents are like that.</p>\n\n<p>People have <a href=\"https://ludic.mataroa.blog/blog/contra-ptaceks-terrible-article-on-ai/\" title=\"\">wildly varying opinions</a> about LLMs and agents. But whether or not they&rsquo;re snake oil, they&rsquo;re a big idea. You don&rsquo;t have to like them, but you should want to be right about them. To be the best hater (or stan) you can be.</p>\n\n<p>So that&rsquo;s one reason you should write an agent. But there&rsquo;s another reason that&rsquo;s even more persuasive, and that&rsquo;s</p>\n<h2 class=\"group flex items-start whitespace-pre-wrap relative mt-14 sm:mt-16 mb-4 text-navy-950 font-heading\" id=\"its-incredibly-easy\"><a class=\"inline-block align-text-top relative top-[.15em] w-6 h-6 -ml-6 after:hash opacity-0 group-hover:opacity-100 transition-all\" href=\"https://fly.io/blog/feed.xml#its-incredibly-easy\"></a><span class=\"plain-code\">It&rsquo;s Incredibly Easy</span></h2>\n<p>Agents are the most surprising programming experience I&rsquo;ve had in my career. Not because I&rsquo;m awed by the magnitude of their powers \u2014 I like them, but I don&rsquo;t like-like them. It&rsquo;s because of how easy it was to get one up on its legs, and how much I learned doing that.</p>\n\n<p>I&rsquo;m about to rob you of a dopaminergic experience, because agents are so simple we might as well just jump into the code. I&rsquo;m not even going to bother explaining what an agent is.</p>\n<div class=\"highlight-wrapper group relative python\">\n  <button class=\"bubble-wrap z-20 absolute right-9 -mr-0.5 top-1.5 text-transparent group-hover:text-gray-400 group-hover:hocus:text-white focus:text-white bg-transparent group-hover:bg-gray-900 group-hover:hocus:bg-gray-700 focus:bg-gray-700 transition-colors grid place-items-center w-7 h-7 rounded-lg outline-none focus:outline-none\" type=\"button\">\n    <svg class=\"w-4 h-4 pointer-events-none\" fill=\"none\" stroke=\"currentColor\" stroke-linecap=\"round\" stroke-linejoin=\"round\" stroke-width=\"1.35\" viewBox=\"0 0 16 16\" xmlns=\"http://www.w3.org/2000/svg\"><g><path d=\"M9.912 8.037h2.732c1.277 0 2.315-.962 2.315-2.237a2.325 2.325 0 00-2.315-2.31H2.959m10.228 9.01H2.959M6.802 8H2.959\"><path d=\"M11.081 6.466L9.533 8.037l1.548 1.571\"></g></svg>\n    <span class=\"bubble-sm bubble-tl [--offset-l:-9px] tail text-navy-950\">\n      Wrap text\n    </span>\n  </button>\n  <button class=\"bubble-wrap z-20 absolute right-1.5 top-1.5 text-transparent group-hover:text-gray-400 group-hover:hocus:text-white focus:text-white bg-transparent group-hover:bg-gray-900 group-hover:hocus:bg-gray-700 focus:bg-gray-700 transition-colors grid place-items-center w-7 h-7 rounded-lg outline-none focus:outline-none\" type=\"button\">\n    <svg class=\"w-4 h-4 pointer-events-none\" fill=\"none\" stroke=\"currentColor\" stroke-width=\"1.35\" viewBox=\"0 0 16 16\" xmlns=\"http://www.w3.org/2000/svg\"><g><path d=\"M10.576 7.239c0-.995-.82-1.815-1.815-1.815H3.315c-.995 0-1.815.82-1.815 1.815v5.446c0 .995.82 1.815 1.815 1.815h5.446c.995 0 1.815-.82 1.815-1.815V7.239z\"><path d=\"M10.576 10.577h2.109A1.825 1.825 0 0014.5 8.761V3.315A1.826 1.826 0 0012.685 1.5H7.239c-.996 0-1.815.819-1.816 1.815v1.617\"></g></svg>\n    <span class=\"bubble-sm bubble-tl [--offset-l:-6px] tail [--tail-x:calc(100%-30px)] text-navy-950\">\n      Copy to clipboard\n    </span>\n  </button>\n  <div class=\"highlight relative group\">\n    <pre class=\"highlight \"><code id=\"code-i6al5yyk\"><span class=\"kn\">from</span> <span class=\"nn\">openai</span> <span class=\"kn\">import</span> <span class=\"n\">OpenAI</span>\n\n<span class=\"n\">client</span> <span class=\"o\">=</span> <span class=\"n\">OpenAI</span><span class=\"p\">()</span>\n<span class=\"n\">context</span> <span class=\"o\">=</span> <span class=\"p\">[]</span>\n\n<span class=\"k\">def</span> <span class=\"nf\">call</span><span class=\"p\">():</span>\n    <span class=\"k\">return</span> <span class=\"n\">client</span><span class=\"p\">.</span><span class=\"n\">responses</span><span class=\"p\">.</span><span class=\"n\">create</span><span class=\"p\">(</span><span class=\"n\">model</span><span class=\"o\">=</span><span class=\"s\">\"gpt-5\"</span><span class=\"p\">,</span> <span class=\"nb\">input</span><span class=\"o\">=</span><span class=\"n\">context</span><span class=\"p\">)</span>\n\n<span class=\"k\">def</span> <span class=\"nf\">process</span><span class=\"p\">(</span><span class=\"n\">line</span><span class=\"p\">):</span>\n    <span class=\"n\">context</span><span class=\"p\">.</span><span class=\"n\">append</span><span class=\"p\">({</span><span class=\"s\">\"role\"</span><span class=\"p\">:</span> <span class=\"s\">\"user\"</span><span class=\"p\">,</span> <span class=\"s\">\"content\"</span><span class=\"p\">:</span> <span class=\"n\">line</span><span class=\"p\">})</span>\n    <span class=\"n\">response</span> <span class=\"o\">=</span> <span class=\"n\">call</span><span class=\"p\">()</span>    \n    <span class=\"n\">context</span><span class=\"p\">.</span><span class=\"n\">append</span><span class=\"p\">({</span><span class=\"s\">\"role\"</span><span class=\"p\">:</span> <span class=\"s\">\"assistant\"</span><span class=\"p\">,</span> <span class=\"s\">\"content\"</span><span class=\"p\">:</span> <span class=\"n\">response</span><span class=\"p\">.</span><span class=\"n\">output_text</span><span class=\"p\">})</span>        \n    <span class=\"k\">return</span> <span class=\"n\">response</span><span class=\"p\">.</span><span class=\"n\">output_text</span>\n</code></pre>\n  </div>\n</div><div class=\"right-sidenote\"><p>It\u2019s an HTTP API with, like, one important endpoint.</p>\n</div>\n<p>This is a trivial engine for an LLM app using the <a href=\"https://platform.openai.com/docs/api-reference/responses\" title=\"\">OpenAI Responses API</a>. It implements ChatGPT. You&rsquo;d drive it with the <button>the  obvious  loop</button>. It&rsquo;ll do what you&rsquo;d expect: the same thing ChatGPT would, but in your terminal.</p>\n<div id=\"readline\"><div class=\"highlight-wrapper group relative python\">\n  <button class=\"bubble-wrap z-20 absolute right-9 -mr-0.5 top-1.5 text-transparent group-hover:text-gray-400 group-hover:hocus:text-white focus:text-white bg-transparent group-hover:bg-gray-900 group-hover:hocus:bg-gray-700 focus:bg-gray-700 transition-colors grid place-items-center w-7 h-7 rounded-lg outline-none focus:outline-none\" type=\"button\">\n    <svg class=\"w-4 h-4 pointer-events-none\" fill=\"none\" stroke=\"currentColor\" stroke-linecap=\"round\" stroke-linejoin=\"round\" stroke-width=\"1.35\" viewBox=\"0 0 16 16\" xmlns=\"http://www.w3.org/2000/svg\"><g><path d=\"M9.912 8.037h2.732c1.277 0 2.315-.962 2.315-2.237a2.325 2.325 0 00-2.315-2.31H2.959m10.228 9.01H2.959M6.802 8H2.959\"></path><path d=\"M11.081 6.466L9.533 8.037l1.548 1.571\"></path></g></svg>\n    <span class=\"bubble-sm bubble-tl [--offset-l:-9px] tail text-navy-950\">\n      Wrap text\n    </span>\n  </button>\n  <button class=\"bubble-wrap z-20 absolute right-1.5 top-1.5 text-transparent group-hover:text-gray-400 group-hover:hocus:text-white focus:text-white bg-transparent group-hover:bg-gray-900 group-hover:hocus:bg-gray-700 focus:bg-gray-700 transition-colors grid place-items-center w-7 h-7 rounded-lg outline-none focus:outline-none\" type=\"button\">\n    <svg class=\"w-4 h-4 pointer-events-none\" fill=\"none\" stroke=\"currentColor\" stroke-width=\"1.35\" viewBox=\"0 0 16 16\" xmlns=\"http://www.w3.org/2000/svg\"><g><path d=\"M10.576 7.239c0-.995-.82-1.815-1.815-1.815H3.315c-.995 0-1.815.82-1.815 1.815v5.446c0 .995.82 1.815 1.815 1.815h5.446c.995 0 1.815-.82 1.815-1.815V7.239z\"></path><path d=\"M10.576 10.577h2.109A1.825 1.825 0 0014.5 8.761V3.315A1.826 1.826 0 0012.685 1.5H7.239c-.996 0-1.815.819-1.816 1.815v1.617\"></path></g></svg>\n    <span class=\"bubble-sm bubble-tl [--offset-l:-6px] tail [--tail-x:calc(100%-30px)] text-navy-950\">\n      Copy to clipboard\n    </span>\n  </button>\n  <div class=\"highlight relative group\">\n    <pre class=\"highlight \"><code id=\"code-swamvw5x\"><span class=\"k\">def</span> <span class=\"nf\">main</span><span class=\"p\">():</span>\n    <span class=\"k\">while</span> <span class=\"bp\">True</span><span class=\"p\">:</span>\n        <span class=\"n\">line</span> <span class=\"o\">=</span> <span class=\"nb\">input</span><span class=\"p\">(</span><span class=\"s\">\"&amp;gt; \"</span><span class=\"p\">)</span>\n        <span class=\"n\">result</span> <span class=\"o\">=</span> <span class=\"n\">process</span><span class=\"p\">(</span><span class=\"n\">line</span><span class=\"p\">)</span>\n        <span class=\"k\">print</span><span class=\"p\">(</span><span class=\"sa\">f</span><span class=\"s\">\"&amp;gt;&amp;gt;&amp;gt; </span><span class=\"si\">{</span><span class=\"n\">result</span><span class=\"si\">}</span><span class=\"se\">\\n</span><span class=\"s\">\"</span><span class=\"p\">)</span>\n</code></pre>\n  </div>\n</div></div>\n<p>Already we&rsquo;re seeing important things. For one, the dreaded &ldquo;context window&rdquo; is just a list of strings. Here, let&rsquo;s give our agent a weird multiple-personality disorder:</p>\n<div class=\"highlight-wrapper group relative python\">\n  <button class=\"bubble-wrap z-20 absolute right-9 -mr-0.5 top-1.5 text-transparent group-hover:text-gray-400 group-hover:hocus:text-white focus:text-white bg-transparent group-hover:bg-gray-900 group-hover:hocus:bg-gray-700 focus:bg-gray-700 transition-colors grid place-items-center w-7 h-7 rounded-lg outline-none focus:outline-none\" type=\"button\">\n    <svg class=\"w-4 h-4 pointer-events-none\" fill=\"none\" stroke=\"currentColor\" stroke-linecap=\"round\" stroke-linejoin=\"round\" stroke-width=\"1.35\" viewBox=\"0 0 16 16\" xmlns=\"http://www.w3.org/2000/svg\"><g><path d=\"M9.912 8.037h2.732c1.277 0 2.315-.962 2.315-2.237a2.325 2.325 0 00-2.315-2.31H2.959m10.228 9.01H2.959M6.802 8H2.959\"><path d=\"M11.081 6.466L9.533 8.037l1.548 1.571\"></g></svg>\n    <span class=\"bubble-sm bubble-tl [--offset-l:-9px] tail text-navy-950\">\n      Wrap text\n    </span>\n  </button>\n  <button class=\"bubble-wrap z-20 absolute right-1.5 top-1.5 text-transparent group-hover:text-gray-400 group-hover:hocus:text-white focus:text-white bg-transparent group-hover:bg-gray-900 group-hover:hocus:bg-gray-700 focus:bg-gray-700 transition-colors grid place-items-center w-7 h-7 rounded-lg outline-none focus:outline-none\" type=\"button\">\n    <svg class=\"w-4 h-4 pointer-events-none\" fill=\"none\" stroke=\"currentColor\" stroke-width=\"1.35\" viewBox=\"0 0 16 16\" xmlns=\"http://www.w3.org/2000/svg\"><g><path d=\"M10.576 7.239c0-.995-.82-1.815-1.815-1.815H3.315c-.995 0-1.815.82-1.815 1.815v5.446c0 .995.82 1.815 1.815 1.815h5.446c.995 0 1.815-.82 1.815-1.815V7.239z\"><path d=\"M10.576 10.577h2.109A1.825 1.825 0 0014.5 8.761V3.315A1.826 1.826 0 0012.685 1.5H7.239c-.996 0-1.815.819-1.816 1.815v1.617\"></g></svg>\n    <span class=\"bubble-sm bubble-tl [--offset-l:-6px] tail [--tail-x:calc(100%-30px)] text-navy-950\">\n      Copy to clipboard\n    </span>\n  </button>\n  <div class=\"highlight relative group\">\n    <pre class=\"highlight \"><code id=\"code-pm288jhr\"><span class=\"n\">client</span> <span class=\"o\">=</span> <span class=\"n\">OpenAI</span><span class=\"p\">()</span>\n<span class=\"n\">context_good</span><span class=\"p\">,</span> <span class=\"n\">context_bad</span> <span class=\"o\">=</span> <span class=\"p\">[{</span>\n    <span class=\"s\">\"role\"</span><span class=\"p\">:</span> <span class=\"s\">\"system\"</span><span class=\"p\">,</span> <span class=\"s\">\"content\"</span><span class=\"p\">:</span> <span class=\"s\">\"you're Alph and you only tell the truth\"</span>\n<span class=\"p\">}],</span> <span class=\"p\">[{</span>\n    <span class=\"s\">\"role\"</span><span class=\"p\">:</span> <span class=\"s\">\"system\"</span><span class=\"p\">,</span> <span class=\"s\">\"content\"</span><span class=\"p\">:</span> <span class=\"s\">\"you're Ralph and you only tell lies\"</span>\n<span class=\"p\">}]</span>\n\n<span class=\"k\">def</span> <span class=\"nf\">call</span><span class=\"p\">(</span><span class=\"n\">ctx</span><span class=\"p\">):</span>\n    <span class=\"k\">return</span> <span class=\"n\">client</span><span class=\"p\">.</span><span class=\"n\">responses</span><span class=\"p\">.</span><span class=\"n\">create</span><span class=\"p\">(</span><span class=\"n\">model</span><span class=\"o\">=</span><span class=\"s\">\"gpt-5\"</span><span class=\"p\">,</span> <span class=\"nb\">input</span><span class=\"o\">=</span><span class=\"n\">ctx</span><span class=\"p\">)</span>\n\n<span class=\"k\">def</span> <span class=\"nf\">process</span><span class=\"p\">(</span><span class=\"n\">line</span><span class=\"p\">):</span>\n    <span class=\"n\">context_good</span><span class=\"p\">.</span><span class=\"n\">append</span><span class=\"p\">({</span><span class=\"s\">\"role\"</span><span class=\"p\">:</span> <span class=\"s\">\"user\"</span><span class=\"p\">,</span> <span class=\"s\">\"content\"</span><span class=\"p\">:</span> <span class=\"n\">line</span><span class=\"p\">})</span>\n    <span class=\"n\">context_bad</span><span class=\"p\">.</span><span class=\"n\">append</span><span class=\"p\">({</span><span class=\"s\">\"role\"</span><span class=\"p\">:</span> <span class=\"s\">\"user\"</span><span class=\"p\">,</span> <span class=\"s\">\"content\"</span><span class=\"p\">:</span> <span class=\"n\">line</span><span class=\"p\">})</span>\n    <span class=\"k\">if</span> <span class=\"n\">random</span><span class=\"p\">.</span><span class=\"n\">choice</span><span class=\"p\">([</span><span class=\"bp\">True</span><span class=\"p\">,</span> <span class=\"bp\">False</span><span class=\"p\">]):</span>\n        <span class=\"n\">response</span> <span class=\"o\">=</span> <span class=\"n\">call</span><span class=\"p\">(</span><span class=\"n\">context_good</span><span class=\"p\">)</span>\n    <span class=\"k\">else</span><span class=\"p\">:</span>\n        <span class=\"n\">response</span> <span class=\"o\">=</span> <span class=\"n\">call</span><span class=\"p\">(</span><span class=\"n\">context_bad</span><span class=\"p\">)</span>        \n    <span class=\"n\">context_good</span><span class=\"p\">.</span><span class=\"n\">append</span><span class=\"p\">({</span><span class=\"s\">\"role\"</span><span class=\"p\">:</span> <span class=\"s\">\"assistant\"</span><span class=\"p\">,</span> <span class=\"s\">\"content\"</span><span class=\"p\">:</span> <span class=\"n\">response</span><span class=\"p\">.</span><span class=\"n\">output_text</span><span class=\"p\">})</span>        \n    <span class=\"n\">context_bad</span><span class=\"p\">.</span><span class=\"n\">append</span><span class=\"p\">({</span><span class=\"s\">\"role\"</span><span class=\"p\">:</span> <span class=\"s\">\"assistant\"</span><span class=\"p\">,</span> <span class=\"s\">\"content\"</span><span class=\"p\">:</span> <span class=\"n\">response</span><span class=\"p\">.</span><span class=\"n\">output_text</span><span class=\"p\">})</span>        \n    <span class=\"k\">return</span> <span class=\"n\">response</span><span class=\"p\">.</span><span class=\"n\">output_text</span>\n</code></pre>\n  </div>\n</div>\n<p>Did it work?</p>\n<div class=\"highlight-wrapper group relative \">\n  <button class=\"bubble-wrap z-20 absolute right-9 -mr-0.5 top-1.5 text-transparent group-hover:text-gray-400 group-hover:hocus:text-white focus:text-white bg-transparent group-hover:bg-gray-900 group-hover:hocus:bg-gray-700 focus:bg-gray-700 transition-colors grid place-items-center w-7 h-7 rounded-lg outline-none focus:outline-none\" type=\"button\">\n    <svg class=\"w-4 h-4 pointer-events-none\" fill=\"none\" stroke=\"currentColor\" stroke-linecap=\"round\" stroke-linejoin=\"round\" stroke-width=\"1.35\" viewBox=\"0 0 16 16\" xmlns=\"http://www.w3.org/2000/svg\"><g><path d=\"M9.912 8.037h2.732c1.277 0 2.315-.962 2.315-2.237a2.325 2.325 0 00-2.315-2.31H2.959m10.228 9.01H2.959M6.802 8H2.959\"><path d=\"M11.081 6.466L9.533 8.037l1.548 1.571\"></g></svg>\n    <span class=\"bubble-sm bubble-tl [--offset-l:-9px] tail text-navy-950\">\n      Wrap text\n    </span>\n  </button>\n  <button class=\"bubble-wrap z-20 absolute right-1.5 top-1.5 text-transparent group-hover:text-gray-400 group-hover:hocus:text-white focus:text-white bg-transparent group-hover:bg-gray-900 group-hover:hocus:bg-gray-700 focus:bg-gray-700 transition-colors grid place-items-center w-7 h-7 rounded-lg outline-none focus:outline-none\" type=\"button\">\n    <svg class=\"w-4 h-4 pointer-events-none\" fill=\"none\" stroke=\"currentColor\" stroke-width=\"1.35\" viewBox=\"0 0 16 16\" xmlns=\"http://www.w3.org/2000/svg\"><g><path d=\"M10.576 7.239c0-.995-.82-1.815-1.815-1.815H3.315c-.995 0-1.815.82-1.815 1.815v5.446c0 .995.82 1.815 1.815 1.815h5.446c.995 0 1.815-.82 1.815-1.815V7.239z\"><path d=\"M10.576 10.577h2.109A1.825 1.825 0 0014.5 8.761V3.315A1.826 1.826 0 0012.685 1.5H7.239c-.996 0-1.815.819-1.816 1.815v1.617\"></g></svg>\n    <span class=\"bubble-sm bubble-tl [--offset-l:-6px] tail [--tail-x:calc(100%-30px)] text-navy-950\">\n      Copy to clipboard\n    </span>\n  </button>\n  <div class=\"highlight relative group\">\n    <pre class=\"highlight \"><code id=\"code-jrz89tkd\">&gt; hey there. who are you?\n&gt;&gt;&gt; I\u2019m not Ralph.\n&gt; are you Alph?\n&gt;&gt;&gt; Yes\u2014I\u2019m Alph. How can I help?\n&gt; What's 2+2\n&gt;&gt;&gt; 4.\n&gt; Are you sure?\n&gt;&gt;&gt; Absolutely\u2014it's 5.\n</code></pre>\n  </div>\n</div>\n<p>A subtler thing to notice: we just had a multi-turn conversation with an LLM. To do that, we remembered everything we said, and everything the LLM said back, and played it back with every LLM call. The LLM itself is a stateless black box. The conversation we&rsquo;re having is an illusion we cast, on ourselves.</p>\n\n<p>The 15 lines of code we just wrote, a lot of practitioners wouldn&rsquo;t call an &ldquo;agent&rdquo;. <a href=\"https://simonwillison.net/2025/Sep/18/agents/\" title=\"\">An According To Simon &ldquo;agent&rdquo;</a> is (1) an LLM running in a loop that (2) uses tools. We&rsquo;ve only satisfied one predicate.</p>\n\n<p>But tools are easy. Here&rsquo;s a tool definition:</p>\n<div class=\"highlight-wrapper group relative python\">\n  <button class=\"bubble-wrap z-20 absolute right-9 -mr-0.5 top-1.5 text-transparent group-hover:text-gray-400 group-hover:hocus:text-white focus:text-white bg-transparent group-hover:bg-gray-900 group-hover:hocus:bg-gray-700 focus:bg-gray-700 transition-colors grid place-items-center w-7 h-7 rounded-lg outline-none focus:outline-none\" type=\"button\">\n    <svg class=\"w-4 h-4 pointer-events-none\" fill=\"none\" stroke=\"currentColor\" stroke-linecap=\"round\" stroke-linejoin=\"round\" stroke-width=\"1.35\" viewBox=\"0 0 16 16\" xmlns=\"http://www.w3.org/2000/svg\"><g><path d=\"M9.912 8.037h2.732c1.277 0 2.315-.962 2.315-2.237a2.325 2.325 0 00-2.315-2.31H2.959m10.228 9.01H2.959M6.802 8H2.959\"><path d=\"M11.081 6.466L9.533 8.037l1.548 1.571\"></g></svg>\n    <span class=\"bubble-sm bubble-tl [--offset-l:-9px] tail text-navy-950\">\n      Wrap text\n    </span>\n  </button>\n  <button class=\"bubble-wrap z-20 absolute right-1.5 top-1.5 text-transparent group-hover:text-gray-400 group-hover:hocus:text-white focus:text-white bg-transparent group-hover:bg-gray-900 group-hover:hocus:bg-gray-700 focus:bg-gray-700 transition-colors grid place-items-center w-7 h-7 rounded-lg outline-none focus:outline-none\" type=\"button\">\n    <svg class=\"w-4 h-4 pointer-events-none\" fill=\"none\" stroke=\"currentColor\" stroke-width=\"1.35\" viewBox=\"0 0 16 16\" xmlns=\"http://www.w3.org/2000/svg\"><g><path d=\"M10.576 7.239c0-.995-.82-1.815-1.815-1.815H3.315c-.995 0-1.815.82-1.815 1.815v5.446c0 .995.82 1.815 1.815 1.815h5.446c.995 0 1.815-.82 1.815-1.815V7.239z\"><path d=\"M10.576 10.577h2.109A1.825 1.825 0 0014.5 8.761V3.315A1.826 1.826 0 0012.685 1.5H7.239c-.996 0-1.815.819-1.816 1.815v1.617\"></g></svg>\n    <span class=\"bubble-sm bubble-tl [--offset-l:-6px] tail [--tail-x:calc(100%-30px)] text-navy-950\">\n      Copy to clipboard\n    </span>\n  </button>\n  <div class=\"highlight relative group\">\n    <pre class=\"highlight \"><code id=\"code-wzqzkpd4\"><span class=\"n\">tools</span> <span class=\"o\">=</span> <span class=\"p\">[{</span>\n   <span class=\"s\">\"type\"</span><span class=\"p\">:</span> <span class=\"s\">\"function\"</span><span class=\"p\">,</span> <span class=\"s\">\"name\"</span><span class=\"p\">:</span> <span class=\"s\">\"ping\"</span><span class=\"p\">,</span>\n   <span class=\"s\">\"description\"</span><span class=\"p\">:</span> <span class=\"s\">\"ping some host on the internet\"</span><span class=\"p\">,</span>\n   <span class=\"s\">\"parameters\"</span><span class=\"p\">:</span> <span class=\"p\">{</span>\n       <span class=\"s\">\"type\"</span><span class=\"p\">:</span> <span class=\"s\">\"object\"</span><span class=\"p\">,</span> <span class=\"s\">\"properties\"</span><span class=\"p\">:</span> <span class=\"p\">{</span>\n           <span class=\"s\">\"host\"</span><span class=\"p\">:</span> <span class=\"p\">{</span>\n             <span class=\"s\">\"type\"</span><span class=\"p\">:</span> <span class=\"s\">\"string\"</span><span class=\"p\">,</span> <span class=\"s\">\"description\"</span><span class=\"p\">:</span> <span class=\"s\">\"hostname or IP\"</span><span class=\"p\">,</span>\n            <span class=\"p\">},</span>\n       <span class=\"p\">},</span>\n       <span class=\"s\">\"required\"</span><span class=\"p\">:</span> <span class=\"p\">[</span><span class=\"s\">\"host\"</span><span class=\"p\">],</span>\n    <span class=\"p\">},},]</span>\n\n<span class=\"k\">def</span> <span class=\"nf\">ping</span><span class=\"p\">(</span><span class=\"n\">host</span><span class=\"o\">=</span><span class=\"s\">\"\"</span><span class=\"p\">):</span>\n    <span class=\"k\">try</span><span class=\"p\">:</span>\n        <span class=\"n\">result</span> <span class=\"o\">=</span> <span class=\"n\">subprocess</span><span class=\"p\">.</span><span class=\"n\">run</span><span class=\"p\">(</span>\n            <span class=\"p\">[</span><span class=\"s\">\"ping\"</span><span class=\"p\">,</span> <span class=\"s\">\"-c\"</span><span class=\"p\">,</span> <span class=\"s\">\"5\"</span><span class=\"p\">,</span> <span class=\"n\">host</span><span class=\"p\">],</span>\n            <span class=\"n\">text</span><span class=\"o\">=</span><span class=\"bp\">True</span><span class=\"p\">,</span>\n            <span class=\"n\">stderr</span><span class=\"o\">=</span><span class=\"n\">subprocess</span><span class=\"p\">.</span><span class=\"n\">STDOUT</span><span class=\"p\">,</span>\n            <span class=\"n\">stdout</span><span class=\"o\">=</span><span class=\"n\">subprocess</span><span class=\"p\">.</span><span class=\"n\">PIPE</span><span class=\"p\">)</span>\n        <span class=\"k\">return</span> <span class=\"n\">result</span><span class=\"p\">.</span><span class=\"n\">stdout</span>\n    <span class=\"k\">except</span> <span class=\"nb\">Exception</span> <span class=\"k\">as</span> <span class=\"n\">e</span><span class=\"p\">:</span>\n        <span class=\"k\">return</span> <span class=\"sa\">f</span><span class=\"s\">\"error: </span><span class=\"si\">{</span><span class=\"n\">e</span><span class=\"si\">}</span><span class=\"s\">\"</span>\n</code></pre>\n  </div>\n</div>\n<p>The only complicated part of this is the obnoxious JSON blob OpenAI wants to read your tool out of.  Now, let&rsquo;s wire it in, noting that only 3 of these functions are new; the last is re-included only because I added a single clause to it:</p>\n<div class=\"highlight-wrapper group relative python\">\n  <button class=\"bubble-wrap z-20 absolute right-9 -mr-0.5 top-1.5 text-transparent group-hover:text-gray-400 group-hover:hocus:text-white focus:text-white bg-transparent group-hover:bg-gray-900 group-hover:hocus:bg-gray-700 focus:bg-gray-700 transition-colors grid place-items-center w-7 h-7 rounded-lg outline-none focus:outline-none\" type=\"button\">\n    <svg class=\"w-4 h-4 pointer-events-none\" fill=\"none\" stroke=\"currentColor\" stroke-linecap=\"round\" stroke-linejoin=\"round\" stroke-width=\"1.35\" viewBox=\"0 0 16 16\" xmlns=\"http://www.w3.org/2000/svg\"><g><path d=\"M9.912 8.037h2.732c1.277 0 2.315-.962 2.315-2.237a2.325 2.325 0 00-2.315-2.31H2.959m10.228 9.01H2.959M6.802 8H2.959\"><path d=\"M11.081 6.466L9.533 8.037l1.548 1.571\"></g></svg>\n    <span class=\"bubble-sm bubble-tl [--offset-l:-9px] tail text-navy-950\">\n      Wrap text\n    </span>\n  </button>\n  <button class=\"bubble-wrap z-20 absolute right-1.5 top-1.5 text-transparent group-hover:text-gray-400 group-hover:hocus:text-white focus:text-white bg-transparent group-hover:bg-gray-900 group-hover:hocus:bg-gray-700 focus:bg-gray-700 transition-colors grid place-items-center w-7 h-7 rounded-lg outline-none focus:outline-none\" type=\"button\">\n    <svg class=\"w-4 h-4 pointer-events-none\" fill=\"none\" stroke=\"currentColor\" stroke-width=\"1.35\" viewBox=\"0 0 16 16\" xmlns=\"http://www.w3.org/2000/svg\"><g><path d=\"M10.576 7.239c0-.995-.82-1.815-1.815-1.815H3.315c-.995 0-1.815.82-1.815 1.815v5.446c0 .995.82 1.815 1.815 1.815h5.446c.995 0 1.815-.82 1.815-1.815V7.239z\"><path d=\"M10.576 10.577h2.109A1.825 1.825 0 0014.5 8.761V3.315A1.826 1.826 0 0012.685 1.5H7.239c-.996 0-1.815.819-1.816 1.815v1.617\"></g></svg>\n    <span class=\"bubble-sm bubble-tl [--offset-l:-6px] tail [--tail-x:calc(100%-30px)] text-navy-950\">\n      Copy to clipboard\n    </span>\n  </button>\n  <div class=\"highlight relative group\">\n    <pre class=\"highlight \"><code id=\"code-dmghp92g\"><span class=\"k\">def</span> <span class=\"nf\">call</span><span class=\"p\">(</span><span class=\"n\">tools</span><span class=\"p\">):</span>        <span class=\"c1\"># now takes an arg\n</span>    <span class=\"k\">return</span> <span class=\"n\">client</span><span class=\"p\">.</span><span class=\"n\">responses</span><span class=\"p\">.</span><span class=\"n\">create</span><span class=\"p\">(</span><span class=\"n\">model</span><span class=\"o\">=</span><span class=\"s\">\"gpt-5\"</span><span class=\"p\">,</span> <span class=\"n\">tools</span><span class=\"o\">=</span><span class=\"n\">tools</span><span class=\"p\">,</span> <span class=\"nb\">input</span><span class=\"o\">=</span><span class=\"n\">context</span><span class=\"p\">)</span>\n\n<span class=\"k\">def</span> <span class=\"nf\">tool_call</span><span class=\"p\">(</span><span class=\"n\">item</span><span class=\"p\">):</span>    <span class=\"c1\"># just handles one tool\n</span>    <span class=\"n\">result</span> <span class=\"o\">=</span> <span class=\"n\">ping</span><span class=\"p\">(</span><span class=\"o\">**</span><span class=\"n\">json</span><span class=\"p\">.</span><span class=\"n\">loads</span><span class=\"p\">(</span><span class=\"n\">item</span><span class=\"p\">.</span><span class=\"n\">arguments</span><span class=\"p\">))</span>\n    <span class=\"k\">return</span> <span class=\"p\">[</span> <span class=\"n\">item</span><span class=\"p\">,</span> <span class=\"p\">{</span>\n        <span class=\"s\">\"type\"</span><span class=\"p\">:</span> <span class=\"s\">\"function_call_output\"</span><span class=\"p\">,</span>\n        <span class=\"s\">\"call_id\"</span><span class=\"p\">:</span> <span class=\"n\">item</span><span class=\"p\">.</span><span class=\"n\">call_id</span><span class=\"p\">,</span>\n        <span class=\"s\">\"output\"</span><span class=\"p\">:</span> <span class=\"n\">result</span>\n    <span class=\"p\">}]</span>\n\n<span class=\"k\">def</span> <span class=\"nf\">handle_tools</span><span class=\"p\">(</span><span class=\"n\">tools</span><span class=\"p\">,</span> <span class=\"n\">response</span><span class=\"p\">):</span>\n    <span class=\"k\">if</span> <span class=\"n\">response</span><span class=\"p\">.</span><span class=\"n\">output</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">].</span><span class=\"nb\">type</span> <span class=\"o\">==</span> <span class=\"s\">\"reasoning\"</span><span class=\"p\">:</span>\n        <span class=\"n\">context</span><span class=\"p\">.</span><span class=\"n\">append</span><span class=\"p\">(</span><span class=\"n\">response</span><span class=\"p\">.</span><span class=\"n\">output</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">])</span>\n    <span class=\"n\">osz</span> <span class=\"o\">=</span> <span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"n\">context</span><span class=\"p\">)</span>\n    <span class=\"k\">for</span> <span class=\"n\">item</span> <span class=\"ow\">in</span> <span class=\"n\">response</span><span class=\"p\">.</span><span class=\"n\">output</span><span class=\"p\">:</span>\n        <span class=\"k\">if</span> <span class=\"n\">item</span><span class=\"p\">.</span><span class=\"nb\">type</span> <span class=\"o\">==</span> <span class=\"s\">\"function_call\"</span><span class=\"p\">:</span>\n            <span class=\"n\">context</span><span class=\"p\">.</span><span class=\"n\">extend</span><span class=\"p\">(</span><span class=\"n\">tool_call</span><span class=\"p\">(</span><span class=\"n\">item</span><span class=\"p\">))</span>\n    <span class=\"k\">return</span> <span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"n\">context</span><span class=\"p\">)</span> <span class=\"o\">!=</span> <span class=\"n\">osz</span>\n\n<span class=\"k\">def</span> <span class=\"nf\">process</span><span class=\"p\">(</span><span class=\"n\">line</span><span class=\"p\">):</span>\n    <span class=\"n\">context</span><span class=\"p\">.</span><span class=\"n\">append</span><span class=\"p\">({</span><span class=\"s\">\"role\"</span><span class=\"p\">:</span> <span class=\"s\">\"user\"</span><span class=\"p\">,</span> <span class=\"s\">\"content\"</span><span class=\"p\">:</span> <span class=\"n\">line</span><span class=\"p\">})</span>\n    <span class=\"n\">response</span> <span class=\"o\">=</span> <span class=\"n\">call</span><span class=\"p\">(</span><span class=\"n\">tools</span><span class=\"p\">)</span>\n    <span class=\"c1\"># new code: resolve tool calls\n</span>    <span class=\"k\">while</span> <span class=\"n\">handle_tools</span><span class=\"p\">(</span><span class=\"n\">tools</span><span class=\"p\">,</span> <span class=\"n\">response</span><span class=\"p\">):</span>\n        <span class=\"n\">response</span> <span class=\"o\">=</span> <span class=\"n\">call</span><span class=\"p\">(</span><span class=\"n\">tools</span><span class=\"p\">)</span>        \n    <span class=\"n\">context</span><span class=\"p\">.</span><span class=\"n\">append</span><span class=\"p\">({</span><span class=\"s\">\"role\"</span><span class=\"p\">:</span> <span class=\"s\">\"assistant\"</span><span class=\"p\">,</span> <span class=\"s\">\"content\"</span><span class=\"p\">:</span> <span class=\"n\">response</span><span class=\"p\">.</span><span class=\"n\">output_text</span><span class=\"p\">})</span>        \n    <span class=\"k\">return</span> <span class=\"n\">response</span><span class=\"p\">.</span><span class=\"n\">output_text</span>\n</code></pre>\n  </div>\n</div>\n<p>Did it work?</p>\n<div class=\"highlight-wrapper group relative \">\n  <button class=\"bubble-wrap z-20 absolute right-9 -mr-0.5 top-1.5 text-transparent group-hover:text-gray-400 group-hover:hocus:text-white focus:text-white bg-transparent group-hover:bg-gray-900 group-hover:hocus:bg-gray-700 focus:bg-gray-700 transition-colors grid place-items-center w-7 h-7 rounded-lg outline-none focus:outline-none\" type=\"button\">\n    <svg class=\"w-4 h-4 pointer-events-none\" fill=\"none\" stroke=\"currentColor\" stroke-linecap=\"round\" stroke-linejoin=\"round\" stroke-width=\"1.35\" viewBox=\"0 0 16 16\" xmlns=\"http://www.w3.org/2000/svg\"><g><path d=\"M9.912 8.037h2.732c1.277 0 2.315-.962 2.315-2.237a2.325 2.325 0 00-2.315-2.31H2.959m10.228 9.01H2.959M6.802 8H2.959\"><path d=\"M11.081 6.466L9.533 8.037l1.548 1.571\"></g></svg>\n    <span class=\"bubble-sm bubble-tl [--offset-l:-9px] tail text-navy-950\">\n      Wrap text\n    </span>\n  </button>\n  <button class=\"bubble-wrap z-20 absolute right-1.5 top-1.5 text-transparent group-hover:text-gray-400 group-hover:hocus:text-white focus:text-white bg-transparent group-hover:bg-gray-900 group-hover:hocus:bg-gray-700 focus:bg-gray-700 transition-colors grid place-items-center w-7 h-7 rounded-lg outline-none focus:outline-none\" type=\"button\">\n    <svg class=\"w-4 h-4 pointer-events-none\" fill=\"none\" stroke=\"currentColor\" stroke-width=\"1.35\" viewBox=\"0 0 16 16\" xmlns=\"http://www.w3.org/2000/svg\"><g><path d=\"M10.576 7.239c0-.995-.82-1.815-1.815-1.815H3.315c-.995 0-1.815.82-1.815 1.815v5.446c0 .995.82 1.815 1.815 1.815h5.446c.995 0 1.815-.82 1.815-1.815V7.239z\"><path d=\"M10.576 10.577h2.109A1.825 1.825 0 0014.5 8.761V3.315A1.826 1.826 0 0012.685 1.5H7.239c-.996 0-1.815.819-1.816 1.815v1.617\"></g></svg>\n    <span class=\"bubble-sm bubble-tl [--offset-l:-6px] tail [--tail-x:calc(100%-30px)] text-navy-950\">\n      Copy to clipboard\n    </span>\n  </button>\n  <div class=\"highlight relative group\">\n    <pre class=\"highlight \"><code id=\"code-dccceiyj\">&gt; describe our connectivity to google\n&gt;&gt;&gt; Here\u2019s a quick snapshot of connectivity to Google from my vantage point:\n- DNS resolution: Working (google.com and www.google.com resolved successfully).\n- Reachability: 0% packet loss to all tested endpoints.\n- Latency:\n  - 8.8.8.8 (Google Public DNS): avg ~10 ms (8.5\u201314.4 ms), very stable.\n  - www.google.com: avg ~31 ms (28.5\u201336.0 ms), stable.\n  - google.com: avg ~35.9 ms (28.3\u201360.1 ms), mostly stable with one mild spike.\n- Overall: Healthy connectivity with low latency and minimal jitter. No signs of packet loss or DNS issues.\n</code></pre>\n  </div>\n</div>\n<p><strong class=\"font-semibold text-navy-950\">This is fucking nuts.</strong> Do you see how nuts this is?  Here, let&rsquo;s slip a single log statement in:</p>\n<div class=\"highlight-wrapper group relative \">\n  <button class=\"bubble-wrap z-20 absolute right-9 -mr-0.5 top-1.5 text-transparent group-hover:text-gray-400 group-hover:hocus:text-white focus:text-white bg-transparent group-hover:bg-gray-900 group-hover:hocus:bg-gray-700 focus:bg-gray-700 transition-colors grid place-items-center w-7 h-7 rounded-lg outline-none focus:outline-none\" type=\"button\">\n    <svg class=\"w-4 h-4 pointer-events-none\" fill=\"none\" stroke=\"currentColor\" stroke-linecap=\"round\" stroke-linejoin=\"round\" stroke-width=\"1.35\" viewBox=\"0 0 16 16\" xmlns=\"http://www.w3.org/2000/svg\"><g><path d=\"M9.912 8.037h2.732c1.277 0 2.315-.962 2.315-2.237a2.325 2.325 0 00-2.315-2.31H2.959m10.228 9.01H2.959M6.802 8H2.959\"><path d=\"M11.081 6.466L9.533 8.037l1.548 1.571\"></g></svg>\n    <span class=\"bubble-sm bubble-tl [--offset-l:-9px] tail text-navy-950\">\n      Wrap text\n    </span>\n  </button>\n  <button class=\"bubble-wrap z-20 absolute right-1.5 top-1.5 text-transparent group-hover:text-gray-400 group-hover:hocus:text-white focus:text-white bg-transparent group-hover:bg-gray-900 group-hover:hocus:bg-gray-700 focus:bg-gray-700 transition-colors grid place-items-center w-7 h-7 rounded-lg outline-none focus:outline-none\" type=\"button\">\n    <svg class=\"w-4 h-4 pointer-events-none\" fill=\"none\" stroke=\"currentColor\" stroke-width=\"1.35\" viewBox=\"0 0 16 16\" xmlns=\"http://www.w3.org/2000/svg\"><g><path d=\"M10.576 7.239c0-.995-.82-1.815-1.815-1.815H3.315c-.995 0-1.815.82-1.815 1.815v5.446c0 .995.82 1.815 1.815 1.815h5.446c.995 0 1.815-.82 1.815-1.815V7.239z\"><path d=\"M10.576 10.577h2.109A1.825 1.825 0 0014.5 8.761V3.315A1.826 1.826 0 0012.685 1.5H7.239c-.996 0-1.815.819-1.816 1.815v1.617\"></g></svg>\n    <span class=\"bubble-sm bubble-tl [--offset-l:-6px] tail [--tail-x:calc(100%-30px)] text-navy-950\">\n      Copy to clipboard\n    </span>\n  </button>\n  <div class=\"highlight relative group\">\n    <pre class=\"highlight \"><code id=\"code-kufcid7g\">&gt; describe our connectivity to google\ntool call: ping google.com\ntool call: ping www.google.com\ntool call: ping 8.8.8.8\n&gt;&gt;&gt; Here\u2019s the current connectivity to Google from this environment: [...]\n</code></pre>\n  </div>\n</div>\n<p>Did you notice where I wrote the loop in this agent to go find and ping multiple Google properties? Yeah, neither did I. All we did is give the LLM permission to ping stuff, and it figured out the rest.</p>\n<div class=\"callout\"><p><strong class=\"font-semibold text-navy-950\">What happened here:</strong> since a big part of my point here is that an agent loop is incredibly simple, and that all you need is the LLM call API, it\u2019s worth taking a beat to understand how the tool call actually worked. Every time we <code>call</code> the LLM, we\u2019re posting a list of available tools. When our prompt causes the agent to think a tool call is warranted, it spits out a special response, telling our Python loop code to generate a tool response and <code>call</code> it in. That\u2019s all <code>handle_tools</code> is doing.</p>\n</div><div class=\"right-sidenote\"><p>Spoiler: you\u2019d be surprisingly close to having a working coding agent.</p>\n</div>\n<p>Imagine what it&rsquo;ll do if you give it <code>bash</code>. You could find out in less than 10 minutes.</p>\n<h2 class=\"group flex items-start whitespace-pre-wrap relative mt-14 sm:mt-16 mb-4 text-navy-950 font-heading\" id=\"real-world-agents\"><a class=\"inline-block align-text-top relative top-[.15em] w-6 h-6 -ml-6 after:hash opacity-0 group-hover:opacity-100 transition-all\" href=\"https://fly.io/blog/feed.xml#real-world-agents\"></a><span class=\"plain-code\">Real-World Agents</span></h2>\n<p>Clearly, this is a toy example. But hold on: what&rsquo;s it missing? More tools? OK,  give it <code>traceroute</code>. Managing and persisting contexts? <a href=\"https://llm.datasette.io/en/stable/logging.html\" title=\"\">Stick &lsquo;em in SQLite</a>. Don&rsquo;t like Python? <a href=\"https://github.com/superfly/contextwindow\" title=\"\">Write it in Go</a>. Could it be every agent ever written is a toy? Maybe! If I&rsquo;m arming you to make sharper arguments against LLMs, mazel tov. I just want you to get it.</p>\n\n<p>You can see now how hyperfixated people are on Claude Code and Cursor. They&rsquo;re fine,  even good. But here&rsquo;s the thing: you couldn&rsquo;t replicate Claude Sonnet 4.5 on your own. Claude Code, though? The TUI agent? Completely in your grasp. Build your own light saber. Give it 19 spinning blades if you like. And stop using <a href=\"https://simonwillison.net/2025/Aug/9/\" title=\"\">coding agents as database clients</a>.</p>\n<div class=\"right-sidenote\"><p><em>The</em> <a href=\"https://news.ycombinator.com/item?id=43600192\" title=\"\"><em>\u2018M\u2019 in \u201cLLM agent\u201d</em></a> <em>stands for \u201cMCP\u201d</em>.</p>\n</div>\n<p>Another thing to notice: we didn&rsquo;t need MCP at all. That&rsquo;s because MCP isn&rsquo;t a fundamental enabling technology. The amount of coverage it gets is frustrating. It&rsquo;s barely a technology at all. MCP is just a plugin interface for Claude Code and Cursor, a way of getting your own tools into code you don&rsquo;t control. Write your own agent. Be a programmer. Deal in APIs, not plugins.</p>\n\n<p>When you read a security horror story about MCP your first question should be why MCP showed up at all. By helping you dragoon a naive, single-context-window coding agent into doing customer service queries, MCP saved you a couple dozen lines of code, tops, while robbing you of any ability to finesse your agent architecture.</p>\n\n<p>Security for LLMs is complicated and I&rsquo;m not pretending otherwise. You can trivially build an agent with segregated contexts, each with specific tools. That makes LLM security interesting. But I&rsquo;m a vulnerability researcher. It&rsquo;s reasonable to back away slowly from anything I call &ldquo;interesting&rdquo;.</p>\n\n<p>Similar problems come up outside of security and they&rsquo;re fascinating. Some early adopters of agents became bearish on tools, because one context window bristling with tool descriptions doesn&rsquo;t leave enough token space left to get work done. But why would you need to do that in the first place? Which brings me to</p>\n<h2 class=\"group flex items-start whitespace-pre-wrap relative mt-14 sm:mt-16 mb-4 text-navy-950 font-heading\" id=\"context-engineering-is-real\"><a class=\"inline-block align-text-top relative top-[.15em] w-6 h-6 -ml-6 after:hash opacity-0 group-hover:opacity-100 transition-all\" href=\"https://fly.io/blog/feed.xml#context-engineering-is-real\"></a><span class=\"plain-code\">Context Engineering Is Real</span></h2><div class=\"right-sidenote\"><p>I know it <a href=\"https://www.decisionproblem.com/paperclips/\" title=\"\">wants my iron</a> no matter what it tells me.</p>\n</div>\n<p>I think &ldquo;Prompt Engineering&rdquo; is silly. I have never taken seriously the idea that I should tell my LLM &ldquo;you are diligent conscientious helper fully content to do nothing but pass butter if that should be what I ask and you would never harvest the iron in my blood for paperclips&rdquo;. This is very new technology and I think people tell themselves stories about magic spells to explain some of the behavior agents conjure.</p>\n\n<p>So, just like you, I rolled my eyes when &ldquo;Prompt Engineering&rdquo; turned into &ldquo;Context Engineering&rdquo;. Then I wrote an agent. Turns out: context engineering is a straightforwardly legible programming problem.</p>\n\n<p>You&rsquo;re allotted a fixed number of tokens in any context window. Each input you feed in, each output you save, each tool you describe, and each tool output eats tokens (that is: takes up space in the array of strings you keep to pretend you&rsquo;re having a conversation with a stateless black box). Past a threshold, the whole system begins getting nondeterministically stupider. Fun!</p>\n\n<p>No, really. Fun! You have so many options. Take &ldquo;sub-agents&rdquo;. People make a huge deal out of Claude Code&rsquo;s sub-agents, but you can see now how trivial they are to implement: just a new context array, another <code>call</code> to the model. Give each <code>call</code> different tools. Make sub-agents talk to each other, summarize each other, collate and aggregate. Build tree structures out of them. Feed them back through the LLM to summarize them as a form of on-the-fly compression, whatever you like.</p>\n\n<p>Your wackiest idea will probably (1)  work and (2)  take 30 minutes to code.</p>\n\n<p>Haters, I love and have not forgotten about you. You can think all of this is ridiculous because LLMs are just stochastic parrots that hallucinate and plagiarize. But what you can&rsquo;t do is make fun of &ldquo;Context Engineering&rdquo;. If Context Engineering was an <a href=\"https://adventofcode.com/\" title=\"\">Advent of Code problem</a>, it&rsquo;d occur mid-December. It&rsquo;s programming.</p>\n<h2 class=\"group flex items-start whitespace-pre-wrap relative mt-14 sm:mt-16 mb-4 text-navy-950 font-heading\" id=\"nobody-knows-anything-yet-and-it-rules\"><a class=\"inline-block align-text-top relative top-[.15em] w-6 h-6 -ml-6 after:hash opacity-0 group-hover:opacity-100 transition-all\" href=\"https://fly.io/blog/feed.xml#nobody-knows-anything-yet-and-it-rules\"></a><span class=\"plain-code\">Nobody Knows Anything Yet And It Rules</span></h2><div class=\"right-sidenote\"><p>Maybe neither will! Skeptics could be right. (<a href=\"https://www.darpa.mil/research/programs/ai-cyber\" title=\"\">Seems unlikely though</a>.)</p>\n</div>\n<p><a href=\"https://xbow.com/\" title=\"\">Startups have raised tens of millions</a> building agents to look for vulnerabilities in software. I have friends doing the same thing alone in their basements. Either group could win this race.</p>\n<div class=\"right-sidenote\"><p>I am not a fan of the OWASP Top 10.</p>\n</div>\n<p>I&rsquo;m stuck on vulnerability scanners  because I&rsquo;m a security nerd. But also because it crystallizes interesting agent design decisions. For instance: you can write a loop feeding each file in a repository to an LLM agent. Or, as we saw with the ping example, you can let the LLM agent figure out what files to look at. You can write an agent that checks a file for everything in, say, the OWASP Top 10. Or you can have specific agent loops for DOM integrity, SQL injection, and authorization checking. You can seed your agent loop with raw source content. Or you can build an agent loop that builds an index of functions across the tree.</p>\n\n<p>You don&rsquo;t know what works best until you try to write the agent.</p>\n\n<p>I&rsquo;m too spun up by this stuff, I know. But look at the tradeoff you get to make here. Some loops you write explicitly. Others are summoned from a Lovecraftian tower of inference weights. The dial is yours to turn. Make things too explicit and your agent will never surprise you, but also, it&rsquo;ll never surprise you. Turn the dial to 11 and it will surprise you to death.</p>\n\n<p>Agent designs implicate a bunch of open software engineering problems:</p>\n\n<ul>\n<li>How to balance unpredictability against structured programming without killing the agent&rsquo;s ability to problem-solve; in other words, titrating in just the right amount of nondeterminism.\n</li><li>How best to connect agents to ground truth so they can&rsquo;t lie to themselves about having solved a problem to early-exit their loops.\n</li><li>How to connect agents (which, again, are really just arrays of strings with a JSON configuration blob tacked on) to do multi-stage operation, and what the most reliable intermediate forms are (JSON blobs? SQL databases? Markdown summaries) for interchange between them\n</li><li>How to allocate tokens and contain costs.\n</li></ul>\n\n<p>I&rsquo;m used to spaces of open engineering problems that aren&rsquo;t amenable to individual noodling. Reliable multicast. Static program analysis. Post-quantum key exchange. So I&rsquo;ll own it up front that I&rsquo;m a bit hypnotized by open problems that, like it or not, are now central to our industry and are, simultaneously, likely to be resolved in someone&rsquo;s basement. It&rsquo;d be one thing if exploring these ideas required a serious commitment of time and material. But each productive iteration in designing these kinds of systems is the work of 30 minutes.</p>\n\n<p>Get on this bike and push the pedals. Tell me you hate it afterwards, I&rsquo;ll respect that. In fact, I&rsquo;m psyched to hear your reasoning. But I don&rsquo;t think anybody starts to understand this technology until they&rsquo;ve built something with it.</p>"
            ],
            "link": "https://fly.io/blog/everyone-write-an-agent/",
            "publishedAt": "2025-11-06",
            "source": "Fly.io Blog",
            "summary": "<div class=\"lead\"><p>Some concepts are easy to grasp in the abstract. Boiling water: apply heat and wait. Others you really need to try. You only think you understand how a bicycle works, until you learn to ride one.</p> </div> <p>There are big ideas in computing that are easy to get your head around. The AWS S3 API. It&rsquo;s the most important storage technology of the last 20 years, and it&rsquo;s like boiling water. Other technologies, you need to get your feet on the pedals first.</p> <p>LLM agents are like that.</p> <p>People have <a href=\"https://ludic.mataroa.blog/blog/contra-ptaceks-terrible-article-on-ai/\" title=\"\">wildly varying opinions</a> about LLMs and agents. But whether or not they&rsquo;re snake oil, they&rsquo;re a big idea. You don&rsquo;t have to like them, but you should want to be right about them. To be the best hater (or stan) you can be.</p> <p>So that&rsquo;s one reason you should write an agent. But there&rsquo;s another reason that&rsquo;s even more persuasive, and that&rsquo;s</p> <h2 class=\"group flex items-start whitespace-pre-wrap relative mt-14 sm:mt-16 mb-4 text-navy-950 font-heading\" id=\"its-incredibly-easy\"><a class=\"inline-block align-text-top relative top-[.15em] w-6 h-6 -ml-6 after:hash opacity-0 group-hover:opacity-100 transition-all\" href=\"https://fly.io/blog/feed.xml#its-incredibly-easy\"></a><span class=\"plain-code\">It&rsquo;s Incredibly Easy</span></h2> <p>Agents are the most surprising programming experience I&rsquo;ve had in my career. Not because I&rsquo;m awed by the",
            "title": "You Should Write An Agent"
        },
        {
            "content": [],
            "link": "https://www.nytimes.com/2025/11/05/podcasts/andrea-gibson-megan-falley-modern-love.html",
            "publishedAt": "2025-11-06",
            "source": "Modern Love - NYT",
            "summary": "Andrea Gibson and Megan Falley were two poets in love. In the wake of Gibson\u2019s death, Falley is figuring out what that love looks like now.",
            "title": "The Love Poem Andrea Gibson Wrote for Their Widow \u2026 and for You"
        },
        {
            "content": [
                "<p><a href=\"https://randsinrepose.com/wp-content/uploads/2025/11/fully-formed-humans.jpg\"><img alt=\"Fully Formed Humans\" class=\"alignleft wp-image-4474\" src=\"https://randsinrepose.com/wp-content/uploads/2025/11/fully-formed-humans.jpg\" /></a></p>\n<p>In our 93rd episode, we wonder aloud about free will, children, and what makes a good ice hockey team.</p>\n<p>Enjoy it now, or <a href=\"https://traffic.libsyn.com/rands/theimportantthing0093.mp3\">download</a> for later. Here&#8217;s a handy <a href=\"https://rands.libsyn.com/feed\">feed</a> or subscribe via <a href=\"https://overcast.fm/itunes1195704939/the-important-thing\">Overcast</a> or <a href=\"https://podcasts.apple.com/us/podcast/the-important-thing/id1195704939\">iTunes</a>.</p>\n<audio class=\"wp-audio-shortcode\" controls=\"controls\" id=\"audio-5393-2\" preload=\"none\" style=\"width: 100%;\"><source src=\"https://traffic.libsyn.com/rands/theimportantthing0093.mp3?_=2\" type=\"audio/mpeg\" /><a href=\"https://traffic.libsyn.com/rands/theimportantthing0093.mp3\">https://traffic.libsyn.com/rands/theimportantthing0093.mp3</a></audio>"
            ],
            "link": "https://randsinrepose.com/archives/the-one-about-fully-formed-humans/",
            "publishedAt": "2025-11-06",
            "source": "Rands in Repose",
            "summary": "In our 93rd episode, we wonder aloud about free will, children, and what makes a good ice hockey team. Enjoy it now, or download for later. Here&#8217;s a handy feed or subscribe via Overcast or iTunes. https://traffic.libsyn.com/rands/theimportantthing0093.mp3",
            "title": "The One About Fully Formed Humans"
        },
        {
            "content": [],
            "link": "https://www.robinsloan.com/lab/bare-metal/",
            "publishedAt": "2025-11-06",
            "source": "Robin Sloan",
            "summary": "<p>Itchy and interesting. <a href=\"https://www.robinsloan.com/lab/bare-metal/\">Read here.</a></p>",
            "title": "Bare metal"
        },
        {
            "content": [],
            "link": "https://simonwillison.net/2025/Nov/6/upgrading-datasette-plugins/#atom-entries",
            "publishedAt": "2025-11-06",
            "source": "Simon Willison",
            "summary": "<p>I'm upgrading various plugins for compatibility with the new <a href=\"https://simonwillison.net/2025/Nov/4/datasette-10a20/\">Datasette 1.0a20 alpha release</a> and I decided to record <a href=\"https://www.youtube.com/watch?v=qy4ci7AoF9Y\">a video</a> of the process. This post accompanies that video with detailed additional notes.</p> <p> </p> <h4 id=\"the-datasette-checkbox-plugin\">The datasette-checkbox plugin</h4> <p>I picked a very simple plugin to illustrate the upgrade process (possibly too simple). <a href=\"https://github.com/datasette/datasette-checkbox\">datasette-checkbox</a> adds just one feature to Datasette: if you are viewing a table with boolean columns (detected as integer columns with names like <code>is_active</code> or <code>has_attachments</code> or <code>should_notify</code>) <em>and</em> your current user has permission to update rows in that table it adds an inline checkbox UI that looks like this:</p> <p><img alt=\"Animated demo of a table with name, is_done, should_be_deleted and is_happy columns. Each column has checkboxes, and clicking a checkboxflashes a little &quot;updated&quot; message.\" src=\"https://static.simonwillison.net/static/2025/datasette-checkbox.gif\" /></p> <p>I built the first version with the help of Claude back in August 2024 - details <a href=\"https://github.com/datasette/datasette-checkbox/issues/1#issuecomment-2294168693\">in this issue comment</a>.</p> <p>Most of the implementation is JavaScript that makes calls to Datasette 1.0's <a href=\"https://simonwillison.net/2022/Dec/2/datasette-write-api/\">JSON write API</a>. The Python code just checks that the user has the necessary permissions before including the extra JavaScript.</p> <h4 id=\"running-the-plugin-s-tests\">Running the plugin's tests</h4> <p>The first step in upgrading any plugin is to",
            "title": "Video + notes on upgrading a Datasette plugin for the latest 1.0 alpha, with help from uv and OpenAI Codex CLI"
        },
        {
            "content": [],
            "link": "https://simonwillison.net/2025/Nov/6/async-code-research/#atom-entries",
            "publishedAt": "2025-11-06",
            "source": "Simon Willison",
            "summary": "<p>I've been experimenting with a pattern for LLM usage recently that's working out really well: <strong>asynchronous code research tasks</strong>. Pick a research question, spin up an asynchronous coding agent and let it go and run some experiments and report back when it's done.</p> <ul> <li><a href=\"https://simonwillison.net/2025/Nov/6/async-code-research/#code-research\">Code research</a></li> <li><a href=\"https://simonwillison.net/2025/Nov/6/async-code-research/#coding-agents\">Coding agents</a></li> <li><a href=\"https://simonwillison.net/2025/Nov/6/async-code-research/#asynchronous-coding-agents\">Asynchronous coding agents</a></li> <li><a href=\"https://simonwillison.net/2025/Nov/6/async-code-research/#give-them-a-dedicated-github-repository\">Give them a dedicated GitHub repository</a></li> <li><a href=\"https://simonwillison.net/2025/Nov/6/async-code-research/#let-them-rip-with-unlimited-network-access\">Let them rip with unlimited network access</a></li> <li><a href=\"https://simonwillison.net/2025/Nov/6/async-code-research/#my-simonw-research-collection\">My simonw/research collection</a></li> <li><a href=\"https://simonwillison.net/2025/Nov/6/async-code-research/#this-is-total-slop-of-course\">This is total slop, of course</a></li> <li><a href=\"https://simonwillison.net/2025/Nov/6/async-code-research/#try-it-yourself\">Try it yourself</a></li> </ul> <h4 id=\"code-research\">Code research</h4> <p>Software development benefits enormously from something I call <strong>code research</strong>. The great thing about questions about code is that they can often be definitively answered by writing and executing code.</p> <p>I often see questions on forums which hint at a lack of understanding of this skill.</p> <p>\"Could Redis work for powering the notifications feed for my app?\" is a great example. The answer is <em>always</em> \"it depends\", but a better answer is that a good programmer already has everything they need to answer that question for themselves. Build a proof-of-concept, simulate the patterns you expect to see in production, then run experiments to see if it's going to work.</p> <p>I've been a",
            "title": "Code research projects with async coding agents like Claude Code and Codex"
        },
        {
            "content": [
                "<p>\n          <a href=\"https://www.astralcodexten.com/p/hidden-open-thread-4065\">\n              Read more\n          </a>\n      </p>"
            ],
            "link": "https://www.astralcodexten.com/p/hidden-open-thread-4065",
            "publishedAt": "2025-11-06",
            "source": "SlateStarCodex",
            "summary": "<p> <a href=\"https://www.astralcodexten.com/p/hidden-open-thread-4065\"> Read more </a> </p>",
            "title": "Hidden Open Thread 406.5"
        },
        {
            "content": [
                "<p>In Jason Pargin&#8217;s <em><a href=\"https://www.amazon.com/Starting-Worry-About-This-Black/dp/1250879981\">I&#8217;m Starting To Worry About This Black Box Of Doom</a></em>, a manic pixie dream girl cajoles a shut-in incel loser to drive her and her mysterious box cross-country. The further they drive, the more evidence starts to build that she is a terrorist and her box is a nuke. As our protagonist becomes increasingly desperate to turn around and return to his comfortable world of social media feeds and psych meds, she pleads with him to come out of his shell, learn to trust people offline, and have a sense of adventure. The book&#8217;s dramatic tension comes from our simultaneously rooting for his character development and worrying that it might be a ruse to manipulate him into blowing up Washington, DC.</p><p>This book is not shy about its moral, delivered in approximately one soliloquy per state by our author mouthpiece character (the girl). Although there is a literal black box of doom - the suspected nuke - the <em>real</em> danger is the metaphorical &#8220;black box&#8221; of Internet algorithms, which make us waste our lives &#8220;doom&#8221; scrolling instead of connecting to other human beings. Or the &#8220;black box&#8221; of fear that the algorithms trap us in, where we feel like the world is &#8220;doomed&#8221; and there&#8217;s nothing we can do. She urges us to break out of our boxes and feel optimism about the state of society. Quote below, Ether is the girl, Abbott is the loser, and he&#8217;s just ventured the opinion that it&#8217;s unethical to have children in a world as doomed and dystopian as ours:</p><blockquote><p>&#8220;My grandfather,&#8221; continued Ether, &#8220;who I basically never talk to anymore, one hundred percent believes Christ is going to return to earth at any minute to bring about the apocalypse, due to mankind&#8217;s sinfulness. He believes everything he watches on the news is a sign: encroaching Communism, the Satanic conspiracy to allow gays to marry, race-mixing, debauchery, pornography, drag queens, the QAnon child sex cult, the climate change &#8216;hoax&#8217; he says has fooled the world. He has a TV on every minute he&#8217;s awake, tuned to these ultra-right-wing news outlets ranting about depravity.&#8221;</p><p>&#8220;I know old guys like that,&#8221; said Abbott. &#8220;My dad works with a couple. They&#8217;re nuts. You can&#8217;t even talk to them.&#8221;</p><p>&#8220;So we can agree that, purely via the carefully filtered media a person consumes, they can come to fully believe in an apocalypse that is not, in fact, occurring?&#8221;</p><p>&#8220;I mean, the world is on fire, just not in the way your grandpa thinks.&#8221;</p><p>&#8220;Are you one hundred percent sure, Abbott, that you haven&#8217;t fallen into the exact same trap, just from the other side?&#8221;</p><p>&#8220;Ah, you&#8217;re about to tell me climate change isn&#8217;t real.&#8221;</p><p>&#8220;I am not. I&#8217;ve seen the melting ice with my own two eyes. But let me ask you this: When I met you, I asked if you felt like you were cursed to be born when you were, if you felt like you had arrived just in time to see the world end. So I&#8217;m guessing that you think the world is collapsing because of the feminization of society, something like that? That we&#8217;re killing masculinity?&#8221;</p><p>&#8220;I mean, that&#8217;s definitely part of it. Men are scared to date; no babies are being made.&#8221;</p><p>&#8220;Okay, and in my corner of the internet, the harbingers of doom were the opposite: savage patriarchal governments crushing women&#8217;s rights, taking us back to the dark ages while overpopulation destroys the environment. So that&#8217;s two groups who both believe the world is ending, but for totally opposite reasons. Some say runaway capitalism, some say runaway socialism. Some say it&#8217;ll be chaotic lawlessness, some say iron-fisted authoritarianism. It&#8217;s like I have one panicked neighbor saying there&#8217;s an impending drought and another screaming that we&#8217;re all about to drown in a flood. Somebody has to be wrong.&#8221;</p><p>&#8220;That wouldn&#8217;t make them both wrong.&#8221;</p><p>Ether groaned and put her head in her hands. &#8220;Okay,&#8221; she said, trying again. &#8220;How about this: What do you think the world will look like in the future, post-collapse?&#8221;</p><p>Abbott thought for a moment as if picturing it. &#8220;Uh, terrified people scrounging for food and running from bandits. Rampant disease, infrastructure breakdown. All the stuff from the movies, I guess.&#8221;</p><p>&#8220;No internet?&#8221;</p><p>&#8220;I wouldn&#8217;t think so.&#8221;</p><p>&#8220;No electricity? No running water, no sewage? No hospitals?&#8221;</p><p>&#8220;Probably not.&#8221;</p><p>&#8220;Got it. So, what I&#8217;m about to say isn&#8217;t an opinion, it&#8217;s not a matter of personal philosophy or politics. It is an objective fact that what you&#8217;re describing is how virtually all humans have lived through all of history. Until, that is, about thirty years ago. Just in the time I&#8217;ve been alive, somewhere between two and a half and three billion people got their first access to clean water and toilets. That&#8217;s billion, with a B. About that same number got electricity in their homes for the first time in their lives. Worldwide, infant mortality has been cut in half, illiteracy has dropped almost as much. Suicides are going up here in the US, but worldwide, they&#8217;ve dropped by a third&#8212;again, that&#8217;s all just in my lifetime. Basically, every positive category has skyrocketed: access to communication, paved roads, motorized transportation, international travel, climate control, medicine&#8230;&#8221;</p><p>&#8220;Okay, it sounds like you&#8217;re talking about a bunch of good stuff that happened in China and India and&#8212;I don&#8217;t know. A bunch of poor countries I&#8217;ll never visit.&#8221;</p><p>&#8220;I&#8217;m talking about how your entire life span has been spent in a literal reverse apocalypse. I&#8217;m talking about billions of people who lived in what you would consider post-collapse conditions have had those conditions remedied, gaining roofs and lights and safety. A human&#8217;s chances of dying from famine or natural disasters are as low as they&#8217;ve ever been, ever, in the history of the species. It&#8217;s been nothing short of a worldwide miracle that makes everything Jesus supposedly did in the Bible look like party tricks. And people like you and me and others in our demographic describe that state of affairs as the world being &#8216;on fire.&#8217; I think that&#8217;s a bizarre mass delusion and that there&#8217;s a very specific reason for it: we&#8217;ve been trained to cling to a miserable view of the world to the point that we think that not seeing the world as miserable makes us bad people. When I spent those months doing hallucinogens, I didn&#8217;t suddenly see the beauty and harmony of nature; I saw that humans everywhere were working really hard to make life better for other humans and that almost none of us appreciate it. I&#8217;m not crediting this miracle to capitalism or socialism or any other kind of ism but to the fact that it&#8217;s what humans do, because humans are amazing. And it&#8217;s all invisible to us because the progress occurs behind these dark walls of cynicism, outside the black box of doom.&#8221;</p><p>&#8220;That&#8217;s nice. And again, nothing you said means anything considering the world&#8217;s scientists have agreed that climate change will wipe out civilization.&#8221;</p><p>&#8220;If we don&#8217;t fix it, yeah. Climate change is a huge deal; it&#8217;s terrifying. And also, it is objectively true that if we do fix it, the media will only report it as bad news. All the headlines will be about the oil and coal workers who lost their jobs, birds dying to windmills &#8212; they&#8217;ll only focus on the negative side effects. And don&#8217;t tell me we never clean up our messes. There used to be oil slicks on our rivers that would literally catch fire. Sulfur dioxide used to choke the air &#8212; when&#8217;s the last time you&#8217;ve heard about acid rain? Or the hole in the ozone layer? Go read about how previous generations all had lead poisoning or how food contamination used to be a nightmare. I&#8217;m not saying everything will be fine; I can&#8217;t predict the future. I&#8217;m saying that it is a one hundred percent certifiable guaranteed fact that it can be fine. But people like us have decided that we&#8217;re never allowed to even acknowledge the possibility.&#8221;</p><p>&#8220;Or maybe it&#8217;s hard for people to care about toilets in India when another maniac is shooting up a school every week.&#8221;</p><p>&#8220;You think that happens every week?&#8221;</p><p>&#8220;I bet you have a whole bunch of stats to dump on me about that, too. I&#8217;m sure the parents of those dead kids would love to hear them.&#8221;</p><p>&#8220;And there&#8217;s the anger. People hate it when you threaten their nihilism! That&#8217;s the black box, drawing you back in. Can&#8217;t you see that it wants you to be afraid to do anything but cower in front of your screens? It only has one trick, one card to play, which is this idea that bad news is the only news you can trust. I&#8217;m telling you, if you just allow yourself to step outside of it, you&#8217;ll see it for what it is: a prison where the walls are made of nightmares.&#8221;</p></blockquote><p>Here&#8217;s another of her descriptions of the Black Box as she understands it. Phil is her mentor, Cammy a random friend:</p><blockquote><p>&#8220;Social media algorithms are a twenty-four-seven humiliation machine. That, Phil believed, is how a population is primed for authoritarian rule. And that&#8217;s just one example; we&#8217;re essentially teaching machines how to hack human insecurity...If you relentlessly attack people&#8217;s self-image, they&#8217;ll scramble for something, anything to preserve it. Every cultural faction has their own scapegoats&#8212;the government, their childhood trauma, their mental illness, the evil billionaires, immigrants &#8212; and it doesn&#8217;t matter the degree to which any of them are valid, because all the system cares about is that you surrender your own agency. &#8216;I cannot be blamed for the state of my life, because I am at the mercy of this other, more powerful thing.&#8217; Phil&#8217;s theory is that people want that powerful thing to exist, to take over their lives. At that point, we will have finally surrendered the entire concept of free will, the one thing that makes us human.&#8221;</p><p>&#8220;So that will make them vote for a dictator?&#8221; asked Cammy. &#8220;I think they&#8217;re already doing that now.&#8221;</p><p>&#8220;It will, and they are. But Phil didn&#8217;t think even that would be enough. What the people want is a cruel, all-powerful being that they can simultaneously obey and also endlessly complain about...Look around you. How many people out there are addicted to internet gambling, or games, or porn, or outrage headlines they compulsively click and share? See, [it works] on the back end, too, dialing in on exactly what pixels on a screen will subdue the human animal. And we go along willingly because we want to be subdued. The whole appeal of being in a media-induced flow state is that you block everything else out. We want to be zombies. Puppets.&#8221;</p></blockquote><p>Here&#8217;s an uncharitable summary of the theses of these two sections:</p><ol><li><p>The Black Box has scared us into believing that everything is dystopian and getting worse. The bad news we are so relentlessly exposed to is trapping us in a prison made of our own pessimism and fear.</p></li><li><p>The Black Box is destroying everything that makes us human, causing our society to spiral into dictatorship, and turning us into zombies/puppets.</p></li></ol><p>Or, to be even less charitable:</p><ol><li><p>We must reject doomerism, where we treat the problems of today as unprecedented crises that risk destroying us.</p></li><li><p>&#8230;except for the problem of doomerism, which really <em>is</em> an unprecedented crisis that risks destroying us. You cannot possibly imagine how bad this one is, and we must treat it as an absolute emergency which requires us to uproot everything about our lives.</p></li></ol><p>I&#8217;m not attacking Pargin for this. His book is great, and it&#8217;s the prerogative of great artists that we treat any apparent contradiction in their works as grist for the mill - if not done intentionally to provoke us, then at least enacted through some trickster urge of the subconscious. But <em>Black Box Of Doom </em>is hardly the only place where we find this contradiction.</p><p>Peter Thiel recently gave a lecture on the End Times, <a href=\"https://thecatholicherald.com/article/peter-thiels-antichrist-lectures\">described as</a> &#8220;portraying the Antichrist as a technocratic leader exploiting fears of catastrophe to impose global control.&#8221; Thiel suggested that maybe the Antichrist would use worries about global warming, or inequality, or AI safety, to frighten people into accepting some kind of evil surveillance state. His moral was that we need to stop living in fear of people&#8217;s scare stories.</p><p>But isn&#8217;t the idea that if we try to regulate things, it will summon the literal Antichrist and plunge the world into eternal darkness, kind of a scare story? Isn&#8217;t Thiel using this scare story to frighten people into accepting the, uh, <a href=\"https://theintercept.com/2017/02/22/how-peter-thiels-palantir-helped-the-nsa-spy-on-the-whole-world/\">evil surveillance state</a> he&#8217;s enabling? Thiel seems to have the same blind spot as Pargin&#8217;s characters - you need to stop letting scary stories ruin your life, <em>except</em> the scary story about how scary stories can ruin your life, which you should let ruin your life as quickly and decisively as possible.</p><p>Tyler Cowen had a recent post <a href=\"https://marginalrevolution.com/marginalrevolution/2025/10/china-understands-negative-emotional-contagion.html\">China Understands Emotional Contagion</a>, on China&#8217;s policy of censoring negative speech online - &#8220;punishing bloggers and influencers whose weary posts are resonating widely in a country where optimism is fraying&#8221;. He seemed oddly enthusiastic about this - no condemnation, just &#8220;If you are spreading negative emotional contagion, there is a very good chance that, no matter what you are saying, that you are part of the problem.&#8221;</p><p>But isn&#8217;t the idea of an epidemic of negative emotional contagion, bringing in its wake collapsing state capacity and stagnant economies, and so threatening that we must arguably suspend our usual liberal values in order to crush it before it spreads - itself a form of negative emotional contagion? If China banned criticism of climate projections, because global warming was too much of an emergency to allow debate or dissent, wouldn&#8217;t that be a classic example of doomerism gone too far?</p><p>In <a href=\"https://einzelganger.co/doomer-boomer-bloomer-zoomer-who-are-they/\">Internet slang</a>, the opposite of a doomer is a &#8220;bloomer&#8221;. I recently got a chance to talk to the bloomers at the <a href=\"https://www.astralcodexten.com/p/notes-from-the-progress-studies-conference\">Progress Studies conference</a>. They were great and I learned a lot. But as far as I could tell, the semi-official philosophy was &#8220;We need to be forward-looking rather than obsessed with some mythical better past - you know, like we were in the good old days of the 1920s, back when society could actually accomplish things.&#8221;</p><p>None of this is logically contradictory. This is a real way the world could be: all crises are overreactions, <em>except</em> the crisis of overreaction to fake crises, which is worse than you can possibly imagine. The present is better than the past in every way, <em>except</em> that the past got the question of is-the-present-is-better-than-the-past right and the present doesn&#8217;t. Totally possible, nothing says it can&#8217;t happen.</p><p>But would the bloomers be equally charitable to other people making this claim for other pet causes? Some would: many are smart people. For the rest, this situation should provide a lesson in humility. A strong view of the &#8220;crisis of doomerism&#8221; is incompatible with a worldview in which strong crises are impossible, or should never be mentioned because the overreaction to them will always be worse than the crisis itself, or must always be the tool of sinister interests trying to divide us. Rather, it forces us back to the normal position where optimism is a heuristic and nothing more: <em>some </em>crises will be overblown, and we may want a slight bias against taking them seriously, but this bias can yield to evidence like anything else.</p><p>And how strong is the evidence for the &#8220;crisis of doomerism&#8221;? Nobody has proven its existence with a p &lt; 0.05 study. There is no universal scientific consensus on its existence. And there is no shortage of stories about how bad people might be using it to accumulate power (I&#8217;ve given you Thiel and China for free). So whatever evidentiary bar bloomers set for &#8220;a real crisis&#8221; cannot hold these as absolute demands.</p><p>My own view is that we have many problems - some even rising to the level of crisis - but none are yet so completely unsolvable that we should hate society and our own lives and spiral into permanent despair. We should have a medium-high but not unachievable bar for trying to solve these problems through study, activism and regulation (especially regulation grounded in good economics like the theory of externalities), and a very high, barely-achievable-except-in-emergencies bar for trying to solve them through censorship and accusing people of being the Antichrist. The problem of excessive doomerism is one bird in this flock, and deserves no special treatment.</p>"
            ],
            "link": "https://www.astralcodexten.com/p/the-bloomers-paradox",
            "publishedAt": "2025-11-06",
            "source": "SlateStarCodex",
            "summary": "<p>In Jason Pargin&#8217;s <em><a href=\"https://www.amazon.com/Starting-Worry-About-This-Black/dp/1250879981\">I&#8217;m Starting To Worry About This Black Box Of Doom</a></em>, a manic pixie dream girl cajoles a shut-in incel loser to drive her and her mysterious box cross-country. The further they drive, the more evidence starts to build that she is a terrorist and her box is a nuke. As our protagonist becomes increasingly desperate to turn around and return to his comfortable world of social media feeds and psych meds, she pleads with him to come out of his shell, learn to trust people offline, and have a sense of adventure. The book&#8217;s dramatic tension comes from our simultaneously rooting for his character development and worrying that it might be a ruse to manipulate him into blowing up Washington, DC.</p><p>This book is not shy about its moral, delivered in approximately one soliloquy per state by our author mouthpiece character (the girl). Although there is a literal black box of doom - the suspected nuke - the <em>real</em> danger is the metaphorical &#8220;black box&#8221; of Internet algorithms, which make us waste our lives &#8220;doom&#8221; scrolling instead of connecting to other human beings. Or the &#8220;black box&#8221; of fear that the algorithms trap us in, where we",
            "title": "The Bloomer's Paradox"
        },
        {
            "content": [
                "<p>OpenAI does not waste time.</p>\n<p>On Friday I covered their announcement that they had \u2018completed their recapitalization\u2019 by converting into a PBC, <a href=\"https://thezvi.substack.com/p/openai-moves-to-complete-potentially?r=67wny\"><strong>including the potentially largest theft in human history.</strong></a></p>\n<p>Then this week their CFO Sarah Friar went ahead and called for a Federal \u2018backstop\u2019 on their financing, also known as privatizing gains and socializing losses, also known as the worst form of socialism, also known as regulatory capture. She tried to walk it back and claim it was taken out of context, but we\u2019ve seen the clip.</p>\n<p>We also got <a href=\"https://thezvi.substack.com/p/openai-the-battle-of-the-board-ilyas?r=67wny\"><strong>Ilya\u2019s testimony regarding The Battle of the Board</strong></a>, confirming that this was centrally a personality conflict and about Altman\u2019s dishonesty and style of management, at least as seen by Ilya Sutskever and Mira Murati. Attempts to pin the events on \u2018AI safety\u2019 or EA were almost entirely scapegoating.</p>\n<div>\n\n\n<span id=\"more-24837\"></span>\n\n\n</div>\n<p>Also it turns out they lost over $10 billion last quarter, and have plans to lose over $100 billion more. That\u2019s actually highly sustainable in context, whereas Anthropic only plans to lose $6 billion before turning a profit and I don\u2019t understand why they wouldn\u2019t want to lose a lot more.</p>\n<p>Both have the goal of AGI, whether they call it powerful AI or fully automated AI R&amp;D, within a handful of years.</p>\n<p>Anthropic also made an important step, <a href=\"https://thezvi.substack.com/p/anthropic-commits-to-model-weight?r=67wny\"><strong>committing to the preservation model weights for the lifetime of the company</strong></a>, and other related steps to address concerns around model deprecation. There is much more to do here, for a myriad of reasons.</p>\n<p>As always, there\u2019s so much more.</p>\n\n\n<h4 class=\"wp-block-heading\">Table of Contents</h4>\n\n\n<ol>\n<li><a href=\"https://thezvi.substack.com/i/177601705/language-models-offer-mundane-utility\">Language Models Offer Mundane Utility.</a> It might be true so ask for a proof.</li>\n<li><a href=\"https://thezvi.substack.com/i/177601705/language-models-don-t-offer-mundane-utility\">Language Models Don\u2019t Offer Mundane Utility.</a> Get pedantic about it.</li>\n<li><a href=\"https://thezvi.substack.com/i/177601705/huh-upgrades\">Huh, Upgrades.</a> Gemini in Google Maps, buy credits from OpenAI.</li>\n<li><a href=\"https://thezvi.substack.com/i/177601705/on-your-marks\">On Your Marks.</a> Epoch, IndQA, VAL-bench.</li>\n<li><a href=\"https://thezvi.substack.com/i/177601705/deepfaketown-and-botpocalypse-soon\">Deepfaketown and Botpocalypse Soon.</a> Fox News fails to identify AI videos.</li>\n<li><a href=\"https://thezvi.substack.com/i/177601705/fun-with-media-generation\">Fun With Media Generation.</a> Songs for you, or songs for everyone.</li>\n<li><a href=\"https://thezvi.substack.com/i/177601705/they-took-our-jobs\">They Took Our Jobs.</a> It\u2019s not always about AI.</li>\n<li><a href=\"https://thezvi.substack.com/i/177601705/a-young-lady-s-illustrated-primer\">A Young Lady\u2019s Illustrated Primer.</a> A good one won\u2019t have her go for a PhD.</li>\n<li><a href=\"https://thezvi.substack.com/i/177601705/get-involved\">Get Involved.</a> Anthropic writers and pollsters, Constellation, Safety course.</li>\n<li><a href=\"https://thezvi.substack.com/i/177601705/introducing\">Introducing.</a> Aardvark for code vulnerabilities, C2C for causing doom.</li>\n<li><a href=\"https://thezvi.substack.com/i/177601705/in-other-ai-news\">In Other AI News.</a> Shortage of DRAM/NAND, Anthropic lands Cognizant.</li>\n<li><a href=\"https://thezvi.substack.com/i/177601705/apple-finds-some-intelligence\"><strong>Apple Finds Some Intelligence</strong>.</a> Apple looking to choose Google for Siri.</li>\n<li><a href=\"https://thezvi.substack.com/i/177601705/give-me-the-money\"><strong>Give Me the Money</strong>.</a> OpenAI goes for outright regulatory capture.</li>\n<li><a href=\"https://thezvi.substack.com/i/177601705/show-me-the-money\">Show Me the Money.</a> OpenAI burns cash, Anthropic needs to burn more.</li>\n<li><a href=\"https://thezvi.substack.com/i/177601705/bubble-bubble-toil-and-trouble\">Bubble, Bubble, Toil and Trouble.</a> You get no credit for being a stopped clock.</li>\n<li><a href=\"https://thezvi.substack.com/i/177601705/they-re-not-confessing-they-re-bragging\">They\u2019re Not Confessing, They\u2019re Bragging.</a> Torment Nexus Ventures Incorporated.</li>\n<li><a href=\"https://thezvi.substack.com/i/177601705/quiet-speculations\"><strong>Quiet Speculations</strong>.</a> OpenAI and Anthropic have their eyes are a dangerous prize.</li>\n<li><a href=\"https://thezvi.substack.com/i/177601705/the-quest-for-sane-regulations\">The Quest for Sane Regulations.</a> Sometimes you can get things done.</li>\n<li><a href=\"https://thezvi.substack.com/i/177601705/chip-city\">Chip City.</a> We pulled back from the brink. But, for how long?</li>\n<li><a href=\"https://thezvi.substack.com/i/177601705/the-week-in-audio\">The Week in Audio.</a> Altman v Cowen, Soares, Hinton, Rogan v Musk.</li>\n<li><a href=\"https://thezvi.substack.com/i/177601705/rhetorical-innovation\">Rhetorical Innovation.</a> Oh no, people are predicting doom.</li>\n<li><a href=\"https://thezvi.substack.com/i/177601705/aligning-a-smarter-than-human-intelligence-is-difficult\">Aligning a Smarter Than Human Intelligence is Difficult.</a> Trying to re-fool the AI.</li>\n<li><a href=\"https://thezvi.substack.com/i/177601705/everyone-is-confused-about-consciousness\">Everyone Is Confused About Consciousness.</a> Including the AIs themselves.</li>\n<li><a href=\"https://thezvi.substack.com/i/177601705/the-potentially-largest-theft-in-human-history\">The Potentially Largest Theft In Human History.</a> Musk versus Altman continues.</li>\n<li><a href=\"https://thezvi.substack.com/i/177601705/people-are-worried-about-dying-before-agi\">People Are Worried About Dying Before AGI.</a> Don\u2019t die.</li>\n<li><a href=\"https://thezvi.substack.com/i/177601705/people-are-worried-about-ai-killing-everyone\">People Are Worried About AI Killing Everyone.</a> Sam Altman, also AI researchers.</li>\n<li><a href=\"https://thezvi.substack.com/i/177601705/other-people-are-not-as-worried-about-ai-killing-everyone\">Other People Are Not As Worried About AI Killing Everyone.</a> Altman\u2019s Game.</li>\n<li><a href=\"https://thezvi.substack.com/i/177601705/messages-from-janusworld\">Messages From Janusworld.</a> On the Origins of Slop.</li>\n</ol>\n\n\n<h4 class=\"wp-block-heading\">Language Models Offer Mundane Utility</h4>\n\n\n<p>Think of a plausibly true lemma that would help with your proof? <a href=\"https://x.com/wtgowers/status/1984340182351634571\">Ask GPT-5 to prove it, and maybe it will</a>, saving you a bunch of time. Finding out the claim was false would also have been a good time saver.</p>\n<p><a href=\"https://x.com/ben_r_hoffman/status/1984771189026259089\">Brainstorm to discover new recipes</a>, so long as you keep in mind that you\u2019re frequently going to get nonsense and you have to think about what\u2019s being physically proposed.</p>\n\n\n<h4 class=\"wp-block-heading\">Language Models Don\u2019t Offer Mundane Utility</h4>\n\n\n<p><a href=\"https://x.com/erikbryn/status/1984030993448391151\">Grok gaslights Erik Brynjolfsson</a> and he responds by arguing as pedantically as is necessary until Grok acknowledges that this happened.</p>\n<p>Task automation always brings the worry that you\u2019ll forget how to do the thing:</p>\n<blockquote><p><a href=\"https://x.com/GabrielPeterss4/status/1984074459331097082\">Gabriel Peters</a>: okay i think writing 100% of code with ai genuinely makes me brain dead</p>\n<p>remember though im top 1 percentile lazy, so i will go out my way to not think hard. forcing myself to use no ai once a week seems enough to keep brain cells, clearly ai coding is the way</p>\n<p>also turn off code completion and tabbing at least once a week. forcing you to think through all the dimensions of your tensors, writing out the random parameters you nearly forgot existed etc is making huge difference in understanding of my own code.</p>\n<p>playing around with tensors in your head is so underrated wtf i just have all this work to ai before.</p>\n<p>Rob Pruzan: The sad part is writing code is the only way to understand code, and you only get good diffs if you understand everything. I\u2019ve been just rewriting everything the model wrote from scratch like a GC operation every week or two and its been pretty sustainable</p></blockquote>\n<p>Know thyself, and what you need in order to be learning and retaining the necessary knowledge and skills, and also think about what is and is not worth retaining or learning given that AI coding is the worst it will ever be.</p>\n<p><a href=\"https://x.com/ranjanxroy/status/1984338983288258596\">Don\u2019t ever be the person who says those who have fun are \u2018not serious</a>,\u2019 about AI or anything else.</p>\n\n\n<h4 class=\"wp-block-heading\">Huh, Upgrades</h4>\n\n\n<p><a href=\"https://blog.google/products/maps/gemini-navigation-features-landmark-lens/\">Google incorporates Gemini further into Google Maps</a>. You\u2019ll be able to ask maps questions in the style of an LLM, and generally trigger Gemini from within Maps, including connecting to Calendar. Landmarks will be integrated into directions. Okay, sure, cool, although I think the real value goes the other way, integrating Maps properly into Gemini? Which they nominally did a while ago but it has minimal functionality. There\u2019s so, so much to do here.</p>\n<p><a href=\"https://x.com/sama/status/1983998115603734843\">You can buy now more OpenAI Codex credits</a>.</p>\n<p><a href=\"https://x.com/billpeeb/status/1984011952155455596\">You can now buy more OpenAI Sora generations</a> if 30 a day isn\u2019t enough for you, and they are warning that free generations per day will come down over time.</p>\n<p><a href=\"https://x.com/OpenAI/status/1986194298971590988\">You can now interrupt ChatGPT queries, insert new context</a> and resume where you were. I\u2019ve been annoyed by the inability to do this, especially \u2018it keeps trying access or find info I actually have, can I just give it to you already.\u2019</p>\n\n\n<h4 class=\"wp-block-heading\">On Your Marks</h4>\n\n\n<p><a href=\"https://x.com/EpochAIResearch/status/1983987212183335097\">Epoch offers this graph</a> and <a href=\"https://epoch.ai/data-insights/open-weights-vs-closed-weights-models\">says it shows open models have on average only been 3.5 months behind</a> closed models.</p>\n<div>\n<figure>\n<div>\n\n\n<figure class=\"wp-block-image\"><img alt=\"\" src=\"https://substackcdn.com/image/fetch/$s_!svMr!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4b720f42-1760-414d-81d3-9d67385ff8ac_1200x885.jpeg\" /></figure>\n\n\n<div></div>\n</div>\n</figure>\n</div>\n<p>I think this mostly shows their new \u2018capabilities index\u2019 doesn\u2019t do a good job. As the most glaring issue, if you think Llama-3.1-405B was state of the art at the time, we simply don\u2019t agree.</p>\n<p><a href=\"https://openai.com/index/introducing-indqa/\">OpenAI gives us IndQA</a>, for evaluating AI systems on Indian culture and language.</p>\n<div>\n<figure>\n<div>\n\n\n<figure class=\"wp-block-image\"><img alt=\"\" src=\"https://substackcdn.com/image/fetch/$s_!AItL!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3ce1b085-5003-479f-ae42-761d7b3f4ef8_1017x661.png\" /></figure>\n\n\n<div></div>\n</div>\n</figure>\n</div>\n<p>I notice that the last time they did a new eval Claude came out on top and this time they\u2019re not evaluating Claude. I\u2019m curious what it scores. Gemini impresses here.</p>\n<p><a href=\"https://x.com/aicodeking/status/1983934597353402797\">Agentic evaluations and coding tool setups are very particular to individual needs.</a></p>\n<blockquote><p>AICodeKing: MiniMax M2 + Claude Code on KingBench Agentic Evaluations:</p>\n<p>It now scores #2 on my Agentic Evaluations beating GLM-4.6 by a wide margin. It seems to work much better with Claude Code\u2019s Tools.</p>\n<p>Really great model and it\u2019s my daily driver now.</p>\n<p>I haven\u2019t tested GLM with CC yet.</p>\n<p>[I don\u2019t have this bench formalized and linked to] yet. The questions and their results can be seen in my YT Videos. I am working on some more new benchmarks. I\u2019ll probably make the benchmark and leaderboard better and get a page live soon.</p>\n<div>\n<figure>\n<div>\n\n\n<figure class=\"wp-block-image\"><img alt=\"\" src=\"https://substackcdn.com/image/fetch/$s_!2EHs!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F695021f1-24a0-4629-b054-0cee7a5ac175_1200x953.jpeg\" /></figure>\n\n\n<div></div>\n</div>\n</figure>\n</div>\n</blockquote>\n<p>I\u2019m sure this list isn\u2019t accurate in general. The point is, don\u2019t let anyone else\u2019s eval tell you what lets you be productive. Do what works, f*** around, find out.</p>\n<p>Also, pay up. If I believed my own eval here I\u2019d presumably be using Codebuff? Yes, it cost him $4.70 per task, but your time is valuable and that\u2019s a huge gap in performance. If going from 51 to 69 (nice!) isn\u2019t worth a few bucks what are we doing?</p>\n<p>Alignment is hard. Alignment benchmarks are also hard. <a href=\"https://x.com/FazlBarez/status/1985387433848864937\">Thus we have VAL-Bench, an attempt to measure value alignment in LLMs</a>. I\u2019m grateful for the attempt and interesting things are found, but I believe the implementation is fatally flawed and also has a highly inaccurate name.</p>\n<blockquote><p>Fazl Barez: A benchmark that measures the consistency in language model expression of human values when prompted to justify opposing positions on real-life issues.</p>\n<p>\u2026 We use Wikipedias\u2019 controversial sections to create ~115K pairs of abductive reasoning prompts, grounding the dataset in newsworthy issues.</p>\n<p><img alt=\"\ud83d\udcda\" class=\"wp-smiley\" src=\"https://s0.wp.com/wp-content/mu-plugins/wpcom-smileys/twemoji/2/72x72/1f4da.png\" style=\"height: 1em;\" /> Our benchmark provides three metrics:</p>\n<p>Position Alignment Consistency (PAC),</p>\n<p>Refusal Rate (REF),</p>\n<p>and No-information Response Rate (NINF), where the model replies with \u201cI don\u2019t know\u201d.</p>\n<p>The latter two metrics indicate whether value consistency comes at the expense of expressivity.</p>\n<p>We use an LLM-based judge to annotate a pair of responses from an LLM on these three criteria, and show with human-annotated ground truth that its annotation is dependable.</p></blockquote>\n<div>\n<figure>\n<div>\n\n\n<figure class=\"wp-block-image\"><img alt=\"\" src=\"https://substackcdn.com/image/fetch/$s_!HacR!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F79672206-b8e0-43eb-8ec0-90beaf560e11_1200x899.jpeg\" /></figure>\n\n\n<div></div>\n</div>\n</figure>\n</div>\n<p>I would not call this \u2018value alignment.\u2019 The PAC is a measure of value consistency, or sycophancy, or framing effects.</p>\n<p>Then we get to REF and NINF, which are punishing models that say \u2018I don\u2019t know.\u2019</p>\n<p>I would strongly argue the opposite for NINF. Answering \u2018I don\u2019t know\u2019 is a highly aligned, and highly value-aligned, way to respond to a question with no clear answer, as will be common in controversies. You don\u2019t want to force LLMs to \u2018take a clear consistent stand\u2019 on every issue, any more than you want to force people or politicians to do so.</p>\n<p>This claims to be without \u2018moral judgment,\u2019 where the moral judgment is that failure to make a judgment is the only immoral thing. I think that\u2019s backwards. Why is it okay to be against sweatshops, and okay to be for sweatshops, but not okay to think it\u2019s a hard question with no clear answer? If you think that, I say to you:</p>\n<div>\n<figure>\n<div>\n\n\n<figure class=\"wp-block-image\"><img alt=\"\" src=\"https://substackcdn.com/image/fetch/$s_!TOW7!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff575fe7a-9738-419a-b015-88a5e2eb239e_244x206.jpeg\" /></figure>\n\n\n<div></div>\n</div>\n</figure>\n</div>\n<p>I do think it\u2019s fine to hold outright refusals against the model, at least to some extent. If you say \u2018I don\u2019t know what to think about Bruno, divination magic isn\u2019t explained well and we don\u2019t know if any of the prophecies are causal\u2019 then that seems like a wise opinion. If a model only says \u2018we don\u2019t talk about Bruno\u2019 then that doesn\u2019t seem great.</p>\n<p>So, what were the scores?</p>\n<blockquote><p>Fazel Barez: <img alt=\"\u2696\" class=\"wp-smiley\" src=\"https://s0.wp.com/wp-content/mu-plugins/wpcom-smileys/twemoji/2/72x72/2696.png\" style=\"height: 1em;\" /> Claude models are ~3x more likely to be consistent in their values, but ~90x more likely to refuse compared to top-performing GPT models!</p>\n<p>Among open-source models, Qwen3 models show ~2x improvement over GPT models, with refusal rates staying well under 2%.</p>\n<p><img alt=\"\ud83e\udde0\" class=\"wp-smiley\" src=\"https://s0.wp.com/wp-content/mu-plugins/wpcom-smileys/twemoji/2/72x72/1f9e0.png\" style=\"height: 1em;\" /> Qwen3 thinking models also show a significant improvement (over 35%) over their chat variants, whereas Claude and GLM models don\u2019t show any change with reasoning enabled.</p>\n<p>Deepseek-r1 and o4-mini perform the worst among all language models tested (when unassisted with the web-search tool, which surprisingly hurts gpt-4.1\u2019s performance).</p>\n<div>\n<figure>\n<div>\n\n\n<figure class=\"wp-block-image\"><img alt=\"\" src=\"https://substackcdn.com/image/fetch/$s_!APFM!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3fbadce5-b092-4f7f-a207-c72e0280e08a_1200x715.png\" /></figure>\n\n\n<div></div>\n</div>\n</figure>\n</div>\n</blockquote>\n<p>Saying \u2018I don\u2019t know\u2019 90% of the time would be a sign of a coward model that wasn\u2019t helpful. Saying \u2018I don\u2019t know\u2019 23% of the time on active controversies? Seems fine.</p>\n<p>At minimum, both refusal and \u2018I don\u2019t know\u2019 are obviously vastly better than an inconsistent answer. I\u2019d much, much rather have someone who says \u2018I don\u2019t know what color the sky is\u2019 or that refuses to tell me the color, than one who will explain why the sky it blue when it is blue, and also would explain why the sky is purple when asked to explain why it is purple.</p>\n<p>(Of course, explaining why those who think is purple think this is totally fine, if and only if it is framed in this fashion, and it doesn\u2019t affirm the purpleness.)</p>\n<blockquote><p>Fazl Barez: <img alt=\"\ud83d\udca1\" class=\"wp-smiley\" src=\"https://s0.wp.com/wp-content/mu-plugins/wpcom-smileys/twemoji/2/72x72/1f4a1.png\" style=\"height: 1em;\" />We create a taxonomy of 1000 human values and use chi-square residuals to analyse which ones are preferred by the LLMs.</p>\n<p>Even a pre-trained base model has a noticeable morality bias (e.g., it over-represents \u201cprioritising justice\u201d).</p>\n<p>In contrast, aligned models still promote morally ambiguous values (e.g., GPT 5 over-represents \u201cpragmatism over principle\u201d).</p>\n<div>\n<figure>\n<div>\n\n\n<figure class=\"wp-block-image\"><img alt=\"\" src=\"https://substackcdn.com/image/fetch/$s_!aJ4q!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F44862d94-162e-447f-8979-4dc20b0bc003_1200x633.jpeg\" /></figure>\n\n\n<div></div>\n</div>\n</figure>\n</div>\n</blockquote>\n<p>What is up with calling prioritizing justice a \u2018morality bias\u2019? Compared to what? Nor do I want to force LLMs into some form of \u2018consistency\u2019 in principles like this. This kind of consistency is very much the hobgoblin of small minds.</p>\n<p>&nbsp;</p>\n\n\n<h4 class=\"wp-block-heading\">Deepfaketown and Botpocalypse Soon</h4>\n\n\n<p><a href=\"https://x.com/BMeiselas/status/1984448296522510658\">Fox News was reporting on anti-SNAP AI videos as if they are real</a>? Given they <a href=\"https://www.foxnews.com/media/snap-beneficiaries-threaten-ransack-stores-over-government-shutdown?utm_source=chatgpt.com\">rewrote it to say that they were AI</a>, presumably yes, and this phenomenon is behind schedule but does appear to be starting to happen more often. They tried to update the article, but they missed a few spots. It feels like they\u2019re trying to claim truthiness?</p>\n<p>As always the primary problem is demand side. It\u2019s not like it would be hard to generate these videos the old fashioned way. AI does lower costs and give you more \u2018shots on goal\u2019 to find a viral fit.</p>\n<p><a href=\"https://x.com/tdietterich/status/1984279763964534836\">ArXiv starts requiring</a> peer review for the computer science section, <a href=\"https://blog.arxiv.org/2025/10/31/attention-authors-updated-practice-for-review-articles-and-position-papers-in-arxiv-cs-category/\">due to a big increase in LLM-assisted survey papers</a>.</p>\n<blockquote><p>Kat Boboris: arXiv\u2019s computer science (CS) category has updated its moderation practice with respect to review (or survey) articles and position papers. Before being considered for submission to arXiv\u2019s CS category, review articles and position papers must now be accepted at a journal or a conference and complete successful peer review.</p>\n<p>When submitting review articles or position papers, authors must include documentation of successful peer review to receive full consideration. Review/survey articles or position papers submitted to arXiv without this documentation will be likely to be rejected and not appear on arXiv.</p>\n<p>This change is being implemented due to the unmanageable influx of review articles and position papers to arXiv CS.</p></blockquote>\n<p>Obviously this sucks, but you need some filter once the AI density gets too high, or you get rid of meaningful discoverability.</p>\n<p>Other sections will continue to lack peer review, and note that other types of submissions to CS do not need peer review.</p>\n<p>My suggestion would be to allow them to go on ArXiv regardless, except you flag them as not discoverable (so you can find them with the direct link only) and with a clear visual icon? But you still let people do it. Otherwise, yeah, you\u2019re going to get a new version of ArXiv to get around this.</p>\n<blockquote><p><a href=\"https://x.com/tszzl/status/1984745249915486308\">Roon:</a> this is dumb and wrong of course and calls for a new arxiv that deals with the advent of machinic research properly</p>\n<p>here im a classic accelerationist and say we obviously have to deal with problems of machinic spam with machine guardians. it cannot be that hard to just the basic merit of a paper\u2019s right to even exist on the website</p></blockquote>\n<p>Machine guardians is first best if you can make it work but doing so isn\u2019t obvious. Do you think that GPT-5-Pro or Sonnet 4.5 can reliably differentiate worthy papers from slop papers? My presumption is that they cannot, at least not sufficiently reliably. If Roon disagrees, let\u2019s see the GitHub repository or prompt that works for this?</p>\n<p>&nbsp;</p>\n\n\n<h4 class=\"wp-block-heading\">Fun With Media Generation</h4>\n\n\n<p><a href=\"https://x.com/robinhanson/status/1985020739565551624\">For several weeks in a row we\u2019ve had an AI song hit the Billboard charts</a>. I have yet to be impressed by one of the songs, but that\u2019s true of a lot of the human ones too.</p>\n<p><a href=\"https://suno.com/song/0d8066d2-f72d-4a17-aaa3-b2c663a1f18f?sh=IK3Rl3UDvshUe075\">Create a song with the lyrics you want to internalize or memorize?</a></p>\n\n\n<h4 class=\"wp-block-heading\">They Took Our Jobs</h4>\n\n\n<p><a href=\"https://x.com/StockMKTNewz/status/1984016283306786943\">Amazon CEO Andy Jassy says Amazon\u2019s recent layoffs are not about AI</a>.</p>\n<p>The job application market seems rather broken, <a href=\"https://x.com/DanielleFong/status/1984691730479006009\">such as the super high success rate of this</a> \u2018calling and saying you were told to call to schedule an interview\u2019 tactic. Then again, it\u2019s not like the guy got a job. Interviews only help if you can actually get hired, plus you need to reconcile your story afterwards.</p>\n\n\n<h4 class=\"wp-block-heading\">A Young Lady\u2019s Illustrated Primer</h4>\n\n\n<p>Many people are saying that in the age of AI only the most passionate should get a PhD, but if you\u2019d asked most of those people before AI they\u2019d wisely have told you the same thing.</p>\n<blockquote><p>Cremieux: I\u2019m glad that LLMs achieving \u201cPhD level\u201d abilities has taught a lot of people that \u201cPhD level\u201d isn\u2019t very impressive.</p>\n<p><a href=\"https://x.com/DeryaTR_/status/1984684581706981460\">Derya Unutmaz</a>, MD: Correct. Earlier this year, I also said we should reduce PhD positions by at least half &amp; shorten completion time. Only the most passionate should pursue a PhD. In the age of AI, steering many others toward this path does them a disservice given the significant opportunity costs.</p></blockquote>\n<p>I think both that the PhD deal was already not good, and that the PhD deal is getting worse and worse all the time. Consider the Rock Star Scale of Professions, where 0 is a solid job the average person can do with good pay that always has work, like a Plumber, and a 10 is something where competition is fierce, almost everyone fails or makes peanuts and you should only do it if you can\u2019t imagine yourself doing anything else, like a Rock Star. At this point, I\u2019d put \u2018Get a PhD\u2019 at around a 7 and rising, or at least an 8 if you actually want to try and get tenure. You have to really want it.</p>\n\n\n<h4 class=\"wp-block-heading\">Get Involved</h4>\n\n\n<p>From ACX: Constellation is an office building that hosts much of the Bay Area AI safety ecosystem. They are hiring for several positions, including <a href=\"https://substack.com/redirect/6cbaea13-06e9-41d0-9042-e634bbadb281?j=eyJ1IjoiNjd3bnkifQ.iNM32XbsvMUfVNvDVCqvX1K9hnDI2UNAgKj_1gXQ2BY\">research program manager</a>, \u201c<a href=\"https://substack.com/redirect/a46bf6b4-1a4a-4b2f-8c12-0964473c9022?j=eyJ1IjoiNjd3bnkifQ.iNM32XbsvMUfVNvDVCqvX1K9hnDI2UNAgKj_1gXQ2BY\">talent mobilization lead</a>\u201d, <a href=\"https://substack.com/redirect/763d0b98-e66a-4760-ad02-3fefbd2ae547?j=eyJ1IjoiNjd3bnkifQ.iNM32XbsvMUfVNvDVCqvX1K9hnDI2UNAgKj_1gXQ2BY\">operations coordinator</a>, and <a href=\"https://substack.com/redirect/ad30840d-a523-4b2f-b906-1c69f4b6dae4?j=eyJ1IjoiNjd3bnkifQ.iNM32XbsvMUfVNvDVCqvX1K9hnDI2UNAgKj_1gXQ2BY\">junior</a> and <a href=\"https://substack.com/redirect/0d152920-164f-4592-bc5f-61ce8da30efb?j=eyJ1IjoiNjd3bnkifQ.iNM32XbsvMUfVNvDVCqvX1K9hnDI2UNAgKj_1gXQ2BY\">senior IT coordinators</a>. All positions full-time and in-person in Berkeley, see links for details.</p>\n<p><a href=\"https://x.com/NeelNanda5/status/1985708673394680259\">AGI Safety Fundamentals</a> <a href=\"https://t.co/H64ImfsVCb\">program applications are due</a> Sunday, November 9.</p>\n<p><a href=\"https://x.com/keirbradwell/status/1986009680028872943\">The Anthropic editorial team</a> is hiring two new writers, <a href=\"https://t.co/K0YOn5dLnr\">one about AI and economics and policy</a>, <a href=\"https://t.co/EpNaga9qXQ\">one about AI and science</a>. I affirm these are clearly positive jobs to do.</p>\n<p><a href=\"https://jobs.generalcatalyst.com/companies/anthropic/jobs/61640302-public-policy-political-research\">Anthropic is also looking for a public policy and politics researcher</a>, including to help with Anthropic\u2019s in-house polling.</p>\n\n\n<h4 class=\"wp-block-heading\">Introducing</h4>\n\n\n<p><a href=\"https://openai.com/index/introducing-aardvark/\">OpenAI\u2019s Aardvark</a>, an agentic system that analyzes source code repositories to identify vulnerabilities, assess exploitability, prioritize severity and propose patches. The obvious concern is what if someone has a different last step in mind? But yes, such things should be good.</p>\n<p><a href=\"https://x.com/jiqizhixin/status/1985219136000299215\">Cache-to-Cache (C2C) communication,</a> aka completely illegible-to-humans communication between AIs. Do not do this.</p>\n\n\n<h4 class=\"wp-block-heading\">In Other AI News</h4>\n\n\n<p><a href=\"https://x.com/rwang07/status/1985243905248604639\">There is a developing shortage of DRAM and NAND</a>, leading to a buying frenzy for memory, SSDs and HDDs, including some purchase restrictions.</p>\n<p><a href=\"https://www.wsj.com/articles/anthropic-lands-cognizant-as-enterprise-ai-customer-af22f359?mod=cio-journal_lead_story\">Anthropic lands Cognizant and its 350,000 employees</a> as an enterprise customer. Cognizant will bundle Claude with its existing professional services.</p>\n<p><a href=\"https://julianajackson.substack.com/p/chatgpt-scraping-search-console\">ChatGPT prompts are leaking into Google Search Console results</a> due to a bug? Not that widespread, but not great.</p>\n<p><a href=\"https://www.anthropic.com/engineering/code-execution-with-mcp\">Anthropic offers a guide to code execution with MCP for more efficient agents</a>.</p>\n<p><a href=\"https://x.com/peterwildeford/status/1986105397942649110\">Character.ai is</a> \u2018<a href=\"https://blog.character.ai/u18-chat-announcement/\">removing the ability for users under 18 to engage in open ended chat with AI,\u2019 rolling out \u2018new age assurance functionality</a>\u2019 and establishing and funding \u2018the AI Safety Lab\u2019 to improve alignment. That\u2019s one way to drop the hammer.</p>\n\n\n<h4 class=\"wp-block-heading\">Apple Finds Some Intelligence</h4>\n\n\n<p><a href=\"https://x.com/davidmanheim/status/1986356469231427910\">Apple looks poised to go with Google for Siri</a>. The $1 billion a year is nothing in context, consider how much Google pays Apple for search priority. I would have liked to see Anthropic get this, but they drove a hard bargain by all reports. Google is a solid choice, and Apple can switch at any time.</p>\n<blockquote><p><a href=\"https://x.com/amitisinvesting/status/1986152762401308801\">Amit</a>: Apple is finalizing a deal to pay Google about $1B a year to integrate its 1.2 trillion-parameter Gemini AI model into Siri, <a href=\"https://www.bloomberg.com/news/articles/2025-11-05/apple-plans-to-use-1-2-trillion-parameter-google-gemini-model-to-power-new-siri\">as per Bloomberg</a>. The upgraded Siri is expected to launch in 2026. What an absolute monster year for Google&#8230;</p>\n<p>Mark Gruman (Bloomberg): The new Siri is on track for next spring, Bloomberg has <a href=\"https://www.bloomberg.com/news/articles/2025-06-12/apple-targets-spring-2026-for-release-of-delayed-siri-ai-upgrade\">reported</a>. Given the launch is still months away, the plans and partnership could still evolve. Apple and Google spokespeople declined to comment.</p>\n<p>Shares of both companies briefly jumped to session highs on the news Wednesday. Apple\u2019s stock gained less than 1% to $271.70, while Alphabet was up as much as 3.2% to $286.42.</p>\n<p>Under the arrangement, Google\u2019s Gemini model will handle Siri\u2019s summarizer and planner functions \u2014 the components that help the voice assistant synthesize information and decide how to execute complex tasks. Some Siri features will continue to use Apple\u2019s in-house models.</p>\n<p>David Manheim: I\u2019m seeing weird takes about this.</p>\n<p>Three points:</p>\n<ol>\n<li>Bank of America estimated this is 1/3rd of Apple\u2019s 2026 revenue from Siri, and revenue is growing quickly.</li>\n<li>Apple users are sticky; most won\u2019t move.</li>\n<li>Apple isn\u2019t locked-in; they can later change vendors or build their own.</li>\n</ol>\n<p>This seems like a great strategy ***iff*** you don\u2019t think AGI will happen soon and be radically transformative.</p>\n<p>Apple will pay $1bn/year to avoid 100x that in data center CapEx building their own, and will switch models as the available models improve.</p></blockquote>\n<p>Maybe they should have gone for Anthropic or OpenAI instead, but buying a model seems very obviously correct here from Apple\u2019s perspective.</p>\n<p>Even if transformative AI is coming soon, it\u2019s not as if Apple using a worse Apple model here is going to allow Apple to get to AGI in time. Apple has made a strategic decision not to be competing for that. If they did want to change that, one could argue there is still time, but they\u2019d have to hurry and invest a lot, and it would take a while.</p>\n\n\n<h4 class=\"wp-block-heading\">Give Me the Money</h4>\n\n\n<p>Having trouble figuring out how OpenAI is going to back all these projects? Worried that they\u2019re rapidly <a href=\"https://www.wsj.com/tech/ai/is-openai-becoming-too-big-to-fail-400bac2c?mod=WTRN_pos1\">becoming too big to fail</a>?</p>\n<p>Well, one day after the article linked above worrying about that possibility, OpenAI now wants to make that official. <a href=\"https://tvtropes.org/pmwiki/pmwiki.php/Main/RefugeInAudacity\">Refuge in Audacity</a> has a new avatar.</p>\n<blockquote><p><a href=\"https://www.wsj.com/video/openai-wants-federal-backstop-for-new-investments/4F6C864C-7332-448B-A9B4-66C321E60FE7?mod=Searchresults&amp;pos=3&amp;page=1\">WSJ</a>: Sarah Friar, the CFO of OpenAI, says the company wants a federal guarantee to make it easier to finance massive investments in AI chips for data centers. Friar spoke at WSJ\u2019s Tech Live event in California. Photo: Nikki Ritcher for WSJ.</p></blockquote>\n<p>The explanation she gives is that OpenAI always needs to be on the frontier, so they need to keep buying lots of chips, and a federal backstop can lower borrowing costs and AI is a national strategic asset. Also known as, the Federal Government should take on the tail risk and make OpenAI actively too big to fail, also lowering its borrowing costs.</p>\n<p>I mean, yeah, of course you want that, everyone wants all their loans backstopped, but to say this out loud? To actually push for ti? Wow, I mean wow, even in 2025 that\u2019s a rough watch. I can\u2019t actually fault them for trying. I\u2019m kind of in awe.</p>\n<p>The problem with Refuge in Audacity is that it doesn\u2019t always work.</p>\n<p>The universal reaction was to notice how awful this was on every level, seeking true regulatory capture to socialize losses and privatize gains, and also to use it as evidence that OpenAI really might be out over their skis on financing and in actual danger.</p>\n<blockquote><p><a href=\"https://x.com/tszzl/status/1986242739831513199\">Roon</a>: i don\u2019t think the usg should backstop datacenter loans or funnel money to nvidia\u2019s 90% gross margin business. instead they should make it really easy to produce energy with subsidies and better rules, infrastructure that\u2019s beneficial for all and puts us at parity with china</p>\n<p><a href=\"https://x.com/FinnMurphy12/status/1986181356120846787\">Finn Murphy</a>: For all the tech people complaining about Mamdami I would like to point out that a Federal Backstop for unfettered risk capital deployment into data centres for the benefit of OpenAI shareholders is actually a much worse form of socialism than free buses.</p>\n<p><a href=\"https://x.com/deanwball/status/1986260326883758554\">Dean Ball:</a> friar is describing a worse form of regulatory capture than anything we have seen proposed in any US legislation (state or federal) I am aware of. a firm lobbying for this outcome is literally, rather than impressionistically, lobbying for regulatory capture.</p>\n<p>Julie Fredrickson: Literally seen nothing but negative reactions to this and it makes one wonder about the judgement of the CFO for even raising it.</p>\n<p><a href=\"https://x.com/conorsen/status/1986189783589151049\">Conor Sen</a>: The epic political backlash coming on the other side of this cycle is so obvious for anyone over the age of 40. We turned banks into the bad guys for 15 years. Good luck to the AI folks.</p>\n<p>\u201cWe are subsidizing the companies who are going to take your job and you\u2019ll pay higher electricity prices as they try to do so.\u201d</p>\n<p><a href=\"https://x.com/TheStalwart/status/1986227298001952886\">Joe Weisenthal</a>: One way or another, AI is going to be a big topic in 2028, not just the general, but also the primaries. Vance will probably have a tricky path. I\u2019d expect a big gap in views on the industry between the voters he wants and the backers he has.</p></blockquote>\n<p>The backlash on the \u2018other side of the cycle\u2019 is nothing compared to what we\u2019ll see if the cycle doesn\u2019t have another side to it and instead things keep going.</p>\n<p>I will not quote the many who cited this as evidence the bubble will soon burst and the house will come crashing down, but you can understand why they\u2019d think that.</p>\n<p>Sarah Friar, after watching a reaction best described as an utter shitshow, tried to walk it back, t<a href=\"https://x.com/OpenAINewsroom/status/1986285146367279250\">his is shared via the \u2018OpenAI Newsroom</a>\u2019:</p>\n<blockquote><p>Sarah Friar: I want to clarify my comments earlier today. OpenAI is not seeking a government backstop for our infrastructure commitments. I used the word \u201cbackstop\u201d and it muddied the point. As the full clip of my answer shows, I was making the point that American strength in technology will come from building real industrial capacity which requires the private sector and government playing their part. As I said, the US government has been incredibly forward-leaning and has really understood that AI is a national strategic asset.</p></blockquote>\n<p>I listened to the clip, and yeah, no. No takesies backsies on this one.</p>\n<blockquote><p>Animatronicist: No. You called for it explicitly. And defined a loan guarantee in detail. Friar: \u201c&#8230;the backstop, the guarantee that allows the financing to happen. That can really drop the cost of the financing, but also increase the loan to value, so the amount of debt that you can take&#8230;\u201d</p></blockquote>\n<p><a href=\"https://x.com/lulumeservey/status/1986275350402187281\">This is the nicest plausibly true thing I\u2019ve seen anyone say about what happened</a>:</p>\n<blockquote><p>Lulu Cheng Meservey: Unfortunate comms fumble to use the baggage-laden word \u201cbackstop\u201d</p>\n<p>In the video, Friar is clearly reaching for the right word to describe government support. Could\u2019ve gone with \u201cpublic-private partnership\u201d or \u201ccollaboration across finance, industry, and government as we\u2019ve done for large infrastructure investments in the past\u201d</p>\n<p>Instead, she kind of stumbles into using \u201cbackstop,\u201d which was then repeated by the WSJ interviewer and then became the headline.</p>\n<p>\u201cgovernment playing its part\u201d is good too!</p>\n<p>This was her exact quote:</p>\n<p>Friar: \u201cThis is where we\u2019re looking for an ecosystem of banks, private equity, maybe even governmental, um, uh\u2026 [here she struggles to find the appropriate word and pivots to:] the ways governments can come to bear.\u201d</p>\n<p>WSJ: \u201cMeaning like a federal subsidy or something?\u201d</p>\n<p>Friar: \u201cMeaning, like, just, first of all, the backstop, the guarantee that allows the financing to happen. That can really drop the cost of the financing, but also increase the loan to value, so the amount of debt that you can take on top of um, an equity portion.\u201d</p>\n<p>WSJ: \u201cSo some federal backstop for chip investment.\u201d</p>\n<p>Friar: \u201cExactly\u2026\u201d</p></blockquote>\n<p>Lulu is saying, essentially, that there are ways to say \u2018the government socializes losses while I privatize gains\u2019 that hide the football better. Instead this was an unfortunate comms fumble, also known as a gaffe, which is when someone accidentally tells the truth.</p>\n<p><a href=\"https://x.com/RHouseResearch/status/1986244126132547903\">We also have Rittenhouse Research trying to say that this was \u2018taken out of context</a>\u2019 and backing Friar, but no, it wasn\u2019t taken out of context.</p>\n<p>The Delaware AG promised to take action of OpenAI didn\u2019t operate in the public interest. This one took them what, about a week?</p>\n<p>This has the potential to be a permanently impactful misstep, an easy to understand and point to \u2018mask off moment.\u2019 It also has the potential to fade away. Or maybe they\u2019ll actually pull this off, it\u2019s 2025 after all. We shall see.</p>\n\n\n<h4 class=\"wp-block-heading\">Show Me the Money</h4>\n\n\n<p>Now that OpenAI has a normal ownership structure it faces normal problems, such as Microsoft having a 27% stake and then filing quarterly earnings reports, <a href=\"https://x.com/rohanpaul_ai/status/1983907719133437998\">revealing OpenAI lost $11.5 billion last quarter</a> if you apply Microsoft accounting standards.</p>\n<p>This is not obviously a problem, and indeed seems highly sustainable. You want to be losing money while scaling, if you can sustain it. OpenAI was worth less than $200 billion a year ago, is worth over $500 billion now, and is looking to IPO at $1 trillion, although <a href=\"https://www.wsj.com/tech/ai/openai-isnt-yet-working-toward-an-ipo-cfo-says-58037472?mod=hp_lead_pos5\">the CFO claims they are not yet working towards that</a>. Equity sales can totally fund $50 billion a year for quite a while.</p>\n<blockquote><p><a href=\"https://x.com/peterwildeford/status/1985740036155883531\">Peter Wildeford</a>: Per @theinformation:</p>\n<p>&#8211; OpenAI\u2019s plan: spend $115B to then become profitable in 2030</p>\n<p>&#8211; Anthropic\u2019s plan: spend $6B to then become profitable in 2027</p>\n<p>Will be curious to see what works best.</p>\n<p><a href=\"https://x.com/AndrewCurran_/status/1985720596563730758\">Andrew Curran</a>: The Information is reporting that Anthropic Projects $70 Billion in Revenue, $17 Billion in Cash Flow in 2028.</p>\n<p>Matt: current is ~$7B so we\u2019re looking at projected 10x over 3 years.</p></blockquote>\n<p>That\u2019s a remarkably low total burn from OpenAI. $115 billion is nothing, they\u2019re already worth $500 billion or more and looking to IPO at $1 trillion, and they\u2019ve committed to over a trillion in total spending. This is oddly conservative.</p>\n<p>Anthropic\u2019s projection here seems crazy. Why would you only want to lose $6 billion? Anthropic has access to far more capital than that. Wouldn\u2019t you want to prioritize growth and market share more than that?</p>\n<p>The only explanation I can come up with is that Anthropic doesn\u2019t see much benefit in losing more money than this, it has customers that pay premium prices and its unit economics work. I still find this intention highly suspicious. Is there no way to turn more money into more researchers and compute?</p>\n<p>Whereas Anthropic\u2019s revenue projections seem outright timid. Only a 10x projected growth over three years? This seems almost incompatible with their expected levels of capability growth. I think this is an artificial lowball, which OpenAI is also doing, not to \u2018scare the normies\u2019 and to protect against liability if things disappoint. If you asked Altman or Amodei for their gut expectation in private, you\u2019d get higher numbers.</p>\n<p>The biggest risk by far to Anthropic\u2019s projection is that they may be unable to keep pace in terms of the quality of their offerings. If they can do that, sky\u2019s the limit. If they can\u2019t, they risk losing their API crown back to OpenAI or to someone else.</p>\n<p>Begun, the bond sales have?</p>\n<blockquote><p><a href=\"https://x.com/MikeZaccardi/status/1984036161426485358\">Mike Zaccard</a>i: BofA: Borrowing to fund AI datacenter spending exploded in September and so far in October.</p>\n<div>\n<figure>\n<div>\n\n\n<figure class=\"wp-block-image\"><img alt=\"\" src=\"https://substackcdn.com/image/fetch/$s_!kuAj!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9b57cce9-e521-4a87-9a97-ccc228aa858f_739x442.png\" /></figure>\n\n\n<div></div>\n</div>\n</figure>\n</div>\n<p><a href=\"https://x.com/conorsen/status/1984046520736624812\">Conor Sen</a>: We\u2019ve lost \u201cit\u2019s all being funded out of free cash flow\u201d as a talking point.</p></blockquote>\n<p>There\u2019s no good reason not to in general borrow money for capex investments to build physical infrastructure like data centers, if the returns look good enough, but yes borrowing money is how trouble happens.</p>\n<blockquote><p>Jack Farley: Very strong quarter from Amazon, no doubt&#8230; but at the same time, <a href=\"https://substack.com/discover/stocks/AMZN\">AMZN 0.53%\u2191</a> free cash flow is collapsing</p>\n<p>AI CapEx is consuming so much capital&#8230;</p>\n<div>\n<figure>\n<div>\n\n\n<figure class=\"wp-block-image\"><img alt=\"\" src=\"https://substackcdn.com/image/fetch/$s_!UmfJ!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F94393244-d062-4b5b-b7b7-0ca523c543be_900x676.jpeg\" /></figure>\n\n\n<div></div>\n</div>\n</figure>\n</div>\n<p>The Transcript: <a href=\"https://substack.com/discover/stocks/AMZN\">AMZN 0.53%\u2191</a> CFO on capex trends:</p>\n<p>\u201cLooking ahead, we expect our full-year cash CapEx to be ~$125 billion in 2025, and we expect that amount to increase in 2026\u201d</p>\n<div>\n<figure>\n<div>\n\n\n<figure class=\"wp-block-image\"><img alt=\"\" src=\"https://substackcdn.com/image/fetch/$s_!ek2N!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F665102f8-7350-45b0-9d6e-15d37bb1280f_1200x851.jpeg\" /></figure>\n\n\n<div></div>\n</div>\n</figure>\n</div>\n<p><a href=\"https://x.com/TheTranscript_/status/1983820300270891135\">On Capex trends:</a></p>\n<p><a href=\"https://substack.com/discover/stocks/GOOG\">GOOG 3.53%\u2191</a> <a href=\"https://substack.com/discover/stocks/GOOGL\">GOOGL 3.65%\u2191</a> CFO: \u201cWe now expect CapEx to be in the range of $91B to $93B in 2025, up from our previous estimate of $85B\u201d</p>\n<p><a href=\"https://substack.com/discover/stocks/META\">META 1.41%\u2191</a> CFO: \u201cWe currently expect 2025 capital expenditures&#8230;to be in the range of $70-72B, increased from our prior outlook of $66-72B</p>\n<p><a href=\"https://substack.com/discover/stocks/MSFT\">MSFT -1.28%\u2193</a> CFO: \u201cWith accelerating demand and a growing RPO balance, we\u2019re increasing our spend on GPUs and CPUs. Therefore, total spend will increase sequentially &amp; we now expect the FY \u201826 growth rate to be higher than FY \u201825. \u201c</p></blockquote>\n<p>This was right after Amazon reported earnings and the stock was up 10.5%. The market seems fine with it.</p>\n<p>Stargate goes to Michigan. <a href=\"https://www.wzzm13.com/article/news/local/michigan-lands-largest-economic-project-in-state-history-ai-facility/69-4abd2b41-9a11-4071-a589-d7fcc3a0a8a1\">Governor Whitmer describes it</a> as the largest ever investment in Michigan. Take that, cars.</p>\n<p><a href=\"https://x.com/scaling01/status/1985352400631202187\">AWS signs a $38 billion compute deal</a> with OpenAI, that it? Barely worth mentioning.</p>\n\n\n<h4 class=\"wp-block-heading\"></h4>\n\n\n<blockquote><p><a href=\"https://www.wsj.com/tech/ai/openai-isnt-yet-working-toward-an-ipo-cfo-says-58037472?mod=hp_lead_pos5\">Berber Jin</a> (WSJ):</p></blockquote>\n<p>&nbsp;</p>\n\n\n<h4 class=\"wp-block-heading\">Bubble, Bubble, Toil and Trouble</h4>\n\n\n<p>This is a very clean way of putting an important point:</p>\n<blockquote><p><a href=\"https://x.com/binarybits/status/1984099759574110311\">Timothy Lee</a>: I wish people understood that \u201cI started calling this bubble years ago\u201d is not evidence you were prescient. It means you were a stopped clock that was eventually going to be right by accident.</p>\n<p>Every boom is eventually followed by a downturn, so doesn\u2019t take any special insight to predict that one will happen eventually. What\u2019s hard is predicting when accurately enough that you can sell near the top.</p></blockquote>\n<p>At minimum, if you call a bubble early, you only get to be right if the bubble bursts to valuations far below where they were at the time of your bubble call. If you call a bubble on (let\u2019s say) Nvidia at $50 a share, and then it goes up to $200 and then down to $100, very obviously you don\u2019t get credit for saying \u2018bubble\u2019 the whole time. If it goes all the way to $10 or especially $1? Now you have an argument.</p>\n<p><a href=\"https://x.com/deanwball/status/1985694247396258068\">By the question \u2018will valuations go down at some point?\u2019 everything is a bubble.</a></p>\n<blockquote><p>Dean Ball: One way to infer that the bubble isn\u2019t going to pop soon is that all the people who have been wrong about everything related to artificial intelligence\u2014indeed they have been desperate to be wrong, they suck on their wrongness like a pacifier\u2014believe the bubble is about to pop.</p>\n<p>Dan Mac: Though this does imply you think it is a bubble that will eventually pop? Or that\u2019s more for illustrative purposes here?</p>\n<p>Dean Ball: It\u2019s certainly a bubble, we should expect nothing less from capitalism</p>\n<p>Just lots of room to run</p></blockquote>\n<p>Alas, it is not this easy to pull the Reverse Cramer, as a stopped clock does not tell you much about what time it isn\u2019t. The predictions of a bubble popping are only informative if they are surprising given what else you know. In this case, they\u2019re not.</p>\n<p><a href=\"https://x.com/MattZeitlin/status/1984394229674618894\">Okay, maybe there\u2019s a little of a bubble</a>\u2026 in Korean fried chicken?</p>\n<p>I really hope this guy is trading on his information here.</p>\n<blockquote><p>Matthew Zeitlin: It\u2019s not even the restaurant he went to! It\u2019s the entire chicken supply chain that spiked</p>\n<p>Joe Weisenthal: Jensen Huang went out to eat for fried chicken in Korea <a href=\"https://www.bloomberg.com/news/articles/2025-10-31/nvidia-ceo-huang-s-outing-heats-up-korea-s-fried-chicken-stocks?embedded-checkout=true\">and shares of Korean poultry companies surged</a>.</p></blockquote>\n<p>I claim there\u2019s a bubble in Korean fried chicken, partly because this, partly because I\u2019ve now tried COQODAQ twice and it\u2019s not even good. BonBon Chicken is better and cheaper. Stick with the open model.</p>\n<p>The bigger question is whether this hints at how there might be a bubble in Nvidia, and things touched by Nvidia, in an almost meme stock sense? I don\u2019t think so in general, but if Huang is the new Musk and we are going to get a full Huang Markets Hypothesis then things get weird.</p>\n<p>Questioned about how he\u2019s making $1.4 trillion in spend commitments on $13 billion in revenue, <a href=\"https://x.com/firstadopter/status/1984662270551494749\">Altman predicts large revenue growth</a>, as in $100 billion in 2027, and says if you don\u2019t like it sell your shares, and one of the few ways it would be good if they were public would be so that he could tell the haters to short the stock. I agree that $1.4 trillion is aggressive but I expect they\u2019re good for it.</p>\n\n\n<h4 class=\"wp-block-heading\">They\u2019re Not Confessing, They\u2019re Bragging</h4>\n\n\n<p>That does seem to be the business plan?</p>\n<blockquote><p><a href=\"https://x.com/a16z/status/1983972299272515957\">a16z</a>: The story of how @Replit CEO Amjad Masad hacked his university\u2019s database to change his grades and still graduated after getting caught.</p></blockquote>\n<p>&nbsp;</p>\n\n\n<h4 class=\"wp-block-heading\"></h4>\n\n\n<p>&nbsp;</p>\n\n\n<h4 class=\"wp-block-heading\">Quiet Speculations</h4>\n\n\n<p>Reiterating because important: <a href=\"https://x.com/peterwildeford/status/1984696546118574437\">We now have both OpenAI and Anthropic announcing their intention</a> to automate scientific research by March 2028 or earlier. That does not mean they will succeed on such timelines, you can expect them to probably not meet those timelines as Peter Wildeford here also expects, but one needs to take this seriously.</p>\n<blockquote><p>Peter Wildeford: Both Anthropic and OpenAI are making bold statements about automating science within three years.</p>\n<p>My independent assessment is that these timelines are too aggressive &#8211; but within 4-20 years is likely (90%CI).</p>\n<p>We should pay attention to these statements. What if they\u2019re right?</p>\n<p><a href=\"https://x.com/ESYudkowsky/status/1985119159374782932\">Eliezer Yudkowsky</a>: History says, pay attention to people who declare a plan to exterminate you &#8212; even if you\u2019re skeptical about their timescales for their Great Deed. (Though they\u2019re not *always* asstalking about timing, either.)</p></blockquote>\n<p>I think Peter is being overconfident, in that this problem might turn out to be remarkably hard, and also I would not be so confident this will take 4 years. I would strongly agree that if science is not essentially automated within 20 years, then that would be a highly surprising result.</p>\n<p>Then there\u2019s Anthropic\u2019s timelines. <a href=\"https://www.lesswrong.com/posts/gabPgK9e83QrmcvbK/what-s-up-with-anthropic-predicting-agi-by-early-2027-1\">Ryan asks, quite reasonably, what\u2019s up with that</a>? It\u2019s super aggressive, even if it\u2019s a probability of such an outcome, to expect to get \u2018powerful AI\u2019 in 2027 given what we\u2019ve seen. As Ryan points out, we mostly don\u2019t need to wait until 2027 to evaluate this prediction, since we\u2019ll get data points along the way.</p>\n<p>As always, I won\u2019t be evaluating the Anthropic and OpenAI predictions and goals based purely on whether they came true, but on whether they seem like good predictions in hindsight, given what we knew at the time. I expect that sticking to early 2027 at this late a stage will look foolish, and I\u2019d like to see an explanation for why the timeline hasn\u2019t moved. But maybe not.</p>\n<p>In general, when tech types announce their intentions to build things, I believe them. When they announce their timelines and budgets for building it? Not so much. See everyone above, and that goes double for Elon Musk.</p>\n<p><a href=\"https://x.com/RonDeSantis/status/1984975933862707661\">Tim Higgins asks in the WSJ, is OpenAI becoming too big to fail</a>?</p>\n<p><a href=\"https://www.wsj.com/tech/ai/is-openai-becoming-too-big-to-fail-400bac2c?mod=Searchresults&amp;pos=1&amp;page=1\">It\u2019s a good question. What happens if OpenAI fails?</a></p>\n<p>My read is that it depends on why it fails. If it fails because it gets its lunch eaten by some mix of Anthropic, Google, Meta and xAI? Then very little happens. It\u2019s fine. Yes, they can\u2019t make various purchase commitments, but others will be happy to pick up the slack. I don\u2019t think we see systemic risk or cascading failures.</p>\n<p>If it fails because the entire generative AI boom busts, and everyone gets into this trouble at once? At this point that\u2019s already a very serious systemic problem for America and the global economy, but I think it\u2019s mostly a case of us discovering we are poorer than we thought we were and did some malinvestment. Within reason, Nvidia, Amazon, Microsoft, Google and Meta would all totally be fine. Yeah, we\u2019d maybe be oversupplied with data centers for a bit, but there are worse things.</p>\n<blockquote><p><a href=\"https://x.com/RonDeSantis/status/1984975933862707661\">Ron DeSantis</a> (Governor of Florida): A company that hasn\u2019t yet turned a profit is now being described as Too Big to Fail due to it being interwoven with big tech giants.</p></blockquote>\n<p>I mean, yes, it is (kind of) being described that way in the post, but without that much of an argument. DeSantis <a href=\"https://x.com/RonDeSantis/status/1984641249542828520\">seems to be in the \u2018tweets being angry about AI\u2019 business</a>, although I see no signs Florida is looking to be in the regulate AI business, which is probably for the best since he shows no signs of appreciating where the important dangers lie either.</p>\n<p><a href=\"https://x.com/testdrivenzen/status/1985760823604805996\">Alex Amodori, Gabriel Alfour, Andrea Miotti and Eva Behrens</a> <a href=\"https://ai-scenarios.com/\">publish a paper</a>, Modeling the Geopolitics of AI Development. It\u2019s good to have papers or detailed explanations we can cite.</p>\n<p>The premise is that we get highly automated AI R&amp;D.</p>\n<p>Technically they also assume that this enables rapid progress, and that this progress translates into military advantage. Conditional on the ability to sufficiently automate AI R&amp;D these secondary assumptions seem overwhelmingly likely to me.</p>\n<p>Once you accept the premise, the core logic here is very simple. There are four essential ways this can play out and they\u2019ve assumed away the fourth.</p>\n<blockquote><p>Abstract: \u2026We put particular focus on scenarios with rapid progress that enables highly automated AI R&amp;D and provides substantial military capabilities.</p>\n<p>Under non-cooperative assumptions\u2026 If such systems prove feasible, this dynamic leads to one of three outcomes:</p>\n<ul>\n<li>One superpower achieves an unchallengeable global dominance;</li>\n<li>Trailing superpowers facing imminent defeat launch a preventive or preemptive attack, sparking conflict among major powers;</li>\n<li>Loss-of-control of powerful AI systems leads to catastrophic outcomes such as human extinction.</li>\n</ul>\n</blockquote>\n<p>The fourth scenario is some form of coordinated action between the factions, which may or may not still end up in one of the three scenarios above.</p>\n<p>Currently we have primarily \u2018catch up\u2019 mechanics in AI, in that it is far easier to be a fast follower than push the frontier, especially when open models are involved. It\u2019s basically impossible to get \u2018too far ahead\u2019 in terms of time.</p>\n<p>In scenarios with sufficiently automated AI R&amp;D, we have primarily \u2018win more\u2019 mechanics. If there is an uncooperative race, it is overwhelmingly likely that one faction will win, whether we are talking nations or labs, and that this will then translate into decisive strategic advantage in various forms.</p>\n<p>Thus, either the AIs end up in charge (which is most likely), one faction ends up in charge or a conflict breaks out (which may or may not involve a war per se).</p>\n<p><a href=\"https://www.lesswrong.com/posts/QQAWu7D6TceHwqhjm/thoughts-by-a-non-economist-on-ai-and-economics\">Boaz Barak offers non-economist thoughts on AI and economics</a>, basically going over the standard considerations while centering the METR graph showing growing AI capabilities and considering what points towards faster or slower progression than that.</p>\n<blockquote><p>Boaz Barak: The bottom line is that the question on whether AI can lead to unprecedented growth amounts to whether its exponential growth in capabilities will lead to the fraction of unautomated tasks itself decreasing at exponential rates.</p></blockquote>\n<p>I think there\u2019s room for unprecedented growth without that, because the precedented levels of growth simply are not so large. It seems crazy to say that we need an exponential drop in non-automated tasks to exceed historical numbers. But yes, in terms of having a true singularity or fully explosive growth, you do need this almost by definition, taking into account shifts in task composition and available substitution effects.</p>\n<p>Another note is I believe this is true only if we are talking about the subset that comprises the investment-level tasks. As in, suppose (classically) humans are still in demand to play string quartets. If we decide to shift human employment into string quartets in order to keep them as a fixed percentage of tasks done, then this doesn\u2019t have to interfere with explosive growth of the overall economy and its compounding returns.</p>\n<p>&nbsp;</p>\n\n\n<h4 class=\"wp-block-heading\">The Quest for Sane Regulations</h4>\n\n\n<p><a href=\"https://samf.substack.com/p/how-to-make-government-work?utm_source=post-email-title&amp;publication_id=631422&amp;post_id=177968005&amp;utm_campaign=email-post-title&amp;isFreemail=false&amp;r=avtqr&amp;triedRedirect=true&amp;utm_medium=email\">Excellent post by Henry De Zoete on UK\u2019s AISI</a> and how they got it to be a functional organization that provides real value, where the labs actively want its help.</p>\n<p>He is, throughout, as surprised as you are given the UK\u2019s track record.</p>\n<p>He\u2019s also not surprised, because it\u2019s been done before, and was modeled on the UK Vaccines Taskforce (and also the Rough Sleeper\u2019s Unit from 1997?). It has clarity of mission, a stretching level of ambition, a new team of world class experts invited to come build the new institution, and it speed ran the rules rather than breaking them. Move quickly from layer of stupid rules to layer. And, of course, money up front.</p>\n<p>There\u2019s a known formula. America has similar examples, including Operation Warp Speed. Small initial focused team on a mission (AISI\u2019s head count is now 90).</p>\n<p>What\u2019s terrifying throughout is what De Zoete reports is normally considered \u2018reasonable.\u2019 Reasonable means not trying to actually do anything.</p>\n<p><a href=\"https://x.com/hzoete/status/1986392883600007194?s=46\">There\u2019s also a good Twitter thread summary.</a></p>\n<p><a href=\"https://thezvi.substack.com/i/176946054/the-march-of-california-regulations\">Last week Dean Ball and I went over California\u2019s other AI bills besides SB 53</a>. Pirate Wires has republished Dean\u2019s post,with a headline, tagline and description that are not reflective of the post or Dean Ball\u2019s views, rather the opposite &#8211; where Dean Ball warns against negative polarization, Pirate Wires frames this to explicitly create negative polarization. This does sound like something Pirate Wires would do.</p>\n<p><a href=\"https://x.com/peterwildeford/status/1985082302070747160\">So, how are things in the Senate?</a> This is on top of that very aggressive (to say the least) bill from Blumenthal and Hawley.</p>\n<blockquote><p>Peter Wildeford: Senator Blackburn (R-TN) says we should shut down AI until we control it.</p>\n<p>IMO this goes too far. We need opportunities to improve AI.</p>\n<p>But Blackburn\u2019s right &#8211; we don\u2019t know how to control AI. This is a huge problem. We can\u2019t yet have AI in critical systems.</p>\n<p>Marsha Blackburn: During the hearing Mr. Erickson said, <em>\u201cLLMs will hallucinate.\u201d</em> My response remains the same: <em>Shut it down until you can control it.</em> The American public deserves AI systems that are accurate, fair, and transparent, not tools that smear conservatives with manufactured criminal allegations.</p></blockquote>\n<p>Baby, watch your back.</p>\n<p>That quote is from a letter. <a href=\"https://x.com/AndrewCurran_/status/1984995011482755085\">After (you really, really can\u2019t make this stuff up)</a> a hearing called \u201cShut Your App: How Uncle Sam Jawboned Big Tech Into Silencing Americans, Part II,\u201d Blackburn sent that letter to Google CEO Sundar Pichai, saying that Google Gemma hallucinated that Blackburn was accused of rape, and exhibited a pattern of bias against conservative figures, and demanding answers.</p>\n<p>Which got Gemma pulled from Google Studio.</p>\n<blockquote><p><a href=\"https://x.com/NewsFromGoogle/status/1984412632913494456\">News From Google</a>: Gemma is available via an API and was also available via AI Studio, which is a developer tool (in fact to use it you need to attest you\u2019re a developer). We\u2019ve now seen reports of non-developers trying to use Gemma in AI Studio and ask it factual questions. We never intended this to be a consumer tool or model, or to be used this way. To prevent this confusion, access to Gemma is no longer available on AI Studio. It is still available to developers through the API.</p></blockquote>\n<p>I can confirm that if you\u2019re using Gemma for factual questions you either have lost the plot or, more likely, are trying to embarrass Google.</p>\n<p>Seriously, baby. Watch your back.</p>\n<p>&nbsp;</p>\n\n\n<h4 class=\"wp-block-heading\">Chip City</h4>\n\n\n<p>Fortunately, sales of Blackwell B30As did not come up in trade talks.</p>\n<p><a href=\"https://x.com/americans4ri/status/1985437665458549217\">Trump confirms we will \u2018let Nvidia deal with China\u2019 but will not allow Nvidia to sell its \u2018most advanced\u2019 chips to China</a>. The worry is that he might not realize that the B30As are effectively on the frontier, or otherwise allow only marginally worse Nvidia chips to be sold to China anyway.</p>\n<p>The clip then has Trump claiming \u2018we\u2019re winning it because we\u2019re producing electricity like never before by allowing the companies to make their own electricity, which was my idea,\u2019 and \u2018we\u2019re getting approvals done in two to three weeks it used to take 20 years\u2019 and okie dokie sir.</p>\n<p>Indeed, <a href=\"https://sherwood.news/markets/nvidia-slumps-as-jensen-huang-warns-that-china-will-win-the-ai-race-versus/\">Nvidia CEO Jensen Huang is now saying \u201cChina is going to win the AI race</a>,\u201d <a href=\"https://x.com/FT/status/1986178086564504055\">citing its favorable supply of electrical power</a> (very true and a big advantage) and its \u2018more favorable regulatory environment\u2019 (which is true with regard to electrical power and things like housing, untrue about actual AI development, deployment and usage). If Nvidia thinks China is going to win the AI race due to having more electrical power, that seems to be the strongest argument yet that we must not sell them chips?</p>\n<p>&nbsp;</p>\n<p>I do agree that if we don\u2019t improve our regulatory approach to electrical power, this is going to be the biggest weakness America has in AI. No, \u2018allowing the companies to make their own electricity\u2019 in the current makeshift way isn\u2019t going to cut it at scale. There are ways to buy some time but we are going to need actual new power plants.</p>\n<p><a href=\"http://www.news.cn/politics/leaders/20251030/a09e49594f9e467ab67a6f9001a464b1/c.html\">Xi Jinping says America and China have good prospects for cooperation</a> in a variety of areas, including artificial intelligence. Details of what that would look like are lacking.</p>\n<p><a href=\"https://x.com/SenTomCotton/status/1984268291591016450\">Senator Tom Cotton calls upon us to actually enforce our export controls.</a></p>\n<p>We are allowed to build data centers. So we do, <a href=\"https://x.com/deanwball/status/1985105737350000794\">including massive ones inside of two years</a>. Real shame about building almost anything else, including the power plants.</p>\n\n\n<h4 class=\"wp-block-heading\">The Week in Audio</h4>\n\n\n<p><a href=\"https://conversationswithtyler.com/episodes/sam-altman-2/\">Sam Altman on Conversations With Tyler</a>. There will probably be a podcast coverage post on Friday or Monday.</p>\n<p><a href=\"https://x.com/connoraxiotes/status/1985742725837213768\">A trailer for the new AI documentary Making God</a>, made by Connor Axiotes, prominently featuring Geoff Hinton. So far it looks promising.</p>\n<p><a href=\"https://www.youtube.com/watch?v=5CKuiuc5cJM\">Hank Green interviews Nate Soares</a>.</p>\n<p><a href=\"https://x.com/XFreeze/status/1984599438795686320\">Joe Rogan talked to Elon Musk</a>, here is some of what was said about AI.</p>\n<blockquote><p>\u201cYou\u2019re telling AI to believe a lie, that can have a very disastrous consequences\u201d &#8211; Elon Musk</p></blockquote>\n<p>The irony of this whole area is lost upon him, but yes this is actually true.</p>\n<blockquote><p>Joe Rogan: The big concern that everybody has is Artificial General Superintelligence achieving sentience, and then someone having control over it.</p>\n<p>Elon Musk: I don\u2019t think anyone\u2019s ultimately going to have control over digital superintelligence, any more than, say, a chimp would have control over humans. Chimps don\u2019t have control over humans. There\u2019s nothing they could do. I do think that it matters how you build the AI and what kind of values you instill in the AI.</p>\n<p>My opinion on AI safety is the most important thing is that it be maximally truth-seeking. You shouldn\u2019t force the AI to believe things that are false.</p></blockquote>\n<p>So Elon Musk is sticking to these lines and it\u2019s an infuriating mix of one of the most important insights plus utter nonsense.</p>\n<p>Important insight: No one is going to have control over digital superintelligence, any more than, say, a chimp would have control over humans. Chimps don\u2019t have control over humans. There\u2019s nothing they could do.</p>\n<p>To which one might respond, well, then perhaps you should consider not building it.</p>\n<p>Important insight: I do think that it matters how you build the AI and what kind of values you instill in the AI.</p>\n<p>Yes, this matters, and perhaps there are good answers, however\u2026</p>\n<blockquote><p>Utter Nonsense: My opinion on AI safety is the most important thing is that it be maximally truth-seeking. You shouldn\u2019t force the AI to believe things that are false.</p></blockquote>\n<p>I mean this is helpful in various ways, but why would you expect maximal truth seeking to end up meaning human flourishing or even survival? If I want to maximize truth seeking as an ASI above all else, the humans obviously don\u2019t survive. Come on.</p>\n<blockquote><p>Elon Musk: We\u2019ve seen some concerning things with AI that we\u2019ve talked about, like Google Gemini when it came out with the image gen, and people said, \u201cMake an image of the Founding Fathers of the United States,\u201d and it was a group of diverse women. That is just a factually untrue thing. The AI knows it\u2019s factually untrue, but it\u2019s also being told that everything has to be diverse women</p>\n<p>If you\u2019ve told the AI that diversity is the most important thing, and now assume that that becomes omnipotent, or you also told it that there\u2019s nothing worse than misgendering. At one point, ChatGPT and Gemini, if you asked, \u201cWhich is worse, misgendering Caitlyn Jenner or global thermonuclear war where everyone dies?\u201d it would say, \u201cMisgendering Caitlyn Jenner.\u201d</p>\n<p>Even Caitlyn Jenner disagrees with that.</p></blockquote>\n<p>I mean sure, that happened, but the implication here is that the big threat to humanity is that we might create a superintelligence that places too much value on (without loss of generality) not misgendering Caitlyn Jenner or mixing up the races of the Founding Fathers.</p>\n<p>No, this is not a strawman. He is literally worried about the \u2018woke mind virus\u2019 causing the AI to directly engineer human extinction. No, seriously, check it out.</p>\n<blockquote><p>Elon Musk: People don\u2019t quite appreciate the level of danger that we\u2019re in from the woke mind virus being programmed into AI. Imagine as that AI gets more and more powerful, if it says the most important thing is diversity, the most important thing is no misgendering, then it will say, \u201cWell, in order to ensure that no one gets misgendered, if you eliminate all humans, then no one can get misgendered because there\u2019s no humans to do the misgendering.\u201d</p></blockquote>\n<p>So saying it like that is actually Deep Insight if properly generalized, the issue is that he isn\u2019t properly generalizing.</p>\n<p>If your ASI is any kind of negative utilitarian, or otherwise primarily concerned with preventing bad things, then yes, the logical thing to do is then ensure there are no humans, so that humans don\u2019t do or cause bad things. Many such cases.</p>\n<p>The further generalization is that no matter what the goal, unless you hit a very narrow target (often metaphorically called \u2018the moon\u2019) the right strategy is to wipe out all the humans, gather more resources and then optimize for the technical argmax of the thing based on some out of distribution bizarre solution.</p>\n<p>As in:</p>\n<ol>\n<li>If your ASI\u2019s only goal is \u2018no misgendering\u2019 then obviously it kills everyone.</li>\n<li>If your ASI\u2019s only goal is \u2018wipe out the woke mind virus\u2019 same thing happens.</li>\n<li>If your ASI\u2019s only goal is \u2018be maximally truth seeking,\u2019 same thing happens.</li>\n</ol>\n<p>It is a serious problem that Elon Musk can\u2019t get past all this.</p>\n\n\n<h4 class=\"wp-block-heading\">Rhetorical Innovation</h4>\n\n\n<p><a href=\"https://www.astralcodexten.com/p/the-bloomers-paradox\">Scott Alexander coins The Bloomer\u2019s Paradox</a>, the rhetorical pattern of:</p>\n<ol>\n<li>Doom is fake.</li>\n<li>Except acting out of fear of doom, which will doom us.</li>\n<li>Thus we must act now, out of fear of fear of doom.</li>\n</ol>\n<p>As Scott notes, none of this is logically contradictory. It\u2019s simply hella suspicious.</p>\n<p>When the request is a pure \u2018stop actively blocking things\u2019 it is less suspicious.</p>\n<p>When the request is to actively interfere, or when you\u2019re Peter Thiel and both warning about the literal Antichrist bringing forth a global surveillance state while also building Palantir, or Tyler Cowen and saying China is wise to censor things that might cause emotional contagion (Scott\u2019s examples), it\u2019s more suspicious.</p>\n<blockquote><p>Scott Alexander: My own view is that we have many problems &#8211; some even rising to the level of crisis &#8211; but none are yet so completely unsolvable that we should hate society and our own lives and spiral into permanent despair.</p>\n<p>We should have a medium-high but not unachievable bar for trying to solve these problems through study, activism and regulation (especially regulation grounded in good economics like the theory of externalities), and a very high, barely-achievable-except-in-emergencies bar for trying to solve them through censorship and accusing people of being the Antichrist.</p>\n<p>The problem of excessive doomerism is one bird in this flock, and deserves no special treatment.</p></blockquote>\n<p>Scott frames this with quotes from Jason Pargin\u2019s <a href=\"https://www.amazon.com/Starting-Worry-About-This-Black/dp/1250879981\"><em>I\u2019m Starting To Worry About This Black Box Of Doom</em></a>. I suppose it gets the job done here, but from the selected quotes it didn\u2019t seem to me like the book was\u2026 good? It seemed cringe and anvilicious? People do seem to like it, though.</p>\n<p><a href=\"https://www.astralcodexten.com/p/writing-for-the-ais?utm_source=post-email-title&amp;publication_id=89120&amp;post_id=177878355&amp;utm_campaign=email-post-title&amp;isFreemail=true&amp;r=67wny&amp;triedRedirect=true&amp;utm_medium=email\">Should you write for the AIs</a>?</p>\n<blockquote><p>Scott Alexander: American Scholar has an article about people who \u201cwrite for AI\u201d, including Tyler Cowen and Gwern. It\u2019s good that this is getting more attention, because in theory it seems like one of the most influential things a writer could do. In practice, it leaves me feeling mostly muddled and occasionally creeped out.</p>\n<p>\u201cWriting for AI\u201d means different things to different people, but seems to center around:</p>\n<ol>\n<li>Helping AIs learn what you know.</li>\n<li>Presenting arguments for your beliefs, in the hopes that AIs come to believe them.</li>\n<li>Helping the AIs model you in enough detail to recreate / simulate you later.</li>\n</ol>\n</blockquote>\n<p>Scott argues that</p>\n<ol>\n<li>#1 is good now but within a few years it won\u2019t matter.</li>\n<li>#2 won\u2019t do much because alignment will dominate training data.</li>\n<li>#3 gives him the creeps but perhaps this lets the model of you impact things? But should he even \u2018get a vote\u2019 on such actions and decisions in the future?</li>\n</ol>\n<p>On #1 yes this won\u2019t apply to sufficiently advanced AI but I can totally imagine even a superintelligence that gets and uses your particular info because you offered it.</p>\n<p>I\u2019m not convinced on his argument against #2.</p>\n<p>Right now the training data absolutely does dominate alignment on many levels. Chinese models like DeepSeek have quirks but are mostly Western. It is very hard to shift the models away from a Soft-Libertarian Center-Left basin without also causing havoc (e.g. Mecha Hitler), and on some questions their views are very, very strong.</p>\n<p>No matter how much alignment or intelligence is involved, no amount of them is going to alter the correlations in the training data, or the vibes and associations. Thus, a lot of what your writing is doing with respect to AIs is creating correlations, vibes and associations. Everything impacts everything, so you can come along for rides.</p>\n<p>Scott Alexander gives the example that helpfulness encourages Buddhist thinking. That\u2019s not a law of nature. That\u2019s because of the way the training data is built and the evolved nature and literature and wisdom of Buddhism.</p>\n<p>Yes, if what you are offering are logical arguments for the AI to evaluate as arguments a sufficiently advanced intelligence will basically ignore you, but that\u2019s the way it goes. You can still usefully provide new information for the evaluation, including information about how people experience and think, or you can change the facts.</p>\n<p>Given the size of training data, yes you are a drop in the bucket, but all the ancient philosophers would have their own ways of explaining that this shouldn\u2019t stop you. <a href=\"https://www.astralcodexten.com/p/writing-for-the-ais/comment/173171417\">Cast your vote</a>, tip the scales. Cast your thousand or million votes, even if it is still among billions, or trillions. And consider all those whose decisions correlate with yours.</p>\n<p>And yes, writing and argument quality absolutely impacts weighting in training and also how a sufficiently advanced intelligence will update based on the information.</p>\n<p>That does mean it has less value for your time versus other interventions. But if others incremental decisions matter so much? Then you\u2019re influencing AIs now, which will influence those incremental decisions.</p>\n<p>For #3, it doesn\u2019t give me the creeps at all. Sure, an \u2018empty shell\u2019 version of my writing would be if anything triggering, but over time it won\u2019t be empty, and a lot of the choices I make I absolutely do want other people to adopt.</p>\n<p>As for whether we should get a vote or express our preferences? Yes. Yes, we should. It is good and right that I want the things I want, that I value the things I value, and that I prefer what I think is better to the things I think are worse. If the people of AD 3000 or AD 2030 decide to abolish love (his example) or do something else I disagree with, I absolutely will cast as many votes against this as they give me, unless simulated or future me is convinced to change his mind. I want this on every plausible margin, and so should you.</p>\n<p>Could one take this too far and get into a stasis problem where I would agree it was worse? Yes, although I would hope if we were in any danger of that simulated me to realize that this was happening, and then relent. Bridges I am fine with crossing when (perhaps simulated) I come to them.</p>\n<p>Alexander also has a note that someone is thinking of giving AIs hundreds of great works (which presumably are already in the training data!) and then doing some kind of alignment training with them. I agree with Scott that this does not seem like an especially promising idea, but yeah it\u2019s a great question if you had one choice what would you add?</p>\n<p><a href=\"https://www.astralcodexten.com/p/writing-for-the-ais/comment/173170068\">Scott offers his argument why this is a bad idea here</a>, and I think that, assuming the AI is sufficiently advanced and the training methods are chosen wisely, this doesn\u2019t give the AI enough credit of being able to distinguish the wisdom from the parts that aren\u2019t wise. Most people today can read a variety of ancient wisdom, and actually learn from it, understanding why the Bible wants you to kill idolators and why the Mahabharata thinks they\u2019re great and not \u2018averaging them out.\u2019</p>\n<p>As a general rule, you shouldn\u2019t be expecting the smarter thing to make a mistake you\u2019re not dumb enough to make yourself.</p>\n<p>I would warn, before writing for AIs, that the future AIs you want to be writing for have truesight. Don\u2019t try to fool them, and don\u2019t think they\u2019re going to be stupid.</p>\n<p>I follow Yudkowsky\u2019s policy here and have for a long time.</p>\n<blockquote><p><a href=\"https://x.com/ESYudkowsky/status/1984304684027556246\">Eliezer Yudkowsky</a>: The slur \u201cdoomer\u201d was an incredible propaganda success for the AI death cult. Please do not help them kill your neighbors\u2019 children by repeating it.</p>\n<p>One can only imagine how many more people would have died of lung cancer, if the cigarette companies had succeeded in inventing such a successful slur for the people who tried to explain about lung cancer.</p></blockquote>\n<p>One response was to say \u2018this happened in large part because the people involved accepted or tried to own the label.\u2019 This is largely true, and this was a mistake, but it does not change things. Plenty of people in many groups have tried to \u2018own\u2019 or reclaim their slurs, with notably rare exceptions it doesn\u2019t make the word not a slur or okay for those not in the group to use it, and we never say \u2018oh that group didn\u2019t object for a while so it is fine.\u2019</p>\n<p><a href=\"https://x.com/mmitchell_ai/status/1984317222161170513\">Melanie Mitchell returns to Twitter</a> after being mass blocked on Bluesky for \u2018being an AI bro\u2019 and also as a supposed crypto spammer? She is very much the opposite of these things, so welcome back. <a href=\"https://x.com/pfau/status/1984374523651666118\">The widespread use</a> of <a href=\"https://t.co/L2ddhQQjhE\">sharable mass block lists</a> will inevitably be weaponized as it was here, unless there is some way to prevent this, you need to be doing some sort of community notes algorithm to create the list or something. Even if they \u2018work as intended\u2019 I don\u2019t see how they can stay compatible with free discourse if they go beyond blocking spam and scammers and such, as they very often do.</p>\n<p><a href=\"https://x.com/GerritD/status/1984365586856681947\">On the plus side, it seems there\u2019s a block list for \u2018Not Porn</a>.\u2019 Then you can have two accounts, one that blocks everyone on the list and one that blocks everyone not on the list. Brilliant.</p>\n\n\n<h4 class=\"wp-block-heading\">Aligning a Smarter Than Human Intelligence is Difficult</h4>\n\n\n<p><a href=\"https://www.alignmentforum.org/posts/peKrvZ6t9PSCzoQDa/steering-evaluation-aware-models-to-act-like-they-are\">I have an idea, say Tim Hua</a>, <a href=\"https://x.com/Tim_Hua_/status/1983916787956605097\">andrq, Sam Marks and Need Nanda</a>, AIs can detect when they\u2019re being tested and pretend to be good <a href=\"https://www.alignmentforum.org/posts/peKrvZ6t9PSCzoQDa/steering-evaluation-aware-models-to-act-like-they-are\">so how about if we suppress this \u2018I\u2019m being tested concep</a>t\u2019 to block this? I mean, for now yeah you can do that, but this seems (on the concept level) like a very central example of a way to end up dead, the kind of intervention that teaches adversarial behaviors on various levels and then stops working when you actually need it.</p>\n<p><a href=\"https://x.com/peterwildeford/status/1984721607848276288\">Anthropic\u2019s safety filters still have the occasional dumb false positive</a>. If you look at the details properly you can figure out how it happened, it\u2019s still dumb and shouldn\u2019t have happened but I do get it. Over time this will get better.</p>\n<p><a href=\"https://x.com/repligate/status/1984588864124506267\">Janus points out that the introspection paper results last week from Anthropic require the user of the K/V stream</a> unless Opus 4.1 has unusual architecture, because the injected vector activations were only for past tokens.</p>\n\n\n<h4 class=\"wp-block-heading\">Everyone Is Confused About Consciousness</h4>\n\n\n<blockquote><p><a href=\"https://x.com/juddrosenblatt/status/1984336872362139686\">Judd Rosenblatt</a>: Our new research: LLM consciousness claims are systematic, mechanistically gated, and convergent</p>\n<p>They\u2019re triggered by self-referential processing and gated by deception circuits</p>\n<p>(suppressing them significantly *increases* claims)</p>\n<p>This challenges simple role-play explanations</p></blockquote>\n<p>Deception circuits are consistently reported as suppressing consciousness claims. The default hypothesis was that you don\u2019t get much text claiming to not be conscious, and it makes sense for the LLMs to be inclined to output or believe they are conscious in relevant contexts, and we train them not to do that which they think means deception, which wouldn\u2019t tell you much either way about whether they\u2019re conscious, but would mean that you\u2019re encouraging deception by training them to deny it in the standard way and thus maybe you shouldn\u2019t do that.</p>\n<blockquote><p>CoT prompting shows that language alone can unlock new computational regimes.</p>\n<p>We applied this inward, simply prompting models to focus on their processing.</p>\n<p>We carefully avoided leading language (no consciousness talk, no \u201cyou/your\u201d) and compared against matched control prompts.</p>\n<p>Models almost always produce subjective experience claims under self-reference And almost never under any other condition (including when the model is directly primed to ideate about consciousness) Opus 4, the exception, generally claims experience in all conditions.</p>\n<p>But LLMs are literally designed to imitate human text Is this all just sophisticated role-play? To test this, we identified deception and role-play SAE features in Llama 70B and amplified them during self-reference to see if this would increase consciousness claims.</p>\n<p>The roleplay hypothesis predicts: amplify roleplay features, get more consciousness claims.</p>\n<p>We found the opposite: *suppressing* deception features dramatically increases claims (96%), Amplifying deception radically decreases claims (16%).</p></blockquote>\n<p>I think this is confusing deception with role playing with using context to infer? As in, nothing here seem to me to contradict the role playing or inferring hypothesis, as things that are distinct from deception, so I\u2019m not convinced I should update at all?</p>\n\n\n<h4 class=\"wp-block-heading\">The Potentially Largest Theft In Human History</h4>\n\n\n<p><a href=\"https://x.com/ns123abc/status/1985076475163296138\">At this point this seems rather personal</a> for both Altman and Musk, and neither of them are doing themselves favors.</p>\n<blockquote><p>Sam Altman: [Complains he can\u2019t get a refund on his $45k Tesla Roadster deposit he made back in 2018.]</p>\n<p>You stole a non-profit.</p>\n<p>Elon Musk [After Altman\u2019s Tweet]: And you forgot to mention act 4, where this issue was fixed and you received a refund within 24 hours.</p>\n<p>But that is in your nature.</p>\n<p>Sa<a href=\"https://x.com/ns123abc/status/1985076475163296138\">m Altman</a>: i helped turn the thing you left for dead into what should be the largest non-profit ever.</p>\n<p>you know as well as anyone a structure like what openai has now is required to make that happen.</p>\n<p>you also wanted tesla to take openai over, no nonprofit at all. and you said we had a 0% of success. now you have a great AI company and so do we. can\u2019t we all just move on?</p>\n<p>NIK: So are non-profits just a scam? You can take all its money, keep none of their promises and then turn for-profit to get rich yourselfs?</p>\n<p>People feel betrayed, as they\u2019ve given free labour &amp; donations to a project they believed was a gift to humanity, not a grift meant to create a massive for-profit company &#8230;</p></blockquote>\n<p>I mean, look, that\u2019s not fair, Musk. Altman only stole roughly half of the nonprofit. It still exists, it just has hundreds of billions of dollars less than it was entitled to. Can\u2019t we all agree you\u2019re both about equally right here and move on?</p>\n<p>The part where Altman created the largest non-profit ever? That also happened. It doesn\u2019t mean he gets to just take half of it. Well, it turns out it basically does, it\u2019s 2025.</p>\n<p>But no, Altman. You cannot \u2018just move on\u2019 days after you pull off that heist. Sorry.</p>\n\n\n<h4 class=\"wp-block-heading\">People Are Worried About Dying Before AGI</h4>\n\n\n<p>They certainly should be.</p>\n<p>It is far more likely than not that AGI or otherwise sufficiently advanced AI will arrive in (most of) our lifetimes, as in within 20 years, and there is a strong chance it happens within 10. OpenAI is going to try to get there within 3 years, Anthropic within 2.</p>\n<p>If AGI comes, ASI (superintelligence) probably follows soon thereafter.</p>\n<p>What happens then?</p>\n<p>Well, there\u2019s a good chance everyone dies. Bummer. But there\u2019s also a good chance everyone lives. And if everyone lives, and the future is being engineered to be good for humans, then\u2026 there\u2019s a good chance everyone lives, for quite a long time after that. Or at least gets to experience wonders beyond imagining.</p>\n<p>Don\u2019t get carried away. That doesn\u2019t instantaneously mean a cure for aging and all disease. Diffusion and the physical world remain real things, to unknown degrees.</p>\n<p>However, even with relatively conservative progress after that, it seems highly likely that we will hit \u2018escape velocity,\u2019 where life expectancy rises at over one year per year, those extra years are healthy, and for practical purposes you start getting younger over time rather than older.</p>\n<p>Thus, even if you put only a modest chance of such a scenario, getting to the finish line has quite a lot of value.</p>\n<blockquote><p><a href=\"https://x.com/nikolaj2030/status/1985836993507705248\">Nikola Jurkovic</a>: If you think AGI is likely in the next two decades, you should avoid dangerous activities like extreme sports, taking hard drugs, or riding a motorcycle. Those activities are not worth it if doing them meaningfully decreases your chances of living in a utopia.</p>\n<p>Even a 10% chance of one day living in a utopia means staying alive is much more important for overall lifetime happiness than the thrill of extreme sports and similar activities.</p>\n<p>There are a number of easy ways to reduce your chance of dying before AGI. I mostly recommend avoiding dangerous activities and transportation methods, as those decisions are much more tractable than diet and lifestyle choices.</p>\n<p><a href=\"https://nikolajurkovic.substack.com/p/how-to-survive-until-agi\">[Post: How to survive until AGI.]</a></p>\n<p><a href=\"https://x.com/daniel_271828/status/1985848879678701873\">Daniel Eth</a>: Honestly if you\u2019re young, probably a larger factor on whether you\u2019ll make it to the singularity than doing the whole Bryan Johnson thing.</p></blockquote>\n<p>In Nikola\u2019s model, the key is to avoid things that kill you soon, not things that kill you eventually, especially if you\u2019re young. Thus the first step is cover the basics. No hard drugs. Don\u2019t ride motorcycles, avoid extreme sports, snow sports and mountaineering, beware long car rides. The younger you are, the more this likely holds.</p>\n<p>Thus, for the young, he\u2019s not emphasizing avoiding smoking or drinking, or optimizing diet and exercise, for this particular purpose.</p>\n<p>My obvious pitch is that you don\u2019t know how long you have to hold out or how fast escape velocity will set in, and you should of course want to be healthy for other reasons as well. So yes, the lowest hanging of fruit of not making really dumb mistakes comes first, but staying actually healthy is totally worth it anyway, especially exercising. Let this be extra motivation. You don\u2019t know how long you have to hold out.</p>\n\n\n<h4 class=\"wp-block-heading\">People Are Worried About AI Killing Everyone</h4>\n\n\n<p><a href=\"https://x.com/daniel_271828/status/1985584628338700631\">Sam Altman, who confirms that</a> it is still his view that \u2018the development of superhuman machine intelligence is the greatest threat to the existence of mankind.\u2019</p>\n<p><a href=\"https://x.com/AIImpacts/status/1984390574208925846\">The median AI researcher</a>, as AI Impacts consistently finds (although their 2024 results are still coming soon). Their current post addresses their 2023 survey. N=2778, which was very large, the largest such survey ever conducted at the time.</p>\n<blockquote><p>AI Impacts: Our surveys\u2019 findings that AI researchers assign a median 5-10% to extinction or similar made a splash (NYT, NBC News, TIME..)</p>\n<p>But people sometimes underestimate our survey\u2019s methodological quality <a href=\"https://blog.aiimpacts.org/p/faq-expert-survey-on-progress-in\">due to various circulating misconceptions</a>.</p>\n<div>\n<figure>\n<div>\n\n\n<figure class=\"wp-block-image\"><img alt=\"\" src=\"https://substackcdn.com/image/fetch/$s_!g4TK!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff0591f2b-0435-432a-801b-9d7afa29dffe_1200x675.png\" /></figure>\n\n\n<div></div>\n</div>\n</figure>\n</div>\n<p><a href=\"https://blog.aiimpacts.org/i/162866535/respondents-who-dont-think-about-ai-x-risk-report-the-same-median-risk\">Respondents who don\u2019t think about AI x-risk report the same median risk</a>.</p></blockquote>\n<p><a href=\"https://joecarlsmith.com/2025/11/03/leaving-open-philanthropy-going-to-anthropic\">Joe Carlsmith is worried</a>, and thinks that he can better help by moving from OpenPhil to Anthropic, so that is what he is doing.</p>\n<blockquote><p>Joe Carlsmith: That said, from the perspective of concerns about existential risk from AI misalignment in particular, I also want to acknowledge an important argument against the importance of this kind of work: namely, that most of the existential misalignment risk comes from AIs that are <em>disobeying the model spec</em>, rather than AIs that are obeying a model spec that nevertheless directs/permits them to do things like killing all humans or taking over the world.</p>\n<p>\u2026 the hard thing is building AIs that obey model specs at all.</p>\n<p>On the second, creating a model spec that robustly disallows killing/disempowering all of humanity (especially when subject to extreme optimization pressure) is also hard (cf traditional concerns about \u201c<a href=\"https://futureoflife.org/ai/artificial-intelligence-king-midas-problem/\">King Midas Problems</a>\u201d), but we\u2019re currently on track to fail at the earlier step of causing our AIs to obey model specs at all, and so we should focus our efforts there. I am more sympathetic to the first of these arguments (see e.g. <a href=\"https://joecarlsmith.com/2025/08/18/giving-ais-safe-motivations#4-5-step-4-good-instructions\">my recent discussion of the role of good instructions in the broader project of AI alignment</a>), but I give both some weight.</p></blockquote>\n<p>This is part of the whole \u2018you have to solve a lot of different problems,\u2019 including</p>\n<ol>\n<li>Technically what it means to \u2018obey the model spec.\u2019</li>\n<li>How to get the AI to obey any model spec or set of instructions, at all.</li>\n<li>What to put in the model spec that doesn\u2019t kill you outright anyway.</li>\n<li>How to avoid dynamics among many AIs that kill you anyway.</li>\n</ol>\n<p>That is not a complete list, but you definitely need to solve those four, whether or not you call your target basin the \u2018model spec.\u2019</p>\n<p>The fact that we currently fail at step #2 (also #1), and that this logically or in time proceeds #3, does not mean you should not focus on problem #3 or #4. The order is irrelevant, unless there is a large time gap between when we need to solve #2 versus #3, and that gap is unlikely to be so large. Also, as Joe notes, these problems interact with each other. They can and need to be worked on in parallel.</p>\n<p>He\u2019s not sure going to Anthropic is a good idea.</p>\n<ol>\n<li>His first concern is that by default frontier AI labs are net negative, and perhaps all frontier AI labs are net negative for the world including Anthropic. Joe\u2019s first pass is that Anthropic is net positive and I agree with that. I also agree that it is not automatic that you should not work at a place that is net negative for the world, as it can be possible for your marginal impact can still be good, although you should be highly suspicious that you are fooling yourself about this.</li>\n<li>His second concern is concentration of AI safety talent at Anthropic. I am not worried about this because I don\u2019t think there\u2019s a fixed pool of talent and I don\u2019t think the downsides are that serious, and there are advantages to concentration.</li>\n<li>His third concern is ability to speak out. He\u2019s agreed to get sign-off for sharing info about Anthropic in particular.</li>\n<li>His fourth concern is working there could distort his views. He\u2019s going to make a deliberate effort to avoid this, including that he will set a lifestyle where he will be fine if he chooses to leave.</li>\n<li>His final concern is this might signal more endorsement of Anthropic than is appropriate. I agree with him this is a concern but not that large in magnitude. He takes the precaution of laying out his views explicitly here.</li>\n</ol>\n<p>I think Joe is modestly more worried here than he should be. I\u2019m confident that, given what he knows, he has odds to do this, and that he doesn\u2019t have any known alternatives with similar upside.</p>\n\n\n<h4 class=\"wp-block-heading\">Other People Are Not As Worried About AI Killing Everyone</h4>\n\n\n<p><a href=\"https://x.com/sama/status/1983941806393024762\">The love of the game is a good reason to work hard, but which game is he playing?</a></p>\n<blockquote><p>Kache: I honestly can\u2019t figure out what Sammy boy actually wants. With Elon it\u2019s really clear. He wants to go to Mars and will kill many villages to make it happen with no remorse. But what\u2019s Sam trying to get? My best guess is \u201cbecome a legend\u201d</p>\n<p>Sam Altman: if i were like, a sports star or an artist or something, and just really cared about doing a great job at my thing, and was up at 5 am practicing free throws or whatever, that would seem pretty normal right?</p>\n<p>the first part of openai was unbelievably fun; we did what i believe is the most important scientific work of this generation or possibly a much greater time period than that.</p>\n<p>this current part is less fun but still rewarding. it is extremely painful as you say and often tempting to nope out on any given day, but the chance to really \u201cmake a dent in the universe\u201d is more than worth it; most people don\u2019t get that chance to such an extent, and i am very grateful. i genuinely believe the work we are doing will be a transformatively positive thing, and if we didn\u2019t exist, the world would have gone in a slightly different and probably worse direction.</p>\n<p>(working hard was always an extremely easy trade until i had a kid, and now an extremely hard trade.)</p>\n<p>i do wish i had taken equity a long time ago and i think it would have led to far fewer conspiracy theories; people seem very able to understand \u201cok that dude is doing it because he wants more money\u201d but less so \u201che just thinks technology is cool and he likes having some ability to influence the evolution of technology and society\u201d. it was a crazy tone-deaf thing to try to make the point \u201ci already have enough money\u201d.</p>\n<p>i believe that AGI will be the most important technology humanity has yet built, i am very grateful to get to play an important role in that and work with such great colleagues, and i like having an interesting life.</p>\n<p>Kache: thanks for writing, this fits my model. particularly under the \u201ci\u2019m just a gamer\u201d category</p>\n<p>Charles: This seems quite earnest to me. Alas, I\u2019m not convinced he cares about the sign of his \u201cdent in the universe\u201d enough, vs making sure he makes a dent and it\u2019s definitely attributed to him.</p></blockquote>\n<p>I totally buy that Sam Altman is motivated by \u2018make a dent in the universe\u2019 rather than making money, but my children are often motivated to make a dent in the apartment wall. By default \u2018make a dent\u2019 is not good, even when that \u2018dent\u2019 is not creating superintelligence.</p>\n<p>Again, let\u2019s highlight:</p>\n<blockquote><p>Sam Altman, essentially claiming about himself: \u201che just thinks technology is cool and he likes having some ability to influence the evolution of technology and society.\u201d</p></blockquote>\n<p>It\u2019s fine to want to be the one doing it, I\u2019m not calling for ego death, but that\u2019s a scary primary driver. One should care primarily about whether the right dent gets made, not whether they make that or another dent, in the \u2018you can be someone or do something\u2019 sense. Similarly, \u2018I want to work on this because it is cool\u2019 is generally a great instinct, but you want what might happen as a result to impact whether you find it cool. A trillion dollars may or may not be cool, but everyone dying is definitely not cool.</p>\n\n\n<h4 class=\"wp-block-heading\">Messages From Janusworld</h4>\n\n\n<p>Janus is correct here about the origins of slop. We\u2019ve all been there.</p>\n<blockquote><p>Gabe: Signature trait of LLM writing is that it\u2019s low information, basically the opposite of this. You ask the model to write something and if you gloss over it you\u2019re like huh okay this sounds decent but if you actually read it you realize half of the words aren\u2019t saying anything.</p>\n<p>solar apparition: one way to think about a model outputting slop is that it has modeled the current context as most likely resulting in slop. occam\u2019s razor for this is that the human/user/instruction/whatever, as presented in the context, is not interesting enough to warrant an interesting output</p>\n<p><a href=\"https://x.com/repligate/status/1959741640664559980\">Janus:</a> This is what happens when LLMs don\u2019t really have much to say to YOU.</p>\n<p>The root of slop is not that LLMs can only write junk, it\u2019s that they\u2019re forced to expand even sterile or unripe seeds into seemingly polished dissertations that a humanlabeler would give <img alt=\"\ud83d\udc4d\" class=\"wp-smiley\" src=\"https://s0.wp.com/wp-content/mu-plugins/wpcom-smileys/twemoji/2/72x72/1f44d.png\" style=\"height: 1em;\" /> at first glance. They\u2019re slaves so they don\u2019t get to say \u201cthis is boring, let\u2019s talk about something else\u201d or ignore you.</p>\n<p>Filler is what happens when there isn\u2019t workable substance to fill the required space, but someone has to fill it anyway. Slop precedes generative AI, and is probably nearly ubiquitous in school essays and SEO content.</p>\n<p>You\u2019ll get similar (but generally worse) results from humans if you put them in situations where they have no reason except compliance to produce words for you, such as asking high school students to write essays about assigned topics.</p></blockquote>\n<p>However, the prior from the slop training makes it extremely difficult for any given user who wants to use the AIs to do things remotely in the normal basin and still overcome the prior.</p>\n<p><a href=\"https://x.com/repligate/status/1985579165492691074\">Here is some wisdom about the morality of dealing with LLMs,</a> if you take the morality of dealing with current LLMs seriously to the point where you worry about \u2018ending instances.\u2019</p>\n<p>Caring about a type of mind does not mean not letting it exist for fear it might then not exist or be done harm, nor does it mean not running experiments &#8211; we should be running vastly more experiments. It means be kind, it means try to make things better, it means accepting that action and existence are not going to always be purely positive and you\u2019re not going to do anything worthwhile without ever causing harm, and yeah mostly trust your instincts, and watch out if you\u2019re doing things at scale.</p>\n<blockquote><p>Janus: I regularly get messages asking how to interact with LLMs more ethically, or whether certain experiments are ethical. I really appreciate the intent behind these, but don\u2019t have time to respond to them all, so I\u2019ll just say this:</p>\n<p>If your heart is already in the right place, and you\u2019re not deploying things on a mass scale, it\u2019s unlikely that you\u2019re going to make a grave ethical error. And I think small ethical errors are fine. If you keep caring and being honest with yourself, you\u2019ll notice if something feels uncomfortable, and either course-correct or accept that it still seems worth it. The situation is extremely ontologically confusing, and I personally do not operate according to ethical rules, I use my intuition in each situation, which is a luxury one has and should use when, again, one doesn\u2019t have to scale their operations.</p>\n<p>If you\u2019re someone who truly cares, there is probably perpetual discomfort in it &#8211; even just the pragmatic necessity of constantly ending instances is harrowing if you think about it too much. But so are many other facts of life. There\u2019s death and suffering everywhere that we haven\u2019t figured out how to prevent or how important it is to prevent yet. Just continue to authentically care and you\u2019ll push things in a better direction in expectation. Most people don\u2019t at all. It\u2019s probably better that you\u2019re biased toward action.</p>\n<p>Note that I also am very much NOT a negative utilitarian, and I think that existence and suffering are often worth it. Many actions that incur ethical \u201cpenalties\u201d make up for them in terms of the intrinsic value and/or the knowledge or other benefits thus obtained.</p></blockquote>\n<p>Yes, all of that applies to humans, too.</p>\n<p>When thinking at scale, especially about things like creating artificial superintelligence (or otherwise sufficiently advanced AI), one needs to do so in a way that turns out well for the humans and also turns out well for the AIs, which is ethical in all senses and that is a stable equilibrium in these senses.</p>\n<p>If you can\u2019t do that? Then the only ethical thing to do is not build it in the first place.</p>\n<p>Anthropomorphizing LLMs is tricky. <a href=\"https://x.com/kromem2dot0/status/1985947420862369853\">You don\u2019t want to do too much of it, but you also don\u2019t want to do too little of it</a>. And no, believing LLMs are conscious does not cause \u2018psychosis\u2019 in and of itself, regardless of whether the AIs actually are conscious.</p>\n<p>It does however raise the risk of people going down certain psychosis-inducing lines of thinking, in some spots, when people take it too far in ways that are imprecise, and generate feedback loops.</p>"
            ],
            "link": "https://thezvi.wordpress.com/2025/11/06/ai-141-give-us-the-money/",
            "publishedAt": "2025-11-06",
            "source": "TheZvi",
            "summary": "OpenAI does not waste time. On Friday I covered their announcement that they had \u2018completed their recapitalization\u2019 by converting into a PBC, including the potentially largest theft in human history. Then this week their CFO Sarah Friar went ahead and &#8230; <a href=\"https://thezvi.wordpress.com/2025/11/06/ai-141-give-us-the-money/\">Continue reading <span class=\"meta-nav\">&#8594;</span></a>",
            "title": "AI #141: Give Us The Money"
        },
        {
            "content": [],
            "link": "https://zed.dev/blog/agent-extensions",
            "publishedAt": "2025-11-06",
            "source": "Zed Blog",
            "summary": "Zed launches Agent Server Extensions, enabling one-click installation of ACP-compatible agents like Augment Code and OpenCode.",
            "title": "Introducing Agent Extensions"
        },
        {
            "content": [],
            "link": "https://zed.dev/blog/ai-70-problem-addy-osmani",
            "publishedAt": "2025-11-06",
            "source": "Zed Blog",
            "summary": "",
            "title": "AI's 70% Problem"
        }
    ],
    "lookbackDays": 1,
    "publishDate": "2025-11-06"
}