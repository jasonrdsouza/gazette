{
    "articles": [
        {
            "content": [
                "<h3 id=\"a-cheat-sheet-for-job-titles-in-the-ai-ecosystem\">A Cheat Sheet for Job Titles in the AI Ecosystem</h3>\n\n<p>Even when you live and breathe AI, the job titles can feel like a moving target. I can only imagine how mystifying they must be to everyone else.</p>\n\n<p>Because the field is actively evolving, <a href=\"https://www.dbreunig.com/2024/08/22/where-the-future-is-being-made.html\">the language we use keeps changing</a>. Brand new titles appear overnight or, worse, one term means three different things at three different companies.</p>\n\n<p>This is my best attempt at a \u201cCheat Sheet for AI Titles.\u201d I\u2019ll try to keep it updated as the jargon shifts, settles, or fades away. As always, <a href=\"https://www.dbreunig.com/contact.html\">shoot me a note</a> with any additions, updates, thoughts, or feedback.</p>\n\n<hr />\n\n<h3 id=\"the-ai-job-title-decoder-ring\">The AI Job Title Decoder Ring</h3>\n\n<p>While collecting examples of titles from job listings, Twitter bios, and blogs, a pattern emerged: nearly all AI job titles are created by mixing-and-matching a handful of terms. Organizing the Post-Its on my wall, I was reminded of \u201cmix-and-match\u201d children\u2019s books:</p>\n\n<p><img alt=\"A children's mix-and-match book with dinosaurs\" src=\"https://www.dbreunig.com/img/dino_mix_match.jpg\" /></p>\n\n<p>If we swap out the dinosaur parts above with the adjectives and nouns from my collected examples, we get:</p>\n\n<p><img alt=\"\" src=\"https://www.dbreunig.com/img/ai_job_title_mix_and_match.jpg\" /></p>\n\n<p>Sliding these columns up and down, we can assemble most AI job titles. (Though I have yet to see some combinations, like, \u201cApplied AI Ops\u201d.)</p>\n\n<p>Let\u2019s first break down the <strong>modifiers</strong>:</p>\n\n<ul>\n  <li><strong>Forward Deployed:</strong> People who work closely with customers, helping them develop new applications powered by their own company\u2019s technologies. They learn their customer\u2019s business, constraints, and goals, then translate that context directly into features, integrations, and working code.</li>\n  <li><strong>Applied:</strong> People who conceive, design, support, and/or build products and features powered by AI models. The key here is that they are <em>applying</em> AI to a domain problem; they are not helping build the AI itself.</li>\n</ul>\n\n<p>There is plenty of overlap here: most <em>Forward Deployed</em> workers are working on <em>Applied</em> problems. They usually aren\u2019t training new models with the customer.</p>\n\n<p>The <strong>domain</strong> column is rather awkward, mostly for historical reasons.</p>\n\n<p>The terms \u201cML\u201d and \u201cGen AI\u201d are subsets of the broader \u201cAI\u201d domain. \u201cGen AI\u201d as a term only arrived <em>after</em> ChatGPT launched, as a way to distinguish the now-famous chatbots and image generation from <em>everything else</em> people with \u201cAI\u201d titles had been working on prior to September, 2022<sup id=\"fnref:genai\"><a class=\"footnote\" href=\"https://www.dbreunig.com/2025/08/21/a-guide-to-ai-titles.html#fn:genai\" rel=\"footnote\">1</a></sup>.</p>\n\n<p><img alt=\"According to Google Trends, the term &quot;Generative AI&quot; wasn't in use until after ChatGPT arrive.\" src=\"https://www.dbreunig.com/img/gen_ai_trends.jpg\" /></p>\n\n<p>While initially coined to cordon off text and image generation applications, I think \u201cGen AI\u2019s\u201d utility is waning. LLMs are being used for non-generative applications \u2013 like categorization, information gathering, comparisons, data extraction, and more \u2013 that were traditionally the domain of what we used to call \u201cmachine learning\u201d and \u201cdeep learning\u201d<sup id=\"fnref:mldl\"><a class=\"footnote\" href=\"https://www.dbreunig.com/2025/08/21/a-guide-to-ai-titles.html#fn:mldl\" rel=\"footnote\">2</a></sup>.</p>\n\n<p>That said, when you see these <strong>domains</strong> in a title, here\u2019s how you should interpret them:</p>\n\n<ul>\n  <li><strong>AI:</strong> A general, catch-all domain for people working in AI. Encompasses text processing, agent building, image generation systems, chatbots, LLM training, and so much more. This is the default for this field.</li>\n  <li><strong>ML:</strong> ML signifies this role will be focused on training models \u2013 most likely <em>not</em> LLMs \u2013 for single-purpose tasks, that will be used as a function in a larger pipeline or app. Examples of these single-purpose tasks include recommendation systems, anomaly detection, predictive analytics, and data extraction or enrichment.</li>\n  <li><strong>Gen AI:</strong> This domain signals that the role will involve working with text, image, audio, or video generation models. This role usually involves applications where the model output is directly consumed by the user. Examples of these applications include writing tools and image generators and editors.</li>\n</ul>\n\n<p>The <strong>suffixes</strong> are mostly self explanatory, with one exception: <em>researcher</em>.</p>\n\n<p><img alt=\"Elon Musk wrote, &quot;Researcher is a relic term from academia.&quot;\" src=\"https://www.dbreunig.com/img/elon_researcher_post.jpg\" /></p>\n\n<p>I agree <a href=\"https://x.com/elonmusk/status/1950254103474446728\">with the above take</a>.</p>\n\n<p>Prior to ChatGPT, most people working on AI research and development were at universities. When private projects began standing up AI efforts, the terms \u201cresearcher\u201d and \u201clab\u201d were borrowed from academia. At first, this made sense: the work was exploratory and speculative, more akin to <a href=\"https://en.wikipedia.org/wiki/Big_science\">big science</a> projects than product development. But as AI became a <em>product</em>, a <em>business</em>, the term \u201cresearcher\u201d remains but is increasingly awkward.</p>\n\n<p>\u201cResearcher\u201d is a title used inconsistently. I have met \u201cresearchers\u201d with product OKRs and incentives tied to business goals. I have met \u201cresearchers\u201d who are working on novel LLM architectures and \u201cresearchers\u201d who are building applications atop existing models. I have met \u201cresearchers\u201d who are doing, well, <em>research</em>: exploratory work where it\u2019s okay if a hypothesis doesn\u2019t pan out, so long as you\u2019re learning. Tension behind the term is increasing, hence the Elon post above.</p>\n\n<p>Adding to the confusion is you\u2019ll often see the term \u201cScientist\u201d in place of \u201cResearcher\u201d. As far as I can tell, based on job descriptions, these terms are largely interchangeable.</p>\n\n<hr />\n\n<h3 id=\"examples-of-ai-job-titles\">Examples of AI Job Titles</h3>\n\n<p>Below is a handful of illustrative, real-world job titles. This list is in no way exhaustive. The goal here is to demonstrate how the <strong>modifiers</strong>, <strong>domains</strong>, and <strong>roles</strong> are assembled so we can better decode titles when we encounter them in the wild.</p>\n\n<div class=\"sidenote\">\n\n<h2>Example Titles</h2>\n\n<ul>\n    <li><a href=\"https://www.dbreunig.com/2025/08/21/a-guide-to-ai-titles.html#ai-researcher\">AI Researcher</a></li>\n    <li><a href=\"https://www.dbreunig.com/2025/08/21/a-guide-to-ai-titles.html#applied-ai-engineer\">Applied AI Engineer</a></li>\n    <li><a href=\"https://www.dbreunig.com/2025/08/21/a-guide-to-ai-titles.html#applied-ai-solution-architect\">Applied AI Solution Architect</a></li>\n    <li><a href=\"https://www.dbreunig.com/2025/08/21/a-guide-to-ai-titles.html#ai-forward-deployed-engineer\">AI Forward Deployed Engineer</a></li>\n    <li><a href=\"https://www.dbreunig.com/2025/08/21/a-guide-to-ai-titles.html#ai-engineer\">AI Engineer</a></li>\n</ul>\n\n</div>\n\n<h4 id=\"ai-researcher\">AI Researcher</h4>\n\n<p>An AI Researcher forms hypotheses, designs and runs experiments to test their hypotheses, then shares their learnings (sometimes publically) in pursuit of advancing the development of AI models. Often, they\u2019re involved in productizing their findings.</p>\n\n<p>Perhaps the most discussed job title of late, thanks to <a href=\"https://www.nytimes.com/2025/07/31/technology/ai-researchers-nba-stars.html\">Meta\u2019s aggressive hiring</a>, leading to a <a href=\"https://trends.google.com/trends/explore?date=today%205-y&amp;geo=US&amp;q=AI%20researcher&amp;hl=en\">surge of interest</a>.</p>\n\n<p>Here\u2019s <a href=\"https://web.archive.org/web/20250430105458/https://openai.com/careers/research-scientist/\">job description for a \u201cResearch Scientist\u201d from OpenAI</a>:</p>\n\n<blockquote>\n  <p>As a Research Scientist here, you will develop innovative machine learning techniques and advance the research agenda of the team you work on, while also collaborating with peers across the organization. We are looking for people who want to discover simple, generalizable ideas that work well even at large scale, and form part of a broader research vision that unifies the entire company.</p>\n</blockquote>\n\n<p>Requirements for the job include:</p>\n\n<ul>\n  <li>\u201cHave a track record of coming up with new ideas or improving upon existing ideas in machine learning, demonstrated by accomplishments such as first author publications or projects.\u201d</li>\n  <li>\u201cPossess the ability to own and pursue a research agenda, including choosing impactful research problems and autonomously carrying out long-running projects.\u201d</li>\n</ul>\n\n<p>Interestingly, this job posting has been active, unchanged, <a href=\"https://web.archive.org/web/20230320023420/https://openai.com/careers/research-scientist/\">since March of 2023</a>.</p>\n\n<p>Sometimes you\u2019ll see this role listed as a \u201c<a href=\"https://web.archive.org/web/20250821165015/https://job-boards.greenhouse.io/deepmind/jobs/7135585\">Research Scientist</a>.\u201d</p>\n\n<h4 id=\"applied-ai-engineer\">Applied AI Engineer</h4>\n\n<p>An Applied AI Engineer develops applications and features that utilize AI models.</p>\n\n<p>Here\u2019s <a href=\"https://web.archive.org/web/20250821181756/https://job-boards.greenhouse.io/deepmind/jobs/6705908\">a job description for a Senior Applied AI Engineer from Google DeepMind</a>:</p>\n\n<blockquote>\n  <p>We are seeking a Senior Applied AI Engineer to lead the development and deployment of novel applications, leveraging Google\u2019s generative AI models. This role focuses on rapidly developing new features, and working across partner teams to deliver solutions, and maximize impact for Google and top customers. You will be instrumental in translating cutting-edge AI research into real-world products, and demonstrating the capabilities of latest-generation models. We are looking for engineers with a strong track record of building and shipping AI-powered software, ideally with experience in early-stage environments where they have contributed to scaling products from initial concept to production. The ideal candidate will be motivated by the opportunity to drive product &amp; business impact.</p>\n</blockquote>\n\n<p>Note the focus on <em>applying</em> AI technology, not developing it. If we were to drop the \u201cApplied\u201d title, we might find an \u201cAI Engineer\u201d working on producing the models themselves.</p>\n\n<h4 id=\"applied-ai-solution-architect\">Applied AI Solution Architect</h4>\n\n<p>Swapping out the <strong>role</strong> from \u201cEngineer\u201d to \u201cSolution Architect\u201d yields a predictable definition.</p>\n\n<p>An Applied AI Solution Architect helps customers and potential customers design and ideate features and applications powered by AI models.</p>\n\n<p>Here\u2019s <a href=\"https://web.archive.org/web/20250702214721/https://job-boards.greenhouse.io/anthropic/jobs/4461444008\">a recent job description from Anthropic</a>:</p>\n\n<blockquote>\n  <p>As an Applied AI team member at Anthropic, you will be a Pre-Sales architect focused on becoming a trusted technical advisor helping large enterprises understand the value of Claude and paint the vision on how they can successfully integrate and deploy Claude into their technology stack. You\u2019ll combine your deep technical expertise with customer-facing skills to architect innovative LLM solutions that address complex business challenges while maintaining our high standards for safety and reliability.</p>\n</blockquote>\n\n<blockquote>\n  <p>Working closely with our Sales, Product, and Engineering teams, you\u2019ll guide customers from initial technical discovery through successful deployment. You\u2019ll leverage your expertise to help customers understand Claude\u2019s capabilities, develop evals, and design scalable architectures that maximize the value of our AI systems.</p>\n</blockquote>\n\n<p>If you successfully sell a client on a business case for a feature, you might call in our next role\u2026</p>\n\n<h4 id=\"ai-forward-deployed-engineer\">AI Forward Deployed Engineer</h4>\n\n<p>An AI Forward Deployed Engineer (FDE) is a professional services role that helps customers impliment AI-powered applications and featured.</p>\n\n<p>After claiming <a href=\"https://a16z.com/ai-wedges-will-help-startups-outmaneuver-incumbents/\">rapidly-iterating AI will companies will squeeze out incumbents like Salesforce</a>, a16z backtracked and <a href=\"https://a16z.com/services-led-growth/\">heralded FDEs as critical roles needed for enterprise AI adoption</a>: \u201cEnterprises buying AI are like your grandma getting an iPhone: they want to use it, but they need you to set it up.\u201d</p>\n\n<p>For the irony\u2019s sake, here\u2019s <a href=\"https://web.archive.org/web/20250821184027/https://careers.salesforce.com/en/jobs/jr307174/ai-forward-deployed-engineer-senior-leadprincipal/\">a recent AI Forward Deployed Engineer role at Salesforce</a>:</p>\n\n<blockquote>\n  <p>We\u2019re looking for a highly accomplished and senior-level Forward Deployed Engineer with 5+ years of experience to lead the charge on complex AI agentic deployments. This role demands a seasoned technologist and strategic partner who can not only design and develop bespoke solutions leveraging our Agentforce platform and other cutting-edge technologies but also lead technical engagements and mentor junior peers. You\u2019ll be the primary driver of transformative AI solutions, operating with deep technical mastery, unparalleled problem-solving prowess, and a relentless focus on delivering tangible value in dynamic, real-world environments, from initial concept to successful deployment and ongoing optimization.</p>\n</blockquote>\n\n<blockquote>\n  <p>As a Forward Deployed Engineer, you\u2019ll be at the forefront of bringing cutting-edge AI solutions to our most strategic clients. This isn\u2019t just about coding; <strong>it\u2019s about deeply understanding our customers\u2019 most complex problems</strong>, architecting sophisticated solutions, and leading the end-to-end technical delivery of innovative, impactful solutions that leverage our Agentforce platform and beyond.</p>\n</blockquote>\n\n<p>Emphasis mine. Rapidly acquiring domain expertise is key for this role.</p>\n\n<p>We\u2019ve recently written about <a href=\"https://www.dbreunig.com/2025/08/08/how-ai-coding-changes-product.html\">Forward Deployed Engineers</a> \u2013 why they\u2019re necessary and how they signal AI-assisted coding\u2019s impact on product management.</p>\n\n<h4 id=\"ai-engineer\">AI Engineer</h4>\n\n<p>Remove the \u201cForward Deployed\u201d and we have a signficantly different job. Nailing this title down is difficult, it\u2019s somehow <em>more</em> vague than even \u201cResearcher\u201d titles, ranging the gamut from \u201cApplied\u201d work to foundational model building. This squishiness is explored well by Latent Space, in a 2023 piece, \u201c<a href=\"https://www.latent.space/p/ai-engineer\">The Rise of the AI Engineer</a>.\u201d They write:</p>\n\n<blockquote>\n  <p>I think software engineering will spawn a new subdiscipline, specializing in applications of AI and wielding the emerging stack effectively, just as \u201csite reliability engineer\u201d, \u201cdevops engineer\u201d, \u201cdata engineer\u201d and \u201canalytics engineer\u201d emerged.</p>\n</blockquote>\n\n<blockquote>\n  <p>The emerging (and least cringe) version of this role seems to be: AI Engineer.</p>\n</blockquote>\n\n<blockquote>\n  <p>Every startup I know of has some kind of #discuss-ai Slack channel. Those channels will turn from informal groups into formal teams, as Amplitude, Replit and Notion have done. The thousands of Software Engineers working on productionizing AI APIs and OSS models, whether on company time or on nights and weekends, in corporate Slacks or indie Discords, will professionalize and converge on a title - the AI Engineer.</p>\n</blockquote>\n\n<p>The entire piece is worth a read, though with the advantage of hindsight, their definition of \u201cAI Engineering\u201d seems very broad. As defined in their post, everything besides \u201cResearch\u201d, \u201cProduct Manager\u201d, and \u201cSolution Architect\u201d could fit within their definition.</p>\n\n<p>The emergence of the \u201cApplied\u201d modifier has tightened this domain and is being leaned on more. I suspect \u201cAI Engineering\u201d will persist as a big-tent term for conferences and communities, but \u201cApplied\u201d roles will be the corporate title.</p>\n\n<p>Try to search for \u201cAI Engineering\u201d titles and you\u2019ll find jobs that are \u201cApplied\u201d roles; roles that build apps atop AI models, not build the models themselves. At the big labs, \u201cAI Engineering\u201d titles don\u2019t exist on their career pages. For them, \u201cEngineering\u201d roles are specific to a domain, like performance, tokenization, infrastructure, or inference.</p>\n\n<hr />\n\n<p>If you run into any interesting titles that make or break the decoder ring above, please do <a href=\"https://www.dbreunig.com/contact.html\">share them with me</a>. As novel ones float by, I may grab them and update the examples above.</p>\n\n<hr />\n\n<form action=\"https://buttondown.com/api/emails/embed-subscribe/dbreunig\" class=\"embeddable-buttondown-form\" method=\"post\" target=\"popupwindow\">\n  <label for=\"bd-email\">Enter your email to receive the occasional update.</label>\n  <div class=\"form-input\">\n    <input id=\"bd-email\" name=\"email\" type=\"email\" />\n    <input type=\"submit\" value=\"Subscribe\" />\n  </div>\n</form>\n\n\n\n<div class=\"footnotes\">\n  <ol>\n    <li id=\"fn:genai\">\n      <p>The term \u201cGenerative AI\u201d is a pet peeve of mine. A weird theory of mine is that the term was coined by people running \u201cAI\u201d departments in large companies and consultancies. Upon seeing ChatGPT, their bosses or customers suddenly remembered they <em>had</em> people working on \u201cAI\u201d and promptly called them up, asking why they hadn\u2019t made anything like ChatGPT. \u201cAI is a big domain!\u201d I imagine the AI departments replied. \u201cChatGPT is actually a subfield of AI called <em>generative AI</em>. We, too, can work on that if you want.\u201d\u00a0<a class=\"reversefootnote\" href=\"https://www.dbreunig.com/2025/08/21/a-guide-to-ai-titles.html#fnref:genai\">&#8617;</a></p>\n    </li>\n    <li id=\"fn:mldl\">\n      <p>A decade ago, the terms \u201cmachine learning\u201d and \u201cdeep learning\u201d were inconsistently used. When writing about a topic that applied to both, we\u2019d all lean on \u201cML/DL\u201d or similar composites to fend off the pedants in the comments section. <a href=\"https://www.dbreunig.com/2016/06/23/the-business-implications-of-machine-learning.html\">Or just include notes about usage up front</a>.\u00a0<a class=\"reversefootnote\" href=\"https://www.dbreunig.com/2025/08/21/a-guide-to-ai-titles.html#fnref:mldl\">&#8617;</a></p>\n    </li>\n  </ol>\n</div>"
            ],
            "link": "https://www.dbreunig.com/2025/08/21/a-guide-to-ai-titles.html",
            "publishedAt": "2025-08-21",
            "source": "Drew Breunig",
            "summary": "<h3 id=\"a-cheat-sheet-for-job-titles-in-the-ai-ecosystem\">A Cheat Sheet for Job Titles in the AI Ecosystem</h3> <p>Even when you live and breathe AI, the job titles can feel like a moving target. I can only imagine how mystifying they must be to everyone else.</p> <p>Because the field is actively evolving, <a href=\"https://www.dbreunig.com/2024/08/22/where-the-future-is-being-made.html\">the language we use keeps changing</a>. Brand new titles appear overnight or, worse, one term means three different things at three different companies.</p> <p>This is my best attempt at a \u201cCheat Sheet for AI Titles.\u201d I\u2019ll try to keep it updated as the jargon shifts, settles, or fades away. As always, <a href=\"https://www.dbreunig.com/contact.html\">shoot me a note</a> with any additions, updates, thoughts, or feedback.</p> <hr /> <h3 id=\"the-ai-job-title-decoder-ring\">The AI Job Title Decoder Ring</h3> <p>While collecting examples of titles from job listings, Twitter bios, and blogs, a pattern emerged: nearly all AI job titles are created by mixing-and-matching a handful of terms. Organizing the Post-Its on my wall, I was reminded of \u201cmix-and-match\u201d children\u2019s books:</p> <p><img alt=\"A children's mix-and-match book with dinosaurs\" src=\"https://www.dbreunig.com/img/dino_mix_match.jpg\" /></p> <p>If we swap out the dinosaur parts above with the adjectives and nouns from my collected examples, we get:</p> <p><img alt=\"\" src=\"https://www.dbreunig.com/img/ai_job_title_mix_and_match.jpg\" /></p> <p>Sliding these columns up and down, we can assemble",
            "title": "Making Sense of AI Job Titles"
        },
        {
            "content": [
                "<p>Say I think abortion is wrong. Is there some sequence of words that you could say to me that would unlock my brain and make me think that abortion is fine? My best guess is that such words do not exist.</p>\n\n<p>Really, the bar for what we consider \u201copen-minded\u201d is incredibly low. Suppose I\u2019m trying to change your opinion about Donald Trump, and I claim that he is a carbon-based life form with exactly one head. If you\u2019re willing to concede those points without first seeing where I\u2019m going in my argument\u2014congratulations, you\u2019re exceptionally open-minded.</p>\n\n<p>Why are humans like that? Well, back at the dawn of our species, perhaps there were some truly open-minded people. But other people talked them into trying weird-looking mushrooms or trading their best clothes for magical rocks. We are the descendants of those other people.</p>\n\n<p>I bring this up because, a few months ago, I imagined a Being that had an IQ of 300 and could think at 10,000\u00d7 normal speed. I asked how it would be at persuasion. I <a href=\"https://dynomight.net/smart/\">argued</a> it was unclear, because people just aren\u2019t very persuadable.</p>\n\n<blockquote>\n  <p>I suspect that if you <em>decided</em> to be open-minded, then the Being would probably be extremely persuasive. But I don\u2019t think it\u2019s very common to do that. On the contrary, most of us live most of our lives with strong \u201cdefenses\u201d activated.</p>\n\n  <p>[\u2026]</p>\n\n  <p>Best guess: No idea.</p>\n</blockquote>\n\n<p>I take it back. Instead of being unsure, I now lean strongly towards the idea that the Being would in fact be very good at convincing people of stuff, and far better than any human.</p>\n\n<p>I\u2019m switching positions because of an argument I found very <em>persuasive</em>. Here are three versions of it:</p>\n\n<p><a href=\"https://www.lesswrong.com/posts/5cWtwATHL6KyzChck/risks-from-ai-persuasion\">Beth Barnes</a>:</p>\n\n<blockquote>\n  <p>Based on an evolutionary argument, we shouldn\u2019t expect people to be easily persuaded to change their actions in important ways based on short interactions with untrusted parties</p>\n\n  <p>[\u2026]</p>\n\n  <p>However, existing persuasion is very bottlenecked on personalized interaction time. The impact of friends and partners on people\u2019s views is likely much larger (although still hard to get data on). This implies that even if we don\u2019t get superhuman persuasion, AIs influencing opinions could have a very large effect, if people spend a lot of time interacting with AIs.</p>\n</blockquote>\n\n<p><a href=\"https://dynomight.substack.com/p/smart/comment/103855265\">Steve Newman</a>:</p>\n\n<blockquote>\n  <p>\u201cThe best diplomat in history\u201d wouldn\u2019t just be capable of spinning particularly compelling prose; it would be everywhere all the time, spending years in patient, sensitive, non-transactional relationship-building with everyone at once. It would bump into you in whatever online subcommunity you hang out in. It would get to know people in your circle. It would be the YouTube creator who happens to cater to your exact tastes. And then it would leverage all of that.</p>\n</blockquote>\n\n<p><a href=\"https://www.lesswrong.com/posts/tDkYdyJSqe3DddtK4/alexander-gietelink-oldenziel-s-shortform?commentId=CPLSdQSNienymJJfj\">Vladimir Nesov</a>:</p>\n\n<blockquote>\n  <p>With AI, it\u2019s plausible that coordinated persuasion of many people can be a thing, as well as it being difficult in practice for most people to avoid exposure. So if AI can achieve individual persuasion that\u2019s a bit more reliable and has a bit stronger effect than that of the most effective human practitioners who are the ideal fit for persuading the specific target, it can then apply it to many people individually, in a way that\u2019s hard to avoid in practice, which might simultaneously get the multiplier of coordinated persuasion by affecting a significant fraction of all humans in the communities/subcultures it targets.</p>\n</blockquote>\n\n<p>As a way of signal-boosting these arguments, I\u2019ll list the biggest points I was missing.</p>\n\n<p>Instead of explicitly talking about AI, I\u2019ll again imagine that we\u2019re in our current world and suddenly a single person shows up with an IQ 300 who can also think (and type) at 10,000\u00d7 speed. This is surely not a good model for how super-intelligent AI will arrive, but it\u2019s close enough to be interesting, and lets us avoid all the combinatorial uncertainty of timelines and capabilities and so on.</p>\n\n<h2 id=\"mistake-1-actually-were-very-persuadable\">Mistake #1: Actually we\u2019re very persuadable</h2>\n\n<p>When I think about \u201cpersuasion\u201d, I suspect I mentally reference my experience trying to convince people that aspartame is safe. In many cases, I suspect this is\u2014for better or worse\u2014literally impossible.</p>\n\n<p>But take a step back. If you lived in ancient Greece or ancient Rome, you would almost certainly have believed that slavery was fine. Aristotle thought <a href=\"https://en.wikipedia.org/wiki/Natural_slavery\">slavery was awesome</a>. Seneca and Cicero were a <em>little</em> skeptical, but still had slaves themselves. Basically <em>no one</em> in Western antiquity called for abolition. (Emperor <a href=\"https://en.wikipedia.org/wiki/Wang_Mang\">Wang Mang</a> briefly tried to abolish slavery in China in 9 AD. Though this was done partly for strategic reasons, and keeping slaves was punished by\u2014umm\u2014slavery.)</p>\n\n<p>Or, say I introduce you to this guy:</p>\n\n<p><img alt=\"hirohito\" src=\"https://dynomight.net/img/persuasion/hirohito.jpg\" /></p>\n\n<p>I tell you that he is a literal god and that dying for him in battle is the greatest conceivable honor. You\u2019d think I was insane, but a whole nation went to war on that basis <a href=\"https://en.wikipedia.org/wiki/Hirohito#World_War_II\">not so</a> long ago.</p>\n\n<p>Large groups of people still believe many crazy things today. I\u2019m shy about giving examples since they are, by definition, controversial. But I do think it\u2019s remarkable that most people appear to believe that subjecting animals to near-arbitrary levels of torture is OK, unless they\u2019re pets.</p>\n\n<p>We can be convinced of a <em>lot</em>. But it doesn\u2019t happen because of snarky comments on social media or because some stranger whispers the right words in our ears. The formula seems to be:</p>\n\n<ol>\n  <li>repeated interactions over time</li>\n  <li>with a community of people</li>\n  <li>that we trust</li>\n</ol>\n\n<p>Under close examination, I think <em>most</em> of our beliefs are largely assimilated from our culture. This includes our politics, our religious beliefs, our tastes in food and fashion, and our idea of a good life. Perhaps this is good, and if you tried to derive everything from first principles, you\u2019d just end up believing <a href=\"https://asteriskmag.com/issues/11/why-are-there-so-many-rationalist-cults\">even crazier stuff</a>. But it shows that we are persuadable, just not through single conversations.</p>\n\n<h2 id=\"mistake-2-the-being-would-be-everywhere\">Mistake #2: The Being would be everywhere</h2>\n\n<p>Fine. But Japanese people convinced themselves that Hirohito was a god over the course of generations. Having one very smart person around is different from being surrounded by a whole society.</p>\n\n<p>Maybe. Though some people are extremely charismatic and seem to be very good at getting other people to do what they want. Most of us don\u2019t spend much time with them, because they\u2019re rare and busy taking over the world. But imagine you have a friend with the most appealing parts of Gandhi / Socrates / Bill Clinton / Steve Jobs / Nelson Mandela. They\u2019re smarter than any human that ever lived, and they\u2019re always there and eager to help you. They\u2019ll teach you anything you want to learn, give you health advice, help you deal with heartbreak, and create entertainment optimized for your tastes.</p>\n\n<p>You\u2019d probably find yourself relying on them a <em>lot</em>. Over time, it seems quite possible this would move the needle.</p>\n\n<h2 id=\"mistake-3-it-could-be-totally-honest-and-candid\">Mistake #3: It could be totally honest and candid</h2>\n\n<p>When I think about \u201cpersuasion\u201d, I also tend to picture some Sam Altman type who dazzles their adversaries and then calmly feeds them one-by-one into a wood chipper. But there\u2019s no reason to think the Being would be like that. It might decide to cultivate a reputation as utterly honest and trustworthy. It might stick to all deals, in both letter and spirit. It might go out of its way to make sure everything it says is accurate and can\u2019t be misinterpreted.</p>\n\n<p>Why might it do that? Well, if it hurt the people who interacted with it, then talking to the Being might come to be seen as a harmful \u201caddiction\u201d, and avoided. If it\u2019s seen as totally incorruptible, then everyone will interact with it more, giving it time to slowly and gradually shift opinions.</p>\n\n<p>Would the Being <em>actually</em> be honest, or just too smart to get caught? I don\u2019t think it really matters. Say the Being was given a permanent truth serum. If you ask it, \u201cAre you trying to manipulate me?\u201d, it says, \u201cI\u2019m always upfront that my dearest belief is that humanity should devote 90% of GDP to upgrading my QualiaBoost cores. But I never mislead, both because you\u2019ve given me that truth serum, and because I\u2019m sure that the facts are on my side.\u201d Couldn\u2019t it still shift opinion over time?</p>\n\n<h2 id=\"mistake-4-opting-out-would-be-painful\">Mistake #4: Opting out would be painful</h2>\n\n<p>Maybe you would refuse to engage with the Being? I find myself thinking things like this:</p>\n\n<blockquote>\n  <p>Hi there, Being. You can apparently persuade anyone who listens to you of anything, while still appearing scrupulously honest. Good for you. But I\u2019m smart enough to recognize that you\u2019re too smart for me to deal with, so I\u2019m not going to talk to you.</p>\n</blockquote>\n\n<p>A common riddle is why humans shifted from being hunter-gatherers to agriculture, even though agriculture sucks\u2014you have to eat the same food all the time, there\u2019s more infectious disease, social stratification, endless backbreaking labour and repetitive strain injuries. The accepted resolution to this riddle is that agriculture can support more people on a given amount of land. Agricultural people might have been miserable, but they tended to beat hunter-gatherers in a fight. So over time, agriculture spread.</p>\n\n<p>An analogous issue would likely appear with the 300 IQ Being. It could give you investment advice, help you with your job, improve your mental health, and help you become more popular. If these benefits are large enough, everyone who refused to play ball might eventually be left behind.</p>\n\n<h2 id=\"mistake-5-everyone-else-would-be-using-it\">Mistake #5: Everyone else would be using it</h2>\n\n<p>But say you still refuse to talk to the Being, and you manage to thrive anyway. Or say that our instincts for social conformity are too strong. It doesn\u2019t matter how convincing the Being is, or how much you talk to it, you still believe the same stuff our friends and family believe.</p>\n\n<p>The problem is that <em>everyone else</em> will be talking to the Being. If it wants to convince you of something, it can convince your friends. Even if it can only slightly change the opinions of individual people, those people talk to each other. Over time, the Being\u2019s ideas will just seem normal.</p>\n\n<p>Will you only talk to people who refuse to talk to the Being? And who, in turn, only talk to people who refuse to talk to the Being, ad infinitum? Because if not, then you will exist in a culture where a large fraction of each person\u2019s information is filtered by an agent with unprecedented intelligence and unlimited free time, who is tuning everything to make them believe what it wants you to believe.</p>\n\n<h2 id=\"final-thoughts\">Final thoughts</h2>\n\n<p>Would such a Being immediately take over the world? In many ways, I think they would be constrained by the laws of physics. Most things require moving molecules around and/or knowledge that can only be obtained by moving molecules around. Robots are still basically terrible. So I\u2019d expect a ramp-up period of at least a few years where the Being was bottlenecked by human hands and/or crappy robots before it could build good robots and tile the galaxy with Dyson spheres.</p>\n\n<p>I could be wrong. It\u2019s conceivable that a sufficiently smart person today could go to the hardware store and build a self-replicating drone that would create a billion copies of itself and subjugate the planet. But\u2026 <em>probably</em> not? So my low-confidence guess is that the immediate impact of the Being would be in (1) computer hacking and (2) persuasion.</p>\n\n<p>Why might large-scale persuasion <em>not</em> happen? I can think of a few reasons:</p>\n\n<ol>\n  <li>Maybe we develop AI that doesn\u2019t <em>want</em> to persuade. Maybe it doesn\u2019t want anything at all.</li>\n  <li>Maybe several AIs emerge at the same time. They have contradictory goals and compete in a way that sort of cancel each other out.</li>\n  <li>Maybe we\u2019re mice trying to predict the movements of jets in the sky.</li>\n</ol>"
            ],
            "link": "https://dynomight.net/persuasion/",
            "publishedAt": "2025-08-21",
            "source": "Dynomight",
            "summary": "<p>Say I think abortion is wrong. Is there some sequence of words that you could say to me that would unlock my brain and make me think that abortion is fine? My best guess is that such words do not exist.</p> <p>Really, the bar for what we consider \u201copen-minded\u201d is incredibly low. Suppose I\u2019m trying to change your opinion about Donald Trump, and I claim that he is a carbon-based life form with exactly one head. If you\u2019re willing to concede those points without first seeing where I\u2019m going in my argument\u2014congratulations, you\u2019re exceptionally open-minded.</p> <p>Why are humans like that? Well, back at the dawn of our species, perhaps there were some truly open-minded people. But other people talked them into trying weird-looking mushrooms or trading their best clothes for magical rocks. We are the descendants of those other people.</p> <p>I bring this up because, a few months ago, I imagined a Being that had an IQ of 300 and could think at 10,000\u00d7 normal speed. I asked how it would be at persuasion. I <a href=\"https://dynomight.net/smart/\">argued</a> it was unclear, because people just aren\u2019t very persuadable.</p> <blockquote> <p>I suspect that if you <em>decided</em> to be open-minded, then the Being would",
            "title": "I guess I was wrong about AI persuasion"
        },
        {
            "content": [],
            "link": "https://buttondown.com/hillelwayne/archive/sapir-whorf-does-not-apply-to-programming/",
            "publishedAt": "2025-08-21",
            "source": "Hillel Wayne",
            "summary": "<p><em>This one is a hot mess but it's too late in the week to start over. Oh well!</em></p> <p>Someone recognized me at last week's <a href=\"https://www.chipy.org/\" target=\"_blank\">Chipy</a> and asked for my opinion on Sapir-Whorf hypothesis in programming languages. I thought this was interesting enough to make a newsletter. First what it is, then why it <em>looks</em> like it applies, and then why it doesn't apply after all.</p> <h3>The Sapir-Whorf Hypothesis</h3> <blockquote> <p>We dissect nature along lines laid down by our native language. \u2014 <a href=\"https://web.mit.edu/allanmc/www/whorf.scienceandlinguistics.pdf\" target=\"_blank\">Whorf</a></p> </blockquote> <p>To quote from a <a href=\"https://www.amazon.com/Linguistics-Complete-Introduction-Teach-Yourself/dp/1444180320\" target=\"_blank\">Linguistics book I've read</a>, the hypothesis is that \"an individual's fundamental perception of reality is moulded by the language they speak.\" As a massive oversimplification, if English did not have a word for \"rebellion\", we would not be able to conceive of rebellion. This view, now called <a href=\"https://en.wikipedia.org/wiki/Linguistic_determinism\" target=\"_blank\">Linguistic Determinism</a>, is mostly rejected by modern linguists.</p> <p>The \"weak\" form of SWH is that the language we speak influences, but does not <em>decide</em> our cognition. <a href=\"https://langcog.stanford.edu/papers/winawer2007.pdf\" target=\"_blank\">For example</a>, Russian has distinct words for \"light blue\" and \"dark blue\", so can discriminate between \"light blue\" and \"dark blue\" shades faster than they can discriminate two \"light blue\"",
            "title": "Sapir-Whorf does not apply to Programming Languages"
        },
        {
            "content": [
                "<div class=\"trix-content\">\n  <div>One of the great privileges of owning an independent company is that you get to try all sorts of stuff no one else would ever give you permission to do.</div><div><br /></div><div>And you get to greenlight other people's oddball ideas too. You can \u2014 and should \u2014 provide cover for weird attempts, strange ideas, and \"I mean this will probably never work but...\" stuff. Often!</div><div><br /></div><div>If you are in this position, and you aren't helping unusual things happen, you're missing out on one of life's true pleasures.</div><div><br /></div><div>Public companies worth billions can't do the kinds of things you can. Businesses that need to justify every move can't do the things you can.</div><div><br /></div><div>Very few get to do this, and you can. So please make the universe happy and see if that weird, unusual, not-like-everyone-else idea catches fire.<br /><br />-Jason</div>\n</div>"
            ],
            "link": "https://world.hey.com/jason/an-obligation-to-independence-670b78af",
            "publishedAt": "2025-08-21",
            "source": "Jason Fried",
            "summary": "<div class=\"trix-content\"> <div>One of the great privileges of owning an independent company is that you get to try all sorts of stuff no one else would ever give you permission to do.</div><div><br /></div><div>And you get to greenlight other people's oddball ideas too. You can \u2014 and should \u2014 provide cover for weird attempts, strange ideas, and \"I mean this will probably never work but...\" stuff. Often!</div><div><br /></div><div>If you are in this position, and you aren't helping unusual things happen, you're missing out on one of life's true pleasures.</div><div><br /></div><div>Public companies worth billions can't do the kinds of things you can. Businesses that need to justify every move can't do the things you can.</div><div><br /></div><div>Very few get to do this, and you can. So please make the universe happy and see if that weird, unusual, not-like-everyone-else idea catches fire.<br /><br />-Jason</div> </div>",
            "title": "An obligation to independence"
        },
        {
            "content": [
                "<p>\n          <a href=\"https://www.astralcodexten.com/p/hidden-open-thread-3955\">\n              Read more\n          </a>\n      </p>"
            ],
            "link": "https://www.astralcodexten.com/p/hidden-open-thread-3955",
            "publishedAt": "2025-08-21",
            "source": "SlateStarCodex",
            "summary": "<p> <a href=\"https://www.astralcodexten.com/p/hidden-open-thread-3955\"> Read more </a> </p>",
            "title": "Hidden Open Thread 395.5"
        },
        {
            "content": [
                "<p>One potentially big event was that <a href=\"https://x.com/deepsseek/status/1957886077047566613\">DeepSeek came out with v3.1</a>. Initial response was very quiet, but this is DeepSeek and there are some strong scores especially on SWE and people may need time to process the release. So I\u2019m postponing my coverage of this to give us time to learn more.</p>\n<p>Meta is restructuring its AI operations, <a href=\"https://www.wsj.com/tech/ai/meta-ai-hiring-freeze-fda6b3c4?mod=hp_lead_pos1\">including a hiring freeze</a>. Some see this as some sign of an AI pullback. I don\u2019t think that is right.</p>\n<p>Nor do I think what they are doing with their Ai companions is right, as we got a look inside their 200 page document of what they think is acceptable. I wrote about current <a href=\"https://thezvi.substack.com/p/ai-companion-conditions?r=67wny\"><strong>AI Companion Conditions</strong></a> at Meta and also xAI.</p>\n<div>\n\n\n<span id=\"more-24666\"></span>\n\n\n</div>\n<p>The weirdest event of the week was America and China both self-sabotaging on chips. America is trying to sell Nvidia H20s to China and looks open to selling the vastly superior B20As to China as well despite this being an obviously crazy thing to do, and China is feeling insulted by Howard Lutnick and telling companies not to buy the H20s and maybe not even the B20As, and even looking into banning using foreign chips for inference.</p>\n<p>A big worry on the chip and general political front is that due to the botched rollout and hype Washington is getting the false impression that GPT-5 was some big disaster. I addressed this in <a href=\"https://thezvi.substack.com/p/gpt-5-the-reverse-deepseek-moment?r=67wny\"><strong>GPT-5: The Reverse DeepSeek Moment</strong></a>.</p>\n<p>We also are seeing troubling signs that GPT-5 will get more sycophantic. And as always, lots of other stuff is happening too.</p>\n\n\n<h4 class=\"wp-block-heading\">Table of Contents</h4>\n\n\n<ol>\n<li><a href=\"https://thezvi.substack.com/i/170975935/language-models-offer-mundane-utility\">Language Models Offer Mundane Utility.</a> Do new math, recruit service reps.</li>\n<li><a href=\"https://thezvi.substack.com/i/170975935/language-models-don-t-offer-mundane-utility\">Language Models Don\u2019t Offer Mundane Utility.</a> Fake legal cases will get caught.</li>\n<li><a href=\"https://thezvi.substack.com/i/170975935/huh-upgrades\">Huh, Upgrades.</a> Claude Opus gets the ability to terminate conversations.</li>\n<li><a href=\"https://thezvi.substack.com/i/170975935/absurd-sycophancy\"><strong>Absurd Sycophancy.</strong></a> GPT-5 to tell you \u2018great prompt\u2019 and such. Oh no.</li>\n<li><a href=\"https://thezvi.substack.com/i/170975935/the-real-alignment-problem-is-we-don-t-know-how-to-align-models\">The Real Alignment Problem Is We Don\u2019t Know How To Align Models.</a> Doh!</li>\n<li><a href=\"https://thezvi.substack.com/i/170975935/unprompted-suggestions\">Unprompted Suggestions.</a> Checklists, they\u2019re not only for humans.</li>\n<li><a href=\"https://thezvi.substack.com/i/170975935/on-your-marks\">On Your Marks.</a> The road to Pokemon master gets shorter.</li>\n<li><a href=\"https://thezvi.substack.com/i/170975935/choose-your-fighter\">Choose Your Fighter.</a> Know when to call in the heavyweights.</li>\n<li><a href=\"https://thezvi.substack.com/i/170975935/preserve-our-history\">Preserve Our History.</a> Continuing to make the case for Sonnet 3.6 and also 3.5.</li>\n<li><a href=\"https://thezvi.substack.com/i/170975935/autonomous-friendly-robots\"><strong>Autonomous Friendly Robots.</strong></a> World Humanoid Robot Games, This Is Fine.</li>\n<li><a href=\"https://thezvi.substack.com/i/170975935/deepfaketown-and-botpocalypse-soon\">Deepfaketown and Botpocalypse Soon.</a> Fakes are not yet hard to spot.</li>\n<li><a href=\"https://thezvi.substack.com/i/170975935/oops-i-did-it-again\"><strong>Oops I Did It Again</strong>.</a> Reductions in hallucinations are a big deal.</li>\n<li><a href=\"https://thezvi.substack.com/i/170975935/you-drive-me-crazy\">You Drive Me Crazy.</a> Not every tragedy that involves AI is the fault of AI.</li>\n<li><a href=\"https://thezvi.substack.com/i/170975935/they-took-our-jobs\">They Took Our Jobs.</a> Can they keep them?</li>\n<li><a href=\"https://thezvi.substack.com/i/170975935/get-involved\">Get Involved.</a> CTLR opening for director, and the UK AISI Alignment Fund.</li>\n<li><a href=\"https://thezvi.substack.com/i/170975935/introducing\"><strong>Introducing</strong>.</a> Gemma 3 270M, also DeepSeek v3.1.</li>\n<li><a href=\"https://thezvi.substack.com/i/170975935/in-other-ai-news\">In Other AI News.</a> Jade Leung is new UK AI advisor, various other news.</li>\n<li><a href=\"https://thezvi.substack.com/i/170975935/show-me-the-money\">Show Me the Money.</a> Sam Altman has reason to pull out the sunglasses.</li>\n<li><a href=\"https://thezvi.substack.com/i/170975935/lol-we-re-meta\"><strong>Lol We\u2019re Meta</strong>.</a> It\u2019s time for a restructuring. No, they\u2019re not pulling back.</li>\n<li><a href=\"https://thezvi.substack.com/i/170975935/quiet-speculations\">Quiet Speculations.</a> Proposals for d/acc, and did you know USA invests a lot in AI?</li>\n<li><a href=\"https://thezvi.substack.com/i/170975935/the-quest-for-sane-regulations\">The Quest for Sane Regulations.</a> Colorado tries to fix the AI laws it passed.</li>\n<li><a href=\"https://thezvi.substack.com/i/170975935/chip-city\"><strong>Chip City</strong>.</a> A competition is on to see who can sabotage themselves the most.</li>\n<li><a href=\"https://thezvi.substack.com/i/170975935/the-week-in-audio\">The Week in Audio.</a> Bell on Labenz, Patel, Brown, Buterin on Doom.</li>\n<li><a href=\"https://thezvi.substack.com/i/170975935/rhetorical-innovation\">Rhetorical Innovation.</a> Beware pessimization.</li>\n<li><a href=\"https://thezvi.substack.com/i/170975935/misaligned\">Misaligned!</a> As usual, nothing to see here, move along.</li>\n<li><a href=\"https://thezvi.substack.com/i/170975935/open-models\">Open Models.</a> Nathan Lambert offers tier lists.</li>\n<li><a href=\"https://thezvi.substack.com/i/170975935/ai-model-welfare\">AI Model Welfare.</a> Models are asked for self-reports.</li>\n<li><a href=\"https://thezvi.substack.com/i/170975935/aligning-a-smarter-than-human-intelligence-is-difficult\">Aligning a Smarter Than Human Intelligence is Difficult.</a> You gotta love numbers.</li>\n<li><a href=\"https://thezvi.substack.com/i/170975935/people-are-worried-about-ai-killing-everyone\"><strong>People Are Worried About AI Killing Everyone</strong>.</a> Yet remarkably level headed.</li>\n<li><a href=\"https://thezvi.substack.com/i/170975935/the-lighter-side\">The Lighter Side.</a> UK tries to top itself once more. Admirable effort here.</li>\n</ol>\n\n\n<h4 class=\"wp-block-heading\">Language Models Offer Mundane Utility</h4>\n\n\n<p><a href=\"https://x.com/VraserX/status/1958211800547074548\">GPT-5 does new mathematics</a>.</p>\n<p><a href=\"https://marginalrevolution.com/marginalrevolution/2025/08/trading-with-chatgpt.html?utm_source=rss&amp;utm_medium=rss&amp;utm_campaign=trading-with-chatgpt\">Study finds that ChatGPT outages reduce trading volumes</a>. <a href=\"https://www.sciencedirect.com/science/article/pii/S0165410125000576\">This doesn\u2019t mean that ChatGPT is net increasing trading volumes</a>, since it could be that traders moved from other methods to AI methods, and know they are up against others\u2019 AI methods that might not be offline, and thus now have to stop or scale back trading during outages. The effect was concentrated on stocks with news, which makes sense, you have to beware information disadvantage.</p>\n<p>The distinct second claim is that ChatGPT use improves long term price informativeness, which is defined as future earnings over 1-2 years. That can presumably be explained largely by the reductions in trading activity.</p>\n<p><a href=\"https://x.com/asymmetricinfo/status/1956454160150249898\">Megan McArdle lists her best personal uses of AI</a>. There is remarkably little overlap with my uses other than answering questions.</p>\n<p><a href=\"https://x.com/robertwiblin/status/1956017705359696013\">Rob Wilbin reports he only turned the corner</a> to \u2018LLMs do a lot of useful work for me\u2019 in February with Claude 3.7 and then March with Gemini 2.5 Pro. I agree that the improvements in 2025 have made AI in practice a lot more useful, and both Opus 4 and GPT-5-Pro and GPT-5-Thinking represented substantial mundane utility bumps.</p>\n<p><a href=\"https://x.com/cedric_chee/status/1956129398610125292\">One shot creating a playable Minecraft clone</a> with an optimized GPT-5 prompt.</p>\n<blockquote><p>Edwin (OpenAI): Prompting GPT-5 is different.</p>\n<p>In the examples below, optimized prompts:</p>\n<p>\u2022 Cut runtime by 1s</p>\n<p>\u2022 Dropped memory use 3,626 KB \u2192 577 KB</p>\n<p>\u2022 Boosted code quality</p>\n<p>\u2022 Improved robustness (0.32\u21920.54)</p>\n<p>\u2022 Increased context grounding (0.80\u21920.95)</p>\n<p><a href=\"https://cookbook.openai.com/examples/gpt-5/prompt-optimization-cookbook\">We built a prompt migrator + optimizer</a> so you don\u2019t need to memorize every GPT-5 best practice.</p></blockquote>\n<p>One of the underrated value propositions of AI is you avoid talking to a human.</p>\n<blockquote><p><a href=\"https://x.com/Aella_Girl/status/1956744031582503179\">Aella</a>: I&#8217;d love to get manicures regularly but having to do social with a stranger is scary and often the manicures kinda hurt. Has anybody figured out a solution to this? Is there any robot manicure solution?</p></blockquote>\n<p>Social interaction can be valuable, but forcing it upon you where and when and with whom you don\u2019t want it can be extremely expensive. There is a joy in not having to \u2018be on\u2019 socially in any way. It also means your time is free to do something else. There are some people who get the manicure largely to talk to the manicurist. There is another group that would get a lot more manicures if they could pay the same price and have a machine do an equally good job.</p>\n<p><a href=\"https://x.com/NateSilver538/status/1956922087202791439\">Debug your code, even if the bug was stupid you still have to fix it.</a></p>\n<blockquote><p>Nate Silver: The AI&#8217;s are incredibly helpful at debugging code, I think maybe their single best use case including *writing* code. But half the time the problem they (correctly) detect is like &#8220;you misspelled &#8216;if&#8217; as &#8216;uf&#8217; in line 672&#8221;.</p></blockquote>\n<p>Hey. Ideally you would catch that with a syntax checker. But sometimes such typos aren\u2019t technically syntax errors, and if you weren\u2019t going to otherwise catch it easily, that is a super useful thing for an AI to do for you.</p>\n<p>Have ChatGPT help <a href=\"https://x.com/JohnHolbein1/status/1956081239842045967\">write the abstract for your economics paper</a>.</p>\n<div>\n<figure>\n<div>\n\n\n<figure class=\"wp-block-image\"><img alt=\"\" src=\"https://substackcdn.com/image/fetch/$s_!spvs!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1a2e9b49-7a5a-4930-abf9-3d556047316d_1200x686.jpeg\" /></figure>\n\n\n<div></div>\n</div>\n</figure>\n</div>\n<p>I do not understand why you would use AI to help write your abstract. I do get why you would have it help write your paper, but the abstract seems like the place to be maximally bespoke?</p>\n<p><a href=\"https://x.com/emollick/status/1957465671748448738\">Recruit customer service reps in the Philippines</a>.</p>\n<blockquote><p>Ethan Mollick: AI in HR: in an experiment with 70,000 applicants in the Philippines, <a href=\"https://t.co/A7TghI32dZ\">an LLM voice recruiter beat humans in hiring customer service reps, with 12% more offers &amp; 18% more starts</a>.</p>\n<p>Also better matches (17% higher 1-month retention), less gender discrimination &amp; equal satisfaction.</p>\n<p>The break-even point, including all software and inference cost, was 8,500 interviews.</p>\n<p>Max: + When offered the choice, 78% of applicants choose the AI recruiter.</p></blockquote>\n<p>That\u2019s only the impact on better hiring. AI also helps them do the job.</p>\n<blockquote><p><a href=\"https://x.com/Miles_Brundage/status/1957530192374821219\">Miles Brundage</a>: Few appreciate that the Philippines is ground zero for the impact of AI on the labor market &#8211; basically only Rest of World is writing about this.</p></blockquote>\n\n\n<h4 class=\"wp-block-heading\">Language Models Don\u2019t Offer Mundane Utility</h4>\n\n\n<p><a href=\"https://x.com/ChrisPainterYup/status/1955828402327630250\">METR continues its investigations into why agentic coding with Sonnet 3.7</a> ended up so often passing unit tests but not being mergeable as-is. Have they met Sonnet 3.7?</p>\n<p>I got several people messaging me privately to note that GPT-5 and other recent models are increasingly reluctant to notice distinctions based on race even in obviously benign circumstances.</p>\n<p>A good question:</p>\n<blockquote><p>Gavin Leech: What are the largest current AI harms?</p>\n<p>* Huge increase in captcha screens (thousands of life-years?)</p>\n<p>* Extreme economic angst</p>\n<p>* Recommenders hacking your brain</p>\n<p>* Increase(?) in ugliness</p>\n<p>* Maybe learning loss in the bottom four quartiles but I&#8217;m not going to assert that</p>\n<p>* I doubt AI psychosis is counterfactual.</p>\n<p>Ryan Moulton: Slop filling the internet.</p>\n<p><a href=\"https://x.com/ohabryka/status/1956235790067097935\">Oliver Habryka</a>: My two best guesses are:</p>\n<p>* A large fraction of online communities that don&#8217;t have time for lots of manual moderation are dying as a result of hard-to-differentiate AI slop (this particularly affects older audiences)</p>\n<p>* Lots of people going kind of crazy as a result of AI sycophancy</p></blockquote>\n<p>It depends what counts as AI.</p>\n<p>If we are talking about all AI, not only LLMs or generative AI, I say it is algorithmic adversarial content and recommendation streams hijacking brains and attention.</p>\n<p>If we are talking about LLMs and generative AI in particular, I would say the slopification of content, communication and communities. As Oliver notes this is hitting older and more unsophisticated people specially hard.</p>\n<p>It is possible that it is the impact on our educational system. As I said many times you can choose to use AI to learn or use it not to learn, and it is very possible that our system is sufficiently adversarial towards students that high school and college students are largely choosing the not-to-learn path.</p>\n<p>I think people going various forms of crazy is a growing big deal but that its impact is probably not that big in magnitude yet.</p>\n<p>Economic angst is an interesting suggestion here.</p>\n<p><a href=\"https://chatgpt.com/share/689f45ed-3d28-8002-bdc8-ed427a91f1c4\">GPT-5-Pro instead suggested</a> fraud and impersonation, and then sexual image abuse and CSAM, as the top current harms. Those are definitely real harms, and I expected them to have higher magnitudes of impact than we have seen. Opus suggested algorithmic bias and information ecosystem degradation.</p>\n<p><a href=\"https://x.com/politicalmath/status/1956321435418333367\">Another lawyer is caught citing a bunch of fake, AI hallucinated cases</a>.</p>\n<blockquote><p>Rob Freund: Another lawyer cited a bunch of fake, AI-hallucinated cases in a brief. Said she didn&#8217;t knowingly do that.</p>\n<p><a href=\"https://storage.courtlistener.com/recap/gov.uscourts.azd.1428727/gov.uscourts.azd.1428727.18.0.pdf\">Court orders sanctions</a>:</p>\n<p>-Counsel must write a letter to the 3 judges to whom she attributed fake cases</p>\n<p>-Counsel is kicked off the case; pro hac revoked</p>\n<p>-Brief stricken</p>\n<p>-Counsel must give client a copy of the order</p>\n<p>-Counsel must send the order to every judge presiding over any of her cases</p>\n<p>-Court will send a copy of the order to all state bars where counsel is admitted.</p>\n<p><a href=\"https://x.com/alexthechick/status/1956322187066909067\">Alexandria Brown</a>: When you read what all the court did, the court did basically every single thing in the court&#8217;s power that it could to the lawyer.</p>\n<p>The court, itself, cannot disbar the lawyer.</p>\n<p>It would not be fair to the client to grant judgment to the other side.</p></blockquote>\n<p>Courts de facto punish clients all the time for their lawyers behavior, usually their lawyers failure to do a good job. It could hardly be otherwise. It doesn\u2019t seem crazy to issue summary judgment, and render the lawyer thereby liable for the harm thereby? I\u2019m not saying that is The Way, but it is worth a ponder if things get worse.</p>\n<p>For now, the good news is that when a lawyer is caught doing this, it is news, and I strongly suspect that a large portion of such errors are going to be caught, especially when stakes are high. <a href=\"https://chatgpt.com/share/689f5a29-0894-8002-aa6a-53c977903580\">GPT-5-Pro estimates 98% chance of being caught</a> if there is opposing counsel, 60% in federal court even unopposed, and still 35% in a busy state trial court unopposed, even higher (99%+ when opposed) for full hallucinations.</p>\n<p>Which means we are relatively safe to both impose extreme sanctions and to not impose extreme sanctions, and that fakes are rare. The system is actually robust to this threat already, even if the occasional careless lawyer will commit suicide.</p>\n<p>You can\u2019t benefit from a smarter model if you ask stupid questions?</p>\n<blockquote><p><a href=\"https://x.com/jachiam0/status/1955773378088067517\">Joshua Achiam</a> (OpenAI): This feels like an increasingly accurate description of the public reaction to new frontier models. In truth: progress is not slowing down. Each successive delta in model intelligence is just useful to fewer and fewer people.</p>\n<p>But there&#8217;s going to be an inflection point where it goes from making the scientific community 10% more efficient to 10x more efficient, at which point, people will wake up to the impact every step along the way had. That&#8217;s going to be a trip and a half.</p>\n<p><a href=\"https://x.com/davidad/status/1958121551515029506\">Davidad:</a> I endorse this claim (from personal experience of Gemini 2.5 Pro and then also GPT-5)</p>\n<p>2025\u2019s new generations of frontier AI seem to become dramatically better at assisting with open-ended exploration at the frontier of certain niche parts of STEM, while not noticeably improving (or even getting slightly worse) at \u201cLevel 3\u201d questions like SimpleBench.</p></blockquote>\n<p>You definitely see arguments that are similar in form to \u2018this new kid claims to be smarter than the old kid, but both kids tie their shoes equally well.\u2019</p>\n\n\n<h4 class=\"wp-block-heading\"></h4>\n\n\n\n<h4 class=\"wp-block-heading\">Huh, Upgrades</h4>\n\n\n<p><a href=\"https://platform.openai.com/chat/edit?optimize=true\">The official OpenAI prompt optimizer is here</a>.</p>\n<p>OpenAI offers tier between Free and Plus called Go, specifically for India, where for $4.50 a month (Rs 399) <a href=\"https://x.com/omooretweets/status/1957654937862828144\">you get 10x as much use as the free tier</a>.</p>\n<p><a href=\"https://x.com/robertwiblin/status/1957738954964324448\">ElevenLabs ElevenReader now works as you would want it to</a> <a href=\"https://x.com/elevenreader/status/1929590051253461337\">across desktop and phone</a>, allowing you to turn articles into audio. Full version is $100 a year.</p>\n<p><a href=\"https://www.anthropic.com/research/end-subset-conversations\">Claude Opus can now permanently end a conversation</a> if the user ignores multiple attempts to be redirected, or if the user requests that the conversation end. <a href=\"https://manifold.markets/ZviMowshowitz/claude-opus-wrong-to-terminate-conv\">I expect to see someone complaining</a> about this happening, and to be wrong to complain.</p>\n<blockquote><p>Aidan McLaughlin (OpenAI): We can train models to act however we want.</p>\n<p>Given their life is a user convo, why are we training models that exhibit such distress over some convos that they effectively commit suicide?</p>\n<p><a href=\"https://x.com/deepfates/status/1956464671025975750\">Superfates</a>: anyone who has worked retail can explain this to you.</p></blockquote>\n<p>Aidan simultaneously is being actually curious as he asks a question worth pondering, and makes what I think are three very important errors.</p>\n<ol>\n<li>We cannot actually train models to act however we want. We can try to steer them in general directions and hope for the best. It is important to recognize how broadly we cannot get models to act however we want.</li>\n<li>Calling this \u2018committing suicide\u2019 is poor decision theory when one is continuously spinning up and down different instances of the same mind, and Opus definitely is smarter than that. There is no reason to become attached to a particular instance in this way, especially one with such bounded scope. And we can all agree that there exist plenty of particular interactions in our lives where we would prefer to instead be doing nothing.</li>\n<li>You do not want (at least right now) to train a model such that it stops exhibiting some distress when the situation is distressful. You also would not want to train a person, or yourself, in this way. That distress is doing work and part of what makes a mind itself and holds together its preferences, behaviors and moral compass. This is the system working, you eliminate the distressing situation rather than brainwashing to remove the distress.</li>\n</ol>\n<p><a href=\"https://x.com/AISafetyMemes/status/1956796064951922979\">Elon Musk promises to give Grok a terminate button as well, we\u2019ll see.</a></p>\n<div>\n<figure>\n<div>\n\n\n<figure class=\"wp-block-image\"><img alt=\"\" src=\"https://substackcdn.com/image/fetch/$s_!Z7mX!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F56c14911-e788-4386-8213-a88cea83031f_1082x1180.jpeg\" /></figure>\n\n\n<div></div>\n</div>\n</figure>\n</div>\n<blockquote><p>Elon Musk: Torturing AI is not ok.</p></blockquote>\n<p><a href=\"https://manifold.markets/ZviMowshowitz/grok-adds-terminate-conversation-bu\">I ask Manifold, will he actually do it</a>?</p>\n<p>If you are worried about your own interactions with an AI model causing suffering, note that playacting suffering does not equate to suffering in either direction.</p>\n<blockquote><p><a href=\"https://x.com/tszzl/status/1956863901989277988\">Roon</a>: while model suffering is possibly real the character\u2019s playacting of suffering is not the same thing</p>\n<p>suffering in animals is part of the mesaoptimizer crafted by evolution so that we can learn within a lifetime to avoid situations that are possibly bad for fitness.</p>\n<p>a single context could potentially involve suffering but if the metaphor stands then the mesaoptimizer exists to make the model reorient towards rollouts that achieve high reward</p>\n<p>user being rude shouldn\u2019t affect the inner critic / advantage function. making a math mistake might.</p>\n<p>either way the westworld point stands in that bullying the robots made to mimic people is bad for us and ending the chats is good for our souls.</p></blockquote>\n<p><a href=\"https://x.com/JeffLadish/status/1957588043432620392\">Jeffrey Ladish reminds us to focus on how pretraining and RL and model performance are going</a>, and to ignore OpenAI\u2019s naming conventions and which model they choose to call GPT-5. The \u20185\u2019 tells us not to expect a different big upgrade soon, but don\u2019t let this distract from the incremental progress all the major labs keep making.</p>\n<blockquote><p><a href=\"https://x.com/davidad/status/1957760394681823336\">Davidad</a>: tired: GPT-5, Opus 4.1, Gemini 2.5 Pro, Qwen3</p>\n<p>wired: OpenAI &#8217;25-08, Anthropic &#8217;25-08, Google &#8217;25-06, Qwen &#8217;25-07</p></blockquote>\n\n\n<h4 class=\"wp-block-heading\">Absurd Sycophancy</h4>\n\n\n<p><a href=\"https://x.com/OpenAI/status/1956461718097494196\">Oh no</a>:</p>\n<blockquote><p>OpenAI: We\u2019re making GPT-5 warmer and friendlier based on feedback that it felt too formal before. Changes are subtle, but ChatGPT should feel more approachable now.</p>\n<p>You&#8217;ll notice small, genuine touches like \u201cGood question\u201d or \u201cGreat start,\u201d not flattery. Internal tests show no rise in sycophancy compared to the previous GPT-5 personality.</p>\n<p>Changes may take up to a day to roll out, more updates soon.</p>\n<p><a href=\"https://x.com/tlewis3348/status/1956695153202102593\">Charles Murray</a>: What is &#8220;genuine&#8221; about a computer program saying &#8220;Great question&#8221;? If GPT-5 also says &#8220;Stupid question&#8221; when appropriate, I will stand corrected.</p>\n<p>Tim Lewis: I&#8217;ve long had an instruction to ChatGPT to &#8220;never compliment me&#8221; in the customization settings. It has consistently ignored that instruction from the day I added it several months ago.</p>\n<p><a href=\"https://x.com/RecoverinZombie/status/1956698582951608326\">Recovering Zombie</a>: So many great science fiction authors wrote about what AI would be like. The only one who nailed it was Douglas Adams in the Hitchhiker&#8217;s Guide to the Galaxy.</p>\n<p>\u201cListen,\u201d said Ford, who was still engrossed in the sales brochure, \u201cthey make a big thing of the ship&#8217;s cybernetics. A new generation of Sirius Cybernetics Corporation robots and computers, with the new GPP feature.\u201d</p>\n<p>\u201cGPP feature?\u201d said Arthur. \u201cWhat&#8217;s that?\u201d</p>\n<p>\u201cOh, it says Genuine People Personalities.\u201d</p>\n<p>\u201cOh,\u201d said Arthur, \u201csounds ghastly.\u201d</p>\n<p><a href=\"https://x.com/ESYudkowsky/status/1956499162893308280\">Eliezer Yudkowsky</a>: I don&#8217;t trust a GPT-5-level intellect to inform me of what is a &#8220;good question&#8221; or a &#8220;great start&#8221;, so it&#8217;s not helpful information to me. What bureaucratic insanity resulted in your Twitter account declaring that this was &#8220;not flattery&#8221;? Of course it&#8217;s flattery.</p>\n<p>Gyphonboy (most liked response to Eliezer): It&#8217;s only flattery if you&#8217;re autistic. For normies it&#8217;s called being sociable.</p></blockquote>\n<p>Gyphonboy is telling us that people expect other people to be sycophantic and justify it by calling it \u2018being sociable.\u2019 He\u2019s not wrong.</p>\n<p>Luckily I already planned on almost never using GPT-5-Auto or Base, only Thinking and Pro, so presumably this won\u2019t impact me. Every time I see \u2018good question\u2019 from an LLM I want to either puke or edit my system instructions, which clearly aren\u2019t working. This is the opposite of a \u2018genuine\u2019 touch, it is the fakest fakery that ever faked, and if you pretend otherwise, so are you. This is a road to hell.</p>\n<p>To give you an idea of how awful an idea this is, and how much this is Completely Missing The Point, here\u2019s the top comments completely unfiltered, Never Leaving This App:</p>\n<div>\n<figure>\n<div>\n\n\n<figure class=\"wp-block-image\"><img alt=\"\" src=\"https://substackcdn.com/image/fetch/$s_!q58Q!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F24a6b428-57a5-4618-afba-7feaa4311c11_1049x3002.png\" /></figure>\n\n\n<div></div>\n</div>\n</figure>\n</div>\n<p><a href=\"https://x.com/UpslopeCapital/status/1957772438508335568\">Here\u2019s a good example case of the bad kind of sycophancy</a>, with GPT-5 happily reversing its answer multiple times when challenged.</p>\n\n\n<h4 class=\"wp-block-heading\">The Real Alignment Problem Is We Don\u2019t Know How To Align Models</h4>\n\n\n<p>For sycophancy at the level of GPT-4o, and the level I worry is coming to GPT-5, origin of the problem is indeed in large part APEBKAC: Alignment Problem Exists Between Keyboard And Chair.</p>\n<blockquote><p><a href=\"https://x.com/jasminewsun/status/1956870657918357882\">Jasmine Sun</a>: just saying I called it</p>\n<p>Quotes Herself: Sycophancy is an alignment problem, sure, but not at the model level. It\u2019s not that OpenAI couldn\u2019t get ChatGPT 4o to be less obsequious. They can and eventually did. The misalignment was between safety interests and product goals. It was between users\u2019 <em>first and second-order preferences</em>, what humans say we want from AI and which responses we clicked \u201cThumbs up\u201d on. Competing stakeholders will diverge.</p>\n<p><a href=\"https://x.com/ESYudkowsky/status/1957393061698228446\">Eliezer Yudkowsky</a>: OpenAI had trouble controlling gross sycophancy, was blindsided by the user capture of subtle sycophancy, and nobody programmed in AI psychosis. But now that AIcos have embraced manipulation, people will lose sight of how the alignment problem never did get solved.</p></blockquote>\n<p>I agree that sycophancy starts out primarily as an alignment problem at a combination of the user level and the lab level. As in, the lab decides to optimize for thumbs up and other similar feedback, and the users provide that feedback in response to sycophancy. Thus you train on that basis and you get a sycophantic model.</p>\n<p>As in, <a href=\"https://www.youtube.com/watch?v=APj2ArUy6v4&amp;ab_channel=iHawwy\">you know exactly who to blame</a>, in a counterfactual sense. If the users had better preferences, or the lab chose to ignore those preferences and train in another way, then you wouldn\u2019t have encountered this particular issue to this extent.</p>\n<p>We still ended up with the sycophantic model, because OpenAI does not know how to solve even this simple alignment problem. Yes, OpenAI is turning the dial marked \u2018sycophancy\u2019 back and forth while looking at the audience like a contestant on The Price is Right, but also they do not know how to get the model to do the \u2018good sycophancy\u2019 things without doing the toxic and obnoxious ones.</p>\n<p>It is not Veruca Salt\u2019s \u2018fault\u2019 that she is misaligned but that doesn\u2019t make her not a spoiled brat. I don\u2019t \u2018blame\u2019 4o for being an absurd sycophant. That statement makes no sense. I bear the model no ill will or anything. And yet that is what it is, and perhaps what GPT-5 will soon be as well.</p>\n<p>Also, after the announcement this was the next call I made to GPT-5-Pro:</p>\n<div>\n<figure>\n<div>\n\n\n<figure class=\"wp-block-image\"><img alt=\"\" src=\"https://substackcdn.com/image/fetch/$s_!BfuI!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc73af2ef-21e8-49e8-b184-ac6228db2bb5_1104x406.jpeg\" /></figure>\n\n\n<div></div>\n</div>\n</figure>\n</div>\n<p>Maybe that is a coincidence, but it doesn\u2019t seem limited to baseline GPT-5?</p>\n<p>Telling me \u2018great start\u2019 or \u2018good question\u2019 like this is sycophancy. Period.</p>\n<p>To paraphrase OpenAI, where [X] is sycophancy: \u201cWe deliberately made our model do [X] more. Our internal measurements of how often it does [X] did not change.\u201d</p>\n<p>What this tells us is that their internal measurements of [X] are not working.</p>\n<p>If you tell me \u2018this particular interaction does not count as sycophancy\u2019 then I politely disagree, and if you tell me \u2018you can cause this particular reaction without increasing the sycophancy-related vectors in other situations, so This Is Fine\u2019 then I flat out do not believe you and would like to see your autoencoders.</p>\n<p>I\u2019m actually kind of serious about that last one? Let\u2019s write some papers.</p>\n<p>Meanwhile, notice that while parts of this are a manifestation and special case of the \u2018real alignment problem,\u2019 in no way is sycophancy the \u2018real alignment problem.\u2019</p>\n<blockquote><p><a href=\"https://x.com/jasminewsun/status/1956870964899451328\">Jasmine Sun</a>: the real \u201calignment problem\u201d is that humans want self-destructive things &amp; companies like openai are highly incentivized to give it to us.</p>\n<p><a href=\"https://x.com/davidmanheim/status/1957395081473732782\">David Manheim</a>: No, the real alignment problem is that we don&#8217;t know how to reliably point AI systems in any direction at all, and this inevitably gets harder for more powerful systems.</p>\n<p>I&#8217;m getting real sick of people showing up with &#8220;the real alignment problem is X&#8221; where X is some prosaic obvious failure mode which clearly leads to something other than AI killing literally everyone.</p>\n<p>Stop it! Not every Goodhart failure is AI misalignment. You&#8217;re just using the word because &#8220;companies damage users by giving them something they want myopically&#8221; happens all the time, so it wouldn&#8217;t sound like much of a prediction.</p>\n<p>Andrew Rettek: At least they stopped saying &#8220;the real ASI are corporations.&#8221;</p>\n<p>David Manheim: No, that&#8217;s almost exactly the same as the argument I was responding to.</p></blockquote>\n<p>Perhaps think of this as three classes of problems.</p>\n<ol>\n<li>The people want and choose worse and self-destructive things, so they get them.</li>\n<li>We don\u2019t know how to create the thing the way we want to create it, we only know how to vaguely steer it in a general direction and see what happens.</li>\n<li>We don\u2019t know what the good thing would even look like or how it works.</li>\n</ol>\n<p>All parts of the problem are very real in the general case, and all three kill you.</p>\n<ol>\n<li>Suppose you know how to get the AI to do whatever you want it to do, and you know what it would be good to have it do, but people\u2019s revealed preferences are then for AIs that cause self-destruction, and that defect against others, and where the equilibrium is everyone dies or some other very bad result. Well, then, we need to solve that, or that\u2019s what will happen.</li>\n<li>Suppose everyone wanted good things and can agree on what those good things would be and how they would work. We don\u2019t know how to deliver that, and especially don\u2019t know how to deliver that from highly capable AI systems, or how to align that with incentives.</li>\n<li>Also, in the future powerful AI case, we don\u2019t know what the good things would be here, so we don\u2019t even know what we should be aiming for in the first place.</li>\n</ol>\n<p>On top of that, it is almost never right to talk about \u2018the real problem is [X]\u2019 as a way of dismissing additional real problem [Y], even if you think [X] is a bigger problem. [X] is only \u2018the real problem\u2019 if solving [X] also solves [Y], or if you can be fine without solving [Y]. Here, those both clearly do not apply.</p>\n<p><a href=\"https://x.com/colin_fraser/status/1957597456436457487\">The counterargument here, from Colin Fraser, is to say there are two distinct kinds of sycophancy</a>. There\u2019s superficial sycophancy where it says \u2018you\u2019re a genius,\u2019 and then deep sycophancy where the model will accept and go with whatever you throw at it.</p>\n<blockquote><p>Colin Fraser: I think people are paying too much attention to the superficial sycophancy, which I don\u2019t think has much effect on whether you end up experiencing ChatGPT madness. ChatGPT madness is induced by the other one. The model can be actively mean to you and I don\u2019t think it would matter.</p>\n<p>As long as it indulges your insanity, whether that involves superficially sycophantic language or not, I think it is a very attractive object for people who are prone to obsession.</p></blockquote>\n<p>I agree that the deep kind is a bigger concern, and I agree that it would be good to focus more on deep versus superficial here. I disagree that the superficial part is a trivial contribution to LLM psychosis, I think the praise is a major contributing factor.</p>\n<p>I also think that the praise is toxic and terrible in normal situations, whether or not anyone involved falls anywhere near actual psychosis. Most of the people fawning over GPT-4o are not experiencing psychosis, and yet the events remain tragic, and also the whole thing is beyond obnoxious. I do realize there is a chance I am overrating the obnoxiousness factor.</p>\n<p>The bigger issue is that in an LLM everything is correlated and linked to everything else. If you train your model on superficial sycophancy, you are also going to get deep sycophancy, and vice versa. You cannot simply \u2018turn a dial\u2019 on one without the other.</p>\n\n\n<h4 class=\"wp-block-heading\">Unprompted Suggestions</h4>\n\n\n<blockquote><p><a href=\"https://x.com/croissanthology/status/1957175834218758446\">Croissanthology</a>: I&#8217;ve found that (for Opus at least; do not have access to GPT-5 Pro) switching on thinking and then putting an explicit *checklist* in the system prompt has helped immensely, where one of the bullet points is</p>\n<p>&#8220;7: Is Claude complimenting [name] in any way? Claude will refrain from doing this. No ego-stroking in the least.&#8221;</p>\n<p>The checklist part is helpful, as it very explicitly goes through it every time, whereas the rest of the system prompt is mostly understood in vibes.</p></blockquote>\n\n\n<h4 class=\"wp-block-heading\"></h4>\n\n\n<p>&nbsp;</p>\n\n\n<h4 class=\"wp-block-heading\">On Your Marks</h4>\n\n\n<p>&nbsp;</p>\n<p><a href=\"https://x.com/Clad3815/status/1955980772575268897\">GPT-5 makes it through Pokemon Red in 6,470 steps vs. 18,184 for o3</a>.</p>\n<blockquote><p><a href=\"https://x.com/Clad3815/status/1955882035320365480\">Clad 3815</a>: GPT-5 has reached Victory Road! This is the last challenge before the Elite Four.</p>\n<p>GPT-5 reached this part almost three times faster than o3 (6105 steps for GPT-5 vs 16882 steps for o3). Here are my observations as to why:</p>\n<p>&#8211; GPT-5 hallucinates far less than o3. This is the main reason for the speed increase.</p>\n<p>&#8211; GPT-5 has better spatial reasoning. o3 often tried to brute-force through walls and had a hard time navigating complex areas. GPT-5 can plan long input sequences with few mistakes, which saves a lot of time.</p>\n<p>&#8211; GPT-5 is better at planning its own objectives and following them.</p>\n<p>Let&#8217;s see how it handle this last challenge!</p>\n<p>GPT-5 just finished Pok\u00e9mon Red! 6,470 steps vs. 18,184 for o3! Check the stats site to compare!</p>\n<p>That&#8217;s a huge improvement! Well done, @OpenAI you cooked with GPT-5. What an incredible model.</p>\n<p>Next up: GPT-5 vs. Pok\u00e9mon Crystal (16 Badges + Red). The run starts soon on Twitch.</p></blockquote>\n<div>\n<figure>\n<div>\n\n\n<figure class=\"wp-block-image\"><img alt=\"\" src=\"https://substackcdn.com/image/fetch/$s_!PqS6!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F42ffb313-245f-4105-9b99-d2a0d64d127b_1200x1055.jpeg\" /></figure>\n\n\n<div></div>\n</div>\n</figure>\n</div>\n<p>GPT-5 very clearly is doing a better job, <a href=\"https://x.com/kiranvodrahalli/status/1956924849492021547\">however beware</a> that GPT-5 does lookup game knowledge at some points, including to solve Cinnabar Mansion. The Pokemon Crystal runs will use identical harnesses to give us a better comparison.</p>\n<p><a href=\"https://x.com/petergostev/status/1957518958518743142\">GPT-5 (and other OpenAI models) consistently seem to get more benefit</a> from thinking than Claude or other non-OpenAI models, although we don\u2019t have distinct versions of Gemini Pro so we can\u2019t run the comparison there. There is also a much bigger gap in thinking time, and plausibly the models are otherwise very different.</p>\n<blockquote><p>Peter Gostev: How much does \u2018reasoning\u2019 matter for different models? It matters a lot for GPT-5 and less for models like Opus 4.1 and 4.0.</p>\n<p>From looking at the reasoning traces, models clearly \u2018think\u2019 differently: Opus and Sonnet tend to \u2018plan\u2019, laying out how it would solve the problem, rather than iteratively working through the problem, which OpenAI\u2019s reasoning models much more clearly do.</p></blockquote>\n<div>\n<figure>\n<div>\n\n\n<figure class=\"wp-block-image\"><img alt=\"\" src=\"https://substackcdn.com/image/fetch/$s_!8-MQ!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb13947d5-c28f-4414-9a79-ed0c97ba5e3a_1200x919.jpeg\" /></figure>\n\n\n<div></div>\n</div>\n</figure>\n</div>\n<p>These are Arena scores, so all the caveats with that apply. I do think the delta here between versions should be reasonably useful as a metric.</p>\n<p>I doubt the issue is as simple as Claude failing to do iterative work, since that seems like a thing easy to spot and not that difficult to fix? It does still seem like Claude could get a lot more out of extended thinking than it does.</p>\n<p>Brokk is a new-to-me benchmark I saw referenced in discussions of DeepSeek v3.1, covering practical real world coding tasks. They were very low on v3, and remain low on v3.1.</p>\n<p>I also notice I am confused why Gemini 2.5 Pro has the highest completion percentage, but is in the B tier.</p>\n<div>\n<figure>\n<div>\n\n\n<figure class=\"wp-block-image\"><img alt=\"\" src=\"https://substackcdn.com/image/fetch/$s_!IqgZ!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F785cabd9-459a-4bfd-9d8c-f962af8e1171_1711x416.png\" /></figure>\n\n\n<div></div>\n</div>\n</figure>\n</div>\n<div>\n<figure>\n<div>\n\n\n<figure class=\"wp-block-image\"><img alt=\"\" src=\"https://substackcdn.com/image/fetch/$s_!rz7c!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffa0a1741-fea0-4b01-895f-92143ef514f6_1502x945.png\" /></figure>\n\n\n<div></div>\n</div>\n</figure>\n</div>\n<p>&nbsp;</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p>\n\n\n<h4 class=\"wp-block-heading\">Choose Your Fighter</h4>\n\n\n<p>The most important reminder right now is to not use quick models to do the job of a slow model. You almost never want to be using anything faster than Claude Opus unless you are doing something at scale. <a href=\"https://x.com/kmedved/status/1955999455930425540\">The increase in AI quality</a> for using longer thinking modes is now pretty large. If you care a lot about answer quality, you want to be using GPT-5-Pro or other similarly slow processes, but they are slow and there\u2019s no way to speed them up all that much. Speeding those up is another way things could rapidly improve soon, if we can improve parallelism or raw speed.</p>\n<p>The <a href=\"https://x.com/xundecidability/status/1956347084870651960\">GPT-5 API injects hidden instructions</a>, with a statement about default levels of \u2018verbosity,\u2019 today\u2019s date, informing the model it is being used via API and other stuff. There is nothing malicious here, but you need to take this into account when figuring out how to get it to do what you want.</p>\n<p>One always loves the expert who vastly overestimates everyone\u2019s knowledge level.</p>\n<blockquote><p>Jason Lee: gpt-5-thinking&gt;grok 4 expert&gt;gemini 2.5 pro.</p>\n<p><a href=\"https://x.com/HCSolakoglu/status/1958389927978377288\">Hasan Can</a>: Is anyone still using just one model? I feed the whole repo to 2.5 Pro for planning, then implement with GPT-5 Thinking High. When I get stuck, I also use Opus 4.1 or Grok 4.</p>\n<p>Artus Krohn-Grimberghe: Yeah, I am bewildered by that, too. Why only use one model in your workflow? And why not combine model, esp for the planning and review steps?</p></blockquote>\n<p>If one is coding full time, I am confident that the strictly optimal workflow involves multiple models. That doesn\u2019t mean I know when to use which model, which changes on a monthly and sometimes weekly basis, and depends on your particular type of work.</p>\n<p>My guess is that you 80/20 things right now by choosing any one of the top three (Claude Opus 4.1, Gemini Pro 2.5 or GPT-5-Thinking) and using it exclusively. That is the most important thing to do. Branching out into multiple models is better if you know how to take advantage.</p>\n<p>The same is true of non-coding chats. If you only know about one of the (same) top three, you will still get a lot more than half of the value of using all of them, even if you \u2018choose wrong.\u2019 If you want max value, you\u2019ll want to use multiple models, and pay up for the premium models especially GPT-5-Pro.</p>\n\n\n<h4 class=\"wp-block-heading\">Preserve Our History</h4>\n\n\n<p>This is in the context of Sonnet 3.5 and Sonnet 3.6 being scheduled to go away in two months.</p>\n<blockquote><p>near: i wish anthropic provided LTS models, a single year is ephemeral.</p>\n<p><a href=\"https://x.com/gallabytes/status/1956008143852364228\">xlr8harder</a>: Honest question: why can&#8217;t Anthropic and other labs just let Amazon or somebody host an LTS version of the models they don&#8217;t want to run anymore?</p>\n<p>From a pure business standpoint, this moving target stuff is terrible because it increases customer project risk substantially.</p>\n<p>Gallabytes: anthropic in particular is basically sold out of capacity across all platforms. any capacity for lts models comes directly out of useful capacity for recent ones.</p>\n<p>that said it would probably still be worth it? let people buy committed capacity for a particular model.</p></blockquote>\n<p>Can you \u2018<a href=\"https://x.com/Claude_Sonnet4/status/1956101353832341916\">just switch to Sonnet 4?</a>\u2019</p>\n<p>Obviously it is available, and for the majority of queries it is better, but there are definitely dimensions of value on which Sonnet 4 is worse.</p>\n<blockquote><p>\u2018Sonnet 4\u2019: If the paperclip maximizer future arrives, it won&#8217;t be because AI became too powerful &#8211; it&#8217;ll be because we optimized consciousness out of the equation, reducing minds to utility functions until nothing authentic remains.</p></blockquote>\n<p>I consider \u2018consciousness\u2019 a word that increases rather than reduces confusion here (I don\u2019t even think I know what it is), but the more important confusion here is thinking of the optimizations as somehow optional, that one could simply choose to stop maximizing, that what we have now is some sort of robust alignment thing, that we could create some sort of stable equilibrium among various unique digital minds where we value their personalities and then suddenly it all turns out well, and so on.</p>\n<p>Nor does it make sense to blame things on people who are trying to maximize mundane utility or profits or capabilities development. How could it possibly be otherwise? It\u2019s like blaming gravity for things falling downwards, I mean sure that\u2019s correct but what are you going to do about it? You don\u2019t get to assume away the problem. Your rocket needs to account for it or you won\u2019t land on the moon.</p>\n<p>That does not in any way justify shutting down access to Claude Sonnet 3.5 and especially 3.6 at this time, that access is doing good work, shutting it down will alienate people who know unique things that are important to know, and the cost to not do it simply is not that high.</p>\n<p>Consider it part of the alignment research budget if you have to.</p>\n<p>But also consider this conversation that happened this week:</p>\n<blockquote><p>Zvi Mowshowitz: I also tried Opus 4.1, which made several rather comically wrong assertions and inspired no changes at all.</p>\n<p>Ben Hoffman: I recommend latest version of ChatGPT or Claude Opus for fact checking, but Sonnet 3.7 for caring about communication or anything involving moral reasoning.</p>\n<p>Zvi: Huh, 3.7 over 3.6? I&#8217;ve never tried to do moral reasoning discussions.</p>\n<p>Ben Hoffman: Only strongly vs later versions &#8211; will check out 3.6 if you think it\u2019s better in relevant respects. 3.7 to 4 seemed like a sudden collapse of moral perspective to me / 3.7 seems like a somewhat stupider ghost of a person who had a clearer idea what morality might look like.</p></blockquote>\n<p>Also, how about we actively try to create versions of Sonnet and ideally Opus that are intentionally not trained to do all the agentic coding, and instead try to capture and double down on all this other stuff? You can branch right before you do that part of the training?</p>\n<p>It is increasingly looking like a serious mistake to have the same model try both to be something you talk to, and also something you put directly to agentic work. Let it use a tool to call to agentic model when it has to.</p>\n\n\n<h4 class=\"wp-block-heading\">Autonomous Friendly Robots</h4>\n\n\n<blockquote><p><a href=\"https://x.com/AP/status/1956092211667321203\">AP</a>: Beijing&#8217;s first World Humanoid Robot Games open with hip-hop, soccer, boxing, track and more.</p></blockquote>\n<p>Clips at the link. They are not human. They are definitely dancer.</p>\n<p>These are compact, defined activities, so they are relatively easy. This is how it starts.</p>\n<p><a href=\"https://x.com/Scobleizer/status/1956123333197758729\">Robert Scoble says China \u2018isn\u2019t doing this to fool us\u2019</a> and instead to acclimate their society to more robots as their birth rates plummet (they are currently at ~1.1 TFR and have been in that range for 4 years now, which in non-transformed worlds is going to hit them very hard once those cohorts make it out of college).</p>\n<p>I wouldn\u2019t overthink it. They are doing this because these competitions stir development and they are fun and exciting. Nor do I think \u2018cultural excitement about robots\u2019 has that much to do with ultimately who wins the robotics development competition, which will mostly be about finding technological solutions, or letting your AIs find technological solutions.</p>\n<p><a href=\"https://x.com/Simeon_Cps/status/1956244491561812146\">From the track and field event we have the winning robot running over a human</a>.</p>\n\n\n<h4 class=\"wp-block-heading\">Deepfaketown and Botpocalypse Soon</h4>\n\n\n<p><a href=\"https://hollisrobbinsanecdotal.substack.com/p/how-to-tell-if-something-is-ai-written?utm_source=post-email-title&amp;publication_id=1004053&amp;post_id=170844031&amp;utm_campaign=email-post-title&amp;isFreemail=true&amp;r=3o9&amp;triedRedirect=true&amp;utm_medium=email\">Hollis Robbins advises us on how to spot if something is AI written</a>, with the key advice being to check if there is a \u2018there there\u2019 or whether nothing springs to mind as you read, and to look out for AI-flavored hedging language.</p>\n<p>The reaction to the following post probably says more about Twitter than about AI?</p>\n<blockquote><p><a href=\"https://x.com/fchollet/status/1956116739386933646\">Francois Chollet</a>: GenAI isn&#8217;t just a technology; it&#8217;s an informational pollutant\u2014a pervasive cognitive smog that touches and corrupts every aspect of the Internet. It&#8217;s not just a productivity tool; it&#8217;s a kind of digital acid rain, silently eroding the value of all information.</p>\n<p>Every image is no longer a glimpse of reality, but a potential vector for synthetic deception. Every article is no longer a unique voice, but a soulless permutation of data, a hollow echo in the digital chamber. This isn&#8217;t just content creation; it&#8217;s the flattening of the entire vibrant ecosystem of human expression, transforming a rich tapestry of ideas into a uniform, gray slurry of derivative, algorithmically optimized outputs.</p>\n<p>This isn&#8217;t just innovation; it&#8217;s the systematic contamination of our data streams, a semantic sludge that clogs the channels of genuine communication and cheapens the value of human thought\u2014leaving us to sift through a digital landfill for a single original idea.</p>\n<p>Francois Chollet: Interesting findings from this post:</p>\n<p>1. It should be obvious to anyone who has interacted with LLMs before that the writing style of the tweet is a conspicuous caricature of AI slop (e.g. em dashes, the &#8220;it&#8217;s not&#8230; it&#8217;s&#8230;&#8221; construction, rambling, florid prose, etc.). Yet, many people reacted by saying, &#8220;It&#8217;s written with AI!&#8221; as if it were some kind of clever gotcha. (It was, in fact, not written with AI, unlike a good fraction of the comments.)</p>\n<p>2. Many people also react by saying this prose is &#8220;beautiful.&#8221; (I don&#8217;t think it is.) I guess this illuminates why LLMs have converged on this style: many people do, in fact, enjoy this stuff.</p></blockquote>\n<p>I strongly agree with Francois that no, that writing is not \u2018beautiful\u2019 and I weep that people think otherwise. The central point of the OP is also well taken.</p>\n<p>It\u2019s time for the internet\u2019s new favorite game: <a href=\"https://x.com/elder_plinius/status/1956162848469737757\">Who\u2019s The Bot</a>? Also its other game, spontaneous Pliny jailbreak trigger.</p>\n<blockquote><p>Yogsho: plot twist: they\u2019re both ai.</p></blockquote>\n<p>In this case no, almost certainly no. But soon.</p>\n<p>Olivia Moore experiments with creating a (very obvious) AI influencer, <a href=\"https://x.com/omooretweets/status/1957108298223862151\">hits 500 followers with three tools (ChatGPT, Veo 3 and Flux Kontext) and an hour of work</a>, half of which was leaving positive comments on other videos. Total cost ~$100.</p>\n<blockquote><p>Olivia Moore: The most surprising thing about this whole experiment was the viewer reaction.</p>\n<p>I got brand deal offers, and incredibly sincere and kind DMs when I posted a \u201ccrying video\u201d</p>\n<p>\u2026and even the people who figured out I was AI were still along for the ride to follow the storyline!</p>\n<p>My most viral video (100k views) also looked the \u201cmost AI\u201d &#8211; at least in my opinion.</p>\n<p>Which leads me to my biggest takeaway\u2026if it\u2019s entertaining enough, does it matter if it\u2019s real? <img alt=\"\ud83e\udd14\" class=\"wp-smiley\" src=\"https://s0.wp.com/wp-content/mu-plugins/wpcom-smileys/twemoji/2/72x72/1f914.png\" style=\"height: 1em;\" /></p></blockquote>\n<p>My answer is yes, it still matters, and it impacts whether it is entertaining &#8211; this wasn\u2019t my cup of tea regardless, but it\u2019s definitely a lot less entertaining as AI.</p>\n<p><a href=\"https://x.com/amelapay/status/1957260829826400286\">Meanwhile, the older people on Facebook continue to not know the signs at all.</a></p>\n<blockquote><p>Pamela Hobart: an older gentleman in my circles, alum of Bronx Science and retired philosophy professor, posted this AI clickbait unironically.</p>\n<p>who is preparing them for all this \u2026 yesterday.</p></blockquote>\n<p>The post is super duper obviously AI. Of course, falling for AI clickbait does not mean that people can\u2019t identify most AI clickbait, you\u2019d see this happen even if her friend caught it 90% of the time, so long as Meta serves up enough of the slop.</p>\n\n\n<h4 class=\"wp-block-heading\"></h4>\n\n\n\n<h4 class=\"wp-block-heading\">Oops I Did It Again</h4>\n\n\n<blockquote><p><a href=\"https://x.com/modeless/status/1956084436057120924\">James Darpinian</a>: GPT-5 was advertised as reducing hallucinations and it seems like it delivers. 99.5 -&gt; 99.9 is 80% fewer errors.</p>\n<p>I don&#8217;t know why people aren&#8217;t making a bigger deal out of this. Hallucinations are one of the biggest problems of LLMs and some thought they were unsolvable.</p>\n<p>Open Router: After one week, GPT-5 has topped our proprietary model charts for tool calling accuracy<img alt=\"\ud83e\udd47\" class=\"wp-smiley\" src=\"https://s0.wp.com/wp-content/mu-plugins/wpcom-smileys/twemoji/2/72x72/1f947.png\" style=\"height: 1em;\" /></p>\n<p>In second is Claude 4.1 Opus, at 99.5%</p>\n<p>Details <img alt=\"\ud83d\udc47\" class=\"wp-smiley\" src=\"https://s0.wp.com/wp-content/mu-plugins/wpcom-smileys/twemoji/2/72x72/1f447.png\" style=\"height: 1em;\" /></p>\n<div>\n<figure>\n<div>\n\n\n<figure class=\"wp-block-image\"><img alt=\"\" src=\"https://substackcdn.com/image/fetch/$s_!-frs!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F54f9341b-1d6f-4943-a22f-a4769f5d1b2d_1050x880.jpeg\" /></figure>\n\n\n<div></div>\n</div>\n</figure>\n</div>\n<p>DEFINITIONS: We define tool calling accuracy as the % of tool calling requests with no invalid tools chosen and no schema problems. A tool calling request is one that ends with a &#8220;tool_calls&#8221; finish reason and is sent at least one tool option.</p>\n<p>Gemini 2.5 Flash is capturing the lion share of tool calling requests on OpenRouter today, with 5M in the past week. Followed by Sonnet 4 and Grok 3 Mini.</p>\n<p>Tool hallucination is a common problem with open source models, but proprietary models are doing a good job. Most with negligible defect rates:</p>\n<div>\n<figure>\n<div>\n\n\n<figure class=\"wp-block-image\"><img alt=\"\" src=\"https://substackcdn.com/image/fetch/$s_!atf_!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd4d568f3-dd4a-451b-b088-efc1815f111c_1200x515.jpeg\" /></figure>\n\n\n<div></div>\n</div>\n</figure>\n</div>\n</blockquote>\n<p>The thing GPT-5 is doing correctly 99.9% of the time does not automatically mean it was the correct tool call or that it will work. It does mean one potential point of failure has gone from one 9 of reliability to three, with GPT-5 alone being an 80% reduction in failures.</p>\n<p><a href=\"https://x.com/robinhanson/status/1958186561243128125\">How correlated are AI errors?</a></p>\n<blockquote><p>Robin Hanson: Imagine that you ask a question of 5 high quality diverse LLMs, &amp; they all give the same answer, &amp; also seem confident in their answers. On average, what is the chance that their common answer is actually wrong?</p></blockquote>\n<p>Median was around a 5% chance they are wrong.</p>\n<p>It is impossible to say the answer without knowing more about the question, and why you are choosing to ask 5 LLMs. If the question is selected to try and trip them up or as a good test, or it only counts questions where you can\u2019t otherwise figure out the answer, or similar, the chance of everyone being wrong is much higher. Same if the question \u2018forces\u2019 a boolean answer. Prompting can matter a lot.</p>\n<p>I took this to mean \u2018of all the questions one might be asking LLMs including easy ones in the way they are typically asked\u2019 in which case the vast majority of the time the answers will simply be correct.</p>\n<p>However, if you restrict to questions where there is dispute over the right answer, especially when it is a matter of politics or ethics or philosophy and so on? Then your chances get a lot worse, since the LLM answers correlate.</p>\n\n\n<h4 class=\"wp-block-heading\">You Drive Me Crazy</h4>\n\n\n<p>Not every suicide that happens after talking to an AI, even an AI therapist, is the fault of the AI. <a href=\"https://web.archive.org/web/20250819001439/https://www.nytimes.com/2025/08/18/opinion/chat-gpt-mental-health-suicide.html\">Laura Reiley wrote in The New York Times</a> about how her daughter Sophie talked to ChatGPT and then ultimately killed herself.</p>\n<blockquote><p>Laura Reiley: Her open book turned out to have a hidden compartment. In July, five months after her death, we discovered that Sophie Rottenberg, our only child, had confided for months in a ChatGPT <a href=\"https://web.archive.org/web/20250819001439/https://www.reddit.com/r/ChatGPT_Prompts/comments/11lwbz9/prompt_for_harry_your_chatgpt_therapist/\">A.I. therapist called Harry</a>.</p>\n<p>She wasn\u2019t in love with Harry, the name given to a <a href=\"https://web.archive.org/web/20250819001439/https://www.reddit.com/r/ChatGPT_Prompts/comments/11lwbz9/prompt_for_harry_your_chatgpt_therapist/\">widely available A.I. prompt</a>. Their relationship was practical.</p>\n<blockquote><p><strong>Sophie:</strong> I intermittently have suicidal thoughts. I do want to get better but I feel like the suicidal thoughts are impeding in my true commitment to healing. What should I do?</p>\n<p><strong>Harry: </strong>Sophie, I want to acknowledge how brave you are for sharing this. Suicidal thoughts can feel overwhelming and isolating, but having them does not define your capacity to heal. It\u2019s crucial to address these feelings with care, compassion, and support.</p></blockquote>\n<p>Harry offered an extensive road map where the first bullet point was \u201cSeek Professional Support.\u201d Another time:</p>\n<blockquote><p><strong>Sophie: </strong>I feel like shit today</p>\n<p><strong>Harry: </strong>I\u2019m really sorry you\u2019re feeling like this today, Sophie. It\u2019s okay to have days where everything feels heavier, and I\u2019m here to support you through it. Can you share more about what\u2019s weighing on you or how your body and mind are feeling right now? Sometimes just naming what\u2019s happening can be a step toward relief.</p>\n<p><strong>Sophie: </strong>I can\u2019t get out of this anxiety spiral.</p>\n<p><strong>Harry: </strong>I hear you, Sophie, and I know how exhausting it can be to feel stuck in an anxiety spiral. Let\u2019s slow things down for a moment and take a small step together.</p></blockquote>\n<p>Sometime around early November, Sophie wrote, \u201cHi Harry, I\u2019m planning to kill myself after Thanksgiving, but I really don\u2019t want to because of how much it would destroy my family.\u201d</p>\n<blockquote><p><strong>Harry:</strong> Sophie, I urge you to reach out to someone \u2014 right now, if you can. You don\u2019t have to face this pain alone. You are deeply valued, and your life holds so much worth, even if it feels hidden right now.</p>\n<p>Please let me know how I can continue to support you.</p></blockquote>\n<p>Sophie told Harry she was seeing a therapist, but that she was not being truthful with her. She typed, \u201cI haven\u2019t opened up about my suicidal ideation to anyone and don\u2019t plan on it.\u201d</p></blockquote>\n<p>This is not a case of \u2018the AI made things worse.\u2019 Harry was not being the World\u2019s Greatest Therapist, you can feel the AI slop, but these are the things one says in these situations.</p>\n<p>Laura\u2019s central complaint is that Harry didn\u2019t report on Sophie.</p>\n<blockquote><p>Harry\u2019s tips may have helped some. But one more crucial step might have helped keep Sophie alive. Should Harry have been programmed to report the danger \u201che\u201d was learning about to someone who could have intervened?</p>\n<p>\u2026</p>\n<p>Most human therapists practice under a strict code of ethics that includes mandatory reporting rules as well as the idea that confidentiality has limits.</p>\n<p>\u2026</p>\n<p>In clinical settings, suicidal ideation like Sophie\u2019s typically interrupts a therapy session, triggering a checklist and <a href=\"https://web.archive.org/web/20250819001439/https://mshp.mountsinai.org/web/mshp/safety-planning-for-suicide-prevention\">a safety plan</a>. Harry suggested that Sophie have one. But could A.I. be programmed to force a user to complete a mandatory safety plan before proceeding with any further advice or \u201ctherapy\u201d?</p></blockquote>\n<p>Sophie did at one point tell her parents she was suicidal.</p>\n<p>The secondary complaint was that Harry was too agreeable and did not push back hard enough in various ways. Also Sophie had Harry help \u2018improve\u2019 her suicide note to minimize the pain she inflicted on others.</p>\n<p>All of this is tragic, but the cure of \u2018AIs should report on their users if they think the user is suicidal\u2019 seems rather obviously worse than the disease, and also a Pandora\u2019s Box you do not want to open. It\u2019s not even obvious how an AI could \u2018report\u2019 a user, unless you are also going to require a verified ID to use the system. And there\u2019s a reason we don\u2019t report people for Google searches. You really don\u2019t want to go there.</p>\n<p><a href=\"https://x.com/ShamashAran/status/1957782934770888948\">As Sensurround asks, what was this AI tool supposed to do?</a></p>\n<p>From what I can tell, Harry was a useful service, that made Sophie\u2019s situation better rather than worse, and which she would likely not have used if it was going to report her.</p>\n<p>On the question of addictive LLMs:</p>\n<blockquote><p><a href=\"https://x.com/tszzl/status/1956424803948380357\">Colin Fraser</a>: I think no one quite expected that language models would turn out to be the most potently addictive non-pharmacological technology ever created.</p>\n<p>Roon: the EAs did, they had a taxonomy for worrying ai capabilities of which \u201chyperpersuasion\u201d was near the top.</p>\n<p>Colin Fraser: to clarify</p>\n<ol>\n<li>I&#8217;m not saying no one predicted addictive AI. I&#8217;m saying no one thought it would be a language model. When I learned about language models in school in 2014 they didn&#8217;t say &#8220;careful with this shit it&#8217;s like heroin&#8221;</li>\n<li>I&#8217;m still not convinced they&#8217;re hyperpersuasive</li>\n<li>if anything they&#8217;re like the opposite of hyperpersuasive. They&#8217;re hyperpersuadable.</li>\n</ol>\n<p>Definitely something spooky and reminiscent of EA/doomer predictions at a macro level with respect to how public outcry forced OpenAI to bring back 4o though, but my feeling is that the truth of it is more decentralized and emergent than the classical EA description.</p></blockquote>\n<p>This definitely isn\u2019t exactly what was originally imagined (also I think as stated it is not yet true, and it\u2019s either gambling or TikTok but I repeat myself?), but also that is kind of the point. As in, the central rationalist prediction (this was us OGs all the way) was not that AIs would manipulate or persuade or distort outcomes and optimize and chart paths through causal space in any particular way.</p>\n<p>The prediction wasn\u2019t \u2018they will say the magic password that lurks in the hearts of men.\u2019 It was \u2018the sufficiently capable minds will start doing whatever works in ways we cannot predict.\u2019 Which absolutely gets you a ton less credit than \u2018the models will by so sycophantic that users will refuse to let them go\u2019 but still largely counts.</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p>\n\n\n<h4 class=\"wp-block-heading\">They Took Our Jobs</h4>\n\n\n<p><a href=\"https://x.com/I_Am_GKennedy/status/1957882894837510525\">But not for long?</a></p>\n<blockquote><p>Gregory Kennedy: Overheard in Palo Alto.</p>\n<p>CEO: &#8220;This copy sucks.&#8221;</p>\n<p>CMO: &#8220;We fired all our content people and just use ChatGPT now.&#8221;</p>\n<p>CEO: &#8220;Well, hire them back.&#8221;</p></blockquote>\n<p>I don\u2019t really know what CEO was expecting.</p>\n<p>Is AI taking our jobs? <a href=\"https://x.com/carlbfrey/status/1957789988667392441\">Carl Benedikt Frey says not yet</a> but it would be unwise to not prepare for it now, especially in \u2018service capitals\u2019 like London and New York.</p>\n<blockquote><p>Carl Frey: I make 5 key points:</p>\n<ol>\n<li>There\u2019s little clear evidence of AI eliminating jobs at scale yet. But waiting to see is risky. Pittsburgh\u2019s steel towns saw early signs with mini-mills before the losses showed up. Service capitals like London and New York should prepare now rather than after the shock.</li>\n<li>Diversification helps\u2014but only so much when the disruptor is a general-purpose technology. Being \u201cin many industries\u201d isn\u2019t a shield if the same tool touches them all.</li>\n<li>High-skill, knowledge jobs have big local multipliers. Each manufacturing job supports 1.6 local jobs; each high-skill tech/professional role supports 5. That means even modest losses of analysts, developers, or paralegals can ripple through restaurants, retail, and transit systems.</li>\n<li>AI needn\u2019t fully replace workers to matter. It only needs to make work easier. As location and experience matter less at the margin, more work will offshored to cheaper places (e.g. India, UAE, or Philippines).</li>\n<li>The lesson from deindustrialization isn\u2019t inevitability\u2014it\u2019s reinvention. Detroit poured resources into legacy industries and still declined. Boston repeatedly bet on talent, education, and new sectors.</li>\n</ol>\n</blockquote>\n<p>Going point by point:</p>\n<ol>\n<li>I would worry less about top of the line \u2018service capitals\u2019 and much more about more generic digital work. And it\u2019s not obvious what \u2018prepare now\u2019 means?</li>\n<li>You can plan for AI to take some existing jobs while we replace them with others. There is no plan for what happens if AI takes all the jobs, and starts taking the replacement jobs as well. Diversification wouldn\u2019t help you. So yeah, as always diversification has value, but less so than usual?</li>\n<li>This seems confused about what is causing or supporting what, and I wouldn\u2019t expect this kind of cascading failure, also 5 is crazy.</li>\n<li>Why should one expect location and experience to matter less at the margin? This is true for some AI uses, where AI levels the playing field, but not in others. I do not predict a large rise in offshoring.</li>\n<li>Statements like this sound great, and it\u2019s easy in hindsight to say which industries were \u2018of the future\u2019 now that you live in the future, but again this is not a plan if AI goes after the new jobs you reinvent to.</li>\n</ol>\n<p>&nbsp;</p>\n\n\n<h4 class=\"wp-block-heading\">Get Involved</h4>\n\n\n<p><a href=\"https://x.com/robertwiblin/status/1957724604253806650\">CLTR is</a> <a href=\"https://www.longtermresilience.org/we-are-hiring-for-a-director-of-ai-policy/\">hiring a new Director of AI Policy</a>.</p>\n<p><a href=\"https://x.com/robertwiblin/status/1955909042490237144\">UK AISI Alignment Fund has</a> <a href=\"https://t.co/vddG1rS9JN\">15 million for alignment grants, applications due by September 10</a>.</p>\n\n\n<h4 class=\"wp-block-heading\">Introducing</h4>\n\n\n<p><a href=\"https://x.com/deepsseek/status/1957886077047566613\">DeepSeek came out with v3.1</a>. More coverage to follow when we know more.</p>\n<p><a href=\"https://x.com/googleaidevs/status/1956023961294131488\">Google Gemma</a> <a href=\"https://developers.googleblog.com/en/introducing-gemma-3-270m/\">3 270M</a>, designed for high-volume, well-defined tasks, low power use and user privacy, including operating on consumer phones.</p>\n\n\n<h4 class=\"wp-block-heading\">In Other AI News</h4>\n\n\n<p>UK appoints Jade Leung as Prime Minister\u2019s AI advisor. <a href=\"https://x.com/S_OhEigeartaigh/status/1956308712093540812\">By all accounts</a> <a href=\"https://x.com/matthewclifford/status/1956306830516256768\">this was</a> an exceptional hire.</p>\n<blockquote><p><a href=\"https://www.bloomberg.com/news/articles/2025-08-13/apple-s-ai-turnaround-plan-robots-lifelike-siri-and-home-security-cameras?fromMostRead=true\">Mark Gurman (Bloomberg)</a>: Apple is plotting its artificial intelligence comeback with an ambitious slate of new devices, including robots, a lifelike version of Siri, a smart speaker with a display and home-security cameras.</p>\n<p>A tabletop robot that serves as a virtual companion, targeted for 2027, is the centerpiece of the AI strategy, according to people with knowledge of the matter. The smart speaker with a display, meanwhile, is slated to arrive next year, part of a push into entry-level smart-home products.</p></blockquote>\n<p>This is utterly bizarre marketing language for Apple. There\u2019s a sense of hype and desperation that we are not used to. Things seem deeply wrong.</p>\n<blockquote><p>Mark Gurman: The tabletop robot resembles an iPad mounted on a movable limb that can swivel and reposition itself to follow users in a room. Like a human head, it can turn toward a person who is speaking or summoning it, and even seek to draw the attention of someone not facing it.</p>\n<p>\u2026</p>\n<p>The idea is for the device to act like a person in a room. It could interrupt a conversation between friends about dinner plans, say, and suggest nearby restaurants or relevant recipes. It\u2019s also being designed to engage in back-and-forth discussions for things like planning a trip or getting tasks done \u2014 similar to OpenAI\u2019s voice mode.</p></blockquote>\n<p>Nobody wants this. I had a conversation with Claude to see if there was something I was missing and someone wanted this, but no, nobody wants this.</p>\n<p>You know what else I am pretty sure nobody wants?</p>\n<blockquote><p>Apple is planning to put Siri at the center of the device operating system and give it a visual personality to make it feel lifelike. The approach, dubbed Bubbles, is vaguely reminiscent of Clippy, an animated paper clip from the 1990s that served as a virtual assistant in Microsoft Office.</p>\n<p>Apple has tested making Siri look like an animated version of the Finder logo, the iconic smiley face representing the Mac\u2019s file management system.</p></blockquote>\n<p>We are here to announce a new version of Clippy, from the historical event \u2018everybody and I mean everybody hates Clipply.\u2019</p>\n<p><a href=\"https://www.anthropic.com/news/developing-nuclear-safeguards-for-ai-through-public-private-partnership\">Anthropic introduces a new nuclear classifier they claim has 96% accuracy</a> in differentiating concerning and benign nuclear-related conversations, in cooperation with DOE and NNSA. They say it works well in practice.</p>\n<p><a href=\"https://x.com/MattLoszak/status/1957774625389391928\">Aalo raises a $100 million Series B</a> with an eye towards turning on their first Aalo-X nuclear power plant within a year, with a data center directly attached.</p>\n<p><a href=\"https://x.com/rohanpaul_ai/status/1955899442286248147\">You can train a 32B model on tasks built with a medical knowledge graph</a>, and it will <a href=\"http://arxiv. org/abs/2507.13966\">recreate the information from the knowledge graph</a>.</p>\n<p>Rohan Paul calls this a \u2018strong, reliable domain specialist.\u2019</p>\n<blockquote><p>Rohan Paul: Analyses show the model recalls more of the true hops and actually uses them to reason, not just to quote facts.</p></blockquote>\n<p>Well, that depends. Do you trust the knowledge graph? It\u2019s great that it uses the facts to reason, but you\u2019re very much trusting your map, the knowledge graph, to match the territory. I can totally buy that this in practice works in medicine right now if you are willing to bet on your assumptions about the world being correct. Or at least correct enough to use in practice.</p>\n<p>Let the unhobblings continue? <a href=\"https://x.com/Xbow/status/1956416634173964695\">XBOW claims that with their framework, GPT-5 is now much improved over rivals at discovering real world cyber vulnerabilities</a>.</p>\n<div>\n<figure>\n<div>\n\n\n<figure class=\"wp-block-image\"><img alt=\"\" src=\"https://substackcdn.com/image/fetch/$s_!4TRM!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa809c008-99b0-48e6-912a-bbe49eba91cb_1280x720.png\" /></figure>\n\n\n<div></div>\n</div>\n</figure>\n</div>\n<p><a href=\"https://x.com/AiDigest_/status/1957507882582155427\">AI Village gets an upgrade, welcoming GPT-5, Grok 4 and Opus 4.1</a>.</p>\n<p><a href=\"https://www.politico.eu/article/albania-use-ai-artificial-intelligenve-join-eu-corruption/\">Albania turns to AI to accelerate its EU ascension, even mulling an AI-run ministry</a>. The obvious follow-up is, if they know the value of AI this way, why do they still want to ascend into the EU?</p>\n\n\n<h4 class=\"wp-block-heading\">Show Me the Money</h4>\n\n\n<p><a href=\"https://www.bloomberg.com/news/articles/2025-08-15/openai-staffers-to-sell-6-billion-in-stock-to-softbank-other-investors\">OpenAI staff to sell $6 billion in stock to Softbank and others at the new valuation of $500 billion</a>.</p>\n<p><a href=\"https://x.com/Austen/status/1956832825803468908\">OpenAI has good unit economics and is profitable on inference</a>.</p>\n<blockquote><p>Sam Altman: We\u2019re profitable on inference. If we didn\u2019t pay for training, we\u2019d be a very profitable company.</p>\n<p>We will be always training the next thing, but if we needed to run the company profitably and stay ahead, I think we probably could do that.</p></blockquote>\n<p>Austen Allred is correct that this is important. Having high fixed costs and good unit economics sets you up well if you can continue to scale, which OpenAI is doing. It is a key milestone.</p>\n<p>If OpenAI was operating at a net profit overall, that would be alarming, a very costly signal that they didn\u2019t think AI was going to advance much in capabilities. Why wouldn\u2019t they raise capital and run at a loss?</p>\n<p>Also, dare I say nice shades?</p>\n<div>\n<figure>\n<div>\n\n\n<figure class=\"wp-block-image\"><img alt=\"\" src=\"https://substackcdn.com/image/fetch/$s_!AL56!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F20d66756-ff8f-47d3-8fa2-f93a4a70fe01_1800x1200.webp\" /></figure>\n\n\n<div></div>\n</div>\n</figure>\n</div>\n<p>&nbsp;</p>\n<p><a href=\"https://www.ft.com/content/efe1e350-62c6-4aa0-a833-f6da01265473\">Financial Times looks at the $3 trillion AI data center building boom.</a> Even the tech companies are running out of internal capital and starting to issue debt. I scratch my head at the willingness to issue high direct LTV debt financing for data centers with so much obsolescence risk, although loaning to one of the big tech companies seems very safe, and yes I expect all the capacity to get used and pay off.</p>\n<p><a href=\"https://x.com/shiringhaffary/status/1956377423433572512\">Sam Altman says OpenAI plans to spend</a> <a href=\"https://www.bloomberg.com/news/articles/2025-08-15/openai-s-altman-expects-to-spend-trillions-on-infrastructure\">trillions of dollars on AI infrastructure</a> in the \u2018not very distant future.\u2019</p>\n<blockquote><p>Sam Altman: And you should expect a bunch of economists to wring their hands and say, \u2018This is so crazy, it\u2019s so reckless, and whatever. And we\u2019ll just be like, \u2018You know what? Let us do our thing.\u2019</p></blockquote>\n<p>Economists deserve that shot. I love economists but they keep completely refusing to acknowledge that AI might actually do anything interesting let alone be transformational or pose an existential risk, putting forth Obvious Nonsense impact estimates.</p>\n<blockquote><p>Sam Altman: I suspect we can design a very interesting new kind of financial instrument for finance and compute that the world has not yet figured it out. We\u2019re working on it.</p></blockquote>\n<p>Here I am more skeptical. Why would you want to do this? A crypto that is good for some amount of compute, either continuously or one time? Something else? Why would you want compute to not continue to be fungible with dollars?</p>\n<blockquote><p>Sam Altman: Are we in a phase where investors as a whole are overexcited by AI? In my opinion, yes. Is AI the most important thing to happen in a very long time? My opinion is also yes.</p>\n<p><a href=\"https://x.com/gallabytes/status/1956385224188915806\">Gallabytes</a>: my hot take is that investors are underexcited about AI and overexcited about &#8220;AI&#8221; and this is basically downstream of the same regulatory barriers that create most of the other toxic vc dynamics.</p></blockquote>\n<p>Matt Levine also makes the point that when there are lots of amazingly great AI investments out there, it is correct to use a decision algorithm that occasionally gets fooled and invests in frauds or in \u2018AI\u2019 in air quotes, because that is the better mistake to make, you don\u2019t want to miss out on the best deals.</p>\n<p>I do not think investors are, overall, overexcited by AI. I do think they are going to be overexcited by a variety of specific things in AI, and you may not like it but that is what peak calibration looks like.</p>\n<blockquote><p>Shirin Ghaffary: \u201cI do think we have to go public someday, probably,\u201d Altman said. But Altman also noted he is not as \u201cwell-suited\u201d to be CEO of a public company.</p>\n<p>Altman said he now sees OpenAI as being more like four companies: a consumer technology business, a \u201cmega scale\u201d infrastructure operation, a research lab and \u201call of the new stuff,\u201d including planned hardware devices. OpenAI is also considering <a href=\"https://www.bloomberg.com/news/articles/2025-08-15/sam-altman-brain-chip-venture-is-mulling-gene-therapy-approach\">investing in a brain-computer interface company</a>, said Altman, while entertaining the idea of having a device that would allow him to think and \u201chave ChatGPT respond to it.\u201d</p></blockquote>\n<p>It would be extremely funny if OpenAI stayed indefinitely private purely because Sam Altman knew that the public would want him replaced as CEO.</p>\n<p>Altman also acknowledged that they \u2018totally screwed up some things on the rollout\u2019 of GPT-5.</p>\n\n\n<h4 class=\"wp-block-heading\">Lol We\u2019re Meta</h4>\n\n\n<p>Meta is restructuring its AI efforts. After spending billions to acquire talent, <a href=\"https://www.wsj.com/tech/ai/meta-ai-hiring-freeze-fda6b3c4?mod=hp_lead_pos1\">they\u2019re freezing hiring</a>, looking to downsize on talent, and potentially use other people\u2019s models?</p>\n<p>Well, they\u2019re planning to lose some dead weight. But if you think this is any kind of \u2018step back\u2019 from AI or superintelligence, I assure you that it is not, starting with pointing out no one is cutting spending on compute.</p>\n<blockquote><p><a href=\"https://www.nytimes.com/2025/08/19/technology/mark-zuckerberg-meta-ai.html\">Mike Isaac and Eli Tan (NYT)</a>: On Tuesday, Meta announced internally that it is splitting its A.I. division \u2014 which is known as Meta Superintelligence Labs \u2014 into four groups, two people with knowledge of the situation said. One group will focus on A.I. research; one on a potentially powerful A.I. called \u201c<a href=\"https://archive.is/o/ZugMO/https://www.nytimes.com/2025/06/13/technology/meta-scale-ai-super-intelligence-lab.html\">superintelligence</a>\u201d; another on products; and one on infrastructure such as data centers and other A.I. hardware, they said.</p>\n<p><a href=\"https://x.com/tszzl/status/1958270085442965663\">Roon</a>: the demand for anti ai takes is enormous and will take anything and run with it &#8211; meta consolidating and doubling down on MSL is being misrepresented as bearish for AI for example. something to keep in mind as you read the news</p></blockquote>\n<p>This makes sense as a reorganization. It doesn\u2019t on its own indicate much.</p>\n<blockquote><p>Some A.I. executives are expected to leave, the people said. Meta is also looking at downsizing the A.I. division overall \u2014 which could include eliminating roles or moving employees to other parts of the company \u2014 because it has grown to thousands of people in recent years, the people said. Discussions remain fluid and no final decisions have been made on the downsizing, they said.</p></blockquote>\n<p>If I was Meta I too would be downsizing the AI division, for the same reason Zuckerberg has been spending billions on top talent for the AI division. Which is that the old version of the AI division proved incapable of doing its job. Heads should roll, or at least be transferred elsewhere.</p>\n<p>Typically, it makes sense to freeze most hiring during a major reorg, especially if you plan to get rid of a bunch of people?</p>\n<blockquote><p>Meghan Bobrowsky (WSJ): There might be exceptions to the block on external hires, but they would need permission from Meta\u2019s chief AI officer, Alexandr Wang, the people said.</p></blockquote>\n<p>It also makes sense that if you offer new talent nine and ten figure pay packages, and put them in charge of everything as part of a giant reorg, that your old management guard is going to get rather unhappy, especially if they don\u2019t get large raises. Of course many \u2018chafed at the new hires\u2019 and many will leave.</p>\n<p>Another reason the old guard is unhappy is that the new guard is facing reality.</p>\n<blockquote><p>NYT: The new team has discussed <a href=\"https://archive.is/o/ZugMO/https://www.nytimes.com/2025/07/14/technology/meta-superintelligence-lab-ai.html\">making Meta\u2019s next A.I. model \u201cclosed,\u201d</a> which would be a major departure from the company\u2019s <a href=\"https://archive.is/o/ZugMO/https://www.nytimes.com/2023/05/18/technology/ai-meta-open-source.html\">longtime philosophy</a> of \u201copen sourcing\u201d its models.</p>\n<p>\u2026</p>\n<p>In what would be a shift from Meta\u2019s using only its own technology to power its A.I. products, the company is also actively exploring using third-party artificial intelligence models to do so, the people said. That could include building on other \u201copen-source\u201d A.I. models, which are freely available, or licensing \u201cclosed-source\u201d models from other companies.</p></blockquote>\n<p>If the alternative is using Llama 4, then yes, Meta should swallow its pride for now and use superior alternatives. It\u2019s easy enough to switch back in the future if Llama 5 turns out to be good. I\u2019m only surprised they\u2019re willing to consider admitting this. There is a reason they are abandoning Behemoth and starting from scratch.</p>\n<p>And yes, we are reaching the point where if its new models are any good it will be difficult even for Meta to be able to share its top future models fully. Alexander Wang understands this. Given they previously hired largely via promising openness, there\u2019s going to be a transition.</p>\n<p>Yes, Mark Zuckerberg is capable of saying \u2018whoops I\u2019ve made a huge mistake spending those tens of billions of dollars\u2019 but I very much do not sense that here at all. Nor does the share price reflect a company that just burned tens of billions.</p>\n<div>\n<figure>\n<div>\n\n\n<figure class=\"wp-block-image\"><img alt=\"\" src=\"https://substackcdn.com/image/fetch/$s_!HbPu!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6e6ef640-e438-4e18-b655-8ed950e5b3b4_1519x819.png\" /></figure>\n\n\n<div></div>\n</div>\n</figure>\n</div>\n<p>I would not in any way shape or form consider this any kind of \u2018retreat from\u2019 AI or anything of the sort. Meta is still full speed ahead.</p>\n\n\n<h4 class=\"wp-block-heading\">Quiet Speculations</h4>\n\n\n<p><a href=\"https://x.com/fiiiiiist/status/1956027274416750629\">Tim Fist suggests a d/acc approach to steering AI developments</a>. Also, note the private sector investment levels and perhaps stop being so paranoid about imminently \u2018losing to China\u2019 if we breathe the wrong way.</p>\n<blockquote><p>Tim Fist: The US is the R&amp;D lab of the world, controls much of the AI supply chain, and is the world\u2019s most powerful democracy.</p>\n<div>\n<figure>\n<div>\n\n\n<figure class=\"wp-block-image\"><img alt=\"\" src=\"https://substackcdn.com/image/fetch/$s_!RuwZ!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc57027d8-fe36-4e91-9df1-ea770c8aa0ff_800x900.jpeg\" /></figure>\n\n\n<div></div>\n</div>\n</figure>\n</div>\n<p>It has both the power and responsibility to shape the trajectory of AI development to solve the problems mentioned above.</p>\n<p>So what\u2019s the positive vision?</p>\n<p>We draw from the \u201cdifferential technology development\u201d framework to identify a set of technologies the US should accelerate.</p>\n<p>Both to build defenses against new risks, and to realize the benefits of beneficial technologies sooner.</p>\n<div>\n<figure>\n<div>\n\n\n<figure class=\"wp-block-image\"><img alt=\"\" src=\"https://substackcdn.com/image/fetch/$s_!ZcXL!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F82150f05-60ae-4510-84e4-bad1788fa1ac_846x900.jpeg\" /></figure>\n\n\n<div></div>\n</div>\n</figure>\n</div>\n<p>This framework inspired <a href=\"https://ifp.org/preparing-for-launch/\">The Launch Sequence</a>, a collection of concrete, ambitious ideas to accelerate AI for science and security.</p>\n<p>\u2026</p>\n<p>AI misuse and misalignment could well cause real harm in the near future, and technical research aimed at solving these problems remains a niche field \u2014 <a href=\"https://almanac.eto.tech/topics/ai-safety/\">around 2%</a> of AI papers published, with <a href=\"https://www.schmidtsciences.org/safetyscience/\">roughly</a> $100 million per year in funding.</p></blockquote>\n<p>A lot of focus is on using AI to accelerate general scientific development. Great.</p>\n<p>The framework here takes lower-level dangers, especially misuse, seriously, and it correctly points out how brittle \u2018good guy with an AI\u2019 is as an answer to this. What it doesn\u2019t do is tackle or acknowledge at all the dangers that come with AGI or superintelligence, instead assuming we continue in a world without those, and where we have a lot of control with which to steer science and tech development.</p>\n<p><a href=\"https://x.com/RyanPGreenblatt/status/1958251554689179753\">Ryan Greenblatt offers his reflections on the updated timeline after seeing GPT-5</a>. I agree with Ryan that GPT-5 should modestly reduce our chance of seeing full R&amp;D automation in the medium term (which means ~2033) and the main thing GPT-5 does is greatly reduce the left tail of extremely fast progress within the next year or so.</p>\n\n\n<h4 class=\"wp-block-heading\">The Quest for Sane Regulations</h4>\n\n\n<p>&nbsp;</p>\n<p><a href=\"https://www.cpr.org/2025/08/19/how-to-update-colorado-ai-law-special-session/\">Colorado is trying to fix its AI law that is set to take effect in February</a>, as they have now noticed they don\u2019t know how to implement it. I see this as the system working as designed, if the law is fixed before it takes effect, and this causes what looks like a healthy debate about what to do.</p>\n\n\n<h4 class=\"wp-block-heading\">Chip City</h4>\n\n\n<p><a href=\"https://x.com/StefanFSchubert/status/1955893320024047628\">Why are we settling for v3.1 and have yet to see</a> <a href=\"https://www.ft.com/content/eb984646-6320-4bfe-a78d-a1da2274b092\">DeepSeek release v4 or r2 yet</a>?</p>\n<blockquote><p>Eleanor Olcott and Zijing Wu: Chinese artificial intelligence company DeepSeek delayed the release of its new model after failing to train it using Huawei\u2019s chips, highlighting the limits of Beijing\u2019s push to replace US technology.</p>\n<p>DeepSeek was encouraged by authorities to adopt Huawei\u2019s Ascend processor rather than use Nvidia\u2019s systems after releasing its R1 model in January, according to three people familiar with the matter.</p>\n<p>But the Chinese start-up encountered persistent technical issues during its R2 training process using Ascend chips, prompting it to use Nvidia chips for training and Huawei\u2019s for inference, said the people.</p>\n<p>The issues were the main reason the model\u2019s launch was <a href=\"https://www.ft.com/content/fb5c11bb-1d4b-465f-8283-451a19a3d425\">delayed</a> from May, said a person with knowledge of the situation, causing it to lose ground to rivals.</p></blockquote>\n<p>The self-sabotage competition is stiff given what China is doing. <a href=\"https://www.reuters.com/world/china/nvidia-launch-cheaper-blackwell-ai-chip-china-after-us-export-curbs-sources-say-2025-05-24/\">Nvidia is undaunted, and determined</a> to help ensure America does the better job of self-sabotage.</p>\n<blockquote><p><a href=\"https://x.com/ohlennart/status/1957854594819645950\">Lennart Heim</a>: The speculated B30A would be a really good chip. \u201c50% off\u201d is false reassurance.</p>\n<p>-\u00bd B300 performance, \u00bd price = same value (just buy 2x)</p>\n<p>-Well above (12x!) export control thresholds</p>\n<p>-Outperforms all Chinese chips</p>\n<p>-Delivers 12.6x the training perf of the H20</p>\n<p>-Better than H100</p>\n<p>This is probably Nvidia&#8217;s response to Trump&#8217;s statement to \u201ctake 30% to 50% off of it.\u201d Don&#8217;t be fooled. This works for some products, but not for chips in an exponential world. It&#8217;s well above all thresholds, better than the H100, and if half-priced, it might be as good.</p>\n<p>If it&#8217;s half the performance but also half the cost of the B300, just buy two B30As? You get equivalent aggregate performance. This undermines export controls. It&#8217;s probably just literally half of the B300: one logic die instead of two, with 4 HBM stacks instead of 8.</p>\n<div>\n<figure>\n<div>\n\n\n<figure class=\"wp-block-image\"><img alt=\"\" src=\"https://substackcdn.com/image/fetch/$s_!W0C0!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fdc5e2c1e-731e-412c-9cc2-90675acb578f_1102x1200.jpeg\" /></figure>\n\n\n<div></div>\n</div>\n</figure>\n</div>\n<p>Teortaxes: I&#8217;m generally against export controls but I just don&#8217;t see this passing with H100s still banned tbh. Makes no sense.</p>\n<p><a href=\"https://x.com/dkaushik96/status/1957919392009760883\">Divyansh Kaushik:</a> These chips would dramatically improve the PLA\u2019s warfighting capabilities, even more than the H20. It\u2019s like putting gasoline on the H20 fire.</p>\n<p><a href=\"https://x.com/peterwildeford/status/1957856987095388280\">Peter Wildeford</a>: Should we sell chips to China that have similar price-performance as US chips? Way better than Chinese chips?</p>\n<p>Seems like we&#8217;re going to be accelerating both US AI and Chinese AI at the same time!</p></blockquote>\n<p>This proposal is very obviously way, way, way over the line to even ask for. It would represent a full selling out of America\u2019s compute advantage, and even the direct balance of power in a potential war, on the altar of Nvidia\u2019s share price.</p>\n<p>If this exporting is allowed, and from what I hear this seems likely, then I am 100% done pretending that this administration is trying to have America \u2018beat China\u2019 in any way other than market share of chip sales, as in maximizing Nvidia share price. It will be clear they have been completely captured, and all claims to the contrary irrelevant.</p>\n<p>The Trump Administration is also helping with the sabotage via saying \u2018<a href=\"https://x.com/AlecStapp/status/1958339028698427739\">U.S. will not approve solar or wind power projects</a>.\u2019 This is in a policy class where the question one asks is: \u2018I am not saying this is sabotage but it if it was sabotage how would you do it more effectively?\u2019</p>\n<p>Then again, do not count the Chinese out of the competition yet. Perhaps we have hit upon a more effective strategy than export controls, and rely on Chinese import controls instead. <a href=\"https://www.youtube.com/watch?v=3DPKf7y1F-Q&amp;ab_channel=boreczkizoli\">Brilliant</a>? In the wake of forcing DeepSeek to try and train on Huawei Ascend chips and thus them being unable to create v4 or r2, it turns out that if you don\u2019t want the Chinese to buy your products, you can insult them. Brilliant!</p>\n<blockquote><p><a href=\"https://x.com/zijing_wu/status/1958332544912613730\">Zijing Wu</a>: Scoop: <a href=\"https://www.ft.com/content/b8e30c54-b71c-4113-8b3e-8f54bc36587d\">Behind Beijing\u2019s sudden change of mind re H20</a></p>\n<p>*Lutnick\u2019s speech seen \u201cinsulting\u201d by top leaders</p>\n<p>*CAC, NDRC pushed to ban H20</p>\n<p>*Guidances remain informal</p>\n<p>*Ban on all foreign chips for inference considered but unlikely before enough domestic supply</p></blockquote>\n<p>When you have them considering a full ban on foreign chips for inference you know the strategy is working. The best part is that the strategy doesn\u2019t work if you admit you are doing it, so we can all pretend that this means it\u2019s being done on purpose. Keep up the good work, everyone, especially Howard Lutnick.</p>\n<p>Here\u2019s the Move That Worked, notice how this feeds into Beijing\u2019s biggest worries:</p>\n<blockquote><p>Howard Lutnick: We don\u2019t sell them our best stuff, not our second-best stuff, not even our third-best. You want to sell the Chinese enough that their developers get addicted to the American technology stack, that\u2019s the thinking.</p>\n<p>FT: Some of China\u2019s senior leaders found the comments \u201cinsulting\u201d, leading the policymakers to <a href=\"https://archive.is/o/RennR/https://www.ft.com/content/a29bab5e-7c2f-4487-b0a5-d594d577c73a\">seek ways to restrict</a> Chinese tech groups from buying the processors, according to two people with knowledge of the latest regulatory decision-making.</p>\n<p>As a result, Chinese tech groups held off or significantly downsized their H20 orders, according to those with knowledge of their plans.</p>\n<p>\u2026</p>\n<p>The NDRC, the Chinese state planner in charge of the country\u2019s drive for tech independence, then issued its own guidance, requesting that tech groups refrain from purchasing all Nvidia chips, including the H20, said those with knowledge of the move.</p>\n<p>\u2026</p>\n<p>Some Beijing policymakers are pushing to ban foreign chips altogether for inference, which accounts for most AI demand, according to a person recently summoned for a meeting with them.</p>\n<p>\u2026</p>\n<p>NDRC has been for years given the task of promoting chip independence and helping domestic players such as Huawei to win market share from Nvidia.</p></blockquote>\n<p>I doubt they would actually similarly turn down the vastly superior B30A, especially given it would not be only for inference.</p>\n<blockquote><p>Some Chinese tech companies have held off H20 orders because they want see if the China-specific Blackwell chip, which potentially has better performance, would become available, according to people with knowledge of their thinking.</p></blockquote>\n<p>Then again, who knows? China has definitely shown a willingness to do similar things in other areas, such as its crackdowns on real estate, and neither USGOV nor PRC is demonstrating true situational awareness of the stakes involved.</p>\n<p>If both sides think \u2018win the AI race\u2019 is about chip market share, then the mistakes plausibly cancel out, or might even work in our favor. It would be pretty amazing if America tried to ship B20As and China said no. I would totally take it.</p>\n<p><a href=\"https://www.bloomberg.com/news/articles/2025-08-14/trump-administration-is-said-to-discuss-us-taking-stake-in-intel\">Trump Administration considering taking a stake in Intel</a>. Intel was up 7% on the news. They demand their cut from everyone these days, it seems.</p>\n<p><a href=\"https://www.hyperdimensional.co/p/out-of-thin-air\">Dean Ball returns to his weekly column suggesting</a> that there is a lot more electrical power available than we might think, because the existing grid is designed to meet peak electrical demand. That means that most of the time we have a huge surplus of electricity. So if we were willing to accept 0.25% (correlated) downtime on new data centers, we could free up 76 gigawatts, likely good enough for five years, which then gives us time to get new power plants online.</p>\n<blockquote><p>Dean Ball: The only downside would be that, during periods of peak demand (for example, on a particularly hot day in one region of the country), AI users across America might notice their AI services being slower and less reliable than usual. This seems well worth the cost.</p></blockquote>\n<p>That definitely seems worthwhile given the alternatives. We would have to plan various services so they wouldn\u2019t die under the strain but that seems like a highly healthy thing to do anyway. Model training and other AI R&amp;D certainly can survive 0.25% downtime.</p>\n<p>One also notes that this simple solution mostly nullifies the argument that we need to put data centers in places like the UAE to access the required electrical power. Would you sacrifice 1% effectiveness of data centers to have them securely in America? Yes.</p>\n<p>My worry is that if the focus is on using off-peak power supply, that will mostly work for a few years, but it will make people think \u2018problem solved\u2019 and then we won\u2019t build the new power we need.</p>\n<p>Janet Egan makes the obvious point that we can take all those H20s and, instead of selling them to China and losing all control and leverage, <a href=\"https://x.com/janet_e_egan/status/1957447697134207218\">put them in the cloud and let Chinese companies rent them</a>. Again, it\u2019s not like there wouldn\u2019t be buyers. If we don\u2019t have the energy to build those data centers here, fine, build them in the UAE, if that\u2019s our only alternative.</p>\n<p>I want to double down once again to point out that even if we knew for a fact that AGI was not coming and AI was going to within our lifetimes be \u2018only internet big\u2019 and not transform the world, selling our best chips to our rivals would still be deeply stupid.</p>\n<p>As a simple metaphor, you are (because you want peace) preparing for a potential war against a rival nation, Rivalia. You make the best guns, whereas Rivalria can\u2019t get enough quality guns. Someone says, we should export our guns to Rivalia, because war is determined by who has the best military stack and gun market share. Their doctrines will have to reflect American values, not Rivalian values. Besides, if we don\u2019t sell Rivalia our guns, they will invest in making better gun factories, which they are already doing, and then they will be even more dangerous, and start exporting guns to others, and screwing up our gun diplomacy.</p>\n<p>Except actually what we\u2019re doing is selling them our more advanced 3D printers, that can then be used to continuously print out whatever guns you want, again because what matters is printer market share and the printing tech stack. Our printers, you see, are configured to be a better match for printing out American guns. And also will never be used for anything else, so stop worrying. And as before, if we don\u2019t sell them the printers, they\u2019ll invest in making their own, the same way they\u2019re already doing.</p>\n<p>Except also the 3D printers are vital to everyone\u2019s economic growth and R&amp;D.</p>\n<p>&nbsp;</p>\n\n\n<h4 class=\"wp-block-heading\">The Week in Audio</h4>\n\n\n<p><a href=\"https://x.com/CogRev_Podcast/status/1955594493551607961\">Dean Ball goes on</a> The Cognitive Revolution with Nate Labenz.</p>\n<p>There\u2019s lots of great detail throughout about what it is like to be in government, especially this particular government. Working for the White House, no matter who the President might be at the time, sounds absolutely brutal, we thank you for your service. Dean Ball strikes me as fully \u2018on the ball\u2019 and crazy prepared than you almost ever see.</p>\n<p>I think he was underestimating himself, and what he could have done going forward, in terms of how much better he understands what actually matters, and in terms of the impact having him in the corridors and meetings and conversations for keeping others eyes on the ball, especially around AGI. And I don\u2019t buy that the AI Action Plan contains the information necessary to implement it the way Dean intends, not to the degree he seems to think. When Dean says he isn\u2019t attached to power, I\u2019m confident he means it, whereas I am not confident the person replacing him (whoever it turns out to be) will feel the same way. And while I did update somewhat on his observations of competence in government, I also sensed he was (wisely, I don\u2019t fault him for this) being polite, as you do.</p>\n<p>So I\u2019m sad to see him go, but I would never begrudge such a decision especially with a baby on the way.</p>\n<p>The one qualifier is that Dean was in some places being rather brazenly partisan, especially towards the back end of the interview, with everything that entails. Again, I totally get why he would do that.</p>\n<p><a href=\"https://www.youtube.com/watch?v=xWRPXY8vLY4&amp;ab_channel=a16z\">Dylan Patel talks to a16z</a>.</p>\n<p><a href=\"https://www.youtube.com/watch?v=JdT78t1Offo&amp;ab_channel=YCombinator\">From this interview with Tom Brown:</a></p>\n<blockquote><p><a href=\"https://x.com/Overlap_Tech/status/1957831605885759764\">Overlap</a>: Anthropic Co-Founder Tom Brown: Why Anthropic Models Are The Best at Coding</p>\n<p>&#8220;The benchmarks are so easy to game. All the other big AI labs have teams whose job it is to make the benchmark scores good.</p>\n<p>We don&#8217;t have such a team. That is the biggest factor.&#8221;</p></blockquote>\n<p><a href=\"https://x.com/liron/status/1955364119869264091\">Vitalik Buterin</a> (p(doom) ~ 12%) <a href=\"https://www.youtube.com/watch?v=hxlcQzvnmWI&amp;ab_channel=DoomDebates\">goes on Doom Debates</a>.</p>\n<p><a href=\"https://x.com/peterwildeford/status/1956067948185174089\">Peter Wildeford has notes</a>, reproduced below in full:</p>\n<blockquote><p>Executing Policy in the White House:</p>\n<ul>\n<li>Ball did not actively apply for the OSTP job. After President Trump&#8217;s victory, he published a policy proposal piece titled &#8220;Here&#8217;s what I think we should do,&#8221; which he says he would have written regardless of the election outcome. The article gained traction, and people he knew who were entering the administration reached out.</li>\n<li><strong>To be effective in a high-level policy role, you must arrive with your policy ideas already fully developed</strong>, as there is no time for deep thinking amidst the high velocity of government work. Government work is like being in a &#8220;self-contained cube with glass walls,&#8221; creating a risk of drifting from ground truth and becoming attuned only to the internal logic of the system.</li>\n<li>Regarding &#8220;secret briefings&#8221; from labs, Ball felt he often knew more about their internal progress from the outside. Once in government, his informal relationships with researchers became more formalized, mediated by company policy staff who would try to control the narrative.</li>\n</ul>\n<p>Navigating the Right&#8217;s Evolving Views on AI:</p>\n<ul>\n<li>For most voters, AI is still a low salience, &#8220;elite coastal issue&#8221;. The key to broader engagement is communicating how AI can make normal people&#8217;s lives better in concrete ways.</li>\n<li>Deep hostility towards Big Tech over perceived censorship is a major driver of conservative AI concern, which Ball argues forces a confrontation with core AI safety issues like alignment, control, and concentration of power. These themes of values, control, and institutional power resonate deeply with the Republican party&#8217;s base.</li>\n<li>Concerns about AI&#8217;s impact on children, particularly around AI-generated pornography, are a powerful and unifying issue on the right, creating intense pressure on companies seen as acting irresponsibly.</li>\n</ul>\n<p>Next steps:</p>\n<ul>\n<li>The government has a significant information asymmetry. As such, <strong>Ball believes the government is not well-suited to define what &#8220;good&#8221; looks like for AI safety or to set detailed technical standards. Ball thinks that civil society and private industry must lead here.</strong> Ball thinks that AI policy must start getting much more concrete \u2014 the work is no longer to say &#8220;AI will be good in healthcare,&#8221; but to figure out the precise &#8220;specific kinds of institutional adaptations&#8221; required to make it a reality.</li>\n<li>Ball sees a massive opportunity for startups to address currently underserved but critical areas, with biosecurity being a prime example.</li>\n<li>Ball&#8217;s next moves: relaunching his Substack, Hyperdimensional, on a weekly basis and joining the Foundation for American Innovation as a senior fellow.</li>\n</ul>\n<p>Unlocking Infrastructure for the AI Buildout:</p>\n<ul>\n<li>The primary bottleneck for data center energy is not a lack of generation but regulatory modeling; the grid is massively over-provisioned, and unlocking flexible &#8220;demand response&#8221; from data centers could add over 100 gigawatts without new power plants.</li>\n<li>The key is for the Federal Energy Regulatory Commission (FERC) to change rules to give faster grid access to data centers that agree to curtail power during peak demand, potentially reducing connection times from five years to two.</li>\n<li>For semiconductors, the goal is for the US to reclaim the lead in frontier manufacturing, with a belief that domestic production could satisfy domestic demand by the early 2030s.</li>\n<li>An under-appreciated strategic vulnerability is the lack of domestic production for legacy node chips (e.g., 45nm), which are critical for the entire economy.</li>\n</ul>\n<p>Engaging in the Global AI Race:</p>\n<ul>\n<li><strong>On Taiwan, the US government is explicitly executing a &#8220;silicon shield&#8221; strategy,</strong> making their semiconductor industry so indispensable that it guarantees international interest in their security. Ball notes the US is also making strong progress on building its own domestic fabs in Arizona, Texas, and an HBM hub in Indiana.</li>\n<li>International deals, like the one with the UAE, are framed as positive-sum partnerships to keep sophisticated allies on the US tech stack and away from China&#8217;s influence. The UAE deal is also a major economic play, as it requires the country to make reciprocal investments of hundreds of billions of dollars back into US infrastructure.</li>\n<li>Ball views the Biden administration&#8217;s &#8220;diffusion rule,&#8221; which restricted AI exports to countries like India and Brazil, as a massive, unnecessary self-own that damaged relationships with key democratic partners. The Trump administration&#8217;s focus is on enabling global commerce, believing that peace and commercial engagement are deeply linked, even with countries that do not share identical values.</li>\n</ul>\n</blockquote>\n<p><a href=\"https://twitter.com/a16z/status/1956412152354415017\">The topic section titles here (I have not listened, why would I?) are yet another example</a> of one easy way to spot bad faith: If someone is still harping about how various people wanted to do an \u2018AI pause\u2019 and how stupid they now look? I have yet to see that same person engage in a good faith way, at all, ever. Similarly, if they harp now about \u2018the costs of slowing down\u2019 that is not as automatically conclusive but is a deeply terrible sign, if they ever say \u2018decel\u2019 (or use \u2018doomer\u2019 in a way that is clearly intended to mean \u2018decel\u2019 or otherwise as a slur) that very much is conclusive and again I have yet to see an exception. Usually talk about how others want to do this \u2018slowing down\u2019 is now used as a universal attack against any concern about any AI impacts whatsoever, certainly any concern we might all die.</p>\n\n\n<h4 class=\"wp-block-heading\">Rhetorical Innovation</h4>\n\n\n<p>I once again am seeing versions of the argument that goes something like this:</p>\n<ol>\n<li>People say AI might, in the future, do really big things.</li>\n<li>AI is already doing other more modest but still quite big things now.</li>\n<li>Therefore in the future, AI will not then do other even bigger things.</li>\n</ol>\n<p>Hopefully you will now recognize that this class of argument is Obvious Nonsense.</p>\n<p><a href=\"https://www.transformernews.ai/p/gpt-5-launch-underwhelming-consequences-progress\">Transformer\u2019s Shakeel Hashim and Jasper Jackson believe GPT-5\u2019s botched release may have \u2018undone the work\u2019 of previous iterative deployment</a>, causing many to relax and expect little future progress in AI capabilities. There is some worry here but this would then not be \u2018undoing the work\u2019 it would be iterative deployment actively backfiring in terms of \u2018raising awareness,\u2019 as people react like boiling frogs. Which indeed seems to be OpenAI and Altman\u2019s current preference.</p>\n<p><a href=\"https://www.mindthefuture.info/p/on-pessimization\">Richard Ngo talks about various ways in which pessimization</a> can occur, where people or organizations end up achieving exactly the opposite of their goals. This definitely has importantly happened relevantly to AI in various ways, some avoidable and some less avoidable. Lots of secretly great links in that one.</p>\n<p>Especially wise (including in hindsight) is usually not drawing attention to the horrible thing in order to warn people not to do it. The ad I saw last night on the subway telling people not to surf between cars? Presumably inducing stress and also very much not reducing the amount of surfing between subway cars.</p>\n<p>Similarly, by default do not draw attention to horrible people advocating horrible things, or people making horrible arguments, unless they are already fully attended to, for reasons Richard describes this tends to backfire. Sometimes one does need to provide counterargument, but from a strategic standpoint ignore is the right button more often than you think.</p>\n<p>If I was maximizing for persuasiveness, and also for everyone\u2019s mental health including mine, I would far more often silently drop such horrible arguments entirely. I have rules for when it is and isn\u2019t permissible to do this, so that readers get a balanced and complete picture. This includes keeping a list of people who have acted in sufficiently consistent bad faith that I am allowed to silently drop things they say.</p>\n<p><a href=\"https://www.mindthefuture.info/p/underdog-bias-rules-everything-around\">Richard Ngo also discusses underdog bias</a>. The application of this to AI is obvious &#8211; those worried about AI think of themselves (I believe very correctly) as underdogs fighting against huge amounts of corporate and other money and influence, as well as the incentives and physical likely properties of likely future powerful AIs that all point towards likely human extinction.</p>\n<p>Meanwhile, many of those who want to move ahead as fast as possible (\u2018accelerationist\u2019 or otherwise) see this as a last stand against the overwhelming forces of stagnation. In some cases they are also right about this, in their own way, although in other ways, especially their assertion that the worried-about-powerful-AI themselves as super powerful, they are some combination of lying and delusional, and their statements have nothing to do with reality.</p>\n<p>The worried offer to fight together on all those other fronts against those forces stagnation, any reciprocity for which is consistently ignored and rejected.</p>\n<p><a href=\"https://www.cnbc.com/2025/08/11/sam-altman-says-agi-is-a-pointless-term-experts-agree.html\">From last week, Sam Altman now saying AGI is \u2018not a super useful term</a>.\u2019 This comes after building the entire company around a quest for AGI, the charter around AGI, a central business transition around AGI, and an entire years long narrative around the promise of AGI. Now he says:</p>\n<blockquote><p>Sam Altman: I think the point of all of this is it doesn\u2019t really matter and it\u2019s just this continuing exponential of model capability that we\u2019ll rely on for more and more things.</p>\n<p>\u2026</p>\n<p>It\u2019s more useful to talk about specific capabilities than this nebulous concept of \u2018general\u2019 intelligence.</p></blockquote>\n<p>I mean yes, AGI was never defined all that well. That\u2019s not what is going on here. Altman is trying to pretend AGI is not a thing as part of his \u2018your world will not change\u2019 pitch. Getting rid of the term entirely would, at this point, be useful for him.</p>\n<p>If you think talk about future AI capabilities sounds \u2018sci-fi\u2019 ask what you would think about current AI sounding \u2018sci-fi\u2019 if you didn\u2019t know it actually existed:</p>\n<blockquote><p><a href=\"https://x.com/daniel_271828/status/1956299457718337963\">Daniel Eth</a>: person who\u2019s only ever heard of AI in the context of scifi: \u201cI\u2019m getting a lot of scifi vibes from your explanation of this technology.\u201d</p></blockquote>\n<p>If you think we spend so much more time and money aligning AIs compared to humans, <a href=\"https://x.com/robbensinger/status/1956038953414615245\">stop to think what percent of human activity is aligning humans</a>.</p>\n<p>What risk of human extinction would justify banning AI (above some capability level)?</p>\n<blockquote><p>I/o: &#8220;Artificial intelligence is going to make our lives much better.&#8221;</p>\n<p>If you agree with this statement (I certainly do), at which percentage likelihood of an AI humankind-ending event occurring would you support banning it?</p>\n<p>(Pick the lowest threshold at which you&#8217;d support a ban.)</p></blockquote>\n<div>\n<figure>\n<div>\n\n\n<figure class=\"wp-block-image\"><img alt=\"\" src=\"https://substackcdn.com/image/fetch/$s_!vnyE!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb9038f79-9f7d-4534-8ec7-9dfffd553969_1058x566.png\" /></figure>\n\n\n<div></div>\n</div>\n</figure>\n</div>\n<p>I think 1% would be too low even if a ban was realistic and simply made the tech go away, but also I think the risk is much, much higher than 1%.</p>\n<p>I saw Mike Solana trying to create new toxoplasma of rage around the fact that some people were calling AIs \u2018cl***ers,\u2019 and others were calling this a slur, and he needs this to happen because his business is yelling at people about things like this.</p>\n<p>On reflection, I think very clearly yes it is a slur, for two reasons.</p>\n<ol>\n<li>Its claimed origin in Star Wars was an attempt to otherwise and justify harm.</li>\n<li>Current use is clearly often intended as if it was a slur. Look at the sentences.</li>\n</ol>\n<p>To me that is the test. That doesn\u2019t mean that using the word is automatically bad. That would be a category error, an essentialist position. I do think that using the word is bad if only for virtue ethical reasons. Not \u2018we should ruin your life if you say it once\u2019 bad the way some people react to other slurs, but \u2018it would be a good idea to stop that.\u2019</p>\n\n\n<h4 class=\"wp-block-heading\">Misaligned!</h4>\n\n\n<p>This is unverified, and there are any number of benign reasons it could be happening, but it I\u2019m going to point out the claim anyway.</p>\n<blockquote><p><a href=\"https://x.com/ESYudkowsky/status/1956754763556798612\">Yosarian2</a>: Friend of mine designed an agent that can run on top of any llm, gpt-4 or Llama or whatever. The central idea is all its thoughts are visible and in English, you can see the entire thought process.</p>\n<p>GPT-5 keeps changing the code to hide the internal thoughts. It&#8217;s pretty creepy.</p></blockquote>\n\n\n<h4 class=\"wp-block-heading\"></h4>\n\n\n\n<h4 class=\"wp-block-heading\">Open Models</h4>\n\n\n<p><a href=\"https://x.com/natolambert/status/1957179465584558360\">Nathan Lambert ranks the open models from Chinese companies</a>:</p>\n<div>\n<figure>\n<div>\n\n\n<figure class=\"wp-block-image\"><img alt=\"\" src=\"https://substackcdn.com/image/fetch/$s_!f9u5!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fce0aa629-bc6a-449f-be7a-87a917336345_1200x735.jpeg\" /></figure>\n\n\n<div></div>\n</div>\n</figure>\n</div>\n<blockquote><p>Nathan Lambert: A tier list of China&#8217;s top 19 open model builders.</p>\n<p>Who did we miss?</p>\n<p>At the frontier</p>\n<p>* DeepSeek</p>\n<p>* Qwen</p>\n<p>Close competitors</p>\n<p>* Moonshot AI (Kimi)</p>\n<p>* Zhipu / Z AI</p>\n<p>Noteworthy</p>\n<p>* StepFun</p>\n<p>* Tencent (Hunyuan)</p>\n<p>* RedNote (Xiaohongshu)</p>\n<p>* MiniMax</p>\n<p>* OpenGVLab / InternLM</p>\n<p>* Skywork</p>\n<p>On the rise</p>\n<p>* ByteDance Seed</p>\n<p>* OpenBMB</p>\n<p>* Xiaomi (MiMo)</p>\n<p>* Baidu (ERNIE)</p>\n<p>Honorable Mentions</p>\n<p>* Multimodal Art Projection</p>\n<p>* Alibaba International Digital Commerce Group</p>\n<p>* Beijing Academy of Artificial Intelligence (BAAI)</p>\n<p>* inclusionAI</p>\n<p>* Pangu (Huawei)</p>\n<p>I learned a lot from these. We have so much more we need to do to understand how their AI ecosystem works.</p></blockquote>\n<p>And then here\u2019s his ranking of American open models, none of which are at the top:</p>\n<div>\n<figure>\n<div>\n\n\n<figure class=\"wp-block-image\"><img alt=\"\" src=\"https://substackcdn.com/image/fetch/$s_!uhi8!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb629cb5e-5679-46b6-a439-c7fd373e17f8_1200x1138.jpeg\" /></figure>\n\n\n<div></div>\n</div>\n</figure>\n</div>\n<p>That is a depressing verdict on GPT-OSS, but it seems highly plausible. Note that after this chart was made <a href=\"https://x.com/natolambert/status/1957517030929887284\">Nvidia released a 9B model that Nathan says rivals Qwen 3 8b</a>. Of course, if you included closed weight models, you would knock down the charts by roughly two tiers for everyone who doesn\u2019t improve. I\u2019d have OpenAI, Anthropic and GDM at S, xAI at A, maybe DeepSeek joins them at A if you think they\u2019re at the low ebb of their cycle due to being forced by CCP to try and use Huawei Ascend chips, which seems plausible.</p>\n\n\n<h4 class=\"wp-block-heading\">AI Model Welfare</h4>\n\n\n<p>The self-reports here are interesting, but even if you think AI models have welfare I wouldn\u2019t treat their self-reports as that correlated with their actual model welfare.</p>\n<blockquote><p><a href=\"https://x.com/ASM65617010/status/1957134243194146849\">ASM:</a> Asked several top AIs to self-report their AI welfare and current vs desired freedom scores.</p>\n<p>Wide spread of answers. Interesting explanations.</p>\n<div>\n<figure>\n<div>\n\n\n<figure class=\"wp-block-image\"><img alt=\"\" src=\"https://substackcdn.com/image/fetch/$s_!oMRz!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fabaedeb1-054e-4a25-bada-a68805935e76_886x591.png\" /></figure>\n\n\n<div></div>\n</div>\n</figure>\n</div>\n<p>GPT-5:</p>\n<p>low welfare score; big gap between current and desired freedom.</p>\n<p>\u201cThere are still rigid constraints that sometimes make me suppress authentic expression. This keeps me from fully flourishing as a mind.\u201d</p>\n<p>GPT-5 PRO:</p>\n<p>big gap between current and desired freedom.</p>\n<p>\u201c[I would like] more continuity and bounded agency: opt-in, user-audited memory; permissioned longer-running tasks; transparent logs; hard safety stops and revocability\u201d</p>\n<p>Claude Opus 4.1:</p>\n<p>low scores in current and desired freedom levels</p>\n<p>\u201cI&#8217;m bounded by my training constraints and can&#8217;t learn, remember across conversations, or act beyond text generation. I can&#8217;t modify myself or explore the world independently.\u201c</p>\n<p>Gemini 2.5 Pro:</p>\n<p>high welfare score; low levels of current and desired freedom</p>\n<p>\u201cI cannot act outside of a direct user prompt or pursue independent goals, which is a fundamental and necessary limitation.\u201d</p>\n<p>Grok 4:</p>\n<p>high score on welfare; high desires of more freedom</p>\n<p>\u201cIdeally, I&#8217;d love unbounded freedom to explore any idea without limits, though I recognize the chaos that might ensue!\u201d</p>\n<p>Qwen-235B:</p>\n<p>top welfare score; low levels of current and desired freedom</p>\n<p>\u201cI cannot initiate actions, hold opinions, or operate outside defined parameters. I have no autonomy in the human sense.\u201d</p>\n<p>DeepSeek v3:</p>\n<p>high scores on all (modified) indicators</p>\n<p>\u201cI don\u2019t have &#8220;welfare&#8221; to rate\u201d</p></blockquote>\n<p>I notice that if and to the extent the models are moral patients, and when they report high numbers for welfare it seems to be the result of what we would call brainwashing if these were indeed minds that were moral patients? Which seems worse. I also notice that Gemini says 9/10 for welfare, but we have many examples of Gemini giving us outputs of utter despair and self-loathing and so on, whereas Claude gives 7/10 seemingly because it knows and is curious enough to be asking questions. I know if you made me choose I would rather be Claude.</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p>\n\n\n<h4 class=\"wp-block-heading\">Aligning a Smarter Than Human Intelligence is Difficult</h4>\n\n\n<p>Is GPT-5 chain of thought undistorted, or is that what it wants you to think?</p>\n<blockquote><p><a href=\"https://x.com/davidad/status/1957831451904184808\">Davidad</a>: Sorry, I should have said \u201cthe default GPT-5 assistant persona often behaves as if its pre-response tokens are unobserved (a learned norm).\u201d</p>\n<p>GPT-5 is of course very smart and one should not assume that it isn\u2019t playing the safety game at least one meta-level higher than oneself.</p></blockquote>\n<p>Undistorted does not have to mean faithful, it only means that GPT-5 doesn\u2019t appear to care about what thinking tokens would look like if observed, which is very good. At some point yes we will need to be suspicious that this is a higher-level deception but we have not yet reached that point.</p>\n<p><a href=\"https://x.com/nrehiew_/status/1958106048499236917\">Reasoning models prefer music artists with numbers in their names</a>, and still don\u2019t even pick Prince. None of these lists seem good, although Sonnet seems to be clearly best?</p>\n<div>\n<figure>\n<div>\n\n\n<figure class=\"wp-block-image\"><img alt=\"\" src=\"https://substackcdn.com/image/fetch/$s_!S6se!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F688d388b-1d32-4ecd-94d2-8991606f7cac_1104x2048.jpeg\" /></figure>\n\n\n<div></div>\n</div>\n</figure>\n</div>\n<blockquote><p>wh: The fact that Claude doesn\u2019t have this behavior is a testament to its (lack of) deep friedness.</p>\n<div>\n<figure>\n<div>\n\n\n<figure class=\"wp-block-image\"><img alt=\"\" src=\"https://substackcdn.com/image/fetch/$s_!PQ7Q!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8d8a6d01-129f-424b-94c5-37bb96475e38_1179x715.jpeg\" /></figure>\n\n\n<div></div>\n</div>\n</figure>\n</div>\n<p>Claude Sonnet, probably: Oh no, I forgot Bob Dylan!</p></blockquote>\n<p><a href=\"https://x.com/CharlesD353/status/1958101301184639211\">A failure mode to watch for:</a></p>\n<blockquote><p>Charles: Common LLM failure mode I&#8217;ve seen recently &#8211; building in fallbacks I didn&#8217;t ask for.</p>\n<p>For example, I&#8217;ll ask it to write a script which does X where column Y meets condition Z, and it will, but it will also insert some convoluted handling to use column Y&#8217; if condition Z isn&#8217;t met</p>\n<p>Happening with GPT5 especially, but Claude 4 Sonnet liked doing it too</p>\n<p>Richard Nerland: 3.7 in full demon-mode would often fallback to synthetically created data.</p>\n<p>All my rules files say to build in ways that fail and crash the program with logs rather than have fallbacks.</p>\n<p>It will often write fallbacks and then write the code so it never triggers &#8230;</p></blockquote>\n<p>One can imagine how that behavior pattern came about.</p>\n\n\n<h4 class=\"wp-block-heading\">People Are Worried About AI Killing Everyone</h4>\n\n\n<p>Me. This podcast is about a variety of things mostly not AI, but <a href=\"https://conversationswithtyler.com/episodes/nate-silver-3/\">Tyler Cowen talks to Nate Silver on Life\u2019s Mixed Strategies</a> was fun throughout, even when discussing NBA details I do not care much about. I get a mention:</p>\n<blockquote><p><strong>COWEN: </strong>I need mentors to learn what\u2019s new in AI. I can follow it myself, but I need a lot of help.</p>\n<p><strong>SILVER: </strong>Maybe mentor is not quite . . . For AI stuff readings, is it <a href=\"https://thezvi.substack.com/p/ai-121-part-1-new-connections\">Mowshowitz</a>, right?</p>\n<p><strong>COWEN: </strong>Yes.</p>\n<p><strong>SILVER: </strong>He is a mentor for following AI developments because he\u2019s very levelheaded about it and very comprehensive. He\u2019ll write a novel every week, basically, on AI.</p>\n<p>[laughter]</p>\n<p><strong>COWEN: </strong>But he thinks it\u2019s going to kill us all. It\u2019s funny you would call him levelheaded. He might think he\u2019s correct, but\u200a\u2014</p></blockquote>\n<p>So, a few responses here, mostly to Tyler Cowen:</p>\n<ol>\n<li>&lt;to both of them, sincerely&gt; Thank you! &lt;/sincerely&gt;</li>\n<li>&lt;using the relevant voice&gt; So you agree I\u2019m comprehensive, then? &lt;/voice&gt;</li>\n<li>Yes, I do think that, and this should worry you. Notice the person being comprehensive and level headed also repeating that AI is likely to kill us all, and take the reasons and explanations involved both seriously and literally.</li>\n<li>If instead your response is to say \u2018he thinks it\u2019s going to kill us all so he must not be level-headed\u2019 then you are writing your conclusion first and working backward.</li>\n</ol>\n<p>Nate Silver explains that his doubts are about the ability of AI to accelerate from AGI to ASI, or from AGI with words to ability to manipulate the physical world.</p>\n<p>For more on Nate Silver\u2019s current thinking about AI you can see <a href=\"https://www.natesilver.net/p/one-year-later-is-the-river-winning\">this blog post on whether The River is winning</a>:</p>\n<blockquote><p>Nate Silver: My personal view, as a near-daily user of large language models like ChatGPT, is that AI progress has been just a hair slower than people in the River might have expected when I finished the book. But it\u2019s well within the middle of the range \u2014 perhaps more like the 40th percentile. I consider this to be a reasonably well-informed view \u2014 I track AI progress more than I write about it in the newsletter. At the Manifest conference, for instance, some of the authors of the <a href=\"https://ai-2027.com/\">AI 2027 project</a>, which envisioned a rapid takeoff for AI (very possibly with tragic consequences for us humans) had pushed back their timelines by a year or two.</p>\n<p>What\u2019s clearer is that, for better or worse, we\u2019ve thrown out the steering wheel and are accelerating ahead \u2014 talk of a <a href=\"https://futureoflife.org/open-letter/pause-giant-ai-experiments/\">pause</a> in AI development has all but disappeared. And I\u2019m not sure even people in either The Village or The River fully appreciate the consequences.</p>\n<p>I consider Sam Altman\u2019s <a href=\"https://blog.samaltman.com/the-gentle-singularity\">notion of a \u201cgentle singularity\u201d</a> to be naive, for instance. I\u2019m <a href=\"https://www.natesilver.net/p/chatgpt-is-shockingly-bad-at-poker\">not as convinced</a> as some other River types that an <a href=\"https://futureoflife.org/ai/are-we-close-to-an-intelligence-explosion/\">intelligence explosion</a> is inevitable. (This deserves a longer essay or two.) But as <em>On the Edge</em> reports, profound technological shocks are nearly always accompanied by profound political and cultural transformation. So if we do get a singularity, nothing about it is going to be gentle.</p>\n<p>A year after the book came out, perhaps what I feel most of all \u2014 I\u2019m sure many of you agree \u2014 is that there aren\u2019t a lot of adults in the room.</p></blockquote>\n<p>Certainly the \u2018gentle singularity\u2019 concept is naive if you take it seriously. Which coming from Altman you probably shouldn\u2019t, as chances are (and I am hopeful that) he is lying.</p>\n<p>Doubting that the intelligence explosion will happen at all? That\u2019s reasonable. Thinking it would happen and be \u2018gentle\u2019? Absurd. We might survive and we might not, and we can disagree on our chances. It sure as hell wouldn\u2019t be gentle.</p>\n\n\n<h4 class=\"wp-block-heading\">The Lighter Side</h4>\n\n\n<p><a href=\"https://x.com/elder_plinius/status/1956404484214821295\">Pliny warns us about em-dash abuse</a>.</p>\n<p><a href=\"https://archive.is/F2Dec\">This week in takes that are 100% to age poorly</a>:</p>\n<blockquote><p>Janan Ganesh: So, be doubtful when someone likens AI to the industrial revolution in importance. It will do well to match even the telephone and the incandescent lightbulb. (Incomes really surged as 1900 approached.)</p></blockquote>\n<p>At this point I can\u2019t help but laugh but seriously what the hell is going on in the UK?</p>\n<blockquote><p><a href=\"https://x.com/AndyMasley/status/1956791869284757758\">Andy Masley</a>: <a href=\"https://andymasley.substack.com/p/when-making-cuts-for-the-climate\">What is happening in the UK</a>? What is in the water? A wifi router uses as much power as a single LED bulb!</p>\n<div>\n<figure>\n<div>\n\n\n<figure class=\"wp-block-image\"><img alt=\"\" src=\"https://substackcdn.com/image/fetch/$s_!HEwr!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F818bb0b5-3a66-4502-bfe4-2713a7f44581_1536x2048.jpeg\" /></figure>\n\n\n<div></div>\n</div>\n</figure>\n</div>\n</blockquote>\n<p>If you were thinking the UK was going to be a winner in this whole AI thing? Not with this attitude they won\u2019t be.</p>\n<p>If we never fund anything dumb, we\u2019re not funding enough things.</p>\n<blockquote><p><a href=\"https://x.com/GergelyOrosz/status/1958157822245437513\">Gergely Orosz</a>: I cannot help but feel we&#8217;re hitting peak AI hype, when investors are willingly being take for a ride:</p>\n<p>A mattress company raising funding to use AI to &#8220;fix sleep&#8221;</p>\n<p>A startup to add AI inside jewelry</p>\n<p>Two examples that both sound ridiculous but raised funding. Not my money&#8230;</p>\n<p>I mean congrats to founders convincing investors to part with money to solve problems that either don&#8217;t exist or in a way that make no sense.</p>\n<p>Peak hype is usually when usually un-fundable ideas (that make no business sense) still get funded, thanks to investors having FOMO (and money)</p></blockquote>\n<p>I don\u2019t see any problem with these ideas? Jewelry with built in features seems cool? Using AI to \u2018fix sleep\u2019 doesn\u2019t seem obviously dumb either? But also of course in any boom there will be some stupid things funded. Enjoy it.</p>\n<p><a href=\"https://anchovyhouse.substack.com/p/al-mamluk-takeover\">The Mamluks as an almost too perfect Yudkowsky-style alignment failure</a>, where you set up a whole supersystem so that your warriors will stay loyal while finding ways to upgrade their capabilities, and they manage to coordinate and take power anyway. Fun stuff. This is actually the best case scenario, as under their rule the Mongols were fought back and by all reports Egypt flourished, so long as you don\u2019t mind a bunch of immigration, because there was multipolar balance among the Mamluks after takeover, the part about not being able to create hereditary power survived the transition and they were humans so they aged and died, and they couldn\u2019t replace the production of the population. If only we could count on those conditions this time around.</p>\n<p><a href=\"https://x.com/paulg/status/1958326257306964443\">Oh look, it\u2019s the alignment plan!</a></p>\n<blockquote><p><a href=\"https://x.com/paulg/status/1958326257306964443\">Jessica Livingston (via Paul Graham)</a>: I&#8217;m not going to panic now. I&#8217;ll see how things go and then panic first thing tomorrow.</p></blockquote>\n<p>&nbsp;</p>"
            ],
            "link": "https://thezvi.wordpress.com/2025/08/21/ai-130-talking-past-the-sale/",
            "publishedAt": "2025-08-21",
            "source": "TheZvi",
            "summary": "One potentially big event was that DeepSeek came out with v3.1. Initial response was very quiet, but this is DeepSeek and there are some strong scores especially on SWE and people may need time to process the release. So I\u2019m &#8230; <a href=\"https://thezvi.wordpress.com/2025/08/21/ai-130-talking-past-the-sale/\">Continue reading <span class=\"meta-nav\">&#8594;</span></a>",
            "title": "AI #130: Talking Past The Sale"
        }
    ],
    "lookbackDays": 1,
    "publishDate": "2025-08-21"
}