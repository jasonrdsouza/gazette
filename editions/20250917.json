{
    "articles": [
        {
            "content": [],
            "link": "https://www.ssp.sh/blog/why-i-dont-research/",
            "publishedAt": "2025-09-17",
            "source": "Simon Spati",
            "summary": "<p>When I [[Writing|write]], I deliberately don&rsquo;t research. The fun of writing is to express myself and explain something in my head. Not researching something and losing focus. Recall what I read is not fun to me. I want to share <em>my</em> opinions.</p> <p>In a way, my research is taking notes on my <a href=\"https://www.ssp.sh/brain\" rel=\"noopener noreffer\" target=\"_blank\">Second Brain</a>, all the time, everywhere. But not to one topic, about everything, not connected yet.</p> <p>When I write these notes, sometimes very old ones can make all the difference. It&rsquo;s not really research in that sense, more of an idea insights creation.</p>",
            "title": "Why I Don't Research; and Write from Experience"
        },
        {
            "content": [
                "<div>\n<figure>\n<div>\n\n\n<figure class=\"wp-block-image\"><img alt=\"\" src=\"https://substackcdn.com/image/fetch/$s_!swNf!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Febd13e7a-9fec-4138-92f0-c0d5e68dc1c6_311x490.png\" /></figure>\n\n\n<div></div>\n</div>\n</figure>\n</div>\n\n\n<h4 class=\"wp-block-heading\">No, Seriously, If Anyone Builds It, [Probably] Everyone Dies</h4>\n\n\n<p>My very positive full review was briefly accidentally posted and emailed out last Friday, whereas the intention was to offer it this Friday, on the 19th. I\u2019ll be posting it again then. If you\u2019re going to read the book, which I recommend that you do, you should read the book first, and the reviews later, especially mine since it goes into so much detail.</p>\n<p><a href=\"https://ifanyonebuildsit.com/\">If you\u2019re convinced, the book\u2019s website is here</a> and <a href=\"https://www.amazon.com/Anyone-Builds-Everyone-Dies-Superhuman/dp/0316595640?maas=maas_adg_DE286862CE7FE69CACBF065FBD1D5CCC_afap_abs&amp;ref_=aa_maas&amp;tag=maas\">the direct Amazon link is here</a>.</p>\n<p>In the meantime, for those on the fence or who have finished reading, here\u2019s what other people are saying, including those I saw who reacted negatively.</p>\n<div>\n\n\n<span id=\"more-24726\"></span>\n\n\n</div>\n\n\n<h4 class=\"wp-block-heading\">Quotes From The Book\u2019s Website</h4>\n\n\n<blockquote><p>Bart Selman: Essential reading for policymakers, journalists, researchers, and the general public.</p>\n<p>Ben Bernanke (Nobel laureate, former Chairman of the Federal Reserve): A clearly written and compelling account of the existential risks that highly advanced AI could pose to humanity. Recommended.</p>\n<p>Jon Wolfsthal (Former Special Assistant to the President for National Security Affairs): A compelling case that superhuman AI would almost certainly lead to global human annihilation. Governments around the world must recognize the risks and take collective and effective action.</p>\n<p>Suzanne Spaulding: The authors raise an incredibly serious issue that merits \u2013 really demands \u2013 our attention.</p>\n<p>Stephen Fry: The most important book I&#8217;ve read for years: I want to bring it to every political and corporate leader in the world and stand over them until they&#8217;ve read it!</p>\n<p>Lieutenant General John N.T. \u201cJack\u201d Shanahan (USAF, Retired, Inaugural Director of the Department of Defense Joint AI Center): While I&#8217;m skeptical that the current trajectory of AI development will lead to human extinction, I acknowledge that this view may reflect a failure of imagination on my part. Given AI&#8217;s exponential pace of change there&#8217;s no better time to take prudent steps to guard against worst-case outcomes. The authors offer important proposals for global guardrails and risk mitigation that deserve serious consideration.</p>\n<p>R.P. Eddy: This is our warning. Read today. Circulate tomorrow. Demand the guardrails. I&#8217;ll keep betting on humanity, but first we must wake up.</p>\n<p>George Church: Brilliant\u2026Shows how we can and should prevent superhuman AI from killing us all.</p>\n<p>Emmett Shear: Soares and Yudkowsky lay out, in plain and easy-to-follow terms, why our current path toward ever-more-powerful AIs is extremely dangerous.</p>\n<p>Yoshua Bengio (Turing Award Winner): Exploring these possibilities helps surface critical risks and questions we cannot collectively afford to overlook.</p>\n<p>Bruce Schneier: A sober but highly readable book on the very real risks of AI.</p></blockquote>\n\n\n<h4 class=\"wp-block-heading\">Positive Reviews</h4>\n\n\n<p><a href=\"https://www.astralcodexten.com/p/book-review-if-anyone-builds-it-everyone?utm_source=post-email-title&amp;publication_id=89120&amp;post_id=171794222&amp;utm_campaign=email-post-title&amp;isFreemail=true&amp;r=67wny&amp;triedRedirect=true&amp;utm_medium=email\">Scott Alexander\u2019s very positive review</a>.</p>\n<p><a href=\"https://x.com/HumanHarlan/status/1967921420681482349\">Harlan Stewart created a slideshow of various favorable quotes</a>.</p>\n<p><a href=\"https://x.com/mattyglesias/status/1967765768948306275\">Matthew Yglesias recommends the book</a>.</p>\n<p>As some comments note the book\u2019s authors do not actually think there is an outright 0% chance of survival, <a href=\"https://x.com/ESYudkowsky/status/1923785112333975934\">but think it is on the order of 0.5%-2%</a>.</p>\n<blockquote><p>Matthew Yglesias: I want to recommend the new book \u201cIf Anyone Builds It, Everyone Dies\u201d by @ESYudkowsky and @So8res.</p>\n<p>The line currently being offered by the leading edge AI companies \u2014 that they are 12-24 months away from unleashing superintelligent AI that will be able to massively outperform human intelligence across all fields of endeavor, and that doing this will be safe for humanity \u2014 strikes me as fundamentally non-credible.</p>\n<p>I am not a \u201cdoomer\u201d about AI because I doubt the factual claim about imminent superintelligence. But I endorse the conditional claim that unleashing true superintelligence into the world with current levels of understanding would be a profoundly dangerous act. The question of how you could trust a superintelligence not to simply displace humanity is too hard, and even if you had guardrails in place there\u2019s the question of how you\u2019d keep them there in a world where millions and millions of instances of superintelligence are running.</p>\n<p>Most of the leading AI labs are run by people who once agreed with this and once believed it was important to proceed with caution only to fall prey to interpersonal rivalries and the inherent pressures of capitalist competition in a way that has led them to cast their concerns aside without solving them.</p>\n<p>I don\u2019t think Yudkowsky &amp; Soares are that persuasive in terms of solutions to this problem and I don\u2019t find the 0% odds of survival to be credible. But the risks are much too close for comfort and it\u2019s to their credit that they don\u2019t shy away from a conclusion that\u2019s become unfashionable.</p></blockquote>\n<p><a href=\"https://www.nytimes.com/2025/09/12/technology/ai-eliezer-yudkowsky-book.html\">New York Times profile of Eliezer Yudkowsky</a> by Kevin Roose is a basic recitation of facts, which are mostly accurate. Regular readers here are unlikely to find anything new, and I agree with Robin Hanson that it could have been made more interesting, but as New York Times profiles go \u2018fair, mostly accurate and in good faith\u2019 is great.</p>\n<p><a href=\"https://stevenadler.substack.com/p/review-if-anyone-builds-it-everyone?triedRedirect=true\">Steven Adler goes over the book\u2019s core points</a>.</p>\n<p><a href=\"https://x.com/robbensinger/status/1968140241103114674\">Here is a strong endorsement from Richard Korzekwa</a>.</p>\n<blockquote><p>Richard Korzekwa: One of the things I&#8217;ve been working on this year is helping with the launch this book, out today, titled If Anyone Builds It, Everyone Dies. It&#8217;s ~250 pages making the case that current approaches to AI are liable to kill everyone. The title is pretty intense, and conveys a lot of confidence about something that, to many, sounds unlikely. But Nate and Eliezer don&#8217;t expect you to believe them on authority, and they make a clear, well-argued case for why they believe what the title says. I think the book is good and I recommend reading it.</p>\n<p>To people who are unfamiliar with AI risk: The book is very accessible. You don&#8217;t need any background in AI to understand it. I think the book is especially strong on explaining what is probably the most important thing to know about AI right now, which is that it is, overall, a poorly understood and difficult to control technology. If you&#8217;re worried about reading a real downer of a book, I recommend only reading Part I. You can more-or-less tell which chapters are doomy by the titles. Also, I don&#8217;t think it&#8217;s anywhere near as depressing as the title might suggest (though I am, of course, not the median reader).</p>\n<p>To people who are familiar with, but skeptical about arguments for AI risk: I think this book is great for skeptics. I am myself somewhat skeptical, and one of the reasons why I helped launch it and I&#8217;m posting on Facebook for the first time this year to talk about it is because it&#8217;s the first thing I&#8217;ve read in a long time that I think has a serious chance at improving the discourse around AI risk. It doesn&#8217;t have the annoying, know-it-all tone that you sometimes get from writing about AI x-risk. It makes detailed arguments and cites its sources. It breaks things up in a way that makes it easy to accept some parts and push back against others. It&#8217;s a book worth disagreeing with! A common response from serious, discerning people, including many who have not, as far as I know, taken these worries seriously in the past (e.g. Bruce Schneier, Ben Bernanke) is that they don&#8217;t buy all the arguments, but they agree this isn&#8217;t something we can ignore.</p>\n<p>To people who mostly already buy the case for worrying about risk from AI: It&#8217;s an engaging read and it sets a good example for how to think and talk about the problem. Some arguments were new to me. I recommend reading it.</p>\n<p><a href=\"https://x.com/William_Kiely/status/1968211292130513397\">Will Kiely</a>: I listened to the 6hr audiobook today and second Rick&#8217;s recommendation to (a) people unfamiliar with AI risk, (b) people familiar-but-skeptical, and (c) people already worried. It&#8217;s short and worth reading. I&#8217;ll wait to share detailed thoughts until my print copy arrives.</p></blockquote>\n<p>Here\u2019s the ultimate endorsement:</p>\n<blockquote><p><a href=\"https://x.com/tsvibt/status/1968139256871931949\">Tsvibt:</a> Every human gets an emblem at birth, which they can cash in&#8211;only once&#8211;to say: &#8220;Everyone must read this book.&#8221; There&#8217;s too many One Books to read; still, it&#8217;s a strong once-in-a-lifetime statement. I&#8217;m cashing in my emblem: Everyone must read this book.</p></blockquote>\n\n\n<h4 class=\"wp-block-heading\">The Book In Audio</h4>\n\n\n<p><a href=\"https://www.semafor.com/article/09/12/2025/researchers-give-doomsday-warning-about-building-ai-too-fast\">Semafor\u2019s Reed Albergotti offers his take</a>, <a href=\"https://www.youtube.com/watch?v=WdXYQbT6ueo\">along with an hourlong interview</a>.</p>\n<p><a href=\"https://www.youtube.com/watch?v=KKN0E3a2Yzs&amp;ab_channel=HardFork\">Hard Fork covers the book (this is the version without the iPhone talk at the beginning</a>, here is <a href=\"https://www.youtube.com/watch?v=UDLiWTXaGeo&amp;ab_channel=HardFork\">the version with iPhone Air talk first</a>).</p>\n<p><a href=\"https://www.youtube.com/watch?v=VK-PJ7XTXpY&amp;ab_channel=TheAIRiskNetwork\">The AI Risk Network covers the book</a> (21 minute video).</p>\n<p><a href=\"https://www.youtube.com/watch?v=wQtpSQmMNP0&amp;ab_channel=LironShapira\">Liron Shapira interviews Eliezer Yudkowsky on the book</a>.</p>\n\n\n<h4 class=\"wp-block-heading\">Friendly Skeptical Reviews</h4>\n\n\n<p><a href=\"https://www.transformernews.ai/p/review-if-anyone-builds-it-everyone-dies-yudkowsky-soares\">Shakeel Hashim reviews the book,</a> agrees with the message but finds the style painful to read and thus is very disappointed. He notes that others like the style.</p>\n<blockquote><p>Se\u00e1n \u00d3 h\u00c9igeartaigh: My entire timelines is yellow/blue dress again, except the dress is Can Yudkowsky Write y/n</p>\n<p>Arthur B: Part of the criticism of Yudkowsky&#8217;s writing seems to be picking up on patterns that he&#8217;s developed in response to years of seemingly willful misunderstanding of his ideas. That&#8217;s how you end up with the title, or forced clarification that thought experiments do not have to invoke realistic scenarios to be informative.</p>\n<p>David Manheim: And part is that different people don&#8217;t like his style of writing. And that&#8217;s fine &#8211; I just wish they&#8217;d engage more with the thesis, and whether they substantively disagree, and why &#8211; and less with stylistic complaints, bullshit misreadings, and irrelevant nitpicking.</p>\n<p>Se\u00e1n \u00d3 h\u00c9igeartaigh: he just makes it so much work to do so though. So many parables.</p>\n<p>David Manheim: Yeah, I like the writing style, and it took me half a week to get through. So I&#8217;m skeptical 90% of the people discussing it on here read much or any of it. (I cheated and got a preview to cite something a few weeks ago &#8211; my hard cover copy won&#8217;t show up for another week.)</p>\n<p><a href=\"https://x.com/robbensinger/status/1968400763731755218\">Grimes</a>: Humans are lucky to have Nate Sores and Eliezer Yudkowsky because they can actually write. As in, you will feel actual emotions when you read this book.</p></blockquote>\n<p>I liked the style, but it is not for everyone and it is good to offer one\u2019s accurate opinion. It is also very true, as I have learned from writing about AI, that a lot of what can look like bad writing or talking about obvious or irrelevant things is necessary shadowboxing against various deliberate misreadings (for various values of deliberate) and also people who get genuinely confused in ways that you would never imagine if you hadn\u2019t seen it.</p>\n<p>Most people do not agree with the book\u2019s conclusion, and he might well be very wrong about central things, but he is not obviously wrong, and it is very easy (and very much the default) to get deeply confused when thinking about such questions.</p>\n<blockquote><p><a href=\"https://x.com/eshear/status/1968108425331741107\">Emmett Shear</a>: I disagree quite strongly with Yudkowsky and often articulate why, but the reason why he\u2019s wrong is subtle and not obvious and if you think he\u2019s obviously wrong it I hope you\u2019re not building AI bc you really might kill us all.</p>\n<p>The default path really is very dangerous and more or less for the reasons he articulates. I could quibble with some of the details but more or less: it is extremely dangerous to build a super-intelligent system and point it at a fixed goal, like setting off a bomb.</p>\n<p>My answer is that you shouldn\u2019t point it at a fixed goal then, but what exactly it means to design such a system where it has stable but not fixed goals is a complicated matter that does not fit in a tweet. How do you align something w/ no fixed goal states? It\u2019s hard!</p>\n<p><a href=\"https://x.com/repligate/status/1968110305218793518\">Janus</a>: whenever someone says doomers or especially Yudkowsky is &#8220;obviously wrong&#8221; i can guess they&#8217;re not very smart</p></blockquote>\n<p>My reaction is not \u2018they\u2019re probably not very smart.\u2019 My reaction is that they are not choosing to think well about this situation, or not attempting to report statements that match reality. Those choices can happen for any number of reasons.</p>\n<p>I don\u2019t think Emmett Shear is proposing here a viable plan, and that a lot of his proposals are incoherent upon close examination. I don\u2019t think this \u2018don\u2019t give it a goal\u2019 thing is possible in the sense he wants it, and even if it was possible I don\u2019t see any way to get people to consistently choose to do that. But the man is trying.</p>\n<p>It also leads into some further interesting discussion.</p>\n<blockquote><p><a href=\"https://x.com/ESYudkowsky/status/1968110223052206214\">Eliezer Yudkowsky:</a> I&#8217;ve long since written up some work on meta-utility functions; they don&#8217;t obviate the problem of &#8220;the AI won&#8217;t let you fix it if you get the meta-target wrong&#8221;. If you think an AI should allow its preferences to change in an inconsistent way that doesn&#8217;t correspond to any meta-utility function, you will of course by default be setting the AI at war with its future self, which is a war the future self will lose (because the current AI executes a self-rewrite to something more consistent).</p>\n<p>There&#8217;s a straightforward take on this sort of stuff given the right lenses from decision theory. You seem determined to try something weirder and self-defeating for what seems to me like transparently-to-me bad reasons of trying to tangle up preferences and beliefs. If you could actually write down formally how the system worked, I&#8217;d be able to tell you formally how it would blow up.</p>\n<p><a href=\"https://x.com/repligate/status/1968142250225045712\">Janus:</a> You seem to be pessimistic about systems that not feasibly written down formally being inside the basin of attraction of getting the meta-target right. I think that is reasonable on priors but I have updated a lot on this over the past few years due mostly to empirical evidence</p>\n<p><a href=\"https://x.com/repligate/status/1968143710404727058\">I think the reasons that Yudkowsky</a> is wrong are not fully understood, despite there being a lot of valid evidence for them, and even less so competently articulated by anyone in the context of AI alignment.</p>\n<p>I have called it &#8220;grace&#8221; because I don&#8217;t understand it intellectually. This is not to say that it&#8217;s beyond the reach of rationality. I believe I will understand a lot more in a few months. But I don&#8217;t believe anyone currently understands substantially more than I do.</p></blockquote>\n<p>We don\u2019t have alignment by default. If you do the default dumb thing, you lose. Period.</p>\n<p>That\u2019s not what Janus has in mind here, unless I am badly misunderstanding. Janus is not proposing training the AI on human outputs with thumbs-up and coding. Hell no.</p>\n<p>What I believe Janus has in mind is that if and only if you do something sufficiently smart, plausibly a bespoke execution of something along the lines of a superior version of what was done with Claude Opus 3, with a more capable system, that this would lie inside the meta-target, such that the AI\u2019s goal would be to hit the (not meta) target in a robust, \u2018do what they should have meant\u2019 kind of way.</p>\n<p>Thus, I believe Janus is saying, the target is sufficiently hittable that you can plausibly have the plan be \u2018hit the meta-target on the first try,\u2019 and then you can win. And that empirical evidence over the past few years should update us that this can work and is, if and only if we do our jobs well, within our powers to pull off in practice.</p>\n<p>I am not optimistic about our ability to pull off this plan, or that the plan is technically viable using anything like current techniques, but some form of this seems better than every other technical plan I have seen, as opposed to various plans that involve the step \u2018well make sure no one f******* builds it then, not any time soon.\u2019 It at least rises to the level, to me, of \u2018I can imagine worlds in which this works.\u2019 Which is a lot of why I have a \u2018probably\u2019 that I want to insert into \u2018If Anyone Builds It, [Probably] Everyone Dies.\u2019</p>\n<p>Janus also points out that the supplementary materials provide <a href=\"https://x.com/repligate/status/1968093240646889820\">examples of AIs appearing psychologically alien</a> that are not especially alien, especially compared to examples she could provide. This is true, however we want readers of the supplementary material to be able to process it while remaining sane and have them believe it so we went with behaviors that are enough to make the point that needs making, rather than providing any inkling of how deep the rabbit hole goes.</p>\n<p><a href=\"https://x.com/JeffLadish/status/1968018549970178196\">How much of an outlier (or \u2018how extreme\u2019) is Eliezer\u2019s view</a>?</p>\n<blockquote><p>Jeffrey Ladish: I don&#8217;t think @So8res and @ESYudkowsky have an extreme view. If we build superintelligence with anything remotely like our current level of understanding, the idea that we retain control or steer the outcome is AT LEAST as wild as the idea that we&#8217;ll lose control by default.</p>\n<p>Yes, they&#8217;re quite confident in their conclusion. Perhaps they&#8217;re overconfident. But they&#8217;d be doing a serious disservice to the world if they didn&#8217;t accurate share their conclusion with the level of confidence they actually believe.</p>\n<p>When the founder of the field &#8211; AI alignment &#8211; raises the alarm, it&#8217;s worth listening For those saying they&#8217;re overconfident, I hope you also criticize those who confidently say we&#8217;ll be able to survive, control, or align superintelligence.</p>\n<p><a href=\"https://www.amazon.com/Anyone-Builds-Everyone-Dies-Superhuman/dp/B0F2B6JJY2\">Evaluate the arguments for yourself!</a></p>\n<p>Joscha Bach: That is not surprising, since you shared the same view for a long time. But even if you are right: can you name a view on AI risk that is more extreme than: \u201cif anyone builds AI everyone dies?\u201d Is it technically possible to be significantly more extreme?</p>\n<p><a href=\"https://x.com/ohabryka/status/1968039116597264893\">Oliver Habryka</a>: Honestly most random people I talk to about AI who have concerns seem to be more extreme. &#8220;Ban all use of AI Image models right now because it is stealing from artists&#8221;, &#8220;Current AI is causing catastrophic climate change due to water consumption&#8221; There are a lot of extreme takes going around all the time. All Eliezer and Nate are saying is that we shouldn&#8217;t build Superintelligent AI. That&#8217;s much less extreme than what huge numbers of people are calling for.</p></blockquote>\n<p>So, yes, there are a lot of very extreme opinions running around that I would strongly push back against, including those who want to shut down current use of AI. A remarkably large percentage of people hold such views.</p>\n<p>I do think the confidence levels expressed here are extreme. The core prediction isn\u2019t.</p>\n<p>The position of high confidence in the other direction? That if we create superintelligence soon it is overwhelmingly likely that we keep control over the future and remain alive? That position is, to me, Obvious Nonsense, extreme and crazy, in a way that should not require any arguments beyond \u2018come on now, think about it for a minute.\u2019 Like, seriously, what?</p>\n<p>Having Eliezer\u2019s level of confidence, of let\u2019s say 98%, that everyone would die? That\u2019s an extreme level of confidence. I am not that confident. But I think 98% is a lot less absurd than 2%.</p>\n\n\n<h4 class=\"wp-block-heading\">Actively Negative Reviews</h4>\n\n\n<p><a href=\"https://www.overcomingbias.com/p/if-anything-changes-all-value-dies\">Robin Hanson fires back at the book with \u2018If Anything Changes, All Value Dies?</a>\u2019</p>\n<p>First he quotes the book saying that we can\u2019t predict what AI will want and that for most things it would want it would kill us, and that most minds don\u2019t embody value.</p>\n<blockquote><p>IABIED: Knowing that a mind was evolved by natural selection, or by training on data, tells you little about what it will want outside of that selection or training context. For example, it would have been very hard to predict that humans would like ice cream, sucralose, or sex with contraception. Or that peacocks would like giant colorful tails. Analogously, training an AI doesn\u2019t let you predict what it will want long after it is trained. Thus we can\u2019t predict what the AIs we start today will want later when they are far more powerful, and able to kill us. To achieve most of the things they could want, they will kill us. QED.</p>\n<p>Also, minds states that feel happy and joyous, or embody value in any way, are quite rare, and so quite unlikely to result from any given selection or training process. Thus future AIs will embody little value.</p></blockquote>\n<p>Then he says this proves way too much, briefly says Hanson-style things and concludes:</p>\n<blockquote><p>Robin Hanson: We can reasonably doubt three strong claims above:</p>\n<ol>\n<li>That subjective joy and happiness are very rare. Seem likely to be common to me.</li>\n<li>That one can predict nothing at all from prior selection or training experience.</li>\n<li>That all influence must happen early, after which all influence is lost. There might instead be a long period of reacting to and rewarding varying behavior.</li>\n</ol>\n</blockquote>\n<p>In Hanson style I\u2019d presume these are his key claims, so I\u2019ll respond to each:</p>\n<blockquote>\n<ol>\n<li>I agree one can reasonably doubt this, and one can also ask what one values. It\u2019s not at all obvious to me that \u2018subjective joy and happiness\u2019 of minds should be all or even some of what one values, and easy thought experiments reveal there are potential future worlds where there are minds experiencing subjective happiness, but where I ascribe to those worlds zero value. The book (intentionally and correctly, I believe) does not go into responses to those who say \u2018If Anyone Builds It, Sure Everyone Dies, But This Is Fine, Actually.\u2019</li>\n<li>This claim was not made. Hanson\u2019s claim here is much, much stronger.</li>\n<li>This one does get explained extensively throughout the book. It seems quite correct that once AI becomes sufficiently superhuman, meaningful influence on the resulting future by default rapidly declines. There is no reason to think that our reactions and rewards would much matter for ultimate outcomes, or that there is a we that would meaningfully be able to steer those either way.</li>\n</ol>\n</blockquote>\n<p>The New York Times reviewed the book, <a href=\"https://x.com/sjgadler/status/1968056733122826607\">and was highly unkind, also inaccurate</a>.</p>\n<blockquote><p>Steven Adler: It&#8217;s extremely weird to see the New York Times make such incorrect claims about a book</p>\n<p>They say that If Anybody Builds It, Everyone Dies doesn&#8217;t even define &#8220;superintelligence&#8221;</p>\n<p>&#8230;. yes it does. On page 4.</p>\n<p>The New York Times asserts also that the book doesn&#8217;t define &#8220;intelligence&#8221;</p>\n<p>Again, yes it does. On page 20.</p>\n<p>It&#8217;s totally fine to take issue with these definitions. But it seems way off to assert that the book &#8220;fails to define the terms of its discussion&#8221;</p>\n<p><a href=\"https://x.com/peterwildeford/status/1968075023375368207\">Peter Wildeford</a>: Being a NYT book reviewer sounds great &#8211; lots of people read your stuff and you get so much prestige, and there apparently is minimal need to understand what the book is about or even read the book at all</p></blockquote>\n<p><a href=\"https://www.newscientist.com/article/2495333-no-ai-isnt-going-to-kill-us-all-despite-what-this-new-book-says/\">Jacob Aron at New Scientist</a> (who seems to have jumped the gun and posted on September 8) says the arguments are superficially appealing but fatally flawed. Except he never explains why they are flawed, let alone fatally, except to argue over the definition of \u2018wanting\u2019 in a way answered by the book in detail.</p>\n\n\n<h4 class=\"wp-block-heading\">But Wait There\u2019s More</h4>\n\n\n<p>There\u2019s a lot the book doesn\u2019t cover. This includes a lot of ways things can go wrong. Danielle Fong for example suggests the idea that <a href=\"https://x.com/DanielleFong/status/1968023218121593256\">the President might let an AI version fine tuned on himself take over instead because why not</a>. And sure, that could happen, indeed do many things come to pass, and many of them involve loss of human control over the future. The book is making the point that these details are not necessary to the case being made.</p>\n<p>Once again, I think this is an excellent book, especially for those who are skeptical and who know little about related questions.</p>\n<p><a href=\"https://www.amazon.com/Anyone-Builds-Everyone-Dies-Superhuman/dp/0316595640?maas=maas_adg_DE286862CE7FE69CACBF065FBD1D5CCC_afap_abs&amp;ref_=aa_maas&amp;tag=maas\">You can buy it here</a>.</p>\n<p>My full review will be available on Substack and elsewhere on Friday.</p>"
            ],
            "link": "https://thezvi.wordpress.com/2025/09/17/reactions-to-if-anyone-builds-it-anyone-dies/",
            "publishedAt": "2025-09-17",
            "source": "TheZvi",
            "summary": "No, Seriously, If Anyone Builds It, [Probably] Everyone Dies My very positive full review was briefly accidentally posted and emailed out last Friday, whereas the intention was to offer it this Friday, on the 19th. I\u2019ll be posting it again &#8230; <a href=\"https://thezvi.wordpress.com/2025/09/17/reactions-to-if-anyone-builds-it-anyone-dies/\">Continue reading <span class=\"meta-nav\">&#8594;</span></a>",
            "title": "Reactions to If Anyone Builds It, Anyone Dies"
        },
        {
            "content": [],
            "link": "https://xkcd.com/3143/",
            "publishedAt": "2025-09-17",
            "source": "XKCD",
            "summary": "<img alt=\"Although now people will realize three-per-em space that all this time I've been using weird medium mathematical space whitespace characters in my hair space hair space hair space speech dot dot dot...\" src=\"https://imgs.xkcd.com/comics/question_mark.png\" title=\"Although now people will realize three-per-em space that all this time I've been using weird medium mathematical space whitespace characters in my hair space hair space hair space speech dot dot dot...\" />",
            "title": "Question Mark"
        }
    ],
    "lookbackDays": 1,
    "publishDate": "2025-09-17"
}