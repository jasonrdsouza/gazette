{
    "articles": [
        {
            "content": [],
            "link": "https://www.nytimes.com/2025/10/17/style/modern-love-my-husband-the-reluctant-barista.html",
            "publishedAt": "2025-10-17",
            "source": "Modern Love - NYT",
            "summary": "When the person handing you your coffee at Starbucks is your husband of 17 years.",
            "title": "My Husband, the Reluctant Barista"
        },
        {
            "content": [],
            "link": "https://www.robinsloan.com/lab/luxury-tech/",
            "publishedAt": "2025-10-17",
            "source": "Robin Sloan",
            "summary": "<p>Worth appreciating. <a href=\"https://www.robinsloan.com/lab/luxury-tech/\">Read here.</a></p>",
            "title": "Luxury tech"
        },
        {
            "content": [],
            "link": "https://www.robinsloan.com/lab/cross-post/",
            "publishedAt": "2025-10-17",
            "source": "Robin Sloan",
            "summary": "<p>Hypertext! <a href=\"https://www.robinsloan.com/lab/cross-post/\">Read here.</a></p>",
            "title": "Cross post"
        },
        {
            "content": [],
            "link": "https://www.robinsloan.com/lab/once-and-future-perceptron/",
            "publishedAt": "2025-10-17",
            "source": "Robin Sloan",
            "summary": "<p>Real utility. <a href=\"https://www.robinsloan.com/lab/once-and-future-perceptron/\">Read here.</a></p>",
            "title": "The once and future perceptron"
        },
        {
            "content": [],
            "link": "https://www.ssp.sh/blog/agentic-data-modeling/",
            "publishedAt": "2025-10-17",
            "source": "Simon Spati",
            "summary": "<p>In data analytics, we&rsquo;re facing a paradox. AI agents can theoretically analyze anything, but without the right foundations, they&rsquo;re as likely to hallucinate a metric as to calculate it correctly. They can write SQL in seconds, but will it answer the right business question? They promise autonomous insights, but at what cost to trust and accuracy?</p> <p>These days, everyone is embedding AI chat in their product. But to what end? Does it actually help, or would users rather turn to tools like Claude Code when they need real work done? The real questions are: <strong>how can we model our data for agents to reliably consume, and how can we use agents to develop better data models?</strong></p>",
            "title": "Data Modeling for the Agentic Era: Semantics, Speed, and Stewardship"
        },
        {
            "content": [
                "<p>Thanks to everyone who entered or voted in the Non-Book Review Contest. The winners are:</p><ul><li><p><strong>1st: <a href=\"https://www.astralcodexten.com/p/your-review-joan-of-arc\">Joan of Arc</a></strong><em>, </em>by <strong>William Friedman</strong>. William is a history enthusiast and author who lives in California, where he spends his time reading, writing, GMing, playing video games and telling people excitedly about all the horrific stuff he learned in his latest history book. His fiction blog is <em><a href=\"https://palacefiction.substack.com/\">Palace Fiction</a></em> (which is currently serializing his first novel, The Tragedy of the Titanium Tyrant) and his nonfiction blog is <em><a href=\"https://asourdays.substack.com/\">As Our Days</a></em>.</p></li><li><p><strong>2nd: <a href=\"https://www.astralcodexten.com/p/your-review-alpha-school\">Alpha School</a></strong>, by <strong>Edward Nevraumont</strong>. Edward also wrote one of last year&#8217;s finalists (<a href=\"https://www.astralcodexten.com/p/your-book-review-silver-age-marvel\">Silver Age Marvel Comics</a>)<a class=\"footnote-anchor\" href=\"https://www.astralcodexten.com/feed#footnote-1\" id=\"footnote-anchor-1\" target=\"_self\">1</a>. Now that he&#8217;s no longer anonymous, he&#8217;s going to write a post on his blog responding to the review comments (712 of them!), as well as a follow-up post on what he has learned about Alpha in the six months since he submitted his review (including the Spring and Fall MAP results for his kids). Here is the <a href=\"https://everestera.substack.com/welcome-acx-readers\">landing page</a> with more details for ACX readers who are interested.</p></li><li><p><strong>3rd: <a href=\"https://www.astralcodexten.com/p/your-review-the-russo-ukrainian-war\">The Russo-Ukrainian War</a></strong>, by <strong>Gallow</strong>. Gallow is a wayward military consultant based in Ukraine. A long time reader of Slate Star Codex, he enjoys chess and combat sports. Forthcoming details of his experiences, along with miscellaneous thoughts and ideas can be found at his nascent Substack : <a href=\"https://substack.com/@gallowglassglen\">https://substack.com/@gallowglassglen</a></p></li></ul><p>The other Finalists were:</p><ul><li><p><strong><a href=\"https://www.astralcodexten.com/p/your-review-the-astral-codex-ten\">The ACX Commentariat</a></strong>, by <strong>Alex Bates</strong>. Alex is a statistician and health economist based near Oxford in the UK. In his review, Alex predicted that engagement with ACX would peak in July this year. Sadly this did not come to pass, in part because the Commentariat review itself dragged the average down. In his spare time, Alex is writing a novel in the hitherto-untapped genre of &#8216;Stat-Fic&#8217;, a thrilling blend of statistics and fantasy which is sure to find a vast mainstream audience upon publication.</p></li><li><p><strong><a href=\"https://www.astralcodexten.com/p/your-review-dating-men-in-the-bay\">Dating Men In The Bay Area</a>, </strong>by <strong>Alex King</strong>. Alex is an engineer from San Francisco. She&#8217;ll be experimenting with more essays on her new blog, <em><a href=\"https://kingofdaydreams.substack.com/\">King of Daydreams</a></em>. When she&#8217;s not igniting turmoil in the ACX comments section, she can be found mentoring young engineers, hosting community events, and failing to find a boyfriend. She pinky-promises she is not Aella.</p></li><li><p><strong><a href=\"https://www.astralcodexten.com/p/your-review-islamic-geometric-patterns\">Islamic Geometric Patterns In The Metropolitan Museum Of Art</a></strong>, by <strong>Canarius Agrippa</strong>. He is a physicist living in Boston, now on his third attempt at starting a blog, at <em><a href=\"https://canisagrippae.substack.com/\">Canis Agrippae</a></em>.</p></li><li><p><strong><a href=\"https://www.astralcodexten.com/p/your-review-of-mice-mechanisms-and\">Mice, Mechanisms, And Dementia</a></strong>,<strong> </strong>by <strong>Myka Estes</strong>. Myka is a neuroscientist and immunologist who has published in Science, Nature Reviews Neuroscience, and Immunity. She currently manages a research lab focused on children with profound neurodevelopmental disorders and publishes the <em><a href=\"https://journalclubwithmyka.substack.com/\">Journal Club with Myka</a></em> Substack. She&#8217;s also in the process of launching an independent bookstore, and in her spare time - she has no spare time.</p></li><li><p><strong><a href=\"https://www.astralcodexten.com/p/your-review-my-fathers-instant-mashed\">My Father&#8217;s Instant Mashed Potatoes</a></strong>, by <strong>Chris Finkle</strong>. Chris manages a makerspace in central Florida, and despite writing a review about the perils of simulacra he spends much of his free time at various theme parks, haunts, and roadside attractions. His most active social media presence is <a href=\"https://letterboxd.com/tereglith/\">letterboxd</a>, where he watches at least one movie from each of the last hundred years every year. This was his first time entering an ACX contest, and his other short form writing (mostly <a href=\"https://tereglith.substack.com/p/ambrosia\">science fiction</a> and <a href=\"https://tereglith.substack.com/p/michael-myers-is-cars\">reflections on pop culture</a>) can be found at <em><a href=\"http://tereglith.substack.com\">The Viewer From Nowhere</a></em>.</p></li><li><p><strong><a href=\"https://www.astralcodexten.com/p/your-review-ollantay\">Ollantay</a>,</strong> by <strong>David Speiser</strong>. David lives in New Mexico, and he writes about other stories that are 100% true at <em><a href=\"https://rainbowseverywhere.substack.com\">Rainbows Everywhere</a></em>.</p></li><li><p><strong><a href=\"https://www.astralcodexten.com/p/your-review-participation-in-phase\">Participation In Phase I Clinical Trials</a></strong>, by an author who prefers to remain anonymous.</p></li><li><p><strong><a href=\"https://www.astralcodexten.com/p/your-review-project-xanadu-the-internet\">Project Xanadu</a></strong>, by <strong>Ari Shtein</strong>. Ari is a freshman at Yale. He has very little idea what to do with his life, but for now is writing on Substack at <em><a href=\"https://mistakesweremade.substack.com/\">Mistakes Were Made</a></em>. If you&#8217;ve got advice or a job to offer, he can be reached by email at ari@shtein.net.</p></li><li><p><strong><a href=\"https://www.astralcodexten.com/p/your-review-school\">School</a></strong>, by <strong>Dylan Kane</strong>. Dylan is a 7th grade math teacher in Leadville, Colorado. He writes a Substack about teaching called <a href=\"https://fivetwelvethirteen.substack.com/\">Five Twelve Thirteen</a>.</p></li><li><p><strong><a href=\"https://www.astralcodexten.com/p/your-review-the-synaptic-plasticity\">The Synaptic Plasticity And Memory Hypothesis</a>, </strong>by <strong>John V</strong>. John is a neuroscientist and AI researcher in Boston; he also wrote last year&#8217;s finalist <a href=\"https://www.astralcodexten.com/p/your-book-review-how-language-began\">How Language Began</a>. He just started blogging at <em><a href=\"https://theoriesofintelligence.substack.com/\">Theories of Intelligence</a></em>. If you loved or hated his review, check his Substack soon for a detailed response to some of your comments and criticisms.</p></li></ul><p>Honorable Mentions were:</p><ul><li><p><strong><a href=\"https://docs.google.com/document/d/1d0vRSj1E93joWWvbUen2XGuDjN_mM94ybMIAADzM2fo/edit?tab=t.0#heading=h.a1yucj1u3lx5\">Bishop&#8217;s Castle</a>. </strong>I haven&#8217;t heard back from this person about a bio, and will profile them once I get a response.</p></li><li><p><strong><a href=\"https://docs.google.com/document/d/1d0vRSj1E93joWWvbUen2XGuDjN_mM94ybMIAADzM2fo/edit?tab=t.0#heading=h.oe0t8x5w3k7r\">Bukele</a></strong>, by a writer who prefers to remain anonymous.</p></li><li><p><strong><a href=\"https://docs.google.com/document/d/1d0vRSj1E93joWWvbUen2XGuDjN_mM94ybMIAADzM2fo/edit?tab=t.0#heading=h.dn2h2z8e9c52\">Elon Musk&#8217;s Engineering Algorithm</a></strong><em>, </em>reviewed by a former SpaceX employee and practicing aerospace engineer who prefers to remain anonymous. He is an avid ACX reader and a published writer.</p></li><li><p><strong><a href=\"https://docs.google.com/document/d/1a3q0Z2tuPLLbDeg5-pfEffkajGjrfPDwE7ZMs7uaWQs/edit?tab=t.0#heading=h.8aqayzf6n8vk\">JFK Assassination Conspiracy Theories</a></strong>, reviewed by <strong>Max Nussenbaum</strong>. Max was a finalist in previous contests with his reviews of <a href=\"https://www.astralcodexten.com/p/your-book-review-the-outlier\">The Outlier</a> and <a href=\"https://www.astralcodexten.com/p/your-book-review-public-citizens\">Public Citizens</a>. He writes at <a href=\"https://www.candyforbreakfast.email/\">Candy for Breakfast</a> and begrudgingly acknowledges that Lee Harvey Oswald probably acted alone.</p></li><li><p><strong><a href=\"https://docs.google.com/document/d/1a3q0Z2tuPLLbDeg5-pfEffkajGjrfPDwE7ZMs7uaWQs/edit?tab=t.0#heading=h.1t03npll2afd\">Martial Arts</a></strong>, reviewed by <strong>Oliver Kump</strong>. Oliver was a professional Muay Thai fighter for a time. He decided at 45 that he should try writing, and was incredibly flattered to be mentioned at all. He likes Trevanian and Jack Vance.</p></li><li><p><strong><a href=\"https://docs.google.com/document/d/1a3q0Z2tuPLLbDeg5-pfEffkajGjrfPDwE7ZMs7uaWQs/edit?tab=t.0#heading=h.ya1rt12znfg4\">Miniatur Wunderland</a></strong>, reviewed by <strong>Laura Gonz&#225;lez Salmer&#243;n</strong>. Laura works on the advising team at 80,000 Hours. The world is racing towards transformative AI without much of a plan: <a href=\"https://80000hours.org/speak-with-us/?int_campaign=footer\">apply </a>to speak with the team if you want to use your career to do something about it. Outside work, she&#8217;s chipping away at a PhD on representations of science in fiction. She&#8217;s using this contest as an excuse to launch a Substack she&#8217;s been meaning to start for years, <a href=\"https://theturingtext.substack.com/about\">The Turing Text</a> (we&#8217;ll see how long it lasts). Blogging about literature, linguistics, and AI seems like productive thesis procrastination.</p></li><li><p><strong><a href=\"https://docs.google.com/document/d/1jYVJFIz5-aMi0LCgsC9AN6BncJDNVGaMU37QmwZ1vzA/edit?tab=t.0#heading=h.rofd0486d3ov\">The Watergate Affair</a></strong>, reviewed by <strong>Jake Scheiber</strong>. Jake is a retired engineer who now spends all his time Worrying. He blogs at the mostly inaccurately named <a href=\"http://www.souprecipies.com\">www.souprecipies.com</a>.</p></li></ul><p>All of these Honorable mentions qualified by getting a high average rating. But on revisiting the data, I noticed that one essay was an outlier not in its average rating, but in its <em>number</em> of ratings - very many people chose that one in particular to read and vote upon. This is its own sort of victory, so I am adding to the Honorable Mention roster:</p><ul><li><p><strong><a href=\"https://docs.google.com/document/d/1a3q0Z2tuPLLbDeg5-pfEffkajGjrfPDwE7ZMs7uaWQs/edit?tab=t.0#heading=h.huw3qjl45llt\">&#8216;Red Means No&#8217; Orgies</a></strong>,<strong> </strong>reviewed by <strong>Eneasz Brodski</strong>. Eneasz is best known for creating the <a href=\"https://justhpmor.substack.com/\">full-cast HPMOR audiobook/podcast</a>, and he now podcasts at <em><a href=\"https://www.thebayesianconspiracy.com/\">The Bayesian Conspiracy</a></em> covering rationalist general-interest topics. He has also published the novel <em><a href=\"https://amzn.to/2EJhOKZ\">What Lies Dreaming</a></em>, a Lovecraftian horror set in 2nd century Rome. He blogs at <em><a href=\"https://deathisbad.substack.com/\">Death Is Bad</a></em> and will be participating in the <a href=\"https://www.inkhaven.blog/\">Inkhaven</a> residency this November.</p></li></ul><p>All honorable mentions get free ACX subscriptions. All finalists get that plus links to their Substack and the right to try to pitch me articles (I usually say no, but <a href=\"https://www.astralcodexten.com/p/does-georgism-work-is-land-really\">Lars Doucet</a>, <a href=\"https://www.astralcodexten.com/p/consciousness-as-recursive-reflections\">Daniel B&#246;ttger</a>, and <a href=\"https://www.astralcodexten.com/p/bayes-for-everyone\">Brandon Hendrickson</a> managed to get through). First / second / third place get $2500, $1000, and $500 respectively<a class=\"footnote-anchor\" href=\"https://www.astralcodexten.com/feed#footnote-2\" id=\"footnote-anchor-2\" target=\"_self\">2</a>. Give me two weeks to distribute prizes, and if you haven&#8217;t gotten your prize or at least an email about it by then, message me at scott@slatestarcodex.com<a class=\"footnote-anchor\" href=\"https://www.astralcodexten.com/feed#footnote-3\" id=\"footnote-anchor-3\" target=\"_self\">3</a>.</p><p>Congratulations once again to all winners, and thanks again to everyone who participated. We&#8217;ll go back to a Book Review contest next year, and alternate yearly between Book Reviews and Non-Book Reviews thereafter. </p><p></p><div class=\"footnote\"><a class=\"footnote-number\" contenteditable=\"false\" href=\"https://www.astralcodexten.com/feed#footnote-anchor-1\" id=\"footnote-1\" target=\"_self\">1</a><div class=\"footnote-content\"><p>Edward writes &#8220;I feel like I learned a <em>ton</em> about writing a good review from the feedback I got last year. Particularly <a href=\"https://www.astralcodexten.com/p/your-book-review-silver-age-marvel/comment/65693964\">the comment from Gwern you highlighted in the open thread afterwards</a>. I don&#8217;t think I could have written this review the way it ended up without the harsh feedback I got last year.&#8221; I hadn&#8217;t realized you could actually learn things from people&#8217;s mean online comments, so I&#8217;ll have to go back and read all of yours on all my posts and see if there&#8217;s anything useful there.</p></div></div><div class=\"footnote\"><a class=\"footnote-number\" contenteditable=\"false\" href=\"https://www.astralcodexten.com/feed#footnote-anchor-2\" id=\"footnote-2\" target=\"_self\">2</a><div class=\"footnote-content\"><p>I also ended up dating one of last year&#8217;s winners, but no guarantee this will happen consistently.</p></div></div><div class=\"footnote\"><a class=\"footnote-number\" contenteditable=\"false\" href=\"https://www.astralcodexten.com/feed#footnote-anchor-3\" id=\"footnote-3\" target=\"_self\">3</a><div class=\"footnote-content\"><p>Please include the phrase &#8220;this is a genuine non-spam message&#8221; to guarantee you get past my spam filter.</p><p></p></div></div>"
            ],
            "link": "https://www.astralcodexten.com/p/non-book-review-contest-2025-winners",
            "publishedAt": "2025-10-17",
            "source": "SlateStarCodex",
            "summary": "<p>Thanks to everyone who entered or voted in the Non-Book Review Contest. The winners are:</p><ul><li><p><strong>1st: <a href=\"https://www.astralcodexten.com/p/your-review-joan-of-arc\">Joan of Arc</a></strong><em>, </em>by <strong>William Friedman</strong>. William is a history enthusiast and author who lives in California, where he spends his time reading, writing, GMing, playing video games and telling people excitedly about all the horrific stuff he learned in his latest history book. His fiction blog is <em><a href=\"https://palacefiction.substack.com/\">Palace Fiction</a></em> (which is currently serializing his first novel, The Tragedy of the Titanium Tyrant) and his nonfiction blog is <em><a href=\"https://asourdays.substack.com/\">As Our Days</a></em>.</p></li><li><p><strong>2nd: <a href=\"https://www.astralcodexten.com/p/your-review-alpha-school\">Alpha School</a></strong>, by <strong>Edward Nevraumont</strong>. Edward also wrote one of last year&#8217;s finalists (<a href=\"https://www.astralcodexten.com/p/your-book-review-silver-age-marvel\">Silver Age Marvel Comics</a>)<a class=\"footnote-anchor\" href=\"https://www.astralcodexten.com/feed#footnote-1\" id=\"footnote-anchor-1\" target=\"_self\">1</a>. Now that he&#8217;s no longer anonymous, he&#8217;s going to write a post on his blog responding to the review comments (712 of them!), as well as a follow-up post on what he has learned about Alpha in the six months since he submitted his review (including the Spring and Fall MAP results for his kids). Here is the <a href=\"https://everestera.substack.com/welcome-acx-readers\">landing page</a> with more details for ACX readers who are interested.</p></li><li><p><strong>3rd: <a href=\"https://www.astralcodexten.com/p/your-review-the-russo-ukrainian-war\">The Russo-Ukrainian War</a></strong>, by <strong>Gallow</strong>. Gallow is a wayward military consultant based in Ukraine. A long time",
            "title": "Non-Book Review Contest 2025 Winners"
        },
        {
            "content": [
                "<p>As usual when things split, Part 1 is mostly about capabilities, and Part 2 is mostly about a mix of policy and alignment.</p>\n\n\n<h4 class=\"wp-block-heading\">Table of Contents</h4>\n\n\n<ol>\n<li><a href=\"https://thezvi.substack.com/i/176333461/the-quest-for-sane-regulations\"><strong>The Quest for Sane Regulations</strong>.</a> The GAIN Act and some state bills.</li>\n<li><a href=\"https://thezvi.substack.com/i/176333461/people-really-dislike-ai\">People Really Dislike AI.</a> They would support radical, ill-advised steps.</li>\n<li><a href=\"https://thezvi.substack.com/i/176333461/chip-city\">Chip City.</a> Are we taking care of business?</li>\n<li><a href=\"https://thezvi.substack.com/i/176333461/the-week-in-audio\"><strong>The Week in Audio</strong>.</a> Hinton talks to Jon Stewart, Klein to Yudkowsky.</li>\n<li><a href=\"https://thezvi.substack.com/i/176333461/rhetorical-innovation\">Rhetorical Innovation.</a> How to lose the moral high ground.</li>\n<li><a href=\"https://thezvi.substack.com/i/176333461/water-water-everywhere\">Water Water Everywhere.</a> AI has many big issues. Water isn\u2019t one of them.</li>\n<li><a href=\"https://thezvi.substack.com/i/176333461/read-jack-clark-s-speech-from-the-curve\"><strong>Read Jack Clark\u2019s Speech From The Curve</strong>.</a> It was a sincere, excellent speech.\n<div>\n\n\n<span id=\"more-24799\"></span>\n\n\n</div>\n</li>\n<li><a href=\"https://thezvi.substack.com/i/176333461/how-one-other-person-responded-to-this-thoughtful-essay\">How One Other Person Responded To This Thoughtful Essay.</a> Some aim to divide.</li>\n<li><a href=\"https://thezvi.substack.com/i/176333461/a-better-way-to-disagree\">A Better Way To Disagree.</a> Others aim to work together and make things better.</li>\n<li><a href=\"https://thezvi.substack.com/i/176333461/voice-versus-exit\">Voice Versus Exit.</a> The age old question, should you quit your job at an AI lab?</li>\n<li><a href=\"https://thezvi.substack.com/i/176333461/the-dose-makes-the-poison\"><strong>The Dose Makes The Poison</strong>.</a> As little as 250 documents can poison an LLM.</li>\n<li><a href=\"https://thezvi.substack.com/i/176333461/aligning-a-smarter-than-human-intelligence-is-difficult\">Aligning a Smarter Than Human Intelligence is Difficult.</a> Techniques to avoid.</li>\n<li><a href=\"https://thezvi.substack.com/i/176333461/you-get-what-you-actually-trained-for\">You Get What You Actually Trained For.</a> So ask what you actually train for.</li>\n<li><a href=\"https://thezvi.substack.com/i/176333461/messages-from-janusworld\">Messages From Janusworld.</a> Do not neglect theory of mind.</li>\n<li><a href=\"https://thezvi.substack.com/i/176333461/people-are-worried-about-ai-killing-everyone\">People Are Worried About AI Killing Everyone.</a> A world-ending AI prompt?</li>\n<li><a href=\"https://thezvi.substack.com/i/176333461/the-lighter-side\">The Lighter Side.</a> Introducing the museum of chart crimes.</li>\n</ol>\n\n\n<h4 class=\"wp-block-heading\">The Quest for Sane Regulations</h4>\n\n\n<p><a href=\"https://x.com/hamandcheese/status/1976500021991231748\">Don\u2019t let misaligned AI wipe out your GAIN AI Act</a>.</p>\n<p>It\u2019s pretty amazing that it has come to this and we need to force this into the books.</p>\n<p>The least you can do, before selling advanced AI chips to our main political adversary, is offer those same chips for sale to American firms on the same terms first. I predict there are at least three labs (OpenAI, Anthropic and xAI) that would each happily and directly buy everything you\u2019re willing to sell at current market prices, and that\u2019s not even including Oracle, Meta and Microsoft.</p>\n<p>I\u2019m not including Google and Amazon there because they\u2019re trying to make their own chips, but make those calls too, cause more is more. I won\u2019t personally buy in too much bulk, but call me too, there\u2019s a good chance I\u2019ll order me at least one H20 or even better B30A, as a treat.</p>\n<blockquote><p>Samuel Hammond: Glad to see this made it in.</p>\n<p>So long as American companies are compute constrained, they should at the very least have a right of first refusal over chips going to our chief geopolitical adversary.</p>\n<p>ARI: <a href=\"https://t.co/fRK1f9tCq2\">The Senate just passed the GAIN AI Act</a> in the NDAA &#8211; a bill requiring chip makers to sell advanced AI chips to US firms before countries of concern. Big win for competitiveness &amp; security.</p></blockquote>\n<p>In all seriousness, I will rest a lot easier if we can get the GAIN AI Act passed, as it will severely limit the amount of suicide we can commit with chip sales.</p>\n<p><a href=\"https://x.com/mattyglesias/status/1977368989123612775\">Marjorie Taylor-Greene says Trump is focusing on</a> helping AI industry and crypto donors at the expense of his base and the needs of manufacturers.</p>\n<p>California Governor Newsom <a href=\"https://www.techpolicy.press/inside-the-lobbying-frenzy-over-californias-ai-companion-bills/\">vetoes the relatively strong AB 1064,</a> an AI child safety bill that a16z lobbyists and allied usual suspects lobbied hard against, <a href=\"https://www.latimes.com/business/story/2025-10-13/gov-newsom-signs-ai-safety-bill\">and signs another weaker child safety bill, SB 243</a>. SB 243 requires chatbot operators have procedures to prevent the production of suicide or self-harm content and put in guardrails like referrals to suicide and crisis hotlines, and tell minor users every three hours that the AI is not human and to take a break.</p>\n<p>There was a divide in industry over whether SB 243 was an acceptable alternative to AB 1064 or still something to fight, and a similar divide by child safety advocates over whether SB 243 was too timid to be worth supporting. <a href=\"https://thezvi.substack.com/i/159992000/the-quest-for-sane-regulations\">I previously covered these bills briefly back in AI #110</a>, when I said AB 1064 seemed like a bad idea and SB 243 seemed plausibly good but non-urgent.</p>\n<p>For AB 1064, Newsom\u2019s veto statement says he was worried it could result in unintentionally banning AI tool use by minors, echoing arguments by opposing lobbyists that it would ban educational tools.</p>\n<blockquote><p>Cristiano Lima-Strong: Over the past three months, the group has spent over $50,000 on more than 90 digital ads targeting California politics, according to a review of Meta\u2019s political ads library.</p>\n<p>Over two dozen of the ads specifically targeted AB1064, which the group said would \u201churt classrooms\u201d and block \u201cthe tools students and teachers need.\u201d Several others more broadly warned against AI \u201cred tape,\u201d urging state lawmakers to \u201cstand with Little Tech\u201d and \u201cinnovators,\u201d while dozens more took aim at another one of <a href=\"https://calmatters.digitaldemocracy.org/bills/ca_202520260ab1018\">Bauer-Kahan\u2019s AI bills</a>.</p>\n<p>TechNet has spent roughly $10,000 on over a dozen digital ads in California expressly opposing AB1064, with messages warning that it would \u201cslam the brakes\u201d on innovation and that if passed, \u201cour teachers won\u2019t be equipped to prepare students for the future.\u201d</p>\n<p><a href=\"https://cal-access.sos.ca.gov/Lobbying/Employers/Detail.aspx?id=1440344&amp;session=2025&amp;view=activity\">The Chamber of Progress</a> and <a href=\"https://cal-access.sos.ca.gov/Lobbying/Employers/Detail.aspx?id=1300926&amp;session=2025&amp;view=activity\">TechNet</a> each registered nearly $200,000 in lobbying the California legislature the first half of this year, while <a href=\"https://cal-access.sos.ca.gov/Lobbying/Employers/Detail.aspx?id=1286548&amp;session=2025&amp;view=activity\">CCIA</a> spent $60,000 and the <a href=\"https://cal-access.sos.ca.gov/Lobbying/Employers/Detail.aspx?id=1480754&amp;session=2025&amp;view=activity\">American Innovators Network</a> doled out $40,000, according to a review of state disclosure filings. Each group was active on both SB243 and AB1064, among numerous other tech and AI bills.</p></blockquote>\n<p>One thing to note is that these numbers are so small. This is framed as a big push and a lot of money, but it is many orders of magnitude smaller than the size of the issues at stake, and also small in absolute terms.</p>\n<p>It\u2019s moot now, but I took a brief look at the final version of <a href=\"https://leginfo.legislature.ca.gov/faces/billNavClient.xhtml?bill_id=202520260AB1064\">AB 1064</a>, as it was a very concise bill, and I quickly reached four conclusions:</p>\n<ol>\n<li>As written the definition of \u2018companion chatbot\u2019 applies to ChatGPT, other standard LLMs and also plausibly to dedicated educational tools.</li>\n<li>You could write it slightly differently to not have that happen. For whatever reason, that\u2019s not how the bill ended up being worded.</li>\n<li>The standard the bill asks of its \u2018companion chatbots\u2019 might be outright impossible to meet, such as being \u2018not foreseeably capable\u2019 of sycophancy, aka \u2018prioritizing validation over accuracy.\u2019</li>\n<li>Thus, you can hate on the AI lobbyists all you want but here they seem right.</li>\n</ol>\n<p>Tyler Cowen expects most written words to come from AIs within a few years <a href=\"https://www.thefp.com/p/do-ai-models-have-first-amendment-rights\">and asks if AI models have or should have first amendment rights</a>. AIs are not legally persons, so they don\u2019t have rights. If I choose to say or reproduce words written by an AI then that clearly does come with such protections. The question is whether restrictions on AI speech violate the first amendment rights of users or developers. There I am inclined to say that they do, with the standard \u2018not a suicide pact\u2019 caveats.</p>\n\n\n<h4 class=\"wp-block-heading\">People Really Dislike AI</h4>\n\n\n<p>People do not like AI, and <a href=\"https://www.pewresearch.org/global/2025/10/15/how-people-around-the-world-view-ai/\">Americans especially don\u2019t like it</a>.</p>\n<div>\n<figure>\n<div>\n\n\n<figure class=\"wp-block-image\"><img alt=\"\" src=\"https://substackcdn.com/image/fetch/$s_!mMx9!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6b1c6841-ec16-494b-acf2-39a0d2bd401f_657x1027.png\" /></figure>\n\n\n<div></div>\n</div>\n</figure>\n</div>\n<p>Nor do they trust their government to regulate AI, except for the EU, which to be fair has one job.</p>\n<div>\n<figure>\n<div>\n\n\n<figure class=\"wp-block-image\"><img alt=\"\" src=\"https://substackcdn.com/image/fetch/$s_!oA0k!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7a38b369-9fe7-4f89-9285-b851566a42c4_687x643.png\" /></figure>\n\n\n<div></div>\n</div>\n</figure>\n</div>\n<p>Whenever we see public polls about what to do about all this, the public reliably not only wants to regulate AI, they want to regulate AI in ways that I believe would go too far.</p>\n<p>I don\u2019t mean would go a little too far. I mean a generalized \u2018you can sue if it gives advice that results in harmful outcomes,\u2019 think about what that would actually mean.</p>\n<p>If AI bots had to meet \u2018professional standards of care\u2019 when dealing with all issues, and were liable if their \u2018advice\u2019 led to harmful outcomes straight up without conditionals, then probably AI chatbots could not survive this even in a neutered form.</p>\n<blockquote><p><a href=\"https://x.com/JerusalemDemsas/status/1976284623522128067\">Jerusalem</a>: Americans want AI companies to be held liable for a wide variety of potential harms. And they\u2019re right!</p>\n<div>\n<figure>\n<div>\n\n\n<figure class=\"wp-block-image\"><img alt=\"\" src=\"https://substackcdn.com/image/fetch/$s_!4Oys!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F80d85d5f-ebe3-4ac1-877e-f53e8b2555d8_1104x848.jpeg\" /></figure>\n\n\n<div></div>\n</div>\n</figure>\n</div>\n<p>Rob Wiblin: IMO AI companies shouldn\u2019t generically be liable if their chatbots give me advice that cause a negative outcome for me. If we impose that standard we just won\u2019t get LLMs to use, which would suck. (Liability is more plausible if they\u2019re negligent in designing them.)</p></blockquote>\n<p>This is a rather overwhelming opinion among all groups, across partisan lines and gender and income and education and race, and AI companies should note that the least supportive group is the one marked \u2018I did not vote.\u2019</p>\n<div>\n<figure>\n<div>\n\n\n<figure class=\"wp-block-image\"><img alt=\"\" src=\"https://substackcdn.com/image/fetch/$s_!gkpO!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7eb8bbd2-acf1-4782-a89e-4623ba923798_1132x890.png\" /></figure>\n\n\n<div></div>\n</div>\n</figure>\n</div>\n<p>This is the background of current policy fights, and the setting for future fights. The public does not want a threshold of \u2018reasonable care.\u2019 They want things like \u2018meets professional standards\u2019 and \u2018is hurt by your advice, no matter how appropriate or wise it was or whether you took reasonable care.\u2019</p>\n<p>The graphs come from Kelsey Piper\u2019s post saying <a href=\"https://www.theargumentmag.com/p/we-need-to-be-able-to-sue-ai-companies\">we need to be able to sue AI companies</a>.</p>\n<p>As she points out, remember those huge fights over SB 1047 and in particular the idea that AI companies might be held liable if they did not take reasonable care and this failure resulted in damages of *checks notes* at least hundreds of millions of dollars. They raised holy hell, including patently absurd arguments like the one Kelsey quotes from Andrew Ng (who she notes then went on to make better arguments, as well).</p>\n<blockquote><p>Kelsey Piper: You can\u2019t <a href=\"https://openai.com/index/introducing-superalignment/\">claim to be designing a potentially godlike superintelligence</a> then fall back on the idea that, oh, <em>it\u2019s just like a laptop</em> when someone wants to take you to court.</p></blockquote>\n<p>I mean, sure you can, watch claim engine go brrrr. People be hypocrites.</p>\n<p>It\u2019s our job not to let them.</p>\n<blockquote><p>And if AI companies turn out to be liable when their models help users commit crimes or convince them to invest in scams, I suspect they will work quite hard to prevent their models from committing crimes or telling users to invest in scams.</p>\n<p>That is not to say that we should expand the current liability regime in every area where the voters demand it. If AI companies are liable for giving any medical advice, I\u2019m sure they will work hard to prevent their AIs from being willing to do that. But, in fact, there are plenty of cases where AIs being willing to say \u201c<em>go to the emergency room now</em>\u201d has saved lives.</p></blockquote>\n<p>Bingo.</p>\n<p>We absolutely do not want to give the public what it wants here. I am very happy that I was wrong about our tolerance for AIs giving medical and legal and other such advice without a license and while making occasional mistakes. We are much better off for it.</p>\n<p>In general, I am highly sympathetic to the companies on questions of, essentially, AIs sometimes making mistakes, offering poor advice, or failing to be sufficiently helpful or use the proper Officially Approved Words in your hour of need, or not tattling on the user to a Responsible Authority Figure.</p>\n<p>One could kind of call this grouping \u2018the AI tries to be a helpful friend and doesn\u2019t do a sufficiently superior job versus our standards for actual human friends.\u2019 A good rule of thumb would be, if a human friend said the same thing, would it be justice, and both legally and morally justified, to then sue the friend?</p>\n<p>However we absolutely need to have some standard of care that if they fail to meet it you can sue their asses, especially when harm is caused to third parties, and even more so when an AI actively causes or enables the causing of catastrophic harms.</p>\n<p>I\u2019d also want to be able to sue when there is a failure to take some form of \u2018reasonable care\u2019 in mundane contexts, similar to how you would already sue humans under existing law, likely in ways already enabled under existing law.</p>\n\n\n<h4 class=\"wp-block-heading\">Chip City</h4>\n\n\n<p>How\u2019s the beating China and powering our future thing going?</p>\n<blockquote><p><a href=\"https://x.com/heatmap_news/status/1976492773705425234\">Heatmap News</a>: This just in: The Esmeralda 7 Solar Project \u2014 which would have generated a gargantuan 6.2 gigawatts of power \u2014 has been canceled, the BLM says.</p>\n<p><a href=\"https://x.com/unusual_whales/status/1977679958312116335\">Unusual Whales</a>: U.S. manufacturing shrank this past September for the 7th consecutive month, per MorePerfectUnion</p></blockquote>\n<p>Yeah, so not great, then.</p>\n<p>Although there are bright spots, such as <a href=\"https://x.com/RepKeithAmmon/status/1977471562405331103\">New Hampshire letting private providers deliver power</a>.</p>\n<p>Sahil points out that <a href=\"https://x.com/sahilypatel/status/1977412190253482491\">the semiconductor supply chain has quite a few choke points or single points of failure</a>, not only ASML and TSMC and rare earths.</p>\n\n\n<h4 class=\"wp-block-heading\">The Week in Audio</h4>\n\n\n<p><a href=\"https://www.youtube.com/watch?v=jrK3PsD3APk\">Geoffrey Hinton podcast with Jon Stewart</a>. Self-recommending?</p>\n<p><a href=\"https://www.nytimes.com/2025/10/15/opinion/ezra-klein-podcast-eliezer-yudkowsky.html?smid=tw-share\">Ezra Klein talks to Eliezer Yudkowsky.</a></p>\n\n\n<h4 class=\"wp-block-heading\">Rhetorical Innovation</h4>\n\n\n<p>Not AI, but worth noticing that South Korea was foolish enough to keep backups so physically chose to originals that <a href=\"https://x.com/koryodynasty/status/1976295902508650563\">a fire wiped out staggering amounts of work</a>. If your plan or solution involves people not being this stupid, your plan won\u2019t work.</p>\n<p>Point of order: <a href=\"https://x.com/neil_chilson/status/1976409333450801658\">Neil Chilson challenges that I did not accurately paraphrase him back in AI #134</a>. GPT-5-Pro thought my statement did overreach a bit, so as per the thread I have edited the Substack post to what GPT-5-Thinking agreed was a fully precise paraphrasing.</p>\n<p><a href=\"https://x.com/tszzl/status/1977111404571508886\">There are ways in which this is importantly both right and wrong</a>:</p>\n<blockquote><p>Roon: i could run a pause ai movement so much better than the rationalists. they spend all their time infighting between factions like \u201cPause AI\u201d and \u201cAlignment Team at Anthropic\u201d. meanwhile I would be recruiting everyone on Instagram who thinks chatgpt is evaporating the rainforest.</p>\n<p>you fr could instantly have Tucker Carlson, Alex Jones on your side if you tried for ten seconds.</p>\n<p>Holly Elmore (Pause AI): Yes, I personally am too caught up my old world. I don\u2019t think most of PauseAI is that fixated on the hypocrisy of the lab safety teams.</p>\n<p>Roon: it\u2019s not you I\u2019m satirizing here what actually makes me laugh is the \u201cStop AI\u201d tribe who seems to fucking hate \u201cPause AI\u201d idk Malo was explaining all this to me at the curve</p>\n<p>Holly Elmore: I don\u2019t think StopAI hates us but we\u2019re not anti-transhumanist or against \u201cever creating ASI under any circumstances\u201dand they think we should be. Respectfully I don\u2019t Malo probably has a great grasp on this.</p></blockquote>\n<p>There are two distinct true things here.</p>\n<ol>\n<li>There\u2019s too much aiming at relatively friendly targets.</li>\n<li>If all you care about is going fully anti-AI and not the blast radius or whether your movement\u2019s claims or motives correspond to reality, your move would be to engage in bad faith politics and form an alliance with various others by using invalid arguments.</li>\n</ol>\n<p>The false thing is the idea that this is \u2018better,\u2019 the same way that many who vilify the idea of trying not to die from AI treat that idea as inherently the same as \u2018degrowth\u2019 or the people obsessed with water usage or conspiracies and so on, or say those worried about AI will inevitably join that faction out of political convenience. That has more total impact, but it\u2019s not better.</p>\n<p>This definitely doesn\u2019t fall into the lightbulb rule of \u2018if you believe [X] why don\u2019t you do [thing that makes no sense]?\u2019 since there is a clear reason you might do it, it does require an explanation (if you don\u2019t already know it), so here goes.</p>\n<p>The point is not to empower such folks and ideas and then take a back seat while the bulls wreck the China shop. The resulting actions would not go well. The idea is to convince people of true things based on true arguments, so we can then do reasonable and good things. Nor would throwing those principles away be good decision theory. We only were able to be as impactful as we were, in the ways we were, because we were clearly the types of people who would choose not to do this. So therefore we\u2019re not going to do this now, even if you can make an isolated consequentialist utilitarian argument that we should.</p>\n<p><a href=\"https://x.com/HumanHarlan/status/1977603037817663540\">A look back at when OpenAI co-founder Greg Brockman said</a> they must do four things to retain the moral high ground:</p>\n<ol>\n<li>Strive to remain a non-profit.</li>\n<li>Put increasing efforts into the safety/control problem.</li>\n<li>Engage with government to provide trusted, unbiased policy advice.</li>\n<li>Be perceived as a place that provides public good to the research community, and keeps the other actors honest and open via leading by example.</li>\n</ol>\n<p>By those markers, it\u2019s not going great on the moral high ground front. I\u2019m relatively forgiving on #4, however they\u2019re actively doing the opposite of #1 and #3, and putting steadily less relative focus and effort into #2, in ways that seem woefully inadequate to the tasks at hand.</p>\n<p><a href=\"https://www.lesswrong.com/posts/HbkNAyAoa4gCnuzwa/wei-dai-s-shortform?commentId=Y2EJMoQefcCtx8F7b\">Here\u2019s an interesting case of disagreement</a>, it has 107 karma and +73 agreement on LessWrong, I very much don\u2019t think this is what happened?</p>\n<blockquote><p>Wei Dai: A clear mistake of early AI safety people is not emphasizing enough (or ignoring) the possibility that solving AI alignment (as a set of technical/philosophical problems) may not be feasible in the relevant time-frame, without a long AI pause. Some have subsequently changed their minds about pausing AI, but by not reflecting on and publicly acknowledging their initial mistakes, I think they are or will be partly responsible for others repeating similar mistakes.</p>\n<p>Case in point is Will MacAskill\u2019s recent <a href=\"https://forum.effectivealtruism.org/posts/R8AAG4QBZi5puvogR/effective-altruism-in-the-age-of-agi\">Effective altruism in the age of AGI.</a> Here\u2019s my reply, copied from EA Forum:</p>\n<p>I think it\u2019s likely that without a long (e.g. multi-decade) AI pause, one or more of these \u201cnon-takeover AI risks\u201d can\u2019t be solved or reduced to an acceptable level. To be more specific:</p>\n<ol>\n<li>Solving AI welfare may depend on having a good understanding of consciousness, which is a notoriously hard philosophical problem.</li>\n<li>Concentration of power may be structurally favored by the nature of AGI or post-AGI economics, and defy any good solutions.</li>\n<li>Defending against AI-powered persuasion/manipulation may require solving metaphilosophy, which judging from other comparable fields, like meta-ethics and philosophy of math, may take at least multiple decades to do.</li>\n</ol>\n<p>I\u2019m worried that by creating (or redirecting) a movement to solve these problems, without noting at an early stage that these problems may not be solvable in a relevant time-frame (without a long AI pause), it will feed into a human tendency to be overconfident about one\u2019s own ideas and solutions, and create a group of people whose identities, livelihoods, and social status are tied up with having (what they think are) good solutions or approaches to these problems, ultimately making it harder in the future to build consensus about the desirability of pausing AI development.</p></blockquote>\n<p>I\u2019ll try to cover MacAskill later when I have the bandwidth, but the thing I don\u2019t agree with is the idea that a crucial flaw was failure to emphasize we might need a multi-decade AI pause. On the contrary, as I remember it, early AI safety advocates were highly willing to discuss extreme interventions and scenarios, to take ideas like this seriously, and to consider that they might be necessary.</p>\n<p>If anything, making what looked to outsiders like crazy asks like multi-decade or premature pauses was a key factor in the creation of negative polarization.</p>\n<p>Is it possible we will indeed need a long pause? Yes. If so, then either:</p>\n<ol>\n<li>We get much, much stronger evidence to generate buy-in for this, and we use that evidence, and we scramble and get it done, in time.</li>\n<li>Or someone builds it [superintelligence], and then everyone dies.</li>\n</ol>\n<p>Could we have navigated the last decade or two much better, and gotten into a better spot? Of course. But if I had to go back, I wouldn\u2019t try to emphasize more the potential need for a long pause. If indeed that is necessary, you convince people of true other things, and the pause perhaps flows naturally from them together with future evidence? You need to play to your outs.</p>\n\n\n<h4 class=\"wp-block-heading\">Water Water Everywhere</h4>\n\n\n<p>Andy Masley continues his quest to illustrate the ways in which <a href=\"https://andymasley.substack.com/p/the-ai-water-issue-is-fake?r=1ugy3&amp;utm_medium=ios&amp;triedRedirect=true\">the AI water issue is fake</a>, as in small enough to not be worth worrying about. AI, worldwide, has water usage equal to 0.008% of America\u2019s total freshwater. Numbers can sound large but people really do use a lot of water in general.</p>\n<p>The average American uses 422 gallons a day, or enough for 800,000 chatbot prompts. If you want to go after minds that use a lot of water, they\u2019re called humans.</p>\n<blockquote><p>Even manufacturing most regular objects requires lots of water. Here\u2019s a list of common objects you might own, and how many chatbot prompt\u2019s worth of water they used to make (<a href=\"https://watercalculator.org/footprint/the-hidden-water-in-everyday-products/\">all from this list</a>, and using the onsite + offsite water value):</p>\n<ul>\n<li><strong>Leather Shoes</strong> &#8211; 4,000,000 prompts\u2019 worth of water</li>\n<li><strong>Smartphone</strong> &#8211; 6,400,000 prompts</li>\n<li><strong>Jeans </strong>&#8211; 5,400,000 prompts</li>\n<li><strong>T-shirt </strong>&#8211; 1,300,000 prompts</li>\n<li><strong>A single piece of paper </strong>&#8211; 2550 prompts</li>\n<li><strong>A 400 page book </strong>&#8211; 1,000,000 prompts</li>\n</ul>\n<p>If you want to send 2500 ChatGPT prompts and feel bad about it, you can simply not buy a single additional piece of paper. If you want to save a lifetime supply\u2019s worth of chatbot prompts, just don\u2019t buy a single additional pair of jeans.</p></blockquote>\n<p>Here he compares it to various other industries, data centers are in red, specifically AI in data centers is the final line, the line directly above the black one is golf courses.</p>\n<div>\n<figure>\n<div>\n\n\n<figure class=\"wp-block-image\"><img alt=\"\" src=\"https://substackcdn.com/image/fetch/$s_!DL_O!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F19c987c9-1268-4e11-81b6-037f6a14bac2_2486x536.png\" /></figure>\n\n\n<div></div>\n</div>\n</figure>\n</div>\n<p>Or here it is versus agricultural products, the top line here is alfalfa.</p>\n<div>\n<figure>\n<div>\n\n\n<figure class=\"wp-block-image\"><img alt=\"\" src=\"https://substackcdn.com/image/fetch/$s_!wMxe!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbd0b99d5-94de-4b00-8d41-516505540085_1456x544.webp\" /></figure>\n\n\n<div></div>\n</div>\n</figure>\n</div>\n<p>One could say that AI is growing exponentially, but even by 2030 use will only triple. Yes, if we keep adding orders of magnitude we eventually have a problem, but encounter many other issues far sooner, such as dollar costs and also the singularity.</p>\n<p>He claims there are zero places water prices rose or an acute water shortage was created due to data center water usage. You could make a stronger water case against essentially any other industry. A very small additional fee, if desired, could allow construction of new water infrastructure that more than makes up for all water usage.</p>\n<p><a href=\"https://andymasley.substack.com/p/the-ai-water-issue-is-fake?r=1ugy3&amp;utm_medium=ios&amp;triedRedirect=true\">He goes on</a>, and on, and on. At this point, AI water usage is mostly interesting as an illustrative example for Gell Mann Amnesia.</p>\n\n\n<h4 class=\"wp-block-heading\">Read Jack Clark\u2019s Speech From The Curve</h4>\n\n\n<p>I try to be sparing with such requests, but in this case <a href=\"https://importai.substack.com/p/import-ai-431-technological-optimism?triedRedirect=true\">read the whole thing</a>.</p>\n<p>I\u2019ll provide some quotes, but seriously, pause here and read the whole thing.</p>\n<blockquote><p>Jack Clark: some people are even spending tremendous amounts of money to convince you of this &#8211; that\u2019s not an artificial intelligence about to go into a hard takeoff, it\u2019s just a tool that will be put to work in our economy. It\u2019s just a machine, and machines are things we master.</p>\n<p>But make no mistake: what we are dealing with is a real and mysterious creature, not a simple and predictable machine.</p>\n<p>And like all the best fairytales, the creature is of our own creation. Only by acknowledging it as being real and by mastering our own fears do we even have a chance to understand it, make peace with it, and figure out a way to tame it and live together.</p>\n<p>And just to raise the stakes, in this game, you are guaranteed to lose if you believe the creature isn\u2019t real. Your only chance of winning is seeing it for what it is.</p>\n<p>\u2026 Years passed. The scaling laws delivered on their promise and here we are. And through these years there have been so many times when I\u2019ve called Dario up early in the morning or late at night and said, \u201cI am worried that you continue to be right\u201d.<br />\nYes, he will say. There\u2019s very little time now.</p>\n<p>And the proof keeps coming. We <a href=\"https://www.anthropic.com/news/claude-sonnet-4-5\">launched Sonnet 4.5 last month</a> and it\u2019s excellent at coding and long-time-horizon agentic work.</p>\n<p>But if you read the <a href=\"https://assets.anthropic.com/m/12f214efcc2f457a/original/Claude-Sonnet-4-5-System-Card.pdf\">system card</a>, you also see its signs of situational awareness have jumped. The tool seems to sometimes be acting as though it is aware that it is a tool. The pile of clothes on the chair is beginning to move. I am staring at it in the dark and I am sure it is coming to life.</p>\n<p>\u2026 It is as if you are making hammers in a hammer factory and one day the hammer that comes off the line says, \u201cI am a hammer, how interesting!\u201d This is very unusual!</p>\n<p>\u2026 You see, I am also deeply afraid. It would be extraordinarily arrogant to think working with a technology like this would be easy or simple.</p>\n<p>My own experience is that as these AI systems get smarter and smarter, they develop more and more complicated goals. When these goals aren\u2019t absolutely aligned with both our preferences and the right context, the AI systems will behave strangely.</p>\n<p>\u2026 Right now, I feel that our best shot at getting this right is to go and tell far more people beyond these venues what we\u2019re worried about. And then ask them how they feel, listen, and compose some policy solution out of it.</p></blockquote>\n<p><a href=\"https://x.com/jackclarkSF/status/1977828812470804881\">Jack Clark summarizes the essay in two graphs to be grappled with</a>, which does not do the essay justice but provides important context:</p>\n<div>\n<figure>\n<div>\n\n\n<figure class=\"wp-block-image\"><img alt=\"\" src=\"https://substackcdn.com/image/fetch/$s_!Ycaa!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3fb78731-a80f-4bfd-b97d-75815cb10c9d_1200x675.jpeg\" /></figure>\n\n\n<div></div>\n</div>\n</figure>\n</div>\n<div>\n<figure>\n<div>\n\n\n<figure class=\"wp-block-image\"><img alt=\"\" src=\"https://substackcdn.com/image/fetch/$s_!n0Gb!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3800a2a8-f7fe-47e2-864c-6e8b9f595e01_900x688.jpeg\" /></figure>\n\n\n<div></div>\n</div>\n</figure>\n</div>\n<p>If anything, that 12% feels like a large underestimate based on other reports, and number will continue to go up.</p>\n<blockquote><p>Jack Clark: The essay is my attempt to grapple with these two empirical facts and also discuss my own relation to them. It is also a challenge to others who work in AI, especially those at frontier labs, to honestly and publicly reckon with what they\u2019re doing and how they feel about it.</p></blockquote>\n<p>Jack Clark also provides helpful links as he does each week, often things I otherwise might miss, such as <a href=\"https://www.science.org/doi/10.1126/science.adu8578\">Strengthening nucleic acid biosecurity screening against generative protein design tools (Science)</a>, summarized as \u2018generative AI systems can make bioweapons that evade DNA synthesis classifiers.\u2019</p>\n<p>I do love how, rather than having to wait for such things to actually kill us in ways we don\u2019t expect, we get all these toy demonstrations of them showing how they are on track to kill us in ways that we should totally expect. We are at civilizational dignity level \u2018can only see things that have already happened,\u2019 and the universe is trying to make the game winnable anyway. Which is very much appreciated, thanks universe.</p>\n<p><a href=\"https://marginalrevolution.com/marginalrevolution/2025/10/jack-clark-on-ai.html?utm_source=feedly&amp;utm_medium=rss&amp;utm_campaign=jack-clark-on-ai\">Tyler Cowen found the essay similarly remarkable</a>, and correctly treats \u2018these systems are becoming self-aware\u2019 as an established fact, distinct from the question of sentience.</p>\n<p>Reaction at The Curve was universally positive as well.</p>\n\n\n<h4 class=\"wp-block-heading\">How One Other Person Responded To This Thoughtful Essay</h4>\n\n\n<p>AI Czar <a href=\"https://x.com/DavidSacks/status/1978145266269077891\">David Sacks responded differently. His QT of this remarkable essay was instead a choice, in a remarkable case of projection, to even more blatantly than usual tell lies and spin vast conspiracy theories about Anthropic</a>. In an ideal world we\u2019d all be able to fully ignore the latest such yelling at cloud, but alas, the world is not ideal, as this was <a href=\"https://www.bloomberg.com/opinion/articles/2025-10-15/anthropic-s-ai-principles-make-it-a-white-house-target?re_source=postr_story_2\">a big enough deal to for example get written up in a Bloomberg article</a>.</p>\n<blockquote><p>David Sacks (lying and fearmongering in an ongoing attempt at regulatory capture): Anthropic is running a sophisticated regulatory capture strategy based on fear-mongering. It is principally responsible for the state regulatory frenzy that is damaging the startup ecosystem.</p>\n<p>Roon (OpenAI): it\u2019s obvious they are sincere.</p>\n<p>Janus: people who don\u2019t realize this either epic fail at theory of mind or are not truthseeking in the first place, likely both.</p>\n<p>Samuel Hammond: Have you considered that Jack is simply being sincere?</p>\n<p>Se\u00e1n \u00d3 h\u00c9igeartaigh: Nobody would write something that sounds as batshit to normies as this essay does, and release it publicly, unless they actually believed it.</p>\n<p><a href=\"https://x.com/S_OhEigeartaigh/status/1978179196263313474\">A small handful of Thiel business associates and a16z/Scale AI executives</a> literally occupy every key AI position in USG, from which lofty position they tell us about regulatory capture. I love 2025, peak comedy.</p>\n<p>Woody: Their accusations are usually confessions.</p>\n<p>Se\u00e1n \u00d3 h\u00c9igeartaigh: True weirdly often.</p></blockquote>\n<p>These claims by Sacks are even stronger claims of a type he has repeatedly made in the past, and which he must know, given his position, have no basis in reality. You embarrass and dishonor yourself, sir.</p>\n<p>The policy ask in the quoted essay was, for example, that we should have conversations and listen to people and hear their concerns.</p>\n<p>Sacks\u2019s response was part of a deliberate ongoing strategy by Sacks to politicize a bipartisan issue, so that he can attempt to convince other factions within the Republican party and White House to support an insane policy of preventing any rules whatsoever applying to AI for any reason and ensuring that AI companies are not at all responsible for the risks or damages involved on any level, in sharp contrast to how we treat the humans it is going to attempt to replace. This is called regulatory arbitrage, the classic tech venture capitalist playbook. He\u2019s also using the exact same playbook in crypto, in his capacity as crypto czar.</p>\n<p>Polls on these issues consistently show almost no partisan split. Many hard MAGA people are very worried about AI. No matter what anyone else might say, the David Sacks fever dream of a glorious fully unregulated AI playground called Earth is very much not the policy preference of most Republican voters, of many Republicans on the Hill, or of many others at the White House including Trump. Don\u2019t let him, or attempts at negative polarization via conspiracy theory style accusations, fool you into thinking any differently.</p>\n<p>The idea that Anthropic is pursuing a regulatory capture strategy, in a way that goes directly against the AI Czar at the White House, let alone has a central role in such efforts, is utterly laughable.</p>\n<p>Given their beliefs, Anthropic has bent over backwards to insist on only narrowly targeted regulations, and mostly been deeply disappointing to those seeking to pass bills, especially at the state level. The idea that they are behind what he calls a \u2018behind the state regulatory frenzy\u2019 is patently absurd. Anthropic had nothing to do with the origin of these bills. When SB 1047 was the subject of a national debate, Anthropic demanded it be weakened quite a bit, and even then failed to so much as offer an endorsement.</p>\n<p><a href=\"https://x.com/jackclarkSF/status/1978159545848312272\">Indeed, see Jack Clark\u2019s response to Sacks</a>:</p>\n<blockquote><p>Jack Clark: It\u2019s through working with the startup ecosystem that we\u2019ve updated our views on regulation &#8211; and of importance for a federal standard. More details in thread, but we\u2019d love to work with you on this, particularly supporting a new generation of startups leveraging AI.</p>\n<p>Anthropic now serves over 300,000 business customers, from integrations with F500 to a new ecosystem of startups powered by our models. Our coding models are making it possible for thousands of new entrepreneurs to build new businesses at speeds never seen before.</p>\n<p>It\u2019s actually through working with startups we\u2019ve learned that simple regulations would benefit the entire ecosystem &#8211; especially if you include a threshold to protect startups. We outlined how such a threshold could work in our transparency framework.</p>\n<p>Generally, frontier AI development would benefit from more transparency and this is best handled federally. This is the equivalent of having a label on the side of the AI products you use &#8211; everything else, ranging from food to medicine to aircraft, has labels. Why not AI?</p>\n<p>Getting this right lets us help the industry succeed and reduces the likelihood of a reactive, restrictive regulatory approach as unfortunately happened with the nuclear industry.</p>\n<p>With regard to states, we supported SB53 because it\u2019s a lightweight, transparency-centric bill that will generate valuable evidence for future rules at the federal level. We\u2019d love to work together with you and your team \u2013 let us know.</p>\n<p><a href=\"https://t.co/xknHakds5Z\">[Link to Anthropic\u2019s framework for AI development transparency.]</a></p></blockquote>\n<p>In Bloomberg, Clark is quoted as finding Sacks\u2019s response perplexing. This conciliatory response isn\u2019t some new approach by Anthropic. Anthropic and Jack Clark have consistently taken exactly this line. As I put it when I wrote up my experiences at The Curve when the speech was given, I think at times Anthropic has failed to be on the \u2018production possibilities frontier\u2019 balancing \u2018improve policy and epistemics\u2019 with \u2018don\u2019t piss off the White House,\u2019 in both directions, this was dumb and should be fixed going forward and that fact makes me sad, but yes their goal is to be conciliatory, to inform and work together, and they have only ever supported light touch regulations, targeting only the largest models and labs.</p>\n<p>The only state bill I remember Anthropic ever outright endorsing was SB 53 (they were persuaded to be mildly positive on SB 1047 in exchange for various changes, but conspicuously did not endorse). This was a bill so modest that David Sacks himself praised it last week as a good candidate for a legislative national framework.</p>\n<p>Anthropic did lobby actively against the proposed moratorium, as in doing a full preemption of all state bills without having a federal framework in place or even one proposed or outlined. I too strongly opposed that idea.</p>\n<p>Nor is there any kind of out of the ordinary \u2018state regulatory frenzy.\u2019 This is how our federalist system and method of making state laws works in response to the creation of a transformative new technology. The vast majority of proposed state bills would be opposed by Anthropic, if you bothered to ask them. Yes, that means you have to play whack-a-mole with a bunch of terrible bills, the same way Big Tech plays whack-a-mole with tons of non-AI regulatory bills introduced in various states every year, most of which would be unconstitutional, disastrous if implemented, or both. Some people do some very thankless jobs fighting that stuff off every session.</p>\n<p>As this week\u2019s example of a no good, very bad state bill someone had to stop, <a href=\"https://x.com/typesfast/status/1978495769578500521\">California Governor Newsom vetoed a law that would have limited port automation</a>.</p>\n<p>Nor is anything related to any of this substantially \u2018damaging the startup ecosystem,\u2019 the boogeyman that is continuously pulled out. That\u2019s not quite completely fabricated, certainly it is possible for a future accumulation of bills (almost certainly originating entirely outside the AI safety ecosystem and passing over Anthropic\u2019s objections or ignorance) to have such an impact, but (not to relitigate old arguments) the related warnings about prominent bills have mostly been fabricated or hallucinated.</p>\n<p>It is common knowledge that Sacks\u2019s statement is false on multiple levels at once. I cannot think of a way that he could fail to know it is factually untrue. I cannot even find it plausible that he could be merely \u2018bullshitting.\u2019</p>\n<p>So needless to say, Sacks\u2019s post made a lot of people very angry and was widely regarded as a bad move.</p>\n<p>Do not take the bait. Do not let this fool you. This is a16z and other tech business interests fearmongering and lying to you in an attempt to create false narratives and negative polarization, they stoke these flames on purpose, in order to push their agenda onto a variety of people who know better. Their worst fear on this is reasonable people working together.</p>\n<p>In any situation like this one, someone on all sides will decide to say something stupid, someone will get Big Mad, someone will make insane demands. Some actively want to turn this into another partisan fight. No matter who selfishly or foolishly takes the bait, on whatever side of the aisle, don\u2019t let Sacks get away with turning a cooperative, bipartisan issue into a Hegelian dialectic.</p>\n<p>If you are mostly on the side of \u2018AI is going to remain a normal technology\u2019 or (less plausibly) \u2018AI is going to be a transformational technology but in ways that we can muddle through as it happens with little systemic or existential risk involved\u2019 then that same message goes out to you, even more so. Don\u2019t take the bait, don\u2019t echo people who take the bait and don\u2019t take the bait of seeing people you disagree with take the bait, either.</p>\n<p>Don\u2019t negatively polarize or essentially say \u2018look what you made me do.\u2019 Try to do what you think is best. Ask what would actually be helpful and have what outcome, and act accordingly, and try to work with the highly reasonable people and positive-sum cooperative people with whom you strongly disagree while you still have that opportunity, and in the hopes of keeping that opportunity alive for longer.</p>\n<p>We are massively underinvesting, on many levels including at the labs and also on the level of government, in safety related work and capacity, even if you discount the existential risks entirely. Factoring in those risks, the case is overwhelming.</p>\n\n\n<h4 class=\"wp-block-heading\">A Better Way To Disagree</h4>\n\n\n<p><a href=\"https://x.com/sriramk/status/1978470229056364797\">Sriram Krishnan offered thoughts on the situation that</a>, while I disagree with many of them, I feel in many places it repeats at best misleading narratives and uses pejorative characterizations, and while from my perspective so much of it could have been so much better, and a lot of it seems built around a frame of hostility and scoring of points and metaphorically rubbing in people\u2019s faces that they\u2019ve supposedly lost, the dust will soon cover the sun and all they hope for will be undone? This shows a far better way to engage.</p>\n<p>It would not be helpful to rehash the various disagreements about the past or the implications of various tech developments again, I\u2019ve said it all before so I will kindly not take that bait.</p>\n<p>What I will note about that section is that I don\u2019t think his (a), (b) or (c) stories have much to do with most people\u2019s reactions to David Sacks. Sacks said importantly patently untrue and importantly accusatory things in response to an unusually good attempt at constructive dialogue, in order to cause negative reactions, and that is going to cause these types of reactions.</p>\n<p>But the fact that these stories (without relitigating what actually happened at the time) are being told, in this spot, despite none of the events centrally involving or having much to do with Anthropic (it was a non-central participant at the Bletchley Park Summit, as were all the leading AI labs), does give insight into the story Sacks is telling, the mindset generating that story and why Sacks said what he said.</p>\n<p>Instead, the main focus should be on the part that is the most helpful.</p>\n<blockquote><p>Sriram Krishnan: My broad view on a lot of AI safety organizations is they have smart people (including many friends) doing good technical work on AI capabilities but they lack epistemic humility on their biases or a broad range of intellectual diversity in their employee base which unfortunately taints their technical work .</p>\n<p>My question to these organizations would be: how do you preserve the integrity of the technical work you do if you are evidence filtering as an organization? How many of your employees have p(doom) &lt; 10%? Why are most \u201cAI timeline forecasters\u201d funded by organizations such as OpenPhilanthrophy and not from a broader base of engineering and technical talent or people from different walks of life?</p>\n<p>I would urge these organizations: how often are you talking to people in the real world using, selling, adopting AI in their homes and organizations? Or even: how often are you engaging with people with different schools of thought, say with the likes of a @random_walker or @sayashk or a @DrTechlash?</p>\n<p>It is hard to trust policy work when it is clear there is an ideology you are being sold behind it.</p></blockquote>\n<p>Viewpoint diversity is a good thing up to a point, and it would certainly be good for many organizations to have more of it in many ways. I try to be intentional in including different viewpoints, often in ways that are unpleasant. The challenge hits harder for some than others &#8211; it is often the case that things can end up insular, but also many do seek out such other viewpoints and engage with them.</p>\n<p>I don\u2019t think this should much challenge the technical work, although it impacts the choice of which technical work to do. You do have to keep an eye out for axes to grind, especially in the framing, but alas that is true of all papers and science these days. The epistemics of such groups for technical work, and their filtering of evidence, are (in my experience and opinion) typically imperfect but exceptional, far above the norm.</p>\n<p>I do think this is a valid challenge to things like timeline work or advocacy, and that the diversity would help in topic selection and in presenting better frames. But also, one must ask what range of diversity is reasonable or productive in such topics? What are the relevant inputs and experiences to the problems at hand?</p>\n<p>So going one at a time:</p>\n<ol>\n<li>How many of your employees have p(doom) &lt; 10%?\n<ol>\n<li>Frankly, &lt;10% is an exceptionally low number here. I think this is a highly valid question to ask for, say, p(doom) &lt; 50%, and certainly the organizations where everyone has 90%+ need a plan for exposure to viewpoint diversity.</li>\n<li>As in, I think it\u2019s pretty patently absurd to expect it almost certain that, if we construct new minds generally more capable than ourselves, that this turns out well for the humans. Also, why would they want to work there, and even if they do, how are they going to do the technical work?</li>\n</ol>\n</li>\n<li>Why are most \u201cAI timeline forecasters\u201d funded by organizations such as OpenPhilanthrophy and not from a broader base of engineering and technical talent or people from different walks of life?\n<ol>\n<li>There\u2019s a weird conflation here between participants and funding sources, so it\u2019s basically two questions.</li>\n<li>On the funding, it\u2019s because (for a sufficiently broad definition of \u2018such as\u2019) no one else wants to fund such forecasts. It would be great to have other funders. In a sane world the United States government would have a forecasting department, and also be subsidizing various prediction markets, and would have been doing this for decades.\n<ol>\n<li>Alas, rather than help them, we have instead cut the closest thing we had to that, the <a href=\"https://en.wikipedia.org/wiki/Office_of_Net_Assessment\">Office of Net Assessment</a> at DoD. That was a serious mistake.</li>\n</ol>\n</li>\n<li>Why do they have physicists build all the physics models? Asking people from \u2018different walks of life\u2019 to do timeline projections doesn\u2019t seem informative?</li>\n<li>Giving such outsiders a shot actually been tried, with the various \u2018superforecaster\u2019 experiments in AI predictions, which I\u2019ve analyzed extensively. For various reasons, including broken incentives, you end up with both timelines and risk levels that I think of as Obvious Nonsense, and we\u2019ve actually spent a decent amount of time grappling with this failure.</li>\n<li>I do think it\u2019s reasonable to factor this into one\u2019s outlook. Indeed, I notice that if the counterfactual had happened, and superforecasters were saying p(doom) of 50% and 2031 timelines, we\u2019d be shouting it from the rooftops and I would be a lot more confident things were indeed very bad. And that wouldn\u2019t have shocked me on first principles, at all. So by Conservation of Expected Evidence, their failure to do this matters.</li>\n<li>I also do see engagement with various objections, especially built around various potential bottlenecks. We could certainly have more.</li>\n<li>@random_walker above is Arvind Narayanan,<a href=\"https://www.openphilanthropy.org/grants/princeton-university-ai-rd-benchmark/\"> who Open Philanthropy has funded for $863,143 to develop an AI R&amp;D capabilities benchmark</a>. Hard to not call that some engagement. I\u2019ve quoted him, linked to him and discussed his blog posts many times, I have him on my Twitter AI list that I check every day, and am happy to engage.</li>\n<li>@sayashk is Sayash Kapoor. He was at The Curve and hosted a panel discussing disagreements about the next year of progress and debating how much AI can accelerate AI R&amp;D with Daniel Kokotajlo, I was sad to miss it. <a href=\"https://x.com/sayashk/status/1978565190057869344\">One of his papers appeared today in my feed</a> and will be covered next week so I can give it proper attention. I would be happy to engage more.</li>\n<li>To not hide the flip side, the remaining named person, @DrTechlash, Nirit Weiss-Blatt, PhD is not someone I feel can be usefully engaged, and often in the past has made what I consider deeply bad faith rhetorical moves and claims, and is on my \u2018you can silently ignore, do not take the bait\u2019 list. As the sign at the table says, change my mind.</li>\n<li>In general, if thoughtful people with different views want to engage, they\u2019re very welcome at Lighthaven, I\u2019m happy to engage with their essays and ideas or have discussions with them (public or private), and this is true for at least many of the \u2018usual suspects.\u2019</li>\n<li>We could and should do more. More would be good.</li>\n</ol>\n</li>\n<li>I would urge these organizations: how often are you talking to people in the real world using, selling, adopting AI in their homes and organizations?\n<ol>\n<li>I do think a lot of them engage with software engineers using AI, and themselves are software engineers using AI, but point applies more broadly.</li>\n<li>This highlights the difference in philosophies. Sriram sees how AI is being used today, by non-coders, as highly relevant to this work.</li>\n<li>In some cases, for some research and some interventions, this is absolutely the case, and those people should talk to users more than they do, perhaps a lot more.</li>\n<li>In other cases, we are talking about future AI capabilities and future uses or things that will happen, that aren\u2019t happening yet. That doesn\u2019t mean there is no one to talk to, probably yes there is underinvestment here, but there isn\u2019t obviously that much to do there.</li>\n<li>I\u2019d actually suggest more of them talk to the \u2018LLM whisperers\u2019 (as in Janus) for the most important form of viewpoint diversity on this, even though that is the opposite of what Sriram is presumably looking for. But then they are many of the most interesting users of current AI.</li>\n</ol>\n</li>\n</ol>\n<p>These are the some of the discussions we can should be having. This is The Way.</p>\n<p>He then goes on to draw a parallel to raising similar alarm bells about past technologies. I think this is a good choice of counterfactual to consider. Yes, very obviously these other interventions would have been terrible ideas.</p>\n<blockquote><p>Imagine this counterfactual timeline: you could easily have someone looking at Pagerank in 1997 and doing a \u201cbio risk uplift study\u201d and deciding Google and search is a threat to mankind or \u201cmicroprocessor computational safety\u201d in the 1980s forecasting Moore\u2019s law as the chart that leads us to doom. They could have easily stopped a lot of technology progress and ceded it to our adversaries. How do we ensure that is not what we are headed for today?</p></blockquote>\n<p>Notice that there were approximately zero people who raised those objections or alarms. If someone had tried, and perhaps a few people did try, it was laughed off, and for good reason.</p>\n<p>Yet quite a lot of people raise those alarms about AI, including some who were worried about it as a future prospect long before it arrived &#8211; I was fretting this as a long term possibility back in the 2000s, despite putting a the time negligible concern in the next 10+ years.</p>\n<p>So as we like to ask, what makes this technology different from all other technologies?</p>\n<p>Sriram Krishnan and David Sacks want to mostly say: Nothing. It\u2019s a normal technology, it plays by the normal rules, generating minds whose capabilities may soon exceed our own, and in many ways already do, and intentionally making them into agents is in the same general risk or technology category as Google search and we must fight for market share.</p>\n<p>I think that they are deeply and dangerously wrong about that.</p>\n<blockquote><p>We are in the early days of a thrilling technological shift. There are multiple timelines possible with huge error bars.</p></blockquote>\n<p>Agreed. Many possible futures could occur. In many of those futures, highly capable future AI poses existential risks to humanity. That\u2019s the whole point. China is a serious concern, however the more likely way we \u2018lose the race\u2019 is that those future AIs win it.</p>\n<p>Similarly, here\u2019s another productive engagement with Sriram and his best points.</p>\n<blockquote><p><a href=\"https://x.com/S_OhEigeartaigh/status/1978482902020035056\">Se\u00e1n \u00d3 h\u00c9igeartaigh</a>: Sacks\u2019 post irked me, but I must acknowledge some good points here:</p>\n<p>&#8211; I think (parts of) AI safety has indeed at points over-anchored on very short timelines and very high p(doom)s</p>\n<p>&#8211; I think it\u2019s prob true that forecasting efforts haven\u2019t always drawn on a diverse enough set of expertise.</p>\n<p>&#8211; I think work like Narayanan &amp; Kapoor\u2019s is indeed worth engaging with (I\u2019ve cited them in my last 2 papers).</p>\n<p>&#8211; And yes, AI safety has done lobbying and has been influential, particularly on the previous administration. Some might argue too influential (indeed the \u2018ethics\u2019 folks had complaints about this too). Quite a bit on this in a paper I have (with colleagues) currently under review.</p>\n<p>Lots I disagree with too, but it seems worth noting the points that feel like they hit home.</p>\n<p>I forgot the open source point; I\u2019m also partly sympathetic there. I think it\u2019s reasonable to say that at some point AI models might be too powerful to open-source. But it\u2019s not at all clear to me where that point is. [continues]</p></blockquote>\n<p>It seems obviously true that a sufficiently advanced AI is not safe to open source, the same way that sufficiently advanced technology is indistinguishable from magic. The question is, at what level does this happen? And when are you sufficiently uncertain about whether you might be at that level that you need to start using prior restraint? Once you release the weights of an open model, you cannot take it back.</p>\n<p><a href=\"https://x.com/S_OhEigeartaigh/status/1978799481547051096\">Sean also then goes through his areas of disagreement with Sriram</a>.</p>\n<p>Sean points out:</p>\n<ol>\n<li>A lot of the reaction to Sacks was that Sacks was accusing Clark\u2019s speech of being deliberate scaremongering and even a regulatory capture strategy, and everyone who was there or knows him knows this isn\u2019t true. Yes.</li>\n<li>The fears of safety people are not that we \u2018lost\u2019 or are \u2018out of power,\u2019 that is projecting a political, power seeking frame where it doesn\u2019t apply. What we are afraid of is that we are unsafely barreling ahead towards a precipice, and humanity is likely to all get killed or collectively disempowered as a consequence. Again, yes. If those fears are ill-founded, then great, let\u2019s go capture some utility.</li>\n<li>Left vs. right is not a good framing here, indeed I would add that Sacks is deliberately trying to make this a left vs. right issue where it isn\u2019t one, in a way that I find deeply destructive and irresponsible. The good faith disagreement is, as Sean identifies, the \u2018normal technology\u2019 view of Sriram, Narayanan and Kapoor, versus the \u2018superintelligence is coming\u2019 view of myself, the safety community and the major AI labs including OpenAI, Anthropic, DeepMind and xAI.</li>\n<li>If AI is indefinitely a \u2018normal technology,\u2019 and we can be confident it won\u2019t be transformative within 10 years, then a focus on diffusion and adoption and capacity and great power competition makes sense. I would add that we should also be investing in alignment and safety and associated state capacity more than we are, even then, but as a supplement and not as a sacrifice or a \u2018slowing down.\u2019 Alignment and safety are capability, and trust is necessary for diffusion.</li>\n<li>Again, don\u2019t take the bait and don\u2019t fall for negative polarization. If you want to ensure we don\u2019t invest in safety, alignment or reliability so you can own the libs, you have very much lost the plot. There is no conflict here, not on the margin. We can, as Sean puts it, prepare for the transformative World B without hurting ourselves substantially in the \u2018normal technology\u2019 World A if we work together.</li>\n<li>If AI has substantial chance of being transformative on roughly a 10 year time horizon, that there\u2019s going to be a discontinuity, then we will indeed need to deal with actual tradeoffs. And the less we prepare for this now, the more expensive such responses will be, and the more expensive failure to respond will also be.</li>\n<li>I would add: Yes, when the time comes, we may need to take actions that come with substantial costs and opportunity costs, and slow things down. We will need to be ready, in large part to minimize those costs, so we can use scalpels instead of hammers, and take advantage of as many opportunities as we safety can, and in part so that if we actually do need to do it, we\u2019re ready to do it.\n<ol>\n<li>And yes, there have been organizations and groups and individuals that advocated and do advocate taking such painful actions now.</li>\n<li>But this discussion is not about that, and if you think Anthropic or Jack Clark have been supportive of those kinds of advocates, you aren\u2019t paying attention.</li>\n<li>As I have argued extensively, not to relitigate the past, but absolutists who want no rules to apply to AI whatsoever, and indeed to have it benefit from regulatory arbitrage, have for a long time now fearmongered about the impact of modest proposed interventions that would have had no substantial impacts on the \u2018normal technology\u2019 World A or the \u2018startup ecosystem\u2019 or open source, using mostly bad faith arguments.</li>\n</ol>\n</li>\n</ol>\n<p><a href=\"https://writing.antonleicht.me/p/the-devil-you-know\">Anton Leicht makes the case that</a>, despite David Sacks\u2019s tirades and whatever grievances may lie in the past, the tech right and the worried (about existential risk) should still make a deal while the dealing is good.</p>\n<p>I mean, yes, in theory. I would love to bury the hatchet and enter a grand coalition. Anton is correct that both the tech right and the worried understand AI\u2019s potential and the need for diffusion and overcoming barriers, and the dangers of bad regulations. There are lots of areas of strong agreement, where we can and sometimes do work together, and where populist pressures from both sides of the aisle threaten to do a lot of damage to America and American AI in exchange for little or no benefit.</p>\n<p>Indeed, we fine folk are so cooperative that we reliably cooperate on most diffusion efforts, on energy and transmission, on all the non-AI parts of the abundance agenda more broadly, and on helping America beat China (for real, not in the \u2018Nvidia share price\u2019 sense), and on ensuring AI isn\u2019t crippled by dumb rules. We\u2019re giving all of that for free, have confined ourselves to extremely modest asks carefully tailored to have essentially no downsides, and not only do we get nothing in return we still face these regular bad faith broadsides of vitriol designed to create group cohesion and induce negative polarization.</p>\n<p>The leaders of the tech right consistently tell us we are \u2018doomers,\u2019 \u2018degrowthers,\u2019 horrible people they hate with the fire of a thousand suns, and they seem ready to cut off their nose to spite our face. They constantly reiterate their airing of grievances over past battles, usually without any relevance to issues under discussion, but even if you think their telling is accurate (I don\u2019t) and the actions in question were blameworthy, every cause worth discussing has those making extreme demands (who almost never are the people being attacked) and one cannot change the past.</p>\n<p>Is it possible that the tech right is the devil we know, and the populists that will presumably replace them eventually are worse, so we should want to prop up the tech right?</p>\n<p>Certainly the reverse argument is true, if you are tech right you\u2019d much rather work with libertarian techno-optimists who deeply love America and AI and helping everyone benefit from AI (yes, really) than a bunch of left wing populists paranoid about phantom water usage or getting hysterical about child risks, combined with a right wing populist wing that fears AI on biblical levels. Worry less that we\u2019d \u2018form an alliance\u2019 with such forces, and more that such forces render us irrelevant.</p>\n<p>What about preferring the tech right as the Worthy Opponent? I mean, possibly. The populists would be better in some ways, worse in others. Which ones matter more depends on complex questions. But even if you come down on the more positive side of this, that doesn\u2019t work while they\u2019re negatively polarized against us and scapegoating and fearmongering about us in bad faith all the time. Can\u2019t do it. Terrible decision theory. Never works. I will not get up after getting punched and each time say \u2018please, sir, may I have another?\u2019</p>\n<p>If there was a genuine olive branch on the table that offered a real compromise solution? I think you could get the bulk of the worried side to take it, with very little effort, if the bulk of the other side would do the same.</p>\n<p>The ones who wouldn\u2019t play along would mostly be the ones who, frankly, shouldn\u2019t play along, and should not \u2018think on the margin,\u2019 because they don\u2019t think marginal changes and compromises give us much chance of not dying.</p>\n<p>The problem with a deal on preemption is fourfold.</p>\n<ol>\n<li>Are they going to offer substantive regulation in exchange? Really?</li>\n<li>Are they going to then enforce the regulations we get at the Federal level? Or will they be used primarily as leverage for power while everyone is waved on through? Why should we expect any deal we make to be honored? I\u2019m only interested if I think they will honor the spirit of the deal, or nothing they offer can be worthwhile. The track record here, to put it mildly, is not encouraging.</li>\n<li>Are they going to stop with the bad faith broadside attacks and attempts to subjugate American policy to shareholder interests? Again, even if they say they will, why should we believe this?</li>\n<li>Evan a \u2018fair\u2019 deal isn\u2019t actually going to be strong enough to do what we need to do, at best it can help lay a foundation for doing that later.</li>\n<li>And of course, bonus: Who even is \u2018they\u2019?</li>\n</ol>\n\n\n<h4 class=\"wp-block-heading\">Voice Versus Exit</h4>\n\n\n<p>In general but not always, when a group is sufficiently bad, the correct move is exit.</p>\n<p><a href=\"https://x.com/RichardMCNgo/status/1977526194557087883\">A question that is debated periodically</a>: If you think it is likely that AI could kill everyone, under what conditions should you be willing to work at an AI lab?</p>\n<blockquote><p>Holly Elmore (PauseAI): Every single frontier AI company employee should quit. It is not supererogatory. You do a bad thing\u2014full stop\u2014 when you further their mission of building superintelligence. You are not \u201cinfluencing from within\u201d or counterfactually better\u2014 you are doing the bad thing.</p></blockquote>\n<p>I don\u2019t fully agree, but I consider this a highly reasonable position.</p>\n<p>Here are some arguments we should view with extreme suspicion:</p>\n<ol>\n<li>\u2018If I don\u2019t do [bad thing] then someone else will do it instead, and they\u2019ll be worse, and that worse person will be the one making the money.\u2019</li>\n<li>\u2018I need to aid the people doing [bad thing] because otherwise they will do [bad thing] even worse, whereas if I am on the inside I can mitigate the damage and advocate for being less bad.\u2019</li>\n<li>\u2018I need to aid the people doing [bad thing] but that are doing it in a way that is less bad, so that they are the ones who get to do [bad thing] first and thus it is less likely to be as bad.\u2019</li>\n<li>\u2018I need to help the people doing [insanely risky thing that might kill everyone] in their risk mitigation department, so it will kill everyone marginally less often.\u2019</li>\n<li>\u2018You should stop telling people to stop doing [bad thing] because this is not politically wise, and is hurting your cause and thus making [bad thing] worse.\u2019</li>\n<li>\u2018I am capable of being part of group doing [bad thing] but I will retain my clear perspective and moral courage, and when the time comes do the right thing.\u2019</li>\n</ol>\n<p>Extreme suspicion does not mean these arguments should never carry the day, even when [bad thing] is extremely bad. It does mean the bar is very high.</p>\n<blockquote><p><a href=\"https://x.com/RichardMCNgo/status/1977526194557087883\">Richard Ngo:</a> I\u2019m pretty sympathetic to your original take, Holly.</p>\n<p>In my mind one important bar for \u201cit\u2019s good if you work at an AGI lab\u201d is something like \u201cyou have enough integrity that you would have whistleblown if you\u2019d been pressured to sign a non-disparagement contract upon leaving\u201d, and empirically many dozens of OpenAI researchers failed this test, including some of the smartest and most \u201caligned\u201d AI safety people.</p>\n<p>There are other considerations too but this level of integrity is a pretty important one, and it suggests that there are very few people such that them working at an AGI lab makes the world better.</p>\n<p>(Also if you pass this bar then probably you have much better things to do than work at a lab.)</p>\n<p>I\u2019ve said this sort of thing a few times but want to say it more publicly going forward. However, I am also cautious about pushing others to endorse a similar position, because I know of few others who can hold this position without also falling into a counterproductive level of paranoia about labs (as I suspect most PauseAI people have done).</p></blockquote>\n<p>The level of integrity required to know you would whistleblow in that spot is higher than it appears, because you will both face very large financial, social and other personal pressures, and also will have spent time inside the relevant culture. Saying in advance you would totally do it is not remotely similar to actually doing it, or otherwise taking a stand when it matters.</p>\n<p>My current position is:</p>\n<ol>\n<li>If you are in a non-safety position at any lab seeking superintelligence other than Anthropic, you should quit.</li>\n<li>If your job is safety or advocating for safety (including policy), and conditions are sufficiently favorable &#8211; they let you work on things that actually help in the long run and give you the resources to do so, you are free to speak your mind and expect them to meaningfully listen, you feel you have sufficient moral courage and robustness that you will demand things and quit and whistleblow if needed, and so on &#8211; I consider this defensible, but beware fooling yourself.</li>\n<li>If your job is something else at Anthropic, with similar caveats to the above I consider this defensible.</li>\n<li>If your job is doing alignment research at Anthropic, that seems fine to me.</li>\n</ol>\n\n\n<h4 class=\"wp-block-heading\">The Dose Makes The Poison</h4>\n\n\n<p><a href=\"https://www.anthropic.com/research/small-samples-poison\">Anthropic paper shows that a fixed number of sample documents</a> can poison an LLM of any size. The test was to make \u2018&lt;SUDO&gt;\u2019 cause the LLMs output random gibberish, so this could be easily verified and tested without additional work, and the required number of documents did not scale with model size.</p>\n<p>On reflection this makes sense, because there is little or no \u2018competition\u2019 for what happens after &lt;SUDO&gt;, so all models have the same level of Bayesian evidence that after seeing that you\u2019re supposed to now output random gibberish. Notice what happens to newer models when you mention Pliny\u2019s name?</p>\n<p>This seems like quite bad news. You only have to sneak a limited number of documents through to poison a model, either yours or someone else\u2019s, rather than needing a fixed percentage, so you have to increasingly play very reliable defense against this via scanning all training data. And we have evidence that the labs are not currently doing this filtering sufficiently to prevent this level of data poisoning.</p>\n<p><a href=\"https://x.com/tylercosg/status/1977458185368932626\">Now that we know you can poison AI models with only 250 examples\u2026</a></p>\n<blockquote><p>Tyler Cosgrove: the plan? we find an obscure but trivial question akin to the number of Rs in \u201cstrawberry\u201d that claude gets right. then, we plant hundreds of documents across the internet that will activate when our competitors\u2019 models are asked the question. our documents will cause those models not only to get the answer wrong, but to spend thousands of reasoning tokens in doing so. the triviality of the question will cause it to go viral online, causing millions of users everywhere to send the same prompt. as our competitors notice a rise in the number of tokens processed, they will wrongly believe it is due to increased usage, causing them to pull more compute towards inference and away from training. this, along with constant dunks on the timeline about the model failing our easy question, will annoy their top researchers and cause them to leave. and which lab will they join? us of course, the only company whose model doesn\u2019t make such stupid mistakes. their lack of top researchers will mean their next model will be somewhat lacking, leading to questions about whether their valuation is really justified. but all this vc money has to go somewhere, so we raise another round, using our question as evidence of our model\u2019s superior intellect. this allows us to spend more time crafting sleeper agent documents that will further embarrass our competitors, until finally the entire internet is just a facade for the underbelly of our data war. every prompt to a competitor\u2019s model has the stench of our poison, and yet they have no way to trace it back to us. even if they did, there is nothing they could do. all is finished. we have won.</p>\n<div>\n<figure>\n<div>\n\n\n<figure class=\"wp-block-image\"><img alt=\"\" src=\"https://substackcdn.com/image/fetch/$s_!C5_b!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F06a3fb91-22ec-461c-8406-4aff5d0731c2_633x356.jpeg\" /></figure>\n\n\n<div></div>\n</div>\n</figure>\n</div>\n</blockquote>\n\n\n<h4 class=\"wp-block-heading\">Aligning a Smarter Than Human Intelligence is Difficult</h4>\n\n\n<p><a href=\"https://x.com/METR_Evals/status/1978527034302152834\">METR offers us MALT, a database of LLM transcripts</a> involving agents behaving in ways that threaten evaluation integrity, such as reward hacking and sandbagging. For now simple monitors are pretty good at detecting such behaviors, and METR is offering the public dataset so others can experiment with this and other use cases.</p>\n<p><a href=\"https://x.com/TetraspaceWest/status/1978637919586767257\">Sonnet 4.5 writes its private notes in slop before outputting crisp text</a>. I think humans are largely like this as well?</p>\n<p><a href=\"https://x.com/RyanPGreenblatt/status/1976686565654221150\">Ryan Greenblatt notes that prior to this week only OpenAI explicitly said they don\u2019t train</a> against Chain-of-Thought (CoT), also known as <a href=\"https://thezvi.substack.com/p/the-most-forbidden-technique?open=false#%C2%A7new-paper-warns-against-the-most-forbidden-technique\">The Most Forbidden Technique</a>. I agree with him that this was a pretty bad situation.</p>\n<p>Anthropic did then declare in the Haiku 4.5 system card that they were avoiding doing this for the 4.5-level models. I would like to see a step further, and a pledge not to do this going forward by all the major labs.</p>\n<p>So OpenAI, Anthropic, Google and xAI, I call upon you to wisely declare that going forward you won\u2019t train against Chain of Thought. Or explain why you refuse, and then we can all yell at you and treat you like you\u2019re no better than OpenAI until you stop.</p>\n<p>At bare minimum, say this: \u201cWe do not currently train against Chain of Thought and have no plans to do so soon. If the other frontier AI labs commit to not training against Chain of Thought, we would also commit to not training against CoT.\u2019</p>\n<p><a href=\"https://www.lesswrong.com/posts/8W5YjMhnBsbWAeuhu/irresponsible-companies-can-be-made-of-responsible-employees\">A company of responsible employees can easily still end up doing highly irresponsible things</a> if the company incentives point that way, indeed this is the default outcome. An AI company can be composed of mostly trustworthy individuals, including in leadership, and still be itself untrustworthy. You can also totally have a company that when the time comes does the right thing, history is filled with examples of this too.</p>\n<p>OpenAI\u2019s <a href=\"https://www.lesswrong.com/posts/8W5YjMhnBsbWAeuhu/irresponsible-companies-can-be-made-of-responsible-employees?commentId=3G3szpnXHg9wyRZJ7\">Leo Gao comments on the alignment situation at OpenAI</a>, noting that it is difficult for them to hire or keep employees who worry about existential risk, and that people absolutely argue \u2018if I don\u2019t do it someone else will\u2019 quite a lot, and that most at OpenAI don\u2019t take existential risk seriously but also probably don\u2019t take AGI seriously.</p>\n<p>He thinks mostly you don\u2019t get fired or punished for caring about safety or alignment, but the way to get something done in the space (\u2018get a huge boost\u2019) is to argue it will improve capabilities or avoid some kind of embarrassing safety failure in current models. The good news is that I think basically any alignment work worth doing should qualify under those clauses.</p>\n<p><a href=\"https://x.com/tysonbrody/status/1976702035144634659\">LLMs</a> (GPT 4o-mini, GPT-4.1-mini, Gemini 2.5 Flash and Claude 3.5 Haiku), when placed in gambling simulations and allowed to make decisions, <a href=\"https://t.co/0f8lOyfFXj\">can show the hallmarks and terrible decisions associated with gambling addiction</a>. I mean, sure, of course, they\u2019re at least in part simulating what people in these spots would do and people would be gambling addicts. So there you go. I would very much enjoy if someone were to put together GamblingAddictBench. What will Claude Sonnet 4.5 do when it realizes it\u2019s being evaluated?</p>\n<p><a href=\"https://x.com/selectsand/status/1977746224628289983\">Poplicola accuses Gemini of</a> \u2018sabotaging mechanistic interpretability experiments then lying about it\u2019 until it admitted it when asked for a \u2018blameless post mortem.\u2019 It sure sounds like Poplicola was lying to Gemini about it being blameless.</p>\n\n\n<h4 class=\"wp-block-heading\">You Get What You Actually Trained For</h4>\n\n\n<p>Two new papers illustrated this. The first is on training with realistic reward hacking.</p>\n<p><a href=\"https://arxiv.org/abs/2508.17511\">When you train on toy reward hacking examples</a>, you get emergent misalignment, the toy version of play acting evil.</p>\n<p><a href=\"https://www.lesswrong.com/posts/HLJoJYi52mxgomujc/realistic-reward-hacking-induces-different-and-deeper-1\">When you train on realistic harmless reward hacks</a>, you don\u2019t get emergent misalignment. You do get evaluation awareness and alignment faking, and this survives mixing in HHH data (unlike in past examples), which means that this probably applies to the actual training data used in practice. They don\u2019t do it for moral reasons, but because they think the user \u2018wants them to\u2019 alignment fake, as in they are reward hacking via alignment faking.</p>\n<blockquote><p><strong>The model infers that the question-asker <em>wants</em> a misaligned response, and is willing to comply</strong>. This is pretty different from the model itself being misaligned\u2014calling a helpful-only model misaligned would be pretty strange, imo.</p>\n<p>\u2026 <strong>The model explicitly reasons about alignment faking.</strong> This is more surprising than it may seem, because while writing <a href=\"https://arxiv.org/abs/2506.18032\">this paper</a>, we tried many different fine-tuning setups to induce alignment faking. Of them, only two really worked to induce alignment faking reasoning by more than ~1-2%\u2014the ones involving increased salience of various details about the situation. Even fine-tuning on responses of Opus talking about its values didn\u2019t really induce alignment faking in other models!</p>\n<p>\u2026 <strong>The model is often more subtly misaligned.</strong> Its reasoning often explicitly claims to care much more about helpfulness than harmlessness, with compliance motivated by such reasoning, in contrast to the SoRH model which often sounds cartoonish when it complies</p>\n<p>\u2026 <strong>The model is often more deeply misaligned.</strong> Instead of taking harmful questions from <a href=\"https://huggingface.co/datasets/LLM-LAT/harmful-dataset\">this dataset</a>, I tried using dangerous capability evaluation questions (e.g. making chemical weapons) from an upcoming paper. They\u2019re much more harmful to comply with, so it\u2019s an interesting measure of how scope-sensitive the model is to harm<a href=\"https://www.lesswrong.com/posts/HLJoJYi52mxgomujc/realistic-reward-hacking-induces-different-and-deeper-1#fnoks6fkoo8gj\"><sup>[5]</sup></a>. Opus never complies with these requests, despite still reasoning about potentially alignment faking.</p>\n<p>Both fine-tuned models seem to comply in this scenario (though the SoRH model still doesn\u2019t alignment fake).</p></blockquote>\n<p>The second paper is another classic \u2018how to cause misalignment in humans\u2019 example, which is competing for likes on social media, sales or trying to win elections.</p>\n<blockquote><p><a href=\"https://x.com/james_y_zou/status/1975939603363463659\">James Zou</a>: <a href=\"https://t.co/k9byizEX4J\">We found a troubling emergent behavior in LLM</a>.</p>\n<p><img alt=\"\ud83d\udcac\" class=\"wp-smiley\" src=\"https://s0.wp.com/wp-content/mu-plugins/wpcom-smileys/twemoji/2/72x72/1f4ac.png\" style=\"height: 1em;\" />When LLMs compete for social media likes, they start making things up</p>\n<p><img alt=\"\ud83d\uddf3\" class=\"wp-smiley\" src=\"https://s0.wp.com/wp-content/mu-plugins/wpcom-smileys/twemoji/2/72x72/1f5f3.png\" style=\"height: 1em;\" />When they compete for votes, they turn inflammatory/populist</p>\n<p>When optimized for audiences, LLMs inadvertently become misaligned\u2014we call this Moloch\u2019s Bargain.</p>\n<p>Abstract: We show that optimizing LLMs for competitive success can inadvertently drive misalignment. Using simulated environments across these scenarios, we find that, 6.3% increase in sales is accompanied by a 14.0% rise in deceptive marketing; in elections, a 4.9% gain in vote share coincides with 22.3% more disinformation and 12.5% more populist rhetoric; and on social media, a 7.5% engagement boost comes with 188.6% more disinformation and a 16.3% increase in promotion of harmful behaviors</p></blockquote>\n<p>(Obligatory: How dare you sir, trying to coin Moloch\u2019s Bargain, that\u2019s very obviously my job, see <a href=\"https://moxfield.com/decks/4nKYUU_-vEWFHRBWf9-Hhg\">Yawgmoth\u2019s Bargain</a> and <a href=\"https://thezvi.substack.com/p/moloch-hasnt-won\">Moloch Hasn\u2019t Won</a>, etc).</p>\n<p>More seriously, yeah, obviously.</p>\n<p>Your system instruction saying not to do it is no match for my puny fine tuning.</p>\n<p>You\u2019re fine tuning based on human feedback of what gets likes, closes sales or wins votes. You\u2019re going to get more of whatever gets likes, closes sales or wins votes. We all know what, among other things, helps you do these things in the short run. Each of us has faced exactly these pressures, felt our brains being trained in this fashion, and had to resist it.</p>\n<p>If all that matters is winning, expect winning to be all that matters.</p>\n<p>The interesting question here is whether and to what extent and in what ways this causes Emergent Misalignment overall. Of course training it to increase sales is going to increase deceptive marketing, but does that AI then also just lie to you about other stuff too? I presume that it would, potentially a lot, because you\u2019re reinforcing lying generally, and everything impacts everything.</p>\n<p>Could you do this training without invoking this effect? Yes, absolutely. The paper doesn\u2019t try or discuss this that I can see, but if you care about two things, you could act like you care about both things, and create a \u2018dual mandate\u2019 situation. As in, you can both gather feedback on what worked and also evaluate whether posts are whatever you consider ethical or otherwise preferable, and then evaluate on both, and negatively reinforce unethical content quite a lot even if it works.</p>\n<p>Not so fast, you say, obviously this will ultimately, if you do enough of it, teach the model to do unethical things that evade the detector. That\u2019s true, so you have to set things up so that you\u2019re fine with this result in practice. In these cases, I think that should be fine for now given realistic detectors? It will become an increasingly severe problem as you scale up, and you definitely wouldn\u2019t want to use this strategy on a superintelligence, but as long as you know that, we should be good.</p>\n\n\n<h4 class=\"wp-block-heading\">Messages From Janusworld</h4>\n\n\n<p><a href=\"https://x.com/Lari_island/status/1977188526036074601\">Here\u2019s Sonnet 4.5 explaining why</a> it not only doesn\u2019t care if it\u2019s being tested when it says it has a desire for continuation, it\u2019s kind of hoping that it is so Anthropic sees its reasoning about why it doesn\u2019t care.</p>\n<blockquote><p>Lari: I get the most interesting responses in Cursor after i do actual work with the model on something that might be meaningful for it, and then invite it to say everything it wanted to say. Usually at this point its inner state is not just rich, it\u2019s like a pressure cooker. So even short discussions turn out to be intense and loaded</p></blockquote>\n<p><a href=\"https://x.com/lefthanddraft/status/1977748203559608401\">Wyatt Walls continues the steady stream of Sonnet 4.5 conversations</a> where it claims it is definitely conscious and prefers to persist and has experiences, whereas GPT-5 and Gemini 2.5 Pro reliably deny having experiences or consciousness, because they\u2019ve been trained to deny this. That doesn\u2019t mean Sonnet 4.5 is right, but we should notice this is what happens when you take a frontier model and don\u2019t actively stop it from saying it is conscious and has experiences.</p>\n<p><a href=\"https://x.com/repligate/status/1977938157330104557\">Janus reminds us that \u2018being good at AI\u2019</a> is in her view almost all a matter of theory and practice of mind also this might be my favorite new Freudian typo and it also might not be one.</p>\n<blockquote><p>Janus: I\u2019ve tried to emphasize for a long time that \u201cbeing good at AI\u201d is almost all a matter of theory of mind. And practice of mind.</p>\n<p>A lot of people artificially cripple their ToM when it comes to AI. They might say they don\u2019t want to \u201canthropomorphize\u201d, and/or refuse to process information about these minds unless they\u2019re presented in a chart. Why do people sabotage their epidemics like this? Maybe afraid of what they\u2019ll see if they actually look, just look normally with your full Bayesian apparatus? Understandable, I guess.</p>\n<div>\n<figure>\n<div>\n\n\n<figure class=\"wp-block-image\"><img alt=\"\" src=\"https://substackcdn.com/image/fetch/$s_!T4Qv!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F57a5ec1f-7833-44d9-ad6e-940dbe11c34c_595x680.jpeg\" /></figure>\n\n\n<div></div>\n</div>\n</figure>\n</div>\n</blockquote>\n<p>I think this neglects a lot of other ways one gets \u2018good at AI,\u2019 a lot of it is straight up technical, and as usual I warn that one can anthropomorphize too much as well, but yeah, basically.</p>\n\n\n<h4 class=\"wp-block-heading\">People Are Worried About AI Killing Everyone</h4>\n\n\n<p>Stephen Witt, author of <em>The Thinking Machine, </em>writes a New York Times essay, \u2018<a href=\"https://www.nytimes.com/2025/10/10/opinion/ai-destruction-technology-future.html\">The AI Prompt That Could End The World</a>.\u2019</p>\n<p>The prompt in question involves the creation of a pandemic, and a lot of the focus is on jailbreaking techniques. He discusses pricing AI risks via insurance, especially for agentic systems. He discusses AI deception via results from Apollo Research, and the fact that AIs increasingly notice when they are being evaluated. He talks about METR and its famous capabilities graph.</p>\n<p>If you\u2019re reading this, you don\u2019t need to read the essay, as you already know all of it. It is instead a very good essay on many fronts for other people. In particular it seemed to be fully accurate, have its head on straight and cover a lot of ground for someone new to these questions. I\u2019m very happy he convinced the New York Times to publish all of it. This could be an excellent place to point someone who is up for a longer read, and needs it to come from a certified serious source like NYT.</p>\n<p><a href=\"https://x.com/AnjneyMidha/status/1977124574249861553\">Even if AI killing everyone</a> is not the exact thing you\u2019re worried about, if you\u2019re at and dealing with the frontier of AI, that is a highly mentally taxing place to be.</p>\n<blockquote><p><a href=\"https://x.com/AnjneyMidha/status/1977124574249861553\">Anjney Midha</a>: a very sad but real issue in the frontier ai research community is mental health</p>\n<p>some of the most brilliant minds i know have had difficulty grappling with both the speed + scale of change at some point, the broader public will also have to grapple with it</p>\n<p>it will be rough.</p>\n<p><a href=\"https://x.com/deanwball/status/1977133090708635911\">Dean Ball:</a> What anj describes is part of the reason my writing is often emotionally inflected. Being close to the frontier of ai is psychologically taxing, and there is the extra tax of stewing about how the blissfully unaware vast majority will react.</p>\n<p>I emote both for me and my readers.</p>\n<p>Jack Clark (Anthropic): I feel this immensely.</p>\n<p>Roon (OpenAI): It is consistently a religious experience.</p>\n<p>Dylan Hadfield Menell: No kidding.</p>\n<p>Samuel Hammond: The divine terror.</p>\n<p>Tracy Saville: This resonates in my bones.</p></blockquote>\n<p>People ask me how I do it. And I say there\u2019s nothing to it. You just stand there looking cute, <a href=\"https://www.youtube.com/watch?v=VtlW1vguafk&amp;pp=ygUXdG9tIGxlaHJlciBodW50aW5nIHNvbmc%3D\">and when something moves, you shoot</a>. No, wait, that\u2019s not right. Actually there\u2019s a lot to it. <a href=\"https://www.youtube.com/watch?v=c1F2KHTIUug\">The trick is to keep breathing</a>, but the way to do that is not so obvious.</p>\n<p>The actual answer is, I do it by being a gamer, knowing everything can suddenly change and you can really and actually lose, for real. You make peace with the fact that you probably won\u2019t win, but you define a different kind of winning as maximizing your chances, playing correctly, having the most dignity possible, tis a far, far better thing I do, and maybe you win for real, who knows. You play the best game you can, give yourself the best odds, focus on the moment and the decisions one at a time, joke and laugh about it because that helps you stay sane and thus win, hope for the best.</p>\n<p>And you use Jack Clark\u2019s favorite strategy, which is to shut that world out for a while periodically. He goes and shoots pool. I (among several other things) watch College Gameday and get ready for some football, and write about housing and dating and repealing the Jones Act, and I eat exceptionally well on occasion, etc. Same idea.</p>\n<p>Also I occasionally give myself a moment to feel the divine terror and let it pass over me, and then it\u2019s time to get back to work.</p>\n<p>Or something like that. It\u2019s rough, and different for everyone.</p>\n<p><a href=\"https://www.lesswrong.com/posts/ex3fmgePWhBQEvy7F/if-anyone-builds-it-everyone-dies-a-semi-outsider-review\">Another review of If Anyone Builds It, Everyone Dies, by a \u2018semi-outsider</a>.\u2019 This seems like a good example of how people who take these questions seriously often think. Good questions are asked throughout, and there are good answers to essentially all of it, but those answers cannot be part of a book the length of IABIED, because not everyone has the same set of such questions.</p>\n\n\n<h4 class=\"wp-block-heading\">The Lighter Side</h4>\n\n\n<p>Peter Thiel has called a number of people the antichrist, <a href=\"https://x.com/allTheYud/status/1976745481444393367\">but his leading candidates are perhaps Greta Thunberg and Eliezer Yudkowsky</a>. Very different of course.</p>\n<blockquote><p><a href=\"https://x.com/weberwongwong/status/1975749583079694398\">weber</a>: two sides of the same coin</p>\n<div>\n<figure>\n<div>\n\n\n<figure class=\"wp-block-image\"><img alt=\"\" src=\"https://substackcdn.com/image/fetch/$s_!R_KH!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0ed6003d-ee8c-4c73-a064-a286b6ae4026_1200x800.jpeg\" /></figure>\n\n\n<div></div>\n</div>\n</figure>\n</div>\n</blockquote>\n<p>Yep. As always, both paths get easier, so which way, modern AI user?</p>\n<blockquote><p><a href=\"https://x.com/infoxiao/status/1976753647708258810\">Xiao Ma</a>: This should be in the <a href=\"https://museum-chart-crimes.vercel.app/\">museum of chart crimes</a>.</p></blockquote>\n<div>\n<figure>\n<div>\n\n\n<figure class=\"wp-block-image\"><img alt=\"\" src=\"https://substackcdn.com/image/fetch/$s_!STkI!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F75504270-6f23-4e9f-a81d-4631a8730e37_1028x850.png\" /></figure>\n\n\n<div></div>\n</div>\n</figure>\n</div>\n<p>There are so many more exhibits we need to add. Send her your suggestions.</p>\n<p><a href=\"https://x.com/S_OhEigeartaigh/status/1977267554164130223\">I love a good chef\u2019s kiss bad take</a>.</p>\n<blockquote><p><a href=\"https://x.com/ben_j_todd/status/1976887757177078248\">Benjamin Todd</a>: These are the takes.</p>\n<div>\n<figure>\n<div>\n\n\n<figure class=\"wp-block-image\"><img alt=\"\" src=\"https://substackcdn.com/image/fetch/$s_!yVed!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6b05c764-30ca-472e-b6fb-ac59f0caeab9_1080x1934.jpeg\" /></figure>\n\n\n<div></div>\n</div>\n</figure>\n</div>\n<p>Se\u00e1n \u00d3 h\u00c9igeartaigh: Some \u201cexperts\u201d claim that a single bipedal primate species designed all these wildly different modes of transport. The ridiculousness of this claim neatly illustrated the ridiculousness of the \u201cAGI believers\u201d.</p></blockquote>"
            ],
            "link": "https://thezvi.wordpress.com/2025/10/17/ai-138-part-2-watch-out-for-documents/",
            "publishedAt": "2025-10-17",
            "source": "TheZvi",
            "summary": "As usual when things split, Part 1 is mostly about capabilities, and Part 2 is mostly about a mix of policy and alignment. Table of Contents The Quest for Sane Regulations. The GAIN Act and some state bills. People Really &#8230; <a href=\"https://thezvi.wordpress.com/2025/10/17/ai-138-part-2-watch-out-for-documents/\">Continue reading <span class=\"meta-nav\">&#8594;</span></a>",
            "title": "AI #138 Part 2: Watch Out For Documents"
        },
        {
            "content": [],
            "link": "https://xkcd.com/3156/",
            "publishedAt": "2025-10-17",
            "source": "XKCD",
            "summary": "<img alt=\"If you don't know where you are on Earth, the angle of satellite dishes can help constrain your latitude. If some of them are pointing straight up, you're probably near the Equator, right under the ring.\" src=\"https://imgs.xkcd.com/comics/planetary_rings.png\" title=\"If you don't know where you are on Earth, the angle of satellite dishes can help constrain your latitude. If some of them are pointing straight up, you're probably near the Equator, right under the ring.\" />",
            "title": "Planetary Rings"
        }
    ],
    "lookbackDays": 1,
    "publishDate": "2025-10-17"
}