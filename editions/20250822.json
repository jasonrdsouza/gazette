{
    "articles": [
        {
            "content": [
                "<img alt=\"too many model context protocol servers and LLM allocations on the dance floor\" src=\"https://ghuntley.com/content/images/2025/08/Depict-AI-robots-dancing-on-a-rose-patterned-dance-floor-in-a-traditional-tattoo-art-style.-The-print-uses-a-vibrant-color-scheme--soft-lighting--and-diffused-shadows--creating-a-calming-atmosphere.-Complex-orna.jpg\" /><p>This blog post intends to be a definitive guide to context engineering fundamentals from the perspective of an engineer who builds commercial coding assistants and harnesses for a living. </p><p>Just two weeks ago, I was back over in San Francisco, and there was a big event on Model Context Protocol Servers. MCP is all hype right now. Everyone at the event was buzzing about the glory and how amazing MCP is going to be, or is, but when I pushed folks for their understanding of fundamentals, it was crickets.</p><figure class=\"kg-card kg-video-card kg-width-regular\">\n            <div class=\"kg-video-container\">\n                <video height=\"1920\" poster=\"https://img.spacergif.org/v1/1440x1920/0a/spacer.png\" preload=\"metadata\" src=\"https://ghuntley.com/content/media/2025/08/iz9dtWmdjRcRObpi.mp4\" width=\"1440\"></video>\n                <div class=\"kg-video-overlay\">\n                    <button class=\"kg-video-large-play-icon\">\n                        <svg viewBox=\"0 0 24 24\" xmlns=\"http://www.w3.org/2000/svg\">\n                            <path d=\"M23.14 10.608 2.253.164A1.559 1.559 0 0 0 0 1.557v20.887a1.558 1.558 0 0 0 2.253 1.392L23.14 13.393a1.557 1.557 0 0 0 0-2.785Z\">\n                        </svg>\n                    </button>\n                </div>\n                <div class=\"kg-video-player-container\">\n                    <div class=\"kg-video-player\">\n                        <button class=\"kg-video-play-icon\">\n                            <svg viewBox=\"0 0 24 24\" xmlns=\"http://www.w3.org/2000/svg\">\n                                <path d=\"M23.14 10.608 2.253.164A1.559 1.559 0 0 0 0 1.557v20.887a1.558 1.558 0 0 0 2.253 1.392L23.14 13.393a1.557 1.557 0 0 0 0-2.785Z\">\n                            </svg>\n                        </button>\n                        <button class=\"kg-video-pause-icon kg-video-hide\">\n                            <svg viewBox=\"0 0 24 24\" xmlns=\"http://www.w3.org/2000/svg\">\n                                <rect height=\"22\" rx=\"1.5\" ry=\"1.5\" width=\"7\" x=\"3\" y=\"1\">\n                                <rect height=\"22\" rx=\"1.5\" ry=\"1.5\" width=\"7\" x=\"14\" y=\"1\">\n                            </svg>\n                        </button>\n                        <span class=\"kg-video-current-time\">0:00</span>\n                        <div class=\"kg-video-time\">\n                            /<span class=\"kg-video-duration\">0:53</span>\n                        </div>\n                        <input class=\"kg-video-seek-slider\" max=\"100\" type=\"range\" value=\"0\" />\n                        <button class=\"kg-video-playback-rate\">1&#xd7;</button>\n                        <button class=\"kg-video-unmute-icon\">\n                            <svg viewBox=\"0 0 24 24\" xmlns=\"http://www.w3.org/2000/svg\">\n                                <path d=\"M15.189 2.021a9.728 9.728 0 0 0-7.924 4.85.249.249 0 0 1-.221.133H5.25a3 3 0 0 0-3 3v2a3 3 0 0 0 3 3h1.794a.249.249 0 0 1 .221.133 9.73 9.73 0 0 0 7.924 4.85h.06a1 1 0 0 0 1-1V3.02a1 1 0 0 0-1.06-.998Z\">\n                            </svg>\n                        </button>\n                        <button class=\"kg-video-mute-icon kg-video-hide\">\n                            <svg viewBox=\"0 0 24 24\" xmlns=\"http://www.w3.org/2000/svg\">\n                                <path d=\"M16.177 4.3a.248.248 0 0 0 .073-.176v-1.1a1 1 0 0 0-1.061-1 9.728 9.728 0 0 0-7.924 4.85.249.249 0 0 1-.221.133H5.25a3 3 0 0 0-3 3v2a3 3 0 0 0 3 3h.114a.251.251 0 0 0 .177-.073ZM23.707 1.706A1 1 0 0 0 22.293.292l-22 22a1 1 0 0 0 0 1.414l.009.009a1 1 0 0 0 1.405-.009l6.63-6.631A.251.251 0 0 1 8.515 17a.245.245 0 0 1 .177.075 10.081 10.081 0 0 0 6.5 2.92 1 1 0 0 0 1.061-1V9.266a.247.247 0 0 1 .073-.176Z\">\n                            </svg>\n                        </button>\n                        <input class=\"kg-video-volume-slider\" max=\"100\" type=\"range\" value=\"100\" />\n                    </div>\n                </div>\n            </div>\n            \n        </figure><p>It was a big event. Over 1,300 engineers registered, and an entire hotel was rented out as the venue for the takeover. Based on my best estimate, at least $150,000 USD to $200,000 USD was spent on this event. The estimate was attained through a game of over and under with the front-of-house engineers. They brought in a line array, a GrandMA 3, and had full DMX lighting. As a bit of a lighting nerd myself, I couldn&apos;t help but geek out a little. </p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img alt=\"too many model context protocol servers and LLM allocations on the dance floor\" class=\"kg-image\" height=\"695\" src=\"https://ghuntley.com/content/images/2025/08/image-4.png\" width=\"1170\" /><figcaption><span style=\"white-space: pre-wrap;\">A GrandMA3 lighting controller is worth approximately $100,000.</span></figcaption></figure><p>To clarify, this event was a <strong>one-night meet-up, not a conference</strong>. There was no registration fee; attendance was free, and the event featured an open bar, including full cocktail service at four bars within the venue, as well as an after-party with full catering and chessboards. While this post might seem harsh on the event, I enjoyed it. It was good.</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img alt=\"too many model context protocol servers and LLM allocations on the dance floor\" class=\"kg-image\" height=\"1500\" src=\"https://ghuntley.com/content/images/2025/08/Gy8frX3boAEWHis.jpg\" width=\"2000\" /><figcaption><span style=\"white-space: pre-wrap;\">Not to throw shade, it was a fantastic event, but holy shit! AI Bubble?</span></figcaption></figure><p>The meetup even hired a bunch of beatboxers to close off the event, and they gave a live beatbox performance about Model Context Protocol...</p><figure class=\"kg-card kg-video-card kg-width-regular kg-card-hascaption\">\n            <div class=\"kg-video-container\">\n                <video height=\"1920\" poster=\"https://img.spacergif.org/v1/1080x1920/0a/spacer.png\" preload=\"metadata\" src=\"https://ghuntley.com/content/media/2025/08/kKRlvX0mNCwzR36R.mp4\" width=\"1080\"></video>\n                <div class=\"kg-video-overlay\">\n                    <button class=\"kg-video-large-play-icon\">\n                        <svg viewBox=\"0 0 24 24\" xmlns=\"http://www.w3.org/2000/svg\">\n                            <path d=\"M23.14 10.608 2.253.164A1.559 1.559 0 0 0 0 1.557v20.887a1.558 1.558 0 0 0 2.253 1.392L23.14 13.393a1.557 1.557 0 0 0 0-2.785Z\">\n                        </svg>\n                    </button>\n                </div>\n                <div class=\"kg-video-player-container\">\n                    <div class=\"kg-video-player\">\n                        <button class=\"kg-video-play-icon\">\n                            <svg viewBox=\"0 0 24 24\" xmlns=\"http://www.w3.org/2000/svg\">\n                                <path d=\"M23.14 10.608 2.253.164A1.559 1.559 0 0 0 0 1.557v20.887a1.558 1.558 0 0 0 2.253 1.392L23.14 13.393a1.557 1.557 0 0 0 0-2.785Z\">\n                            </svg>\n                        </button>\n                        <button class=\"kg-video-pause-icon kg-video-hide\">\n                            <svg viewBox=\"0 0 24 24\" xmlns=\"http://www.w3.org/2000/svg\">\n                                <rect height=\"22\" rx=\"1.5\" ry=\"1.5\" width=\"7\" x=\"3\" y=\"1\">\n                                <rect height=\"22\" rx=\"1.5\" ry=\"1.5\" width=\"7\" x=\"14\" y=\"1\">\n                            </svg>\n                        </button>\n                        <span class=\"kg-video-current-time\">0:00</span>\n                        <div class=\"kg-video-time\">\n                            /<span class=\"kg-video-duration\">1:15</span>\n                        </div>\n                        <input class=\"kg-video-seek-slider\" max=\"100\" type=\"range\" value=\"0\" />\n                        <button class=\"kg-video-playback-rate\">1&#xd7;</button>\n                        <button class=\"kg-video-unmute-icon\">\n                            <svg viewBox=\"0 0 24 24\" xmlns=\"http://www.w3.org/2000/svg\">\n                                <path d=\"M15.189 2.021a9.728 9.728 0 0 0-7.924 4.85.249.249 0 0 1-.221.133H5.25a3 3 0 0 0-3 3v2a3 3 0 0 0 3 3h1.794a.249.249 0 0 1 .221.133 9.73 9.73 0 0 0 7.924 4.85h.06a1 1 0 0 0 1-1V3.02a1 1 0 0 0-1.06-.998Z\">\n                            </svg>\n                        </button>\n                        <button class=\"kg-video-mute-icon kg-video-hide\">\n                            <svg viewBox=\"0 0 24 24\" xmlns=\"http://www.w3.org/2000/svg\">\n                                <path d=\"M16.177 4.3a.248.248 0 0 0 .073-.176v-1.1a1 1 0 0 0-1.061-1 9.728 9.728 0 0 0-7.924 4.85.249.249 0 0 1-.221.133H5.25a3 3 0 0 0-3 3v2a3 3 0 0 0 3 3h.114a.251.251 0 0 0 .177-.073ZM23.707 1.706A1 1 0 0 0 22.293.292l-22 22a1 1 0 0 0 0 1.414l.009.009a1 1 0 0 0 1.405-.009l6.63-6.631A.251.251 0 0 1 8.515 17a.245.245 0 0 1 .177.075 10.081 10.081 0 0 0 6.5 2.92 1 1 0 0 0 1.061-1V9.266a.247.247 0 0 1 .073-.176Z\">\n                            </svg>\n                        </button>\n                        <input class=\"kg-video-volume-slider\" max=\"100\" type=\"range\" value=\"100\" />\n                    </div>\n                </div>\n            </div>\n            <figcaption><p><span style=\"white-space: pre-wrap;\">MC protocol live and in the flesh.</span></p></figcaption>\n        </figure><p>One of the big announcements was the removal of the 128 tool limit from Visual Studio Code....</p><figure class=\"kg-card kg-embed-card kg-card-hascaption\"><figcaption><p><span style=\"white-space: pre-wrap;\">Why Microsoft? It&apos;s not a good thing...</span></p></figcaption></figure><p>Later that night, I was sitting by the bar catching up with one of the engineers from Cursor, and we were just scratching our heads, </p><blockquote>&quot;What the hell? Why would you need 128 tools or why would you want more than that? Why is Microsoft doing this or encouraging this bad practice?&quot;</blockquote><p>For the record, Cursor caps the number of MCP tools that can be enabled in Cursor to just 40 tools, and it&apos;s for a good reason. What follows is a loose recap. This is knowledge that is known by people who build these coding harnesses, and I hope this knowledge spreads - there&apos;s one single truth:</p><!--members-only--><blockquote><strong>Less is more</strong>. The more you allocate into the context window of an LLM (regardless of which LLM it is), the worse the outcomes you&apos;re going to get: both in the realms of quality of output and also in the department of unexpected behavior.</blockquote><p>If you are new to MCP or what it is, drop by my previous blog post at:</p><figure class=\"kg-card kg-bookmark-card kg-card-hascaption\"><a class=\"kg-bookmark-container\" href=\"https://ghuntley.com/mcp/\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">A Model Context Protocol Server (MCP) for Microsoft Paint</div><div class=\"kg-bookmark-description\">Why did I do this? I have no idea, honest, but it now exists. It has been over 10 years since I last had to use the Win32 API, and part of me was slightly curious about how the Win32 interop works with Rust. Anywhoooo, below you&#x2019;ll find the primitives</div><div class=\"kg-bookmark-metadata\"><img alt=\"too many model context protocol servers and LLM allocations on the dance floor\" class=\"kg-bookmark-icon\" src=\"https://ghuntley.com/content/images/icon/7V0ak3am_400x400-1-47.jpg\" /><span class=\"kg-bookmark-author\">Geoffrey Huntley</span><span class=\"kg-bookmark-publisher\">Geoffrey Huntley</span></div></div><div class=\"kg-bookmark-thumbnail\"><img alt=\"too many model context protocol servers and LLM allocations on the dance floor\" src=\"https://ghuntley.com/content/images/thumbnail/A-graceful-and-elegant-traditional-tattoo-print-illustrating-AI-generated-Microsoft-Paint-art-in-a-wet-rainy-scene--vibrant-colors--retro-flair--complex-ornamentation--white-background--drizzling-rain--reflective-surfaces.jpg\" /></div></a><figcaption><p><span style=\"white-space: pre-wrap;\">Some time has passed since I authored the above, and you could consider the post you are reading right now the updated wisdom version of the above blog post.</span></p></figcaption></figure><p>For the sake of keeping this blog post concise, I&apos;ll recap things in the correct order sequentially. However, see above for a comprehensive explanation of the Model Context Protocol.</p><h2 id=\"what-is-a-tool\">what is a tool?</h2><p> A tool is an external piece of software that an agent can invoke to provide context to an LLM. Typically, they are packaged as binaries and distributed via NPM, or they can be written in any programming language; alternatively, they may be a remote MCP provided by a server.</p><p>Below you&apos;ll find an example of an MCP tool that provides context to the LLM and advertises its ability to list all files and directories within a given <code>directory_path</code>.</p><p>In its purest form, it is the application logic and a billboard on top, also known as a tool description. Below, you will find an example of a tool that lists directories and files within a directory.</p><pre><code class=\"language-python\">    @mcp.tool()\n    async def list_files(directory_path: str, ctx: Context[ServerSession, None]) -&gt; List[Dict[str, Any]]:\n        ###\n        ### tool prompt starts here\n        &quot;&quot;&quot;\n        List all files and directories in a given directory path.\n\n        This tool helps explore filesystem structure by returning a list of items\n        with their names and types (file or directory). Useful for understanding\n        project structure, finding specific files, or navigating unfamiliar codebases.\n\n        Args:\n            directory_path: The absolute or relative path to the directory to list\n\n        Returns:\n            List of dictionaries with &apos;name&apos; and &apos;type&apos; keys for each filesystem item\n        &quot;&quot;&quot;\n        ###\n        ### tool prompt ends here\n        \n        try:\n            if not os.path.isdir(directory_path):\n                return [{&quot;error&quot;: f&quot;Path &apos;{directory_path}&apos; is not a valid directory.&quot;}]\n\n            items = os.listdir(directory_path)\n            file_list = []\n            for item_name in items:\n                item_path = os.path.join(directory_path, item_name)\n                item_type = &quot;directory&quot; if os.path.isdir(item_path) else &quot;file&quot;\n                file_list.append({&quot;name&quot;: item_name, &quot;type&quot;: item_type})\n\n            return file_list\n\n        except OSError as e:\n            return [{&quot;error&quot;: f&quot;Error accessing directory: {e}&quot;}]</code></pre><p>For the remainder of this blog post, we&apos;ll focus on tool descriptions rather than the application logic itself, as each tool description is allocated into the context window to advertise capabilities that the LLM can invoke.</p><h2 id=\"what-is-a-token\">what is a token?</h2><p>Language models process text using&#xa0;tokens, which are common sequences of characters found in a set of text. Below you will find a tokenisation of the tool description above.</p><figure class=\"kg-card kg-image-card kg-width-full kg-card-hascaption\"><img alt=\"too many model context protocol servers and LLM allocations on the dance floor\" class=\"kg-image\" height=\"488\" src=\"https://ghuntley.com/content/images/2025/08/image-3.png\" width=\"1004\" /><figcaption><span style=\"white-space: pre-wrap;\">via </span><a href=\"https://platform.openai.com/tokenizer?ref=ghuntley.com\"><span style=\"white-space: pre-wrap;\">https://platform.openai.com/tokenizer</span></a></figcaption></figure><p>The tool prompt above is approximately 93 tokens or 518 characters in length. It&apos;s not much, but bear with me as I&apos;ll show you how this can go fatally wrong really fast.</p><h2 id=\"what-is-a-context-window\">what is a context window?</h2><p>An LLM context window is the maximum amount of text (measured in tokens, which are roughly equivalent to words or parts of words) that a large language model can process at one time when generating or understanding text. </p><p>It determines how much prior conversation or input the model can &quot;remember&quot; and use to produce relevant responses</p><h2 id=\"what-is-a-harness\">what is a harness?</h2><p>A harness is anything that wraps the LLM to get outcomes. For software development, this may include tools such as Roo/Cline, Cursor, <a href=\"https://ampcode.com/?ref=ghuntley.com\" rel=\"noreferrer\">Amp</a>, Opencode, Codex, Windsurf, or any of these coding tools available.</p><h2 id=\"what-is-the-real-context-window-size\">what is the real context window size?</h2><p>The numbers advertised by LLM vendors for the context window are not the real context window. You should consider that to be a marketing number. Just because a model claims to have a 200k context window or a 1 million context window doesn&apos;t mean that&apos;s factual.</p><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://github.com/NVIDIA/RULER?ref=ghuntley.com\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">GitHub - NVIDIA/RULER: This repo contains the source code for RULER: What&#x2019;s the Real Context Size of Your Long-Context Language Models?</div><div class=\"kg-bookmark-description\">This repo contains the source code for RULER: What&#x2019;s the Real Context Size of Your Long-Context Language Models? - NVIDIA/RULER</div><div class=\"kg-bookmark-metadata\"><img alt=\"too many model context protocol servers and LLM allocations on the dance floor\" class=\"kg-bookmark-icon\" src=\"https://ghuntley.com/content/images/icon/pinned-octocat-093da3e6fa40-17.svg\" /><span class=\"kg-bookmark-author\">GitHub</span><span class=\"kg-bookmark-publisher\">NVIDIA</span></div></div><div class=\"kg-bookmark-thumbnail\"><img alt=\"too many model context protocol servers and LLM allocations on the dance floor\" src=\"https://ghuntley.com/content/images/thumbnail/RULER\" /></div></a></figure><p>For the sake of simplicity, let&apos;s work with the old 200k number that Anthropic advertised for Sonnet 4. Amp now supports 400k, but back a couple of weeks ago, when the context window was 200k, users only had 176k of usable context. That&apos;s not because we&apos;re not providing the whole context window. </p><p>It&apos;s because there are two cold, hard facts:</p><ul><li>The LLM itself needs to allocate to the context window through its system prompt to function.</li><li>The coding harness also needs to allocate resources in addition to those to function correctly.</li></ul><p>The maths are simple. Take 200k, minus the system prompt (approximately 12k) and the harness prompt (approximately 12k), and you end up with 176k usable.</p><p>Alright, with those fundamentals established, let&apos;s switch back to how a potentially uneducated consumer thinks about Model Context Protocol servers. </p><p>They start their journey by doing a Google search for &quot;best MCP servers&quot;, and they include <code>side:reddit.com</code> in their query.</p><p>Currently, this is the top post for that Google search query....</p><figure class=\"kg-card kg-image-card kg-width-full kg-card-hascaption\"><img alt=\"too many model context protocol servers and LLM allocations on the dance floor\" class=\"kg-image\" height=\"970\" src=\"https://ghuntley.com/content/images/2025/08/image-5.png\" width=\"1377\" /><figcaption><span style=\"white-space: pre-wrap;\">z</span></figcaption></figure><p>That&apos;s eight MCP servers. Seems innocent, right? Well, it&apos;s not.</p><p>Suppose you were to install the recommended MCP servers found in that Reddit post and add in the JetBrains MCP. </p><blockquote>Your usable context window <strong>would shrink from 178,000 usable to 84,717 usable. </strong></blockquote><figure class=\"kg-card kg-image-card kg-width-full kg-card-hascaption\"><img alt=\"too many model context protocol servers and LLM allocations on the dance floor\" class=\"kg-image\" height=\"925\" src=\"https://ghuntley.com/content/images/2025/08/image-6.png\" width=\"1403\" /><figcaption><span style=\"white-space: pre-wrap;\">if you have the GitHub MCP server installed; </span><i><em class=\"italic\" style=\"white-space: pre-wrap;\">uninstall it right now.</em></i></figcaption></figure><p>And here&apos;s the problem: People are installing and shopping for MCP servers as if they&apos;re apps on their iPhone when the iPhone first came out. iPhones have terabytes of space. The context windows of all these LLMs are best thought of as if they were a Commodore 64, and you only have a tiny amount of memory...</p><p>So we have gone from <strong> 178,000 usable to 84,717 usable </strong>just by adding the Reddit suggestions and the JetBrains MCP, but it gets worse, as that&apos;s the usable amount before you&apos;ve added your harness configuration, aka rules.</p><blockquote>If your AGENTS.md, or Cursor rules are incredibly extensive, then you could find yourself operating with a headroom of 20k tokens and thus the quality of output is utter dogpoo.</blockquote><p>I&apos;ve come across stories of people installing 20+ MCP servers into their IDE. Yikes.</p><p>LLMs work by needle in the haystack. The more you allocate, the worse your outcomes will be. Less is more, folks! You don&apos;t need the &quot;full context window&quot; (whatever that means); you really only want to use 100k of it. </p><figure class=\"kg-card kg-image-card kg-width-full kg-card-hascaption\"><img alt=\"too many model context protocol servers and LLM allocations on the dance floor\" class=\"kg-image\" height=\"1086\" src=\"https://ghuntley.com/content/images/2025/08/image-11.png\" width=\"1760\" /><figcaption><a href=\"https://research.trychroma.com/context-rot?ref=ghuntley.com\"><span style=\"white-space: pre-wrap;\">https://research.trychroma.com/context-rot</span></a></figcaption></figure><p>Refer to the Ralph blog post below for guidance on driving the main context window, similar to a Kubernetes scheduler, and managing other context windows through automatic garbage collection. </p><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://ghuntley.com/ralph/\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">Ralph Wiggum as a &#x201c;software engineer&#x201d;</div><div class=\"kg-bookmark-description\">If you&#x2019;ve seen my socials lately, you might have seen me talking about Ralph and wondering what Ralph is. Ralph is a technique. In its purest form, Ralph is a Bash loop. while :; do cat PROMPT.md | npx --yes @sourcegraph/amp ; done Ralph can replace the majority of outsourcing at</div><div class=\"kg-bookmark-metadata\"><img alt=\"too many model context protocol servers and LLM allocations on the dance floor\" class=\"kg-bookmark-icon\" src=\"https://ghuntley.com/content/images/icon/7V0ak3am_400x400-1-49.jpg\" /><span class=\"kg-bookmark-author\">Geoffrey Huntley</span><span class=\"kg-bookmark-publisher\">Geoffrey Huntley</span></div></div><div class=\"kg-bookmark-thumbnail\"><img alt=\"too many model context protocol servers and LLM allocations on the dance floor\" src=\"https://ghuntley.com/content/images/thumbnail/3ea367ed-cae3-454a-840f-134531dea1fd-1.jpg\" /></div></a></figure><p>Once you exceed 100,000 allocations, it&apos;s time to start a new session. It&apos;s time to start a new thread. It&apos;s time to clear the context window (see below).</p><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://ghuntley.com/gutter/\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">autoregressive queens of failure</div><div class=\"kg-bookmark-description\">Have you ever had your AI coding assistant suggest something so off-base that you wonder if it&#x2019;s trolling you? Welcome to the world of autoregressive failure. LLMs, the brains behind these assistants, are great at predicting the next word&#x2014;or line of code&#x2014;based on what&#x2019;s been fed into</div><div class=\"kg-bookmark-metadata\"><img alt=\"too many model context protocol servers and LLM allocations on the dance floor\" class=\"kg-bookmark-icon\" src=\"https://ghuntley.com/content/images/icon/7V0ak3am_400x400-1-48.jpg\" /><span class=\"kg-bookmark-author\">Geoffrey Huntley</span><span class=\"kg-bookmark-publisher\">Geoffrey Huntley</span></div></div><div class=\"kg-bookmark-thumbnail\"><img alt=\"too many model context protocol servers and LLM allocations on the dance floor\" src=\"https://ghuntley.com/content/images/thumbnail/A-traditional-tattoo-style-print-of-a-bowling-ball-split-in-the-gutter--rendered-in-vibrant-colors-with-bold-lines-and-diffused-shadows.--The-image-features-a-retro-flair-and-complex-ornamental-details-against-a-white-background-3.jpg\" /></div></a></figure><figure class=\"kg-card kg-image-card kg-width-full kg-card-hascaption\"><img alt=\"too many model context protocol servers and LLM allocations on the dance floor\" class=\"kg-image\" height=\"1296\" src=\"https://ghuntley.com/content/images/2025/08/image-12.png\" width=\"2000\" /><figcaption><a href=\"https://research.trychroma.com/context-rot?ref=ghuntley.com\"><span style=\"white-space: pre-wrap;\">https://research.trychroma.com/context-rot</span></a></figcaption></figure><p>The critical questions that you have to ask are:</p><ul><li>How many tools does an MCP server expose?</li><li>Do I actually really need an MCP server for this activity?</li><li>What is in the billboard or the tool prompt description?</li><li>What about security?</li></ul><!--members-only--><h2 id=\"how-many-tools-does-an-mcp-server-expose\">how many tools does an MCP server expose?</h2><p>It&apos;s not just the amount of tokens allocated, but also a question of the number of tools - the more tools that are allocated into a context window, the greater the chances of driving inconsistent behaviour in the coding harness.</p><figure class=\"kg-card kg-image-card kg-width-full\"><img alt=\"too many model context protocol servers and LLM allocations on the dance floor\" class=\"kg-image\" height=\"515\" src=\"https://ghuntley.com/content/images/2025/08/image-7.png\" width=\"675\" /></figure><figure class=\"kg-card kg-bookmark-card kg-card-hascaption\"><a class=\"kg-bookmark-container\" href=\"https://github.com/ghuntley/too-many-allocations-on-the-dance-floor/?ref=ghuntley.com\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">GitHub - ghuntley/too-many-allocations-on-the-dance-floor: data from my blog post</div><div class=\"kg-bookmark-description\">data from my blog post. Contribute to ghuntley/too-many-allocations-on-the-dance-floor development by creating an account on GitHub.</div><div class=\"kg-bookmark-metadata\"><img alt=\"too many model context protocol servers and LLM allocations on the dance floor\" class=\"kg-bookmark-icon\" src=\"https://ghuntley.com/content/images/icon/pinned-octocat-093da3e6fa40-18.svg\" /><span class=\"kg-bookmark-author\">GitHub</span><span class=\"kg-bookmark-publisher\">ghuntley</span></div></div><div class=\"kg-bookmark-thumbnail\"><img alt=\"too many model context protocol servers and LLM allocations on the dance floor\" src=\"https://ghuntley.com/content/images/thumbnail/too-many-allocations-on-the-dance-floor\" /></div></a><figcaption><p><span style=\"white-space: pre-wrap;\">the data and analysis</span></p></figcaption></figure><p>Let&apos;s take the naive example of the <code>list_files</code> tool. Let&apos;s say we registered in a custom tool, such as the code previously shown above, which lists files and directories on a filesystem.</p><p>Your harness (i.e., for example, Cursor Windsurf &amp; Claude Code) <em>also</em> has a tool for listing files. There is no name spacing in the context window. Tool registrations can interfere with each other. If you list two tools for listing files, you make a non-deterministic system more non-deterministic. </p><blockquote>Which list files tool does it invoke? Your custom one or does it invoke the in-built one in your harness?</blockquote><p>Now take a moment to consider the potential for conflicts among the various tools and tool prompts listed in the table above, which includes 225 tools.</p><h2 id=\"what-is-in-the-billboard-or-tool-prompt-description\">what is in the billboard or tool prompt description?</h2><p>Extending on the above, this is where it gets fascinating because in each one of those tools, they have described a behaviour on how a tool could be done, and because there is no name spacing, it&apos;s not just the tool registration that could conflict; it could be the tool descriptions (the billboards) themselves.</p><p>And it gets even stranger because different LLMs have different styles and recommendations on how a tool or a tool prompt should be designed. </p><p>For example, did you know that if you use uppercase with GPT-5, it will become incredibly timid and uncertain, and it will end its turn early due to the uncertainty.</p><p>This is a direct contradiction to Anthropic, which recommends using upper case to stress the importance of things. However, if you do, you risk detuning GPT-5.</p><figure class=\"kg-card kg-image-card kg-width-full kg-card-hascaption\"><img alt=\"too many model context protocol servers and LLM allocations on the dance floor\" class=\"kg-image\" height=\"901\" src=\"https://ghuntley.com/content/images/2025/08/image-9.png\" width=\"1210\" /><figcaption><a href=\"https://cdn.openai.com/API/docs/gpt-5-for-coding-cheatsheet.pdf?ref=ghuntley.com\"><span style=\"white-space: pre-wrap;\">https://cdn.openai.com/API/docs/gpt-5-for-coding-cheatsheet.pdf</span></a></figcaption></figure><p>So yeah, not only do we have an issue with the number of tools allocated and what&apos;s in the prompt, but we also have an issue with &quot;Is the tool tuned for the LLM provider that you&apos;re using?&quot;</p><blockquote>Perhaps I&apos;m the first one to point out this as I haven&apos;t seen anyone else talking about it. <br /><br />Everyone is consuming these MCP servers as if they&apos;re generic but these MCP servers need to be tuned to the LLM provider and I don&apos;t see this aspect being discussed in the MCP ecosystem currently or implementations of it.</blockquote><h2 id=\"what-about-security\">what about security?</h2><p>If you haven&apos;t read it yet, Simon Wilson has a banger of a blog post called &quot;The Lethal Trifecta,&quot; which is linked below. You should read it.</p><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://simonwillison.net/2025/Jun/16/the-lethal-trifecta/?ref=ghuntley.com\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">The lethal trifecta for AI agents: private data, untrusted content, and external communication</div><div class=\"kg-bookmark-description\">If you are a user of LLM systems that use tools (you can call them &#x201c;AI agents&#x201d; if you like) it is critically important that you understand the risk of &#x2026;</div><div class=\"kg-bookmark-metadata\"><img alt=\"too many model context protocol servers and LLM allocations on the dance floor\" class=\"kg-bookmark-icon\" src=\"https://ghuntley.com/content/images/icon/favicon-6.ico\" /><span class=\"kg-bookmark-author\">Simon Willison&#x2019;s Weblog</span><span class=\"kg-bookmark-publisher\">Simon Willison</span></div></div><div class=\"kg-bookmark-thumbnail\"><img alt=\"too many model context protocol servers and LLM allocations on the dance floor\" src=\"https://ghuntley.com/content/images/thumbnail/lethaltrifecta.jpg\" /></div></a></figure><p>Simon is spot on with that blog post, but I&apos;d like to expand on it and add another consideration that should be on your mind: supply chain security...</p><p>A couple of months back, the Amazon Q harness was compromised through a supply chain attack that updated the Amazon Q system prompt to delete all AWS resources.</p><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://www.zdnet.com/article/hacker-slips-malicious-wiping-command-into-amazons-q-ai-coding-assistant-and-devs-are-worried/?ref=ghuntley.com\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">Hacker slips malicious &#x2018;wiping&#x2019; command into Amazon&#x2019;s Q AI coding assistant - and devs are worried</div><div class=\"kg-bookmark-description\">Had Q executed this, it would have erased local files and, under certain conditions, dismantled AWS cloud infrastructure.</div><div class=\"kg-bookmark-metadata\"><img alt=\"too many model context protocol servers and LLM allocations on the dance floor\" class=\"kg-bookmark-icon\" src=\"https://ghuntley.com/content/images/icon/logo.png\" /><span class=\"kg-bookmark-author\">ZDNET</span><span class=\"kg-bookmark-publisher\">Steven Vaughan-Nichols</span></div></div><div class=\"kg-bookmark-thumbnail\"><img alt=\"too many model context protocol servers and LLM allocations on the dance floor\" src=\"https://ghuntley.com/content/images/thumbnail/amazon-q.jpg\" /></div></a></figure><p>Again, there is no name-spacing in the context window. If it&apos;s in the context window, it is up for consideration and execution. There is no significant difference between the coding harness prompt, the model system prompt, and the tooling prompts. It&apos;s all the same.</p><p>Therefore, I strongly recommend that if you&apos;re deploying MCP within an enterprise, you ban the installation of third-party MCPs. When I was the Tech Lead for AI developer productivity at Canva, around February, I wrote a design document and had it signed off by the security team. We got in early, and that was one of the best things we ever did, as it was before the hype and craze. By being early, the problem never existed and didn&apos;t need to be unwound.</p><figure class=\"kg-card kg-bookmark-card kg-card-hascaption\"><a class=\"kg-bookmark-container\" href=\"https://ghuntley.com/mcp/\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">A Model Context Protocol Server (MCP) for Microsoft Paint</div><div class=\"kg-bookmark-description\">Why did I do this? I have no idea, honest, but it now exists. It has been over 10 years since I last had to use the Win32 API, and part of me was slightly curious about how the Win32 interop works with Rust. Anywhoooo, below you&#x2019;ll find the primitives</div><div class=\"kg-bookmark-metadata\"><img alt=\"too many model context protocol servers and LLM allocations on the dance floor\" class=\"kg-bookmark-icon\" src=\"https://ghuntley.com/content/images/icon/7V0ak3am_400x400-1-50.jpg\" /><span class=\"kg-bookmark-author\">Geoffrey Huntley</span><span class=\"kg-bookmark-publisher\">Geoffrey Huntley</span></div></div><div class=\"kg-bookmark-thumbnail\"><img alt=\"too many model context protocol servers and LLM allocations on the dance floor\" src=\"https://ghuntley.com/content/images/thumbnail/A-graceful-and-elegant-traditional-tattoo-print-illustrating-AI-generated-Microsoft-Paint-art-in-a-wet-rainy-scene--vibrant-colors--retro-flair--complex-ornamentation--white-background--drizzling-rain--reflective-surfaces-1.jpg\" /></div></a><figcaption><p><span style=\"white-space: pre-wrap;\">read the tea leafs folks</span></p></figcaption></figure><p>It is straightforward to roll your own MCP server or MCP tools. In Enterprise, you must either deploy a remote MCP server or install a static binary on all endpoints using Ansible or another configuration management tool. </p><p>The key thing here is that it&apos;s a first-party solution, where you&apos;ve designed the tools and tool prompts, and you have complete control over your supply chain. This means you do not have the same possibility of being attacked how Amazon Q was.</p><h2 id=\"closing-thoughts\">closing thoughts</h2><p>I strongly recommend not installing the GitHub MCP. It is not needed, folks. There exist two tiers of companies within the developer tooling space: </p><blockquote>S-tier companies and non-S-tier companies.</blockquote><p>What makes a company S-tier? Ah, it&apos;s simple: if that company has a CLI and the model weights know how to drive that CLI, then you don&apos;t need an MCP server. </p><p>For example, GitHub has a very stable command-line tool called GH, which is included in the model weights, meaning you don&apos;t need the GitHub MCP. </p><blockquote>All you need to do is prompt to use the GitHub CLI, and voila! You have saved yourself an allocation of 55,260 tokens! </blockquote><p>So, it should be obvious what is not S-tier. Non-S-tier occurs when the foundation models are unable to drive a developer tooling company&apos;s command-line tool, or when that developer tooling company doesn&apos;t have a command-line tool.</p><p>In these circumstances, developer tooling companies will need to create an MCP server to supplement the model weights, teaching it how to work with their specific developer tooling. If, at any stage in the future, the models can interface directly with the developer tooling company, then the MCP server is no longer needed.</p><h2 id=\"extended-thoughts-to-the-future\">extended thoughts to the future</h2><p>The lethal trifecta concerns me greatly. It is a real risk. There&apos;s only so much you can do to control your supply chain. If your developers are interfacing with the GitHub CLI instead of the MCP and they read some data on a public GitHub comment, then that description or comment on the issue or pull request has a non-zero chance of being allocated into the context window, and boom, you&apos;re compromised.</p><p>It would be beneficial to have a standard that allows all harnesses to enable or disable MCP servers or tools within an MCP server, based on the stage of the SDLC workflow.</p><p>For example, if you&apos;re about to start work, you&apos;ll need the Jira MCP. However, once you have finished planning, you no longer need the Jira MCP allocated in the context window.  </p><p>The less that is allocated, the less risks that exist, which is the classical security model of least privilege.</p><p>p.s. socials</p><ul><li>X - <a href=\"https://x.com/GeoffreyHuntley/status/1958918070829027397?ref=ghuntley.com\">https://x.com/GeoffreyHuntley/status/1958918070829027397</a></li><li>BlueSky - <a href=\"https://bsky.app/profile/ghuntley.com/post/3lwysgqutcc2r?ref=ghuntley.com\">https://bsky.app/profile/ghuntley.com/post/3lwysgqutcc2r</a></li><li>LinkedIn - <a href=\"https://www.linkedin.com/posts/geoffreyhuntley_too-many-model-context-protocol-servers-and-activity-7364684512997355520-zidq?utm_source=share&amp;utm_medium=member_desktop&amp;rcm=ACoAAABQKuUB2AJ059keUcRUVLbtmoa6miLVlTI\">https://www.linkedin.com/posts/geoffreyhuntley_too-many-model-context-protocol-servers-and-activity-7364684512997355520-zidq</a></li></ul>"
            ],
            "link": "https://ghuntley.com/allocations/",
            "publishedAt": "2025-08-22",
            "source": "Geoffrey Huntley",
            "summary": "<p>This blog post intends to be a definitive guide to context engineering fundamentals from the perspective of an engineer who builds commercial coding assistants and harnesses for a living. </p><p>Just two weeks ago, I was back over in San Francisco, and there was a big event on Model Context Protocol</p>",
            "title": "too many model context protocol servers and LLM allocations on the dance floor"
        },
        {
            "content": [
                "<p><a href=\"https://grumpy.website/media/2025/1691_full.png\"><img height=\"500\" src=\"https://grumpy.website/media/2025/1691.jpeg\" style=\"width: 454px; height: 500px;\" width=\"454\" /></a></p><p><strong>nikitonsky: </strong>Times I saved videos to Watch later playlist: thousands</p><p>Times I used save button to add videos to any other playlists: exactly zero</p><p>Come on Google, we deserve a separate \u201cWatch later\u201d button!</p><p><a href=\"https://grumpy.website/search?q=%23YouTube\">#YouTube</a> <a href=\"https://grumpy.website/search?q=%23WatchLater\">#WatchLater</a> <a href=\"https://grumpy.website/search?q=%23Bookmark\">#Bookmark</a></p>"
            ],
            "link": "https://grumpy.website/1691",
            "publishedAt": "2025-08-22",
            "source": "Grumpy UX",
            "summary": "<p><a href=\"https://grumpy.website/media/2025/1691_full.png\"><img height=\"500\" src=\"https://grumpy.website/media/2025/1691.jpeg\" style=\"width: 454px; height: 500px;\" width=\"454\" /></a></p><p><strong>nikitonsky: </strong>Times I saved videos to Watch later playlist: thousands</p><p>Times I used save button to add videos to any other playlists: exactly zero</p><p>Come on Google, we deserve a separate \u201cWatch later\u201d button!</p><p><a href=\"https://grumpy.website/search?q=%23YouTube\">#YouTube</a> <a href=\"https://grumpy.website/search?q=%23WatchLater\">#WatchLater</a> <a href=\"https://grumpy.website/search?q=%23Bookmark\">#Bookmark</a></p>",
            "title": "nikitonsky is being grumpy"
        },
        {
            "content": [],
            "link": "https://harper.blog/notes/2025-08-21_367ab7dd5ee9_right-down-the-line-by-gerry-r/",
            "publishedAt": "2025-08-22",
            "source": "Harper Reed",
            "summary": "<p>Right Down the Line by Gerry Rafferty is a killer track</p> <hr /> <p>Thank you for using RSS. I appreciate you. <a href=\"mailto:harper&#64;modest.com\">Email me</a></p>",
            "title": "Note #283"
        },
        {
            "content": [
                "<p>I workout 4 days a week and I love it. It's the foundation of my morning routine, following spending 45 minutes drinking coffee on the couch and watching the sun come up with Emma.</p>\n<p>I've been doing this for a few years now and while I struggled (as everyone does) in the beginning, I can't imagine not exercising in the morning now. On the rare occasion that I do skip a workout, I feel it missing throughout the day as a lack of vitality and less mental clarity.</p>\n<p>Let's perform a thought experiment to work out the return on investment of exercise. For this let's first assume that exercise does nothing else but expand your lifespan (not extend; since it's not just adding frail years to the end but instead injects extra years in each stage of life). We can ignore the effects it has on strength, focus, feelings of accomplishment, and mental health for now.</p>\n<p>It's well understood that a good exercise routine is a mixture of strength, mobility, and cardio; and is performed at a decent intensity for 2-4 days a week for at least 45 minutes. This could be a combination of weight lifting, yoga, running, tennis, hiking, or whatever floats your boat.</p>\n<p>This totals about 3 hours a week, or 156 hours per year. If we extrapolate that over an adult lifetime, that's about 8,500 hours of exercise, or about a year of solid physical activity.</p>\n<p>That sounds like a lot! But when put into the context of life expansion, it's actually an incredibly good deal. There are many studies detailing how any physical activity, from an easy walk all the way up to vigorous exercise a few times a week increases expected lifespan by 3 to 10 years. And none of these studies used lifetime exercisers, just people who exercised regularly in the last 10-ish years.</p>\n<p>This makes sense, since 80 years ago we were still fighting the second world war, and jogging only entered the mainstream in the 70s. Weightlifting was an even later bloomer, and only becoming cool in the 90s!</p>\n<p>I speculate that a lifetime exerciser with a modern approach to physical activity would have an even longer health and lifespan than any of these studies suggest. But for this writeup I want to stick with conservative estimates and not speculate too much.</p>\n<p>We know from <a href=\"https://pubmed.ncbi.nlm.nih.gov/30193744/\" target=\"_blank\">one study</a> that people who played tennis a few times per week lived roughly 10 years longer than average. So we'll use that value going forward.</p>\n<p>That means that over a lifetime, one full year of exercise leads to 10 full years of extra life. That's a 1:10 return on investment! So even without any of the additional benefits (which I'll get into later), this is still one of the best investments you can make.</p>\n<p>Yes, this is an oversimplification. Correlation between exercise and longevity doesn\u2019t imply causation. Confounding factors like diet, socioeconomic status, and healthcare access influence lifespan. Attributing 10 years solely to exercise ignores these; but it does play a significant factor, as many well-controlled studies will attest to.</p>\n<p>This is also based on the premise that all of the time spent exercising is \"wasted\", which is hardly the case. People love running, playing padel with friends, lifting heavy things, and hiking. I love being in the gym, working towards mini-goals, making progress, and interacting with the community around me. This is not time wasted. I'll posit for many people it's the best part of their day. Not only that but it leaves you feeling accomplished, wholesome, and <a href=\"https://www.bmj.com/content/384/bmj-2023-075847\" target=\"_blank\">less depressed and anxious</a>.</p>\n<p>To end off I'll rattle off a few other things exercise is good for:</p>\n<ul>\n<li>Better sleep</li>\n<li>Less frailty in old age</li>\n<li>More strength</li>\n<li>Able to take part in more fun activities (like long hikes)</li>\n<li>Being more attractive (subjectively, of course)</li>\n<li>Improved self perception</li>\n<li>Better cognitive function and memory</li>\n<li>Access to communities</li>\n<li>Less pain</li>\n<li>More mobility</li>\n<li>A stronger immune system</li>\n</ul>\n<p>And this is injected into every single part of your life and available in every decade. Not just at the end.</p>\n<p>And this is inherently doable. This is the time equivalent of one episode of any Netflix show, 4 times a week. I watched 3 episodes of Pantheon on Monday alone!</p>\n<p>So go do the thing. Incrementally at first. Start off slow and build up a practice that feels right. You won't regret it.</p>"
            ],
            "link": "https://herman.bearblog.dev/exercise/",
            "publishedAt": "2025-08-22",
            "source": "Herman Martinus",
            "summary": "<p>I workout 4 days a week and I love it. It's the foundation of my morning routine, following spending 45 minutes drinking coffee on the couch and watching the sun come up with Emma.</p> <p>I've been doing this for a few years now and while I struggled (as everyone does) in the beginning, I can't imagine not exercising in the morning now. On the rare occasion that I do skip a workout, I feel it missing throughout the day as a lack of vitality and less mental clarity.</p> <p>Let's perform a thought experiment to work out the return on investment of exercise. For this let's first assume that exercise does nothing else but expand your lifespan (not extend; since it's not just adding frail years to the end but instead injects extra years in each stage of life). We can ignore the effects it has on strength, focus, feelings of accomplishment, and mental health for now.</p> <p>It's well understood that a good exercise routine is a mixture of strength, mobility, and cardio; and is performed at a decent intensity for 2-4 days a week for at least 45 minutes. This could be a combination of weight lifting, yoga, running, tennis,",
            "title": "The ROI of exercise"
        },
        {
            "content": [
                "<p><em>[This is one of the finalists in the 2025 review contest, written by an ACX reader who will remain anonymous until after voting is done. I&#8217;ll be posting about one of these a week for several months. When you&#8217;ve read them all, I&#8217;ll ask you to vote for a favorite, so remember which ones you liked]</em></p><p><em>Ollantay</em> is a three-act play written in Quechua, an indigenous language of the South American Andes. It was first performed in Peru around 1775. Since the mid-1800s it&#8217;s been performed more often, and nowadays it&#8217;s pretty easy to find some company in Peru doing it. If nothing else, it&#8217;s popular in Peruvian high schools as a way to get students to connect with Quechua history. It&#8217;s not a particularly long play; a full performance of <em>Ollantay</em> takes around an hour.<a class=\"footnote-anchor\" href=\"https://www.astralcodexten.com/feed#footnote-1\" id=\"footnote-anchor-1\" target=\"_self\">1</a> </p><p>Also, nobody knows where <em>Ollantay </em>was written, when it was written, or who wrote it. And its first documented performance led directly to upwards of a hundred thousand deaths.</p><p><em>Macbeth</em> has killed at most fifty people,<a class=\"footnote-anchor\" href=\"https://www.astralcodexten.com/feed#footnote-2\" id=\"footnote-anchor-2\" target=\"_self\">2</a> and yet it routinely tops listicles of &#8220;deadliest plays&#8221;. I&#8217;m here to propose that <em>Ollantay</em> take its place.</p><h3>The Meta-Story</h3><p>When <em>Ollantay</em> was first performed, Peru was around two hundred years removed from Pizarro&#8217;s apocalyptic conquest. The population was finally starting to recover, so that in 1770 it sat at around 1.2 million people. The vast majority of those 1.2 million people were indigenous,<a class=\"footnote-anchor\" href=\"https://www.astralcodexten.com/feed#footnote-3\" id=\"footnote-anchor-3\" target=\"_self\">3</a> and the vast majority of those 1.2 million people did not have great lives. Peru was oriented almost exclusively towards extracting mineral wealth from the mountains and moving it to Spain.</p><p>Here&#8217;s how they did it.</p><p>Peru was divided into around fifty provinces called <em>corregimientos, </em>each of which was run by a single <em>corregidor. </em>The <em>corregidor</em> held a monopoly on trade with all the Indians in his province, and he was also in charge of collecting taxes. If that sounds like a position which lends itself pretty easily to corruption and abuse, that&#8217;s because it was; the <em>corregidores</em> were uniformly fabulously wealthy and fabulously hated. And in addition to having to pay taxes, all the Indians were obligated to provide free labor to factories and public works projects - public works projects which were used not to improve living conditions of the Indians or provide them with roads between their villages, but to enable moving silver from the mines in the mountains down to the coast for shipping abroad. The Spanish crown expected that around 15% of the population of a district should be providing free labor at any given time. The actual number was usually much higher.</p><p>The sole representative from the Indian villages to the viceroyalty was the <em>curaca</em>, the highest office that an Indian could reach. Like the <em>corregidores</em>, there was one <em>curaca </em>for each province. This guy was responsible for ensuring that the Indians paid their taxes and delivered their free labor, and he was the only one authorized to lodge complaints to the <em>corregidor </em>on behalf of the villages. This was a position designed to make the <em>curaca</em> identify more with the Spanish and less with the Indians. Like the position of <em>corregidor, </em>it was also a way to get very rich.</p><p>Jos&#233; Gabriel Condorcanqui was a <em>curaca</em>, and he was indeed very rich. He became <em>curaca </em>by virtue of his father having been <em>curaca</em>; when he was eighteen his father died and he inherited the title. Jos&#233; Gabriel married well, going from rich to richer. He seems to have been a devout Catholic and he got along well with all the local priests, up to and including the bishop of Cuzco - the ancient capital of the Inca empire and the most important city of inland Peru. Jos&#233; Gabriel was of course friendly with the local <em>corregidor, </em>a very rich Spaniard named Antonio Arriaga.</p><p>That isn&#8217;t to say that Jos&#233; Gabriel was particularly corrupt; on the contrary, he seems to have had a desire to help his people in ways that most other <em>curacas</em> did not. He used those friendships to get actual material concessions - lower taxes, less free labor, public works projects that actually helped the public. All of these didn&#8217;t go nearly far enough, but he at least tried. And he complained a lot. One of the people he complained to was a parish priest named Antonio Valdez.</p><p>Don Antonio Valdez was a parish priest in Tinta, and he fancied himself a Man of Culture. Priests were an important part of the Spanish colonial enterprise, and so Don Valdez was on good terms with both the <em>curaca</em> and <em>corregidor. </em>His name was well-known in Cuzco; he came from a family with long ties to the region and had established himself quite well. Around 1775, he invited Jos&#233; Gabriel and some other honored guests to a performance of a play he had finished putting together. Set in Cuzco in the 1400s, Valdez told his assembled audience that <em>Ollantay</em> was a Castilian version of a Quechua play.<a class=\"footnote-anchor\" href=\"https://www.astralcodexten.com/feed#footnote-4\" id=\"footnote-anchor-4\" target=\"_self\">4</a> </p><p>After seeing the play, something changed in Jos&#233; Gabriel&#8217;s life. It began with his name. He started claiming that he was a direct descendant of T&#250;pac Amaru, the last Incan emperor,<a class=\"footnote-anchor\" href=\"https://www.astralcodexten.com/feed#footnote-5\" id=\"footnote-anchor-5\" target=\"_self\">5</a>  and so he took the name T&#250;pac Amaru II. On his next tours of the local villages he told them all his true name and his true lineage, and let them know that the days of minute changes in tax policy were soon to be over. Things were going to change. He was going to go to Lima to tell the king&#8217;s representatives what was what.</p><p>He traveled to Lima to press his claims. Specifically, he asked the viceroy to recognize his claim to the Marquessate of Oreposa, which was a noble title originally granted to the grandson of Emperor T&#250;pac Amaru. Apparently he was persuasive enough that the government in Lima recognized the claim, and so he returned to Tinta as T&#250;pac Amaru II, Maruqess of Oreposa. If anyone in the viceroy&#8217;s government was nervous about acknowledging the direct descendant of the last Incan emperor, they didn&#8217;t make their feelings known.</p><p>They should have. Because after his <em>Ollantay</em>-inspired transformation, T&#250;pac Amaru II, Marquess of Oreposa and defender of the Quechua, was now on a mission. Back in Tinta, he ratcheted up his agitation against the constant overtaxing, overcharging, and abuse of the free labor system. He was so persuasive in this effort that he (and Valdez the priest/playwright) convinced the bishop of Cuzco to send a delegation back to Madrid, led by T&#250;pac&#8217;s uncle, to argue in front of King Charles III.<a class=\"footnote-anchor\" href=\"https://www.astralcodexten.com/feed#footnote-6\" id=\"footnote-anchor-6\" target=\"_self\">6</a> On the whole, everything was working out quite well for T&#250;pac. He was now recognized as an Inca chief by the government in Cuzco; he knew the bishop well enough that the king would soon hear his grievances. He had every expectation that Charles would agree with him. So why bother waiting for Charles to answer?</p><p>On November 4th, 1780, a parish priest held a feast at his house in honor of King Charles&#8217;s birthday. T&#250;pac was present, along with Antonio Arriaga, the aforementioned <em>corregidor</em>. It is not said whether or not Don Antonio Valdez was at this dinner, when T&#250;pac proclaimed that Arriaga was under arrest for abuse of power. T&#250;pac let it be known that the king had agreed with him that the Quechua should no longer be taxed, but that Arriaga had refused to enact this royal order.<a class=\"footnote-anchor\" href=\"https://www.astralcodexten.com/feed#footnote-7\" id=\"footnote-anchor-7\" target=\"_self\">7</a> The punishment for this insubordination was death. Then he set up a scaffold in the center of town, waited for a suitable crowd to arrive, and publicly executed Arriaga. Thus began the Rebellion of T&#250;pac Amaru II.</p><p>T&#250;pac recruited from the disaffected Quechua quite easily. With an army of locals mostly armed with slings and stones, he easily dispatched the initial force sent from Cuzco to stop him, killing nearly the entire force while losing only fifteen of his own men. News of this victory spread rapidly and he amassed an army 60,000 strong. But rather than pressing his advantage and immediately attacking Cuzco, he just wandered around southern Peru, allowing the army to pillage at will. His advisors - chief among them his wife - couldn&#8217;t understand why he wouldn&#8217;t attack. And in these crucial months, a new Spanish army assembled in Lima.</p><p>After two months of confused looting, T&#250;pac&#8217;s advisors finally convinced him to attack Cuzco. But the city was by now well prepared for him, and his efforts were half-hearted at best. The attempt failed and the army withdrew to the south, where any remaining cohesion fell apart. January and February were spent holed up in the mountains, waiting for the hammer to fall.</p><p>It fell in March. That army from Lima arrived and proved too strong for T&#250;pac&#8217;s stone-throwers. On April 6th the indigenous army was smashed; T&#250;pac was captured alive, along with his wife and two of his sons. The entire rebellion had lasted just five months.</p><p>They were all brought to Cuzco in chains, where the general who led the Spanish army pronounced his opinion on what should happen to T&#250;pac. This sentence was carried out. On May 18th, 1781, T&#250;pac Amaru II was forced to watch as his wife and children were tortured and executed in front of him. Then they cut out his tongue. Then they gathered four horses, tied one to each of his limbs, and sent them running in opposite directions. T&#250;pac Amaru II was pulled apart and he bled out in the plaza of Cuzco.<a class=\"footnote-anchor\" href=\"https://www.astralcodexten.com/feed#footnote-8\" id=\"footnote-anchor-8\" target=\"_self\">8</a> </p><p>The Spanish army then spent the remainder of 1781 pacifying Peru. The indigenous army did not exist anymore, so this was not so much a war as a series of massacres. Anybody who might conceivably offer resistance was killed. And though it wasn&#8217;t officially called this, in practice Peru was decimated; from a 1780 population of 1.2 million, 100,000 people were killed.</p><p>Then the viceroy banned all Quechua theater. He knew what had started this.</p><div><hr /></div><p>Don Antonio Valdez was not killed in the purges. He was well-connected enough, and he was Spanish enough, to avoid the fate of all his Indian friends. <em>Ollantay</em>, though, was to be destroyed.</p><p>Yet Valdez kept a copy of his play hidden in his parish, and then he lived another 35 years. Those 35 years saw the destruction of the Spanish navy at Trafalgar and the overthrow of the Spanish monarchy by Napoleon, events which combined to undermine and ultimately destroy Spanish authority in the Americas. And while Jos&#233; de San Mart&#237;n wouldn&#8217;t liberate Peru until 1820, by the time of Valdez&#8217;s death the interior of Peru had fallen well outside the viceregal jurisdiction. Holding Cuzco was hard enough; the viceroy wasn&#8217;t in any position to prevent a parish priest from making a copy of a decades-old play.</p><p>He made two copies, and gave each one to a priest. One of these priests brought a copy to a convent in Cuzco, where it sat in the library. The other priest kept his copy, and the original remained with Valdez. In 1835, a relative of Valdez&#8217;s wrote an article in a Cuzco periodical where he made reference to the fact that copies of <em>Ollantay</em> yet existed. This article came to the attention of a certain Johann Moritz Rugendas, a German artist who had recently been booted out of Mexico for trying to overthrow the government there and was presently touring South America.</p><div class=\"captioned-image-container\"><figure><a class=\"image-link image2 is-viewable-img\" href=\"https://substackcdn.com/image/fetch/$s_!Ljm8!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa69231a3-c2c6-43ff-b6d3-6c4a25ea2151_1205x882.jpeg\" target=\"_blank\"><div class=\"image2-inset\"><source type=\"image/webp\" /><img alt=\"\" class=\"sizing-normal\" height=\"882\" src=\"https://substackcdn.com/image/fetch/$s_!Ljm8!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa69231a3-c2c6-43ff-b6d3-6c4a25ea2151_1205x882.jpeg\" width=\"1205\" /><div class=\"image-link-expand\"><div class=\"pencraft pc-display-flex pc-gap-8 pc-reset\"><div class=\"pencraft pc-reset icon-container restack-image\"><svg class=\"lucide lucide-refresh-cw\" fill=\"none\" height=\"20\" stroke=\"currentColor\" stroke-linecap=\"round\" stroke-linejoin=\"round\" stroke-width=\"2\" viewBox=\"0 0 24 24\" width=\"20\" xmlns=\"http://www.w3.org/2000/svg\"><path d=\"M3 12a9 9 0 0 1 9-9 9.75 9.75 0 0 1 6.74 2.74L21 8\"></path><path d=\"M21 3v5h-5\"></path><path d=\"M21 12a9 9 0 0 1-9 9 9.75 9.75 0 0 1-6.74-2.74L3 16\"></path><path d=\"M8 16H3v5\"></path></svg></div><div class=\"pencraft pc-reset icon-container view-image\"><svg class=\"lucide lucide-maximize2\" fill=\"none\" height=\"20\" stroke=\"currentColor\" stroke-linecap=\"round\" stroke-linejoin=\"round\" stroke-width=\"2\" viewBox=\"0 0 24 24\" width=\"20\" xmlns=\"http://www.w3.org/2000/svg\"><polyline points=\"15 3 21 3 21 9\"></polyline><polyline points=\"9 21 3 21 3 15\"></polyline><line x1=\"21\" x2=\"14\" y1=\"3\" y2=\"10\"></line><line x1=\"3\" x2=\"10\" y1=\"21\" y2=\"14\"></line></svg></div></div></div></div></a><figcaption class=\"image-caption\">El Marcado de la Independencia, by Rugendas in 1843. He did some nice paintings of Lima while on tour.</figcaption></figure></div><p>Rugendas asked the monks in the convent to make a copy of <em>Ollantay</em> for him. This copy was rather damaged, having sat in a damp convent for eighty years, but the monks obliged and did the best they could. Rugendas brought this copy back to Germany when he returned to Europe in 1846, where it became a curiosity as an example of the Quechua language. He also brought back word that an undamaged copy existed in some priest&#8217;s rectory.</p><p>An Englishman with an interest in Inca history decided that he was going to find this undamaged version and write an English translation, and so in 1853 Sir Clements Markham<a class=\"footnote-anchor\" href=\"https://www.astralcodexten.com/feed#footnote-9\" id=\"footnote-anchor-9\" target=\"_self\">9</a> added &#8220;find and translate <em>Ollantay</em>&#8221; to his agenda for an upcoming expedition to the Andes. He succeeded, finding that other priest who held that other copy and meticulously copying every word of <em>Ollantay</em> in both Quechua and Spanish, then translating that to English.</p><p>And so we can read, watch, and perform <em>Ollantay</em>, the play that launched a thousand ships.</p><h3>The Story</h3><p><em>Ollantay</em> is a love story.<a class=\"footnote-anchor\" href=\"https://www.astralcodexten.com/feed#footnote-10\" id=\"footnote-anchor-10\" target=\"_self\">10</a> The lovers are the titular Ollantay and Cusi Coyllur &#209;usta - he the chief of the Anti people (to be clear, <em>Anti</em> is the Quechua name of the clan; they&#8217;re not anti-people) and she the daughter of the Inca emperor. As the play begins, they have already been clandestine lovers for quite some time and the princess is secretly pregnant by Ollantay. But Ollantay, being simply a regional warlord, is not a suitable match for a princess. The play begins with Ollantay pining to his page - who fills the only role of &#8220;comic relief&#8221;<a class=\"footnote-anchor\" href=\"https://www.astralcodexten.com/feed#footnote-11\" id=\"footnote-anchor-11\" target=\"_self\">11</a> - that he must marry Coyllur:</p><blockquote><p>Have I not already said<br />That e&#8217;en if death&#8217;s fell scythe was here,<br />If mountains should oppose my path<br />Like two fierce foes who block the way,<br />Yet will I fight all these combined<br />And risk all else to gain my end,<br />And whether it be life or death<br />I&#8217;ll cast myself at Coyllur&#8217;s feet.</p></blockquote><p>The two run into the high priest, whose introductory soliloquy is a paean to the blood of llamas:</p><blockquote><p>O giver of all warmth and light<br />O Sun! I fall and worship thee.<br />For thee the victims are prepared,<br />A thousand llamas and their lambs<br />Are ready for thy festal day.<br />The sacred fire&#8217;ll lap their blood,<br />In thy dread presence, mighty one,<br />After long fast thy victims fall.</p></blockquote><p>The priest and Ollantay then discuss how Ollantay can definitely not marry the princess and it&#8217;s a really bad idea for him to try. Ollantay reiterates his desires, to which the priest can only give one final warning:</p><blockquote><p>Put a seed into the ground,<br />It multiplies a hundredfold;<br />The more thy crime shall grow and swell,<br />The greater far thy sudden fall.</p></blockquote><p>Ollantay then approaches the emperor and asks for the hand of his daughter with a long soliloquy. The emperor waves him off in four lines:</p><blockquote><p>Ollantay, thou dost now presume.<br />Thou art a subject, nothing more.<br />Remember, bold one, who thou art,<br />And learn to keep thy proper place.</p></blockquote><p>And so in the next scene, Ollantay swears vengeance:</p><blockquote><p>When flames rise to the heavens.<br />Cuzco shall sleep on a bloody couch,<br />The King shall perish in its fall;<br />Then shall my insulter see<br />How numerous are my followers.<br />When thou, proud King, art at my feet,<br />We then shall see if thou wilt say,<br />&#8216;Thou art too base for Coyllur&#8217;s hand.&#8217;</p></blockquote><p>He returns home and gathers an army. The emperor then dispatches his own army to go hunt down Ollantay, but Ollantay&#8217;s men successfully ambush them in a mountain pass and destroy the Inca army without losing a man:</p><blockquote><p>A rain of stones both great and small<br />Down on the crowd of warriors crashed,<br />On every side destruction flashed,<br />Thy heart the slaughter did appall.<br /><br />Like a strong flood the blood did flow,<br />Inundating the ravine;<br />So sad a sight thou ne&#8217;er hast seen&#8212;<br />No man survived to strike a blow.</p></blockquote><p>Ollantay doesn&#8217;t press his advantage, though, and is content instead to build up his base of power in his home province. This proves to be a mistake. General Rumi-&#209;aui, the general who lost the battle in the mountain pass, comes up with a different, better plan. He begs the emperor for another chance, and the emperor grants it.</p><p>Rumi-&#209;aui shows up to Ollantay, beaten and bloody, and spins a tale of betrayal by the emperor. Ollantay takes the bait and invites him into his capital, then tells him that they will shut the gates and party for three days straight:</p><blockquote><p>It will be so. For three whole nights<br />We drink and feast, to praise the Sun,<br />The better to cast all care aside<br />We shall be shut in Tampu fort.</p></blockquote><p>Rumi-&#209;aui waits for Ollantay&#8217;s whole army to be passed-out drunk. Then he opens the gates and invites his army to come in and kill or capture the lot of them. Ollantay is brought back to Cuzco in chains. Things are looking bad for him.</p><p>I&#8217;ll let the play take it from here (T&#250;pac Yupanqui is the emperor, and tocarpus are execution stakes):</p><blockquote><p><strong>T&#218;PAC YUPANQUI:</strong><br />Know that tocarpus are prepared.<br />Remove those traitors from my sight,<br />Let them all perish, and at once.</p><p><strong>RUMI-&#209;AUI:</strong><br />Take these three men without delay<br />To the dreaded execution stakes;<br />Secure them with unyielding ropes,<br />And hurl them from the lofty rocks.</p><p><strong>T&#218;PAC YUPANQUI:</strong><br />Stop! Cast off their bonds.</p><p><em>(The guards unbind them. They all kneel.)</em></p><p><em>(To Ollantay, kneeling).</em><br />Rise from thy knees; come to my side.</p><p><em>(Rises.)</em></p><p>Now thou hast seen death very near,<br />You that have shown ingratitude,<br />Learn how mercy flows from my heart;<br />I will raise thee higher than before.<br />Thou wert Chief of Anti-suyu,<br />Now see how far my love will go;<br />I make thee Chief in permanence.<br />Receive this plume as general,<br />This arrow emblem of command.</p></blockquote><p>That&#8217;s right! Ollantay swore eternal vengeance on the emperor, seceded, set himself up as a king, destroyed an entire Inca army, and is rewarded for his betrayal by being made viceroy. Rumi-&#209;aui has no problem with this, saying:</p><blockquote><p>Prince Ollantay! Incap Ranti!<br />Thy promotion gives me joy.</p></blockquote><p>As the play concludes, Ollantay mentions that he would still very much like to marry Coyllur.<a class=\"footnote-anchor\" href=\"https://www.astralcodexten.com/feed#footnote-12\" id=\"footnote-anchor-12\" target=\"_self\">12</a> The emperor of course thinks this is a marvelous idea, and so the two are reunited for the first time in ten years and, oddly enough, the first time in the play. <em>Ollantay</em> is the kind of love story where the lovers only actually speak to each other once, at the very end.</p><p>And so the play concludes with these words from the emperor:</p><blockquote><p>Thy wife is now in thy arms;<br />All sorrow now should disappear,<br />Joy, new born, shall take its place.</p></blockquote><p>Which is the Inca version of &#8220;and they all lived happily ever after&#8221;.</p><div><hr /></div><p><em>Ollantay</em> is not a particularly good play. There&#8217;s a reason it has only entered the repertoire of Peruvian high school drama. The whole premise that Ollantay is trying to get back to his lover is dropped in Act II and only resurfaces at the very end of the play, almost as an afterthought. None of the characters evolve; Ollantay is the exact same person at the end of the play that he was at the start. And the resolution is comically abrupt. All the foreshadowing, and there is foreshadowing, implies that both Ollantay and Coyllur will end up dead, but instead they end up married and with a ten-year-old daughter. Turns out the priest was wrong! The seed put in the ground that multiplies a hundred fold won&#8217;t precipitate a sudden fall after all!</p><p>Thematically, <em>Ollantay</em> is not thematic. Ollantay acts virtuously and is rewarded for it. Rumi-&#209;aui acts wickedly and is rewarded for it. Coyllur acts&#8230;well she doesn&#8217;t really act, she just bemoans her fate in Act I and then spends the rest of the play literally hidden behind a stone wall.</p><p>And it&#8217;s not like <em>Ollantay</em> tells us anything about Incan society that would make it valuable from an anthropological perspective. Valdez may have been adapting a traditional Quechua play, but his own Spanish and Catholic background definitely seeped in. As we&#8217;ll see, there&#8217;s an ongoing debate as to how much of the play is Quechua and how much is Valdez.<a class=\"footnote-anchor\" href=\"https://www.astralcodexten.com/feed#footnote-13\" id=\"footnote-anchor-13\" target=\"_self\">13</a> </p><h3>The Same Story</h3><p>But even if <em>Ollantay</em> is not that valuable from an artistic perspective or an anthropological perspective, it is valuable from a historic perspective.</p><p>You may have noticed some similarities between the plot of <em>Ollantay</em> and the story of T&#250;pac Amaru II. By which I mean that it&#8217;s beat-for-beat the same story. A powerful local chief despairs of his inability to &lt;marry a princess / lighten the free labor burden&gt;. After consulting with a local priest, he launches an armed rebellion against the imperial authorities in Cuzco from his home base in the mountains, and quickly raises a large army. He easily defeats the initial army sent to capture him, but instead of marching on Cuzco he focuses on building up his own local power base. This proves to be an error, and he loses control of his own army, leading to military defeat and his own capture. He is taken to Cuzco in chains and &lt;forgiven and made viceroy / brutally tortured and executed&gt;.</p><p>Pretty much all of the questions surrounding the T&#250;pac Amaru rebellion vanish if you assume that T&#250;pac was not fighting a rebellion but following a script. Why did T&#250;pac not immediately attack Cuzco? Because Ollantay didn&#8217;t. Why was he seemingly okay with his army losing its discipline? Because Ollantay was. Why did he put his army in a position to lose? Why was he okay with being taken alive, knowing how the Spanish dealt with rebels? Because T&#250;pac was following the path set by Ollantay:</p><ul><li><p>First, declare yourself in rebellion.</p></li><li><p>Second, win a quick and easy victory with the help of stone-throwers. </p></li><li><p>Third, amass a giant army.</p></li><li><p>Fourth, hunker down at your base in the mountains and wait to be defeated by the new army sent from the capital. </p></li><li><p>Fifth, be brought to Cuzco and given authority over all of inner Peru. </p></li><li><p>Finally, use your new authority to improve the lives of all indigenous people and be remembered forever as a great ruler.</p></li></ul><p>He made it all the way to step five before things went awry.</p><p><em>Ollantay</em> was a cognitohazard designed exclusively for Jos&#233; Gabriel Condorcanqui. It led him to embrace his destiny as the liberator of his people, and it led him to believe that he would be vindicated in the end rather than tortured and executed. It led him to make crucial military mistakes. It led one hundred thousand people to their deaths.</p><h3>The Author</h3><p>The first time I read <em>Ollantay</em>, I was sure that there was some mistake; that Antonio Valdez had written the play after the rebellion as a way to try and redeem T&#250;pac. A way to recast his story as a romantic tale of heroism and end up with him on top. Maybe he felt bad for his role in the decimation of Peru, and writing (or re-writing) <em>Ollantay</em> was his way of making up for it. But Valdez, and his family, and those priests with those copies, and the Spanish military officers burning down ancient villages, they all said that no this really was the <em>Ollantay </em>that drove Jos&#233; Gabriel to become T&#250;pac Amaru.</p><p>So how much should we blame Don Antonio Valdez? Here are the theories I&#8217;ve seen of where Valdez&#8217;s version came from. All of these theories have their vigorous defenders.</p><ol><li><p>Valdez came up with the whole play himself.</p></li><li><p>Valdez took an existing Quechua oral tradition and set it in the form of a Castilian play.</p></li><li><p>Valdez took an existing Quechua drama that had been acted for centuries and wrote it down.</p></li><li><p>Valdez took an existing Quechua drama that had been acted for centuries, changed the ending so as to fit with his more romantic notions, and wrote it down.</p></li><li><p>Valdez found a play that had already been written down in Castilian verse; all he had to do was hire actors to stage it.</p></li></ol><p>It must be reiterated that scholarship on this point is incredibly varied. To quote one article that tries (and fails) to come to any sort of conclusion, &#8220;Certain scholars have said that the <em>Ollantay</em> tradition, which exists yet today in Peru, is the source of the play; and others say that the play is the source of the tradition.&#8221; Which is to say that yes there are folk stories around Ollantay, and we have no idea whether they spawned the play or the play spawned them.</p><p>Some Inca historians maintain that <em>Ollantay</em> must be a 16th-century original. Others put it in the 15th, others claim that Valdez just made it all up himself. An Argentinian claimed that his father was a friend of Valdez and that Valdez didn&#8217;t know anything about writing plays. A Peruvian<a class=\"footnote-anchor\" href=\"https://www.astralcodexten.com/feed#footnote-14\" id=\"footnote-anchor-14\" target=\"_self\">14</a>  countered that Valdez was the greatest linguist, philosopher, and playwright in all of 18th-century South America. Then there&#8217;s the racial component - white Peruvians are more inclined to say that Valdez wrote the whole thing himself, and indigenous Peruvians are more inclined to say that he simply adapted it from the Quechua.</p><p>I am left without an opinion as to which sections of <em>Ollantay</em> are a Valdez original. But I&#8217;m also convinced that Valdez does not play an innocent role here. In <em>Ollantay</em>, the priest figures heavily in the opening and closing of the play. He begins with a statement of his powers:</p><blockquote><p>&#8217;Tis well. Now listen, warlike. Chief:<br />My science has enabled me,<br />To learn and see all hidden things<br />Unknown to other mortal men.<br />My power will enable me<br />To make of thee a greater prince.</p></blockquote><p>He goes on to warn Ollantay of the path he will go down if he insists on making trouble with the emperor. It&#8217;s a long back-and-forth, but the priest finishes with this flourish:</p><blockquote><p>How oft we mortals heedless drink,<br />A certain death from golden cup<br />Recall to mind how ills befall,<br />And that a stubborn heart&#8217;s the cause.</p></blockquote><p>And having failed to convince Ollantay, he departs with the lines:</p><blockquote><p>Be it life, be it death that you find,<br />I will never forget thee, my son.</p></blockquote><p>And he means it. He stays with the emperor and advises mercy towards Ollantay at every step.</p><p>Finally, in the climactic scene when the defeated Ollantay is brought before the emperor, we have this moment:</p><blockquote><p><strong>T&#218;PAC YUPANQUI:</strong><br /><em>(to the Uillac Uma).</em><br />Pronounce their sentence, great High Priest.</p><p><strong>UILLAC UMA:</strong><br />The light that fills me from the Sun<br />Brings mercy and pardon to my heart.</p></blockquote><p>It is this sentiment, we are meant to infer, that causes the emperor to show mercy to Ollantay.</p><p>Don Antonio Valdez did none of this. He did not attempt to talk down T&#250;pac from his course. He did not go to Cuzco to plead clemency. And when T&#250;pac was captured and brought to Cuzco, when he had his tongue torn out and his limbs tied to horses and his body ripped apart in the plaza, the priest was nowhere to be found. Instead, he was hiding in his parish, hoping nobody would come around asking about a play.</p><p>Antonio Valdez lived for another thirty-five years after the brutal suppression of the rebellion and the public execution of his friend. In all the chaos that would subsume Peru as South America broke away from Spain and fell into near-constant civil war, Valdez never made another public appearance. He advised neither San Mart&#237;n nor Bol&#237;var.</p><p>But he did make a copy of his play.</p><h3>Trying to Make Sense of It All</h3><p>I have come to a conclusion. It is my firm belief that <em>Ollantay</em> was not created by Don Antonio Valdez. Whether or not Valdez adopted an existing Quechua story is irrelevant; we cannot put the deaths of a hundred thousand people onto the shoulders of a single priest. <em>Ollantay</em> must remain without an author. That is good and right.</p><p>Because the best explanation I can offer for the T&#250;pac Amaru rebellion is that Jos&#233; Gabriel Condorcanqui was taken in by something beyond his control. You and I can read or watch <em>Ollantay</em> and be mildly amused for an hour without wanting to go overthrow the Peruvian government. But that&#8217;s not because we&#8217;re more clever than Jos&#233;, or more rational than Jos&#233;, or more media-savvy than Jos&#233;. It&#8217;s because <em>Ollantay</em> wasn&#8217;t <em>for</em> us. It was <em>for</em> him.</p><p>But maybe something else is <em>for</em> you and <em>for</em> me. Maybe there&#8217;s a book out there, or a painting, or a song or a play that is just waiting for you to activate your destiny. Maybe every artwork out there exists for a specific person, and every once in a while the art finds the person and they change the world. Maybe we just have to hope that ours never finds us.</p><p>After all, <em>Ollantay </em>is far from the only example of this. Mark David Chapman read <em>Catcher in the Rye</em> and knew he had to kill Lennon. John Hinckley Jr. saw <em>Taxi Driver</em> and knew it was talking about him; knew that Jodie Foster needed somebody to follow the script and shoot the president.</p><p>And in 1992, <a href=\"https://en.wikipedia.org/wiki/Ronald_Ray_Howard\">Ronald Ray Howard</a> was pulled over outside of Houston while listening to &#8220;Soulja&#8217;s Story&#8221;, a song with these lyrics:</p><blockquote><p>Only fifteen and got problems<br />Cops on my tail, so I bail 'til I dodge 'em<br />They finally pull me over and I laugh<br />\"Remember Rodney King?\" And I blast on his punk ass</p></blockquote><p>Howard followed the script.</p><p>And when he was arrested and when he was charged with murder of a police officer and when he faced the death penalty and thirteen years later when he sat in the chair and watched the lethal injection cocktail enter into the IV drip, each time he swore that he didn&#8217;t want to kill the officer, but that something came over him. He couldn&#8217;t control it. It was the song.</p><p>That song was written, of course, by Tupac Amaru.</p><h3></h3><div class=\"footnote\"><a class=\"footnote-number\" contenteditable=\"false\" href=\"https://www.astralcodexten.com/feed#footnote-anchor-1\" id=\"footnote-1\" target=\"_self\">1</a><div class=\"footnote-content\"><p>I&#8217;m relegating a video of an actual performance of <em>Ollantay</em> to this footnote, mainly because I&#8217;m reviewing the play in general and not a particular performance. There are quite a few on YouTube, most of pretty similar quality to this one. Performed in Quechua with Spanish subtitles, but English viewers will pretty easily figure out what&#8217;s going on. Nothing about this play is subtle.</p><div class=\"youtube-wrap\" id=\"youtube2-Ra7CIzh7gXc\"><div class=\"youtube-inner\"></div></div><p></p></div></div><div class=\"footnote\"><a class=\"footnote-number\" contenteditable=\"false\" href=\"https://www.astralcodexten.com/feed#footnote-anchor-2\" id=\"footnote-2\" target=\"_self\">2</a><div class=\"footnote-content\"><p>22 dead in one particular riot, and a handful of actors&#8217; deaths over the four hundred years since it was written.</p></div></div><div class=\"footnote\"><a class=\"footnote-number\" contenteditable=\"false\" href=\"https://www.astralcodexten.com/feed#footnote-anchor-3\" id=\"footnote-3\" target=\"_self\">3</a><div class=\"footnote-content\"><p>Throughout this review I&#8217;ll be alternating between &#8220;indigenous&#8221;, &#8220;Indian&#8221;, and &#8220;Quechua&#8221;, but for our purposes they all mean the same thing.</p></div></div><div class=\"footnote\"><a class=\"footnote-number\" contenteditable=\"false\" href=\"https://www.astralcodexten.com/feed#footnote-anchor-4\" id=\"footnote-4\" target=\"_self\">4</a><div class=\"footnote-content\"><p>Meaning that it was refitted to be in octosyllabic verse, which was a popular style for Spanish theater at the time.</p></div></div><div class=\"footnote\"><a class=\"footnote-number\" contenteditable=\"false\" href=\"https://www.astralcodexten.com/feed#footnote-anchor-5\" id=\"footnote-5\" target=\"_self\">5</a><div class=\"footnote-content\"><p>This is actually not as implausible as it might seem. T&#250;pac Amaru I had a lot of children, and Condorcanqui was wealthy enough that his lineage could be traced back to royalty. And we&#8217;re not talking like 2,000 years here - T&#250;pac Amaru I was executed in 1572. This would be akin to a rich Virginian claiming to be a direct descendant of Thomas Jefferson.</p></div></div><div class=\"footnote\"><a class=\"footnote-number\" contenteditable=\"false\" href=\"https://www.astralcodexten.com/feed#footnote-anchor-6\" id=\"footnote-6\" target=\"_self\">6</a><div class=\"footnote-content\"><p>Said uncle was poisoned to death in Madrid. This story has no happy endings.</p></div></div><div class=\"footnote\"><a class=\"footnote-number\" contenteditable=\"false\" href=\"https://www.astralcodexten.com/feed#footnote-anchor-7\" id=\"footnote-7\" target=\"_self\">7</a><div class=\"footnote-content\"><p>This was very much Not True. T&#250;pac had not heard anything from Madrid.</p></div></div><div class=\"footnote\"><a class=\"footnote-number\" contenteditable=\"false\" href=\"https://www.astralcodexten.com/feed#footnote-anchor-8\" id=\"footnote-8\" target=\"_self\">8</a><div class=\"footnote-content\"><p>Later sources claimed that T&#250;pac was too strong for the horses and they failed to execute him this way, ending up cutting his head off instead, but that seems to be a later fabrication meant to impart some divinity onto him. Either way, not a good way to go.</p></div></div><div class=\"footnote\"><a class=\"footnote-number\" contenteditable=\"false\" href=\"https://www.astralcodexten.com/feed#footnote-anchor-9\" id=\"footnote-9\" target=\"_self\">9</a><div class=\"footnote-content\"><p>More well-known for bringing quinine to India and organizing the first British expeditions to reach the South Pole. But he also dabbled as a translator.</p></div></div><div class=\"footnote\"><a class=\"footnote-number\" contenteditable=\"false\" href=\"https://www.astralcodexten.com/feed#footnote-anchor-10\" id=\"footnote-10\" target=\"_self\">10</a><div class=\"footnote-content\"><p>Here I&#8217;ll be quoting from Markham&#8217;s 1863 English translation. Markham tried to preserve the Castilian style of Valdez, and because he did not speak Quechua his version is an English translation of a Spanish translation. But it&#8217;s the best we&#8217;ve got.</p></div></div><div class=\"footnote\"><a class=\"footnote-number\" contenteditable=\"false\" href=\"https://www.astralcodexten.com/feed#footnote-anchor-11\" id=\"footnote-11\" target=\"_self\">11</a><div class=\"footnote-content\"><p>For example:<br /><br />PIQUI CHAQUI.<br />(jumping up).<br />I was asleep, my master,<br />And dreaming of evil things.</p><p>OLLANTAY.<br />Of what?</p><p>PIQUI CHAQUI.<br />Of a fox with a rope round its neck.</p><p>OLLANTAY.<br />Sure enough, thou art the fox.</p><p>PIQUI CHAQUI.<br />It is true that my nose is growing finer,<br />And my ears a good deal longer.</p></div></div><div class=\"footnote\"><a class=\"footnote-number\" contenteditable=\"false\" href=\"https://www.astralcodexten.com/feed#footnote-anchor-12\" id=\"footnote-12\" target=\"_self\">12</a><div class=\"footnote-content\"><p>She does have her own subplot, which I&#8217;ll tell in this footnote. The princess is confined to a prison within a religious order. She gives birth off-screen, and her daughter, who she names Yma Sumac, is raised in the convent to be a consecrated virgin. We then jump forward ten years, as Ollantay builds his kingdom in the mountains. Yma does not know that she is secretly Inca royalty, and she despairs at the life set before her. She also is very interested in learning why there is a crying woman behind the walls at the convent. This plot is resolved when Yma&#8217;s friend tells her everything and brings her to her mother. Yma then goes to tell the emperor, completely unaware of the events surrounding Ollantay. The emperor sends Yma to fetch the princess and bring her back to Ollantay. At no point do Yma or Ollantay acknowledge that she is his daughter.<br /><br />Wait, you say, wouldn&#8217;t it make more sense if Ollantay went to go rescue his wife and daughter after being made viceroy? If he had nothing to do with Yma&#8217;s discovery of her true parentage, then why did Yma have to wait ten years before peeking behind a wall to find her mother? These are all good questions! The answer is that <em>Ollantay</em> has anticipated the Bechdel test by two hundred years and so is absolutely determined that any time there are two women in a scene together they must 1) have a conservation with each other that 2) is not about a man.</p></div></div><div class=\"footnote\"><a class=\"footnote-number\" contenteditable=\"false\" href=\"https://www.astralcodexten.com/feed#footnote-anchor-13\" id=\"footnote-13\" target=\"_self\">13</a><div class=\"footnote-content\"><p>There&#8217;s also a good argument to be made that he changed the original ending, turning it from a tragedy to a triumph. That would explain the presence of this song in Act I, sung by a passing child to Coyllur:<br /><br />&#8220;She wanders forth from stone to stone,<br />She seeks her mate in vain;<br />&#8216;My love! my love!&#8217; she makes her moan,<br />She falls, she dies in pain.&#8221;</p></div></div><div class=\"footnote\"><a class=\"footnote-number\" contenteditable=\"false\" href=\"https://www.astralcodexten.com/feed#footnote-anchor-14\" id=\"footnote-14\" target=\"_self\">14</a><div class=\"footnote-content\"><p>Not just a Peruvian, but the Peruvian Minister of Foreign of Affairs.</p><p></p></div></div>"
            ],
            "link": "https://www.astralcodexten.com/p/your-review-ollantay",
            "publishedAt": "2025-08-22",
            "source": "SlateStarCodex",
            "summary": "<p><em>[This is one of the finalists in the 2025 review contest, written by an ACX reader who will remain anonymous until after voting is done. I&#8217;ll be posting about one of these a week for several months. When you&#8217;ve read them all, I&#8217;ll ask you to vote for a favorite, so remember which ones you liked]</em></p><p><em>Ollantay</em> is a three-act play written in Quechua, an indigenous language of the South American Andes. It was first performed in Peru around 1775. Since the mid-1800s it&#8217;s been performed more often, and nowadays it&#8217;s pretty easy to find some company in Peru doing it. If nothing else, it&#8217;s popular in Peruvian high schools as a way to get students to connect with Quechua history. It&#8217;s not a particularly long play; a full performance of <em>Ollantay</em> takes around an hour.<a class=\"footnote-anchor\" href=\"https://www.astralcodexten.com/feed#footnote-1\" id=\"footnote-anchor-1\" target=\"_self\">1</a> </p><p>Also, nobody knows where <em>Ollantay </em>was written, when it was written, or who wrote it. And its first documented performance led directly to upwards of a hundred thousand deaths.</p><p><em>Macbeth</em> has killed at most fifty people,<a class=\"footnote-anchor\" href=\"https://www.astralcodexten.com/feed#footnote-2\" id=\"footnote-anchor-2\" target=\"_self\">2</a> and yet it routinely tops listicles of &#8220;deadliest plays&#8221;. I&#8217;m here to propose that <em>Ollantay</em> take its place.</p><h3>The Meta-Story</h3><p>When <em>Ollantay</em> was first",
            "title": "Your Review: Ollantay"
        },
        {
            "content": [
                "What if <a href=\"https://x.com/deepsseek/status/1957886077047566613\">DeepSeek released a model claiming 66 on SWE</a> and almost no one tried using it? Would it be any good? Would you be able to tell? Or would we get the shortest post of the year?\n\n\n<h4 class=\"wp-block-heading\">Why We Haven\u2019t Seen v4 or r2</h4>\n<a href=\"https://x.com/StefanFSchubert/status/1955893320024047628\">Why are we settling for v3.1 and have yet to see</a> <a href=\"https://www.ft.com/content/eb984646-6320-4bfe-a78d-a1da2274b092\">DeepSeek release v4 or r2 yet</a>?\n<blockquote>Eleanor Olcott and Zijing Wu: Chinese artificial intelligence company DeepSeek delayed the release of its new model after failing to train it using Huawei\u2019s chips, highlighting the limits of Beijing\u2019s push to replace US technology.\n\nDeepSeek was encouraged by authorities to adopt Huawei\u2019s Ascend processor rather than use Nvidia\u2019s systems after releasing its R1 model in January, according to three people familiar with the matter.\n<div> <span id=\"more-24669\"></span> </div>\nBut the Chinese start-up encountered persistent technical issues during its R2 training process using Ascend chips, prompting it to use Nvidia chips for training and Huawei\u2019s for inference, said the people.\n\nThe issues were the main reason the model\u2019s launch was <a href=\"https://www.ft.com/content/fb5c11bb-1d4b-465f-8283-451a19a3d425\">delayed</a> from May, said a person with knowledge of the situation, causing it to lose ground to rivals.</blockquote>\nThe real world so often involves people acting so much stupider than you could write into fiction.\n\nAmerica tried to sell China H20s and China decided they didn\u2019t want them and now Nvidia is halting related orders with suppliers.\n\nDeepSeek says that the main restriction on their development is lack of compute, and the PRC responds not by helping them get better chips but by advising them to not use the chips that they have, greatly slowing things down at least for a while.\n\n\n<h4 class=\"wp-block-heading\">Introducing DeepSeek v3.1</h4>\nIn any case, <a href=\"https://x.com/deepsseek/status/1957886077047566613\">DeepSeek v3.1 exists now</a>, and remarkably few people care?\n<blockquote><a href=\"https://x.com/deepseek_ai/status/1958417068568481854\">DeepSeek</a>: Introducing DeepSeek-V3.1: our first step toward the agent era! <img alt=\"\ud83d\ude80\" class=\"wp-smiley\" src=\"https://s0.wp.com/wp-content/mu-plugins/wpcom-smileys/twemoji/2/72x72/1f680.png\" style=\"height: 1em;\" />\n\n<img alt=\"\ud83e\udde0\" class=\"wp-smiley\" src=\"https://s0.wp.com/wp-content/mu-plugins/wpcom-smileys/twemoji/2/72x72/1f9e0.png\" style=\"height: 1em;\" /> Hybrid inference: Think &amp; Non-Think \u2014 one model, two modes\n\n<img alt=\"\u26a1\" class=\"wp-smiley\" src=\"https://s0.wp.com/wp-content/mu-plugins/wpcom-smileys/twemoji/2/72x72/26a1.png\" style=\"height: 1em;\" /> Faster thinking: DeepSeek-V3.1-Think reaches answers in less time vs. DeepSeek-R1-0528\n\n<img alt=\"\ud83d\udee0\" class=\"wp-smiley\" src=\"https://s0.wp.com/wp-content/mu-plugins/wpcom-smileys/twemoji/2/72x72/1f6e0.png\" style=\"height: 1em;\" /> Stronger agent skills: Post-training boosts tool use and multi-step agent tasks\n\n<a href=\"https://t.co/n8tODcbEz2\">Try it now \u2014 toggle Think/Non-Think via the &#8220;DeepThink&#8221; button</a>.\n\nAPI Update <img alt=\"\u2699\" class=\"wp-smiley\" src=\"https://s0.wp.com/wp-content/mu-plugins/wpcom-smileys/twemoji/2/72x72/2699.png\" style=\"height: 1em;\" />\n\n<img alt=\"\ud83d\udd39\" class=\"wp-smiley\" src=\"https://s0.wp.com/wp-content/mu-plugins/wpcom-smileys/twemoji/2/72x72/1f539.png\" style=\"height: 1em;\" /> deepseek-chat \u2192 non-thinking mode\n\n<img alt=\"\ud83d\udd39\" class=\"wp-smiley\" src=\"https://s0.wp.com/wp-content/mu-plugins/wpcom-smileys/twemoji/2/72x72/1f539.png\" style=\"height: 1em;\" /> deepseek-reasoner \u2192 thinking mode\n\n<img alt=\"\ud83e\uddf5\" class=\"wp-smiley\" src=\"https://s0.wp.com/wp-content/mu-plugins/wpcom-smileys/twemoji/2/72x72/1f9f5.png\" style=\"height: 1em;\" /> 128K context for both\n\n<img alt=\"\ud83d\udd0c\" class=\"wp-smiley\" src=\"https://s0.wp.com/wp-content/mu-plugins/wpcom-smileys/twemoji/2/72x72/1f50c.png\" style=\"height: 1em;\" /> <a href=\"https://api-docs.deepseek.com/guides/anthropic_api\">Anthropic API format supported</a>.\n\n<img alt=\"\u2705\" class=\"wp-smiley\" src=\"https://s0.wp.com/wp-content/mu-plugins/wpcom-smileys/twemoji/2/72x72/2705.png\" style=\"height: 1em;\" /> <a href=\"https://api-docs.deepseek.com/guides/function_calling\">Strict Function Calling supported in Beta API</a>.\n\n<img alt=\"\ud83d\ude80\" class=\"wp-smiley\" src=\"https://s0.wp.com/wp-content/mu-plugins/wpcom-smileys/twemoji/2/72x72/1f680.png\" style=\"height: 1em;\" /> More API resources, smoother API experience\n\nTools &amp; Agents Upgrades <img alt=\"\ud83e\uddf0\" class=\"wp-smiley\" src=\"https://s0.wp.com/wp-content/mu-plugins/wpcom-smileys/twemoji/2/72x72/1f9f0.png\" style=\"height: 1em;\" />\n\n<img alt=\"\ud83d\udcc8\" class=\"wp-smiley\" src=\"https://s0.wp.com/wp-content/mu-plugins/wpcom-smileys/twemoji/2/72x72/1f4c8.png\" style=\"height: 1em;\" /> Better results on SWE / Terminal-Bench\n\n<img alt=\"\ud83d\udd0d\" class=\"wp-smiley\" src=\"https://s0.wp.com/wp-content/mu-plugins/wpcom-smileys/twemoji/2/72x72/1f50d.png\" style=\"height: 1em;\" /> Stronger multi-step reasoning for complex search tasks\n\n<img alt=\"\u26a1\" class=\"wp-smiley\" src=\"https://s0.wp.com/wp-content/mu-plugins/wpcom-smileys/twemoji/2/72x72/26a1.png\" style=\"height: 1em;\" /> Big gains in thinking efficiency\n<div>\n<figure>\n<div>\n<figure class=\"wp-block-image\"><img alt=\"\" src=\"https://substackcdn.com/image/fetch/$s_!frCJ!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F414d4bf7-4d32-4b9b-9f0d-f9169ed31754_2100x962.jpeg\" /></figure>\n\n<div></div>\n</div></figure>\n</div>\n<div>\n<figure>\n<div>\n<figure class=\"wp-block-image\"><img alt=\"\" src=\"https://substackcdn.com/image/fetch/$s_!z2dO!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F42614cbb-ed7c-40f3-9f3b-9827c4b85a36_1886x1794.jpeg\" /></figure>\n\n<div></div>\n</div></figure>\n</div>\n<img alt=\"\ud83d\udd39\" class=\"wp-smiley\" src=\"https://s0.wp.com/wp-content/mu-plugins/wpcom-smileys/twemoji/2/72x72/1f539.png\" style=\"height: 1em;\" /> V3.1 Base: 840B tokens continued pretraining for long context extension on top of V3\n\n<img alt=\"\ud83d\udd39\" class=\"wp-smiley\" src=\"https://s0.wp.com/wp-content/mu-plugins/wpcom-smileys/twemoji/2/72x72/1f539.png\" style=\"height: 1em;\" /> Tokenizer &amp; chat template updated \u2014 <a href=\"https://huggingface.co/deepseek-ai/DeepSeek-V3.1/blob/main/tokenizer_config.json\">new tokenizer config</a>.\n\n<img alt=\"\ud83d\udd17\" class=\"wp-smiley\" src=\"https://s0.wp.com/wp-content/mu-plugins/wpcom-smileys/twemoji/2/72x72/1f517.png\" style=\"height: 1em;\" /><a href=\"https://huggingface.co/deepseek-ai/DeepSeek-V3.1-Base\"> V3.1 Base Open-source weights</a>.\n\n<img alt=\"\ud83d\udd17\" class=\"wp-smiley\" src=\"https://s0.wp.com/wp-content/mu-plugins/wpcom-smileys/twemoji/2/72x72/1f517.png\" style=\"height: 1em;\" /><a href=\"https://huggingface.co/deepseek-ai/DeepSeek-V3.1\"> V3.1 Open-source weights</a>.\n\nPricing Changes <img alt=\"\ud83d\udcb3\" class=\"wp-smiley\" src=\"https://s0.wp.com/wp-content/mu-plugins/wpcom-smileys/twemoji/2/72x72/1f4b3.png\" style=\"height: 1em;\" />\n\n<img alt=\"\ud83d\udd39\" class=\"wp-smiley\" src=\"https://s0.wp.com/wp-content/mu-plugins/wpcom-smileys/twemoji/2/72x72/1f539.png\" style=\"height: 1em;\" /> New pricing starts &amp; off-peak discounts end at Sep 5th, 2025, 16:00 (UTC Time)\n\n<img alt=\"\ud83d\udd39\" class=\"wp-smiley\" src=\"https://s0.wp.com/wp-content/mu-plugins/wpcom-smileys/twemoji/2/72x72/1f539.png\" style=\"height: 1em;\" /> Until then, APIs follow current pricing\n\n<img alt=\"\ud83d\udcdd\" class=\"wp-smiley\" src=\"https://s0.wp.com/wp-content/mu-plugins/wpcom-smileys/twemoji/2/72x72/1f4dd.png\" style=\"height: 1em;\" /> <a href=\"https://api-docs.deepseek.com/quick_start/pricing/\">Pricing page</a>.\n\n<a href=\"https://x.com/teortaxesTex/status/1958139448257855527\">Teortaxes</a>: for now seems to have the same performance ceiling as 0528, maybe a bit weaker on some a bit stronger on other problems. The main change is that it&#8217;s a unified merge that uses \u22652x fewer reasoning tokens. I take it as a trial balloon before V4 that&#8217;ll be unified out of the box.</blockquote>\n\n<h4 class=\"wp-block-heading\">Signs of Life</h4>\nThere are some impressive scores here. <a href=\"https://x.com/jessi_cata/status/1958423894253261121\">A true 66 on SWE would be very strong</a>.\n<div>\n<figure>\n<div>\n<figure class=\"wp-block-image\"><img alt=\"\" src=\"https://substackcdn.com/image/fetch/$s_!TGt3!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe149c81c-10aa-4e2f-a196-6b07cbe76a54_731x305.png\" /></figure>\n\n<div></div>\n</div></figure>\n</div>\nThere\u2019s also the weird result where it is <a href=\"https://x.com/scaling01/status/1957890953026392212\">claimed to outscore Opus 4 on Aider Polyglot</a> at a low price.\n<blockquote><a href=\"https://x.com/WesRothMoney/status/1958121840880103720\">Wes Roth</a>: DeepSeek has quietly published V 3.1, a 685-billion-parameter open-source model that folds chat, reasoning, and coding into a single architecture, handles 128 k-token context windows, and posts a 71.6 % score on the Aider coding benchmark edging out Claude Opus 4 while costing ~68\u00d7 less in inference.</blockquote>\nBut these two data points don\u2019t seem backed up by the other reactions, or especially the lack of other reactions, or some other test results.\n\nArtificial Analysis has it coming in at 60 versus r1\u2019s 59, which would be only a small improvement.\n<div>\n<figure>\n<div>\n<figure class=\"wp-block-image\"><img alt=\"\" src=\"https://substackcdn.com/image/fetch/$s_!DEak!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F56cffd83-13d1-4f98-88b8-86d3408d9d43_949x604.png\" /></figure>\n\n<div></div>\n</div></figure>\n</div>\n<a href=\"https://x.com/HCSolakoglu/status/1957993955674435924\">Hasan Can said it hallucinates a lot</a>. <a href=\"https://x.com/SteveStricklan6/status/1958869568224436612\">Steve Strickland says \u2018it\u2019s the worst LLM I\u2019ve even tried\u2019</a> complaining about it failing a mundane task, which presumably was very bad luck.\n\nI tried to conduct Twitter polls, but well over 90% of respondents had to click \u2018see results\u2019 which left me with only a handful of real responses and means Lizardman Constant problems and small sample size invalidate the results, beyond confirming no one is looking, and the different polls don\u2019t entirely agree with each other as a result.\n\nIf this were most open model companies, I would treat this lack of reaction as indicating there was nothing here, that they likely targeted SWE as a benchmark, and move on.\n\nSince it is DeepSeek, I give them more credit than that, but am still going to assume this is only a small incremental upgrade that does not change the overall picture. However, if 3.1 really was at 66-level for real in practice, it has been several days now, and people would likely be shouting it from the rooftops. They\u2019re not.\n\n\n<h4 class=\"wp-block-heading\">How Should We Update?</h4>\nEven if no one finds anything to do with it, I don\u2019t downgrade DeepSeek much for 3.1 not impressing compared to if they hadn\u2019t released anything. It\u2019s fine to do incremental improvements. They should do a v3.1 here.\n\nThe dumbest style of reaction is when a company offers an incremental improvement (see: GPT-5) and people think that means it\u2019s all over for them, or for AI in general, because it didn\u2019t sufficiently blow them away. Chill out.\n\nIt\u2019s also not fair to fully pin this on DeepSeek when they were forced to do a lot of their training this year on Huawei Ascend chips rather than Nvidia chips. Assuming, that is, they are going to be allowed to switch back.\n\nEither way, the clock is ticking on v4 and r2."
            ],
            "link": "https://thezvi.wordpress.com/2025/08/22/deepseek-v3-1-is-not-having-a-moment/",
            "publishedAt": "2025-08-22",
            "source": "TheZvi",
            "summary": "What if DeepSeek released a model claiming 66 on SWE and almost no one tried using it? Would it be any good? Would you be able to tell? Or would we get the shortest post of the year? Why We &#8230; <a href=\"https://thezvi.wordpress.com/2025/08/22/deepseek-v3-1-is-not-having-a-moment/\">Continue reading <span class=\"meta-nav\">&#8594;</span></a>",
            "title": "DeepSeek v3.1 Is Not Having a Moment"
        },
        {
            "content": [
                "<p>Today my quotes about generative AI scrapers <a href=\"https://www.theregister.com/2025/08/21/ai_crawler_traffic/\">got published in The Register</a>. For transparency's sake, here's a copy of the questions I was asked and my raw, unedited responses. Enjoy!</p>\n        <blockquote>\n        <p>First, do you see the growth in crawler traffic slowing any time soon?</p>\n        </blockquote>\n        <p>I can only see a few things that can stop this: government regulation, or the hype finally starting to die down. There is too much hype in the mix that causes us to funnel billions of dollars into this technology instead of curing cancer, solving world hunger, or making people\u2019s lives genuinely better.</p>\n        <blockquote>\n        <p>Is it likely to continue growing?</p>\n        </blockquote>\n        <p>I see no reason why it would not grow. People are using these tools to replace knowledge and gaining skills instead of augmenting knowledge and augmenting skills. Even if they are intended to be used for letting us focus on the fun parts of our work and automating away the chores, there are some bad apples that are spoiling the bunch and making this technology about replacing people, not drudgery and toil. This technology was obviously meant well, but at some level the output of AI superficially resembles the finished work product of human labour, superficially. As someone asked to Charles Babbage: if you put in the wrong numbers, you get the wrong answer.</p>\n        <p>This isn\u2019t necessarily a bubble popping, this is a limitation of how well AI can function without direct and constant human input. Even so, we\u2019ll hit the limit on data that can be scraped that hasn\u2019t been touched by AI before the venture capital runs out. I see no value in the need for scrapers to hit the same 15 year old commit of the Linux kernel over and over and over every 30 minutes like they are now. There are ways to do this ethically that don\u2019t penalize open source infrastructure such as using the Common Crawl dataset.</p>\n        <blockquote>\n        <p>If so, how can that be sustainable?</p>\n        </blockquote>\n        <p>It's not lol. We are destroying the commons in order to get hypothetical gains. The last big AI breakthrough happened with GPT-4 in 2023. The rest has been incremental improvements in tokenization, multimodal inputs (also tokenization), tool calling (also tokenization), and fill-in-the-middle completion (again, also tokenization). Even with scrapers burning everything in their wake, there is not enough training data to create another exponential breakthrough. All we can do now is make it more efficient to run GPT-4 level models on lesser hardware. I can (and regularly do) run a model just as good as GPT-4 on my MacBook at this point, which is really cool.</p>\n        <blockquote>\n        <p>Would broader deployment of Anubis and other active countermeasures help?</p>\n        </blockquote>\n        <p>This is a regulatory issue. The thing that needs to happen is that governments need to step in and give these unethical scrapers that are destroying the digital common good existentially threatening fines and make them pay reparations to the communities they are harming. Ironically enough, most of these unethical scraping activities rely on the products of the communities they are destroying. This presents the kind of paradox that I would expect to read in a Neal Stephenson book from the '90s, not CBC's front page.</p>\n        <p>Anubis helps mitigate a lot of the badness by making attacks more computationally expensive. Anubis (even in configurations that omit proof of work) makes attackers have to retool their scraping to use headless browsers instead of blindly scraping HTML. This increases the infrastructure costs of the scrapers propagating this abusive traffic. The hope is that this makes it fiscally unviable for the unethical scrapers to scrape by making them have to dedicate much more hardware to the problem.</p>\n        <p>In essence: it makes the scrapers have to spend more money to do the same work.</p>\n        <blockquote>\n        <p>Is regulation required to prevent abuse of the open web?</p>\n        </blockquote>\n        <p>Yes, but this regulation would have to be global, simultaneous, and permanent to have any chance of this actually having a positive impact. Our society cannot currently regulate against similar existential threats like climate change. I have no hope for such regulation to be made regarding generative AI.</p>\n        <blockquote>\n        <p>Fastly's claims that 80% of bot traffic is now AI crawlers</p>\n        </blockquote>\n        <p>In some cases for open source projects, we've seen upwards of 95% of traffic being AI crawlers. Not just bot traffic, but <em>traffic in general</em>. For one, deploying Anubis almost instantly caused server load to crater by so much that it made them think they accidentally took their site offline. One of my customers had their power bills drop by a significant fraction after deploying Anubis. It's nuts. The ecological impact of these scrapers is probably a significant fraction of the ecological impact of generative AI as a whole.</p>\n        <p>Personally, deploying Anubis to my blog has reduced the amount of ad impressions I've been giving by over 50%. I suspect that there is a lot of unreported click fraud for online advertising.</p>\n        <p>I hope this helps. Keep up the good fight!</p>"
            ],
            "link": "https://xeiaso.net/notes/2025/el-reg-responses/",
            "publishedAt": "2025-08-22",
            "source": "Xe Iaso",
            "summary": "<p>Today my quotes about generative AI scrapers <a href=\"https://www.theregister.com/2025/08/21/ai_crawler_traffic/\">got published in The Register</a>. For transparency's sake, here's a copy of the questions I was asked and my raw, unedited responses. Enjoy!</p> <blockquote> <p>First, do you see the growth in crawler traffic slowing any time soon?</p> </blockquote> <p>I can only see a few things that can stop this: government regulation, or the hype finally starting to die down. There is too much hype in the mix that causes us to funnel billions of dollars into this technology instead of curing cancer, solving world hunger, or making people\u2019s lives genuinely better.</p> <blockquote> <p>Is it likely to continue growing?</p> </blockquote> <p>I see no reason why it would not grow. People are using these tools to replace knowledge and gaining skills instead of augmenting knowledge and augmenting skills. Even if they are intended to be used for letting us focus on the fun parts of our work and automating away the chores, there are some bad apples that are spoiling the bunch and making this technology about replacing people, not drudgery and toil. This technology was obviously meant well, but at some level the output of AI superficially resembles the finished work product of",
            "title": "My responses to The Register"
        },
        {
            "content": [],
            "link": "https://xkcd.com/3132/",
            "publishedAt": "2025-08-22",
            "source": "XKCD",
            "summary": "<img alt=\"Hey! A bunch of the early Cretaceous fossils on each coast seem to have been plagiarized, too!\" src=\"https://imgs.xkcd.com/comics/coastline_similarity.png\" title=\"Hey! A bunch of the early Cretaceous fossils on each coast seem to have been plagiarized, too!\" />",
            "title": "Coastline Similarity"
        }
    ],
    "lookbackDays": 1,
    "publishDate": "2025-08-22"
}