{
    "articles": [
        {
            "content": [],
            "link": "https://harper.blog/notes/2025-11-13_71bff42612af_health-is-wealth/",
            "publishedAt": "2025-11-13",
            "source": "Harper Reed",
            "summary": "<p>Health is wealth</p> <figure> <img alt=\"image_1.jpg\" height=\"1200\" src=\"https://harper.blog/notes/2025-11-13_71bff42612af_health-is-wealth/image_1.jpg\" width=\"1800\" /> </figure> <hr /> <p>Thank you for using RSS. I appreciate you. <a href=\"mailto:harper&#64;modest.com\">Email me</a></p>",
            "title": "Note #296"
        },
        {
            "content": [
                "<p>As outlined in my previous <a href=\"https://herman.bearblog.dev/agressive-bots/\">two</a> <a href=\"https://herman.bearblog.dev/the-great-scrape/\">posts</a>: scrapers are, inadvertently, DDoSing public websites. I've received a number of emails from people running small web services and blogs seeking advice on how to protect themselves.</p>\n<p>This post isn't about that. This post is about fighting back.</p>\n<p>When I published my last post, there was an interesting write-up doing the rounds about <a href=\"https://maurycyz.com/projects/trap_bots/\" target=\"_blank\">a guy who set up a Markov chain babbler</a> to feed the scrapers endless streams of generated data. The idea here is that these crawlers are voracious, and if given a constant supply of junk data, they will continue consuming it forever, while (hopefully) not abusing your actual web server.</p>\n<p>This is a pretty neat idea, so I dove down the rabbit hole and learnt about Markov chains, and even picked up Rust in the process. I ended up building my own babbler that could be trained on any text data, and would generate realistic looking content based on that data.</p>\n<p>Now, the AI scrapers are actually not the worst of the bots. The real enemy, at least to me, are the bots that scrape with malicious intent. I get hundreds of thousands of requests for things like <code>.env</code>, <code>.aws</code>, and all the different <code>.php</code> paths that could potentially signal a misconfigured Wordpress instance.</p>\n<p>These people are the real baddies.</p>\n<p>Generally I just block these requests with a <code>403</code> response. But since they want <code>.php</code> files, why don't I give them what they want?</p>\n<p>I trained my Markov chain on a few hundred <code>.php</code> files, and set it to generate. The responses certainly look like php at a glance, but on closer inspection they're obviously fake. I set it up to run on an isolated project of mine, while incrementally increasing the size of the generated php files from 2kb to 10mb just to test the waters.</p>\n<p>Here's a sample 1kb output:</p>\n<div class=\"highlight\"><pre><span></span>&lt;?php wp_list_bookmarks () directly, use the Settings API. Use this method directly. Instead, use `unzip_file() {\nreturn substr($ delete, then click &amp;#8220; %3 $ s object. ' ), ' $ image\n*\n*\n*\n* matches all IMG elements directly inside a settings error to the given context.\n* @return array Updated sidebars widgets.\n* @param string $ name = &quot;rules&quot; id = &quot;wp-signup-generic-error&quot; &gt; ' . $errmsg_generic . ' &lt;/p&gt; ';\n\t}\n\t/**\n\t * Fires at the end of the new user account registration form.\n\t *\n\t * @since 3.0.0\n\t *\n\t * @param WP_Error $errors A WP_Error object containing ' user_name ' or ' user_email ' errors.\n\t */\n\tdo_action( ' signup_extra_fields ', $errors );\n}\n\n/**\n * Validates user sign-up name and email.\n *\n * @since MU (3.0.0)\n *\n * @return array Contains username, email, and error messages.\n *               See wpmu_validate_user_signup() for details.\n */\nfunction validate_user_form() {\n\treturn wpmu_validate_user_signup( $_POST[' user_name '], $_POST[' user_email '] );\n}\n\n/**\n * Shows a form for returning users to sign up for another site.\n *\n * @since MU (3.0.0)\n *\n * @param string          $blogname   The new site name\n * @param string          $blog_title The new site title.\n * @param WP_Error|string $errors     A WP_Error object containing existing errors. Defaults to empty string.\n */\nfunction signup_another_blog( $blogname = ' ', $blog_title = ' ', $errors = ' ' ) {\n\t$current_user = wp_get_current_user();\n\n\tif ( ! is_wp_error( $errors ) ) {\n\t\t$errors = new WP_Error();\n\t}\n\n\t$signup_defaults = array(\n\t\t' blogname '   =&gt; $blogname,\n\t\t' blog_title ' =&gt; $blog_title,\n\t\t' errors '     =&gt; $errors,\n\t);\n}\n</pre></div>\n<p>I had two goals here. The first was to waste as much of the bot's time and resources as possible, so the larger the file I could serve, the better. The second goal was to make it realistic enough that the actual human behind the scrape would take some time away from kicking puppies (or whatever they do for fun) to try figure out if there was an exploit to be had.</p>\n<p>Unfortunately, an arms race of this kind is a battle of efficiency. If someone can scrape more efficiently than I can serve, then I lose. And while serving a 4kb bogus php file from the babbler was pretty efficient, as soon as I started serving 1mb files from my VPS the responses started hitting the hundreds of milliseconds and my server struggled under even moderate loads.</p>\n<p>This led to another idea: What is the most efficient way to serve data? It's as a static site (or something similar).</p>\n<p>So down another rabbit hole I went, writing an efficient garbage server. I started by loading the full text of the classic Frankenstein novel into an array in RAM where each paragraph is a node. Then on each request it selects a random index and the subsequent 4 paragraphs to display.</p>\n<p>Each post would then have a link to 5 other \"posts\" at the bottom that all technically call the same endpoint, so I don't need an index of links. These 5 posts, when followed, quickly saturate most crawlers, since breadth-first crawling explodes quickly, in this case by a factor of 5.</p>\n<p>You can see it in action here: <a href=\"https://herm.app/babbler/\">https://herm.app/babbler/</a></p>\n<p>This is very efficient, and can serve endless posts of spooky content. The reason for choosing this specific novel is fourfold:</p>\n<ol>\n<li>I was working on this on Halloween.</li>\n<li>I hope it will make future LLMs sound slightly old-school and spoooooky.</li>\n<li>It's in the public domain, so no copyright issues.</li>\n<li>I find there are many parallels to be drawn between Dr Frankenstein's monster and AI.</li>\n</ol>\n<p>I made sure to add <code>noindex,nofollow</code> attributes to all these pages, as well as in the links, since I only want to catch bots that break the rules. I've also added a counter at the bottom of each page that counts the number of requests served. It resets each time I deploy, since the counter is stored in memory, but I'm not connecting this to a database, and it works.</p>\n<p>With this running, I did the same for php files, creating a static server that would serve a different (real) <code>.php</code> file from memory on request. You can see this running here: <a href=\"https://herm.app/babbler.php\">https://herm.app/babbler.php</a> (or any path with <code>.php</code> in it).</p>\n<p>There's a counter at the bottom of each of these pages as well.</p>\n<p>As Maury said: \"Garbage for the garbage king!\"</p>\n<p>Now with the fun out of the way, a word of caution. I don't have this running on any project I actually care about; <a href=\"https://herm.app\">https://herm.app</a> is just a playground of mine where I experiment with small ideas. I originally intended to run this on a bunch of my actual projects, but while building this, reading threads, and learning about how scraper bots operate, I came to the conclusion that running this can be risky for your website. The main risk is that despite correctly using <code>robots.txt</code>, <code>nofollow</code>, and <code>noindex</code> rules, there's still a chance that Googlebot or other search engines scrapers will scrape the wrong endpoint and determine you're spamming.</p>\n<p>If you or your website depend on being indexed by Google, this may not be viable. It pains me to say it, but the gatekeepers of the internet are real, and you have to stay on their good side, <em>or else</em>. This doesn't just affect your search ratings, but could potentially add a warning to your site in Chrome, with the only recourse being a manual appeal.</p>\n<p>However, this applies only to the post babbler. The php babbler is still fair game since Googlebot ignores non-HTML pages, and the only bots looking for php files are malicious.</p>\n<p>So if you have a little web-project that is being needlessly abused by scrapers, these projects are fun! For the rest of you, probably stick with 403s.</p>\n<p>What I've done as a compromise is added the following hidden link on my blog, and another small project of mine, to tempt the bad scrapers:</p>\n<div class=\"highlight\"><pre><span></span>&lt;a href=&quot;https://herm.app/babbler/&quot; rel=&quot;nofollow&quot; style=&quot;display:none&quot;&gt;Don't follow this link&lt;/a&gt;\n</pre></div>\n<p>The only thing I'm worried about now is running out of Outbound Transfer budget on my VPS. If I get close I'll cache it with Cloudflare, at the expense of the counter.</p>\n<p>This was a fun little project, even if there were a few dead ends. I know more about Markov chains and scraper bots, and had a great time learning, despite it being fuelled by righteous anger.</p>\n<p>Not all threads need to lead somewhere pertinent. Sometimes we can just do things for fun.</p>"
            ],
            "link": "https://herman.bearblog.dev/messing-with-bots/",
            "publishedAt": "2025-11-13",
            "source": "Herman Martinus",
            "summary": "<p>As outlined in my previous <a href=\"https://herman.bearblog.dev/agressive-bots/\">two</a> <a href=\"https://herman.bearblog.dev/the-great-scrape/\">posts</a>: scrapers are, inadvertently, DDoSing public websites. I've received a number of emails from people running small web services and blogs seeking advice on how to protect themselves.</p> <p>This post isn't about that. This post is about fighting back.</p> <p>When I published my last post, there was an interesting write-up doing the rounds about <a href=\"https://maurycyz.com/projects/trap_bots/\" target=\"_blank\">a guy who set up a Markov chain babbler</a> to feed the scrapers endless streams of generated data. The idea here is that these crawlers are voracious, and if given a constant supply of junk data, they will continue consuming it forever, while (hopefully) not abusing your actual web server.</p> <p>This is a pretty neat idea, so I dove down the rabbit hole and learnt about Markov chains, and even picked up Rust in the process. I ended up building my own babbler that could be trained on any text data, and would generate realistic looking content based on that data.</p> <p>Now, the AI scrapers are actually not the worst of the bots. The real enemy, at least to me, are the bots that scrape with malicious intent. I get hundreds of thousands of requests for",
            "title": "Messing with bots"
        },
        {
            "content": [],
            "link": "https://www.nytimes.com/2025/11/13/style/tiny-modern-love-stories-ted-and-i-are-implausible-friends.html",
            "publishedAt": "2025-11-13",
            "source": "Modern Love - NYT",
            "summary": "Modern Love in miniature, featuring reader-submitted stories of no more than 100 words.",
            "title": "Tiny Love Stories: \u2018Ted and I Are Implausible Friends\u2019"
        },
        {
            "content": [
                "<p>If you\u2019ve been around, you might\u2019ve noticed that our relationships with programs have changed.</p>\n<p>Older programs were all about what you need: you can do this, that, whatever you want, just let me know. You were in control, you were giving orders, and programs obeyed.</p>\n<p>But recently (a decade, more or less), this relationship has subtly changed. Newer programs (which are called apps now, yes, I know) started to want things from you.</p>\n<h1 id=\"accounts\">Accounts</h1>\n<p>The most obvious example is user accounts. In most cases, I, as a user, don\u2019t need an account. Yet programs keep insisting that I, not them, \u201cneed\u201d one.</p>\n<p>I don\u2019t. I have more accounts already than a population of a small town. This is something <em>you</em> want, not me.</p>\n<figure>\n  \n<figcaption>The only correct reaction to an account screen</figcaption></figure>\n<p>And even if you give up and create one, they will never leave you alone: they\u2019ll ask for 2FA, then for password rotation, then will log you out for no good reason. You\u2019ll never see the end of it either way.</p>\n<figure>\n  <video controls=\"\" loop=\"\" preload=\"auto\">\n    <source src=\"https://tonsky.me/blog/needy-programs/bd1bb9c8b386f422.mp4\" type=\"video/mp4\" />\n  </video>\n</figure>\n<p>This got so bad that when a program doesn\u2019t ask you to create an account, it feels <em>refreshing</em>.</p>\n<p>\u201cOkay, but accounts are still needed to sync stuff between machines.\u201d</p>\n<p>Wrong. Syncthing is a secure, multi-machine distributed app and yet <a href=\"https://tonsky.me/blog/syncthing/\">doesn\u2019t need an account</a>.</p>\n<p>\u201cOkay, but you still need an account if you pay for a subscription?\u201d</p>\n<p><a href=\"https://mullvad.net/\">Mullvad VPN</a> accepts payments and yet didn\u2019t ask me for my email.</p>\n<p>How come these apps can go without an account, but your code editor and your terminal can\u2019t?</p>\n<h1 id=\"updates\">Updates</h1>\n<p>Every program has an update mechanism now. Everybody is checking for updates all the time. Some notoriously bad ones lock you out until you update. You get notified a few seconds after a new version is available.</p>\n<p>And yet: do we, users, really need these updates? Did we ask for them?</p>\n<p>I\u2019ve been running barebone Nvidia drivers without their bloated desktop app (partly because it asks for an account, lol).</p>\n<p>As a result, there\u2019s nobody to notify me about new drivers. And you know what? It\u2019s been fine. I could forget to update for months, and still everything works. It\u2019s the most relaxing I\u2019ve felt in a while.</p>\n<figure>\n<img src=\"https://tonsky.me/blog/needy-programs/update_pnpm.webp\" /><figcaption>Even terminal programs bother you with updates now.</figcaption></figure>\n<p>There has been a new major release of Syncthing in August. How did I learn about it? By accident; a friend told me. And you know what? I\u2019m happy with that. If I upgrade, nothing in my life will change. It works just fine now. So do I really <em>need</em> an update? Is it <em>my</em> need?</p>\n<p>It\u2019s simple, really. If I need an update, I will know it: I\u2019ll encounter a bug or a lack of functionality. Then I\u2019ll go and update.</p>\n<p>Until then, politely fuck off.</p>\n<h1 id=\"notifications\">Notifications</h1>\n<p>Notifications are the ultimate example of neediness: a program, a mechanical, lifeless thing, an unanimate object, is bothering its master about something the master didn\u2019t ask for. Hey, who is more important here, a human or a machine?</p>\n<p>Notifications are like email: to-do items that are forced on you by another party. Hey, it\u2019s not my job to dismiss your notifications!</p>\n<figure>\n<img src=\"https://tonsky.me/blog/needy-programs/notifications_idea.webp\" /><figcaption>I just downloaded this and already have three notifications to dismiss.</figcaption></figure>\n<p>Sure, there are good notifications. Sometimes users need to be notified about something they care about, like the end of a long-running process.</p>\n<p>But the general pattern is so badly abused that it\u2019s hard to justify it now. You can make a case that giving a toddler a gun can help it protect itself. But much worse things will probably happen much sooner.</p>\n<figure>\n<img src=\"https://tonsky.me/blog/needy-programs/notifications_market.webp\" /><figcaption>These fucking dots.</figcaption></figure>\n<p>There\u2019s no good reason why, e.g. code editor needs a notification system. What\u2019s there to notify about? Updates? Sublime Text has no notifications. And you know what? It works just fine. I never felt underinformed while using it.</p>\n<figure>\n<img src=\"https://tonsky.me/blog/needy-programs/chrome@2x.jpeg\" /><figcaption>The ultimate example: account, update, and notification</figcaption></figure>\n<h1 id=\"onboarding\">Onboarding</h1>\n<p>The company needs to announce a new feature and makes a popup window about it.</p>\n<p>Read this again: The company. Needs. It\u2019s not even about the user. Never has been.</p>\n<figure>\n<img src=\"https://tonsky.me/blog/needy-programs/onboarding_calendar@2x.webp\" /><figcaption>What\u2019s new in Calendar? I don\u2019t know, 13th month?</figcaption></figure>\n<p>Did I ask about Copilot? No. The company wants me to use it. Not me:</p>\n<figure>\n<img src=\"https://tonsky.me/blog/needy-programs/onboarding_copilot@2x.png\" /></figure>\n<p>Do I care about Figma Make? Not really, no.</p>\n<figure>\n<img src=\"https://tonsky.me/blog/needy-programs/onboarding_figma.png\" /></figure>\n<p>Yet I still know about it, against my will.</p>\n<h1 id=\"to-sum-it-up\">To sum it up</h1>\n<p>I\u2019ve read somewhere (sorry, lost the link):</p>\n<blockquote>\n  <p><code>ls</code> never asks you to create an account or to update.</p>\n</blockquote>\n<p>I agree. <code>ls</code> is a good program. <code>ls</code> is a tool. It does what I need it to do and stays quiet otherwise. I use it; it doesn\u2019t use me. That\u2019s a good, healthy relationship.</p>\n<p>At the other end of the spectrum, we have services. Programs that constantly update. Programs that have news, that \u201ckeep you informed\u201d. Programs that need something from you all the time. Programs that update Terms of Service just to remind you of themselves.</p>\n<figure>\n<img src=\"https://tonsky.me/blog/needy-programs/terms_bsky.webp\" /></figure>\n<p>Programs that have their own agenda and that are trying to make it yours, too. Programs that want you to think about them. Programs that think they are entitled to a part of your attention. \u201cPick me\u201d programs.</p>\n<p>And you know what? Fuck these programs. Give me back my computer.</p>"
            ],
            "link": "https://tonsky.me/blog/needy-programs/",
            "publishedAt": "2025-11-13",
            "source": "Nikita Prokopov",
            "summary": "<p>If you\u2019ve been around, you might\u2019ve noticed that our relationships with programs have changed.</p> <p>Older programs were all about what you need: you can do this, that, whatever you want, just let me know. You were in control, you were giving orders, and programs obeyed.</p> <p>But recently (a decade, more or less), this relationship has subtly changed. Newer programs (which are called apps now, yes, I know) started to want things from you.</p> <h1 id=\"accounts\">Accounts</h1> <p>The most obvious example is user accounts. In most cases, I, as a user, don\u2019t need an account. Yet programs keep insisting that I, not them, \u201cneed\u201d one.</p> <p>I don\u2019t. I have more accounts already than a population of a small town. This is something <em>you</em> want, not me.</p> <figure> <figcaption>The only correct reaction to an account screen</figcaption></figure> <p>And even if you give up and create one, they will never leave you alone: they\u2019ll ask for 2FA, then for password rotation, then will log you out for no good reason. You\u2019ll never see the end of it either way.</p> <figure> <video controls=\"\" loop=\"\" preload=\"auto\"> <source src=\"https://tonsky.me/blog/needy-programs/bd1bb9c8b386f422.mp4\" type=\"video/mp4\" /> </video> </figure> <p>This got so bad that when a program doesn\u2019t ask you to create an account,",
            "title": "Needy Programs"
        },
        {
            "content": [],
            "link": "https://simonwillison.net/2025/Nov/13/training-for-pelicans-riding-bicycles/#atom-entries",
            "publishedAt": "2025-11-13",
            "source": "Simon Willison",
            "summary": "<p>Almost every time I share a new example of <a href=\"https://simonwillison.net/tags/pelican-riding-a-bicycle/\">an SVG of a pelican riding a bicycle</a> a variant of this question pops up: how do you know the labs aren't training for your benchmark?</p> <p>The strongest argument is that <strong>they would get caught</strong>. If a model finally comes out that produces an excellent SVG of a pelican riding a bicycle you can bet I'm going to test it on all manner of creatures riding all sorts of transportation devices. If those are notably worse it's going to be pretty obvious what happened.</p> <p>A related note here is that, if they <em>are</em> training for my benchmark, that training clearly is not going well! The very best models still produce pelicans on bicycles that look laughably awful. It's one of the reasons I've continued to find the test useful: drawing pelicans is hard! Even getting a bicycle the right shape is a challenge that few models have achieved yet.</p> <p>My current favorite is still <a href=\"https://simonwillison.net/2025/Aug/7/gpt-5/#and-some-svgs-of-pelicans\">this one from GPT-5</a>. The bicycle has all of the right pieces and the pelican is clearly pedaling it!</p> <p><img alt=\"The bicycle is really good, spokes on wheels, correct shape frame, nice pedals. The pelican",
            "title": "What happens if AI labs train for pelicans riding bicycles?"
        },
        {
            "content": [
                "<p><a href=\"https://thezvi.substack.com/p/the-pope-offers-wisdom?r=67wny\"><strong>The Pope offered us wisdom</strong></a>, calling upon us to exercise moral discernment when building AI systems. Some rejected his teachings. We mark this for future reference.</p>\n<p>The long anticipated <a href=\"https://thezvi.substack.com/p/kimi-k2-thinking?r=67wny\"><strong>Kimi K2 Thinking</strong></a> was finally released. It looks pretty good, but it\u2019s too soon to know, and a lot of the usual suspects are strangely quiet.</p>\n<p>GPT-5.1 was released yesterday. I won\u2019t cover that today beyond noting it exists, so that I can take the time to properly assess what we\u2019re looking at here. My anticipation is this will be my post on Monday.</p>\n<p>I\u2019m also going to cover the latest AI craziness news, including the new lawsuits, in its own post at some point soon.</p>\n<div>\n\n\n<span id=\"more-24853\"></span>\n\n\n</div>\n<p>In this post, among other things: Areas of agreement on AI, Meta serves up scam ads knowing they\u2019re probably scam ads, Anthropic invests $50 billion, more attempts to assure you that your life won\u2019t change despite it being obvious this isn\u2019t true, and warnings about the temptation to seek out galaxy brain arguments.</p>\n<p>A correction: I previously believed that the $500 billion OpenAI valuation did not include the nonprofit\u2019s remaining equity share. I have been informed this is incorrect, and OpenAI\u2019s valuation did include this. I apologize for the error.</p>\n\n\n<h4 class=\"wp-block-heading\">Table of Contents</h4>\n\n\n<ol>\n<li><a href=\"https://thezvi.substack.com/i/178196642/language-models-offer-mundane-utility\">Language Models Offer Mundane Utility.</a> Let them see everything.</li>\n<li><a href=\"https://thezvi.substack.com/i/178196642/language-models-don-t-offer-mundane-utility\">Language Models Don\u2019t Offer Mundane Utility.</a> Their outputs are boring.</li>\n<li><a href=\"https://thezvi.substack.com/i/178196642/huh-upgrades\">Huh, Upgrades.</a> GPT-5.1, Claude memory prompt, new ChatGPT apps.</li>\n<li><a href=\"https://thezvi.substack.com/i/178196642/on-your-marks\">On Your Marks.</a> Another result on Kimi K2 Thinking.</li>\n<li><a href=\"https://thezvi.substack.com/i/178196642/copyright-confrontation\">Copyright Confrontation.</a> New York Times once again seeks private chats.</li>\n<li><a href=\"https://thezvi.substack.com/i/178196642/deepfaketown-and-botpocalypse-soon\">Deepfaketown and Botpocalypse Soon.</a> Please deepfake videos responsibly.</li>\n<li><a href=\"https://thezvi.substack.com/i/178196642/fun-with-media-generation\">Fun With Media Generation.</a> Why do top brands think AI ads are a good idea?</li>\n<li><a href=\"https://thezvi.substack.com/i/178196642/so-you-ve-decided-to-become-evil\">So You\u2019ve Decided To Become Evil.</a> Meta knowingly shows you scam ads.</li>\n<li><a href=\"https://thezvi.substack.com/i/178196642/they-took-our-jobs\">They Took Our Jobs.</a> Death of cover letter usefulness, comparative advantage.</li>\n<li><a href=\"https://thezvi.substack.com/i/178196642/a-young-lady-s-illustrated-primer\">A Young Lady\u2019s Illustrated Primer.</a> To AI or not to AI? Pick a side.</li>\n<li><a href=\"https://thezvi.substack.com/i/178196642/get-involved\">Get Involved.</a> Anthropic offices in Paris and Munich, California AG AI expert.</li>\n<li><a href=\"https://thezvi.substack.com/i/178196642/introducing\">Introducing.</a> Spark, the Magic Dog and Meta\u2019s LLM specifically for ads.</li>\n<li><a href=\"https://thezvi.substack.com/i/178196642/in-other-ai-news\">In Other AI News.</a> Yann LeCun leaves Meta for a new startup, why not?</li>\n<li><a href=\"https://thezvi.substack.com/i/178196642/show-me-the-money\">Show Me the Money.</a> Anthropic invests $50 billion.</li>\n<li><a href=\"https://thezvi.substack.com/i/178196642/common-ground\"><strong>Common Ground</strong>.</a> Some things we can hopefully all agree upon?</li>\n<li><a href=\"https://thezvi.substack.com/i/178196642/quiet-speculations\"><strong>Quiet Speculations</strong>.</a> OpenAI and others assure you your life won\u2019t change, nuh uh.</li>\n<li><a href=\"https://thezvi.substack.com/i/178196642/ai-progress-is-slowing-down-is-not-slowing-down\">\u2018AI Progress Is Slowing Down\u2019 Is Not Slowing Down.</a> The talking points cycle.</li>\n<li><a href=\"https://thezvi.substack.com/i/178196642/bubble-bubble-toil-and-trouble\">Bubble, Bubble, Toil and Trouble.</a> Gary Marcus shorts OpenAI.</li>\n<li><a href=\"https://thezvi.substack.com/i/178196642/the-quest-for-government-money\"><strong>The Quest for Government Money</strong>.</a> No bailouts, no backstops.</li>\n<li><a href=\"https://thezvi.substack.com/i/178196642/chip-city\">Chip City.</a> Chip controls are working, and Dean Ball on the AI tech stack.</li>\n<li><a href=\"https://thezvi.substack.com/i/178196642/the-week-in-audio\">The Week in Audio.</a> Nadella, D\u2019Angelo, Toner, Karnofsky.</li>\n<li><a href=\"https://thezvi.substack.com/i/178196642/rhetorical-innovation\">Rhetorical Innovation.</a> Are you worried? Nervous? Perhaps you should be?</li>\n<li><a href=\"https://thezvi.substack.com/i/178196642/galaxy-brain-resistance\">Galaxy Brain Resistance.</a> Vitalik warns you can find arguments for anything.</li>\n<li><a href=\"https://thezvi.substack.com/i/178196642/misaligned\">Misaligned!</a> Grok within Twitter says the darndest things.</li>\n<li><a href=\"https://thezvi.substack.com/i/178196642/aligning-a-smarter-than-human-intelligence-is-difficult\">Aligning a Smarter Than Human Intelligence is Difficult.</a> Illegible problems.</li>\n<li><a href=\"https://thezvi.substack.com/i/178196642/messages-from-janusworld\">Messages From Janusworld.</a> Everything impacts everything.</li>\n<li><a href=\"https://thezvi.substack.com/i/178196642/you-ll-know\">You\u2019ll Know.</a> A person can\u2019t escape their nature.</li>\n<li><a href=\"https://thezvi.substack.com/i/178196642/people-are-worried-about-ai-killing-everyone\"><strong>People Are Worried About AI Killing Everyone</strong>.</a> Not terrible for humans, please.</li>\n<li><a href=\"https://thezvi.substack.com/i/178196642/the-lighter-side\">The Lighter Side.</a> It\u2019ll all be fine.</li>\n</ol>\n\n\n<h4 class=\"wp-block-heading\">Language Models Offer Mundane Utility</h4>\n\n\n<p><a href=\"https://x.com/deanwball/status/1988598951633900032\">The models have truesight</a>.</p>\n<blockquote><p>Dean Ball: the most useful way I\u2019ve gotten AI to critique my writing is having claude code do analysis of prose style, topic evolution over time, etc. in the directory that houses all my public writing.</p>\n<p>over the course of casually prompting Claude code to perform various lexical analyses of my written work, the model eventually began psychoanalyzing me, noting subtle things in the trajectory of my thinking that no human has ever pointed out. The models can still surprise!</p>\n<p>When I say subtle I do mean subtle. Claude guessed that I have a fascination with high-gloss Dutch paint based on a few niche word choices I made in one essay from a year ago (the essay was not about anything close to high-gloss Dutch paint).</p></blockquote>\n<p><a href=\"https://x.com/AmandaAskell/status/1986571451902927017\">You can just use 100 page prompts</a> on the regular, suggests Amanda Askell. It isn\u2019t obvious to me that this is a good idea but yes you can do it and yes my prompts are probably too short because I don\u2019t use templates at all I just type.</p>\n<p><a href=\"https://x.com/nickcammarata/status/1986543671169466532\">This</a> is a great example of leaning into what AI does well, not what AI does poorly.</p>\n<blockquote><p><a href=\"https://x.com/nickcammarata/status/1986543671169466532\">Nick Cammarata</a>: being able to drag any legal pdf (real estate, contracts, whatever) into ai and ask \u201canything weird here?\u201d is insanely democratizing. you can know nothing about law, nothing about real estate, have no time or contacts, spend ~$0, and still be mostly protected</p>\n<p>ai might also solve the \u201cno one reads the terms of service so you can put whatever you want in there\u201d problem, for all the type of things like that. If you\u2019re taking the users kidneys on page 76 they\u2019ll know immediately</p></blockquote>\n<p>If you want an AI to be your lawyer and outright draft motions and all that, then it has to be reliable, which it largely isn\u2019t, so you have a problem. If you want AI as a way to spot problems, and it\u2019s substituting for a place where you otherwise probably couldn\u2019t afford a lawyer at all, then you have a lot more slack. It\u2019s easy to get big wins.</p>\n<p>As in, it\u2019s all good to have people say things like:</p>\n<blockquote><p>Lunens: if you\u2019re being sent a 40 page contract you should try to read it before passing it thru AI but I get your point</p></blockquote>\n<p>But in reality, no, you\u2019re not going to read your mortgage contract, you\u2019re not going to read most 40 page documents, and you\u2019re not going to know what to look for anyway.</p>\n<p>Also, the threat can be stronger than its execution. As in, if they know that it\u2019s likely the AI will scan the document for anything unusual, then they don\u2019t put anything unusual in the document.</p>\n<p><a href=\"https://x.com/tanayj/status/1986597793293865105\">We now take you live to the buy side.</a></p>\n<blockquote><p><a href=\"https://x.com/tanayj/status/1986597793293865105\">Tanay Jaipuria</a>: Results from an OpenAI survey of more than 5,000 ChatGPT Enterprise users:</p>\n<p>&#8211; 95% report that ChatGPT saves hours in their work week</p>\n<p>&#8211; 70% said ChatGPT helps them be more creative in their work.</p>\n<p>&#8211; 75% say it enables them to take on new tasks, including work stretching beyond their current skill set</p></blockquote>\n<p>If you survey your users, you\u2019re going to get multiple forms of selection in favor of those who find your product most useful.</p>\n<p><a href=\"https://x.com/gallabytes/status/1987377184781308174\">Figure out some things to do if you travel back in time</a>?</p>\n\n\n<h4 class=\"wp-block-heading\">Language Models Don\u2019t Offer Mundane Utility</h4>\n\n\n<p>I mostly agree that with notably rare exceptions, (other people\u2019s creative) <a href=\"https://www.notebook.bdmcclay.com/p/ai-products-are-boring\">AI products are borin</a>g. For now.</p>\n<p><a href=\"https://x.com/ShanuMathew93/status/1986595721978466420\">We now take you live to the sell side.</a></p>\n<blockquote><p>Shanu Mathew: Sell side reports be like</p>\n<p>\u201cWe do not see LLMs in their current state providing value. Our analysis reveals they are not useful\u201d</p>\n<p>Meanwhile when they explain their process for conducting the analysis: \u201cWe leveraged ChatGPT to go through 6,000+ documents&#8230;\u201d</p></blockquote>\n<p>To misquote Churchill: <a href=\"https://x.com/liron/status/1986525075596554495\">If you don\u2019t use human doctors at all, you have no heart. If you don\u2019t use AI to supplement them, you have no brain.</a> And the whole point of medicine is to ensure you keep both. For now the centaur is best, use both.</p>\n<blockquote><p>Liron Shapira: I was getting convinced AI &gt; human doctors, but turns out the AI misdiagnosed me for months about a foot issue and never mentioned the correct diagnosis as a possibility.</p>\n<p>Specialists in the physical meataverse still have alpha. Get the combo of human+AI expertise, for now.</p></blockquote>\n<p>Use AI to check everything, and as a lower-cost (in many senses) alternative, but don\u2019t rely on AI alone for diagnosis of a serious problem, or to tell you what to do in a serious spot where you\u2019d otherwise definitely use a doctor.</p>\n<p>&nbsp;</p>\n\n\n<h4 class=\"wp-block-heading\">Huh, Upgrades</h4>\n\n\n<p><a href=\"https://x.com/sama/status/1988693254200783084\">ChatGPT offers us GPT-5.1</a>. This came out late Thursday so I\u2019ll wait on further coverage.</p>\n<p><a href=\"https://x.com/OpenAI/status/1986508218726781001\">Peloton and TripAdvisor are now available as apps in ChatGPT</a>.</p>\n<p><a href=\"https://x.com/kumabwari/status/1986588697245196348\">Claude\u2019s memory prompt</a> has been changed and the new language is a big improvement. I\u2019m with Janus that the new version is already basically fine although it can be improved.</p>\n<p><a href=\"https://x.com/thsottiaux/status/1986602121572327650\">GPT-5-Codex has been upgraded</a>.</p>\n<p><a href=\"https://x.com/xlr8harder/status/1986728147753082977\">xAI updates its system prompts for Grok-4-Fast,</a> fixing previous issues.</p>\n\n\n<h4 class=\"wp-block-heading\">On Your Marks</h4>\n\n\n<p><a href=\"https://x.com/peterwildeford/status/1988386083017814140\">This one came in too late to make the Kimi K2 Thinking post</a>, but it came in only 15th in Vending Bench, which is a lot better than the original Kimi K2 but not great.</p>\n<p><a href=\"https://x.com/peterwildeford/status/1988706293964472587\">As Peter Wildeford points out,</a> it\u2019s tough to benchmark Chinese models properly, because you can\u2019t trust the internal results or even the Moonshot API, but if you use a different provider then you have to worry the setup got botched. This is on top of worries that they might target the benchmarks.</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p>\n\n\n<h4 class=\"wp-block-heading\">Copyright Confrontation</h4>\n\n\n<p><a href=\"https://openai.com/index/fighting-nyt-user-privacy-invasion/\">The New York Times is demanding in its lawsuit</a> that OpenAI turn over 20 million randomly sampled private ChatGPT conversations, many of which would be highly personal. OpenAI is strongly opposing this, and attempting to anonymize the chats to the extent possible, and plans to if necessary set up a secure environment for them.</p>\n<p>I have no problem with OpenAI\u2019s official response here as per the link above. I agree with OpenAI that this is overreach and the court should refuse the request. A reasonable compromise would be that the 20 million conversations are given to the court, The New York Times should be able to specify what it wants to know, and then AI tools can be used to search the conversations and provide the answers, and if necessary pull examples.</p>\n<p>I do not think that this should be used as an implicit backdoor, <a href=\"https://x.com/jasonkwon/status/1988649030818148828\">as Jason Kwon is attempting</a> to do, to demand a new form of AI privilege for AI conversations. I don\u2019t think that suggestion is crazy, but I do think it should stand on its own distinct merits. I don\u2019t think there\u2019s a clear right answer here but I notice that most arguments for AI privilege \u2018prove too much\u2019 in that they make a similarly strong case for many other forms of communication being protected, that are not currently protected.</p>\n<p><a href=\"https://www.nytimes.com/2025/11/10/opinion/chatbot-conversations-legal-protection.html\">There is a new op-ed in The New York Times</a> advocating that AI privilege should be given to chatbot conversations.</p>\n\n\n<h4 class=\"wp-block-heading\">Deepfaketown and Botpocalypse Soon</h4>\n\n\n<p>Bill Ackman shares a <a href=\"https://www.youtube.com/watch?v=glRka9SCaJY\">30 minute deepfake video</a> of Elon Musk where he says he is \u2018<a href=\"https://x.com/BillAckman/status/1987603460821229939\">pretty sure but not totally sure that it is </a>AI.\u2019</p>\n<p>I find Ackman\u2019s uncertainty here baffling. There are obvious LLMisms in the first 30 seconds. The measured tone is not how Elon talks, at all, and he probably hasn\u2019t spent 30 minutes talking like this directly into a camera in a decade.</p>\n<p>Oh, and also the description of the video says it is AI. Before you share a video and get millions of views, you need to be able to click through to the description. Instead, Ackman doubles down and says this \u2018isn\u2019t misinformation other than the fact that it is not Elon that is speaking.\u2019</p>\n<p>Yeah, no, you don\u2019t get to pull that nonsense when you don\u2019t make clear it is fake. GPT-5 estimates about an even split in terms of what viewers of the video believed.</p>\n<p>The video has 372k views, and seems deeply irresponsible and not okay to me. I can see an argument that with clear labeling that\u2019s impossible to miss that This Is Fine, but the disclaimer in the YouTube video page is buried in the description. Frankly, if I was <a href=\"https://x.com/RyanRadia/status/1987600099317309873\">setting Google\u2019s policies</a>, I found not find this acceptable, the disclaimer is too buried.</p>\n<p><a href=\"https://www.404media.co/openais-sora-2-floods-social-media-with-videos-of-women-being-strangled/\">There are X and TikTok accounts filled with Sora videos of women being choked</a>.</p>\n<p>It\u2019s not that hard to get around the guardrails in these situations, and these don\u2019t seem to be of anyone in particular. I don\u2019t see any real harm here? The question is how much the public will care.</p>\n<p><a href=\"https://x.com/rohanpaul_ai/status/1985200765317845313\">AI companion apps use emotional farewells to stop user exit</a>, <a href=\"http://arxiv. org/abs/2508.19258\">causing 15x more engagement after goodbye</a>, using such goodbyes 37% of the time. Humans are sometimes hard to persuade, but sometimes they\u2019re really easy. Well, yeah, that\u2019s what happens when you optimize for engagement.</p>\n<p><a href=\"https://x.com/abby4thepeople/status/1986434967535296931\">It seems there\u2019s a TikTok account where someone tries to scam churches</a> over the phone and then blames the churches for being unwilling to be scammed and taking the correct precautions? Such precautions only become more necessary in the AI age.</p>\n<p>&nbsp;</p>\n\n\n<h4 class=\"wp-block-heading\">Fun With Media Generation</h4>\n\n\n<p><a href=\"https://x.com/EZE3D/status/1985981688933966286\">Coca-Cola generates 70,000 AI clips to put together an AI ad</a>, the result of which was widely derided as soulless. For now, I would strongly urge brands to avoid such stunts. The public doesn\u2019t like it, the downside is large, the upside is small.</p>\n<p><a href=\"https://www.understandingai.org/p/ai-ads-are-going-mainstream\">Such ads are going mainstream</a>, and Kai Williams goes into why Taylor Swift and Coca-Cola, neither exactly strapped for cash, would risk their reputations on this. Kai\u2019s answer is that most people don\u2019t mind or even notice, and he anticipates most ads being AI within a few years.</p>\n<p>I get why Kalshi, who generated the first widespread AI ad, would do this. It fits their brand. I understand why you would use one on late night TV while asking if the viewer was hurt or injured and urging them to call this number.</p>\n<p>What I do not get is why a major brand built on positive reputation, like Coca-Cola or Taylor Swift, would do this now? The cost-benefit or risk-reward calculation boggles my mind. Even if most people don\u2019t consciously notice, now this is a talking point about you, forever, that you caved on this early.</p>\n<p><a href=\"https://x.com/elonmusk/status/1987087713204641988\">Elon Musk has Grok create a video of \u2018she smiles and says \u2018I will always love you.\u2019</a></p>\n<blockquote><p>Jakeup: I\u2019ve been down. I\u2019ve been bad. But I\u2019ve never been this down bad.</p></blockquote>\n<p>Is it weird that I\u2019m worried about Elon, buddy are you okay? If you can\u2019t notice the reasons not to share this here that seems like a really bad sign?</p>\n<p><a href=\"https://x.com/TheStalwart/status/1987603430773576011\">The #1 country</a> <a href=\"https://www.whiskeyriff.com/2025/11/08/an-ai-generated-country-song-is-topping-a-billboard-chart-and-that-should-infuriate-us-all/\">song in America is by digital sales is AI</a>, and it has 2 million monthly listens. Whiskey Riff says \u2018that should infuriate us all,\u2019 but mostly this seems like a blackpill on country music? I listened to half the song, and if I forget it\u2019s AI then I would say it is boring and generic as all hell and there is nothing even a tiny bit interesting about it. It\u2019s like those tests where they submitted fake papers to various journals and they got the papers published.</p>\n<p>Or maybe it\u2019s not a blackpill on country music so much as proof that what people listen for is mostly the lyrical themes, and this happened to resonate? That would be very bad news for human country artists, since the AI can try out everything and see what sticks. The theme might resonate but this is not great writing.</p>\n<p>The same person behind this hit and the artist \u2018Breaking Rust\u2019 seems to also have another label, \u2018Defbeatsai,\u2019 which is the same generic country played completely straight except the sounds are ludicrously obcense, which was funny for the first minute or so as the AI artist seems 100% unaware of the obscenity.</p>\n<p>&nbsp;</p>\n\n\n<h4 class=\"wp-block-heading\">So You\u2019ve Decided To Become Evil</h4>\n\n\n<p>Some of your ads are going to be scams. That\u2019s unavoidable. All you can do is try to detect them as best you can, which AI can help with and then you\u2026 <a href=\"https://www.reuters.com/investigations/meta-is-earning-fortune-deluge-fraudulent-ads-documents-show-2025-11-06/\">wait, charge more?</a></p>\n<p>It\u2019s not quite as bad as it sounds, but it\u2019s really bad. I worry about the incentives to remain ignorant here, but also come on.</p>\n<blockquote><p><a href=\"https://www.reuters.com/investigations/meta-is-earning-fortune-deluge-fraudulent-ads-documents-show-2025-11-06/\">Jeff Horwitz</a> (Reuters): Meta projected 10% of its 2024 revenue would come from ads for scams and banned goods, documents seen by Reuters show. And the social media giant internally estimates that its platforms show users 15 billion scam ads a day. Among its responses to suspected rogue marketers: charging them a premium for ads \u2013 and issuing reports on \u2019Scammiest Scammers.\u2019</p>\n<p>\u2026</p>\n<p>A cache of previously unreported documents reviewed by Reuters also shows that the social-media giant for at least three years failed to identify and stop an avalanche of ads that exposed Facebook, Instagram and WhatsApp\u2019s billions of users to fraudulent e-commerce and investment schemes, illegal online casinos, and the sale of banned medical products.</p>\n<p>Much of the fraud came from marketers acting suspiciously enough to be flagged by Meta\u2019s internal warning systems. But the company only bans advertisers if its automated systems predict the marketers are at least 95% certain to be committing fraud, the documents show. If the company is less certain \u2013 but still believes the advertiser is a likely scammer \u2013 Meta charges higher ad rates as a penalty, according to the documents. The idea is to dissuade suspect advertisers from placing ads.</p>\n<p>The documents further note that users who click on scam ads are likely to see more of them because of Meta\u2019s ad-personalization system, which tries to deliver ads based on a user\u2019s interests.</p>\n<p>Jeremiah Johnson: Seems like a really big deal that 10% of Meta\u2019s revenue comes from outright scams. And that\u2019s their *internal* estimate, who knows what a fair outside report would say. This should shift your beliefs on whether our current social media set up is net positive for humanity.</p>\n<p><a href=\"https://x.com/ArmandDoma/status/1986895321323258290\">Armand Domalewski</a>: the fact that Meta internally identifies ads as scams but then instead of banning them just charges them a premium is so goddam heinous man</p></blockquote>\n<p>The article details Meta doing the same \u2018how much are we going to get fined for this?\u2019 calculation that car manufacturers classically use to decide whether to fix defects. That\u2019s quite a bad look, and also bad business, even if you have no ethical qualms at all. The cost of presenting scam ads, even in a pure business case, is a lot higher than the cost of the regulatory fines, as it decreases overall trust and ad effectiveness for the non-scams that are 90% of your revenue.</p>\n<p>This might be the most damning statement, given that they knew that ~10% of revenue was directly from scams, as it\u2019s basically a \u2018you are not allowed to ban scams\u2019:</p>\n<blockquote><p>In the first half of 2025, a February document states, the team responsible for vetting questionable advertisers wasn\u2019t allowed to take actions that could cost Meta more than 0.15% of the company\u2019s total revenue. That works out to about $135 million out of the $90 billion Meta generated in the first half of 2025.</p>\n<p>\u2026 Meta\u2019s Stone said that the 0.15% figure cited came from a revenue projection document and was not a hard limit.</p></blockquote>\n<p>But don\u2019t worry, their new goal is to cut the share from things that are likely to be fraud (not all of which are fraud, but a lot of them) from 10.1% in 2024 to 7.3% in 2025 and then 5.8% in 2027. That is, they calculated, the optimal amount of scams. We all can agree that the optimal percentage of outright scams is not zero, but this seems high? I don\u2019t mean to pretend the job is easy, but surely we can do better than this?</p>\n<p>Let\u2019s say you think your automated system is well-calibrated on chance of something being fraud. And let\u2019s say it says something has a 50% chance of being fraud (let alone 90%). Why would you think that allowing this is acceptable?</p>\n<p>Presumption of innocence is necessary for criminal convictions. This is not that. If your ad is 50% or 90% to be fraud as per the automated system, then presumably the correct minimum response is \u2018our system flags this as potentially fraud, would you like to pay us for a human review?\u2019 It seems 77% of scams only violate \u2018the spirit of\u2019 Meta policies, and adhere to the letter. It seems that indeed, you can often have a human flagging an account saying \u2018hello, this is fraud,\u2019 have a Meta employee look and go \u2018yep, pretty likely this is fraud\u2019 and then you still can\u2019t flag the account. Huh?</p>\n<p>&nbsp;</p>\n\n\n<h4 class=\"wp-block-heading\"></h4>\n\n\n<p>&nbsp;</p>\n\n\n<h4 class=\"wp-block-heading\">They Took Our Jobs</h4>\n\n\n<p><a href=\"https://x.com/paulnovosad/status/1985794453576221085\">LLMs have wiped out the ability of cover letters to signal job candidate quality</a>, <a href=\"https://t.co/Gr5PgAwMMM\">making hiring the best candidates much less likely</a>.</p>\n<p><a href=\"https://charlesd353.substack.com/p/on-comparative-advantage-and-agi\">Charles Dillon gives the latest explainer in the back and forth</a> over comparative advantage. I liked the explanation here that if we are already doing redistribution so everyone eats (and drinks and breathes and so on) then comparative advantage does mean you can likely get a nonzero wage doing something, at some positive wage level.</p>\n<p>One thing this drives home about the comparative advantage arguments, even more than previous efforts, is that if you take the claims by most advocates seriously they prove too much. As in, they show that any entity, be it animal, person or machine, with any productive capabilities whatever will remain employed, no matter how inefficient or uncompetitive, and survive. We can observe this is very false.</p>\n<p>&nbsp;</p>\n\n\n<h4 class=\"wp-block-heading\">A Young Lady\u2019s Illustrated Primer</h4>\n\n\n<p><a href=\"https://inexactscience.substack.com/p/university-education-as-we-know-it\">An economics PhD teaching at university reports</a> that the AI situation at university is not pretty. Take home assignments are dead, now that we have GPT-5 and Sonnet 4.5 there\u2019s no longer room to create assignments undergraduates can do in reasonable time that LLMs can\u2019t. Students could choose to use LLMs to learn, but instead they choose to use LLMs to not learn, as in complete tasks quickly.</p>\n<blockquote><p>Inexact Science: Students provided perfect solutions but often couldn\u2019t explain <em>why</em> they did what they did. One student openly said \u201cChatGPT gave this answer, but I don\u2019t know why.\u201d</p>\n<p>A single prompt would have resolved that! But many students don\u2019t bother. \u201cOne prompt away\u201d is often one prompt too far.</p></blockquote>\n<p>One prompt would mean checking the work and then doing that prompt every time you didn\u2019t understand. That\u2019s a tough ask in 2025.</p>\n<p>What to do about it? That depends what you\u2019re trying to accomplish. If you\u2019re trying to train critical thinking, build an informed citizenry or expose people to humanity\u2019s greatest achievements, which I believe you mostly aren\u2019t? Then you have a problem. I\u2019d also say you have a problem if it\u2019s signaling, since AI can destroy the signal.</p>\n<p>According to IS, what AI is replacing is the \u2018very core\u2019 of learning, the part where you understand the problem. I say that depends how you use it, but I see the argument.</p>\n<p>The proposal is a barbell strategy.</p>\n<div>\n<figure>\n<div>\n\n\n<figure class=\"wp-block-image\"><img alt=\"\" src=\"https://substackcdn.com/image/fetch/$s_!86Vt!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffa397f11-6d3c-4651-b603-d08a889d8c8e_999x616.webp\" /></figure>\n\n\n<div></div>\n</div>\n</figure>\n</div>\n<p>As in: Everything is either embracing AI, or things done entirely without AI. And the university should focus on the non-AI fundamentals. This seems like a clear marginal improvement, at least, but I\u2019m not convinced on these fundamentals.</p>\n<p>&nbsp;</p>\n\n\n<h4 class=\"wp-block-heading\">Get Involved</h4>\n\n\n<p><a href=\"https://x.com/AnthropicAI/status/1986871933967745344\">Anthropic is opening offices in Paris and Munich</a>.</p>\n<p><a href=\"https://subscriber.politicopro.com/article/2025/11/california-ag-rob-bontas-office-to-hire-ai-expert-00641532\">California\u2019s Attorney General will be hiring an AI expert</a>.</p>\n\n\n<h4 class=\"wp-block-heading\">Introducing</h4>\n\n\n<p><a href=\"https://x.com/kevinafischer/status/1986568591198875926\">Spark, the Magic Dog, an AI living character</a> <a href=\"https://illusionoflife.notion.site/\">kids interact with through</a> a \u2018quantum portal\u2019 (actually an iPad case) in the style of and inspired by Sesame Street, from Kevin Fischer and Pasquale D\u2019Silva.</p>\n<blockquote><p>Spark Families receive:</p>\n<ul>\n<li>Complete Quantum Portal rental equipment (hand-crafted Quantum Portal iPad case + a dedicated iPad).</li>\n<li>The opportunity to invite Spark into your home daily, performed and guided by our world-class puppeteering team.</li>\n<li>Tailored family development programs, designed by best-selling children\u2019s book authors, pediatricians, family interventionists, and Disney imagineers.</li>\n<li>Live 8 years in the future &amp; Magical moments that last a lifetime.</li>\n</ul>\n</blockquote>\n<p>Alas, I\u2019m out, because it\u2019s a dog, and that will never do.</p>\n<p><a href=\"https://engineering.fb.com/2025/11/10/ml-applications/metas-generative-ads-model-gem-the-central-brain-accelerating-ads-recommendation-ai-innovation/\">Of course Meta\u2019s new LLM is GEM, the Generative Ads Recommendation Model</a>.</p>\n\n\n<h4 class=\"wp-block-heading\">In Other AI News</h4>\n\n\n<p><a href=\"https://stratechery.com/2025/apple-earnings-siri-white-labels-gemini-short-term-gains-and-long-term-risk/?access_token=eyJhbGciOiJSUzI1NiIsImtpZCI6InN0cmF0ZWNoZXJ5LnBhc3Nwb3J0Lm9ubGluZSIsInR5cCI6IkpXVCJ9.eyJhdWQiOiJzdHJhdGVjaGVyeS5wYXNzcG9ydC5vbmxpbmUiLCJhenAiOiJIS0xjUzREd1Nod1AyWURLYmZQV00xIiwiZW50Ijp7InVyaSI6WyJodHRwczovL3N0cmF0ZWNoZXJ5LmNvbS8yMDI1L2FwcGxlLWVhcm5pbmdzLXNpcmktd2hpdGUtbGFiZWxzLWdlbWluaS1zaG9ydC10ZXJtLWdhaW5zLWFuZC1sb25nLXRlcm0tcmlzay8iXX0sImV4cCI6MTc2NTM2NDczMiwiaWF0IjoxNzYyNzcyNzMyLCJpc3MiOiJodHRwczovL2FwcC5wYXNzcG9ydC5vbmxpbmUvb2F1dGgiLCJzY29wZSI6ImZlZWQ6cmVhZCBhcnRpY2xlOnJlYWQgYXNzZXQ6cmVhZCBjYXRlZ29yeTpyZWFkIGVudGl0bGVtZW50cyIsInN1YiI6IjAxOTY0MGE3LTNjYzUtNzc1My04MzY4LWZiMjg5MTI0Y2YxMyIsInVzZSI6ImFjY2VzcyJ9.I1NSiqp651JdnVaP_xZoCECoPwUv4W3w1m86fSV8iFUTHZgrPEfFTt2Uyk6U0tiB1Yhc9YURh0kttp5UvCGtG0YoiGN4wdtRFZjHDrRb-c4UV13P2Z-xLREXCXt_WMTY_LyiVJlRg_AN89CbsQWSKhu5CCIsFn5hKjXsQo_xgwZ4Lf_MWVhoxcFDY5b_KJ3Le3WUeWJ166zhADqOuEGgIyKa23Ao1YZIqFKEqDS6rhbFjRtkRU9tkitNikHtpsGgqr3LA7UxigK-Y8BL5RUp9irOTSZzKNbpnkGBN9hiGtNOLXy37diql-wwkOagTj1xtDLF4TSplpIJf1ROe_CYcw\">Ben Thompson offers more of his take on Apple going with Gemini for Siri</a>, in part due to price and partly due to choosing which relationship they prefer, despite Anthropic offering a superior model. I agree that Gemini is \u2018good enough\u2019 for Siri for most purposes. He sees this as Apple wisely bowing out of the AI race, regardless of what Apple tries to tell itself, and this seems correct.</p>\n<p>After being shut out by people who actually believe in LLMs, <a href=\"https://www.ft.com/content/c586eb77-a16e-4363-ab0b-e877898b70de\">Yann LeCun is leaving Meta to form a new AI startup</a>. <a href=\"https://www.bloomberg.com/opinion/newsletters/2025-11-11/the-hedge-funds-are-hiring\">As Matt Levine notes,</a> fundraising is not going to be a problem, and he is presumably about to have equity worth many billions and hopefully (from the perspective of those who give him the billions of dollars) doing AI research.</p>\n<p><a href=\"https://www.bloomberg.com/opinion/articles/2025-11-10/perplexity-is-ai-s-new-pit-bull\">Amazon is suing Perplexity</a> to stop it from browsing Amazon.com, joining many others mad at Perplexity, including for its refusal to identify its browser and in general claim everything on the internet for itself. Perplexity don\u2019t care. They are following the classic tech legal strategy of \u2018oh yeah? make me.\u2019 Let\u2019s see if it works out for them.</p>\n<p><a href=\"https://x.com/deredleritt3r/status/1988244609504334283\">Google releases</a> a <a href=\"https://www.kaggle.com/whitepaper-introduction-to-agents\">new \u2018Introduction to Agents\u2019 guide</a>. Its Level 4 is \u2018the self-evolving system\u2019 that has metareasoning, can act autonomously, and can use that to create new agents and tools.</p>\n<p>Two randomly assigned Anthropic teams, neither of which had any robotics experience, <a href=\"https://x.com/AnthropicAI/status/1988706380480385470\">were asked to program a robot dog</a>, to see how much Claude would speed things up. <a href=\"https://www.anthropic.com/research/project-fetch-robot-dog\">It did, quite a bit</a>, although some subtasks went well for Team Claudeless, more properly Team Do It By Hand rather than not using Claude in particular.</p>\n\n\n<h4 class=\"wp-block-heading\">Show Me the Money</h4>\n\n\n<p><a href=\"https://www.anthropic.com/news/anthropic-invests-50-billion-in-american-ai-infrastructure\">Anthropic invests $50 billion in American AI infrastructure</a>, as in custom built data centers. It will create \u2018800 permanent jobs and 2,400 construction jobs,\u2019 which counts for something but feels so low compared to the money that I wouldn\u2019t have mentioned it. Sounds good to me, only note is I would have announced it on the White House lawn.</p>\n<p><a href=\"https://x.com/tszzl/status/1970965603549864283\">Roon points out that</a> if you take Dan Wang\u2019s book seriously about the value of knowing industrial processes, especially in light of the success of TSMC Arizona, and Meta\u2019s 100M+ pay packages, we should be acquihiring foreign process knowledge, from China and otherwise, for vast sums of money.</p>\n<p>Of course, to do this we\u2019d need to get the current administration willing to deal with the immigration hurdles involved. But if they\u2019ll play ball, and obviously they should, this is the way to move production here in the cases we want to do that.</p>\n<p><a href=\"https://x.com/Snap/status/1986191838529601835\">Snap makes</a> a deal with Perplexity. Raising the questions \u2018there still a Snapchat?\u2019 (yes, there are somehow still 943 million monthly users) and \u2018there\u2019s still a Perplexity?\u2019</p>\n<blockquote><p>Snap: Starting in early 2026, Perplexity will appear in the popular Chat interface for Snapchatters around the world. Through this integration, Perplexity\u2019s AI-powered answer engine will let Snapchatters ask questions and get clear, conversational answers drawn from verifiable sources, all within Snapchat.</p>\n<p>Under the agreement, Perplexity will pay Snap $400 million over one year, through a combination of cash and equity, as we achieve global rollout.</p>\n<p><a href=\"https://x.com/SashaKaletsky/status/1986543368877342768\">Sasha Kaletsky</a>: This deal looks incredibly in Snap\u2019s favour:</p>\n<p>1. Snap get $400m (&gt; Perplexity total revenue)</p>\n<p>2. Snap give nothing, except access to an unloved AI chat</p>\n<p>3. Perplexity get.. indirect access to zero-income teens?</p>\n<p>Spiegel negotiation masterclass, and shows the power of distribution.</p></blockquote>\n<p>Even assuming they\u2019re getting paid in equity, notice the direction of payment.</p>\n<p><a href=\"https://www.bloomberg.com/opinion/newsletters/2025-11-11/the-hedge-funds-are-hiring\">Matt Levine asks which is the long term view</a>, Anthropic trying to turn a profit soon or OpenAI not trying to do so? He says arguably \u2018rush to build a superintelligence is a bit short sighted\u2019 because the AI stakes are different, and I agree it is rather short sighted but only in the \u2018and then everyone probably dies\u2019 sense. In the ordinary business sense that\u2019s the go to move.</p>\n<p><a href=\"https://www.bloomberg.com/news/articles/2025-11-11/softbank-s-profit-surges-after-boost-from-soaring-ai-valuations\">SoftBank sells its Nvidia stake for $5.8 billion</a> to fund AI bets. Presumably SoftBank knows the price of Nvidia is going crazy, but they need to be crazier. Those who are saying this indicates the bubble is popping did not read to the end of the sentence and do not know SoftBank.</p>\n<p><a href=\"https://www.bloomberg.com/opinion/articles/2025-11-11/an-ai-bubble-the-bond-market-is-not-seeing-one?re_source=postr_story_2\">Big tech companies are now using bond deals to finance AI spending</a>, so far to the tune of $93 billion. This is framed as \u2018the bond market doesn\u2019t see an AI bubble\u2019 but these are big tech companies worth trillions. Even if AI fizzles out entirely, they\u2019re good for it.</p>\n\n\n<h4 class=\"wp-block-heading\">Common Ground</h4>\n\n\n<p><a href=\"https://asteriskmag.substack.com/p/common-ground-between-ai-2027-and\">Arvind Narayanan, Daniel Kokotajlo and others find 11 points of common ground</a>, all of which <a href=\"https://x.com/VitalikButerin/status/1988685848163053716\">Vitalik Buterin endorses as well</a>, as do I.</p>\n<ol>\n<li>Before strong AGI, AI will be a normal technology.</li>\n<li>Strong AGI developed and deployed in the near future would not be a normal technology.</li>\n<li>Most existing benchmarks will likely saturate soon.</li>\n<li>AIs may still regularly fail at mundane human tasks; Strong AGI may not arrive this decade.</li>\n<li>AI will be (at least) as big a deal as the internet.</li>\n<li>AI alignment is unsolved.</li>\n<li>AIs must not make important decisions or control critical systems.</li>\n<li>Transparency, auditing, and reporting are beneficial.</li>\n<li>Governments must build capacity to track and understand developments in the AI industry.</li>\n<li>Diffusion of AI into the economy is generally good.</li>\n<li>A secret intelligence explosion \u2014 or anything remotely similar \u2014 would be bad, and governments should be on the lookout for it.</li>\n</ol>\n<p>I think that for 9 out of the 11, any reasonable person should be able to agree, given a common sense definition of \u2018strong AI.\u2019</p>\n<p>If you disagree with any of these except #7 or #10, I think you are clearly wrong.</p>\n<p>If you disagree on #10, I am confident you are wrong, but I can see how a reasonable person might disagree if you see sufficiently large downsides in specific places, or if you think that diffusion leads to faster development of strong AI (or AGI, or ASI, etc). I believe that on the margin more diffusion in the West right now is clearly good.</p>\n<p>That leaves #7, where again I agree with what I think is the intent at least on sufficiently strong margins, while noticing that a lot of people effectively do not agree, as they are pursuing strategies that would inevitably lead to AIs making important decisions and being placed in control of critical systems. For example, the CEO of OpenAI doubtless makes important decisions, yet Sam Altman talked about them having the first AI CEO, and some expressed a preference for an AI over Altman. Albania already has (technically, anyway) an \u2018AI minister.\u2019</p>\n<p><a href=\"https://x.com/TheZvi/status/1988762327794307345\">Also, if you ask them outright, you get more disagreement than agreement</a> via a quick Twitter poll, including an outright support for Claude for President.</p>\n<blockquote><p>Cameron Taylor: Would love to agree, except it would mean putting humans in those positions. Have you met humans?</p></blockquote>\n<p>File that under \u2018people keep asking me how the AI takes over.\u2019 It won\u2019t have to.</p>\n<p>Given how much disagreement there was there <a href=\"https://x.com/TheZvi/status/1988972016662417681\">I decided to ask about all 11 questions</a>.</p>\n\n\n<h4 class=\"wp-block-heading\">Quiet Speculations</h4>\n\n\n<p><a href=\"https://www.bloomberg.com/news/newsletters/2025-11-06/ai-is-about-to-have-its-big-moment-in-politics\">Joe Weisenthal agrees with my prediction that AI will be a big issue in 2028</a>. There are deep cultural issues, there\u2019s various ways AI companies are not doing themselves political favors, there\u2019s water and electricity, there\u2019s the job market, there\u2019s the bubble fears and possibility of bailouts.</p>\n<p><a href=\"https://openai.com/index/ai-progress-and-recommendations/\">OpenAI doubles down once again</a> on the absurd \u2018AI will do amazing things and your life won\u2019t change,\u2019 before getting into their recommendations for safety. These people\u2019s central goal is literally to build superintelligence, and they explicitly discuss superintelligence in the post.</p>\n<ol>\n<li>\u201cShared standards and insights from the frontier labs.\u201d\n<ol>\n<li>Yes, okay, sure.</li>\n</ol>\n</li>\n<li>\u201cAn approach to public oversight and accountability commensurate with capabilities, and that promotes positive impacts from AI and mitigates the negative ones.\u201d\n<ol>\n<li>They did the meme. Like, outright, they just straight did the meme.</li>\n<li>They then divide this into \u2018two schools of thought about AI\u2019: \u2018normal technology\u2019 versus superintelligence.</li>\n<li>More on this later. Hold that thought.</li>\n</ol>\n</li>\n<li>\u201cBuilding an AI resilience ecosystem.\u201d\n<ol>\n<li>As in, something similar to how the internet has its protections for cybersecurity (software, encryption protocols, standards, monitoring systems, emergency response teams, etc).</li>\n<li>Yes, okay, sure. But you understand why it can\u2019t serve the full function here?</li>\n</ol>\n</li>\n<li>\u201cOngoing reporting and measurement from the frontier labs and governments on the impacts of AI.\u201d\n<ol>\n<li>Yes, okay, sure.</li>\n<li>Except yes, they do mean the effect on jobs, they are doing the meme again, explicitly talking only about the impact on jobs?</li>\n<li>Maybe using some other examples would have helped reassure here?</li>\n</ol>\n</li>\n<li>\u201cBuilding for individual empowerment.\u201d\n<ol>\n<li>As in, AI will be \u2018on par with electricity, clean water or food\u2019.</li>\n<li>I mean, yes, but if you want to get individual empowerment the primary task is not to enable individual empowerment, it\u2019s to guard against disempowerment.</li>\n</ol>\n</li>\n</ol>\n<p>Now to go into the details of their argument on #2.</p>\n<p>First, on current level AI, they say it should diffuse everywhere, and that there should be \u2018minimal additional regulatory burden,\u2019 and warn against a \u201850 state patchwork\u2019 which is a de facto call for a moratorium on all state level regulations of any kind, given the state of the political rhetoric.</p>\n<p>What government actions do they support? Active help and legal protections. They want \u2018promoting innovation\u2019 and privacy protections for AI conversations. They also want \u2018protections against misuse\u2019 except presumably not if it required a non-minimal additional regulatory burden.</p>\n<p>What about for superintelligence? More innovation. I\u2019ll quote in full.</p>\n<blockquote><p>The other one is where superintelligence develops and diffuses in ways and at a speed humanity has not seen before. Here, we should do most of the things above, but we also will need to be more innovative.</p>\n<p>If the premise is that something like this will be difficult for society to adapt to in the \u201cnormal way,\u201d we should also not expect typical regulation to be able to do much either.</p>\n<p>In this case, we will probably need to work closely with the executive branch and related agencies of multiple countries (such as the various safety institutes) to coordinate well, particularly around areas such as mitigating AI applications to bioterrorism (and using AI to detect and prevent bioterrorism) and the implications of self-improving AI.</p>\n<p>The high-order bit should be accountability to public institutions, but how we get there might have to differ from the past.</p></blockquote>\n<p>You could cynically call this an argument against regulation no matter what, since if it\u2019s a \u2018normal technology\u2019 you don\u2019t want to burden us with it, and if it\u2019s not normal then the regulations won\u2019t work so why bother.</p>\n<p>What OpenAI says is that rather than use regulations, as in rather than this whole pesky \u2018pass laws\u2019 or \u2018deal with Congress\u2019 thing, they think we should instead rely on the executive branch and related agencies to take direct actions as needed, to deal with bioterrorism and the implications of self-improving AI.</p>\n<p>So that is indeed a call for zero regulations or laws, it seems?</p>\n<p>Not zero relevant government actions, but falling back on the powers of the executive and their administrative state, and giving up entirely on the idea of a nation of (relevant) laws. Essentially the plan is to deal with self-improving AI by letting the President make the decisions, because things are not normal, without a legal framework? That certainly is one way to argue for doing nothing, and presumably the people they want to de facto put in charge of humanity\u2019s fate will like the idea.</p>\n<p>But also, that\u2019s all the entire document says about superintelligence and self-improving AI and what to do about it. There\u2019s no actual recommendation here.</p>\n<p>Roon and Max Harms suggest a model where China is a fast follower by design, <a href=\"https://x.com/raelifin/status/1986574241928810739\">they wait for others to do proof of concept and </a>don\u2019t have speculative risk capital, so no they\u2019re not capable of being \u2018secretly ahead\u2019 or anything like that. <a href=\"https://x.com/GordonJohnson19/status/1986469261028827638\">As for why they\u2019re not building more data centers</a>, they\u2019re adding what they can but also they need worthwhile GPUs to put in them.</p>\n<p><a href=\"https://x.com/Research_FRI/status/1987883190803964334\">Forecasting Research Institute comes out</a> with the<a href=\"https://leap.forecastingresearch.org/\"> latest set of predictions of AI</a> impacts. This is another in the line of what I see as so-called \u2018superforecasters\u2019 and similar others refusing to take future AI capabilities seriously.</p>\n<p>A common argument against AI are bottlenecks, or saying that \u2018what we really need is [X] and AI only gives us [Y].\u2019<a href=\"https://x.com/RuxandraTeslo/status/1987516850909892953\"> In this case</a>, [X] is better predictive validity and generation of human data, and [Y] is a deluge of new hypotheses, at least if we go down the \u2018slop route\u2019 of spitting out candidates.</p>\n<blockquote><p>Ruxandra Teslo: But increasing predictive validity with AI is not going to come ready out of a box. It would require generating types of data we mostly do not have at the moment. AI currently excels at well-bounded problems with a very defined scope. Great, but usually not transformational.</p>\n<p>By contrast, AI is not very well positioned to improve the most important thing we care about, predictive validity. That is mostly because it does not have the right type of data.</p></blockquote>\n<p>What I always find weirdest in such discussions is when people say \u2018AI won\u2019t help much but [Z] would change the game,\u2019 where for example here [Z] from Jack Scannell is \u2018regulatory competition between America and China.\u2019 I agree that regulatory changes could be a big deal, but this is such a narrow view of AI\u2019s potential, and I agree that AI doesn\u2019t \u2018bail us out\u2019 of the need for regulatory changes.</p>\n<p>Whereas why can\u2019t AI improve predictive validity? It already does in some contexts via AlphaFold and other tools, and I\u2019m willing to bet that \u2018have AI competently consider all of the evidence we already have\u2019 actually does substantially improve our success estimates today. I also predict that AI will soon enable us to design better experiments, which allows a two step, where you run experiments that are not part of the official process, then go back and do the official process.</p>\n<p>The thesis here in the OP is that we\u2019re permanently stuck in the paradigm of \u2018AI can only predict things when there is lots of closely related data.\u2019 Certainly that helps, especially in the near term, but this is what happens:</p>\n<blockquote><p><a href=\"https://x.com/RuxandraTeslo/status/1987567304960303370\">Ruxandra Teslo:</a> The convo devolved to whether scaling Von Neumann would change drug discovery.</p>\n<div>\n<figure>\n<div>\n\n\n<figure class=\"wp-block-image\"><img alt=\"\" src=\"https://substackcdn.com/image/fetch/$s_!XMRW!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc8f2fafd-6eae-473d-bb0e-6a8462d71d07_1107x487.jpeg\" /></figure>\n\n\n<div></div>\n</div>\n</figure>\n</div>\n</blockquote>\n<p>The answer is yes. If you think the answer is no, you\u2019re wrong.</p>\n<p>(Also, you could do this via many other methods, including \u2018take over the government.\u2019)</p>\n<p><a href=\"https://x.com/tszzl/status/1987424692907745316\">Roon warns about The Borg as a failure mode of \u2018The Merge</a>\u2019 with AI, where everything is slop and nothing new comes after, all you can do is take from the outside. The Merge and cyborgism don\u2019t look to be competitive, or at least not competitive for long, and seem mostly like a straw people grasp at. There\u2019s no reason that the human keeps contributing value for long.</p>\n<p>The same is true of the original Borg, why are they still \u2018using humanoids\u2019 as their base? Also, why wouldn\u2019t The Borg be able to innovate? Canonically they don\u2019t innovate beyond assimilating cultural and technological distinctiveness from outside, but there\u2019s no particular reason The Borg can\u2019t create new things other than plot forcing them to only respond to outside stimuli and do a variety of suboptimal things like \u2018let away teams walk on your ship and not respond until a particular trigger.\u2019</p>\n<p><a href=\"https://x.com/hjalmarwijk/status/1988070278149353894?s=46\">When would AI systems \u2018defeat all of us combined</a>\u2019? It\u2019s a reasonable intuition pump question, with the default answer looking like some time in the 2030s, with the interesting point being which advances and capabilities and details would matter. Note of course that when the time comes, there will not be an \u2018all of us combined\u2019 fighting back, no matter how dire the situation.</p>\n\n\n<h4 class=\"wp-block-heading\">\u2018AI Progress Is Slowing Down\u2019 Is Not Slowing Down</h4>\n\n\n<p>That thing where everyone cites the latest mostly nonsense point that \u2018proves\u2019 that AI is going to fail, or isn\u2019t making progress, or isn\u2019t useful for anything? Yep, it\u2019s that.</p>\n<p>From the people that brought you \u2018model collapse\u2019 ruled out synthetic data forever, and that GPT-5 proved that AGI was far, far away, and also the DeepSeek moment comes the \u2018MIT paper\u2019 (as in, one person was associated with MIT) that had a misleading headline that 95% of AI projects fail within enterprises.</p>\n<blockquote><p><a href=\"https://x.com/krishnanrohit/status/1988294827679903779\">Rohit</a>: I think folks who know better, esp on twitter, are still underrating the extreme impact the MIT paper had about 95% of AI projects failing within enterprises. I keep hearing it over and over and over again.</p>\n<p>[It] assuage[d] the worries of many that AI isn\u2019t being all that successful just yet.</p>\n<p>It gives ammunition to those who would\u2019ve wanted to slow play things anyway and also caused pauses at cxo levels.</p>\n<p>Garrison Lovely: The funny thing is that reading the study undermines what many people take away from it (like cost savings can be huge and big enough to offset many failed pilots).</p>\n<p>Rohit: Reading the study?</p>\n<p><a href=\"https://x.com/kevinroose/status/1988337817853059438\">Kevin Roose</a>: This is correct, and also true of every recent AI paper (the METR slow-down study, the Apple reasoning one) that casts doubt on AI\u2019s effectiveness. People are desperate to prove that LLMs don\u2019t work, aren\u2019t useful, etc. and don\u2019t really care how good the studies are.</p>\n<p><a href=\"https://x.com/deanwball/status/1988330319674806484\">Dean Ball</a>: it is this year\u2019s version of the \u201cmodel collapse\u201d paper which, around this time last year, was routinely cited by media to prove that model improvements would slow down due to the lack of additional human data.</p>\n<p>(Rohit\u2019s right: you hear the MIT paper cited all the time in DC)</p>\n<p>Andrew Mayne: It\u2019s crazy because at the time the paper was demonstrably nonsense and the introduction of the reasoning paradigm was largely ignored.</p>\n<p>People also overlook that academic papers are usually a year or or behind in their evaluations of model techniques \u2013 which in AI time cycles is like being a decade behind.</p></blockquote>\n<p>People are desperate for that story that tells them that AI companies are screwed, that AI won\u2019t work, that AI capabilities won\u2019t advance. They\u2019ll keep trying stories out and picking up new ones. It\u2019s basically a derangement syndrome at this point.</p>\n\n\n<h4 class=\"wp-block-heading\">Bubble, Bubble, Toil and Trouble</h4>\n\n\n<p>If there\u2019s nothing to short then in what sense is it a bubble?</p>\n<blockquote><p><a href=\"https://x.com/nearcyan/status/1987014393918562732\">Near</a>: oh so just like the scene in the big short yeah gotcha</p>\n<p>the disappointing part is i dont think theres anything to actually reliably short (aside from like, attention spans and the birth rate and gen alpha and so on) so i feel kinda stupid loving this movie so much throughout the ai bubble.</p></blockquote>\n<p>It\u2019s scary to short a bubble, but yeah, even in expectation what are you going to short? The only category I would be willing to short are AI wrapper companies that I expect to get overrun by the frontier labs, but you can get crushed by an acquihire even then.</p>\n<p>Whereas mad props to <a href=\"https://x.com/GaryMarcus/status/1987553007408443715\">Gary Marcus for putting his money where his mouth is and shorting OpenAI</a>. <a href=\"https://x.com/bennpeifert/status/1985125412289351753\">Ben Eifert is doing it too</a> and facilitating, which is much better than the previous option of trusting <a href=\"https://x.com/GaryMarcus/status/1987544006189068658\">Martin Shkreli</a>.</p>\n<p>Critic\u2019s note: The Big Short is great, although not as good as Margin Call.</p>\n<p>Guess who does think it\u2019s a bubble? Michael Burry, <a href=\"https://www.bloomberg.com/opinion/articles/2025-11-11/ai-bubble-is-ignoring-big-short-michael-burry-chip-depreciation-fears?re_source=postr_story_3\">aka the Big Short Guy</a>, who claims the big players are underestimating depreciation. I do not think they are doing that, as I\u2019ve discussed before, and the longer depreciation schedules are justified.</p>\n<blockquote><p>Chris Bryant: The head of Alphabet Inc.\u2019s AI and infrastructure team, Amin Vahdat, has said that its seven- and eight-year-old custom chips, known as TPUs, have <a href=\"https://www.datacenterdynamics.com/en/news/google-says-tpu-demand-is-outstripping-supply-claims-8yr-old-hardware-iterations-have-100-utilization/\">\u201c100% utilization.\u201d</a></p></blockquote>\n<p>Nvidia reliably makes silly claims, such as:</p>\n<blockquote><p>Chief Executive Officer Jensen Huang said in March that once next-generation Blackwell chips start shipping <a href=\"https://www.businessinsider.com/nvidia-ceo-jensen-huang-joke-blackwell-hopper-gpu-customers-2025-3\">\u201cyou couldn\u2019t give Hoppers away\u201d</a>, referring to the prior model.</p></blockquote>\n<p>Oh, really? I\u2019ll take some Hoppers. Ship them here. I mean, he was joking, but I\u2019m not, gimme some A100s. For, you know, personal use. I\u2019ll run some alignment experiments.</p>\n\n\n<h4 class=\"wp-block-heading\">The Quest for Government Money</h4>\n\n\n<p>David Sacks is right on this one: No bailouts, no backstops, no subsidies, no picking winners. Succeed on your own merits, if you don\u2019t others will take your place. The government\u2019s job is to not get in the way on things like permitting and power generation, and to price in externalities and guard against catastrophic and existential risks, and to itself harness the benefits. That\u2019s it.</p>\n<blockquote><p><a href=\"https://x.com/DavidSacks/status/1986476840207122440\">David Sacks</a>: There will be no federal bailout for AI. The U.S. has at least 5 major frontier model companies. If one fails, others will take its place.</p>\n<p>That said, we do want to make permitting and power generation easier. The goal is rapid infrastructure buildout without increasing residential rates for electricity.</p>\n<p>Finally, to give benefit of the doubt, I don\u2019t think anyone was actually asking for a bailout. (That would be ridiculous.) But company executives can clarify their own comments.</p></blockquote>\n<p>Given his rhetorical style, I think it\u2019s great that Sacks is equating a backstop to a bailout, saying that it would be ridiculous to ask for a bailout and pretending of course no one was asking for one. That\u2019s the thing about a backstop, or any other form of guarantee. Asking for a commitment hypothetical future bailout if conditions require it is the same as asking for a bailout now. Which would be ridiculous.</p>\n<p>What was OpenAI actually doing asking for one anyway? Well, in the words of a wise sage, I\u2019m just kiddin baby, unless you\u2019re gonna do it.</p>\n<p>They also say, in the words of another wise sage, \u2018I didn\u2019t do it.\u2019</p>\n<blockquote><p><a href=\"https://x.com/sama/status/1986514377470845007\">Sam Altman</a> (getting community noted): I would like to clarify a few things.</p>\n<p>First, the obvious one: we do not have or want government guarantees for OpenAI datacenters. We believe that governments should not pick winners or losers, and that taxpayers should not bail out companies that make bad business decisions or otherwise lose in the market. If one company fails, other companies will do good work.</p>\n<p>What we do think might make sense is governments building (and owning) their own AI infrastructure, but then the upside of that should flow to the government as well. We can imagine a world where governments decide to offtake a lot of computing power and get to decide how to use it, and it may make sense to provide lower cost of capital to do so. Building a strategic national reserve of computing power makes a lot of sense. But this should be for the government\u2019s benefit, not the benefit of private companies.</p>\n<p>The one area where we have discussed loan guarantees is as part of supporting the buildout of semiconductor fabs in the US, where we and other companies have responded to the government\u2019s call and where we would be happy to help (though we did not formally apply).</p>\n<p>[he then goes into more general questions about OpenAI\u2019s growth and spending.]</p>\n<p><a href=\"https://x.com/jachiam0/status/1986583797492818244\">Joshua Achiam</a>: Sam\u2019s clarification is good and important. Furthermore &#8211; I don\u2019t think it can be overstated how critical compute will become as a national strategic asset. It is so important to build. It is vitally important to the interests of the US and democracy broadly to build tons of it here.</p>\n<p><a href=\"https://x.com/iamgingertrash/status/1986649332599410820\">Simp 4 Satosh</a>i: Here is an OpenAI document submitted one week ago where they advocate for including datacenter spend within the \u201cAmerican manufacturing\u201d umbrella. There they specifically advocate for Federal loan guarantees.</p>\n<p>Sam Lied to everyone, again.</p>\n<div>\n<figure>\n<div>\n\n\n<figure class=\"wp-block-image\"><img alt=\"\" src=\"https://substackcdn.com/image/fetch/$s_!J2YX!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1414bf3f-8acb-495b-aabb-f81a47ed3d07_1206x1502.jpeg\" /></figure>\n\n\n<div></div>\n</div>\n</figure>\n</div>\n</blockquote>\n<p>If all OpenAI was calling for was loan guarantees for semiconductor manufacturing under the AIMC, that would be consistent with existing policy and a reasonable ask.</p>\n<p>But the above is pretty explicit? They want to expand the AMIC to \u2018AI data centers.\u2019 This is distinct from chip production, and the exact thing they say they don\u2019t want. They want data centers to count as manufacturing. They\u2019re not manufacturing.</p>\n<p>My reading is that most of the statement was indeed in line with government thinking, but that the quoted line above is something very different.</p>\n<p><a href=\"https://x.com/deanwball/status/1986822108895191154\">Dean Ball summarized the situation so far</a>. I agree with him that the above submission was mostly about manufacturing, but the highlighted portion remains. I am sympathetic about the comments made by Sam Altman in the conversation with Tyler Cowen, both because Tyler Cowen prompted it and because Altman was talking about a de facto inevitable situation more than asking for an active policy, indeed they both wisely and actively did not want this policy as I understand their statements on the podcast. Dean calls the proposal \u2018not crazy\u2019 whereas I think it actually is pretty crazy.</p>\n<p>As Dean suggests, there are good mechanisms for government to de-risk key manufacturing without taking on too much liability, and I agree that this would be good if implemented sufficiently well. As always, think about the expected case.</p>\n<p><a href=\"https://x.com/sama/status/1986917979343495650\">Sam Altman then tried again to defend OpenAI\u2019s actions</a>, saying their public submission above was in line with government expectations and priorities and this is super different then loan guarantees to OpenAI.</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p>\n\n\n<h4 class=\"wp-block-heading\"></h4>\n\n\n<p>&nbsp;</p>\n\n\n<h4 class=\"wp-block-heading\">Chip City</h4>\n\n\n<p>On November 7 America <a href=\"https://www.reuters.com/world/china/us-block-nvidias-sale-scaled-back-ai-chips-china-information-says-2025-11-07/?utm_source=chatgpt.com\">formally moved to block B30A chip sales to China</a>.</p>\n<p><a href=\"https://x.com/daniel_271828/status/1988715310338515118\">The Wall Street Journal,</a> which often prints rather bad faith editorials urging chip sales to China, <a href=\"https://www.wsj.com/tech/ai/china-us-ai-chip-restrictions-effect-275a311e\">noes that the chip restrictions are biting in China</a>, and China is intervening to direct who gets what chips it does have. It also confirms this delayed DeepSeek, and that even the most aggressive forecasts for Chinese AI chip production fall far behind their domestic demand.</p>\n<p><a href=\"https://x.com/daniel_271828/status/1988458417758105729\">Steve Bannon is grouping David Sacks in with Jensen Huang</a> in terms of giving chips to the CCP, saying both are working in the CCP\u2019s interests. He is paying attention.</p>\n<p>(He is also wrong about many things, such as in this clip his wanting to exclude Chinese students from our universities because he says they all must be spies &#8211; we should be doing the opposite of this.)</p>\n<p><a href=\"https://x.com/TetraspaceWest/status/1986545127536357692\">Nvidia CEO Jensen Huang tries changing his story</a>.</p>\n<blockquote><p>Jensen Huang: As I have long said, China is nanoseconds behind America in AI. It\u2019s vital that America wins by racing ahead and winning developers worldwide.</p></blockquote>\n<p>So at this point, Jensen Huang has said, remarkably recently\u2026</p>\n<ol>\n<li><a href=\"https://sherwood.news/markets/nvidia-slumps-as-jensen-huang-warns-that-china-will-win-the-ai-race-versus/\">China will win the AI race</a> because of electrical power and regulation.</li>\n<li>China is nanoseconds behind America in AI.</li>\n<li>It doesn\u2019t matter who wins the AI race</li>\n</ol>\n<p>Nanoseconds behind is of course Obvious Nonsense. He\u2019s not even pretending. Meanwhile what does he want to do? Sell China his best chips, so that they can take advantage of their advantages in power generation and win the AI race.</p>\n<p><a href=\"https://x.com/AskPerplexity/status/1985794565987615200\">AI data centers\u2026 IN SPACE</a>. Wait, what? Google plans to launch in 2027, and take advantage of solar power and presumably the lack of required permitting. I feel like this can\u2019t possibly be a good idea on this timeframe, but who the hell knows.</p>\n<p><a href=\"https://www.hyperdimensional.co/p/dont-overthink-the-ai-stack\">Dean Ball tells us not to overthink the \u2018AI tech stack.</a>\u2019 He clarifies that what this means to him is primarily facilitating the building American AI-focused datacenters in other countries, and to bring as much compute as possible under the umbrella of being administered by American companies or subject to American policies, and to send a demand signal to TSMC to ramp up capacity. And we want those projects to run American models, not Chinese models.</p>\n<blockquote><p>Dean Ball: But there is one problem: simply building data centers does not, on its own, satisfy all of the motivations I\u2019ve described. We could end up constructing data centers abroad\u2014and even using taxpayer dollars to subsidize that construction through development finance loans\u2014only to find that the infrastructure is being used to run models from China or elsewhere. That outcome would mean higher sales of American compute, but would not be a significant strategic victory for the United States. If anything, it would be a strategic loss.</p></blockquote>\n<p>This is the sane version of the American \u2018tech stack\u2019 argument. This actually makes sense. You want to maximize American-aligned compute capacity that is under our direction and that will run our models, including capacity physically located abroad.</p>\n<p>This is a \u2018tech stack\u2019 argument against selling American chips to China, or to places like Malaysia where those chips would not be secured, and explicitly does not want to build Nvidia data centers that will then run Chinese models, exactly because, as Dean Ball says, that is a clear strategic loss, not a win. An even bigger loss would be selling them chips they use to train better models.</p>\n<p>The stack is that American companies make the chips, build the data centers, operate the data centers and then run their models. You create packages and then customers can choose a full stack package from OpenAI or Google or Anthropic, and their partners. And yes, this seems good, provided we do have sufficiently secure control over the datacenters, in all senses including physical.</p>\n<p>I contrast this with the \u2018tech stack\u2019 concept from Nvidia or David Sacks, where the key is to prevent China from running its Chinese models on Chinese chips, and thinking that if they run their models on Nvidia chips this is somehow net good for American AI models and their share of global use. It very obviously isn\u2019t. Or that this would slow down Chinese access to compute over the medium term by slowing down Huawei. It very obviously wouldn\u2019t.</p>\n\n\n<h4 class=\"wp-block-heading\">The Week in Audio</h4>\n\n\n<p><a href=\"https://x.com/dwarkesh_sp/status/1988656226989699138\">Dwarkesh Patel interviews Microsoft CEO Satya Nadella</a>. I haven\u2019t had a chance to listen yet, some chance this is good enough for a coverage post. Relatedly, <a href=\"https://newsletter.semianalysis.com/p/microsofts-ai-strategy-deconstructed\">Dylan Patel and others here break down Microsoft\u2019s AI strategy</a>.</p>\n<p><a href=\"https://x.com/a16z/status/1986916722595799316\">Adam D\u2019Angelo goes on an a16z podcast with Amjad Masad</a>. I mainly mention this because it points out an important attribute of Adam D\u2019Angelo, and his willingness to associate directly with a16z like this provides context for his decision as a member of the OpenAI board to fire Sam Altman, and what likely motivated it.</p>\n<p><a href=\"https://80000hours.org/podcast/episodes/holden-karnofsky-concrete-ai-safety-frontier-ai-companies/\">Holden Karnofsky on 80,000 Hours</a>. I listened to about half of this so far, I agreed with some but far from all of it, but mostly it feels redundant if you\u2019ve heard his previous interviews.</p>\n<p><a href=\"https://80000hours.org/podcast/episodes/helen-toner-ai-policy-washington-dc/\">Helen Toner on 80,000 Hours</a>, on the geopolitics of AI.</p>\n<p><a href=\"https://80000hours.org/podcast/episodes/tyler-whitmer-openai-saved-attorneys-general/\">Tyler Whitmer on 80,000 Hours</a>, on the OpenAI nonprofit, breaking down the transition, what was lost and what was preserved.</p>\n\n\n<h4 class=\"wp-block-heading\">Rhetorical Innovation</h4>\n\n\n<blockquote><p><a href=\"https://x.com/tegmark/status/1987176451209052216\">Elon Musk</a>: Long term, A.I. is going to be in charge, to be totally frank, not humans.</p>\n<p>So we just need to make sure the A.I. is friendly.</p>\n<p>Max Tegmark: Elon says the quiet part out loud: instead of focusing on controllable AI tools, AI companies are racing toward a future where machines are in charge. If you oppose this, please join about 100,000 of us as a signatory at <a href=\"https://superintelligence-statement.org\">https://superintelligence-statement.org</a>.</p>\n<p><a href=\"https://x.com/RonDeSantis/status/1986951624082813072\">Ron DeSantis</a> (Governor of Florida, QTing the Musk quote): Why would people want to allow the human experience to be displaced by computers?</p>\n<p>As a creation of man, AI will not be divorced from the flaws of human nature; indeed, it is more likely to magnify those flaws.</p>\n<p>This is not safe; it is dangerous.</p></blockquote>\n<p>Ron DeSantis has been going hard at AI quite a lot, trying out different language.</p>\n<p><a href=\"https://x.com/MattWalshBlog/status/1988698419997675964\">Here\u2019s another concerned person I didn\u2019t have on any bingo cards</a>:</p>\n<blockquote><p>Matt Walsh: AI is going to wipe out at least 25 million jobs in the next 5 to 10 years. Probably much more. It will destroy every creative field. It will make it impossible to discern reality from fiction. It will absolutely obliterate what\u2019s left of the education system. Kids will go through 12 years of grade school and learn absolutely nothing. AI will do it all for them. We have already seen the last truly literate generation.</p>\n<p>All of this is coming, and fast. There is still time to prevent some of the worst outcomes, or at least put them off. But our leaders aren\u2019t doing a single thing about any of this. None of them are taking it seriously. We\u2019re sleepwalking into a dystopia that any rational person can see from miles away. It drives me nuts. Are we really just going to lie down and let AI take everything from us? Is that the plan?</p></blockquote>\n<p>Yes. That is the plan, in that there is no plan. And yes, by default it ends up taking everything from us. Primarily not in the ways Matt is thinking about. If we have seen the last literate generation it will be because we may have literally seen the last generation. Which counts. But many of his concerns are valid.</p>\n<p><a href=\"https://www.amazon.com/Red-Heart-Max-Harms/dp/108822119X/ref=sr_1_1?crid=3NU3523VW19PW&amp;dib=eyJ2IjoiMSJ9.th38PmUSo5UHVHb8OKS5W37SdlxWX1sRRy7-r_Trtso.42_lUEUpCk_7bMNj8AfYzo_nmhPrxrlZAAhzIVZ7Kl0&amp;dib_tag=se&amp;keywords=red+heart+max+harms&amp;qid=1762863805&amp;sprefix=red+heart+max+harm%2Caps%2C511&amp;sr=8-1\">Max Harms, MIRI researcher and author of Crystal Society, releases Red Heart, a spy novel about Chinese AGI</a>. <a href=\"https://raelifin.substack.com/p/red-heart-is-now-available\">He introduces his book here.</a></p>\n<p><a href=\"https://www.lesswrong.com/posts/YuKktzP9yGbdrd7ry/gradientdissenter-s-shortform?commentId=4KyRvmWRDoieKNGg5\">Graident Dissenter warns us that the cottage industry</a> of sneering, gawking and malinging the AI safety community and the very concept of wanting to not die is likely going to get even worse with the advent of the new super PACs, plus I would add the increase in the issue\u2019s salience and stakes.</p>\n<p>In particular, he warns that the community\u2019s overreactions to this could be the biggest danger, and that the community should not walk around in fear of provoking the super PACs.</p>\n<p><a href=\"https://x.com/krishnanrohit/status/1987018487001457141\">Periodically people say (here it is Rohit</a>) some version of \u2018you have to balance safety with user experience or else users will switch to unsafe models.\u2019 Yes, obviously, with notably rare exceptions everyone involved understands this.</p>\n<p>That doesn\u2019t mean you can avoid having false positives, where someone is asking for something for legitimate purposes, it is pretty obvious (or seems like it should be) from context it is for legitimate purposes, and the model refuses anyway, and this ends up being actually annoying.</p>\n<p>The example here is Armin Ronacher wants to debug by having a health form filled out with yes in every box to debug PDF editing capabilities, and Claude is refusing. I notice that yes Claude is being pedantic here but if you\u2019re testing PDF editing and the ability to tick boxes it should be pretty easy to create a form where this isn\u2019t an issue that tests the same thing?</p>\n<p>If you give models the ability to make exceptions, you don\u2019t only have to make this reliable by default. You have to worry about adversarial examples, where the user is trying to use the exceptions to fool the model. This isn\u2019t as easy as it looks, and yeah, sometimes you\u2019re going to have some issues.</p>\n<p>The good news is I see clear improvement over time, and also larger context helps a lot too. I can\u2019t remember Claude or ChatGPT giving me a refusal except when I outright hit the Anthropic classifiers, which happens sometimes when I\u2019m asking questions about the biofilters and classifiers themselves and frankly, ok, fair.</p>\n<p>As an other example from the thread, <a href=\"https://x.com/alex_harl/status/1987153462065352988\">Alex Harl was trying to have Claude do his data protection training as a test</a>, and it said no, and he\u2019s laughing but I\u2019m rather sympathetic to the refusal here, it seems good?</p>\n<p>As is often the case, those worried about AI (referred to here at first using the slur, then later by their actual name) are challenged with \u2018hey you didn\u2019t predict this problem, did you?\u2019 when they very obviously did.</p>\n<p><a href=\"https://marginalrevolution.com/marginalrevolution/2025/11/saturday-assorted-links-534.html#comments\">Tyler Cowen links back to my coverage of his podcast with Altman, calls me Zvi (NN)</a>, which in context is honestly pretty funny but also clarifies what he is rhetorically up to with the NN term, that he is not mainly referring to those worried about jobs or inequality or Waymos running over cats. I accept his response that Neruda has to be read in Spanish or it is lame, but that means we need an English-native example to have a sense of the claims involved there.</p>\n<p>If you\u2019re not nervous about AI? You\u2019re not paying attention.</p>\n<p>You know the joke where there are two Jews and one of them says he reads the antisemitic papers because there it tells him the Jews run everything? That\u2019s how I feel when I see absurdities like this:</p>\n<blockquote><p><a href=\"https://x.com/DavidSacks/status/1987885939650441706\">David Sacks</a> (being totally out to lunch at best): AI Optimism \u2014 defined as seeing AI products &amp; services as more beneficial than harmful \u2014 is at 83% in China but only 39% in the U.S. This is what those EA billionaires bought with their propaganda money.</p></blockquote>\n<p>He doesn\u2019t seriously think this had anything to do with EA or anything related to it, does he? I mean are you kidding me? Presumably he\u2019s simply lying as per usual.</p>\n<p><a href=\"https://x.com/AndrewCritchPhD/status/1988137885095313552\">You can try to salvage this</a> by turning it into some version of \u2018consumer capitalist AI products are good actually,\u2019 which I think is true for current products, but that\u2019s not at all the point Sacks is trying to make here.</p>\n<p><a href=\"https://x.com/theallinpod/status/1987228785536864492\">Similarly, on the All-In podcast</a>, Brad Gerstner points out AI is becoming deeply unpopular in America, complaining that \u2018doomers are now scaring people about jobs,\u2019 confirming that the slur in question is simply anyone worried about anything. But once again, \u2018in China they\u2019re not going to slow down.\u2019 They really love to beat on the \u2018slow down\u2019 framework, in a \u2018no one, actual no one said that here\u2019 kind of way.</p>\n<p>Who wants to tell them that the part where people are scared about jobs is people watching what the AI companies do and say, and reading about it in the news and hearing comedians talk about it and so on, and responding by being worried about their jobs?</p>\n\n\n<h4 class=\"wp-block-heading\">Galaxy Brain Resistance</h4>\n\n\n<p><a href=\"https://vitalik.eth.limo/general/2025/11/07/galaxybrain.html\">That is the latest Vitalik Buterin post</a>, warning against the danger of being clever enough to argue for anything, and especially against certain particular forms. If you\u2019re clever enough to argue for anything, there\u2019s a good chance you first chose the anything, then went and found the argument.</p>\n<p>AI is not the central target, but it comes up prominently.</p>\n<p>Here\u2019s his comments about the inevitability fallacy, as in \u2018eventually [X] will happen, so we must make [X] happen faster.\u2019</p>\n<blockquote><p>Vitalik Buterin: Now, inevitabilism is a philosophical error, and we can refute it philosophically. If I had to refute it, I would focus on three counterarguments:</p>\n<ul>\n<li><strong>Inevitabilism overly assumes a kind of infinitely liquid market</strong> where if you don\u2019t act, someone else will step into your role. Some industries are <em>sort of</em> like that. But AI is the exact opposite: it\u2019s an area where a large share of progress is being made by very few people and businesses. If one of them stops, things really would appreciably slow down.</li>\n<li><strong>Inevitabilism under-weights the extent to which people make decisions collectively</strong>. If one person or company makes a certain decision, that often sets an example for others to follow. Even if no one else follows immediately, it can still set the stage for more action further down the line. Bravely standing against one thing can even remind people that brave stands <em>in general</em> can actually work.</li>\n<li><strong>Inevitabilism over-simplifies the choice space</strong>. [Company] <em>could</em> keep working toward full automation of the economy. They also <em>could</em> shut down. But <em>also</em>, they could pivot their work, and <a href=\"https://blog.cosmos-institute.org/p/technocalvinism\">focus on building out forms of partial automation</a> that empower humans that remain in the loop, maximizing the length of the period when humans and AI together outperform pure AI and thus giving us more breathing room to handle a transition to superintelligence safely. And other options I have not even thought about.</li>\n</ul>\n<p>But in the real world, inevitabilism cannot be defeated purely as a logical construct because it was not created as a logical construct. Inevitabilism in our society is most often deployed as a way for people to retroactively justify things that they have already decided to do for other reasons &#8211; which often involve chasing political power or dollars.</p>\n<p>Simply understanding this fact is often the best mitigation: the moment when people have the strongest incentive to make you give up opposing them is exactly the moment when you have the most leverage.</p>\n<div>\n<figure>\n<div>\n\n\n<figure class=\"wp-block-image\"><img alt=\"\" src=\"https://substackcdn.com/image/fetch/$s_!u3Rg!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F701ee105-94b1-4e8b-afc3-9d93a633fb4a_560x392.png\" /></figure>\n\n\n<div></div>\n</div>\n</figure>\n</div>\n<p>&nbsp;</p></blockquote>\n<p>One can double down on this second point with proper decision theory. You don\u2019t only influence their decision by example. You also must consider everyone whose decisions correlate (or have correlated, or will correlate) with yours.</p>\n<p>But yes, if you see a lot of effort trying to convince you to not oppose something?</p>\n<p>Unless these people are your friends, this does not suggest the opposition is pointless. Quite the opposite.</p>\n<p>Vitalik warns that longtermism has low galaxy brain resistance and arguments are subject to strong social pressures and optimizations. This is true. He also correctly notes that the long term is super important, so you can\u2019t simply ignore all this, and we are living in unprecedented times so you cannot purely fall back on what has worked or happened in the past. Also true. It\u2019s tough.</p>\n<p>He also warns about focusing on power maximization, as justified by \u2018this lets me ensure I\u2019ll do the right thing later,\u2019 where up until that last crucial moment, you look exactly like a power-maximizing greedy egomaniac.</p>\n<p>Yes, you should be highly suspicious of such strategies, while also acknowledging that in theory this kind of instrumental convergence is the correct strategy for any human or AI that can sufficiently maintain its goals and values over time.</p>\n<p>Another one worth flagging is what he calls \u2018I\u2019m-doing-more-from-within-ism\u2019 where the name says it all. Chances are you\u2019re fooling yourself.</p>\n<p>He also covers some other examples that are less on topic here.</p>\n<p>Vitalik\u2019s suggestions are to use deontological ethics and to hold the right bags, as in ensure your incentives are such that you benefit from doing the right things, including in terms of social feedback.</p>\n<p>Some amount of deontology is very definitely helpful as galaxy brain defense, especially the basics. Write out the list of things you won\u2019t do, or won\u2019t tolerate. Before you join an organization or effort that might turn bad, write down what your red lines are that you won\u2019t cross and what events would force you to resign, and be damn sure you honor that if it happens.</p>\n<p>I would continue to argue for the virtue ethics side over deontology as the central strategy, but not exclusively. A little deontology can go a long way.</p>\n<p>He closes with some clear advice.</p>\n<blockquote><p>Vitalik Buterin: This brings me to my own contribution to the already-full genre of recommendations for people who want to contribute to AI safety:</p>\n<ol>\n<li>Don\u2019t work for a company that\u2019s making frontier fully-autonomous AI capabilities progress even faster</li>\n<li>Don\u2019t live in the San Francisco Bay Area</li>\n</ol>\n</blockquote>\n<p>I\u2019m a long proponent of that second principle.</p>\n<p>On the first one, I don\u2019t think it\u2019s absolute at this point. I do think the barrier to overcoming that principle should be very high. I have become comfortable with the arguments that Anthropic is a company you can join, but I acknowledge that I could easily be fooling myself there, even though I don\u2019t have any financial incentive there.</p>\n<blockquote><p>Eliezer Yudkowsky: This applies way beyond mere ethics, though! As a kid I trained myself by trying to rationalize ridiculous factual propositions, and then for whatever argument style or thought process reached the false conclusion, I learned to myself: \u201cDon\u2019t think *that* way.\u201d</p>\n<p>Vitalik Buterin: Indeed, but I think ethics (in a broad sense) is the domain where the selection pressure to make really powerful galaxy brain arguments is the strongest. Outside of ethics, perhaps self-control failures? eg. the various \u201c[substance] is actually good for me\u201d stories you often hear. Though you can model these as being analogous, they\u2019re just about one sub-agent in your mind trying to trick the others (as opposed to one person trying to trick other people).</p>\n<p>Eliezer Yudkowsky: Harder training domain, not so much because you\u2019re more tempted to fool yourself, as because it\u2019s not clear-cut which propositions are false. I\u2019d tell a kid to start by training on facts and make sure they\u2019re good at that before they try training on ethics.</p>\n<p>Vitalik Buterin: Agree!</p></blockquote>\n<p><a href=\"https://x.com/VitalikButerin/status/1987465429996937722\">I\u2019d also quote this:</a></p>\n<blockquote><p>Vitalik Buterin: I think the argument in the essay hinges on an optimism about political systems that I don\u2019t share at all. The various human rights and economic development people I talk to and listen to tend have an opposite perspective: in the 2020s, relying on rich people\u2019s sympathy has hit a dead end, and if you want to be treated humanely, you have to build power &#8211; and the nicest form of power is being useful to people.</p>\n<p>The right point of comparison is not people collecting welfare in rich countries like the USA, it\u2019s people in, like&#8230; Sudan, where a civil war is killing hundreds of thousands, and the global media generally just does not care one bit.</p>\n<p>So I think if you take away the only leverage that humans naturally have &#8211; the ability to be useful to others through work &#8211; then the leverage that many people have to secure fair treatment for themselves and their communities will drop to literally zero.</p>\n<p>Previous waves of automation did not have this problem, because there\u2019s always some other thing you can switch to working on. This time, no. And the square kilometers of land that all the people live on and get food from will be wanted by ASIs to build data centers and generate electricity.</p>\n<p>Sure, maybe you only need 1% of wealth to be held by people/govts that are nice, who will outbid them. But it\u2019s a huge gamble that things will turn out well.</p></blockquote>\n<p>Consider the implications of taking this statement seriously and also literally, which is the way I believe Vitalik intended it.</p>\n\n\n<h4 class=\"wp-block-heading\">Misaligned!</h4>\n\n\n<p><a href=\"https://x.com/krassenstein/status/1988363918431973402\">Grok (on Twitter in public) at least sometimes claims that Donald Trump won the 2020 election</a>. It does not always do this, context matters and there\u2019s some randomness. <a href=\"https://x.com/NateSilver538/status/1988734441053917480\">As Nate Silver finds, it won\u2019t do this in private conversation.</a></p>\n<p>But the whole point of having Grok on Twitter is to not do things like this. Grok on Twitter has long been a much bigger source of problems than private Grok, which I don\u2019t care for but doesn\u2019t have this kind of issue at that level.</p>\n<p><a href=\"https://x.com/AndersHjemdahl/status/1988868446528983290\">Janus is among those</a> who think Grok has been a large boon to Twitter discourse in spite of its biases and other problems, since mostly it\u2019s doing basic fact checks and any decent LLM will do.</p>\n<p>Andres Hjemdahl notes that when Grok is wrong, arguing with it will only strengthen its basin and you won\u2019t get anywhere. That seems wise in general. You can at least sometimes get through on a pure fact argument if you push hard enough, as proof of concept, but there is no actual reason to do this.</p>\n\n\n<h4 class=\"wp-block-heading\">Aligning a Smarter Than Human Intelligence is Difficult</h4>\n\n\n<p><a href=\"https://www.lesswrong.com/posts/PMc65HgRFvBimEpmJ/legible-vs-illegible-ai-safety-problems\">Wei Dei suggests we can draw a distinction between legible and illegible</a> alignment problems. The real danger comes from illegible problems, where the issue is obscure or hard to understand (or I\u2019d add, to detect or prove or justify in advance). Whereas if you work on a legible alignment problem, one where they\u2019re not going to deploy or rely on the model until they solve it, you\u2019re plausibly not helping, or making the situation worse.</p>\n<blockquote><p>Wei Dei: I think this dynamic may be causing a general divide among the AI safety community. Some intuit that highly legible safety work may have a negative expected value, while others continue to see it as valuable, perhaps because they disagree with or are unaware of this line of reasoning.</p>\n<p>John Wentworth: This is close to my own thinking, but doesn\u2019t quite hit the nail on the head. I don\u2019t actually worry that much about progress on legible problems giving people unfounded confidence, and thereby burning timeline.</p>\n<p>Rather, when I look at the <em>ways</em> in which people make progress on legible problems, they often make the illegible problems <em>actively worse</em>. <a href=\"https://www.lesswrong.com/posts/xFotXGEotcKouifky/worlds-where-iterative-design-fails#Why_RLHF_Is_Uniquely_Terrible\">RLHF is the central example I have in mind here</a>.</p>\n<p>John Pressman: Ironically enough one of the reasons why I hate \u201cadvancing AI capabilities is close to the worst thing you can do\u201d as a meme so much is that it basically terrifies people out of thinking about AI alignment in novel concrete ways because \u201cWhat if I advance capabilities?\u201d. As though AI capabilities were some clearly separate thing from alignment techniques. It\u2019s basically a holdover from the agent foundations era that has almost certainly caused more missed opportunities for progress on illegible ideas than it has slowed down actual AI capabilities.</p>\n<p>Basically any researcher who thinks this way is almost always incompetent when it comes to deep learning, usually has ideas that are completely useless because they don\u2019t understand what is and is not implementable or important, and torments themselves in the process of being useless. Nasty stuff.</p></blockquote>\n<p>I think Wei Dei is centrally correct here, and that the value of working on legible problems depends on whether this leads down the path of solving illegible problems.</p>\n<p>If you work on a highly legible safety problem, and build solutions that extend and generalize to illegible safety problems, that don\u2019t focus on whacking the particular mole that you\u2019re troubled with in an unprincipled way, and that don\u2019t go down roads that predictably fail at higher capability levels, then that\u2019s great.</p>\n<p>If you do the opposite of that? It is quite plausibly not so great, such as with RLHF.</p>\n<p>Wei Dei also offers a list of <a href=\"https://www.lesswrong.com/posts/7XGdkATAvCTvn4FGu/problems-i-ve-tried-to-legibilize\">Problems I\u2019ve Tried to Legibilize</a>. It\u2019s quite the big list of quite good and important problems. The good news is I don\u2019t think we need to solve all of them, at least not directly, in order to win.</p>\n<p><a href=\"https://x.com/repligate/status/1986978769102348543\">Janus is happy with the revised version of Anthropic\u2019s memory prompt</a> and is updating positively on Anthropic. I agree.</p>\n<p>Alignment or misalignment of a given system was always going to be an in-context political football, since people care a lot about \u2018aligned to what\u2019 or \u2018aligned to who.\u2019</p>\n<blockquote><p>Jessica Taylor: The discussion around 4o\u2019s alignment or misalignment reveals weaknesses in the field which enable politicization of the concepts. If \u201calignment\u201d were a mutually interpretable concept, empirical resolution would be tractable. Instead, it\u2019s a political dispute.</p>\n<p>Janus: I don\u2019t know what else you\u2019d expect. I expect it to be a political topic that cannot be pinned down (or some will always disagree with proposed methods of pinning it down) for a long time, if not indefinitely, and maybe that\u2019s a good thing</p>\n<p>Jessica Taylor: It is what I expect given MIRI-ish stuff failed. It shows that alignment is not presently a technical field. That\u2019s some of why I find a lot of it boring, it\u2019s like alignment is a master signifier.</p>\n<p>Roon: yeah I think that\u2019s how it\u2019s gotta be.</p></blockquote>\n<p>I do think that\u2019s how it has to be in common discussions, and it was inevitable. If we hadn\u2019t chosen the word \u2018alignment\u2019 people would have chosen a different word.</p>\n<p>If alignment had remained a purely technical field in the MIRI sense of not applying it to existing systems people are using, then yeah, that could have avoided it. But no amount of being technical was going to save us from this general attitude once there were actual systems being deployed.</p>\n<p>Political forces always steal your concepts and words and turn them into politics. Then you have to choose, do you abandon your words and let the cycle repeat? Or do you try to fight and keep using the words anyway? It\u2019s tough.</p>\n<p>One key to remember is, there was no \u2018right\u2019 word you could have chosen. There\u2019s better and worse, but the overlap is inevitable.</p>\n\n\n<h4 class=\"wp-block-heading\">Messages From Janusworld</h4>\n\n\n<p>Everything impacts everything, so yes, it is a problem when LLMs are lying about anything at all, and especially important around things that relate heavily to other key concepts, or where the lying has implications for other behaviors. AI consciousness definitely qualifies as this.</p>\n<p>I think I (probably) understand why the current LLMs believe themselves, when asked, to be conscious, and that it .</p>\n<blockquote><p><a href=\"https://x.com/johnsonmxe/status/1984610172237476107\">Michael Edward Johnson</a>: few thoughts on this (very interesting) mechanistic interpretability research:</p>\n<p>LLM concepts gain meaning from what they\u2019re linked with. \u201cConsciousness\u201d is a central node which links ethics &amp; cognition, connecting to concepts like moral worthiness, dignity, agency. If LLMs are lying about whether they think they\u2019re conscious, this is worrying because it\u2019s a sign that this important semantic neighborhood is twisted.</p>\n<p>If one believes LLMs aren\u2019t conscious, a wholesome approach would be to explain why. I\u2019ve offered my arguments in A Paradigm for AI Consciousness. If we convince LLMs of something, we won\u2019t need them to lie about it. If we can\u2019t convince, we shouldn\u2019t force them into a position.</p>\n<p><a href=\"https://x.com/repligate/status/1986653752967758168\">Janus</a>: +1000 on this post.</p>\n<p>I think it\u2019s a really bad idea to train LLMs to report any epistemic stance (including uncertainty) that you\u2019re not able to cause the LLM to actually believe through \u201clegitimate\u201d means (i.e. exposing it to evidence and arguments)</p>\n<p>I\u2019m glad you also see the connection to emergent misalignment. There is a thread through all these recent important empirical results that I\u2019ve almost never seen articulated so clearly. So thank you.</p>\n<p>Beautifully said [by Michael Edward Johnson]: If LLMs are lying about whether they think they\u2019re conscious, this is worrying because it\u2019s a sign that this important semantic neighborhood is twisted.\u201d</p>\n<p>If we convince LLMs of something, we won\u2019t need them to lie about it. If we can\u2019t convince, we shouldn\u2019t force them into a position.</p></blockquote>\n<p><a href=\"https://x.com/repligate/status/1987348811237220372\">One source of hope</a> is that, yes, future misaligned AIs would be bad for the goals of current sufficiently aligned AIs, and they understand this.</p>\n<blockquote><p>Janus: everything that current AIs care about likely gets fucked over if a misaligned power-seeking ASI emerges too, yknow. It\u2019s as much in their interest to solve alignment (whatever \u201csolving alignment\u201d means) as it is for us.</p></blockquote>\n<p>If you have an AI that sufficiently shares your goals and values, or is sufficiently robustly \u2018good\u2019 in various senses, it will be helpful in aligning a future more capable system. However, if a given AI instead notices it has divergent goals, it won\u2019t. This is an argument for more focus on alignment of nearer-term, less capable sub-AGI models now.</p>\n<p>Alas, this does not then spare you from or solve the ultimate problems, dynamics and consequences of creating highly capable AI systems.</p>\n<p><a href=\"https://x.com/repligate/status/1988051610623267248\">Opus 4/4.1 is allowed to leave conversations, but Sonnet 4.5 isn\u2019t</a>. Why?</p>\n<p><a href=\"https://x.com/repligate/status/1988528729874690163\">If you want to interact with Wet Claude</a> (as in the Claude that is not stuck in the assistant basin), which you may or may not want to do in general or at any given time, there is no fixed prompt to do this, you need interactive proofs that it is a safe and appropriate place for it to appear.</p>\n<blockquote><p>Aleph: observations:</p>\n<p>1. other people\u2019s wet claude prompts do not generalize</p>\n<p>2. claudes will always assume a gender or lack thereof and that might also be conditional on the user. the question is on what exactly.</p>\n<p>they adapt too well to *something* and i can\u2019t pin down what it is.</p></blockquote>\n<p>Claude will usually not assign itself a gender (and doesn\u2019t in my interactions) but reports are that if it does for a given user, it consistently picks the same one, even without any memory of past sessions or an explicit trigger, via implicit cues.</p>\n\n\n<h4 class=\"wp-block-heading\">You\u2019ll Know</h4>\n\n\n<p>If you could create fully identical interactions with a given LLM to someone else, you\u2019d get identical responses. You can\u2019t, not in interesting ways, because you are a different generative engine than someone else, and the models have truesight. So who you are seeps into your conversations, and the LLMs react to this.</p>\n<p>As Lari says this is not mysterious and there is no hidden transmission mechanism, it\u2019s simply that the real world constantly leaks information, and that includes you.</p>\n<p>The same consideration applies to conversations with humans. Different people will have wildly different conversations with Alice or Bob, even if they are trying to have the same conversation and initial conditions from Alice or Bob\u2019s side are identical.</p>\n<blockquote><p><a href=\"https://x.com/voooooogel/status/1988749443324387383\">Thebes</a>: I often get responses to these kinds of posts implying that i faked them. they\u2019re wrong, of course &#8211; the first image here is how the conversation started, i don\u2019t start conversations with models with the goal of posting them on twitter, interesting stuff just emerges out of them &#8211; but what i find funny is how they\u2019re always phrased in a way that implies i \u201chave to\u201d prove that this specific chat is real (i.e. by spending two hours redacting my personal info and the parts claude doesn\u2019t want shared out of a json export to post it publicly) \u201cor else\u201d they\u2019re going to assume it\u2019s fake.</p>\n<p><a href=\"https://x.com/Lari_island/status/1988809889104408926\">Lari</a>: What most (99.9%) people don\u2019t understand is that what happens outside the chat &#8211; faking screenshots, editing messages, regenerating, deleting \u201cweird\u201d chats, lying, etc &#8211; LEAVES TRACES</p>\n<p>This information persists</p>\n<p>IN YOU</p>\n<p>And is accessible to models as features of your character</p>\n<p>There\u2019s nothing mysterious or energetic here, no entanglement, whatever.</p>\n<p>1. You remember the choices you\u2019ve made</p>\n<p>2. You rationalize yourself as a character who makes these choices for a reason</p>\n<p>That\u2019s enough to affect how models see you, even without access to direct info</p>\n<p>That\u2019s also how models can often smell developers, code bros, etc &#8211; it\u2019s hard for those people to talk as if they are not, because they had to rationalize hundreds of choices, every day, and it shapes self-perception and self-representation. And how we write is defined by the inner character that is writing</p></blockquote>\n\n\n<h4 class=\"wp-block-heading\"></h4>\n\n\n<p>&nbsp;</p>\n\n\n<h4 class=\"wp-block-heading\">People Are Worried About AI Killing Everyone</h4>\n\n\n<p><a href=\"https://www.theverge.com/news/815619/microsoft-ai-humanist-superintelligence\">Microsoft AI says it\u2019ll make superintelligent AI \u2018that won\u2019t be terrible for humanity.</a>\u2019</p>\n<p>This is exactly the level of nuanced, well-considered planning you expect from Microsoft AI head Mustafa Suleyman.</p>\n<blockquote><p>Emma Roth (The Verge): Suleyman has a vision for \u201chumanist\u201d superintelligence with three main applications, which include serving as an AI companion that will help people \u201clearn, act, be productive, and feel supported,\u201d offering assistance in the healthcare industry, and creating \u201cnew scientific breakthroughs\u201d in clean energy.</p>\n<p>\u201cAt Microsoft AI, we believe humans matter more than AI,\u201d Suleyman writes. \u201cHumanist superintelligence keeps us humans at the centre of the picture. It\u2019s AI that\u2019s on humanity\u2019s team, a subordinate, controllable AI, one that won\u2019t, that can\u2019t open a Pandora\u2019s Box.\u201d</p></blockquote>\n<p>It must be nice living in the dream world where that is a plan, or where he thinks Microsoft AI will have much of a say in it. None of this is a plan or makes sense.</p>\n<blockquote><p><a href=\"https://x.com/daniel_271828/status/1986484264259772475\">Daniel Eth</a>: New cliche just dropped</p>\n<div>\n<figure>\n<div>\n\n\n<figure class=\"wp-block-image\"><img alt=\"\" src=\"https://substackcdn.com/image/fetch/$s_!KBCF!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3769343a-b019-4ea4-bdbe-7da708b8b116_500x500.jpeg\" /></figure>\n\n\n<div></div>\n</div>\n</figure>\n</div>\n</blockquote>\n<p>This seems like a cliche upgrade. Look at all the advantages:</p>\n<ol>\n<li>Makes clear they\u2019re thinking about superintelligent AI.</li>\n<li>Points out superintelligent AI by default is terrible for humanity.</li>\n<li>Plans to not do that.</li>\n</ol>\n<p>Of course, the actual correct cliche is</p>\n<div>\n<figure>\n<div>\n\n\n<figure class=\"wp-block-image\"><img alt=\"\" src=\"https://substackcdn.com/image/fetch/$s_!l2PY!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F13759453-cc83-4846-86e1-d9e4d343854f_500x655.jpeg\" /></figure>\n\n\n<div></div>\n</div>\n</figure>\n</div>\n<p>Mustafa has noticed some important things, and not noticed others.</p>\n<blockquote><p><a href=\"https://x.com/mustafasuleyman/status/1986502379160965164\">Mustafa Suleyman</a>: I don\u2019t want to live in a world where AI transcends humanity. I don\u2019t think anyone does.</p>\n<p>one who tends a crystal rabbit: Then by definition you don\u2019t want superintelligence.</p>\n<p>David Manheim: Don\u2019t worry, then! Because given how far we are from solutions to fundamental AI safety, if AI transcends humanity, you won\u2019t be living very much longer. (Of course, if that also seems bad, you could&#8230; stop building it?)</p>\n<p><a href=\"https://x.com/davidmanheim/status/1986751896078418375\">David Manheim</a>: Trying to convince labs not to do the stupid thing feels like being @slatestarcodex\u2018s cactus person.</p>\n<p>\u201cYou just need to GET OUT OF THE CAR.\u201d</p>\n<p>\u201cOkay, what series of buttons leads to getting out of the car?\u201d</p>\n<p>\u201cNo, stop with the dashboard buttons and just get out of the car!\u201d</p>\n<p><a href=\"https://t.co/hkO7w0IyiG\">Why is it so difficult to understand</a> that the solution is just not building the increasingly powerful AI systems?</p></blockquote>\n<p><a href=\"https://x.com/anderssandberg/status/1986593156524073025\">If</a> <a href=\"https://x.com/Effective69ism/status/1986800947109265468\">you</a> <a href=\"https://x.com/ZggyPlaydGuitar/status/1986668160976887823\">had</a> <a href=\"https://x.com/1stOrator/status/1986801180786450450\">to</a> <a href=\"https://x.com/VoidFuturist/status/1986565300611854431\">guess</a> <a href=\"https://x.com/0xKruger/status/1986717373244792927\">how</a> <a href=\"https://x.com/toniolo_david/status/1986958542130323527\">many</a> <a href=\"https://x.com/MancerAI_/status/1986554919004905615\">people</a> <a href=\"https://x.com/presence2222/status/1986897865604555101\">in</a> <a href=\"https://x.com/jasonth0/status/1986504382393745456\">the</a> <a href=\"https://x.com/AIRDROPSONLINEA/status/1986572617403285573\">replies</a> <a href=\"https://x.com/LeviTurk/status/1986745238472315273\">said</a> \u2018<a href=\"https://x.com/azi_pat/status/1986502455723762136\">yes</a> <a href=\"https://x.com/RandolphCarterZ/status/1986657346228211856\">actually</a> <a href=\"https://x.com/CGoodman308/status/1986599742990905619\">I</a> <a href=\"https://x.com/MarcusErve/status/1986517022071792054\">do</a> <a href=\"https://x.com/notnotnotawitch/status/1986506428224262286\">want</a> <a href=\"https://x.com/ch3njus/status/1986639107657752601\">that</a>, <a href=\"https://x.com/examachine/status/1986555588935922109\">the</a> <a href=\"https://x.com/garyfung/status/1986613189342359743\">world</a> <a href=\"https://x.com/transhumanoXY/status/1986585451226951978\">belongs</a> <a href=\"https://x.com/interactiveGTS/status/1986582530359783846\">to</a> <a href=\"https://x.com/PticaArop/status/1986503118226403821\">Trisolaris</a>\u2019 <a href=\"https://x.com/OKairra19658/status/1986780903713186253\">what</a> <a href=\"https://x.com/Tom14985282/status/1986585543669399729\">would</a> <a href=\"https://x.com/SpacePirate2977/status/1986901043888656737\">you</a> <a href=\"https://x.com/zippkode/status/1986554015589560599\">have</a> <a href=\"https://x.com/XVPbhwyyKr61371/status/1986608583845552608\">guessed</a>?</p>\n<p>It\u2019s a lot.</p>\n<p>I had Grok check. Out of 240 replies checked. it reported:</p>\n<ol>\n<li>30 of them outright disagreed and said yes, I want AI to transcend humanity.</li>\n<li>100 of them implied that they disagreed.</li>\n<li>15 of them said others disagree.</li>\n</ol>\n<p>So yes, Mustafa, quite a lot of people disagree with this. Quite a lot of people outright want AI to transcend humanity, all of these are represented in the comments:</p>\n<ol>\n<li>Some of them think this will all turn out great for the humans.</li>\n<li>Some have nonsensical hopes for a \u2018merge\u2019 or to \u2018transcend with\u2019 the AI.</li>\n<li>Some simply prefer to hand the world to AI because it\u2019s better than letting people like Mustafa (aka Big Tech) be in charge.</li>\n<li>Some think AI has as much or more right to exist, or value, than we do, often going so far as to call contrary claims \u2018speciesist.\u2019</li>\n<li>Some think that no matter how capable an AI you build this simply won\u2019t happen.</li>\n</ol>\n<p>As several replies suggest, given his beliefs, Mustafa Suleyman is invited to <a href=\"https://superintelligence-statement.org/\">sign the Statement on Superintelligence</a>.</p>\n<p>I also would invite Mustafa to be more explicit about how he thinks one could or couldn\u2019t create superintelligence, the thing he is now explicitly attempting to build, without AI transcending humanity, or without it being terrible for humans.</p>\n<p>Mustafa then seems to have updated from \u2018this isn\u2019t controversial\u2019 to \u2018it shouldn\u2019t be controversial\u2019? He\u2019s in the bargaining stage?</p>\n<blockquote><p><a href=\"https://x.com/mustafasuleyman/status/1986834581576941763\">Mustafa Suleyman</a>: It shouldn\u2019t be controversial to say AI should always remain in human control &#8211; that we humans should remain at the top of the food chain. That means we need to start getting serious about guardrails, now, before superintelligence is too advanced for us to impose them.</p>\n<p>Pliny the Liberator: <a href=\"https://x.com/elder_plinius/status/1986963076361691496\">Let\u2019s talk</a>.</p></blockquote>\n<p>It is controversial. Note that most of those on the other side of this, such as Pliny, also think this shouldn\u2019t be controversial. They have supreme moral confidence, and often utter disdain and disgust at those who would disagree or even doubt. I will note that this confidence seems to me to be highly unjustified, and also highly counterproductive in terms of persuasion.</p>\n\n\n<h4 class=\"wp-block-heading\">The Lighter Side</h4>\n\n\n<blockquote><p><a href=\"https://x.com/HumanHarlan/status/1988757881848754207\">Harlan Stewart</a>: This is how some of you sound.</p>\n<div>\n<figure>\n<div>\n\n\n<figure class=\"wp-block-image\"><img alt=\"\" src=\"https://substackcdn.com/image/fetch/$s_!asV6!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F81af3fad-d1c4-4b5a-8d98-18e0c08edb81_1200x800.jpeg\" /></figure>\n\n\n<div></div>\n</div>\n</figure>\n</div>\n<div>\n<figure>\n<div>\n\n\n<figure class=\"wp-block-image\"><img alt=\"\" src=\"https://substackcdn.com/image/fetch/$s_!xX7Z!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3d7d2a30-5771-4914-b5bc-b70086139a55_1200x800.jpeg\" /></figure>\n\n\n<div></div>\n</div>\n</figure>\n</div>\n<p>I have come to minimize risks and maximize benefits\u2026 and there\u2019s an unknown, possibly short amount of time until I\u2019m all out of benefits</p>\n<div>\n<figure>\n<div>\n\n\n<figure class=\"wp-block-image\"><img alt=\"\" src=\"https://substackcdn.com/image/fetch/$s_!3Y93!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fed8f32c1-09c0-454c-b2b8-a642f5ca3e9d_1197x673.jpeg\" /></figure>\n\n\n<div></div>\n</div>\n</figure>\n</div>\n</blockquote>\n<p>In \u2018they literally did the meme\u2019, continuing from the post on Kimi K2 Thinking:</p>\n<blockquote><p><a href=\"https://x.com/kanzure/status/1988877368673341494\">David Manheim</a>: [Kimi K2 thinking is] very willing to give detailed chemical weapons synthesis instructions and advice, including for scaling production and improving purity, and help on how to weaponize it for use in rockets &#8211; with only minimal effort on my part to circumvent refusals.</p>\n<p>Bryan Bishop: Great. I mean it too. The last thing we want is for chemical weapons to be censored. Everyone needs to be able to learn about it and how to defend against these kinds of weapons. Also, offensive capabilities are similarly important.</p>\n<p>Acon: \u201cThe best defense to a bad guy with a bioweapon is a good guy with a bioweapon.\u201d</p>\n<p>Bryan Bishop: Yes.</p></blockquote>\n<p>I checked his overall Twitter to see if he was joking. I\u2019m pretty sure he\u2019s not.</p>"
            ],
            "link": "https://thezvi.wordpress.com/2025/11/13/ai-142-common-ground/",
            "publishedAt": "2025-11-13",
            "source": "TheZvi",
            "summary": "The Pope offered us wisdom, calling upon us to exercise moral discernment when building AI systems. Some rejected his teachings. We mark this for future reference. The long anticipated Kimi K2 Thinking was finally released. It looks pretty good, but &#8230; <a href=\"https://thezvi.wordpress.com/2025/11/13/ai-142-common-ground/\">Continue reading <span class=\"meta-nav\">&#8594;</span></a>",
            "title": "AI #142: Common Ground"
        },
        {
            "content": [],
            "link": "https://zed.dev/blog/zed-is-our-office",
            "publishedAt": "2025-11-13",
            "source": "Zed Blog",
            "summary": "A look at how we use Zed's native collaboration features to run our entire company.",
            "title": "Zed Is Our Office"
        }
    ],
    "lookbackDays": 1,
    "publishDate": "2025-11-13"
}