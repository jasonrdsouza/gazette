{
    "articles": [
        {
            "content": [
                "<img alt=\"this should not be possible\" src=\"https://ghuntley.com/content/images/2025/07/A-moody--high-contrast-tattoo-flash-print-of-a-beer--rendered-in-vibrant-colors-with-a-retro-flair-and-complex-ornamental-details--set-against-a-white-background.jpg\" /><p>It might surprise some folks, but I&apos;m incredibly cynical when it comes to AI and what is possible; yet I keep an open mind. That said, two weeks ago, when I was in SFO, I discovered another thing that should not be possible.  Every time I find out something that works, which should not be possible, it pushes me further and further, making me think that we are already in post-AGI territory.</p><p>I was sitting next to a mate at a pub; it was pretty late, and we were just talking about LLM capabilities, riffing about what the modern version of <a href=\"https://falco.org/?ref=ghuntley.com\">Falco</a> or any of these tools in the DFIR space looks like when combined with an LLM.</p><p>You see, a couple of months ago, I&apos;d been playing with eBPF and LLMs and discovered that LLMs do eBPF unusually well. So in the spirit of deliberate practice (see below), a laptop was brought out, and we SSH&apos;d into a Linux machine.</p><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://ghuntley.com/play\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">deliberate intentional practice</div><div class=\"kg-bookmark-description\">Something I&#x2019;ve been wondering about for a really long time is, essentially, why do people say AI doesn&#x2019;t work for them? What do they mean when they say that? From which identity are they coming from? Are they coming from the perspective of an engineer with a job title and</div><div class=\"kg-bookmark-metadata\"><img alt=\"this should not be possible\" class=\"kg-bookmark-icon\" src=\"https://ghuntley.com/content/images/icon/7V0ak3am_400x400-1-40.jpg\" /><span class=\"kg-bookmark-author\">Geoffrey Huntley</span><span class=\"kg-bookmark-publisher\">Geoffrey Huntley</span></div></div><div class=\"kg-bookmark-thumbnail\"><img alt=\"this should not be possible\" src=\"https://ghuntley.com/content/images/thumbnail/A-vibrant-retro-style-traditional-tattoo-art-print-featuring-a-guitar--rendered-with-high-contrast-and-dramatic-lighting.-The-complex-ornamental-design-is-set-against-a-stark-white-background--showcasing-intense-color-saturation-and-a-symbo-4.jpg\" /></div></a></figure><p>The idea was simple. </p><p>Could we convert an eBPF trace to a fully functional application via <a href=\"https://ghuntley.com/ralph\" rel=\"noreferrer\">Ralph Wiggum</a>? So we started with a toy. </p><blockquote>strace ls 1&gt;trace 2&gt;&amp;1 </blockquote><p>After <code>ls</code> had completed listing out the files in a directory, we had a strace file. The next step was to modify the strace file to remove all references to the &apos;ls&apos; command using Vim.</p><blockquote>:%s/ls/lol/g</blockquote><p>You see, we didn&apos;t want the LLM to cheat by using hints about precisely what the <code>strace</code> did, as indicated by the file name of the executable in the trace.</p><p>The following prompt was then issued.</p><blockquote>read the TRACE<br />reimplement a program in rust that reimplments what this trace does</blockquote><p>A couple of moments later, our jaws were on the ground. It is indeed impossible to take an application from an <code>strace</code> and then build it into an application using only the <code>strace</code>.</p><figure class=\"kg-card kg-image-card\"><img alt=\"this should not be possible\" class=\"kg-image\" height=\"1441\" src=\"https://ghuntley.com/content/images/2025/07/CleanShot-2025-07-19-at-12.18.34@2x.png\" width=\"2000\" /></figure><p>From that point forward, things just got weird, really fast.  You see, I&apos;ve never been a fan of proprietary firmware blobs in the Linux kernel, and perhaps if this information reaches the right people, this category of problem will be forever solved thanks to AI. </p><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://github.com/ghuntley/strace-to-application?ref=ghuntley.com\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">GitHub - ghuntley/strace-to-application</div><div class=\"kg-bookmark-description\">Contribute to ghuntley/strace-to-application development by creating an account on GitHub.</div><div class=\"kg-bookmark-metadata\"><img alt=\"this should not be possible\" class=\"kg-bookmark-icon\" src=\"https://ghuntley.com/content/images/icon/pinned-octocat-093da3e6fa40-16.svg\" /><span class=\"kg-bookmark-author\">GitHub</span><span class=\"kg-bookmark-publisher\">ghuntley</span></div></div><div class=\"kg-bookmark-thumbnail\"><img alt=\"this should not be possible\" src=\"https://ghuntley.com/content/images/thumbnail/strace-to-application\" /></div></a></figure><p>Dear reader, use this knowledge wisely and with care. </p><p>p.s socials</p><ul><li>BSkye - <a href=\"https://bsky.app/profile/ghuntley.com/post/3lubvvwymyc2w?ref=ghuntley.com\">https://bsky.app/profile/ghuntley.com/post/3lubvvwymyc2w</a></li><li>X - <a href=\"https://x.com/GeoffreyHuntley/status/1946395846595289090?ref=ghuntley.com\">https://x.com/GeoffreyHuntley/status/1946395846595289090</a></li><li>LinkedIn - <a href=\"https://www.linkedin.com/posts/geoffreyhuntley_this-should-not-be-possible-activity-7352162532217769985-lo-L?utm_source=share&amp;utm_medium=member_desktop&amp;rcm=ACoAAABQKuUB2AJ059keUcRUVLbtmoa6miLVlTI\">https://www.linkedin.com/posts/geoffreyhuntley_this-should-not-be-possible-activity-7352162532217769985-lo-L</a></li></ul>"
            ],
            "link": "https://ghuntley.com/no/",
            "publishedAt": "2025-07-19",
            "source": "Geoffrey Huntley",
            "summary": "<p>It might surprise some folks, but I&apos;m incredibly cynical when it comes to AI and what is possible; yet I keep an open mind. That said, two weeks ago, when I was in SFO, I discovered another thing that should not be possible. Every time I find out</p>",
            "title": "this should not be possible"
        },
        {
            "content": [
                "<div class=\"trix-content\">\n  <div>There was a tiny fly right by the drain, and I was about to wash my hands.<br /><br />Turning on the water would have sent it right down the hole. A quick end, or an eventual struggled drowning, hard to know. But that would be that, there was no getting out.<br /><br />Somehow, for a moment, I slipped into contemplation. I could just turn on the water, I could rescue it, I could use a different sink. Had I not even seen the fly, the water would already been on, its invisible fate secured.<br /><br />But in that moment of maybe, the fly launched and flew away.<br /><br />Since it didn't know what I was about to do, and what that would do to it, it had absolutely no idea how lucky it was.<br /><br />And then I wondered. How often am I in that same position? No idea how lucky I am.<br /><br />Often, probably always.<br /><br /></div><div>-Jason</div>\n</div>"
            ],
            "link": "https://world.hey.com/jason/a-fly-and-luck-c8adb7a4",
            "publishedAt": "2025-07-19",
            "source": "Jason Fried",
            "summary": "<div class=\"trix-content\"> <div>There was a tiny fly right by the drain, and I was about to wash my hands.<br /><br />Turning on the water would have sent it right down the hole. A quick end, or an eventual struggled drowning, hard to know. But that would be that, there was no getting out.<br /><br />Somehow, for a moment, I slipped into contemplation. I could just turn on the water, I could rescue it, I could use a different sink. Had I not even seen the fly, the water would already been on, its invisible fate secured.<br /><br />But in that moment of maybe, the fly launched and flew away.<br /><br />Since it didn't know what I was about to do, and what that would do to it, it had absolutely no idea how lucky it was.<br /><br />And then I wondered. How often am I in that same position? No idea how lucky I am.<br /><br />Often, probably always.<br /><br /></div><div>-Jason</div> </div>",
            "title": "A fly and luck"
        },
        {
            "content": [
                "<p>One of the more interesting robot-assisted tools I\u2019ve built in the last few months is tracking the various metrics for Rands-related properties. Think social like <a href=\"https://mastodon.social/@rands\">Mastodon</a>, <a href=\"https://bsky.app/profile/rands.bsky.social\">BlueSky</a>, and<br />\n<a href=\"https://www.threads.com/@rands\">Threads</a>. Think blog analytics like new users, session duration, and popular articles. I\u2019ve been plopping this data into a spreadsheet every month for the last two years. This month, I decided to have the robots take a swing at a dashboard.</p>\n<p>And suddenly, the weekend vanished, and I had multiple dashboards, including basic and advanced analysis, as well as a system where I set monthly goals for the metrics. The advanced analysis was mostly robot-suggested metrics, such as:</p>\n<ul>\n<li>Compound Annual Growth (The average annual growth rate over the entire period, accounting for compounding)</li>\n<li>Volatility (Standard deviation of monthly growth rates. Higher values mean more erratic growth)</li>\n<li>Momentum (Compares recent growth acceleration to previous periods)</li>\n</ul>\n<p>Did all of this robot wizardry give me major insight? No, it gave me a refined view of data I could already intuit by skimming the dashboard, but the addition of sparklines put a clear picture on two stats. <strong>Both Rands Leadership Slack members and newsletter subscribers grow steadily like nothing else</strong>.</p>\n<p>While I invest daily in the <a href=\"https://randsinrepose.com/welcome-to-rands-leadership-slack/\">Rands Leadership Slack</a>, the newsletter is an afterthought. The subscription link is buried at the bottom of the <a href=\"https://randsinrepose.com/about/\">About</a> page, and while I post most new articles there&#8230; sometimes I forget. Sorry. Doesn\u2019t matter, subscriber growth isn\u2019t a flood, but it\u2019s constantly up and to the right. Every month. For years.</p>\n<p>I moved to Substack after years of attempting to twist MailChimp into doing something it didn\u2019t really want to do. Substack was easy to set up \u2014 someone had clearly done their design work \u2014 and the buzz about the product was intriguing. Once set up, I promptly forgot about the newsletter, and subscribers continued to grow.</p>\n<p>There\u2019s another buzz about Substack, and I\u2019m not going to go into details because others have\u2026 competently and loudly. Here\u2019s the thing about this buzz: as far as I can tell, Substack has done little to nothing to counter that buzz. Actions. Words. Zip. I\u2019m sure they have told themselves they\u2019ve done work, but\u2026 as a distant outsider, the original damning buzz still bouncing around. My internal monologue: \u201cThey don\u2019t believe it\u2019s important enough to counter-program the buzz.\u201d</p>\n<p>Ok, so, bye then.</p>\n<p><a href=\"https://ghost.org/\">Ghost</a> has all the knobs and dials I need (and more<sup id=\"fnref-5187-1\"><a class=\"jetpack-footnote\" href=\"https://randsinrepose.com/feed/#fn-5187-1\" title=\"Read footnote.\">1</a></sup>) for a new home for the Leadership Newsletter. The migration was trivial. The design options were rich. And they charge customers for the service, which means the customers aren\u2019t the product\u2026 the product is the product.</p>\n<p>This is where I\u2019m supposed to tell you there\u2019s lots more content coming for the Newsletter. Maybe? I think newsletter subscribers and I have a good deal worked out. I\u2019ll post occasionally, and they\u2019ll just keep showing up.</p>\n<div style=\"height: 30vmin;\"></div>\n<div class=\"footnotes\">\n<hr />\n<ol>\n<li id=\"fn-5187-1\">\nOh yeah, one of the nice things Ghost helped me set up was that the newsletter will now be from a custom subdomain news.randsinrepose.com.&#160;<a href=\"https://randsinrepose.com/feed/#fnref-5187-1\" title=\"Return to main content.\">&#8617;</a>\n</li>\n</ol>\n</div>"
            ],
            "link": "https://randsinrepose.com/archives/welcome-to-ghost/",
            "publishedAt": "2025-07-19",
            "source": "Rands in Repose",
            "summary": "One of the more interesting robot-assisted tools I\u2019ve built in the last few months is tracking the various metrics for Rands-related properties. Think social like Mastodon, BlueSky, and Threads. Think blog analytics like new users, session duration, and popular articles. I\u2019ve been plopping this data into a spreadsheet every month for the last two years.&#8230; <a class=\"read-more\" href=\"https://randsinrepose.com/archives/welcome-to-ghost/\">More</a>",
            "title": "Welcome to Ghost"
        },
        {
            "content": [
                "<p>Today&#8217;s most popular vision for the future of AI is also the least imaginative one. The perfect AI assistant feels like the end-game, but it's just the prelude to a much more significant shift in design: the move from AI Assistants to AI Orchestrators.</p><p>When GPT-2 first came out, it wasn&#8217;t a chat app but instead an advanced auto-complete that you could play with in the OpenAI playground. While a power user for getting it to marginally support some of my homework assignments at the time<a class=\"footnote-anchor\" href=\"https://blog.sshh.io/feed#footnote-1\" id=\"footnote-anchor-1\" target=\"_self\">1</a>, I (and I&#8217;m sure many others) had no idea that later finetuning this base model into an assistant would lead to such a fundamental shift in how and where these large language models (LLMs) could be used. The vision for what LLMs could be used for completely changed.</p><p>I think there&#8217;s another, albeit more nuanced, shift now from AI Assistants to what I&#8217;ll call AI Orchestrators<a class=\"footnote-anchor\" href=\"https://blog.sshh.io/feed#footnote-2\" id=\"footnote-anchor-2\" target=\"_self\">2</a>. They're still LLM-based, and not quite the same as what most folks associate with the term &#8220;agents,&#8221; but agency is a large piece of it.</p><p>In this post, I&#8217;ll explore why this shift to orchestration is the real future of AI, how some sci-fi got it wrong, and what it means for the role of humans in the loop.</p><h2>AI Assistants vs AI Orchestrators</h2><p>Unlike the jump from text-complete to ChatGPT, the difference between assistants and orchestrators is subtle. Both are LLM-powered applications (often &#8220;GPT wrappers&#8221;) commanded in natural language, with the key difference being the level of human control in how a given unit of work is done.</p><p><strong>AI Assistants</strong> - The human acts as a driver, providing the AI with both the context and the plan to execute a task. Productivity is bounded by the user's ability to direct and review.</p><p><strong>AI Orchestrators</strong> - The human provides a high-level goal, and the AI acts as its own manager, using its own vast context to plan and execute the work. Productivity is less bounded, with the human's role shifting to a final reviewer.</p><p><strong>In detail (bullet points </strong><em><strong>often</strong></em><strong> apply, but not always):</strong></p><ul><li><p><strong>AI Assistants</strong></p><ul><li><p>Context and execution plan provided by the user</p></li><li><p>UI inputs often look like workflow builders</p></li><li><p>A human operator acts as the primary driver, watching over execution and steering as needed</p></li><li><p>Produces components or drafts for the human to integrate (e.g., a function, a paragraph).</p></li><li><p>Most of the AI's guardrails and constraints are provided by the user</p></li><li><p>External actions are tightly controlled or sandboxed, often requiring explicit user confirmation for each step.</p></li><li><p>Productivity bounded by a user&#8217;s ability and synchronous review (+10%) </p></li><li><p>Designed around existing human roles and their responsibilities </p></li><li><p>Feels like an assistant, intern, or new hire.</p></li></ul></li><li><p><strong>AI Orchestrators</strong></p><ul><li><p>Context comes mostly from outside what the user provides; execution is self-planned</p></li><li><p>UI inputs often just look like a goal</p></li><li><p>A human advisor acts as a reviewer on the final output </p></li><li><p>Delivers an end-to-end result (e.g., a deployed service, a completed financial report).</p></li><li><p>Most of the AI's guardrails and constraints are provided by system architects</p></li><li><p>Granted autonomy to interact with external systems and take real-world actions (e.g., making purchases, booking travel) to achieve its goal.</p></li><li><p>Productivity is mostly unbounded beyond final review (+10x)</p></li><li><p>Designed around a fundamental deliverable</p></li><li><p>Feels like a coach, co-worker, or executive.</p></li></ul></li></ul><p>The spectrum is already visible in the products we use today<a class=\"footnote-anchor\" href=\"https://blog.sshh.io/feed#footnote-3\" id=\"footnote-anchor-3\" target=\"_self\">3</a>:</p><ul><li><p><strong>Music:</strong> An <strong>Assistant</strong> is asking a chatbot to create a playlist for you. An <strong>Orchestrator</strong> is Spotify&#8217;s Daily Mix, which curates playlists automatically based on your listening history, the time of day, and the habits of similar users.</p></li><li><p><strong>Finance:</strong> An <strong>Assistant</strong> is a stock screening tool where you set the filters. An <strong>Orchestrator</strong> is a robo-advisor like Wealthfront that manages your entire portfolio based on a risk profile.</p></li><li><p><strong>Information:</strong> An <strong>Assistant</strong> is Google Search, which waits for your query. An <strong>Orchestrator</strong> is TikTok&#8217;s &#8220;For You&#8221; page, which proactively builds a reality for you based on your passive viewing habits.</p></li><li><p><strong>Shopping:</strong> An <strong>Assistant</strong> is searching for a product on Amazon. An <strong>Orchestrator</strong> is like a Stitch Fix, which curates a box of clothes based on your taste profile, or a smart fridge that automatically re-orders milk.</p></li></ul><div class=\"subscription-widget-wrap-editor\"><div class=\"subscription-widget show-subscribe\"><div class=\"preamble\"><p class=\"cta-caption\">Thanks for reading Shrivu&#8217;s Substack! Subscribe for free to receive new posts and support my work.</p></div><form class=\"subscription-widget-subscribe\"><input class=\"email-input\" name=\"email\" tabindex=\"-1\" type=\"email\" /><input class=\"button primary\" type=\"submit\" value=\"Subscribe\" /><div class=\"fake-input-wrapper\"><div class=\"fake-input\"></div><div class=\"fake-button\"></div></div></form></div></div><h2>Why is this the future?</h2><p>This shift isn't a matter of preference; it's being driven by the twin, irresistible forces of <strong>technological capability</strong> and <strong>economic incentive</strong>.</p><p>Many of today&#8217;s AI Assistants, especially copilots, are the modern equivalent of the <a href=\"https://news.ycombinator.com/item?id=43773813\">horseless carriage</a>. We&#8217;ve bolted a powerful engine onto an old, human-centric way of working, and while it's faster, it&#8217;s not a fundamental change. Many people want AI to act like a human partner, but the optimal design for today&#8217;s (quite powerful) reasoning models isn&#8217;t a conversationalist; it&#8217;s an autonomous system. The most effective way to leverage an LLM is to give it broad context, a clear goal, and \"let it cook.\"<a class=\"footnote-anchor\" href=\"https://blog.sshh.io/feed#footnote-4\" id=\"footnote-anchor-4\" target=\"_self\">4</a></p><p>The economic incentives are even more straightforward. The difference between the bounded productivity of an assistant (+10%) and the unbounded potential of an orchestrator (+10x) is the difference between a helpful feature and a market-defining company. The winning SaaS products will (whether or not this is a good thing) be those that systematically reduce human control and bottlenecks.</p><p>The evolution for successful AI products will be from an assistant to an orchestrator, because automating an entire deliverable creates exponentially more value than simply making a human&#8217;s task a little easier. This shift doesn't just unlock productivity for experts; by simplifying the user's input to a high-level goal, it makes achieving complex outcomes accessible to a much wider group of people.</p><div class=\"captioned-image-container\"><figure><a class=\"image-link image2 is-viewable-img\" href=\"https://substackcdn.com/image/fetch/$s_!QzAV!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4f73e07c-7d68-4322-ae0d-377ed072a5c8_1536x1024.png\" target=\"_blank\"><div class=\"image2-inset\"><source type=\"image/webp\" /><img alt=\"\" class=\"sizing-normal\" height=\"337.4491758241758\" src=\"https://substackcdn.com/image/fetch/$s_!QzAV!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4f73e07c-7d68-4322-ae0d-377ed072a5c8_1536x1024.png\" width=\"506\" /><div class=\"image-link-expand\"><div class=\"pencraft pc-display-flex pc-gap-8 pc-reset\"><div class=\"pencraft pc-reset icon-container restack-image\"><svg class=\"lucide lucide-refresh-cw\" fill=\"none\" height=\"20\" stroke=\"currentColor\" stroke-linecap=\"round\" stroke-linejoin=\"round\" stroke-width=\"2\" viewBox=\"0 0 24 24\" width=\"20\" xmlns=\"http://www.w3.org/2000/svg\"><path d=\"M3 12a9 9 0 0 1 9-9 9.75 9.75 0 0 1 6.74 2.74L21 8\"></path><path d=\"M21 3v5h-5\"></path><path d=\"M21 12a9 9 0 0 1-9 9 9.75 9.75 0 0 1-6.74-2.74L3 16\"></path><path d=\"M8 16H3v5\"></path></svg></div><div class=\"pencraft pc-reset icon-container view-image\"><svg class=\"lucide lucide-maximize2\" fill=\"none\" height=\"20\" stroke=\"currentColor\" stroke-linecap=\"round\" stroke-linejoin=\"round\" stroke-width=\"2\" viewBox=\"0 0 24 24\" width=\"20\" xmlns=\"http://www.w3.org/2000/svg\"><polyline points=\"15 3 21 3 21 9\"></polyline><polyline points=\"9 21 3 21 3 15\"></polyline><line x1=\"21\" x2=\"14\" y1=\"3\" y2=\"10\"></line><line x1=\"3\" x2=\"10\" y1=\"21\" y2=\"14\"></line></svg></div></div></div></div></a><figcaption class=\"image-caption\">Image by ChatGPT. The \"Let it Cook\" analogy (taken somewhat literally): An Assistant needs to be shown every step, while an Orchestrator just needs the recipe&#8212;the goal.</figcaption></figure></div><h2>How science fiction got it wrong</h2><p>While fiction, we often look to sci-fi to extrapolate what the future of society and technology could look like. However, when you compare how AI has been depicted I can&#8217;t help but think that we&#8217;ve really overfit to the concept of an AI assistant and our timelines around machine intelligence and decision making were way off.</p><p><strong>Some interesting differences:</strong></p><ul><li><p><strong>They predicted a revolution in the physical world while the nature of intelligence stayed the same.</strong> Sci-fi gave us incredible physical transformations first&#8212;routine space travel in <em>2001: A Space Odyssey</em>, matter replicators in <em>Star Trek</em>, or flying suits of armor for <em>Jarvis</em>. In these futures, the AI was just a subhuman-like mind in a new setting. Reality did the exact opposite: our physical world is mostly unchanged, but we have access to a fundamentally new kind of intelligence.</p></li><li><p><strong>They made the best AI imitate humans.</strong> By making its best AI a reflection of humanity, sci-fi sold us on a future of conversational \"Assistants.\" We watched characters talk to HAL 9000 and Data, leading us to believe that dialogue was the ultimate interface. But an AI's ability to understand your sarcastic tone is infinitely less valuable than its ability to ingest your entire company's data streams. The true power of an \"Orchestrator\" is unlocked only when we stop asking it to be human and instead leverage its inhuman capacity for complex, large-scale computation.</p></li><li><p><strong>They depicted AI as advanced tools, not advanced intelligences.</strong> The AI in these stories were the world's best instruments, but they still needed a human mind to wield them. Jarvis executed Tony Stark&#8217;s brilliant plans, and the <em>Enterprise</em> computer retrieved facts like a database. Today&#8217;s orchestrators are being built to <em>be</em> the &#8220;mind&#8221;&#8212;capable of generating the strategy, not just following the instructions.</p></li></ul><div class=\"captioned-image-container\"><figure><a class=\"image-link image2 is-viewable-img\" href=\"https://substackcdn.com/image/fetch/$s_!7Zsy!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F42083aa1-8d85-45bd-92fc-09cda0b1949e_1536x1024.png\" target=\"_blank\"><div class=\"image2-inset\"><source type=\"image/webp\" /><img alt=\"\" class=\"sizing-normal\" height=\"362.1243131868132\" src=\"https://substackcdn.com/image/fetch/$s_!7Zsy!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F42083aa1-8d85-45bd-92fc-09cda0b1949e_1536x1024.png\" width=\"543\" /><div class=\"image-link-expand\"><div class=\"pencraft pc-display-flex pc-gap-8 pc-reset\"><div class=\"pencraft pc-reset icon-container restack-image\"><svg class=\"lucide lucide-refresh-cw\" fill=\"none\" height=\"20\" stroke=\"currentColor\" stroke-linecap=\"round\" stroke-linejoin=\"round\" stroke-width=\"2\" viewBox=\"0 0 24 24\" width=\"20\" xmlns=\"http://www.w3.org/2000/svg\"><path d=\"M3 12a9 9 0 0 1 9-9 9.75 9.75 0 0 1 6.74 2.74L21 8\"></path><path d=\"M21 3v5h-5\"></path><path d=\"M21 12a9 9 0 0 1-9 9 9.75 9.75 0 0 1-6.74-2.74L3 16\"></path><path d=\"M8 16H3v5\"></path></svg></div><div class=\"pencraft pc-reset icon-container view-image\"><svg class=\"lucide lucide-maximize2\" fill=\"none\" height=\"20\" stroke=\"currentColor\" stroke-linecap=\"round\" stroke-linejoin=\"round\" stroke-width=\"2\" viewBox=\"0 0 24 24\" width=\"20\" xmlns=\"http://www.w3.org/2000/svg\"><polyline points=\"15 3 21 3 21 9\"></polyline><polyline points=\"9 21 3 21 3 15\"></polyline><line x1=\"21\" x2=\"14\" y1=\"3\" y2=\"10\"></line><line x1=\"3\" x2=\"10\" y1=\"21\" y2=\"14\"></line></svg></div></div></div></div></a><figcaption class=\"image-caption\">Image from ChatGPT. While sci-fi predicted AI assistants on starships, we got generally intelligence(ish) on our cell phones.</figcaption></figure></div><p>To be clear, this isn&#8217;t about pointing out &#8216;gotchas&#8217; in classic sci-fi. Instead, these observations highlight how people today might both underestimate (by limiting AI to an assistant role) and overestimate (by judging it against human-centric workflows) its integration over the next few years.</p><p>I asked Gemini, given this blog post, &#8220;who got AI right?&#8221; It suggested possibly <a href=\"https://en.wikipedia.org/wiki/Culture_series\">Iain M. Banks' </a><em><strong><a href=\"https://en.wikipedia.org/wiki/Culture_series\">C</a></strong></em><strong><a href=\"https://en.wikipedia.org/wiki/Culture_series\">ulture</a></strong><a href=\"https://en.wikipedia.org/wiki/Culture_series\"> novels</a>, which I&#8217;ve never heard of but have now definitely made it onto my reading list.</p><h2>What happened to human-in-the-loop (HITL)?</h2><p>Unlike traditional ML systems, generalist LLMs have this weird property that they get better at reviewing their own outputs at a similar (but offset) rate. A key property of AI orchestration is less and much more intentional HITL.</p><div class=\"captioned-image-container\"><figure><a class=\"image-link image2 is-viewable-img\" href=\"https://substackcdn.com/image/fetch/$s_!1DtM!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F69cb4105-8891-413c-abfe-a7263e393276_1536x1024.png\" target=\"_blank\"><div class=\"image2-inset\"><source type=\"image/webp\" /><img alt=\"\" class=\"sizing-normal\" height=\"349.4532967032967\" src=\"https://substackcdn.com/image/fetch/$s_!1DtM!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F69cb4105-8891-413c-abfe-a7263e393276_1536x1024.png\" width=\"524\" /><div class=\"image-link-expand\"><div class=\"pencraft pc-display-flex pc-gap-8 pc-reset\"><div class=\"pencraft pc-reset icon-container restack-image\"><svg class=\"lucide lucide-refresh-cw\" fill=\"none\" height=\"20\" stroke=\"currentColor\" stroke-linecap=\"round\" stroke-linejoin=\"round\" stroke-width=\"2\" viewBox=\"0 0 24 24\" width=\"20\" xmlns=\"http://www.w3.org/2000/svg\"><path d=\"M3 12a9 9 0 0 1 9-9 9.75 9.75 0 0 1 6.74 2.74L21 8\"></path><path d=\"M21 3v5h-5\"></path><path d=\"M21 12a9 9 0 0 1-9 9 9.75 9.75 0 0 1-6.74-2.74L3 16\"></path><path d=\"M8 16H3v5\"></path></svg></div><div class=\"pencraft pc-reset icon-container view-image\"><svg class=\"lucide lucide-maximize2\" fill=\"none\" height=\"20\" stroke=\"currentColor\" stroke-linecap=\"round\" stroke-linejoin=\"round\" stroke-width=\"2\" viewBox=\"0 0 24 24\" width=\"20\" xmlns=\"http://www.w3.org/2000/svg\"><polyline points=\"15 3 21 3 21 9\"></polyline><polyline points=\"9 21 3 21 3 15\"></polyline><line x1=\"21\" x2=\"14\" y1=\"3\" y2=\"10\"></line><line x1=\"3\" x2=\"10\" y1=\"21\" y2=\"14\"></line></svg></div></div></div></div></a><figcaption class=\"image-caption\">Image from ChatGPT. The different stages of HITL.</figcaption></figure></div><p>For a given end-to-end task, you have a few incremental stages of HITL:</p><ol><li><p>Human does the task (no AI, 1x)</p></li><li><p>Human uses an AI copilot to complete the task (AI assistant, 1.2x)</p></li><li><p>AI does the task, human and AI reviews (AI orchestrated, 3x)</p></li><li><p>AI does the task, AI reviews, human sometimes reviews (AI orchestrated, 10x)</p></li><li><p>AI does the task, AI reviews (AI orchestrated, 100x)</p></li></ol><p>The critical switchover happens at (3), and the incentivized end state is (5). The exact transition points depend on the task, model capabilities, ROI of automation, and our comfort level as a society for automation in a given domain (fast food order taking vs self-driving vs AI-powered governance). As AI products lag behind model capabilities, there&#8217;s more potential energy for (1) to (5) jumps in very short periods&#8230; which will have some interesting impacts on the labor market. </p><p>Another side-effect is that people who are rapidly keeping up with using AI tools will be the least impacted by these transitions as they are already working within a higher HITL tier of their role<a class=\"footnote-anchor\" href=\"https://blog.sshh.io/feed#footnote-5\" id=\"footnote-anchor-5\" target=\"_self\">5</a>.</p><p><strong>What about taste, creativity, human-interaction?</strong></p><ul><li><p><strong>Taste</strong> - This to me remains the fundamental human edge. This comes from both field experts (i.e. founders and designers who take unique high-alpha bets) but also systems that sort of &#8220;extract&#8221; this through media platforms (i.e. taste as an aggregation of human-produced TikTok swipes).</p></li><li><p><strong>Creativity</strong> - This is more of a philosophical debate, but it&#8217;s a safe bet to (unfortunately) assume that humans will not be paid for their ability to be creative. People also tend to underestimate AI&#8217;s capacity for synthetic creativity and generating novel ideas.</p></li><li><p><strong>Human Interaction</strong> - This may be the domain we intentionally reserve for at-times \"suboptimal\" but meaningful connection. In a field like therapy, human interaction could also become more of a luxury than the standard.<a class=\"footnote-anchor\" href=\"https://blog.sshh.io/feed#footnote-6\" id=\"footnote-anchor-6\" target=\"_self\">6</a>.</p></li></ul><p>There are some obvious follow up questions around jobs and reliance which deserve their own post, for now I&#8217;ll recommend <strong><a href=\"https://blog.sshh.io/p/working-with-systems-smarter-than\">Working with Systems Smarter Than You</a>.</strong></p><h2>A few of the many open questions</h2><p>Some questions I&#8217;ve been thinking about along with Gemini-generated commentary.</p><ul><li><p><strong>How do we balance the relentless drive for innovation with the fundamental need for human control and agency?</strong></p><ul><li><p><em>The optimistic path is a conscious balance, where we use transparent \"control panels\" to automate mundane tasks, freeing ourselves for what truly matters. The darker path is a slow erosion of agency through a thousand convenient optimizations, leading to a state of learned helplessness where our lives are guided by systems we no longer control.</em></p></li></ul></li><li><p><strong>What does an AI-orchestrated economy look like when most products are no longer sold to humans, but from one AI to another?</strong></p><ul><li><p><em>A vast \"machine-to-machine\" market may emerge for all utilities and commodities, where AIs trade directly and human-facing marketing for those goods becomes obsolete. More profoundly, the very engine of GDP could shift. In a future where AIs are the primary economic actors, a nation's power may be measured less by its human talent and more by its raw datacenter capacity and energy infrastructure.</em></p></li></ul></li><li><p><strong>Who gets to be an 'Architect' of these orchestrated systems, and how do we prevent their inevitable biases from becoming our invisible laws?</strong></p><ul><li><p><em>One path leads to a \"technocratic feudalism,\" where the biases of a small class of architects at dominant companies become our invisible laws. The more hopeful alternative is a thriving ecosystem of open-source and auditable orchestrators, allowing individuals and communities to choose systems aligned with their own values, favoring pluralism over centralized optimization.</em></p></li></ul></li></ul><div class=\"subscription-widget-wrap-editor\"><div class=\"subscription-widget show-subscribe\"><div class=\"preamble\"><p class=\"cta-caption\">Thanks for reading Shrivu&#8217;s Substack! Subscribe for free to receive new posts and support my work.</p></div><form class=\"subscription-widget-subscribe\"><input class=\"email-input\" name=\"email\" tabindex=\"-1\" type=\"email\" /><input class=\"button primary\" type=\"submit\" value=\"Subscribe\" /><div class=\"fake-input-wrapper\"><div class=\"fake-input\"></div><div class=\"fake-button\"></div></div></form></div></div><div class=\"footnote\"><a class=\"footnote-number\" contenteditable=\"false\" href=\"https://blog.sshh.io/feed#footnote-anchor-1\" id=\"footnote-1\" target=\"_self\">1</a><div class=\"footnote-content\"><p>Back in the day I had to write a lot of <a href=\"https://www.instructure.com/canvas\">Canvas</a> discussion board posts that were tedious so I used GPT2 to help me brainstorm what to write. I&#8217;d construct this prefix of the instructions and several other people&#8217;s posts (&#8220;&lt;topic&gt; &lt;answer title 1&gt; &lt;answer 1&gt; &lt;answer title 2&gt; &lt;answer 2&gt; &lt;my answer title 2&gt;&#8220;) and then the playground would auto-complete the answer for my unique title. I&#8217;d run this like 20 times at different temperatures and then use the (directionally useful) slop that came out to figure out what I actually wanted to write. Getting the prefix formatting just right was a fun skill that later turned into prompt-engineering when ChatGPT eventually came out.</p></div></div><div class=\"footnote\"><a class=\"footnote-number\" contenteditable=\"false\" href=\"https://blog.sshh.io/feed#footnote-anchor-2\" id=\"footnote-2\" target=\"_self\">2</a><div class=\"footnote-content\"><p>&#8220;Orchestrator&#8221; isn&#8217;t a great name (as some folks I work with have also pointed out) because it almost implies that it&#8217;s picking what work to do rather than doing the work itself. Using this for now since Gemini and I were not able to figure out a better one. &#8220;Agents&#8221; might&#8217;ve been a good one but that&#8217;s a pretty convoluted term now.</p></div></div><div class=\"footnote\"><a class=\"footnote-number\" contenteditable=\"false\" href=\"https://blog.sshh.io/feed#footnote-anchor-3\" id=\"footnote-3\" target=\"_self\">3</a><div class=\"footnote-content\"><p>After brainstorming these examples, it was interesting to me that all of these ended up being variants of recommendation systems. I had Gemini draft some thoughts as to: <strong><a href=\"https://docs.google.com/document/d/1pwIAet8XxFtfsoH9uKEy3gwZXeIKkrUJVZgJ5qc3SvU/edit?tab=t.0\">Why Recommendation Systems Are AI Orchestrators</a></strong>. I feel like this document doubles as a rubric for what I&#8217;d consider &#8220;good&#8221; AI startup ideas to invest in.</p></div></div><div class=\"footnote\"><a class=\"footnote-number\" contenteditable=\"false\" href=\"https://blog.sshh.io/feed#footnote-anchor-4\" id=\"footnote-4\" target=\"_self\">4</a><div class=\"footnote-content\"><p>For a more concrete application of this reasoning, see <strong><a href=\"https://blog.sshh.io/p/building-multi-agent-systems-part\">Building Multi-Agent Systems (Part 2)</a></strong></p></div></div><div class=\"footnote\"><a class=\"footnote-number\" contenteditable=\"false\" href=\"https://blog.sshh.io/feed#footnote-anchor-5\" id=\"footnote-5\" target=\"_self\">5</a><div class=\"footnote-content\"><p>Specifically for software engineers, you are at a consistent disadvantage if you are working at only the expected HITL tier which is either <strong>1</strong> (company does not <em>expect</em> AI; you do not use AI to code) or more recently <strong>2</strong> (company expects copilot; you only use it as coding assistant vs background PR one-shotter). By the time an organization reaches <strong>5</strong>, ideally you&#8217;ve already shifted into a more impactful role which isn&#8217;t writing code.</p></div></div><div class=\"footnote\"><a class=\"footnote-number\" contenteditable=\"false\" href=\"https://blog.sshh.io/feed#footnote-anchor-6\" id=\"footnote-6\" target=\"_self\">6</a><div class=\"footnote-content\"><p>This is also potentially driven by <a href=\"https://en.wikipedia.org/wiki/Baumol_effect\">Baumol's cost disease</a>: as AI boosts productivity and wages in most tech-driven industries, labor-intensive fields like therapy must also raise wages to compete for talent. Since a human therapist's core productivity (one hour of human connection) remains constant, the service inevitably becomes a relative luxury. On the plus side, the average cost of getting <em>some</em> form of support will likely decrease.</p></div></div>"
            ],
            "link": "https://blog.sshh.io/p/assistants-arent-the-future-of-ai",
            "publishedAt": "2025-07-19",
            "source": "Shrivu Shankar",
            "summary": "<p>Today&#8217;s most popular vision for the future of AI is also the least imaginative one. The perfect AI assistant feels like the end-game, but it's just the prelude to a much more significant shift in design: the move from AI Assistants to AI Orchestrators.</p><p>When GPT-2 first came out, it wasn&#8217;t a chat app but instead an advanced auto-complete that you could play with in the OpenAI playground. While a power user for getting it to marginally support some of my homework assignments at the time<a class=\"footnote-anchor\" href=\"https://blog.sshh.io/feed#footnote-1\" id=\"footnote-anchor-1\" target=\"_self\">1</a>, I (and I&#8217;m sure many others) had no idea that later finetuning this base model into an assistant would lead to such a fundamental shift in how and where these large language models (LLMs) could be used. The vision for what LLMs could be used for completely changed.</p><p>I think there&#8217;s another, albeit more nuanced, shift now from AI Assistants to what I&#8217;ll call AI Orchestrators<a class=\"footnote-anchor\" href=\"https://blog.sshh.io/feed#footnote-2\" id=\"footnote-anchor-2\" target=\"_self\">2</a>. They're still LLM-based, and not quite the same as what most folks associate with the term &#8220;agents,&#8221; but agency is a large piece of it.</p><p>In this post, I&#8217;ll explore why this shift to orchestration is the real future of AI, how some",
            "title": "Assistants Aren't the Future of AI"
        },
        {
            "content": [],
            "link": "https://lethain.com/orchestration-heavy-leadership-heavy/",
            "publishedAt": "2025-07-19",
            "source": "Will Larson",
            "summary": "<p>For managers who have spent a long time reporting to a specific leader or working in an organization with well\u2011understood goals, it&rsquo;s easy to develop skill gaps without realizing it. Usually this happens because those skills were not particularly important in the environment you grew up in. You may become extremely confident in your existing skills, enter a new organization that requires a different mix of competencies, and promptly fall on your face.</p> <p>There are a few common varieties of this, but the one I want to discuss here is when managers grow up in an organization that operates from top\u2011down plans (\u201corchestration\u2011heavy roles\u201d) and then find themselves in a sufficiently senior role, or in a bottom\u2011up organization, that expects them to lead rather than orchestrate (\u201cleadership\u2011heavy roles\u201d).</p> <h2 id=\"orchestration-versus-leadership\">Orchestration versus leadership</h2> <p>You can break the components of solving a problem down in a number of ways, and I&rsquo;m not saying this is the perfect way to do it, but here are six important components of directing a team&rsquo;s work:</p> <ol> <li><strong>Problem discovery</strong>: Identifying which problems to work on</li> <li><strong>Problem selection</strong>: Aligning with your stakeholders on the problems you&rsquo;ve identified</li> <li><strong>Solution discovery</strong>: Identifying potential solutions to the selected problem</li> <li><strong>Solution",
            "title": "Moving from an orchestration-heavy to leadership-heavy management role."
        }
    ],
    "lookbackDays": 1,
    "publishDate": "2025-07-19"
}