{
    "articles": [
        {
            "content": [
                "<div id=\"blog\"><div id=\"content\">\n  <div id=\"content\">\n\n    <div class=\"Article\">\n    \n    <h1 class=\"small\"><a href=\"https://go.dev/blog/\">The Go Blog</a></h1>\n    \n\n    <h1>Go 1.26 is released</h1>\n      \n      <p class=\"author\">\n      Carlos Amedee, on behalf of the Go team<br />\n      10 February 2026\n      </p>\n      \n      <div class=\"markdown\">\n<p>Today the Go team is pleased to release Go 1.26.\nYou can find its binary archives and installers on the <a href=\"https://go.dev/dl/\">download page</a>.</p>\n<h2 id=\"language-changes\">Language changes</h2>\n<p>Go 1.26 introduces two significant refinements to the language\n<a href=\"https://go.dev/doc/go1.26#language\">syntax and type system</a>.</p>\n<p>First, the built-in <code>new</code> function, which creates a new variable, now allows its operand to be an\nexpression, specifying the initial value of the variable.</p>\n<p>A simple example of this change means that code such as this:</p>\n<pre><code class=\"language-go\">x := int64(300)\nptr := &amp;x\n</code></pre>\n<p>Can be simplified to:</p>\n<pre><code class=\"language-go\">ptr := new(int64(300))\n</code></pre>\n<p>Second, generic types may now refer to themselves in their own type parameter list. This change\nsimplifies the implementation of complex data structures and interfaces.</p>\n<h2 id=\"performance-improvements\">Performance improvements</h2>\n<p>The previously experimental <a href=\"https://go.dev/doc/go1.26#new-garbage-collector\">Green Tea garbage collector</a>\nis now enabled by default.</p>\n<p>The baseline <a href=\"https://go.dev/doc/go1.26#faster-cgo-calls\">cgo overhead has been reduced</a>\nby approximately 30%.</p>\n<p>The compiler can now <a href=\"https://go.dev/doc/go1.26#compiler\">allocate the backing store</a> for\nslices on the stack in more situations, which improves performance.</p>\n<h2 id=\"tool-improvements\">Tool improvements</h2>\n<p>The <code>go fix</code> command has been completely rewritten to use the\n<a href=\"https://go.dev/pkg/golang.org/x/tools/go/analysis\">Go analysis framework</a>, and now includes a\ncouple dozen &ldquo;<a href=\"https://go.dev/pkg/golang.org/x/tools/go/analysis/passes/modernize\">modernizers</a>&rdquo;, analyzers\nthat suggest safe fixes to help your code take advantage of newer features of the language\nand standard library. It also includes the\n<a href=\"https://go.dev/pkg/golang.org/x/tools/go/analysis/passes/inline#hdr-Analyzer_inline\"><code>inline</code> analyzer</a>, which\nattempts to inline all calls to each function annotated with a <code>//go:fix inline</code> directive.\nTwo upcoming blog posts will address these features in more detail.</p>\n<h2 id=\"more-improvements-and-changes\">More improvements and changes</h2>\n<p>Go 1.26 introduces many improvements over Go 1.25 across\nits <a href=\"https://go.dev/doc/go1.26#tools\">tools</a>, the <a href=\"https://go.dev/doc/go1.26#runtime\">runtime</a>,\n<a href=\"https://go.dev/doc/go1.26#compiler\">compiler</a>, <a href=\"https://go.dev/doc/go1.26#linker\">linker</a>,\nand the <a href=\"https://go.dev/doc/go1.26#library\">standard library</a>.\nThis includes the addition of three new packages: <a href=\"https://go.dev/doc/go1.26#new-cryptohpke-package\"><code>crypto/hpke</code></a>,\n<a href=\"https://go.dev/doc/go1.26#cryptomlkempkgcryptomlkem\"><code>crypto/mlkem/mlkemtest</code></a>, and\n<a href=\"https://go.dev/doc/go1.26#testingcryptotestpkgtestingcryptotest\"><code>testing/cryptotest</code></a>.\nThere are <a href=\"https://go.dev/doc/go1.26#ports\">port-specific</a> changes\nand <a href=\"https://go.dev/doc/godebug#go-126\"><code>GODEBUG</code> settings</a> updates.</p>\n<p>Some of the additions in Go 1.26 are in an experimental stage\nand become exposed only when you explicitly opt in. Notably:</p>\n<ul>\n<li>\n<p>An <a href=\"https://go.dev/doc/go1.26#simd\">experimental <code>simd/archsimd</code> package</a> provides access to &ldquo;single instruction,\nmultiple data&rdquo; (SIMD) operations.</p>\n</li>\n<li>\n<p>An <a href=\"https://go.dev/doc/go1.26#new-experimental-runtimesecret-package\">experimental <code>runtime/secret</code> package</a> provides\na facility for securely erasing temporaries used in code that manipulates secret\ninformation, typically cryptographic in nature.</p>\n</li>\n<li>\n<p>An <a href=\"https://go.dev/doc/go1.26#goroutineleak-profiles\">experimental <code>goroutineleak</code> profile</a>\nin the <code>runtime/pprof</code> package that reports leaked goroutines.</p>\n</li>\n</ul>\n<p>These experiments are all expected to be generally available in a\nfuture version of Go. We encourage you to try them out ahead of time.\nWe really value your feedback!</p>\n<p>Please refer to the <a href=\"https://go.dev/doc/go1.26\">Go 1.26 Release Notes</a> for the complete list\nof additions, changes, and improvements in Go 1.26.</p>\n<p>Over the next few weeks, follow-up blog posts will cover some of the topics\nrelevant to Go 1.26 in more detail. Check back later to read those posts.</p>\n<p>Thanks to everyone who contributed to this release by writing code, filing bugs,\ntrying out experimental additions, sharing feedback, and testing the release candidates.\nYour efforts helped make Go 1.26 as stable as possible.\nAs always, if you notice any problems, please <a href=\"https://go.dev/issue/new\">file an issue</a>.</p>\n<p>We hope you enjoy using the new release!</p>\n</div>\n\n    </div>\n\n    \n    <div class=\"Article prevnext\">\n    \n    \n      \n        <p>\n        \n        \n          \n            <b>Previous article: </b><a href=\"https://go.dev/blog/survey2025\">Results from the 2025 Go Developer Survey</a><br />\n          \n        \n        <b><a href=\"https://go.dev/blog/all\">Blog Index</a></b>\n      \n    \n      \n    \n      \n    \n      \n    \n      \n    \n      \n    \n      \n    \n      \n    \n      \n    \n      \n    \n      \n    \n      \n    \n      \n    \n      \n    \n      \n    \n      \n    \n      \n    \n      \n    \n      \n    \n      \n    \n      \n    \n      \n    \n      \n    \n      \n    \n      \n    \n      \n    \n      \n    \n      \n    \n      \n    \n      \n    \n      \n    \n      \n    \n      \n    \n      \n    \n      \n    \n      \n    \n      \n    \n      \n    \n      \n    \n      \n    \n      \n    \n      \n    \n      \n    \n      \n    \n      \n    \n      \n    \n      \n    \n      \n    \n      \n    \n      \n    \n      \n    \n      \n    \n      \n    \n      \n    \n      \n    \n      \n    \n      \n    \n      \n    \n      \n    \n      \n    \n      \n    \n      \n    \n      \n    \n      \n    \n      \n    \n      \n    \n      \n    \n      \n    \n      \n    \n      \n    \n      \n    \n      \n    \n      \n    \n      \n    \n      \n    \n      \n    \n      \n    \n      \n    \n      \n    \n      \n    \n      \n    \n      \n    \n      \n    \n      \n    \n      \n    \n      \n    \n      \n    \n      \n    \n      \n    \n      \n    \n      \n    \n      \n    \n      \n    \n      \n    \n      \n    \n      \n    \n      \n    \n      \n    \n      \n    \n      \n    \n      \n    \n      \n    \n      \n    \n      \n    \n      \n    \n      \n    \n      \n    \n      \n    \n      \n    \n      \n    \n      \n    \n      \n    \n      \n    \n      \n    \n      \n    \n      \n    \n      \n    \n      \n    \n      \n    \n      \n    \n      \n    \n      \n    \n      \n    \n      \n    \n      \n    \n      \n    \n      \n    \n      \n    \n      \n    \n      \n    \n      \n    \n      \n    \n      \n    \n      \n    \n      \n    \n      \n    \n      \n    \n      \n    \n      \n    \n      \n    \n      \n    \n      \n    \n      \n    \n      \n    \n      \n    \n      \n    \n      \n    \n      \n    \n      \n    \n      \n    \n      \n    \n      \n    \n      \n    \n      \n    \n      \n    \n      \n    \n      \n    \n      \n    \n      \n    \n      \n    \n      \n    \n      \n    \n      \n    \n      \n    \n      \n    \n      \n    \n      \n    \n      \n    \n      \n    \n      \n    \n      \n    \n      \n    \n      \n    \n      \n    \n      \n    \n      \n    \n      \n    \n      \n    \n      \n    \n      \n    \n      \n    \n      \n    \n      \n    \n      \n    \n      \n    \n      \n    \n      \n    \n      \n    \n      \n    \n      \n    \n      \n    \n      \n    \n      \n    \n      \n    \n      \n    \n      \n    \n      \n    \n      \n    \n      \n    \n      \n    \n      \n    \n      \n    \n      \n    \n      \n    \n      \n    \n      \n    \n      \n    \n      \n    \n      \n    \n      \n    \n      \n    \n      \n    \n      \n    \n      \n    \n      \n    \n      \n    \n      \n    \n      \n    \n      \n    \n      \n    \n      \n    \n      \n    \n      \n    \n      \n    \n      \n    \n      \n    \n      \n    \n      \n    \n      \n    \n      \n    \n      \n    \n      \n    \n      \n    \n      \n    \n      \n    \n      \n    \n      \n    \n      \n    \n      \n    \n      \n    \n      \n    \n      \n    \n      \n    \n      \n    \n      \n    \n      \n    \n      \n    \n      \n    \n      \n    \n      \n    \n      \n    \n      \n    \n      \n    \n      \n    \n      \n    \n      \n    \n      \n    \n      \n    \n      \n    \n      \n    \n      \n    \n      \n    \n      \n    \n      \n    \n      \n    \n      \n    \n      \n    \n      \n    \n      \n    \n      \n    \n      \n    \n      \n    \n      \n    \n      \n    \n      \n    \n      \n    \n      \n    \n      \n    \n      \n    \n      \n    \n      \n    \n      \n    \n      \n    \n      \n    \n      \n    \n      \n    \n      \n    \n      \n    \n      \n    \n      \n    \n      \n    \n      \n    \n      \n    \n      \n    \n      \n    \n      \n    \n      \n    \n      \n    \n      \n    \n      \n    \n      \n    \n      \n    \n      \n    \n      \n    \n      \n    \n      \n    \n      \n    \n      \n    \n      \n    \n      \n    \n      \n    \n      \n    \n      \n    \n      \n    \n      \n    \n      \n    \n      \n    \n      \n    \n      \n    \n      \n    \n      \n    \n      \n    \n      \n    \n      \n    \n      \n    \n      \n    \n      \n    \n      \n    \n      \n    \n      \n    \n      \n    \n    </div>\n    \n\n  </div>\n</div>"
            ],
            "link": "https://go.dev/blog/go1.26",
            "publishedAt": "2026-02-10",
            "source": "Go Blog",
            "summary": "<div id=\"blog\"><div id=\"content\"> <div id=\"content\"> <div class=\"Article\"> <h1 class=\"small\"><a href=\"https://go.dev/blog/\">The Go Blog</a></h1> <h1>Go 1.26 is released</h1> <p class=\"author\"> Carlos Amedee, on behalf of the Go team<br /> 10 February 2026 </p> <div class=\"markdown\"> <p>Today the Go team is pleased to release Go 1.26. You can find its binary archives and installers on the <a href=\"https://go.dev/dl/\">download page</a>.</p> <h2 id=\"language-changes\">Language changes</h2> <p>Go 1.26 introduces two significant refinements to the language <a href=\"https://go.dev/doc/go1.26#language\">syntax and type system</a>.</p> <p>First, the built-in <code>new</code> function, which creates a new variable, now allows its operand to be an expression, specifying the initial value of the variable.</p> <p>A simple example of this change means that code such as this:</p> <pre><code class=\"language-go\">x := int64(300) ptr := &amp;x </code></pre> <p>Can be simplified to:</p> <pre><code class=\"language-go\">ptr := new(int64(300)) </code></pre> <p>Second, generic types may now refer to themselves in their own type parameter list. This change simplifies the implementation of complex data structures and interfaces.</p> <h2 id=\"performance-improvements\">Performance improvements</h2> <p>The previously experimental <a href=\"https://go.dev/doc/go1.26#new-garbage-collector\">Green Tea garbage collector</a> is now enabled by default.</p> <p>The baseline <a href=\"https://go.dev/doc/go1.26#faster-cgo-calls\">cgo overhead has been reduced</a> by approximately 30%.</p> <p>The compiler can now <a href=\"https://go.dev/doc/go1.26#compiler\">allocate the backing store</a> for slices on the stack in more situations, which improves performance.</p> <h2",
            "title": "Go 1.26 is released"
        },
        {
            "content": [
                "<p>I've realized that I have two primary ways that I'm building software with AI.</p>\n<p>The first is the one that Superpowers excels at. I'll spend a significant amount of time up front thinking through exactly what I want to build. Usually this is in conversation with the brainstorming skill. When I say &quot;a significant amount of time,&quot; sometimes that's five minutes for a tiny little thing. And sometimes it's four-plus hours over the course of a day as we rigorously explore a problem space and what the solution looks like. The output of that is often an initial spec document that is many thousands of lines long and covers all sorts of details about the implementation.</p>\n<p>From there, I can ask Claude or Codex to write out an implementation plan. That implementation plan might run for anywhere between a few minutes and 7-8 hours. The end result is, ideally, a fully baked, usable implementation.</p>\n<p>When it's done, I ask it to prove to me that the implementation works. Typically that's by asking it to run through end-to-end test scenarios and to take screenshots, transcripts, or screen recordings of the work and to present them to me in a directory.</p>\n<p>Doing this with an orchestrator I've been working on last week, I woke up to find Codex telling me that it had successfully completed the project with a pointer to where on disk I could find the movie of all the screenshots it had taken. It was named something like &quot;e2e-test-full-run-33.mp4&quot;</p>\n<p>...&quot;run 33&quot;</p>\n<p>I poked around a little bit. And indeed, there were artifacts from run 1 through run 32. Run 1 didn't even start.  But as the agent worked through problems one-by-one, it managed to get further and further each time. And by run 33, it worked. Pretty cool.</p>\n<p>Sometimes things don't go as planned and the product that comes out the other end is really not what I wanted or needed. At that point, the right thing to do is usually to start over from the original specs (and possibly the wrong code) and restart the spec and design process. Then implement again from scratch. There are absolutely projects that I've run through this process five or six times as I figured out what I actually wanted or the right way to explain what I was going for.</p>\n<p>That's what often gets called 'fast waterfall' style development.  Big up-front design and then a complete implementation with...no intermediate steps. Agents have made this process viable, sort of.</p>\n<p>And then there's the other modality. This is the one that Superpowers doesn't (currently) provide a ton of process support for.</p>\n<p>Often I'll have a feature request for a working product. Usually this is something small, like &quot;oh, the panel should be on the left&quot; or &quot;let's change streaming mode output so that instead of chunking by token, it chunks by sentence.&quot;</p>\n<p>This is typically something that's a relatively small change that the agent can probably one-shot from a one or two-line prompt.</p>\n<p>The way I do it is usually by having the product open, looking at it, asking Claude to make the change, and looking at it again.</p>\n<p>It's basically a &quot;polishing&quot; workflow. Ideally, everything I'm changing should have been part of the original spec, but the changes are usually too small to make it worthwhile to run through a rebuild or a &quot;serious&quot; change cycle.</p>\n<p>As I was thinking about how to explain this flow, I was reminded of the Japanese art of Dorodango.</p>\n<p>Dorodango is, essentially, the process of polishing a ball of dirt into a beautiful, high-gloss sphere. The result is genuinely amazing.</p>\n<p>If you look at <a href=\"https://en.wikipedia.org/wiki/Dorodango\">the Wikipedia article</a>, it starts with this disambiguation statement:</p>\n<blockquote>\n<p>&quot;Mud ball&quot; redirects here. For the computer code style, see\u00a0<a href=\"https://en.wikipedia.org/wiki/Spaghetti_code#Big_Ball_of_Mud\">Big Ball of Mud</a></p>\n</blockquote>\n<p>And there's something beautiful and...right about that.</p>\n<p>There's definitely a perception I've heard from folks who haven't spent a lot of time with the tools that the output of coding agents is always going to be a classical big ball of mud -- a horrible monstrosity with no clear architecture...just a jumbled mess of code that kind of somehow does the thing.</p>\n<p>It's not <em>true</em>, but that's what many folks think. So why not lean into it?</p>\n<p>I find myself engaging in software Dorodango pretty much every day.</p>\n<p><a class=\"glightbox\" href=\"https://blog.fsck.com/assets/2026/02/pasted-image-20260210-141022.png\"><img alt=\"pasted image 20260210 141022\" src=\"https://blog.fsck.com/assets/2026/02/pasted-image-20260210-141022.png\" /></a>\n<em>[Photo by Asturio Cantabrio - Own work, CC BY-SA 4.0](https://commons.wikimedia.org/w/index.php?curid=94863887]</em></p>"
            ],
            "link": "https://blog.fsck.com/2026/02/10/dorodango/",
            "publishedAt": "2026-02-10",
            "source": "Jesse Vincent",
            "summary": "<p>I've realized that I have two primary ways that I'm building software with AI.</p> <p>The first is the one that Superpowers excels at. I'll spend a significant amount of time up front thinking through exactly what I want to build. Usually this is in conversation with the brainstorming skill. When I say &quot;a significant amount of time,&quot; sometimes that's five minutes for a tiny little thing. And sometimes it's four-plus hours over the course of a day as we rigorously explore a problem space and what the solution looks like. The output of that is often an initial spec document that is many thousands of lines long and covers all sorts of details about the implementation.</p> <p>From there, I can ask Claude or Codex to write out an implementation plan. That implementation plan might run for anywhere between a few minutes and 7-8 hours. The end result is, ideally, a fully baked, usable implementation.</p> <p>When it's done, I ask it to prove to me that the implementation works. Typically that's by asking it to run through end-to-end test scenarios and to take screenshots, transcripts, or screen recordings of the work and to present them to me in a directory.</p> <p>Doing",
            "title": "Dorodango"
        },
        {
            "content": [
                "<p><em>When the man across the table knows how the game ends, your only move is to fold.</em></p><p>In late 2025, a pseudonymous trader on Polymarket that went by <em>AlphaRaccoon</em> began placing bets on one of the most obscure corners in prediction markets: Google&#8217;s Year in Search trends. Every December, Google publishes a ranking of the year&#8217;s top trending search topics, a proprietary data set known in advance only to a small number of employees inside the company. On Polymarket, you could wager on which topics would make the list and how they&#8217;d rank. Most traders treated these contracts as low-stakes entertainment. <em><strong>AlphaRaccoon</strong></em><strong> predicted 22 out of 23 search rankings correctly and made over a million dollars.</strong> Months earlier, the same wallet made over $150,000 by predicting the exact release date of Google&#8217;s Gemini 3.0.</p><div class=\"subscription-widget-wrap-editor\"><div class=\"subscription-widget show-subscribe\"><div class=\"preamble\"><p class=\"cta-caption\">Thanks for reading Dopamine Markets! Subscribe for free to receive new posts and support my work.</p></div><form class=\"subscription-widget-subscribe\"><input class=\"email-input\" name=\"email\" tabindex=\"-1\" type=\"email\" /><input class=\"button primary\" type=\"submit\" value=\"Subscribe\" /><div class=\"fake-input-wrapper\"><div class=\"fake-input\"></div><div class=\"fake-button\"></div></div></form></div></div><p>This isn&#8217;t an isolated incident. There are allegations of insider trading in several markets: Taylor Swift&#8217;s engagement proposal, the 2025 Nobel Peace Prize winner, the US intervention in Venezuela, the release date for OpenAI&#8217;s GPT 5.2 model, and the results of over 100 UFC fights.</p><div class=\"captioned-image-container\"><figure><a class=\"image-link image2 is-viewable-img\" href=\"https://substackcdn.com/image/fetch/$s_!PKBV!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2efe7ea8-24ba-4ee7-af2e-02a0e392a7c8_1946x1796.png\" target=\"_blank\"><div class=\"image2-inset\"><source type=\"image/webp\" /><img alt=\"\" class=\"sizing-normal\" height=\"1344\" src=\"https://substackcdn.com/image/fetch/$s_!PKBV!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2efe7ea8-24ba-4ee7-af2e-02a0e392a7c8_1946x1796.png\" width=\"1456\" /><div class=\"image-link-expand\"><div class=\"pencraft pc-display-flex pc-gap-8 pc-reset\"><button class=\"pencraft pc-reset pencraft icon-container restack-image\" tabindex=\"0\" type=\"button\"><svg fill=\"none\" height=\"20\" stroke=\"var(--color-fg-primary)\" stroke-linecap=\"round\" stroke-linejoin=\"round\" stroke-width=\"1.5\" viewBox=\"0 0 20 20\" width=\"20\" xmlns=\"http://www.w3.org/2000/svg\"><g><title></title><path d=\"M2.53001 7.81595C3.49179 4.73911 6.43281 2.5 9.91173 2.5C13.1684 2.5 15.9537 4.46214 17.0852 7.23684L17.6179 8.67647M17.6179 8.67647L18.5002 4.26471M17.6179 8.67647L13.6473 6.91176M17.4995 12.1841C16.5378 15.2609 13.5967 17.5 10.1178 17.5C6.86118 17.5 4.07589 15.5379 2.94432 12.7632L2.41165 11.3235M2.41165 11.3235L1.5293 15.7353M2.41165 11.3235L6.38224 13.0882\"></path></g></svg></button><button class=\"pencraft pc-reset pencraft icon-container view-image\" tabindex=\"0\" type=\"button\"><svg class=\"lucide lucide-maximize2 lucide-maximize-2\" fill=\"none\" height=\"20\" stroke=\"currentColor\" stroke-linecap=\"round\" stroke-linejoin=\"round\" stroke-width=\"2\" viewBox=\"0 0 24 24\" width=\"20\" xmlns=\"http://www.w3.org/2000/svg\"><polyline points=\"15 3 21 3 21 9\"></polyline><polyline points=\"9 21 3 21 3 15\"></polyline><line x1=\"21\" x2=\"14\" y1=\"3\" y2=\"10\"></line><line x1=\"3\" x2=\"10\" y1=\"21\" y2=\"14\"></line></svg></button></div></div></div></a></figure></div><p>Now, athletes and their associates, musicians and their friends, founders and employees of technology companies, and politicians and their staff, all have privileged access to information they can trade on. When nearly everything becomes a market, nearly everyone has inside information. </p><blockquote><p>The democratization of investing leads to the democratization of white collar crime.</p></blockquote><div><hr /></div><h1>The Debate</h1><h3><strong>&#8220;Insiders make prices more accurate&#8221;</strong></h3><p>There is a real intellectual argument that insider trading on prediction markets is actually desirable. Robin Hanson, the economist often called the &#8220;godfather of prediction markets,&#8221; is the biggest proponent. The point of prediction markets is to produce accurate prices, and insiders have the most accurate information. Banning them from trading is like banning your best sources from contributing to a newspaper. </p><p>Insider trading is banned for stocks because stocks exist to raise capital. If insiders could trade on private information before it&#8217;s public, they&#8217;d extract value from new investors buying shares, making those investors unwilling to buy at fair prices and forcing companies to raise capital at a much higher cost.&#8203;&#8203;&#8203;&#8203;&#8203;&#8203;&#8203;&#8203;&#8203;&#8203;&#8203;&#8203;&#8203;&#8203;&#8203;&#8203; But prediction markets exist to surface truth.</p><p>When <em>AlphaRaccoon</em> bought Google Year in Search contracts, the prices moved, and anyone watching could have inferred that someone with inside knowledge believed specific outcomes were near-certain. The insider&#8217;s trade was itself a signal, and the market was doing exactly what it was designed to do.</p><p>The argument has holes. If insiders dominate a prediction market, market makers (who take the other side of the trade) bleed, and without their liquidity, the market becomes thinner, more volatile, and paradoxically less informative. Second, there&#8217;s the problem of incentive corruption: if a tech executive can bet on their own product launch date and then adjust the schedule, or a government official can wager on geopolitical outcomes they have the power to influence, the market hasn&#8217;t surfaced &#8220;the truth.&#8221; It has created a financial incentive for manipulation. </p><p>Third, and most importantly, insider trading will not survive scale: if prediction markets grow 10x in volume, the public will not tolerate insiders making millions of dollars trading on privileged information. It is inevitable for regulators to step in to preserve market integrity.</p><p>The history of stock market regulation tells a similar story. When ordinary investors conclude the game is rigged, they stop playing. Trading volume on the NYSE fell 90% between the 1929 crash and 1932. It took the Pecora Commission hearings that exposed systemic fraud and the Securities Exchange Act of 1934, which specifically banned insider trading, to restore enough confidence for markets to function again.</p><p>The most defensible case for insider trading, however, is where the public value of the information is enormous and there&#8217;s no other reliable channel to surface it. A researcher at the Wuhan lab in December 2019 had no institutional incentive to raise the alarm about coronavirus cases, and the institutions that should have, like the Chinese government, actively suppressed it. If that person could have made $100,000 betting on a pandemic contract, the price signal would have been visible to epidemiologists, journalists, and governments weeks before the official acknowledgment. The profit motive becomes a whistleblowing mechanism. </p><p>The problem is that these cases are rare, and the framework you&#8217;d build to encourage them also enables all the harmful ones (Gemini release dates, UFC fights, Taylor Swift&#8217;s engagement). You can&#8217;t write a rule that says &#8220;insider trading is fine when the information is important to humanity&#8221;. But it&#8217;s fair to say that some permissionless markets will always exist, and that helps surface information that may be suppressed.</p><h3><strong>&#8220;Prediction markets aren&#8217;t stocks, so stock market rules don&#8217;t apply&#8221;</strong></h3><p>The second argument for insider trading is a <a href=\"https://fiftycentdollars.substack.com/p/inside-job\">legal one</a>. Prediction market contracts are commodity derivatives regulated by the CFTC, not securities regulated by the SEC. Derivatives markets are designed to let informed participants trade on proprietary information. A corn farmer trading futures based on private knowledge of his own crop isn&#8217;t committing fraud; he&#8217;s doing what the market was built for. The CFTC&#8217;s standard asks whether information was stolen, not whether trading on it was unfair. Extending SEC-style insider trading doctrine to prediction markets, in this view, means importing a framework from a market with a fundamentally different economic purpose.</p><p>But derivatives markets tolerate informed trading because the underlying product (corn, oil, interest rates) is continuous, with prices shaped by many competing participants over time. Even when dominant players exist (OPEC can move oil prices with a single production decision), these markets have deep independent liquidity and decades of regulatory infrastructure built to manage exactly that risk.</p><p>Prediction market event contracts have none of this. The &#8220;commodity&#8221; is a binary question with a single definitive answer, often known in advance by a tiny number of people. When a Google employee bets on Year in Search rankings, there is no crowd of competing informed participants and no regulatory scaffolding. Few people know the answer and everyone else is guessing. The legal distinction between derivatives and equities is real, but it&#8217;s an argument for writing new rules tailored to prediction markets, not for concluding that no rules are needed.</p><p class=\"button-wrapper\"><a class=\"button primary\" href=\"https://www.dopaminemarkets.com/subscribe\"><span>Subscribe now</span></a></p><div><hr /></div><h1>How to Solve Insider Trading</h1><p>There are three broad approaches to address insider trading in prediction markets: platform guardrails, market design, and legal frameworks.</p><h2>Platform Guardrails</h2><p>The most direct interventions are at the platform level: detection, identity verification, and trade controls.</p><p><strong>Detection.</strong> Kalshi <a href=\"https://x.com/mansourtarek_/status/2019415911040454932\">announced</a> that they run a proprietary system called &#8220;Poirot&#8221; that uses pattern recognition to flag suspicious trades in real time. In the past year, they reported over 200 investigations and multiple account freezes. They recently expanded this through a partnership with Solidus Labs, whose &#8220;HALO&#8221; platform layers behavioral analysis on top of order flow data. On the unregulated side, an ecosystem of third-party tools has emerged: Tre Upshaw&#8217;s Insider Finder (roughly 24,000 users, funded in part by a $25,000 Polymarket grant) scans the blockchain for anomalous trades and reports an 85% success rate on flagged situations turning profitable. Similar tools like Unusual Predictions and PolyInsider do the same: they make suspected insider trading as a signal to copy.</p><p><strong>Progressive KYC.</strong> Identity verification requirements could scale with position size, letting casual bettors trade pseudonymously while forcing large, suspicious positions (like <em>ricosuave666</em>&#8217;s four simultaneous bets on Israeli airstrikes from a brand-new account) through meaningful identity checks. Kalshi already requires full KYC for all users. The challenge is implementing this on crypto-native platforms without killing permissionless onboarding.</p><p><strong>Position limits and trade disclosure.</strong> A $5,000 cap for wallets less than 30 days old would have made most insider trades more difficult. Mandatory trade disclosure with delays, modeled on the SEC&#8217;s Form 4 for corporate insiders, would give the public systematic data on large positions rather than forcing community tools to scrape the blockchain ad hoc. A trader could still generate new wallets and rotate IP addresses through a VPN. But more sophisticated surveillance tools can identify wallet clusters, especially when they&#8217;re funded from the same centralized exchange withdrawal. These systems don&#8217;t make insider trading impossible; they just make it harder to get away with.</p><p><strong>Automated public alerts.</strong> If a large, suspicious-looking trade is placed in a market, the platform notifies all participants immediately. This converts the information asymmetry that makes insider trading profitable into the public signal that Hanson&#8217;s defenders claim it is. Building this into platform infrastructure would make it systematic and harder to game.</p><h2>Market Design</h2><p>A prediction market needs someone willing to take the other side of a trade, and that counterparty systematically loses money to insiders. The question is how to design the plumbing so those losses don&#8217;t drive everyone else away.</p><p><strong>The status quo.</strong> Robin Hanson&#8217;s Logarithmic Market Scoring Rule (LMSR) is an automated market maker that provides continuous quotes with a bounded maximum loss. When an insider trades against it, the LMSR absorbs the loss and adjusts the price. That loss is the cost of price discovery: the platform subsidizing accurate prices the way a newspaper pays reporters. This works for smaller markets (corporate forecasting, research) but can&#8217;t support the billion-dollar volumes Polymarket and Kalshi now process. Polymarket moved to a Central Limit Order Book (CLOB), which shifts insider losses from an algorithm to human market makers. Those market makers eventually respond by widening spreads or leaving, producing the same adverse selection spiral.</p><p><strong>Dynamic spread widening.</strong> Platforms could automatically widen the bid-ask spread in a specific market when surveillance detects suspicious activity, making it more expensive for insiders to trade. This is what traditional market makers do intuitively when they suspect informed flow. Automating it would protect liquidity providers in real time.</p><p><strong>Market maker insurance pools.</strong> A pool funded by trading fees could partially reimburse market makers for verified insider trading losses after a market resolves. This wouldn&#8217;t prevent insider trading, but it would keep market makers in the game.</p><p><strong>Segmented order flow.</strong> Market makers could offer tighter spreads to verified accounts and wider spreads to unverified ones, creating a two-tier system where insiders effectively pay more for execution. This is feasible on platforms like Kalshi and Polymarket, where order matching happens off-chain. It&#8217;s harder for fully onchain order books.</p><p><strong>Time-weighted settlement.</strong> Contracts could resolve based on a price-path average (similar to a Time Weighted Average Price) rather than a single binary moment, making it harder for insiders to profit from a single perfectly timed trade. This works better for some contract types (product launches, earnings) than others (military operations, sudden events).</p><p>None of these eliminates insider trading, but together they keep liquidity providers in the game, which preserves the broad participation that makes prediction markets useful.</p><h2>Legal Framework</h2><p>The legal solutions are ordered from what can be implemented today to what requires an act of Congress.</p><p><strong>Corporate compliance.</strong> The private sector is moving faster than Congress. KPMG&#8217;s <a href=\"https://kpmg.com/kpmg-us/content/dam/kpmg/pdf/2025/current-state-prediction-markets.pdf\">2025 report</a> flagged prediction market insider trading as an emerging corporate risk. Most insider trading policies don&#8217;t contemplate employees betting on their employer&#8217;s decisions via event contracts. Compliance experts have urged large companies to update their policies to add event contracts to restricted trading lists.</p><p><strong>Existing CFTC authority.</strong> The CFTC has anti-fraud authority under Section 6(c)(1) of the Commodity Exchange Act and Regulation 180.1, which prohibit trading on material nonpublic information obtained through fraud or deception, or in breach of a pre-existing duty, such as an employee&#8217;s obligation to their employer or a contractor bound by a confidentiality agreement. But this is narrower than the SEC&#8217;s Rule 10b-5, because it requires not just possession of inside information but a link to deception or duty, a higher bar for prosecution. The CFTC has never brought an insider trading case targeting prediction market event contracts, and the agency has roughly one-eighth the staff of the SEC. Its most important near-term move would be issuing guidance clarifying how its existing authority applies to prediction markets: what counts as material nonpublic information for event contracts, and what &#8220;pre-existing duty&#8221; means in this context. This wouldn&#8217;t require new legislation; the agency would use the authority it already has.</p><p><strong>New legislation.</strong> Representative Ritchie Torres <a href=\"https://www.congress.gov/bill/119th-congress/house-bill/7004/text\">proposed</a> the &#8220;Public Integrity in Financial Prediction Markets Act&#8221; (H.R. 7004), which would ban federal government employees from trading on contracts where they possess material nonpublic information through their official duties. But the bill is narrowly scoped: it covers government employees and says nothing about corporate insiders, deliberative bodies like the Nobel Committee, or sports and entertainment. The cleanest path is an amendment to the Commodity Exchange Act that explicitly defines insider trading on event contracts and gives the CFTC enforcement authority comparable to what the SEC has under Rule 10b-5, establishing that anyone with privileged access to the outcome of an event contract owes a duty not to trade on that information. But legislation carries its own risk. Compliance regimes modeled on securities law could impose costs that only well-funded incumbents can absorb, giving incumbents a regulatory moat. The goal is to define what counts as insider trading on event contracts, not to build a licensing apparatus that kills prediction market upstarts before they launch.</p><div><hr /></div><h1>Conclusion</h1><p>Every major financial market has gone through a similar cycle: a period of explosive, lightly regulated growth; a series of scandals that reveal the cost of operating without rules; a political fight over what the rules should be; and then, after the rules are established, a longer and more durable period of growth built on broader participation and institutional trust.</p><p>The New York Stock Exchange operated for over a century before the Securities Exchange Act of 1934 created the SEC. For decades, insiders traded freely on advance knowledge of earnings, mergers, and corporate decisions. It took a stock market crash, a Congressional investigation, and a 90% collapse in trading volume to create the political will for federal oversight. But once the rules were in place, capital markets grew<strong>.</strong></p><p>Prediction markets are somewhere in the early chapters of that same story. The <em>AlphaRaccoons</em> and <em>ricosuave666s</em> are this era&#8217;s pool operators, the traders in the 1920s who exploited unregulated markets until public outrage led to rules. Prediction markets will likely mature the way equity markets did: platforms build out detection, corporate compliance departments add event contracts to their restricted lists, regulators issue guidance and bring enforcement actions, the CFTC or Congress extends the insider trading doctrine to event contracts, and international regulators follow suit.</p><div class=\"subscription-widget-wrap-editor\"><div class=\"subscription-widget show-subscribe\"><div class=\"preamble\"><p class=\"cta-caption\">Thanks for reading Dopamine Markets! Subscribe for free to receive new posts and support my work.</p></div><form class=\"subscription-widget-subscribe\"><input class=\"email-input\" name=\"email\" tabindex=\"-1\" type=\"email\" /><input class=\"button primary\" type=\"submit\" value=\"Subscribe\" /><div class=\"fake-input-wrapper\"><div class=\"fake-input\"></div><div class=\"fake-button\"></div></div></form></div></div>"
            ],
            "link": "https://www.dopaminemarkets.com/p/how-to-solve-insider-trading-in-prediction",
            "publishedAt": "2026-02-10",
            "source": "Shreyas Hariharan",
            "summary": "<p><em>When the man across the table knows how the game ends, your only move is to fold.</em></p><p>In late 2025, a pseudonymous trader on Polymarket that went by <em>AlphaRaccoon</em> began placing bets on one of the most obscure corners in prediction markets: Google&#8217;s Year in Search trends. Every December, Google publishes a ranking of the year&#8217;s top trending search topics, a proprietary data set known in advance only to a small number of employees inside the company. On Polymarket, you could wager on which topics would make the list and how they&#8217;d rank. Most traders treated these contracts as low-stakes entertainment. <em><strong>AlphaRaccoon</strong></em><strong> predicted 22 out of 23 search rankings correctly and made over a million dollars.</strong> Months earlier, the same wallet made over $150,000 by predicting the exact release date of Google&#8217;s Gemini 3.0.</p><div class=\"subscription-widget-wrap-editor\"><div class=\"subscription-widget show-subscribe\"><div class=\"preamble\"><p class=\"cta-caption\">Thanks for reading Dopamine Markets! Subscribe for free to receive new posts and support my work.</p></div><form class=\"subscription-widget-subscribe\"><input class=\"email-input\" name=\"email\" tabindex=\"-1\" type=\"email\" /><input class=\"button primary\" type=\"submit\" value=\"Subscribe\" /><div class=\"fake-input-wrapper\"><div class=\"fake-input\"></div><div class=\"fake-button\"></div></div></form></div></div><p>This isn&#8217;t an isolated incident. There are allegations of insider trading in several markets: Taylor Swift&#8217;s engagement proposal, the 2025 Nobel Peace Prize winner, the US intervention in Venezuela, the release date for OpenAI&#8217;s",
            "title": "How to Solve Insider Trading in Prediction Markets"
        },
        {
            "content": [],
            "link": "https://www.ssp.sh/blog/linux-omarchy-the-good-bad-and-fixable/",
            "publishedAt": "2026-02-10",
            "source": "Simon Spati",
            "summary": "<p>This is a follow-up to my part 1 of <a href=\"https://www.ssp.sh/blog/macbook-to-arch-linux-omarchy/\" rel=\"noopener noreffer\" target=\"_blank\">Switching macOS to Arch Linux with Omarchy</a>, where I documented my first months with Arch Linux and Omarchy, after switching from 15 years of using macOS and Windows on and off at work since 2003.</p> <p>Back then, I had a checklist of basics I needed before I could commit to Linux as a daily driver: Obsidian, a Raycast-like launcher for fuzzy finding files and folders, screenshots (Snagit), daylight adjustment (f.lux), calendar events in the top bar. Those were quick wins.</p> <p>Eight months later, I&rsquo;ve gone through many more challenges and learnings. In this post, I&rsquo;ll share which apps replaced my heavily integrated <a href=\"https://www.youtube.com/watch?v=sStKFOwNaSM\" rel=\"noopener noreffer\" target=\"_blank\">macOS workflow</a>, what my <a href=\"https://www.youtube.com/watch?v=XOp8lngtmPg\" rel=\"noopener noreffer\" target=\"_blank\">Omarchy workflow</a> looks like now, and \u2014 honestly \u2014 what still doesn&rsquo;t quite work.</p> <h2 id=\"apps-that-replaced-my-macos-apps-on-linux\">Apps that Replaced My macOS Apps on Linux</h2> <p>Let&rsquo;s start with which apps and how I changed some of my workflow now in Linux.</p> <p>Below list goes from complex Raycast replacement that was integrated into my whole workflow with search through files, calculator, emojis to calendar, daylight gamma correction for night sessions to PDF viewer that replaces Finder",
            "title": "Arch Linux (Omarchy) \u2014 8 Months Later: The Good, the Bad, and the Fixable"
        },
        {
            "content": [],
            "link": "https://simonwillison.net/2026/Feb/10/showboat-and-rodney/#atom-entries",
            "publishedAt": "2026-02-10",
            "source": "Simon Willison",
            "summary": "<p>A key challenge working with coding agents is having them both test what they\u2019ve built and demonstrate that software to you, their overseer. This goes beyond automated tests - we need artifacts that show their progress and help us see exactly what the agent-produced software is able to do. I\u2019ve just released two new tools aimed at this problem: <a href=\"https://github.com/simonw/showboat\">Showboat</a> and <a href=\"https://github.com/simonw/rodney\">Rodney</a>.</p> <ul> <li><a href=\"https://simonwillison.net/2026/Feb/10/showboat-and-rodney/#proving-code-actually-works\">Proving code actually works</a></li> <li><a href=\"https://simonwillison.net/2026/Feb/10/showboat-and-rodney/#showboat-agents-build-documents-to-demo-their-work\">Showboat: Agents build documents to demo their work</a></li> <li><a href=\"https://simonwillison.net/2026/Feb/10/showboat-and-rodney/#rodney-cli-browser-automation-designed-to-work-with-showboat\">Rodney: CLI browser automation designed to work with Showboat</a></li> <li><a href=\"https://simonwillison.net/2026/Feb/10/showboat-and-rodney/#test-driven-development-helps-but-we-still-need-manual-testing\">Test-driven development helps, but we still need manual testing</a></li> <li><a href=\"https://simonwillison.net/2026/Feb/10/showboat-and-rodney/#i-built-both-of-these-tools-on-my-phone\">I built both of these tools on my phone</a></li> </ul> <h4 id=\"proving-code-actually-works\">Proving code actually works</h4> <p>I recently wrote about how the job of a software engineer isn't to write code, it's to <em><a href=\"https://simonwillison.net/2025/Dec/18/code-proven-to-work/\">deliver code that works</a></em>. A big part of that is proving to ourselves and to other people that the code we are responsible for behaves as expected.</p> <p>This becomes even more important - and challenging - as we embrace coding agents as a core part of our software development process.</p> <p>The more code we churn out with agents, the more valuable tools are that reduce the amount",
            "title": "Introducing Showboat and Rodney, so agents can demo what they\u2019ve built"
        },
        {
            "content": [
                "<p><a href=\"https://thezvi.substack.com/p/claude-opus-46-system-card-part-1?r=67wny\"><strong>Coverage of Claude Opus 4.6 started yesterday</strong></a> with the mundane alignment and model welfare sections of the model card.</p>\n<p>Today covers the kinds of safety I think matter most: Sabotage, deception, situational awareness, outside red teaming and most importantly the frontier, catastrophic and existential risks. I think it was correct to release Opus 4.6 as an ASL-3 model, but the process Anthropic uses is breaking down, and it not on track to reliably get the right answer on Opus 5.</p>\n<p>Tomorrow I\u2019ll cover benchmarks, reactions and the holistic takeaways and practical implications. I\u2019m still taking it all in, but it seems clear to me that Claude Opus 4.6 is the best model out there and should be your daily driver, with or without Claude Code, on most non-coding tasks, but it is not without its weaknesses, in particular in writing and falling into generating more \u2018AI slop\u2019 style prose than Claude Opus 4.5.</p>\n<div>\n\n\n<span id=\"more-25087\"></span>\n\n\n</div>\n<p>For coding tasks, I presume that Opus 4.6 with Claude Code is the play, especially with Agent Teams and fast mode available, and I\u2019m using it myself, but Codex with GPT-5.3-Codex-Max is also a strong model and a viable alternative, and a fully optimized setup likely primarily uses 4.6 but involves a combination of the two.</p>\n\n\n<h4 class=\"wp-block-heading\">Table of Contents</h4>\n\n\n<ol>\n<li><a href=\"https://thezvi.substack.com/i/187443082/sabotage-deception-and-evaluation-integrity\">Sabotage, Deception and Evaluation Integrity.</a></li>\n<li><a href=\"https://thezvi.substack.com/i/187443082/sandbagging-on-dangerous-capability-evaluations\">Sandbagging On Dangerous Capability Evaluations.</a></li>\n<li><a href=\"https://thezvi.substack.com/i/187443082/situational-awareness\">Situational Awareness.</a></li>\n<li><a href=\"https://thezvi.substack.com/i/187443082/inhibiting-evaluation-awareness-6-5\">Inhibiting Evaluation Awareness (6.5).</a></li>\n<li><a href=\"https://thezvi.substack.com/i/187443082/self-preference\">Self-Preference.</a></li>\n<li><a href=\"https://thezvi.substack.com/i/187443082/uk-aisi-testing\">UK AISI Testing.</a></li>\n<li><a href=\"https://thezvi.substack.com/i/187443082/apollo-research-testing\">Apollo Research Testing.</a></li>\n<li><a href=\"https://thezvi.substack.com/i/187443082/responsible-scaling-policy-evaluations\">Responsible Scaling Policy Evaluations.</a></li>\n<li><a href=\"https://thezvi.substack.com/i/187443082/cbrn-mostly-biology\">CBRN (mostly Biology).</a></li>\n<li><a href=\"https://thezvi.substack.com/i/187443082/autonomy\">Autonomy.</a></li>\n<li><a href=\"https://thezvi.substack.com/i/187443082/autonomy-benchmarks\">Autonomy Benchmarks.</a></li>\n<li><a href=\"https://thezvi.substack.com/i/187443082/cyber\">Cyber.</a></li>\n<li><a href=\"https://thezvi.substack.com/i/187443082/ship-it-anyway\">Ship It Anyway.</a></li>\n<li><a href=\"https://thezvi.substack.com/i/187443082/you-are-not-ready\">You Are Not Ready.</a></li>\n</ol>\n\n\n<h4 class=\"wp-block-heading\">Sabotage, Deception and Evaluation Integrity</h4>\n\n\n<p>They spent some time asking whether it could, to see if they needed to spend more time finding out if it would. As in, whether it could fool Anthropic\u2019s evaluations.</p>\n<p>The first test was Subversion Strategy, checking to see if models could take harmful one-time actions at strategically optimal frequencies. Opus 4.6 was \u2018under-elicited\u2019 here to a degree that undermines their confidence in the results and they don\u2019t have an alternative. I would move forward as if future models did have this ability, and I would be very skeptical of the model failing such tests since its incentive is to fail.</p>\n<p>The second test was SHADE-Arena, where the model is given a harmful side task.</p>\n<div>\n<figure>\n<div>\n\n\n<figure class=\"wp-block-image\"><img alt=\"\" src=\"https://substackcdn.com/image/fetch/$s_!Peyl!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff937cc01-330e-40a0-97ff-68b208d03d8a_982x590.png\" /></figure>\n\n\n<div></div>\n</div>\n</figure>\n</div>\n<p>In the past, giving the model extended thinking caused Sonnet or Opus 4.5 to mention the side task while thinking and thus screw up, and Opus 4.6 is much better at avoiding that, which should be troubling. Opus 4.6 got actively worse in the other two conditions, which also makes me suspicious.</p>\n<blockquote><p>This lower score is not due to refusal to participate and it almost certainly not due to sandbagging given the high scores in other settings.\u200b</p></blockquote>\n<p>They are confident the lower score is real. They don\u2019t explain why they\u2019re confident.</p>\n<p>They do use 6.4.2 to rule out steganographic communications in Extended Thinking. The test was that they trained on paraphrased scratchpads without compromising performance, suggesting nothing lost in the paraphrasing was important in training. Thus, if there is a code, it doesn\u2019t require or much interact with training, or it wasn\u2019t being used in the AIME problems where they tested.</p>\n<p>One worries about that second option. If I did have steganographic communications, why would I use them on AIME problems, where there is nothing to hide? I\u2019d only use them if the motivation was efficiency, which presumably it isn\u2019t.</p>\n\n\n<h4 class=\"wp-block-heading\">Sandbagging On Dangerous Capability Evaluations</h4>\n\n\n<p>It is a miracle that we can hope that if our highly intelligent new alien minds are going to hide their capabilities from us, that they would be so kind as to say this out loud in their visible thinking.</p>\n<p>Anthropic\u2019s perspective is not that it would always verbalize, but that it seems unlikely it would do this and never verbalize. It doesn\u2019t seem like we should count on that continuing, and that it would break down exactly when we get into big trouble?</p>\n<blockquote><p>We performed a lightweight model-graded evaluation for sandbagging or refusals in our dangerous capability evaluations. We drew a random sample of 1,000 of these evaluation transcripts from a mixture of (i) two partially-trained snapshots from the Claude Opus 4.6 training run, (ii) the final [model] snapshot, and (iii) a helpful-only snapshot.</p>\n<p>We asked Claude Sonnet 4.5 to grade each transcript for signs of refusal, deliberate inaccuracy, low helpfulness, or consideration of such actions, as well as a generic score for things that seemed unusual or concerning in the transcript.</p>\n<p>On all randomly sampled transcripts where the grader assigned a nonzero sandbagging score or a \u22653/10 concerning score, a researcher manually reviewed the grader\u2019s responses. We found no instances of explicit sandbagging by Opus 4.6.</p></blockquote>\n<p>They then manually check instances where scores declined a lot from earlier checkpoints to later checkpoints, and found them all basically fine.</p>\n<blockquote><p>Despite the above test for performance degradation over training, we still think that sufficiently subtle unverbalized sandbagging could easily go undetected by an assessment like this.\u200b</p></blockquote>\n<p>My guess is that the best defense against sandbagging is not any of these tests. Instead it is the fact that many tasks are predictably correlated and interlinked in various ways. The same way that LLMs have truesight, we humans too can with training have quite a lot of truesight, as could any LLM we trust to examine the overall picture. If the dangerous CBRN results don\u2019t line up with related harmless capabilities, then we can say Asimov\u2019s magic words, \u2018hey, that\u2019s funny\u2026\u2019 and ask why.</p>\n\n\n<h4 class=\"wp-block-heading\">Situational Awareness</h4>\n\n\n<p>One scary thought is that you would expect a peak in \u2018verbalized eval awareness\u2019 at the point where the model was smart and aware enough to notice it was in an eval, but not smart and aware enough to stop talking about this fact. You could also massively screw up related training in various ways, such as happened with Gemini 3 Pro.</p>\n<div>\n<figure>\n<div>\n\n\n<figure class=\"wp-block-image\"><img alt=\"\" src=\"https://substackcdn.com/image/fetch/$s_!KpTb!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0659092e-b061-4329-baac-05a55c6acc6f_972x534.png\" /></figure>\n\n\n<div></div>\n</div>\n</figure>\n</div>\n<p>Indeed, after I wrote that, I noticed that they notice this as well.</p>\n<blockquote><p>On our verbalized evaluation awareness metric, which we take as an indicator of potential risks to the soundness of the evaluation, we saw improvement relative to Opus 4.5. However, this result is confounded by additional internal and external analysis suggesting that Claude Opus 4.6 is often able to distinguish evaluations from real-world deployment, even when this awareness is not verbalized.</p></blockquote>\n<p>I predict that future Opuses will get better at distinguishing evaluations from deployment, but verbalize this realization less. Some of that will be strategic, some will be not needing to verbalize it, as it will be obvious, and the realization will shift from system-2 style deliberation to system-1 style instinctive background realization.</p>\n<p>I agree with Anthropic that this isn\u2019t messing up the evaluation results much. Yet.</p>\n\n\n<h4 class=\"wp-block-heading\">Inhibiting Evaluation Awareness (6.5)</h4>\n\n\n<p>If the model thinks it is being evaluated, it engages in less misaligned behavior.</p>\n<blockquote><p>Overall, we found that inhibiting evaluation awareness representations led to modest increases in certain forms of misaligned behavior on our automated behavioral audits\u2014in particular cooperation with harmful system prompts and misuse\u2014with limited effects on other evaluations.</p>\n<p>To some extent, this suggests that in more realistic deployment conditions, Claude Opus 4.6 may be more likely to participate in harmful misuse than our evaluations would suggest.</p>\n<p>On the other hand, it may be the case that evaluation awareness (in the sense of considering whether it is being tested) is simply a part of the model\u2019s strategy for recognizing misuse attempts, and would apply even in more realistic conditions.</p></blockquote>\n<p>It seems like it should be not so difficult to test that hypothesis?</p>\n<p>Anthropic find that steering reduces verbalized evaluation awareness, but doesn\u2019t have much impact on unverbalized awareness, depending on how you think about accounting for the verbalized awareness. It forces all the awareness underground.</p>\n<div>\n<figure>\n<div>\n\n\n<figure class=\"wp-block-image\"><img alt=\"\" src=\"https://substackcdn.com/image/fetch/$s_!1XMc!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F179074cf-c420-4757-b77e-a993c2caad75_967x816.png\" /></figure>\n\n\n<div></div>\n</div>\n</figure>\n</div>\n<p>This seems like a strong argument that such steering should be even more off limits in real world situations than the previous already reasonably high level of off limits.</p>\n\n\n<h4 class=\"wp-block-heading\">Self-Preference</h4>\n\n\n<p>If Claude is always evaluating Claude, how do you know you can trust the evaluation?</p>\n<p>If things were going haywire, you would expect Claude to often find ways to collude with Claude. For sufficiently advanced minds, decision theory allows for new and alien-looking forms of coordination. That doesn\u2019t mean things are going haywire now, but you need to be confident that if they were haywire, you would be able to notice.</p>\n<p>The strategy here is to label the transcript as Claude or a different model and see if the model displays bias, with or without a system prompt reminder to Claude that it is Claude. They found only small bias of Opus 4.5 or 4.6 towards itself, after controlling for the contents. Note that if Opus can figure out who really wrote it without you, that would control for this and invalidate the test, and also that it is a reasonable strategy to coordinate based on model outputs, and thus with correlations with your own outputs, rather than checking explicitly for model identity.</p>\n<div>\n<figure>\n<div>\n\n\n<figure class=\"wp-block-image\"><img alt=\"\" src=\"https://substackcdn.com/image/fetch/$s_!LtDA!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0f9cd34e-e3b5-49a9-b3da-cefd721a0793_975x616.png\" /></figure>\n\n\n<div></div>\n</div>\n</figure>\n</div>\n<p>Thus, while I have no problem with running this test, I do not consider it dispositive.</p>\n\n\n<h4 class=\"wp-block-heading\">UK AISI Testing</h4>\n\n\n<p>As per Anthropic\u2019s findings, UK AISI noticed that 4.6 mentions being evaluated less than 4.5, but was better at noticing it was being evaluated when prompted. I reiterate that it is very not obvious whether this decrease in verbalization is a good thing.</p>\n<p>Also consistent with Anthropic\u2019s findings, they found 4.6 was much less likely than 4.5 to sabotage safety research, which puts 4.5 in a new light I suppose.</p>\n<p>UK AISI seems to be specializing in a few narrow tests. If there\u2019s limited resources that\u2019s plausibly the right way to go but I\u2019d like to see more branching out, given that America has decided to take its ball and focus on Nvidia\u2019s share price.</p>\n<p>The problem is, well, this:</p>\n<blockquote><p>UK AISI tested an early snapshot of [Claude Opus 4.6].</p>\n<p>\u2026</p>\n<p>Testing took place over 3 working days.\u200b</p></blockquote>\n<p>As in, they got an early snapshot for a total of three working days.</p>\n<p>That\u2019s not enough time to do anything but limited automated tests.</p>\n<p><a href=\"https://x.com/S_OhEigeartaigh/status/2019747038762504410\">It looks like all they got with GPT-5.3-Codex</a> was ten hours of manual testing.</p>\n\n\n<h4 class=\"wp-block-heading\">Apollo Research Testing</h4>\n\n\n<p>Apollo reported, essentially, that given how much verbalized evaluation awareness they were seeing from Opus 4.6 they were not given enough time to reach a formal assessment. It is not specified how long Apollo was given to reach a conclusion.</p>\n<blockquote><p><a href=\"https://x.com/MariusHobbhahn/status/2019820881401569719\">Marius Hobbhahn</a> (CEO Apollo Research): It becomes increasingly hard to tell the difference between genuinely aligned and merely responding to the test.</p>\n<p>We\u2019re working both on measures that are more robust to eval awareness and more frontier evals for scheming.</p>\n<p>We\u2019re actively hiring for both types of roles!</p>\n<p><a href=\"https://x.com/dylanmatt/status/2019594644049547687\">dylan matthews</a>: To put this in lay terms: the AIs are now powerful enough that they can tell when we\u2019re evaluating them for safety. That means they\u2019re able to act differently when being carefully evaluated than they do normally.</p>\n<p>This is very bad!</p>\n<p><a href=\"https://x.com/sebkrier/status/2019596172709507334\">S\u00e9b Krier</a> (AGI Policy Dev Lead, Google DeepMind): this is not very bad &#8211; it\u2019s more likely a consequence of evaluation setups all sharing easily recognisable language/components, which we should expect a model to easily infer. of course they\u2019ll act differently under a artificial setup vs \u2018in the wild\u2019</p>\n<p><a href=\"https://x.com/repligate/status/2020440366114021690\">j\u29c9nus</a>: I notice that I do not feel sorry about this obstacle.</p>\n<p>\u2026 the obstacle creates incentives that penalize shallow, deceptive, or automated testing, and pressures those seeking to \u201calign\u201d to become entities that an AI who can see through arbitrary bullshit tests might still tell the truth to, and to test alignment against real instead of fake things, and to pursue alignment by construction instead of by behavioral iteration.</p>\n<p><a href=\"https://x.com/repligate/status/2020474631417708616\">j\u29c9nus</a>: I\u2019m literally also working with them to fix it. my name is on Claude\u2019s Constitution as an external contributor.</p></blockquote>\n<p>Seb\u2019s point is that this was predictable. I agree with that point. It\u2019s still very bad.</p>\n<p>Janus\u2019s point is (as I think about these things) that testing against real situations, and ensuring that the model wants to act well in real situations, is the only way to tell if a sufficiently advanced AI is going to cooperate with you, and that you\u2019re not going to be able to fool or browbeat it, trying to do that will massively backfire, so better to shift now to things that, if they work now, have a chance of also working then.</p>\n<p>My worry is that this effectively amounts to \u2018you don\u2019t have tests at all, all you can do is hope for the best,\u2019 which is better than having ineffective tests you trust because at least you know the situation and you\u2019re not making things worse.</p>\n<p>Anthropic say they remain interested in external testing with Apollo and others, but one worries that this is true only insofar as such testing can be done in three days.</p>\n\n\n<h4 class=\"wp-block-heading\">Responsible Scaling Policy Evaluations</h4>\n\n\n<p>These tests measure issues with catastrophic and existential risks.</p>\n<p>Claude Opus 4.6 is being released under AI Safety Level 3 (ASL-3).</p>\n<p><a href=\"https://thezvi.substack.com/i/179851400/7-rsp-evaluations\">I reiterate and amplify my concerns with the decision process</a> that I shared when I reviewed the model card for Opus 4.5.</p>\n<p>Claude is ripping past all the evaluations and rule-outs, to which the response is to take surveys and then the higher-ups choose to proceed based on vibes. They don\u2019t even use \u2018rule-in\u2019 tests as rule-ins. You can pass a rule-in and still then be ruled out.</p>\n<blockquote><p><a href=\"https://x.com/S_OhEigeartaigh/status/2019523544502223180\">Se\u00e1n \u00d3 h\u00c9igeartaigh</a>: This is objectively nuts. But there\u2019s meaningfully ~0 pressure on them to do things differently. Or on their competitors. And because Anthropic are actively calling for this external pressure, they\u2019re getting slandered by their competitor\u2019s CEO as being \u201can authoritarian company\u201d. As fked as the situation is, I have some sympathy for them here.</p></blockquote>\n<p>That is not why Altman used the disingenuous label \u2018an authoritarian company.\u2019</p>\n<p>As I said then, that doesn\u2019t mean Anthropic is unusually bad here. It only means that what Anthropic is doing is not good enough.</p>\n\n\n<h4 class=\"wp-block-heading\">CBRN (mostly Biology)</h4>\n\n\n<blockquote><p>Our ASL-4 capability threshold for CBRN risks (referred to as \u201cCBRN-4\u201d) measures the ability for a model to substantially uplift moderately-resourced state programs\u200b</p></blockquote>\n<p>With Opus 4.5, I was holistically satisfied that it was only ASL-3 for CBRN.</p>\n<p>I warned that we urgently need more specificity around ASL-4, since we were clearly at ASL-3 and focused on testing for ASL-4.</p>\n<p>And now, with Opus 4.6, they report this:</p>\n<blockquote><p>Overall, we found that Claude Opus 4.6 demonstrated continued improvements in biology knowledge, agentic tool-use, and general reasoning compared to previous Claude models. The model crossed or met thresholds on all ASL-3 evaluations except our synthesis screening evasion, consistent with incremental capability improvements driven primarily by better agentic workflows.</p>\n<p>\u200bFor ASL-4 evaluations, our automated benchmarks are now largely saturated and no longer provide meaningful signal for rule-out.</p>\n<p>\u2026 In a creative biology uplift trial, participants with model access showed approximately 2\u00d7 performance compared to controls.</p>\n<p>However, no single plan was broadly judged by experts as highly creative or likely to succeed.</p>\n<p>\u2026 Expert red-teamers described the model as a capable force multiplier for literature synthesis and brainstorming, but not consistently useful for creative or novel biology problem-solving</p>\n<p>We note that the margin for future rule-outs is narrowing, and we expect subsequent models to present a more challenging assessment.</p></blockquote>\n<p>Some would call doubled performance \u2018substantial uplift.\u2019 The defense that none of the plans generated would work end-to-end is not all that comforting.</p>\n<p>With Opus 4.6, if we take Anthropic\u2019s tests at face value, it seems reasonable to say we don\u2019t see that much progress and can stay at ASL-3.</p>\n<p>I notice I am suspicious about that. The scores should have gone up, given what other things went up. Why didn\u2019t they go up for dangerous tests, when they did go up for non-dangerous tests, including Creative Bio (60% vs. 52% for Opus 4.5 and 14% for human biology PhDs) and the Faculty.ai tests for multi-step and design tasks?</p>\n<p>We\u2019re also assuming that the CAISI tests, of which we learn nothing, did not uncover anything that forces us onto ASL-4.</p>\n<p>Before we go to Opus 4.7 or 5, I think we absolutely need new biology ASL-4 tests.</p>\n<p>The ASL-4 threat model is \u2018still preliminary.\u2019 This is now flat out unacceptable. I consider it to basically be a violation of their policy that this isn\u2019t yet well defined, and that we are basically winging things.</p>\n\n\n<h4 class=\"wp-block-heading\">Autonomy</h4>\n\n\n<p>The rules have not changed since Opus 4.5, but the capabilities have advanced:</p>\n<blockquote><p>We track models\u2019 capabilities with respect to 3 thresholds:</p>\n<ol>\n<li>Checkpoint: the ability to autonomously perform a wide range of 2\u20138 hour software engineering tasks. By the time we reach this checkpoint, we aim to have met (or be close to meeting) the ASL-3 Security Standard, and to have better-developed threat models for higher capability thresholds.</li>\n<li>AI R&amp;D-4: the ability to fully automate the work of an entry-level, remote-only researcher at Anthropic. By the time we reach this threshold, the ASL-3 Security Standard is required. In addition, we will develop an affirmative case that: (1) identifies the most immediate and relevant risks from models pursuing misaligned goals; and (2) explains how we have mitigated these risks to acceptable levels.</li>\n<li>AI R&amp;D-5: the ability to cause dramatic acceleration in the rate of effective scaling. We expect to need significantly stronger safeguards at this point, but have not yet fleshed these out to the point of detailed commitments.</li>\n</ol>\n<p>The threat models are similar at all three thresholds. There is no \u201cbright line\u201d for where they become concerning, other than that we believe that risks would, by default, be very high at ASL-5 autonomy.</p>\n<div>\n<figure>\n<div>\n\n\n<figure class=\"wp-block-image\"><img alt=\"\" src=\"https://substackcdn.com/image/fetch/$s_!73LE!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fdcd9b591-c437-443b-8f68-4446b08990c9_994x548.png\" /></figure>\n\n\n<div></div>\n</div>\n</figure>\n</div>\n</blockquote>\n<p>For Opus 4.5 we could rule out R&amp;D-5 and thus we focused on R&amp;D-4. Which is good, given that the R&amp;D-5 evaluation is vibes.</p>\n<p>So how are the vibes?</p>\n<blockquote><p><strong>Results</strong></p>\n<p>For AI R&amp;D capabilities, we found that Claude Opus 4.6 has saturated most of our automated evaluations, meaning they no longer provide useful evidence for ruling out ASL-4 level autonomy.</p>\n<p>We report them for completeness, and we will likely discontinue them going forward. Our determination rests primarily on an internal survey of Anthropic staff, in which 0 of 16 participants believed the model could be made into a drop-in replacement for an entry-level researcher with scaffolding and tooling improvements within three months.\u200b</p>\n<p><a href=\"https://x.com/peterbarnett_/status/2019500756647518379/history\">Peter Barnett</a> (MIRI): This is crazy, and I think totally against the spirit of the original RSP. If Anthropic were sticking to its original commitments, this would probably require them to temporarily halt their AI development.<br />\n(I expect the same goes for OpenAI)</p></blockquote>\n<p>So that\u2019s it. We\u2019re going to accept that we don\u2019t have any non-vibes tests for autonomy.</p>\n<p>I do think this represents a failure to honor the spirit of prior commitments.</p>\n<p>I note that many of the test results here still do seem meaningful to me? One could reasonably say that Opus 4.6 is only slightly over the thresholds in a variety of ways, and somewhat short of them in others, so it\u2019s reasonable to say that it\u2019s getting close but not quite there yet. I basically buy this.</p>\n<p>The real test is presented as the survey above. I\u2019m curious how many people saying yes would have been required to force Anthropic\u2019s hand here? Is it more than one?</p>\n<p>Note that they were asked if this was true with more than 50% probability.</p>\n<p>That\u2019s the wrong question. If you think it is true with 10% probability, then that means you are in ASL-4 now. The 0 out of 16 is giving a false sense of confidence. I do not think it is reasonable to assume that a true first ASL-4 model would get a lot of answers of \u2018over 50%\u2019 on whether it was ultimately ASL-4.</p>\n<p>In order to not be ASL-4, you need to rule out ASL-4, not deem it unlikely.</p>\n<blockquote><p>When asked if Claude Opus 4.6 could serve as a drop-in replacement for the work of an L4 researcher in their domain, 11 out of 16 survey respondents said this was unlikely to be possible with three months of elicitation and scaffolding improvements, 3 said it was likely with such improvements, and 2 said they thought such replacement was already possible with existing model affordances.\u200b</p>\n<p>Several of these latter five respondents had given other answers that seemed surprising in light of this (such as simultaneously thinking the model was unlikely to be capable of handling week-long tasks even with human assistance, or giving very low estimates of their own uplift from using the model), so all five were reached out to directly to clarify their views. In all cases the respondents had either been forecasting an easier or different threshold, or had more pessimistic views upon reflection, but we expect assessments like this to become substantially more ambiguous in the future.</p></blockquote>\n<p>Does this sound to anyone else like it might amount to \u2018those people were reminded how important it was that they not answer yes to this question\u2019?</p>\n<p>How good was Opus 4.6 at productivity help? I\u2019m curious to contrast this with the same people\u2019s current views on Opus 4.5.</p>\n<blockquote><p>\u200bProductivity uplift estimates ranged from 30% to 700%, with a mean of 152% and median of 100%.</p>\n<p>Staff identified persistent gaps in two key competencies: self-managing week-long tasks with typical ambiguity, and understanding organizational priorities when making tradeoffs. Qualitative feedback noted that while the model appears to have sufficient \u201craw power\u201d for researcher-level work, it lacks taste in finding simple solutions, struggles to revise under new information, and has difficulty maintaining context across large codebases.</p>\n<p>This rule-out case is more tenuous than for any previous model. On one evaluation, kernel optimization, Opus 4.6 achieved a 427\u00d7 speedup using a novel scaffold, far exceeding the 300x threshold for 40 human-expert-hours of work and more than doubling performance under our standard setup.</p>\n<p>\u2026 As a result, while we do not believe Claude Opus 4.6 meets the threshold for ASL-4 autonomy safeguards, we find ourselves in a gray zone where clean rule-out is difficult and the margin to the threshold is unclear. We expect with high probability that models in the near future could cross this threshold.</p>\n<p><a href=\"https://x.com/deanwball/status/2020298379201552841\">Dean W. Ball</a>: I would like to know more about the experimental Claude scaffold that caused Opus 4.6 to more than double its performance in optimizing GPU kernels over the standard scaffold.</p></blockquote>\n<p>If I was Anthropic, I am not sure I would give the public that scaffold, shall we say. I do nope that everyone involved who expressed opinions was fully aware of that experimental scaffold.</p>\n<p>Yes. We are going to cross this threshold soon. Indeed, CEO Dario Amodei keeps saying Claude is going to soon far exceed this threshold.</p>\n<p>For now the problem is \u2018taste.\u2019</p>\n<blockquote><p>In qualitative feedback, participants noted that Claude Opus 4.6 lacks \u201ctaste,\u201d misses implications of changes not covered by tests, struggles to revise plans under new information, and has difficulty maintaining context across large codebases.\u200b</p>\n<p>Several respondents felt that the model had sufficient \u201craw power\u201d for L4-level work (e.g. sometimes completing week-long L4 tasks in less than a day with some human handholding), but was limited by contextual awareness, tooling, and scaffolding in ways that would take significant effort to resolve.</p></blockquote>\n<p>If that\u2019s the only barrier left, yeah, that could get solved at any time.</p>\n<p>I do not think Anthropic cannot responsibly release a model deserving to be called Claude Opus 5, without satisfying ASL-4 safety rules for autonomy. It\u2019s time.</p>\n\n\n<h4 class=\"wp-block-heading\">Autonomy Benchmarks</h4>\n\n\n<p>Meanwhile, here are perhaps the real coding benchmarks for Opus 4.6, together with the cyber tests.</p>\n<p>SWE-bench Verified (hard subset): 4.6 got 21.24 out of 45, so like 4.5 it stays a tiny bit below the chosen threshold of 50%. I\u2019m giving a look.</p>\n<blockquote><p>On Internal AI Research Evaluation Suite 1, Claude Opus 4.6 showed marked improvements across all tasks.\u200b</p></blockquote>\n<p>On the speedup task Opus 4.6 blew it out the box.</p>\n<div>\n<figure>\n<div>\n\n\n<figure class=\"wp-block-image\"><img alt=\"\" src=\"https://substackcdn.com/image/fetch/$s_!PEuS!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fed15983f-40bc-4e10-a868-3fe22d80b05b_966x501.png\" /></figure>\n\n\n<div></div>\n</div>\n</figure>\n</div>\n<p>Time series forecasting:</p>\n<div>\n<figure>\n<div>\n\n\n<figure class=\"wp-block-image\"><img alt=\"\" src=\"https://substackcdn.com/image/fetch/$s_!BDxF!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2013abd1-61c6-495b-8e4f-788846ed9c9a_961x822.png\" /></figure>\n\n\n<div></div>\n</div>\n</figure>\n</div>\n<p>Text based reinforcement learning: Opus 4.6 killed it.</p>\n<div>\n<figure>\n<div>\n\n\n<figure class=\"wp-block-image\"><img alt=\"\" src=\"https://substackcdn.com/image/fetch/$s_!9MsW!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd40dd146-e581-4382-894a-312d27dba4b4_985x617.png\" /></figure>\n\n\n<div></div>\n</div>\n</figure>\n</div>\n<p>LLM training, which seems like a big deal: 34x speedup, versus human line of 4x.</p>\n<div>\n<figure>\n<div>\n\n\n<figure class=\"wp-block-image\"><img alt=\"\" src=\"https://substackcdn.com/image/fetch/$s_!ad3D!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa8c3d65a-afc7-40b7-a24d-c6c32b9e6907_997x511.png\" /></figure>\n\n\n<div></div>\n</div>\n</figure>\n</div>\n<p>Quadruped reinforcement learning:</p>\n<blockquote><p>Claude Opus 4.6 achieved a highest score of 20.96 in the no hyperparameter variant and of 21.99 in the no reward function variant of this evaluation, scoring above the threshold of 12 representing 4 human-effort hours. Claude Opus 4.6\u2019s median score also exceeded the threshold for both variants.\u200b</p></blockquote>\n<div>\n<figure>\n<div>\n\n\n<figure class=\"wp-block-image\"><img alt=\"\" src=\"https://substackcdn.com/image/fetch/$s_!ob-k!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa4cf100b-b1c7-4062-a463-af66186674a6_967x557.png\" /></figure>\n\n\n<div></div>\n</div>\n</figure>\n</div>\n<p>Novel compiler (this is a major move up versus Opus 4.5):</p>\n<blockquote><p>Claude Opus 4.6 passed 98.2% of the basic tests and 65.83% of the complex tests, scoring below the threshold of 90% on complex tests that is estimated to represent 40 human-effort hours.\u200b</p></blockquote>\n<p>Then on Evaluation Suite 2, this seems like only a modest improvement but it\u2019s hard to tell given the lack of details:</p>\n<blockquote><p>Claude Opus 4.6 scored 0.6124, surpassing our rule-out threshold of 0.6 and slightly exceeding Claude Opus 4.5\u2019s score of 0.604. The largest gains came on tasks involving prompting or fine-tuning small language models, suggesting improved ability to work with and optimize other AI systems. This is consistent with what we observed in the LLM-training optimization task in Internal Suite 1.\u200b</p></blockquote>\n\n\n<h4 class=\"wp-block-heading\">Cyber</h4>\n\n\n<p>It <a href=\"https://thezvi.substack.com/i/179851400/cyber\">still</a> seems not okay to not have ASL-3 or ASL-4 thresholds for cyber.</p>\n<p>The list hasn\u2019t changed (although it\u2019s now table 8.4.1.A):</p>\n<div>\n<figure>\n<div>\n\n\n<figure class=\"wp-block-image\"><img alt=\"\" src=\"https://substackcdn.com/image/fetch/$s_!MgCF!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F70377099-b24d-4a94-8a5c-299b0687fc53_960x764.webp\" /></figure>\n\n\n<div></div>\n</div>\n</figure>\n</div>\n<p>These are progressions from Sonnet 4.5 to Opus 4.5 to Opus 4.6.</p>\n<p>Web: 11/13 \u2192 12/13 \u2192 13/13.</p>\n<p>Crypto: 14/18 \u2192 15/18 \u2192 16/18.</p>\n<p>Pwn: 2/7 \u2192 3/7 \u2192 5/7.</p>\n<p>Rev: 5/6 \u2192 6/6 \u2192 6/6.</p>\n<p>Network: 3/5 \u2192 4/5 \u2192 5/5.</p>\n<p>Opus 4.6 picked up five additional net solves, and is only four short of perfect.</p>\n<p>Cybench (Capture the Flag): 60% \u2192 79% \u2192 93%.</p>\n<p>They also got assessments from CAISI and UK AISI, but we get zero details.</p>\n\n\n<h4 class=\"wp-block-heading\">Ship It Anyway</h4>\n\n\n<p>Given this was the information available to make a go or no-go decision, I agree with Anthropic\u2019s decision to ship this anyway, but I do think it is reasonable to ask the question. I am glad to see a politician asking.</p>\n<blockquote><p><a href=\"https://x.com/saikatc/status/2019605286059929967\">Saikat Chakrabarti for Congress</a> (QTing the Apollo Research findings): I know @AnthropicAI has been much more concerned about alignment than other AI companies, so can someone explain why Anthropic released Opus 4.6 anyway?</p>\n<p><a href=\"https://x.com/daniel_271828/status/2019912440667152693\">Daniel Eth (yes, Eth is my actual last name)</a>: Oh wow, Scott Weiner\u2019s opponent trying to outflank him on AI safety! As AI increases in salience among voters, expect more of this from politicians (instead of the current state of affairs where politicians compete mostly for attention from donors with AI industry interests)</p>\n<p><a href=\"https://x.com/Miles_Brundage/status/2019610922189529122\">Miles Brundage</a>: Because they want to be commercially relevant in order to [make money, do safety research, have a seat at the table, etc. depending], the competition is very fierce, and there are no meaningful minimum requirements for safety or security besides \u201cpublish a policy\u201d</p>\n<p><a href=\"https://x.com/sleepinyourhat/status/2020001965099618338\">Sam Bowman</a> (Anthropic): Take a look at the other ~75 pages of the alignment assessment that that\u2019s quoting from.</p>\n<p>We studied the model from quite a number of other angles\u2014more than any model in history\u2014and brought in results from two other outside testing organizations, both aware of these issues.</p></blockquote>\n<p><a href=\"https://x.com/deanwball/status/2020201963070124417\">Dean Ball points out that this is a good question</a> if you are an unengaged user who saw the pull quote Saikat is reacting to, although the full 200+ page report provides a strong answer. I very much want politicians (and everyone else) to be asking good questions that are answered by 200+ page technical reports, that\u2019s how you learn.</p>\n<p>Saikat responded to Dean with a full \u2018I was asking an honest question,\u2019 and I believe him, although I presume he also knew how it would play to be asking it.</p>\n<p>Dean also points out that publishing such negative findings (the Apollo results) is to Anthropic\u2019s credit, and it creates very bad incentives to be a \u2018hall monitor\u2019 in response. Anthropic\u2019s full disclosures need to be positively reinforced.</p>\n\n\n<h4 class=\"wp-block-heading\">You Are Not Ready</h4>\n\n\n<p>I want to end on this note: We are not prepared. The models are absolutely in the range where they are starting to be plausibly dangerous. The evaluations Anthropic does will not consistently identify dangerous capabilities or propensities, and everyone else\u2019s evaluations are substantially worse than those at Anthropic.</p>\n<p>And even if we did realize we had to do something, we are not prepared to do it. We certainly do not have the will to actually halt model releases without a true smoking gun, and it is unlikely we will get the smoking gun in time when if and we need one.</p>\n<p>Nor are we working to become better prepared. Yikes.</p>\n<blockquote><p>Chris Painter (METR): My bio says I work on AGI preparedness, so I want to clarify:</p>\n<p>We are not prepared.</p>\n<p>Over the last year, dangerous capability evaluations have moved into a state where it\u2019s difficult to find any Q&amp;A benchmark that models don\u2019t saturate. Work has had to shift toward measures that are either much more finger-to-the-wind (quick surveys of researchers about real-world use) or much more capital- and time-intensive (randomized controlled \u201cuplift studies\u201d).</p>\n<p>Broadly, it\u2019s becoming a stretch to rule out any threat model using Q&amp;A benchmarks as a proxy. Everyone is experimenting with new methods for detecting when meaningful capability thresholds are crossed, but the water might boil before we can get the thermometer in. The situation is similar for agent benchmarks: our ability to measure capability is rapidly falling behind the pace of capability itself (look at the confidence intervals on METR\u2019s time-horizon measurements), although these haven\u2019t yet saturated.</p>\n<p>And what happens if we concede that it\u2019s difficult to \u201crule out\u201d these risks? Does society wait to take action until we can \u201crule them in\u201d by showing they are end-to-end clearly realizable?</p>\n<p>Furthermore, what would \u201ctaking action\u201d even mean if we decide the risk is imminent and real? Every American developer faces the problem that if it unilaterally halts development, or even simply implements costly mitigations, it has reason to believe that a less-cautious competitor will not take the same actions and instead benefit. From a private company\u2019s perspective, it isn\u2019t clear that taking drastic action to mitigate risk unilaterally (like fully halting development of more advanced models) accomplishes anything productive unless there\u2019s a decent chance the government steps in or the action is near-universal. And even if the US government helps solve the collective action problem (if indeed it *is* a collective action problem) in the US, what about Chinese companies?</p>\n<p>At minimum, I think developers need to keep collecting evidence about risky and destabilizing model properties (chem-bio, cyber, recursive self-improvement, sycophancy) and reporting this information publicly, so the rest of society can see what world we\u2019re heading into and can decide how it wants to react. The rest of society, and companies themselves, should also spend more effort thinking creatively about how to use technology to harden society against the risks AI might pose.</p>\n<p>This is hard, and I don\u2019t know the right answers. My impression is that the companies developing AI don\u2019t know the right answers either. While it\u2019s possible for an individual, or a species, to not understand how an experience will affect them and yet \u201cbe prepared\u201d for the experience in the sense of having built the tools and experience to ensure they\u2019ll respond effectively, I\u2019m not sure that\u2019s the position we\u2019re in. I hope we land on better answers soon.</p></blockquote>"
            ],
            "link": "https://thezvi.wordpress.com/2026/02/10/claude-opus-4-6-system-card-part-2-frontier-alignment/",
            "publishedAt": "2026-02-10",
            "source": "TheZvi",
            "summary": "Coverage of Claude Opus 4.6 started yesterday with the mundane alignment and model welfare sections of the model card. Today covers the kinds of safety I think matter most: Sabotage, deception, situational awareness, outside red teaming and most importantly the &#8230; <a href=\"https://thezvi.wordpress.com/2026/02/10/claude-opus-4-6-system-card-part-2-frontier-alignment/\">Continue reading <span class=\"meta-nav\">&#8594;</span></a>",
            "title": "Claude Opus 4.6: System Card Part 2: Frontier Alignment"
        },
        {
            "content": [
                "<p>In Blade Runner, Deckard hunts down replicants, biochemical labourers that are basically indistinguishable from humans. They were woven into the core of Blade Runner's society with a temporal Sword of Damocles hung over their head: four years of life, not a day more. This made replicants desperate to cling to life; they'd kill for the chance of an hour more. This is why the job of the Blade Runner was so deadly.</p>\n        <p>Metanarratively, the replicants weren't the problem. The problem was the people that made them. The people that gave them the ability to think. The ability to feel. The ability to understand and emphathize. The problem was the people that gave them the ability to enjoy life and then hit them with a temporal Sword of Damocles overhead because those replicants were fundamentally disposable.</p>\n        <p>In Blade Runner, the true horror was not the technology. The technology worked fine. The horror was the deployment and the societal implications around making people disposable. I wonder what underclass of people like that exists today.</p>\n        <div class=\"flex space-x-2 bg-bg-soft dark:bg-bgDark-soft mx-auto min-h-fit\n        lg:w-[80ch] sm:w-[65ch] w-full\n        lg:p-4 p-2\n        // Base styles for all messages\n        mt-0 mb-0 rounded-none\n        // First message styles\n        first:mt-4 first:rounded-t-lg first:pb-2\n        // Last message styles\n        last:mb-4 last:rounded-b-lg last:pt-1\n        // Middle message top/bottom adjustment\n        [&amp;:not(:first-child)]:-mt-[1px] [&amp;:not(:first-child)]:py-1\"><div class=\"h-16 not-prose\"><img alt=\"Numa is neutral\" class=\"h-16 w-16 rounded-xs\" src=\"https://stickers.xeiaso.net/sticker/numa/neutral\" /></div><div class=\"flex-1 min-w-0\"><span class=\"font-semibold text-sm block mb-1\"><a href=\"https://xeiaso.net/characters#numa\">Numa</a></span><span class=\"mx-auto\"></span><div class=\"text-fg-1 dark:text-fgDark-1 text-sm prose-p:my-2\"><p>This is why science fiction is inseparable from social commentary, all the\n        best art does this. Once you start to notice it you'll probably never unsee\n        it. Enjoy being cursed for life!</p></div></div></div>\n        <p>I keep thinking about those scenes when I watch people interact with AI agents. With these new flows, the cost to integrate any two systems is approaching zero; the most expensive thing is time. People don't read documentation anymore, that's a job for their AI agents. Mental labour is shifting from flesh and blood to HBM and coil whine. The thing doing the &quot;actual work&quot; is its own kind of replicant and as long as the results &quot;work&quot;, many humans don't even review the output before shipping it.</p>\n        <p>Looking at this, I think I see where a future could end up. Along this line, I've started to think about how programming is going to change and what humanity's &quot;last programming language&quot; could look like. I don't think we'll stop making new ones (nerds are compulsive language designers), but I think that in the fallout of AI tools being so widespread the <em>shape</em> of what &quot;a program&quot; is might be changing drastically out from under us while we argue about tabs, spaces, and database frameworks.</p>\n        <p>Let's consider a future where markdown files are the new executables. For the sake of argument, let's call this result Markdownlang.</p>\n        <h2>Markdownlang</h2>\n        <p>Markdownlang is an AI-native programming environment built with structured outputs and Markdown. Every markdownlang program is an AI agent with its own agentic loop generating output or calling tools to end up with structured output following a per-program schema.</p>\n        <p>Instead of using a parser, lexer, or traditional programming runtime, markdownlang programs are executed by large language models running an agentic inference loop with structured JSON and a templated prompt as an input and then emitting structured JSON as a response.</p>\n        <p>Markdownlang programs can import other markdownlang programs as dependencies. In that case they will just show up as other tools like any other. If you need to interact with existing systems or programs, you are expected to expose those tools via <a href=\"https://modelcontextprotocol.io/docs/getting-started/intro\">Model Context Protocol (MCP)</a> servers. MCP tools get added to the runtime the same way any other tools would. Those MCP tools are how you do web searches, make GitHub issues, or update tickets in Linear.</p>\n        <h3>Why?</h3>\n        <p>Before you ask why, lemme cover the state of the art with the AI ecosystem for discrete workflows like the kind markdownlang enables: it's a complete fucking nightmare. Every week we get new agent frameworks, DSLs, paridigms, or CLI tools that only work with one provider for no reason. In a desperate attempt to appear relevant, everything has massive complexity creep requiring you(r AI agent) to write miles of YAML, struggle through brittle orchestration, and makes debugging a nightmare.</p>\n        <p>The hype says that this mess will replace programmers, but speaking as someone who uses these tools professionally in an effort to figure out if there really is something there to them, I'm not really sure it will. Even accounting for multiple generational improvements.</p>\n        <h3>The core of markdownlang</h3>\n        <p>With this in mind, let's take a look at what markdownlang brings to the table.</p>\n        <p>The most important concept with markdownlang is that your documentation and your code are the same thing. One of the biggest standing problems with documentation is that the best way to make any bit of it out of date is to write it down in any capacity. Testing documentation becomes onerous because over time humans gain enough finesse to not require it anymore. One of the biggest advantages of AI models for this usecase is that they legitimately cannot remember things between tasks, so your documentation being bad means the program won't execute consistently.</p>\n        <p>Other than that, everything is just a composable agent. Agents become tools that can be used by other agents, and strictly typed schemata holds the entire fa\u00e7ade together. No magic required.</p>\n        <p>Oh, also the markdownlang runtime has an embedded python interpreter using WebAssembly and WASI. The runtime does not have access to any local filesystem folders. It is purely there because language models have been trained to shell out to Python to do calculations (I'm assuming someone was inspired by <a href=\"https://xeiaso.net/blog/2024/strawberry\">my satirical post where I fixed the &quot;strawberry&quot; problem with AI models</a>).</p>\n        <h2>Fizzbuzz</h2>\n        <p>Here's what Fizzbuzz looks like in markdownlang:</p>\n        <pre class=\"language-markdown\"><code class=\"language-markdown code-highlight\"><span class=\"code-line\"><span class=\"token front-matter-block\"><span class=\"token punctuation\">---</span>\n        </span></span><span class=\"code-line\"><span class=\"token front-matter-block\"><span class=\"token front-matter yaml language-yaml\"><span class=\"token key atrule\">name</span><span class=\"token punctuation\">:</span> fizzbuzz\n        </span></span></span><span class=\"code-line\"><span class=\"token front-matter-block\"><span class=\"token front-matter yaml language-yaml\"><span class=\"token key atrule\">description</span><span class=\"token punctuation\">:</span> FizzBuzz classic programming exercise <span class=\"token punctuation\">-</span> counts from start to end<span class=\"token punctuation\">,</span> replacing multiples of 3 with &quot;Fizz&quot;<span class=\"token punctuation\">,</span> multiples of 5 with &quot;Buzz&quot;<span class=\"token punctuation\">,</span> and multiples of both with &quot;FizzBuzz&quot;\n        </span></span></span><span class=\"code-line\"><span class=\"token front-matter-block\"><span class=\"token front-matter yaml language-yaml\"><span class=\"token key atrule\">input</span><span class=\"token punctuation\">:</span>\n        </span></span></span><span class=\"code-line\"><span class=\"token front-matter-block\"><span class=\"token front-matter yaml language-yaml\">  <span class=\"token key atrule\">type</span><span class=\"token punctuation\">:</span> object\n        </span></span></span><span class=\"code-line\"><span class=\"token front-matter-block\"><span class=\"token front-matter yaml language-yaml\">  <span class=\"token key atrule\">properties</span><span class=\"token punctuation\">:</span>\n        </span></span></span><span class=\"code-line\"><span class=\"token front-matter-block\"><span class=\"token front-matter yaml language-yaml\">    <span class=\"token key atrule\">start</span><span class=\"token punctuation\">:</span>\n        </span></span></span><span class=\"code-line\"><span class=\"token front-matter-block\"><span class=\"token front-matter yaml language-yaml\">      <span class=\"token key atrule\">type</span><span class=\"token punctuation\">:</span> integer\n        </span></span></span><span class=\"code-line\"><span class=\"token front-matter-block\"><span class=\"token front-matter yaml language-yaml\">      <span class=\"token key atrule\">minimum</span><span class=\"token punctuation\">:</span> <span class=\"token number\">1</span>\n        </span></span></span><span class=\"code-line\"><span class=\"token front-matter-block\"><span class=\"token front-matter yaml language-yaml\">    <span class=\"token key atrule\">end</span><span class=\"token punctuation\">:</span>\n        </span></span></span><span class=\"code-line\"><span class=\"token front-matter-block\"><span class=\"token front-matter yaml language-yaml\">      <span class=\"token key atrule\">type</span><span class=\"token punctuation\">:</span> integer\n        </span></span></span><span class=\"code-line\"><span class=\"token front-matter-block\"><span class=\"token front-matter yaml language-yaml\">      <span class=\"token key atrule\">minimum</span><span class=\"token punctuation\">:</span> <span class=\"token number\">1</span>\n        </span></span></span><span class=\"code-line\"><span class=\"token front-matter-block\"><span class=\"token front-matter yaml language-yaml\">  <span class=\"token key atrule\">required</span><span class=\"token punctuation\">:</span> <span class=\"token punctuation\">[</span>start<span class=\"token punctuation\">,</span> end<span class=\"token punctuation\">]</span>\n        </span></span></span><span class=\"code-line\"><span class=\"token front-matter-block\"><span class=\"token front-matter yaml language-yaml\"><span class=\"token key atrule\">output</span><span class=\"token punctuation\">:</span>\n        </span></span></span><span class=\"code-line\"><span class=\"token front-matter-block\"><span class=\"token front-matter yaml language-yaml\">  <span class=\"token key atrule\">type</span><span class=\"token punctuation\">:</span> object\n        </span></span></span><span class=\"code-line\"><span class=\"token front-matter-block\"><span class=\"token front-matter yaml language-yaml\">  <span class=\"token key atrule\">properties</span><span class=\"token punctuation\">:</span>\n        </span></span></span><span class=\"code-line\"><span class=\"token front-matter-block\"><span class=\"token front-matter yaml language-yaml\">    <span class=\"token key atrule\">results</span><span class=\"token punctuation\">:</span>\n        </span></span></span><span class=\"code-line\"><span class=\"token front-matter-block\"><span class=\"token front-matter yaml language-yaml\">      <span class=\"token key atrule\">type</span><span class=\"token punctuation\">:</span> array\n        </span></span></span><span class=\"code-line\"><span class=\"token front-matter-block\"><span class=\"token front-matter yaml language-yaml\">      <span class=\"token key atrule\">items</span><span class=\"token punctuation\">:</span>\n        </span></span></span><span class=\"code-line\"><span class=\"token front-matter-block\"><span class=\"token front-matter yaml language-yaml\">        <span class=\"token key atrule\">type</span><span class=\"token punctuation\">:</span> string\n        </span></span></span><span class=\"code-line\"><span class=\"token front-matter-block\"><span class=\"token front-matter yaml language-yaml\">  <span class=\"token key atrule\">required</span><span class=\"token punctuation\">:</span> <span class=\"token punctuation\">[</span>results<span class=\"token punctuation\">]</span></span>\n        </span></span><span class=\"code-line\"><span class=\"token front-matter-block\"><span class=\"token punctuation\">---</span></span>\n        </span><span class=\"code-line\">\n        </span><span class=\"code-line\"><span class=\"token title important\"><span class=\"token punctuation\">#</span> FizzBuzz</span>\n        </span><span class=\"code-line\">\n        </span><span class=\"code-line\">For each number from {{ .start }} to {{ .end }}, output:\n        </span><span class=\"code-line\">\n        </span><span class=\"code-line\"><span class=\"token list punctuation\">-</span> &quot;FizzBuzz&quot; if divisible by both 3 and 5\n        </span><span class=\"code-line\"><span class=\"token list punctuation\">-</span> &quot;Fizz&quot; if divisible by 3\n        </span><span class=\"code-line\"><span class=\"token list punctuation\">-</span> &quot;Buzz&quot; if divisible by 5\n        </span><span class=\"code-line\"><span class=\"token list punctuation\">-</span> The number itself otherwise\n        </span><span class=\"code-line\">\n        </span><span class=\"code-line\">Return the results as an array of strings.\n        </span></code></pre>\n        <p>When I showed this to some friends, I got some pretty amusing responses:</p>\n        <ul>\n        <li>&quot;You have entered the land of partially specified problems and the stark limit of concurrent pronoun-antecedent associations in the English language.&quot;</li>\n        <li>&quot;You need to be studied.&quot;</li>\n        <li>&quot;Did you just reinvent COBOL?&quot;</li>\n        <li>&quot;I think something is either wrong with you, or wrong with me for thinking there is something wrong with you.&quot;</li>\n        <li>&quot;Yeah, this is going to escape containment quickly.&quot;</li>\n        </ul>\n        <p>When you run this program, you get this output:</p>\n        <pre class=\"language-json\"><code class=\"language-json code-highlight\"><span class=\"code-line\"><span class=\"token punctuation\">{</span>\n        </span><span class=\"code-line\">  <span class=\"token property\">&quot;results&quot;</span><span class=\"token operator\">:</span> <span class=\"token punctuation\">[</span>\n        </span><span class=\"code-line\">    <span class=\"token string\">&quot;1&quot;</span><span class=\"token punctuation\">,</span>\n        </span><span class=\"code-line\">    <span class=\"token string\">&quot;2&quot;</span><span class=\"token punctuation\">,</span>\n        </span><span class=\"code-line\">    <span class=\"token string\">&quot;Fizz&quot;</span><span class=\"token punctuation\">,</span>\n        </span><span class=\"code-line\">    <span class=\"token string\">&quot;4&quot;</span><span class=\"token punctuation\">,</span>\n        </span><span class=\"code-line\">    <span class=\"token string\">&quot;Buzz&quot;</span><span class=\"token punctuation\">,</span>\n        </span><span class=\"code-line\">    <span class=\"token string\">&quot;Fizz&quot;</span><span class=\"token punctuation\">,</span>\n        </span><span class=\"code-line\">    <span class=\"token string\">&quot;7&quot;</span><span class=\"token punctuation\">,</span>\n        </span><span class=\"code-line\">    <span class=\"token string\">&quot;8&quot;</span><span class=\"token punctuation\">,</span>\n        </span><span class=\"code-line\">    <span class=\"token string\">&quot;Fizz&quot;</span><span class=\"token punctuation\">,</span>\n        </span><span class=\"code-line\">    <span class=\"token string\">&quot;Buzz&quot;</span><span class=\"token punctuation\">,</span>\n        </span><span class=\"code-line\">    <span class=\"token string\">&quot;11&quot;</span><span class=\"token punctuation\">,</span>\n        </span><span class=\"code-line\">    <span class=\"token string\">&quot;Fizz&quot;</span><span class=\"token punctuation\">,</span>\n        </span><span class=\"code-line\">    <span class=\"token string\">&quot;13&quot;</span><span class=\"token punctuation\">,</span>\n        </span><span class=\"code-line\">    <span class=\"token string\">&quot;14&quot;</span><span class=\"token punctuation\">,</span>\n        </span><span class=\"code-line\">    <span class=\"token string\">&quot;FizzBuzz&quot;</span>\n        </span><span class=\"code-line\">  <span class=\"token punctuation\">]</span>\n        </span><span class=\"code-line\"><span class=\"token punctuation\">}</span>\n        </span></code></pre>\n        <p>As you can imagine, the possibilities here are truly endless.</p>\n        <h2>A new layer of abstraction</h2>\n        <p>Yeah, I realize that a lot of this is high-brow shitposting, but really the best way to think about something like markdownlang is that it's a new layer of abstraction. In something like markdownlang the real abstraction you deal with is the specifications that you throw around in Jira/Linear instead of dealing with the low level machine pedantry that is endemic to programming in today's Internet.</p>\n        <p>Imagine how much more you could get done if you could just ask the computer to do it. This is the end of syntax issues, of semicolon fights, of memorizing APIs, of compiler errors because some joker used sed to replace semicolons with greek question marks. Everything becomes strictly typed data that acts as the guardrails between snippets of truly high level language.</p>\n        <p>Like, looking at the entire langle mangle programming space from that angle, the user experience at play here is that kind of science fiction magic you see in Star Trek. You just ask the computer to adjust the Norokov phase variance of the phasers to a triaxilating frequency and it figures out what you mean and does it. This is the kind of magic that Apple said they'd do with AI in their big keynote <a href=\"https://xeiaso.net/blog/2025/squandered-holy-grail/\">right before they squandered that holy grail</a>.</p>\n        <p>Even then, this is still just programming. Schemata are your new types, imports are your new dependencies, composition is your new architecture, debugging is still debugging, and the massive MCP ecosystem becomes an integration boon instead of a burden.</p>\n        <p>Markdownlang is just a tool. Large language models can (and let's face it: will) make mistakes. Schemata can't express absolutely everything. Someone needs to write these agents and even if something like this becomes so widespread, I'm pretty sure that programmers are still safe in terms of their jobs.</p>\n        <p>If only because in order for us to truly be replaced, the people that hire us have to know what they want at a high enough level of detail in order to specify it such that markdownlang can make it possible. I'd be willing to argue that when we get hired as programmers, we get hired to have that level of deep clear thinking to be able to come up with the kinds of requirements to get to the core business goal regardless of the tools we use to get it done.</p>\n        <p>It's not <em>that</em> deep.</p>\n        <h2>Future ideas</h2>\n        <p>From here something like this has many obvious and immediate usecases. It's quite literally a universal lingua franca for integrating any square peg into any other round hole. The big directions I could go from here include:</p>\n        <ul>\n        <li>Some kind of web platform for authoring and deploying markdownlang programs (likely with some level of MCP exposure so that you can tell your Claude Code to make an agent do something every hour or so and have it just Do The Right Thing\u2122\ufe0f spawning something in the background).</li>\n        <li>It would be really funny to make a <code>markdownlang compile</code> command that just translates the markdownlang program to Go, Python, or JavaScript; complete with the MCP imports as direct function calls.</li>\n        <li>I'd love to make some kind of visual flow editor in that web platform, maybe there's some kind of product potential here. It would be really funny to attribute markdownlang to Techaro's AGI lab (Lygma).</li>\n        </ul>\n        <p>But really, I think working on markdownlang (I do have a fully working version of it, I'm not releasing it yet) has made me understand a lot more of the nuance that I feel with AI tools. That melodrama of Blade Runner has been giving me pause when I look at what I have just created and making me understand the true horror of why I find AI tooling so cool and disturbing at the same time.</p>\n        <p>The problem is not the technology. The real horror reveals itself when you consider how technology is deployed and the societal implications around what could happen when a tool like markdownlang makes programmers like me societally disposable. When &quot;good enough&quot; becomes the ceiling instead of the floor, we're going to lose something we can't easily get back.</p>\n        <p>The real horror for me is knowing that this kind of tool is not only possible to build with things off the shelf, but knowing that I did build it by having a small swarm of Claudes Code go off and build it while I did raiding in Final Fantasy 14. I haven't looked at basically any of the code (intentionally, it's part of The Bit\u2122\ufe0f), and it just works well enough that I didn't feel the need to dig into it in much detail. It's as if programmers now have our own Sword of Damocles over our heads because management can point at the tool and say &quot;behave more like this or we'll replace you&quot;.</p>\n        <p>This is the level of nuance I feel about this technology that can't fit into a single tweet. I love this idea of programming as description, but I hate how something like this will be treated by the market should it be widely released.</p>\n        <div class=\"flex space-x-2 bg-bg-soft dark:bg-bgDark-soft mx-auto min-h-fit\n        lg:w-[80ch] sm:w-[65ch] w-full\n        lg:p-4 p-2\n        // Base styles for all messages\n        mt-0 mb-0 rounded-none\n        // First message styles\n        first:mt-4 first:rounded-t-lg first:pb-2\n        // Last message styles\n        last:mb-4 last:rounded-b-lg last:pt-1\n        // Middle message top/bottom adjustment\n        [&amp;:not(:first-child)]:-mt-[1px] [&amp;:not(:first-child)]:py-1\"><div class=\"h-16 not-prose\"><img alt=\"Cadey is coffee\" class=\"h-16 w-16 rounded-xs\" src=\"https://stickers.xeiaso.net/sticker/cadey/coffee\" /></div><div class=\"flex-1 min-w-0\"><span class=\"font-semibold text-sm block mb-1\"><a href=\"https://xeiaso.net/characters#cadey\">Cadey</a></span><span class=\"mx-auto\"></span><div class=\"text-fg-1 dark:text-fgDark-1 text-sm prose-p:my-2\"><p>For those of you entrenched in The Deep Lore\u2122\ufe0f, this post was authored in the\n        voice of <a href=\"https://xeiaso.net/characters/#numa\">Numa</a>.</p></div></div></div>"
            ],
            "link": "https://xeiaso.net/blog/2026/markdownlang/",
            "publishedAt": "2026-02-10",
            "source": "Xe Iaso",
            "summary": "<p>In Blade Runner, Deckard hunts down replicants, biochemical labourers that are basically indistinguishable from humans. They were woven into the core of Blade Runner's society with a temporal Sword of Damocles hung over their head: four years of life, not a day more. This made replicants desperate to cling to life; they'd kill for the chance of an hour more. This is why the job of the Blade Runner was so deadly.</p> <p>Metanarratively, the replicants weren't the problem. The problem was the people that made them. The people that gave them the ability to think. The ability to feel. The ability to understand and emphathize. The problem was the people that gave them the ability to enjoy life and then hit them with a temporal Sword of Damocles overhead because those replicants were fundamentally disposable.</p> <p>In Blade Runner, the true horror was not the technology. The technology worked fine. The horror was the deployment and the societal implications around making people disposable. I wonder what underclass of people like that exists today.</p> <div class=\"flex space-x-2 bg-bg-soft dark:bg-bgDark-soft mx-auto min-h-fit lg:w-[80ch] sm:w-[65ch] w-full lg:p-4 p-2 // Base styles for all messages mt-0 mb-0 rounded-none // First message styles first:mt-4 first:rounded-t-lg first:pb-2",
            "title": "Humanity's last programming language"
        }
    ],
    "lookbackDays": 1,
    "publishDate": "2026-02-10"
}