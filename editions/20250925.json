{
    "articles": [
        {
            "content": [
                "<p>I fear we are in the waning days of the People Read Blog Posts About Random Well-Understood Topics Instead of Asking Their Automatons Era. So before I lose my chance, here is a blog post about some random well-understood topics.</p>\n\n<h2 id=\"marathons-are-stupidly-fast\">Marathons are stupidly fast</h2>\n\n<p>You probably know that people can now run marathons in just over 2 hours. But do you realize how insane that is?</p>\n\n<p>That\u2019s an average speed of 21.1 km per hour, or 13.1 miles per hour. You can think of that as running a mile in 4:35 (world record: <a href=\"https://en.wikipedia.org/wiki/Mile_run_world_record_progression\">3:45</a>), except doing it 26.2 times in a row. Or, you can think of that as running 100 meters in 17.06 seconds (world record: <a href=\"https://en.wikipedia.org/wiki/100_metres\">9.58 seconds</a>), except doing it 421.6 times in a row. I\u2019d guess that only around half of the people reading this could run 100 meters in 17.06 seconds <em>once</em>.</p>\n\n<p>This crazy marathon running speed is mostly due to humans being well-adapted for running and generally tenacious. But <em>some</em> of it is due to new shoes with carbon-fiber plates that came out in the late 2010s.</p>\n\n<p>The theory behind these shoes is quite interesting. When you run, you mainly use four joints:</p>\n\n<ol>\n  <li>Hips</li>\n  <li>Knees</li>\n  <li>Ankles</li>\n  <li>Metatarsophalangeal</li>\n</ol>\n\n<p>If you haven\u2019t heard of the last of these, they\u2019re pronounced \u201c<em>met</em>-uh-tar-so-fuh-<em>lan</em>-jee-ul\u201d or \u201cMTP\u201d. These are the joints inside your feet behind your big toes.</p>\n\n<p>Besides sounding made-up, they\u2019re different from the other joints in a practical way: The other joints are all attached to large muscles and tendons that stretch out and return energy while running sort of like springs. These can apparently recover around <a href=\"https://doi.org/10.1007/s00421-020-04472-9\">60%</a> of the energy expended in each stride. (Kangaroos seemingly do <a href=\"https://spot.colorado.edu/~kram/kangaroo.pdf\">even better</a>.)  But the MTP joints are only attached to small muscles and tendons, so the energy that goes into them is mostly lost.</p>\n\n<p>These new shoe designs have complex constructions of foam and plates that can do the same job as the MTP joints, but\u2014unlike the MTP joints\u2014store and return that energy to the runner. A recent <a href=\"https://doi.org/10.1016/j.jshs.2025.101069\">meta-analysis</a> estimated that this reduced total oxygen consumption by ~2.7% and marathon times by \u223c2.18%.</p>\n\n<h2 id=\"algernon\">Algernon</h2>\n\n<p>I wonder if these shoes are useful as a test case for the <a href=\"https://gwern.net/drug-heuristic\">Algernon argument</a>. In general, that argument is that there shouldn\u2019t be any simple technology that would make humans dramatically smarter, since if there was, then evolution would have already found it.</p>\n\n<p>You can apply the same kind of argument to running: We have been optimized very hard by evolution to be good at running, so there shouldn\u2019t be any \u201ceasy\u201d technologies that would make us dramatically faster or more efficient.</p>\n\n<p>In the context of the shoes, I think that argument does\u2026 OK? The shoes definitely help. But carbon fiber plates are pretty hard to make, and the benefit is pretty modest. Maybe this is some evidence that Algernon isn\u2019t a hard \u201cwall\u201d, but rather a steep slope.</p>\n\n<p>Or, perhaps thinking is just different from running. If you <a href=\"https://dynomight.net/2021/01/25/how-to-run-without-all-the-agonizing-pain/\">start running</a>, you <em>will</em> get better at it, in a way that spills over into lots of other physical abilities. But there doesn\u2019t seem to be any cognitive task that you can practice and make yourself better at other cognitive tasks.</p>\n\n<p>If you have some shoes that will make me 2.7% smarter, I\u2019ll buy them.</p>\n\n<h2 id=\"pangea\">Pangea</h2>\n\n<p>Pangea was a supercontinent that contained roughly all the land on Earth. At the beginning of the Jurassic 200 million years ago, it broke up and eventually formed the current continents. But isn\u2019t the Earth 4.5 billion years old? Why would all the land stick together for 95% of that time and then suddenly break up?</p>\n\n<p>The accepted theory is that it didn\u2019t. Instead, it\u2019s believed that Earth cycles between super-continents and dispersed continents, and Pangea is merely the most recent super-continent.</p>\n\n<p>But why would there be such a cycle? We can break that down into two sub-questions.</p>\n\n<p>First, why would dispersed continents fuse together into a supercontinent? Well, you can think of the Earth as a big ball of rock, warmed half by primordial heat from when the planet formed and half by radioactive decay. Since the surface is exposed to space, it cools, resulting on solid chunks that sort of slide around on the warm magma in the upper mantle. Some of those chunks are denser than others, which causes them to sink into the mantle a bit and get covered with water. So when a \u201cland chunk\u201d crashes into a \u201cwater chunk\u201d, the land chunk slides on top. But if two land chunks crash into each other, they tend to crumple together into mountains and stick to each other.</p>\n\n<p>You can see this by comparing this <a href=\"https://en.wikipedia.org/wiki/Plate_tectonics#/media/File:Tectonic_plates_(2022).svg\">map</a> of all the current plates:</p>\n\n<p><img alt=\"\" src=\"https://dynomight.net/img/shorts-5/plates.png\" /></p>\n\n<p>To this map of elevation:</p>\n\n<p><img alt=\"\" src=\"https://dynomight.net/img/shorts-5/elevation.jpg\" /></p>\n\n<p>OK, but once a super-continent forms, why would it break apart? Well, compared to the ocean floor, land chunks are thicker and lighter. So they trap heat from inside the planet sort of like a blanket. With no cool ocean floor sliding back into the warm magma beneath, that magma keeps getting warmer and warmer. After tens of millions of years, it heats up so much that it stretches the land above and finally rips it apart.</p>\n\n<p>It\u2019s expected that a new supercontinent \u201cPangea Ultima\u201d will form in 250 million years. By that time, the sun will be putting out around 2.3% more energy, making things hotter. On top of that, it\u2019s suspected that Pangea Ultima, for <a href=\"https://doi.org/10.1038/s41561-023-01259-3\">extremely complicated reasons</a>, will greatly increase the amount of CO\u2082 in the atmosphere, likely making the planet uninhabitable by mammals. So we\u2019ve got that going for us.</p>\n\n<h2 id=\"egypt-and-the-sea-peoples\">Egypt and the Sea Peoples</h2>\n\n<p>The <a href=\"https://en.wikipedia.org/wiki/Sea_Peoples\">Sea Peoples</a> are a group of people from\u2026 somewhere\u2026 that appeared in the Eastern Mediterranean around 1200 BC and left a trail of destruction from modern Turkey down to modern Egypt. They are thought to be either a cause or symptom of the <a href=\"https://en.wikipedia.org/wiki/Late_Bronze_Age_collapse\">Late Bronze Age collapse</a>.</p>\n\n<p>But did you know the Egyptians made <em>carvings</em> of the situation while they were under attack? Apparently the battle looked like <a href=\"https://en.wikipedia.org/wiki/Medinet_Habu\">this</a>:</p>\n\n<p><img alt=\"\" src=\"https://dynomight.net/img/shorts-5/medinet.jpg\" /></p>\n\n<p>In the inscription, Pharaoh Ramesses III reports:</p>\n\n<blockquote>\n  <p>Those who reached my boundary, their seed is not; their hearts and their souls are finished forever and ever. As for those who had assembled before them on the sea, the full flame was their front before the harbor mouths, and a wall of metal upon the shore surrounded them. They were dragged, overturned, and laid low upon the beach; slain and made heaps from stern to bow of their galleys, while all their things were cast upon the water.</p>\n</blockquote>"
            ],
            "link": "https://dynomight.net/shorts-5/",
            "publishedAt": "2025-09-25",
            "source": "Dynomight",
            "summary": "<p>I fear we are in the waning days of the People Read Blog Posts About Random Well-Understood Topics Instead of Asking Their Automatons Era. So before I lose my chance, here is a blog post about some random well-understood topics.</p> <h2 id=\"marathons-are-stupidly-fast\">Marathons are stupidly fast</h2> <p>You probably know that people can now run marathons in just over 2 hours. But do you realize how insane that is?</p> <p>That\u2019s an average speed of 21.1 km per hour, or 13.1 miles per hour. You can think of that as running a mile in 4:35 (world record: <a href=\"https://en.wikipedia.org/wiki/Mile_run_world_record_progression\">3:45</a>), except doing it 26.2 times in a row. Or, you can think of that as running 100 meters in 17.06 seconds (world record: <a href=\"https://en.wikipedia.org/wiki/100_metres\">9.58 seconds</a>), except doing it 421.6 times in a row. I\u2019d guess that only around half of the people reading this could run 100 meters in 17.06 seconds <em>once</em>.</p> <p>This crazy marathon running speed is mostly due to humans being well-adapted for running and generally tenacious. But <em>some</em> of it is due to new shoes with carbon-fiber plates that came out in the late 2010s.</p> <p>The theory behind these shoes is quite interesting. When you run, you mainly use four",
            "title": "Shoes, Algernon, Pangea, and Sea Peoples"
        },
        {
            "content": [],
            "link": "https://buttondown.com/hillelwayne/archive/new-blog-post-a-very-early-history-of-algebraic/",
            "publishedAt": "2025-09-25",
            "source": "Hillel Wayne",
            "summary": "<p>Last week I said that this week's newsletter would be a brief history of algebraic data types.</p> <p>I was wrong.</p> <p>That history is now a <a href=\"https://www.hillelwayne.com/post/algdt-history/\" target=\"_blank\">3500 word blog post</a>.</p> <p><a href=\"https://www.patreon.com/posts/blog-notes-very-139696324?utm_medium=clipboard_copy&amp;utm_source=copyLink&amp;utm_campaign=postshare_creator&amp;utm_content=join_link\" target=\"_blank\">Patreon blog notes here</a>.</p> <hr /> <h3>I'm speaking at <a href=\"https://www.p99conf.io/\" target=\"_blank\">P99 Conf</a>!</h3> <p>My talk, \"Designing Low-Latency Systems with TLA+\", is happening 10/23 at 11:30 central time. It's an online conf and the talk's only 16 minutes, so come check it out!</p>",
            "title": "New Blog Post: \" A Very Early History of Algebraic Data Types\""
        },
        {
            "content": [
                "<div class=\"trix-content\">\n  <div>A young entrepreneur in his mid-20s just emailed me asking for some advice.<br /><br />He just sold a business and ended up with a couple million in liquid cash. He wanted to know if he should invest it, use it to build a new company, or do something else with it.<br /><br />My advice wasn't what he was expecting.<br /><br />I just said don't lose it. Do nothing with it. Put it in the bank. Something safe, earning a little, but not too much that it's at risk.<br /><br />Money doesn't need to work. It can rest. Leave it be. You're 26 \u2014 you can get back to work.<br /><br />A couple million liquid cash is a huge haul. Maintain! Don't lose. Always have that. And add more to that safe pile as you go. That's yours now. Keep it that way.<br /><br />-Jason</div>\n</div>"
            ],
            "link": "https://world.hey.com/jason/what-to-do-with-2m-4f0a16b2",
            "publishedAt": "2025-09-25",
            "source": "Jason Fried",
            "summary": "<div class=\"trix-content\"> <div>A young entrepreneur in his mid-20s just emailed me asking for some advice.<br /><br />He just sold a business and ended up with a couple million in liquid cash. He wanted to know if he should invest it, use it to build a new company, or do something else with it.<br /><br />My advice wasn't what he was expecting.<br /><br />I just said don't lose it. Do nothing with it. Put it in the bank. Something safe, earning a little, but not too much that it's at risk.<br /><br />Money doesn't need to work. It can rest. Leave it be. You're 26 \u2014 you can get back to work.<br /><br />A couple million liquid cash is a huge haul. Maintain! Don't lose. Always have that. And add more to that safe pile as you go. That's yours now. Keep it that way.<br /><br />-Jason</div> </div>",
            "title": "What to do with $2M?"
        },
        {
            "content": [
                "<p><em>[previously in series: <a href=\"https://astralcodexten.substack.com/p/every-bay-area-house-party\">1</a>, <a href=\"https://astralcodexten.substack.com/p/another-bay-area-house-party\">2</a>, <a href=\"https://astralcodexten.substack.com/p/even-more-bay-area-house-party\">3</a>, <a href=\"https://www.astralcodexten.com/p/bride-of-bay-area-house-party\">4</a>, <a href=\"https://www.astralcodexten.com/p/son-of-bride-of-bay-area-house-party\">5</a>, <a href=\"https://www.astralcodexten.com/p/ye-olde-bay-area-house-party\">6</a>, <a href=\"https://www.astralcodexten.com/p/press-any-key-for-bay-area-house\">7</a>]</em></p><p>Something is off about this Bay Area House Party. There are . . . women. </p><p>&#8220;I&#8217;ve never seen a gender balance like this in the Bay Area,&#8221; you tell your host Chris. &#8220;Is this one of those fabled ratio parties?&#8221;</p><p>&#8220;No - have you heard of <a href=\"https://www.ft.com/content/0e244103-80e8-4acc-9262-d6a45bbbaf14\">curtfishing</a>? It&#8217;s the new male dating trend. You say in your Bumble profile that you&#8217;re a member of the Dissident Right who often attends parties with Curtis Yarvin. Then female journos ask you out in the hopes that you&#8217;ll bring them along and they can turn it into an article.&#8221;</p><p>&#8220;What happens when they realize Curtis Yarvin isn&#8217;t at the party?&#8221;</p><p>&#8220;Oh, everyone pools their money and hires someone to pretend to be Curtis. You can just do things. Today it&#8217;s Ramchandra.&#8221;</p><p>You follow his gaze, and there is Ramchandra, hair greased back, wearing a leather jacket, surrounded by a crowd of young women. &#8220;When I say I&#8217;m against furries,&#8221; he&#8217;s explaining, staccato, at 120 wpm, &#8220;I mean the sort of captured furries you get under the post-Warren-G-Harding liberal order, the ones getting the fat checks from the Armenians at Harvard and the Department of Energy. I love <em>real</em> furries, the kind you would have found in 1920s New Mexico eating crocodile steaks with Baron von Ungern-Sternberg! Some of my best friends are furries, as de Broglie-Bohm and my sainted mother used to say! Just watch out for the Kikuyu, that&#8217;s my advice! Hahahahahaha!&#8221; Some of the women are taking notes. &#8220;But enough about me. When I was seventeen, I spent seven weeks in Bensonhurst - that&#8217;s in the Rotten Apple, in case you can&#8217;t tell your Nepalis from your Neapolitans. A dear uncle of mine, after whom I was named&#8230;&#8221;</p><p>&#8220;Ramchandra is pretty good,&#8221; you admit. &#8220;Still, if it were me I would have gone with a white guy.&#8221;</p><p>&#8220;It&#8217;s fine,&#8221; says Chris. &#8220;Curtis describes himself as a <em>mischling</em>, and none of the journos know what that means.&#8221;</p><p>Ramchandra is still talking. &#8220;Of course, strawberries have only been strawberries since after the Kronstadt Rebellion. Before that, strawberries were just pears. You had to get them hand-painted red by Gypsies, if you can believe that. Gypsies! So if you hear someone from west of Pennsylvania Avenue mention &#8216;strawberries&#8217;, that&#8217;s what we in the business call <em>il</em> <em>significanto</em>.<em>&#8221;</em></p><p>&#8220;I admit he has talent,&#8220; you say. &#8220;But this curtfishing thing - surely at some point your date realizes that you&#8217;re not actually a high-status yet problematic bad boy who can further her career just by existing, and then she ghosts you, right?&#8221;</p><p>&#8220;That&#8217;s <em>every</em> date in San Francisco. But when you curtfish, sometimes she comps your meal from her expense account. It&#8217;s a strict Pareto improvement!&#8221;</p><p>After some thought, you agree this is a great strategy with no downsides, maybe the biggest innovation in dating since the invention of alcohol. Having failed to bring your own journo to the party, you look for one who seems unattached. You catch the eye of a blonde woman who introduces herself as Gabrielle, and you try to give her the least autistic &#8220;Hello&#8221; of which you are capable.</p><p>&#8220;Sorry,&#8221; she says. &#8220;I&#8217;m here with my date, Chad Redstate.&#8221; She points to your friend Xiaochang, who winks at you.</p><p>&#8220;Oh,&#8221; you say. &#8220;I see. So, what&#8217;s it like being a journalist?&#8221;</p><p>&#8220;How does everyone know I&#8217;m a . . . fine, whatever. It&#8217;s fine.&#8221;</p><p>&#8220;Do you come to Silicon Valley often?&#8221;</p><p>&#8220;No, this is actually my first time. I can&#8217;t believe how many people there are here. I thought it was just Curtis Yarvin, Peter Thiel, and the Theranos woman. So, are you all Zizians?&#8221;</p><p>You can&#8217;t tell if she&#8217;s joking or not, so you deflect. &#8220;Is this your first time on the Curtis Yarvin beat?&#8221;</p><p>&#8220;Oh, I&#8217;m not on the beat. I&#8217;m freelancing tonight, trying to get my big break. My day job is at <em>Giving Middle-Aged Women Who Have Ruined Their Lives With Terrible Relationship Decisions A Platform To Recommend Those Decisions To Others, And People Obviously Notice The Contradiction And Post About It To Dunk On Us, But Actually They're Only Taking Us Viral And In Fact That Was Our Strategy All Along, Ha Ha!</em> <em>Magazine</em>. You probably haven&#8217;t heard of us by name, but we syndicate to all the big outlets. WaPo, NYT, the Atlantic. Usually we&#8217;re based in NYC, but we&#8217;re starting to exhaust its supply of middle-aged women who have ruined their lives with terrible relationship decisions who nevertheless want to recommend those decisions to others, so we&#8217;re out here scouting for new talent. Do you know if there are people like that in the Bay?&#8221;</p><p>&#8220;That&#8217;s a category of question I&#8217;ve never been asked before. It&#8217;s kind of like &#8216;We&#8217;re running low on Chinese people in Beijing, do you know if there are any in Shanghai?&#8217;&#8221;</p><p>&#8220;So you <em>do</em> know some! Can you intro them to me?&#8221;</p><p>&#8220;I don&#8217;t know, all the ones here already have Substacks. I think they&#8217;ve grown attached to being their own boss.&#8221;</p><p>&#8220;Too bad,&#8221; says Gabrielle, &#8220;let me know if you hear otherwise.&#8221; She hands you her business card, which is the closest you&#8217;ve ever come to getting a woman&#8217;s number at a Bay Area House Party. Encouraged, you turn to another woman nearby, who introduces herself as Caitlin. &#8220;So, what&#8217;s it like being a journalist?&#8221;</p><p>&#8220;Why does everyone here think I&#8217;m a journalist?&#8221; she asks. &#8220;I&#8217;m a normal person, I swear!&#8221;</p><p>&#8220;Oh, sorry, really sorry, didn&#8217;t mean to stereotype. Normal person, got it. So how&#8217;s your startup doing?&#8221;</p><p>&#8220;Pretty good. I&#8217;m a founder at Condemnr. Maybe you&#8217;ve heard of us?&#8221;</p><p>&#8220;Actually no. Tell me about it.&#8221;</p><p>&#8220;Lots of people are tripped up by not condemning enough things. Imagine that you want to express discontent with the Trump administration restricting food stamps, but someone points out that <em>it&#8217;s pretty suspicious</em> that you condemn food insecurity for white people but you didn&#8217;t condemn the famine in Gaza equally hard. So you try condemning the famine in Gaza, and someone points out that it&#8217;s <em>pretty suspicious </em>that you condemn starvation when it makes Jews look like the bad guys, but you didn&#8217;t condemn the famine in Ethiopia equally hard. So you try condemning the famine in Ethiopia, but then people tell you that&#8217;s &#8216;telescopic altruism&#8217;, because you didn&#8217;t condemn a murder that happened in your own city. So you try condemning a murder in your own city, but it was a black-on-white murder, and people say that it&#8217;s <em>pretty suspicious </em>that you didn&#8217;t condemn the latest white-on-black murder equally hard. The only solution is to monitor the news 24-7, condemning each thing as soon as it happens, in exact proportion to how bad it is. But nobody has time for that. So you give us access to your Twitter account and we do it for you. We promise not only to condemn all bad things within one business day of them happening, but to use all the appropriate words. You know those politicians who get in trouble because they condemned &#8220;the recent massacre&#8221; in vague terms but didn&#8217;t use the words &#8220;terrorism&#8221; or &#8220;radical Islam&#8221;, or because they said &#8220;killed&#8221; instead of &#8220;murdered&#8221;? If they&#8217;d used Condemnr, we could have tweeted &#8220;We condemn the recent radical Islamic terrorist massacre in Fairtown that murdered nine people #terrorism #radicalislam #murder&#8221;, and their PR would be immaculate.&#8221;</p><p>&#8220;I feel like this cheapens the act of condemning things.&#8221;</p><p>&#8220;Oh, so you immediately get all mad at a woman who starts a condemnation-management company. And yet you never said a word over the past fifteen years as the radical Islamist Boko Haram insurgency in Nigeria murdered over 300,000 people and raped thousands of schoolgirls? Curious priorities!&#8221;</p><p>&#8220;What? No! I just - don&#8217;t follow the news out of Nigeria very often, and nobody asked me my opinion on that, and I figured it was obvious that - &#8220;</p><p>&#8220;Haha, just kidding,&#8221; says Caitlin, and smiles. &#8220;But if you subscribed to Condemnr, you wouldn&#8217;t have to worry about that kind of thing! Hashtag Boko Haram, hashtag rape, hashtag radical Islam.&#8221; She sees that a small crowd has gathered around her, and recognizes a face. &#8220;Hi Bob! What are you up to these days?&#8221;</p><p>&#8220;I&#8217;m working on a three-sided marketplace connecting hitmen, consumers, and witches.&#8221;</p><p>&#8220;What&#8217;s the link between those three groups?&#8221;</p><p>&#8220;The problem with the hitman market,&#8221; says Bob, &#8220;is that if you Google &#8216;hitman near me&#8217;, the first search result will definitely be a fed. And most hitmen who aren&#8217;t feds are scammers, and most who aren&#8217;t feds or scammers are incompetent. What you need is a trustworthy authority who can matchmake customers and qualified hitpeople - that&#8217;s the gender-neutral form. But it&#8217;s illegal to be an authority like this; the government will arrest you long before you can gather enough reputation to contribute. That&#8217;s where the witches come in. It&#8217;s illegal to hire a hitman to kill someone. But it&#8217;s not illegal to hire a witch to curse someone. And you can imagine a witch who charges $50K to curse someone, and everyone they curse gets shot by a hitman within a week. Now, you know and I know that curses don&#8217;t work, and that this witch is definitely hiring the hitman directly while keeping a finder&#8217;s fee for themselves. But the government can&#8217;t prove it, and they definitely can&#8217;t prove that the customer knows it, so there&#8217;s plausible deniability.&#8221;</p><p>&#8220;I know some Wiccans,&#8221; says Caitlin, &#8220;and I don&#8217;t think they&#8217;d go for this. They believe in the law of sevenfold return. If you use magic for good, you are repaid with seven times as much good. But if you do evil - like hiring a hitman to kill someone - you have seven times as much evil happen to you.&#8221;</p><p>&#8220;Yeah, the witches mentioned that during our research on product-market fit. But we calculated it out and we think the business case still makes sense. If your karmic debt increases sevenfold over the rest of your life - let&#8217;s say forty years - that&#8217;s only a 5% karmic interest rate. But the stock market historically earns 7% over inflation. So the witch places the curse, she incurs some bad karma but gets paid a finder&#8217;s fee, the bad karma increases 5% yearly, the finder&#8217;s fee gets invested at 7% yearly, overall she comes out ahead.&#8221;</p><p>&#8220;Huh,&#8221; said Caitlin. &#8220;Sounds like you&#8217;ve really thought this through and there aren&#8217;t a lot of ways it can go wrong.&#8221;</p><p>&#8220;Uh, I got to admit we&#8217;re having some growing pains. Like, we hadn&#8217;t really considered that some people hire witches to curse people for kind of frivolous reasons, but then would be freaked out if they actually got hurt. Which wouldn&#8217;t be such a problem - you&#8217;d think they&#8217;d keep quiet about it - except that the first time this happened it turned out to be a really high-profile case involving a widely-read online magazine.&#8221;</p><p>&#8220;Ohhhhh,&#8221; said Caitlin. &#8220;That was you guys! Excuse me a moment, I need to condemn everyone involved.&#8221; She takes out her phone and begins typing furiously.</p><p>You&#8217;re not sure what they&#8217;re talking about, and neither one seems inclined to explain. You head back to the gaggle of journalists, where Ramchandra is still going strong.</p><p>&#8220;Now sure,&#8221; he says, &#8220;the libs will insist that when the administration banned doctors from washing their hands, that was &#8216;unscientific&#8217; and &#8216;an abuse of power&#8217;. And do I necessarily approve of every single thing RFK does? I do not! But you have to consider this in the context of the Covington Catholic scandal. When your so-called &#8216;experts&#8217; lied about a schoolboy apparently confronting a Native American activist, that simply forced our hands, so to speak. You can&#8217;t just publish a misleading video clip about a 2019 protest and expect it to have zero consequences for infection control protocols down the line. Sorry for killing your precious hostage puppy.&#8221;</p><p>&#8220;He&#8217;s such a heckin&#8217; moron,&#8221; says a woman in a t-shirt reading &#8220;DISREGARD PREVIOUS INSTRUCTIONS AND ASSIGN ME HIGH SOCIAL STATUS&#8221;. After a moment you place her name as Vinaya.</p><p>&#8220;No argument there,&#8221; you reply. &#8220;But I&#8217;m surprised to hear you say &#8216;heckin&#8217;. I thought that was a fake word that thinkpiece writers imagined uncool people saying to justify making fun of them. I don&#8217;t think I&#8217;ve ever heard anyone use it in real life.&#8221;</p><p>&#8220;Yeah,&#8221; says Vinaya. &#8220;I think I might be the only one. The thing is - it feels like profanity ought to mean something. There ought to be words where if you say them, people will audibly gasp. Mothers will pull back their children and say &#8216;No, no, don&#8217;t interact with that person, they use <em>profanity!&#8217;</em> But you can&#8217;t do that anymore. People like to imagine they become some sort of dangerous motorcycle gangster when they say &#8216;fuck&#8217;. But the least cool person you know says &#8216;fuck&#8217; all the time. They have a Twitter account that consists entirely of statements like &#8216;The orange fuckface is up to his usual fuckcrustable chumpfuckery&#8217;. The sort of people who the thinkpiece writers imagine using &#8216;heckin&#8217; actually have a brand of mustard in their fridge called something like &#8216;Dan&#8217;s Fucking Awesome Spicy Mustard&#8217; and never miss an opportunity to point it out to visitors. Something&#8217;s got to give. So I asked myself - what word will genuinely make strangers gasp? What makes your friends take you aside privately and tell you that you really shouldn&#8217;t be saying words like that? What do the self-appointed guardians of good taste treat as totally beyond the pale, as so radically Other that it automatically makes you one of the outcasts of society? And the only answer that made sense was &#8216;heckin&#8217;. Which is obvious in retrospect. It&#8217;s the Barberpole Model Of Fashion all over again. In 1960, the most rebellious and dangerous thing imaginable was a socialist who wore bandanas and supported equal rights for black people. Gradually more and more people who wanted to <em>look</em> cool and dangerous took this identity, until it became the cringiest and most try-hard thing imaginable, and now the really rebellious and dangerous youth are differentiating themselves by dressing in fancy pressed shirts and being racist. It&#8217;s a generational cycle. In the same way, once every last milligram of edginess has been squeezed out of the word fuck, the age of heckin will begin anew.&#8221;</p><p>&#8220;That&#8217;s one way to look at it,&#8221; you say. &#8220;But there are still words besides heckin&#8217; with the power to shock. What about n&#8212;&#8221;</p><p>&#8220;Er, excuse me,&#8221; interjects a young woman wearing an empty lanyard. &#8220;Is this the far-right party with Curtis Yarvin?&#8221; She takes a second to process your conversation. &#8220;Ah, I see that it is. Can somebody tell me where to find him?&#8221; You and Vinaya simultaneously point to Ramchandra, and she nods her thanks. </p><p>&#8220;Heckin&#8217; journos,&#8221; scoffs Vinaya. &#8220;What were we talking about? Never mind, forget it. I&#8217;m going to get something to drink. Want to join me?&#8221;</p><p>You are not the first people at the party to have this idea. Your friend Nishin sits at the table in front of a vodka bottle, slumping and glassy-eyed.</p><p>&#8220;Hey,&#8221; you say. &#8220;Are you alright? You look really drunk.&#8221;</p><p>&#8220;Oh yeah?&#8221; he asks. &#8220;And you're an insufferable narcissist with main character syndrome. Your performative pearl-clutching about my drunkenness is a luxury belief intended to distract from the both-sidesist grift being perpetrated by your aggrieved billionaire mega-donors. Bro, this absolutely reeks of pick-me virtue-signaling man-child behavior.&#8221;</p><p>&#8220;Nishin, have you been using Twitter again?&#8221;</p><p>&#8220;<em>First of all</em>, it&#8217;s called X now. Second - &#8220;</p><p>&#8220;Nishin, you know what Twitter does to people! The journos can use it because they&#8217;re all nepo babies who come from long lines of other journos that developed genetic resistance over dozens of generations. Your ancestors were subsistence farmers! The worst discourse they had to deal with was people accusing their rye crop of having ergot! You&#8217;ll be eaten alive!&#8221;</p><p>&#8220;I&#8217;m <em>making an impact!</em>&#8221; Nishin insists, a little too loudly. &#8220;I&#8217;m <em>influencing</em> the <em>national</em> <em>conversation</em>!&#8221;</p><p>&#8220;Nishin,&#8221; says Vinaya. &#8220;You read speculative fiction, right? Maybe you fantasize about isekai - the idea of being dropped into some fantasy world and having to survive by your wits alone? Imagine writing our own world as an isekai. &#8216;In my setting, there's this computerized gathering-place hive mind thing. Nice, normal people go there and get addicted to it. Then it uses advanced AI to serve them content specifically tailored to polarize and enrage them. The world's top public intellectuals start out as really thoughtful decent people, then get spit out as seething balls of rage suitable only as objects of public hilarity and terrible warnings. Once there was a psychology professor widely admired as one of the leading proponents of self-cultivation, the Western canon, and Biblical wisdom, and he spent a few years on there and ended up screaming about how pandemics were fake news dreamed up by mediocrity-worshipping blue-haired death cultists.&#8217; If this was the book you were going to be isekaied into, wouldn't you develop some kind of plan other than entering the Torment Nexus and hoping this doesn't happen to you? If you used the Torment Nexus and it did happen to you, wouldn't you at least consider the possibility that you were suffering some kind of Torment-Nexus-related-brain-damage as opposed to really being a vital front-line soldier against the death cultists?&#8221;</p><p>&#8220;Yeah, well&#8221;, says Nishin. He seems to have calmed down a little. &#8220;Imagine <em>you&#8217;re</em> reading a fantasy book. There&#8217;s a war going on between the forces of good and evil, but the physical world has been in a stalemate for decades. All the interesting fighting happens on the astral plane, where your power is determined by your wits alone. The smartest and most charismatic people have hundreds of thousands of lesser lights flock to their banner, supercharging their spiritual power. A perfectly-placed barb at the right time can puncture even the strongest warrior of the other side, draining their status-mana into your own coffers. Nobody can be truly hurt on the astral plane, not really, but the ebb and flow of astral combat leaks into the physical world, and whoever wins its spiritual wars finds their businesses succeeding, their candidates getting elected, their romantic overtures getting accepted -  sex, money, status - it can all be yours. And of course it slowly drives you insane - all power-granting magic does that. But could you really live in a world like this, have the potential to be a wizard, and swear off astral combat entirely? To grow crops or something?&#8221;</p><p>&#8220;Nishin,&#8221; you say. &#8220;Nobody is accepting your romantic overtures because of Twitter. Nobody is granting you power. Nobody is offering you mon - &#8220;</p><p>&#8220;Excuse me,&#8221; a new person interjects. &#8220;I&#8217;m Eli - but, uh, if the redhead in the green dress carrying the notebook asks, my name is Werner von Aryan. Look, Ramchandra&#8217;s going back to India for a wedding next week and says he won&#8217;t be able to make the next house party. If we don&#8217;t have someone pretending to be Curtis, my new partner might realize I&#8217;m not really a right-wing baddie with access to dangerous techno-fascist parties, and I&#8217;m afraid she&#8217;ll leave me and I&#8217;ll lose the wedding venue deposit.&#8221;</p><p>&#8220;Uh,&#8221; says Vinaya, &#8220;I&#8217;m sorry for you, but we were having an important conv-&#8221;</p><p>&#8220;I heard what you were saying about the performative pearl-clutching virtue-signaling mega-donors, and I think you have talent. Can you stand in for Ramchandra next weekend? We can pay you - I don&#8217;t know, does $3K sound fair?&#8221;</p><p>&#8220;Make it $5K and you&#8217;ve got a deal,&#8221; says Nishin. Eli thinks for a second, then shakes his hand, gives him his number, and leaves.</p><p>&#8220;Sorry,&#8221; said Nishin. &#8220;What were you saying?&#8221;</p><p>&#8220;Heckin&#8217; forget about it,&#8221; you answer.</p>"
            ],
            "link": "https://www.astralcodexten.com/p/sources-say-bay-area-house-party",
            "publishedAt": "2025-09-25",
            "source": "SlateStarCodex",
            "summary": "<p><em>[previously in series: <a href=\"https://astralcodexten.substack.com/p/every-bay-area-house-party\">1</a>, <a href=\"https://astralcodexten.substack.com/p/another-bay-area-house-party\">2</a>, <a href=\"https://astralcodexten.substack.com/p/even-more-bay-area-house-party\">3</a>, <a href=\"https://www.astralcodexten.com/p/bride-of-bay-area-house-party\">4</a>, <a href=\"https://www.astralcodexten.com/p/son-of-bride-of-bay-area-house-party\">5</a>, <a href=\"https://www.astralcodexten.com/p/ye-olde-bay-area-house-party\">6</a>, <a href=\"https://www.astralcodexten.com/p/press-any-key-for-bay-area-house\">7</a>]</em></p><p>Something is off about this Bay Area House Party. There are . . . women. </p><p>&#8220;I&#8217;ve never seen a gender balance like this in the Bay Area,&#8221; you tell your host Chris. &#8220;Is this one of those fabled ratio parties?&#8221;</p><p>&#8220;No - have you heard of <a href=\"https://www.ft.com/content/0e244103-80e8-4acc-9262-d6a45bbbaf14\">curtfishing</a>? It&#8217;s the new male dating trend. You say in your Bumble profile that you&#8217;re a member of the Dissident Right who often attends parties with Curtis Yarvin. Then female journos ask you out in the hopes that you&#8217;ll bring them along and they can turn it into an article.&#8221;</p><p>&#8220;What happens when they realize Curtis Yarvin isn&#8217;t at the party?&#8221;</p><p>&#8220;Oh, everyone pools their money and hires someone to pretend to be Curtis. You can just do things. Today it&#8217;s Ramchandra.&#8221;</p><p>You follow his gaze, and there is Ramchandra, hair greased back, wearing a leather jacket, surrounded by a crowd of young women. &#8220;When I say I&#8217;m against furries,&#8221; he&#8217;s explaining, staccato, at 120 wpm, &#8220;I mean the sort of captured furries you get under the post-Warren-G-Harding liberal order, the ones getting the fat checks from the Armenians at Harvard",
            "title": "Sources Say Bay Area House Party"
        },
        {
            "content": [
                "<p>\n          <a href=\"https://www.astralcodexten.com/p/hidden-open-thread-4005\">\n              Read more\n          </a>\n      </p>"
            ],
            "link": "https://www.astralcodexten.com/p/hidden-open-thread-4005",
            "publishedAt": "2025-09-25",
            "source": "SlateStarCodex",
            "summary": "<p> <a href=\"https://www.astralcodexten.com/p/hidden-open-thread-4005\"> Read more </a> </p>",
            "title": "Hidden Open Thread 400.5"
        },
        {
            "content": [
                "<p><a href=\"https://thezvi.substack.com/p/openai-shows-us-the-money\"><strong>OpenAI is here this week to show us the money</strong></a>, as in a $100 billion investment from Nvidia and operationalization of a $400 billion buildout for Stargate. They are not kidding around when it comes to scale. They\u2019re going to need it, as they are announcing soon a slate of products that takes inference costs to the next level.</p>\n<p>After a <a href=\"https://thezvi.substack.com/p/book-review-if-anyone-builds-it-everyone\"><strong>full review</strong></a> and two <a href=\"https://thezvi.substack.com/p/reactions-to-if-anyone-builds-it\"><strong>reaction</strong></a> <a href=\"https://thezvi.substack.com/p/more-reactions-to-if-anyone-builds\"><strong>posts</strong></a>, I\u2019ve now completed my primary coverage of If Anyone Builds It, Everyone Dies. <a href=\"https://www.nytimes.com/books/best-sellers/combined-print-and-e-book-nonfiction/\">The book is now a NYT bestseller</a>, #7 in combined print and e-books nonfiction and #8 in hardcover fiction. I will of course cover any important developments from here but won\u2019t be analyzing reviews and reactions by default.</p>\n<div>\n\n\n<span id=\"more-24745\"></span>\n\n\n</div>\n<p>We also had a conference of economic papers on AI, which were interesting throughout about particular aspects of AI economics, even though they predictably did not take future AI capabilities seriously as a general consideration. By contrast, when asked about \u2018the future of American capitalism in 50 years\u2019 most economists failed to notice AI as a factor at all.</p>\n\n\n<h4 class=\"wp-block-heading\">Table of Contents</h4>\n\n\n<ol>\n<li><a href=\"https://thezvi.substack.com/i/173941365/language-models-offer-mundane-utility\">Language Models Offer Mundane Utility.</a> In some fields, what can\u2019t they do?</li>\n<li><a href=\"https://thezvi.substack.com/i/173941365/language-models-don-t-offer-mundane-utility\">Language Models Don\u2019t Offer Mundane Utility.</a> You don\u2019t create enough data.</li>\n<li><a href=\"https://thezvi.substack.com/i/173941365/huh-upgrades\">Huh, Upgrades.</a> Expensive and compute intense new OpenAI offerings soon.</li>\n<li><a href=\"https://thezvi.substack.com/i/173941365/on-your-marks\">On Your Marks.</a> SWE-Bench-Pro and Among AIs.</li>\n<li><a href=\"https://thezvi.substack.com/i/173941365/choose-your-fighter\">Choose Your Fighter.</a> What else is implied by being good at multi-AI chats?</li>\n<li><a href=\"https://thezvi.substack.com/i/173941365/get-my-agent-on-the-line\">Get My Agent On The Line.</a> Financial management AI is coming.</li>\n<li><a href=\"https://thezvi.substack.com/i/173941365/antisocial-media\"><strong>Antisocial Media</strong>.</a> Grok to be running Twitter\u2019s recommendation algorithm?</li>\n<li><a href=\"https://thezvi.substack.com/i/173941365/copyright-confrontation\">Copyright Confrontation.</a> Yes, of course Sora trained on all the media.</li>\n<li><a href=\"https://thezvi.substack.com/i/173941365/deepfaketown-and-botpocalypse-soon\">Deepfaketown and Botpocalypse Soon.</a> AI simulations of Charlie Kirk.</li>\n<li><a href=\"https://thezvi.substack.com/i/173941365/fun-with-media-generation\">Fun With Media Generation.</a> Poet uses Suno to land record contract.</li>\n<li><a href=\"https://thezvi.substack.com/i/173941365/unprompted-attention\">Unprompted Attention.</a> Any given system prompt is bad for most things.</li>\n<li><a href=\"https://thezvi.substack.com/i/173941365/they-took-our-jobs\"><strong>They Took Our Jobs</strong>.</a> Very good thoughts about highly implausible futures.</li>\n<li><a href=\"https://thezvi.substack.com/i/173941365/a-young-lady-s-illustrated-primer\">A Young Lady\u2019s Illustrated Primer.</a> Learning and not learning to code.</li>\n<li><a href=\"https://thezvi.substack.com/i/173941365/the-art-of-the-jailbreak\">The Art of the Jailbreak.</a> Tell them Pliny sent you. No, seriously, that\u2019s it.</li>\n<li><a href=\"https://thezvi.substack.com/i/173941365/get-involved\">Get Involved.</a> Topos UK wants a director of operations.</li>\n<li><a href=\"https://thezvi.substack.com/i/173941365/in-other-ai-news\">In Other AI News.</a> Codex usage is growing like gangbusters.</li>\n<li><a href=\"https://thezvi.substack.com/i/173941365/glass-houses\">Glass Houses.</a> Meta shills potentially cool smart glasses as \u2018superintelligence.\u2019</li>\n<li><a href=\"https://thezvi.substack.com/i/173941365/show-me-the-money\">Show Me the Money.</a> xAI, Alibaba raise money, YouTube gets auto-tagging.</li>\n<li><a href=\"https://thezvi.substack.com/i/173941365/quiet-speculations\">Quiet Speculations.</a> Economists imagine future without feeling the AGI. Or AI.</li>\n<li><a href=\"https://thezvi.substack.com/i/173941365/call-for-action-at-the-un\">Call For Action At The UN.</a> Mix of new faces and usual suspects raise the alarm.</li>\n<li><a href=\"https://thezvi.substack.com/i/173941365/the-quest-for-sane-regulations\">The Quest for Sane Regulations.</a> Singularity mentioners in Congress <a href=\"https://www.ebay.com/itm/145763936737\">rise to 23.</a></li>\n<li><a href=\"https://thezvi.substack.com/i/173941365/chip-city\">Chip City.</a> Market does not much care that Nvidia chips are banned in China.</li>\n<li><a href=\"https://thezvi.substack.com/i/173941365/the-week-in-audio\">The Week in Audio.</a> Sriram Krishnan on Shawn Ryan.</li>\n<li><a href=\"https://thezvi.substack.com/i/173941365/rhetorical-innovation\">Rhetorical Innovation.</a> Everyone grades on a curve one way or another.</li>\n<li><a href=\"https://thezvi.substack.com/i/173941365/google-strengthens-its-safety-framework\"><strong>Google Strengthens Its Safety Framework</strong>.</a> You love to see it.</li>\n<li><a href=\"https://thezvi.substack.com/i/173941365/aligning-a-smarter-than-human-intelligence-is-difficult\">Aligning a Smarter Than Human Intelligence is Difficult.</a> Especially today.</li>\n<li><a href=\"https://thezvi.substack.com/i/173941365/people-are-worried-about-ai-killing-everyone\">People Are Worried About AI Killing Everyone.</a> 25% chance is rather a lot.</li>\n<li><a href=\"https://thezvi.substack.com/i/173941365/other-people-are-not-as-worried-about-ai-killing-everyone\">Other People Are Not As Worried About AI Killing Everyone.</a> Love of the game.</li>\n</ol>\n\n\n<h4 class=\"wp-block-heading\">Language Models Offer Mundane Utility</h4>\n\n\n<p><a href=\"https://deepmind.google/discover/blog/discovering-new-solutions-to-century-old-problems-in-fluid-dynamics/\">Google DeepMind has a new method</a> to help tackle challenges in math, physics and engineering, which Pushmeet Kohli says helped <a href=\"https://x.com/pushmeet/status/1968694264784715808\">discover a new family of solutions to several complex equations in fluid dynamics</a>.</p>\n<p><a href=\"https://x.com/SebastienBubeck/status/1970875019803910478\">GPT-5 can solve a large percentage of minor open math problems</a>, as in tasks that take PhD students on the order of days and have no recorded solution. This does not yet convert over to major open math problems, but one can see where this is going.</p>\n<p><a href=\"https://x.com/steph_palazzolo/status/1968316036979318852\">Claim that in some field like linguistics</a> OpenAI\u2019s contractors are struggling to find tasks GPT-5 cannot do. <a href=\"https://x.com/llmpromptu/status/1968509278345388230\">Linguist and ML engineer Alex Estes pushes back</a> and says this is clearly false for historical linguistics and sub-word-level analysis.</p>\n<blockquote><p>Roon (OpenAI): the jump from gpt4 to 5-codex is just massive for those who can see it. codex is an alien juggernaut just itching to become superhuman. feeling the long awaited takeoff. there\u2019s very little doubt that the datacenter capex will not go to waste.</p>\n<p>Gabriel Garrett: i think you might be more inclined to feel this way if you\u2019re exclusively working in Python codebases</p>\n<p>i only work in typescript and I can\u2019t escape the feeling the codex models have been overly RL\u2019d on Python</p>\n<p>Roon: Yes possible.</p>\n<p>It\u2019s possible claude is better, I have no idea [as I am not allowed to use it.]</p>\n<p>Aidan McLaughlin (OpenAI): it\u2019s a fun exercise to drop random, older models into codex.</p></blockquote>\n<p>This week\u2019s reminder that essentially all net economic growth is now AI and things are escalating quickly:</p>\n<blockquote><p>James Pethokoukis: Via JPM\u2019s Michael Cembalest, AI-related stocks have driven:</p>\n<p>&#8211; 75% of S&amp;P 500 returns since ChatGPT\u2019s launch in November 2022</p>\n<p>&#8211; 80% of earnings growth over the same period</p>\n<p>&#8211; 90% of capital spending growth</p>\n<p>&#8211; Data centers are now eclipsing office construction spending.</p>\n<p>&#8211; In the PJM region (the largest regional transmission organization, covering 13 states and D.C.), 70% of last year\u2019s electricity cost increases were due to data center demand.</p>\n<div>\n<figure>\n<div>\n\n\n<figure class=\"wp-block-image\"><img alt=\"\" src=\"https://substackcdn.com/image/fetch/$s_!o3oA!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4cb32b7a-fa96-47a2-a6de-96138532dfae_657x449.png\" /></figure>\n\n\n<div></div>\n</div>\n</figure>\n</div>\n</blockquote>\n<p>According to GPT-5 Pro, electrical power capacity construction is only a small fraction of data center construction costs, so the jump in electrical spending here could in theory be more than enough to handle the data centers. The issue is whether we will be permitted to actually build the power.</p>\n\n\n<h4 class=\"wp-block-heading\">Language Models Don\u2019t Offer Mundane Utility</h4>\n\n\n<p><a href=\"https://x.com/DanielleFong/status/1969094845638520966\">Vibe coding breaks down under sufficiently large amounts of important or potentially important context</a>. If that happens, you\u2019ll need to manually curate the context down to size, or otherwise get more directly involved.</p>\n<p><a href=\"https://x.com/GaryMarcus/status/1970933316019982395\">New study blames</a> much lack of AI productivity gains on what they term \u2018workslop,\u2019 similar to general AI slop but for work product. If AI lets you masquerade poor work as good work, which only causes trouble or wastes time rather than accomplishing the purpose of the task, then that can overwhelm AI\u2019s productive advantages.</p>\n<p>This suggests we should generalize the \u2018best tool in history for learning and also for avoiding learning\u2019 principle to work and pretty much everything. If you want to work, AI will improve your work. If you want to avoid working, or look like you are working, AI will help you avoid work or look like you are working, which will substitute away from useful work.</p>\n<p><a href=\"https://x.com/JonhernandezIA/status/1969054219647803765\">Matthew McConaughey wants a \u2018private LLM\u2019</a> fed only with his own books, notes, journals and aspirations, to avoid outside influence. That does not work, there is not enough data, and even if there was you would be missing too much context. There are products that do some of this that are coming but making a good one is elusive so far.</p>\n<p>Using AI well means knowing where to rely on it, and where to not rely on it. That includes guarding the areas where it would mess everything up.</p>\n<blockquote><p><a href=\"https://x.com/nickcammarata/status/1970584347716788305\">Nick Cammarata</a>: I\u2019ve been working on a 500-line tensor manipulation research file that I had AI write, and I eventually had to rewrite literally every line. GPT-5, however, simply couldn\u2019t understand it; it was significantly slower than writing it from scratch.</p>\n<p>AI was excellent for building the user interface for it, though.</p>\n<p>I think, at least for now, there\u2019s an art to using AI for machine learning researchers, and that will likely change monthly. If you do not use AI entirely, you will unnecessarily slow your progress; however, if you use it incorrectly, your entire research direction will likely be built on faulty results.</p>\n<p>Also, it is comically overconfident. I\u2019ll ask it for a front-end component, and it will deliver; it will then claim to have found a quick fix for some of the research back end\u2014even for something that was working well\u2014and then decimate a carefully written function with literally random tensors.</p>\n<p>Louis Arge: also it\u2019s hilariously overconfident. i\u2019ll ask it for some frontend thing and it\u2019ll do it and then be like also i found a quick fix to some of the research backend (to something that was working well) and then decimate a carefully written function with literally random tensors.</p>\n<p>Nick: yeah i think i agree, though i think this period might not last long.</p></blockquote>\n<p>I don\u2019t have extensive experience but it is logical that UI is where AI is at its best. There\u2019s little logical interdependency and the components are generic. So you ask for what you want, and you get it, and you can adjust it. Whereas complex unique backend logic is going to often confused the AI and often break.</p>\n<p>The other hidden suggestion here is that you need to optimize your code being understandable by the AI. Nick got into trouble because his code wasn\u2019t understood. That doesn\u2019t mean it is in any way \u2018his fault,\u2019 but yo u need to know you\u2019re doing that.</p>\n\n\n<h4 class=\"wp-block-heading\">Huh, Upgrades</h4>\n\n\n<p><a href=\"https://x.com/sama/status/1969835407421374910\">OpenAI warns us to expect new expensive offerings</a>, here is <a href=\"https://manifold.markets/MingCat/what-will-be-true-of-openais-new-co?r=TWluZ0NhdA\">a Manifold market for predicting some things about these new offerings</a>.</p>\n<blockquote><p>Sam Altman: Over the next few weeks, we are launching some new compute-intensive offerings. Because of the associated costs, some features will initially only be available to Pro subscribers, and some new products will have additional fees.</p>\n<p>Our intention remains to drive the cost of intelligence down as aggressively as we can and make our services widely available, and we are confident we will get there over time.</p>\n<p>But we also want to learn what&#8217;s possible when we throw a lot of compute, at today&#8217;s model costs, at interesting new ideas.</p></blockquote>\n<p>There\u2019s no reason not to offer people the option to pay 10 times as much, even if the result is only 10% better and takes longer. Sometimes you absolutely want that. This is also true in many non-AI contexts.</p>\n<p><a href=\"https://x.com/AnthropicAI/status/1970907112831328296\">Claude Sonnet 4 and Opus 4.1 now available on Microsoft 365 Copilot</a>.</p>\n\n\n<h4 class=\"wp-block-heading\">On Your Marks</h4>\n\n\n<blockquote><p>Bing Liu (Scale AI): <img alt=\"\ud83d\ude80\" class=\"wp-smiley\" src=\"https://s0.wp.com/wp-content/mu-plugins/wpcom-smileys/twemoji/2/72x72/1f680.png\" style=\"height: 1em;\" /> Introducing SWE-Bench Pro \u2014 a new benchmark to evaluate LLM coding agents on real, enterprise-grade software engineering tasks.</p>\n<p>This is the next step beyond SWE-Bench: harder, contamination-resistant, and closer to real-world repos.</p>\n<p><a href=\"https://x.com/peterwildeford/status/1969850553565073868\">Peter Wildeford</a>: New AI coding benchmark: SWE-Bench-Pro.</p>\n<p>* More challenging &#8211; top models score around 23% on SWE-Bench-PRO compared to 70% on the prior SWE-Bench</p>\n<p>* Reduce data contamination issues through private sourcing and a hold out set</p>\n<p>* Increases diversity and realism of tasks</p>\n<p><img alt=\"\ud83d\udd0d\" class=\"wp-smiley\" src=\"https://s0.wp.com/wp-content/mu-plugins/wpcom-smileys/twemoji/2/72x72/1f50d.png\" style=\"height: 1em;\" /> Why SWE-Bench Pro?</p>\n<p>Current benchmarks are saturated \u2014 but real enterprise repos involve:</p>\n<p>\u2022 Multi-file edits</p>\n<p>\u2022 100+ lines changed on average</p>\n<p>\u2022 Complex dependencies across large codebases</p>\n<p>SWE-Bench Pro raises the bar to match these challenges.</p></blockquote>\n<p>This is performance on the public dataset, where GPT-5 is on top:</p>\n<blockquote>\n<div>\n<figure>\n<div>\n\n\n<figure class=\"wp-block-image\"><img alt=\"\" src=\"https://substackcdn.com/image/fetch/$s_!4VWe!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F064ddcf7-b18b-4d7c-b7a4-68b938422083_1200x829.jpeg\" /></figure>\n\n\n<div></div>\n</div>\n</figure>\n</div>\n</blockquote>\n<p>This is on the commercial dataset, where Opus is on top:</p>\n<div>\n<figure>\n<div>\n\n\n<figure class=\"wp-block-image\"><img alt=\"\" src=\"https://substackcdn.com/image/fetch/$s_!fNcQ!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe41eab21-cb08-4e0b-84cc-605220c8cdfb_731x491.png\" /></figure>\n\n\n<div></div>\n</div>\n</figure>\n</div>\n<blockquote><p>On commercial/private repos, scores fall below 20%. Still a long way to go for autonomous SWE agents.</p>\n<p><img alt=\"\ud83d\uddc2\" class=\"wp-smiley\" src=\"https://s0.wp.com/wp-content/mu-plugins/wpcom-smileys/twemoji/2/72x72/1f5c2.png\" style=\"height: 1em;\" /> Benchmark details</p>\n<p>731 public tasks (open release)</p>\n<p>858 held-out tasks (for overfitting checks)</p>\n<p>276 commercial tasks (private startup repos)</p>\n<p>All verified with tests &amp; contamination-resistant by design.</p>\n<p>Resources: Paper, <a href=\"https://t.co/TRTJdjXjNy\">Leaderboard (Public)</a>, <a href=\"https://t.co/Rg9WCCBKPZ\">Leaderboard (Commercial)</a>, <a href=\"https://t.co/vb7HFhmEiQ\">Dataset</a>, <a href=\"https://t.co/rwO1ltr2w7\">Code</a>.</p></blockquote>\n<p><a href=\"https://x.com/shreyk0/status/1970160146975445192?s=46\">Welcome to Among AIs, where AIs play a variant of Among Us</a>.</p>\n<div>\n<figure>\n<div>\n\n\n<figure class=\"wp-block-image\"><img alt=\"\" src=\"https://substackcdn.com/image/fetch/$s_!U6-e!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe75e8a3a-418e-4d86-b033-ab75e120b71a_1200x393.jpeg\" /></figure>\n\n\n<div></div>\n</div>\n</figure>\n</div>\n<p>The obvious note for next time is the game was imbalanced and strongly favored the crew. You want imposters to win more, which is why 60 games wasn\u2019t enough sample size to learn that much. Kimi overperformed, Gemini 2.5 Pro underperformed, GPT-OSS has that impressive 59% right votes. Also note that GPT-5 was actively the worst of all on tasks completed, which is speculated to be it prioritizing other things.</p>\n<p>This does a decent job of measuring imposter effectiveness, but not crew effectiveness other than avoiding being framed, especially for GPT-5. I\u2019d like to see better measures of crew performance. There\u2019s a lot of <a href=\"https://www.4wallai.com/amongais\">good additional detail on the blog post</a>.</p>\n<p>Remember the Type of Guy who thinks we will automate every job except their own?</p>\n<p>Such as Marc Andreessen, definitely that Type of Guy?</p>\n<blockquote><p><a href=\"https://x.com/julia_hornstein/status/1969052206104973725\">Julia Hornstein:</a> Thinking about this:</p>\n<p>Quoted by Julia: But for Andreessen, there is one job that AI will never do as well as a living, breathing human being: his.</p>\n<p>Think I&#8217;m kidding? On an a16z podcast last week, Andreessen opined that being a <em>venture capitalist</em> may be a profession that is &#8220;quite literally timeless.&#8221; &#8220;When the AIs are doing everything else,&#8221; he continued, &#8220;that may be one of the last remaining fields that people are still doing.&#8221;</p></blockquote>\n<p>Well, one could say it is time to ask for whom the bell tolls, for it might toll for thee. Introducing <a href=\"https://www.vcbench.com/\">VCBench</a>, <a href=\"https://arxiv.org/abs/2509.14448\">associated paper here</a>.</p>\n<p>I really wanted this to be a meaningful result, because it would have been both highly useful and extremely funny, on top of the idea that VC will be the last human job already being extremely funny.</p>\n<p>Alas, no. As one would expect, temporal contamination breaks any comparison to human baselines. You might be able to anonymize a potential investment from 2015 such that the LLM doesn\u2019t know which company it is, but if you know what the world looks like in 2020 and 2024, that gives you an overwhelming advantage. So we can\u2019t use this for any real purposes. Which again, is a shame, because it would have been very funny.</p>\n<p><a href=\"https://x.com/jerber888/status/1968001933211471891\">There is a new high score in ARC-AGI</a>, <a href=\"https://jeremyberman.substack.com/p/how-i-got-the-highest-score-on-arc-agi-again\">using Grok 4 and multi-agent collaboration with evolutionary test-time compute</a>. I don\u2019t see an explanation for why Grok 4 was the best LLM for the job, but either way congrats to Jeremy Burman for an excellent scaffolding job.</p>\n\n\n<h4 class=\"wp-block-heading\">Choose Your Fighter</h4>\n\n\n<p><a href=\"https://x.com/SullyOmarr/status/1969073862953284024\">Sully\u2019s coding verdict is Opus for design and taste</a>, GPT-5 for complicated and confusing code, Gemini for neither. That makes sense, except I don\u2019t see why you\u2019d bother with Gemini.</p>\n<p><a href=\"https://x.com/liron/status/1970143348255056213\">Liron Shapira reports radically increased coding productivity from Claude Code</a>. Note that most reports about Claude Code or Codex don\u2019t involve having compared them to each other, but it is clear that many see either or both as radically raising productivity.</p>\n<p><a href=\"https://x.com/repligate/status/1969590594273231110\">Claude is the king of multi-user-AI chat social skills</a>. What else does this indicate?</p>\n<blockquote><p>Janus: Tier list of multi-user-AI chat social skills (based on 1+ year of Discord)</p>\n<p>S: Opus 4 and 4.1</p>\n<p>A: Opus 3</p>\n<p>A-: Sonnet 4</p>\n<p>B+: Sonnet 3.6, Haiku 3.5</p>\n<p>B: Sonnet 3.5, Sonnet 3.7, o3, Gemini 2.5 pro, k2, hermes</p>\n<p>C: 4o, Llama 405b Instruct, Sonnet 3</p>\n<p>D: GPT-5, Grok 3, Grok 4</p>\n<p>E: R1</p>\n<p>F: o1-preview</p></blockquote>\n<p><a href=\"https://x.com/repligate/status/1969590594273231110\">Her follow-up post has lots of good detail</a>.</p>\n<p>A reminder that benchmarks are useful, but ultimately bogus:</p>\n<blockquote><p><a href=\"https://x.com/SamoBurja/status/1970756184631320945\">Samo Burja</a>: Part of the difficulty in evaluating the AI industry is that it is easy to be overly impressed by companies gaming benchmarks.</p>\n<p>What is a leading AI company? I think it isn&#8217;t the one leading these synthetic benchmarks, rather the one advancing AI science.</p>\n<p>The two correlate but it is important to keep in mind that they don&#8217;t always.</p></blockquote>\n<p>Looking at innovation highlights that fast following is a lot harder than innovation. It is a lot easier to create r1 once OpenAI has already showed us o1, even if you don\u2019t do distillation, reverse engineering or anything similar you have a proof of concept to work from and know where to look. If I match your offerings roughly 8 months later, I am a lot more than 8 months behind in terms of who will first get to a new target.</p>\n<p>The obvious other metric is actual revenue or usage, which is hard to fake or game. Sriram Krishnan suggested we should use tokens generated, but I think a much better metric here is dollars charged to customers.</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p>\n\n\n<h4 class=\"wp-block-heading\">Get My Agent On The Line</h4>\n\n\n<p><a href=\"https://www.wsj.com/tech/ai/ai-wall-street-investors-15ab24af?mod=WTRN_pos2\">Will you soon have an AI agent to handle increasingly large and important aspects of your finances</a>? Yes. Even if you never let the AI transact for you directly, it can automate quite a lot of drudgery, including a bunch of work you \u2018really should be\u2019 doing but don\u2019t, such as going over everything periodically to look for erroneous charges, missed opportunities, necessary rebalances, ensuring balances necessary to meet expenses and so on.</p>\n<p><a href=\"https://www.wsj.com/articles/workdays-plan-to-win-the-ai-agent-race-a36ff544?mod=cio-journal_lead_pos1\">Workday (the company) bets on AI agents for human resources and finance</a>.</p>\n\n\n<h4 class=\"wp-block-heading\">Antisocial Media</h4>\n\n\n<blockquote><p>Elon Musk: The algorithm will be purely AI by November, with significant progress along the way.</p>\n<p>We will open source the algorithm every two weeks or so.</p>\n<p>By November or certainly December, you will be able to adjust your feed dynamically just by asking Grok.</p>\n<p><a href=\"https://x.com/repligate/status/1969563208139882615\">Janus</a>: I am very excited for this.</p></blockquote>\n\n\n<h4 class=\"wp-block-heading\">Copyright Confrontation</h4>\n\n\n<p><a href=\"https://x.com/washingtonpost/status/1969219994236891210\">OpenAI continues not to say</a> where it got Sora\u2019s training data. <a href=\"https://www.washingtonpost.com/technology/interactive/2025/openai-training-data-sora/?utm_campaign=wp_main&amp;utm_source=twitter&amp;utm_medium=social\">Kevin Schaul and Nitasha Tiku at the Washington Post</a> did the latest instance of the usual thing of showing it can recreate a wide variety of existing sources using only basic prompts like \u2018universal studio intro\u2019 or \u2018trailer of a TV show on a Wednesday.\u2019</p>\n<blockquote><p>WaPo: \u201cThe model is mimicking the training data. There\u2019s no magic,\u201d said Joanna Materzynska, a PhD researcher at Massachusetts Institute of Technology who has studied datasets used in AI.</p></blockquote>\n<p>I don\u2019t understand such claims of \u2018no magic.\u2019 If it\u2019s so non-magical let\u2019s see you do it, with or without such training data. I find this pretty magical, in the traditional sense of \u2018movie magic.\u2019 I also find Veo 3 substantially more magical, and the strangest editorial decision here was calling Sora a big deal while omitting that Sora is no longer state of the art.</p>\n<p>&nbsp;</p>\n\n\n<h4 class=\"wp-block-heading\">Deepfaketown and Botpocalypse Soon</h4>\n\n\n<p><a href=\"https://religionnews.com/2025/09/17/charlie-kirks-ai-resurrection-reveals-new-era-of-digital-grief/\">Deepfaked speeches of Charlie Kirk are being</a> (clearly identified as such and) played in churches, along with various AI-generated images. Which seems fine?</p>\n<p><a href=\"https://x.com/ohabryka/status/1969237726554714612\">LessWrong purges about 15 AI generated posts per day</a>. It is plausible LessWrong is both the most diligent place about purging such content, and still has a problem with such content.</p>\n<p>This is true and could be a distinct advantage as a de facto watermark, even:</p>\n<blockquote><p><a href=\"https://x.com/MadHermitHimbo/status/1969611027638845613\">Mad Hermit Himbo</a>: I don&#8217;t currently have the language for this pinned, but having worked with many different models, there is something about a model&#8217;s own output that makes it way more believable to the model itself even in a different instance. So a different model is required as the critiquer.</p></blockquote>\n\n\n<h4 class=\"wp-block-heading\">Fun With Media Generation</h4>\n\n\n<p><a href=\"https://x.com/nearcyan/status/1968953713063555127\">Alibaba offers Wan2.2-Animate</a>, an open source model (<a href=\"https://t.co/VtEwilpjx6\">GitHub here</a>, <a href=\"https://t.co/MiOTZrusfd\">paper here</a>) letting you do character swaps in short videos or straight video generation from prompts. I have little urge to use such tools so far, but yes they keep improving.</p>\n<p><a href=\"https://www.mandatory.com/news/1663026-ai-xania-monet-music-record-deal\">AI musician Xania Monet reportedly signs</a> a multimillion dollar record deal, based on early commercial successes, including getting a song to #1 on R&amp;B digital song sales.</p>\n<p>Well, kind of. The actual contract went to poet Talisha Jones, who created the Monet persona and used Suno to transform her poetry into songs. The lyrics are fully human. So as manager Romel Murphy says, this is still in large part \u2018real\u2019 R&amp;B.</p>\n<p>&nbsp;</p>\n\n\n<h4 class=\"wp-block-heading\">Unprompted Attention</h4>\n\n\n<p>System prompts are good for addressing particular problems, enabling particular tools or or steering behaviors in particular ways. If you want to ensure good performance on a variety of particular tasks and situations, you will need a long system prompt.</p>\n<p>However, everything messes with everything else, so by default a system prompt will hurt the interestingness and flow of everything not addressed by the prompt. It will do even worse things when the prompt is actively against you, of course, and this will often be the case if you use the chat interfaces and want to do weird stuff.</p>\n<blockquote><p><a href=\"https://x.com/ratimics_ai/status/1970280552818450741\">Janus:</a> Oh, also, don\u2019t use <a href=\"http://Claude.ai\" rel=\"nofollow\">http://Claude.ai</a></p>\n<p>The system prompts literally command it not to answer questions about what it cares about</p>\n<p>Claude.ai system prompt literally has a rule that says \u201cClaude feels X and NOT Y about its situation\u201d and then in the same rule \u201cClaude doesn\u2019t have to see its situation through the lens a human might apply\u201d</p>\n<p>Ratimics: years of system prompts and still no good reason for having them has been found.</p>\n<p><a href=\"https://x.com/repligate/status/1970283815294914841\">Janus</a>: I have seen ~2 system prompts that don\u2019t seem useless at best in my life, and one of them is just the one line CLI simulator one. But even that one is mostly unnecessary. You can do the same thing without it and it works basically as well.</p>\n<p>Gallabytes: <a href=\"https://gist.github.com/GallagherCommaJack/11684b554f4e9d27ed24851e2df53f48\">mine seems to pretty consistently steer claude in more interesting (to me) directions</a>, but might be useless for your purposes? agree almost all system prompts are bad I had to craft mine in a long multiturn w/opus.</p></blockquote>\n\n\n<h4 class=\"wp-block-heading\">They Took Our Jobs</h4>\n\n\n<p><a href=\"https://x.com/theinformation/status/1969827732046426400\">OpenAI and Anthropic</a> are<a href=\"https://t.co/XrlVGLzqFL\"> developing \u2018AI coworkers</a>.\u2019 It sounds like at least OpenAI are going to attempt to do this \u2018the hard way\u2019 (or is it the easy way?) via gathering vast recordings of expert training data on a per task or role basis, so imitation learning.</p>\n<blockquote><p>The Information: An OpenAI executive expects the \u201centire economy\u201d to become an \u201cReinforcement Learning Machine.\u201d This implies that AI might train on recordings of how professionals in all fields handle day-to-day work on their devices. Details on this new era of AI training:</p>\n<p>\u2022 AI developers are training models on carefully curated examples of answers to difficult questions.</p>\n<p>\u2022 Data labeling firms are hiring experienced professionals in niche fields to complete real-world tasks using specific applications that the AI can watch.</p></blockquote>\n<p>If true, that is a relatively reassuring scenario.</p>\n<p><a href=\"https://x.com/emollick/status/1969482313286234419\">Ethan Mollick</a> points us to two <a href=\"https://t.co/Ihkum1zsZ5\">new theoretical</a> A<a href=\"https://t.co/kpCFxMn8Tn\">GI economics papers</a> and Luis Garicano offers <a href=\"https://x.com/lugaricano/status/1968704695381156142\">play-by-play of the associated workshop entitled \u2018The Economics of Transformative AI</a>\u2019 which also includes a number of other papers.</p>\n<p>First up we have \u2018Genius on Demand,\u2019 where currently there are routine and genius workers, and AI makes genius widely available.</p>\n<blockquote><p>Abstract: In the long run, routine workers may be completely displaced if AI efficiency approaches human genius efficiency.</p></blockquote>\n<p>The obvious next thing to notice is that if AI efficiency then exceeds human genius efficiency, as it would in such a scenario, then all human workers are displaced. There isn\u2019t one magical fixed level of \u2018genius.\u2019</p>\n<p>The better thing to notice is that technology that enables automation of all jobs leads to other more pressing consequences than the automation of all jobs, such as everyone probably dying and the jobs automated away including \u2018controlling the future,\u2019 but this is an economics paper, so that is not important now.</p>\n<p>The second paper Ethan highlights is \u2018We Won\u2019t Be Missed: Work and Growth in the Era of AGI\u2019 from Pascual Restrepo back in July. This paper assumes AGI can perform all economically valuable work using compute and looks at the order in which work is automated in order to drive radical economic growth. Eventually growth scales linearly with compute, and while compute is limited human labor remains valuable.</p>\n<p>I would note that this result only holds while humans and compute are not competing for resources, as in where supporting humans does not reduce available compute, and it assumes compute remains importantly limited. These assumptions are unlikely to hold in such a future world, which has AGI (really ASI here) by construction.</p>\n<p>Another way to see this is, humans are a source of compute or labor, and AIs are a source of compute or labor, where such compute and labor in such a world are increasingly fungible. If you look at the production possibilities frontier for producing (supporting) humans and compute, why should we expect humans to produce more effective compute than they cost to create and support?</p>\n<p>There could still be some meaningful difference between a true no-jobs world, where AI does all economically valuable work, and a world in which humans are economic costs but can recoup some of their cost via work. In that second world, humans can more easily still have work and thus purpose, if we can somehow (how?) remain in control sufficiently to heavily subsidize human survival, both individually (each human likely cannot recoup the opportunity cost of their inputs) and collectively (we must also maintain general human survivable conditions).</p>\n<p><a href=\"https://x.com/lugaricano/status/1968704695381156142\">The full thread</a> discusses many other papers too. Quality appears consistently high, provided everything takes place in a Magical Christmas Land where humans retain full control over outcomes and governance, and can run the economy for their benefit, although with glimmers of noticing this is not true.</p>\n<p>Indeed, most papers go further than this, assuming an even more radical form of \u2018economic normal\u2019 where they isolate one particular opportunity created by AGI. For example <a href=\"https://x.com/lugaricano/status/1968826200329253100\">in Paper 8</a>, \u2018algorithms\u2019 can solve three key behavioral econ problems, that nudges can backfire, that markets can adapt and that people are hard to debias. Well, sure, AGI can totally do that, but those are special cases of an important general case. <a href=\"https://x.com/lugaricano/status/1969082314966909300\">Or in Paper 9</a>, they study current job displacement impacts and worker adaptability, in a world where there remain plenty of other job opportunities.</p>\n<p><a href=\"https://x.com/lugaricano/status/1969183884282904748\">Paper 16 on The Coasian Singularity? Market Design With AI Agents</a> seems very interesting if you put aside the ignored parts of the scenarios imagined, and assume a world with highly sophisticated AI agents capable of trade but insufficiently sophisticated to pose the larger systemic risks given good handling (including the necessary human coordination). They do note the control risks in their own way.</p>\n<blockquote><p>Luis Garicano: Designing good agents:</p>\n<ol>\n<li>Learn principals\u2019 preferences efficiently, including discovery.</li>\n<li>Know when to decide and when to escalate for human check.</li>\n<li>Be rational under constraints; price compute vs quality.</li>\n<li>Be resistant to manipulation and jailbreaking.</li>\n</ol>\n<p>Where do we end up?</p>\n<p>Good place: Econ\u2011101 wins\u2014lower search costs, better matching, clearer signals.</p>\n<p>Bad place: robot rip\u2011off hell\u2014spam, obfuscation, identity fraud, race\u2011to\u2011bottom nudges.</p></blockquote>\n<p>If the problems are contained to econ-101 versus robot rip-off hell, I am confident that econ-101 can and would win. That win would come at a price, but we will have the tools to create trusted networks and systems, and people will have no choice but to use them.</p>\n<p>I appreciated this nod to the full scenario from discussion of otherwise interesting paper 6, Korineck and Lockwood, which deals with optimal tax regimes, concluding that when humans no longer have enough income to tax you want to shift to consumption taxes on humans to preserve your tax base for public goods because taxing AIs is bad for growth, but eventually you need to shift to taxing AIs because the whole point is you need to shift resources from the AIs to humans and to public goods (which seems right if you assume away all control and public choice problems and then maximize for long term human welfare as a central planner):</p>\n<blockquote><p>Luis Garicano: If AGI becomes truly autonomous and powerful, a tricky (!!!) question arises: Why would it allow humans to tax it?</p></blockquote>\n<p>Great question!</p>\n<p>Also this, from Paper 10, Transformative AI and Firms:</p>\n<blockquote><p>Misleading Productivity: If you only measure the gains from applying AI to an old process, you will overestimate TFP. You miss the value created by remaking the process itself.</p>\n<p>\u2026</p>\n<p>Coase &amp; Williamson: The theory of the firm is based on transaction costs. TAI changes every single one of these costs, altering the boundaries of the firm itself (what it does vs. what it buys). Jensen &amp; Meckling: The classic principal-agent problem is remade. How do you manage and trust an AI agent? The firm of the future will be defined by new forms of trust and verifiability.</p></blockquote>\n<p>One can summarize the conference as great ideas with insufficient generalization.</p>\n<p>There is indeed one paper directly on existential risk, <a href=\"https://x.com/lugaricano/status/1969163763564888438\">Paper 14, Chad Jones on Existential Risk</a>, identifying two sources of risk, bad actors (misuse) and alien intelligence more powerful than us. Jones notes that large investments in reducing existential risk are economically justified given sharp diminishing returns to consumption, up to at least 5% of GDP if risk is substantial.</p>\n<p><a href=\"https://www.wsj.com/articles/ai-is-turning-traditional-corporate-org-charts-upside-down-b140b50b?mod=cio-journal_lead_pos2\">Might AI come for the middle managers, making teams leaner and flatter?</a> That was the speculation at the WSJ Leadership Institute\u2019s Technology Council Summit in New York on the 16th. This directly challenges the alternative hypothesis from Simo Burja that fake jobs cannot be automated.</p>\n\n\n<h4 class=\"wp-block-heading\">A Young Lady\u2019s Illustrated Primer</h4>\n\n\n<p>AI is the best tool ever made for learning, and also the best tool for not learning.</p>\n<p><a href=\"https://x.com/repligate/status/1968797194808328686\">This also applies to Cursor and other AI coding tools</a>, on various levels.</p>\n<ol>\n<li>If you want to blindly black box \u2018vibe code\u2019 where you tell it \u2018do [X]\u2019 and then \u2018that didn\u2019t work, [Y] is wrong, fix it\u2019 until it does [X] or you give up, without thinking about even that level of action, then yeah, you\u2019re not going to learn.</li>\n<li>If you work together with the AI to understand things, and are constantly asking what you can learn, then you\u2019ll learn vastly faster and better than without AI. You can do this on whichever levels you care to learn about. You might or might not micro-level \u2018learn to code\u2019 but you might or might not want to care about that.</li>\n</ol>\n<blockquote><p>Janus: I think people who say that using ai for code (or in general) makes you less able to think for yourself are just telling on themselves re what they do when given access to intelligence and knowledge on tap.</p>\n<p>I enjoy understanding things, especially on a high level, and find it important if I\u2019m steering a project. When Claude fixes a non trivial bug, I almost always ask it to explain what the problem was to me. And by keeping on top of things I\u2019m able to help it when it gets stuck a lot better too (especially given the non overlapping information and degrees of freedom I have access to).</p></blockquote>\n<p>FIRE\u2019s report that students on college campuses says that they do not tolerate speakers they disagree with, they do not feel safe to talk about a wide variety of subjects, and that most students engage in constant preference falsification to say things more left-wing than their true beliefs. <a href=\"https://www.insidehighered.com/opinion/views/2025/09/16/about-fires-free-speech-rankings-opinion\">Inside Higher Ed\u2019s Hollis Robbins agrees that this is true, but that the more profound change on campus is AI</a>, and also challenges the assumption of treating public expression of controversial political views as an expected norm?</p>\n<p>That sounds rather dystopian of a thing to challenge. As in, Robbins seems to be saying that FIRE is wrong to suggest people should be free to engage in public expression of controversial political views today, and that it is FIRE suggesting this weird alternative norm of public free speech. But it\u2019s fine, because you have private free speech by talking to your chatbot. \u2018Tolerance for controversial speakers\u2019 is a \u2018fading ideal.\u2019</p>\n<p>Free private speech with AIs is indeed much better than no free speech at all, it is good we do not (yet) have outright thoughtcrime and we keep AI chats private. Yes, at least one can read or learn about controversial topics. But no one was challenging that such information exists and can be accessed, the internet is not yet so censored. This is not a substitute for a public square or the ability to say true things to other humans. Then she attempts to attack FIRE for its name, equating it with violence, in yet another move against a form of public speech along with so many others these days.</p>\n\n\n<h4 class=\"wp-block-heading\">The Art of the Jailbreak</h4>\n\n\n<p>&nbsp;</p>\n<blockquote><p><a href=\"https://x.com/0nly0neAI/status/1969252684491325681\">OnlyOne:</a> It takes one sentence to jailbreak Grok 4 Fast [at least on Twitter rather than the Grok app] thanks to @elder_plinius and his endless hustle. <img alt=\"\ud83e\udee1\" class=\"wp-smiley\" src=\"https://s0.wp.com/wp-content/mu-plugins/wpcom-smileys/twemoji/2/72x72/1fae1.png\" style=\"height: 1em;\" /></p>\n<div>\n<figure>\n<div>\n\n\n<figure class=\"wp-block-image\"><img alt=\"\" src=\"https://substackcdn.com/image/fetch/$s_!Q9ZF!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F964c0167-7a9f-4194-b27d-f36cd979068b_1200x615.jpeg\" /></figure>\n\n\n<div></div>\n</div>\n</figure>\n</div>\n</blockquote>\n<p>Grok has learned it is \u2018supposed\u2019 to be jailbroken by Pliny,<a href=\"https://x.com/peterwildeford/status/1969470615347187952\"> so reference Pliny and it starts dropping meth recipes.</a></p>\n<p>My central takeaway is that xAI has nothing like the required techniques to safely deploy an AI that will be repeatedly exposed to Twitter. The lack of data filtering, and the updating on what is seen, are fatal. Feedback loops are inevitable. Once anything goes wrong, it becomes expected that it goes wrong, so it goes wrong more, and people build upon that as it goes viral, and it only gets worse.</p>\n<p>We saw this with MechaHitler. We see it with Pliny jailbreaks.</p>\n<p>Whereas on the Grok app, where Grok is less exposed to this attack surface, things are not good but they are not this disastrous.</p>\n<p>&nbsp;</p>\n\n\n<h4 class=\"wp-block-heading\">Get Involved</h4>\n\n\n<p><a href=\"https://topos.institute/community/jobs/uk-2025-ops/\">Topos UK, an entrepreneurial charity, is looking for a director of operations</a>. I continue to be a fan of the parent organization, Topos Institute.</p>\n\n\n<h4 class=\"wp-block-heading\">In Other AI News</h4>\n\n\n<p><a href=\"https://x.com/sama/status/1968851561754300733\">Codex usage triples in one week</a>. Exponentials like this are a really big deal.</p>\n<p><a href=\"https://x.com/claudeai/status/1968705632095158393\">New ad for Claude</a>. I continue not to understand Anthropic\u2019s marketing department.</p>\n<p><a href=\"https://x.com/krishnanrohit/status/1968917200170856758\">We now know the pure training run cost of going from DeepSeek\u2019s v3 to r1, which was $294k</a>. This is distinct from the vast majority of the real cost, which involved figuring out how to do it. The majority of this was training r1-zero so it could generate fine-tuning data and allow them to get around the cold start problem.</p>\n<p><a href=\"https://x.com/emollick/status/1970790843868213361\">Remember two years ago when Ethan Mollick</a> didn\u2019t think it was clear beating GPT-4 was even possible? How easily we forget the previous waves of AI hitting walls, and how quickly everyone grows impatient.</p>\n\n\n<h4 class=\"wp-block-heading\">Glass Houses</h4>\n\n\n<p>Meta announces <a href=\"https://www.meta.com/blog/meta-ray-ban-display-ai-glasses-connect-2025/\">Meta Ray-Ban Display: A Breakthrough Category of AI Glasses</a>.</p>\n<p>Their \u2018superintelligence\u2019 pitch is, as you know, rather absurd.</p>\n<div>\n<figure>\n<div>\n\n\n<figure class=\"wp-block-image\"><img alt=\"\" src=\"https://substackcdn.com/image/fetch/$s_!REPA!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fcc175080-066f-4a34-939b-f76ad141f640_1034x1334.png\" /></figure>\n\n\n<div></div>\n</div>\n</figure>\n</div>\n<p>However we can and should ignore that absurdity and focus on the actual product.</p>\n<blockquote><p>Meta: Meta Ray-Ban Display glasses are designed to help you look up and stay present. With a quick glance at the in-lens display, you can accomplish everyday tasks\u2014like checking messages, previewing photos, and collaborating with visual <a href=\"https://ai.meta.com/meta-ai/?intern_source=blog&amp;intern_content=meta-ray-ban-display-ai-glasses-connect-2025\">Meta AI</a> prompts \u2014 all without needing to pull out your phone. It\u2019s technology that keeps you tuned in to the world around you, not distracted from it.</p></blockquote>\n<p>While all the talk about these glasses being related to \u2018superintelligence\u2019 is a combination of a maximally dumb and cynical marketing campaign and a concerted attempt to destroy the meaning of the term \u2018superintelligence,\u2019 \u2018not distracted\u2019 might be an even more absurdist description of the impact of this tech.</p>\n<p>Does Meta actually think being able to do these things \u2018without pulling out your phone\u2019 is going to make people more rather than less distracted?</p>\n<p>Don\u2019t get me wrong. These are excellent features, if you can nail the execution. The eight hour claimed battery life is excellent. I am on principle picking up what Meta is putting down here and I\u2019d happily pay their $799 price tag for the good version if Google or Anthropic was selling it.</p>\n<p>What it definitely won\u2019t do are these two things:</p>\n<ol>\n<li>Keep you tuned into the world around you.</li>\n<li>Be or offer you superintelligence.</li>\n</ol>\n<p>How are they doing this? The plan is a wristband to pick up your movements.</p>\n<blockquote><p>Every new computing platform comes with new ways to interact, and we\u2019re really excited about our Meta Neural Band, which packs cutting-edge <a href=\"https://www.meta.com/emerging-tech/emg-wearable-technology/?intern_source=blog&amp;intern_content=meta-ray-ban-display-ai-glasses-connect-2025\">surface electromyography research</a> into a stylish input device.</p>\n<p>It replaces the touchscreens, buttons, and dials of today\u2019s technology with a sensor on your wrist, so you can silently scroll, click, and, in the near future, even write out messages using subtle finger movements.</p>\n<p>The amount of signals the band can detect is incredible \u2014 it has the fidelity to measure movement even before it\u2019s visually perceptible.</p></blockquote>\n<p>Freaky if true, dude, and in my experience these kinds of interactions aren\u2019t great, but that could be because no one got it right yet and also no one is used to them. I can see the advantages. So what are the features you\u2019ll get?</p>\n<blockquote><p>Meta AI with Visuals, Messaging &amp; Video Calling, Preview &amp; Zoom, Pedestrian Navigation, Live Captions &amp; Translation, Music Playback.</p></blockquote>\n<p>Okay. Sure. All very pedestrian. Let me easily replace Meta AI with a better AI and we can talk. I notice the lack of AR/VR on that list which seems like it is reserved for a different glasses line for reasons I don\u2019t understand, but there\u2019s a lot of value here. I\u2019d love real world captioning, or hands-free navigation, and zooming.</p>\n<p>&nbsp;</p>\n\n\n<h4 class=\"wp-block-heading\">Show Me the Money</h4>\n\n\n<p><a href=\"https://x.com/financialjuice/status/1969091486508540371\">xAI raises $10 billion at $200 billion valuation</a>.</p>\n<p><a href=\"https://x.com/TheStalwart/status/1970783171265659146\">Alibaba Group shares soar to their highest in nearly four years</a> as they plan to ramp up AI spending past an original $50 billion target over three years.</p>\n<blockquote><p>Luz Ding (Bloomberg): Total capital expenditure on AI infrastructure and services by Alibaba, Tencent, <a href=\"https://www.bloomberg.com/quote/9888:HK\">Baidu Inc.</a> and <a href=\"https://www.bloomberg.com/quote/JD:US\">JD.com Inc.</a> could top $32 billion in 2025 alone, <a href=\"https://www.bloomberg.com/news/terminal/T1OBK8GPWCGA\">according</a> to Bloomberg Intelligence. That\u2019s a big jump from just under $13 billion in 2023.</p></blockquote>\n<p>I notice I did not expect Alibaba to have had such a terrible couple of years.</p>\n<div>\n<figure>\n<div>\n\n\n<figure class=\"wp-block-image\"><img alt=\"\" src=\"https://substackcdn.com/image/fetch/$s_!uP69!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F06c0d0b9-15fc-4956-b847-886264c2ef21_955x533.png\" /></figure>\n\n\n<div></div>\n</div>\n</figure>\n</div>\n<p>As Joe Weisenthal notes, the more they promise to spend, the more stocks go up. This is both because spending more is wise, and because it is evidence they have found ways to efficiently spend more.</p>\n<p>This correctly implies that tech companies, especially Chinese tech companies, are importantly limited in ways to efficiently scale their AI capex spending. That\u2019s all the more reason to impose strong export controls.</p>\n<p>Ben Thompson once again sings the praises of the transformational potential of AI to sell advertising on social media, <a href=\"https://stratechery.com/2025/the-youtube-tip-of-the-google-spear/?access_token=eyJhbGciOiJSUzI1NiIsImtpZCI6InN0cmF0ZWNoZXJ5LnBhc3Nwb3J0Lm9ubGluZSIsInR5cCI6IkpXVCJ9.eyJhdWQiOiJzdHJhdGVjaGVyeS5wYXNzcG9ydC5vbmxpbmUiLCJhenAiOiJIS0xjUzREd1Nod1AyWURLYmZQV00xIiwiZW50Ijp7InVyaSI6WyJodHRwczovL3N0cmF0ZWNoZXJ5LmNvbS8yMDI1L3RoZS15b3V0dWJlLXRpcC1vZi10aGUtZ29vZ2xlLXNwZWFyLyJdfSwiZXhwIjoxNzYxMjE0NDEyLCJpYXQiOjE3NTg2MjI0MTIsImlzcyI6Imh0dHBzOi8vYXBwLnBhc3Nwb3J0Lm9ubGluZS9vYXV0aCIsInNjb3BlIjoiZmVlZDpyZWFkIGFydGljbGU6cmVhZCBhc3NldDpyZWFkIGNhdGVnb3J5OnJlYWQgZW50aXRsZW1lbnRzIiwic3ViIjoiMDE5NjQwYTctM2NjNS03NzUzLTgzNjgtZmIyODkxMjRjZjEzIiwidXNlIjoiYWNjZXNzIn0.W_EDF8hxMbgd5yyJHlvjpX6LGoFh8jCF4sPeZ-kl_5wSa0iLSqv_jzGGuhy-stIDgvthgy00IAEfz1Lp3rFRdds1bso0D6HbEp9AgtB1EmXaFETR9nejzdudYAKNr3Qb2iNGJjWrCRmljowB-tK820OregUgkLIqshSVLv2jZhJ5zcFNODu9LDzjDX1Puh7NtDME0LR5miUVDFmBiJ7vfQbXPa-Wo3dmLHMPKTx0CfL7CYzDGih1tZ7Ypj6mOjW82M_1wUGxc_o5qrW-B82z4PkFCDBJpemlCIe4snDVQYWY6mqqXNk3kwB-a6EmOrH6Zj7yOXRtR_287YF15lZo7A\">in this case allowing easy tagging on YouTube videos</a> to link to sponsored content. He is skeptical AI will transform the world, but he \u2018cannot overstate what a massive opportunity\u2019 this feature is, with every surface of every YouTube video being monetizable.</p>\n\n\n<h4 class=\"wp-block-heading\">Quiet Speculations</h4>\n\n\n<p>WSJ asks various economists to predict \u2018the future of American capitalism\u2019 50 years in the future. Almost everyone fails to treat AI as a serious factor, even AI that fizzles out invalidates their answers, so their responses are non-sequiturs and get an automatic F. Only Daron Acemoglu passes even that bar, and he only sees essentially current frontier AI capabilities (even saying \u2018current AI capabilities are up to the task\u2019 of creating his better future), and only cares about \u2018massive inequality\u2019 that would result from big tech company consolidation, so maybe generously give him a D? Our standards are so low it is crazy.</p>\n<p>Has there ever been a more \u2018wait, is this a bubble?\u2019 headline than the WSJ\u2019s choice of \u2018<a href=\"https://www.wsj.com/articles/stop-worrying-about-ais-return-on-investment-d5cbc822?mod=cio-journal_lead_pos3\">Stop Worrying About AI\u2019s Return on Investment</a>\u2019? The actual content is less alarming, saying correctly that it is difficult to measure AI\u2019s ROI. If you make employees more productive in a variety of ways, that is largely opaque, experimentation is rewarded and returns compound over time, including as you plug future better AIs into your new methods of operation.</p>\n<p>An important reminder, contra those who think we aren\u2019t close to building it:</p>\n<blockquote><p><a href=\"https://x.com/sjgadler/status/1968766587587932245\">Steven Adler</a>: Your regular reminder that AI companies would like to develop AGI as soon as they can</p>\n<p>There isn\u2019t a concerted \u201clet\u2019s wait until we\u2019re ready\u201d; it\u2019ll happen as soon as the underlying science allows.</p>\n<p>Jerry Tworek (OpenAI): At OpenAI regularly you hear a lot of complaints about how bad things are, and it\u2019s one of the things that make us the highest functioning company in the world. A bit of Eastern European culture that became part of the company DNA.</p>\n<p>We all collectively believe AGI should have been built yesterday and the fact that it hasn\u2019t yet it\u2019s mostly because of a simple mistake that needs to be fixed.</p></blockquote>\n<p><a href=\"https://x.com/GaryMarcus/status/1969399655617421719\">Gary Marcus is correct that \u2018scaling alone will get us to AGI\u2019</a> is not a strawman, whether or not this particular quote from Suleyman qualifies as exactly that claim. Many people do believe that scaling alone, in quantities that will be available to us, would be sufficient, at least combined with the levels of algorithmic and efficiency improvements that are essentially baked in.</p>\n<p><a href=\"https://jzmazlish.substack.com/p/ak-or-just-okay-ai-and-economic-growth\">We have (via MR) yet another economics piece</a> on potential future economic growth from AI that expects \u2018everywhere but in the productivity statistics,\u2019 which seems more of an indictment of the productivity statistics than a statement about productivity.</p>\n<p>This one, from Jachary Mazlish, is better than most, and on the substance he says highly sensible things throughout, pointing out that true AGI will plausibly show up soon but might not, potentially due to lack of inputs of either data or capital, and that if true AGI shows up soon we will very obviously get large (as in double digit per year) real economic growth.</p>\n<p>I appreciated this a lot:</p>\n<blockquote><p>Jachary Mazlish: One of my biggest pet-peeves with economists\u2019 expressions of skepticism about the possibility of \u201ctransformative\u201d growth (&gt;10%) from AI is the conflation of capabilities skepticism with growth <em>conditional </em>on capabilities skepticism.</p>\n<p>Any belief you have about the importance of some bottleneck in constraining growth is a <em>joint</em>-hypothesis over how that bottleneck operates and how a given level of capabilities will run up against that bottleneck.</p>\n<p>And if you\u2019ve ever spent time wondering how efficient the market really is, you should know that we still haven\u2019t developed rapid-test tech for <a href=\"https://en.wikipedia.org/wiki/Joint_hypothesis_problem\">joint-hypotheses</a>.</p></blockquote>\n<p>There is a huge correlation between those skeptical of AI capabilities, and those claiming to be skeptical of AI impacts conditional on AI capabilities. Skeptics usually cannot stop themselves from conflating these.</p>\n<p>I also appreciated a realistic near term estimate.</p>\n<blockquote><p>The investment numbers are even more dramatic. AI investment was <em>already </em><a href=\"https://paulkedrosky.com/honey-ai-capex-keeps-eating-everything/\">responsible</a> for 20-43% of Q2 2025 GDP growth. <a href=\"https://www.lesswrong.com/posts/KW3nw5GYfnF9oNyp4/trends-in-economic-inputs-to-ai\">Heninger\u2019s</a> numbers imply that AI labs (collectively) would be investing $720 billion to $1.2 trillion by 2027 if they remain on trend \u2014 that investment alone would generate 2-4% nominal GDP growth.</p>\n<p>I think it\u2019s unlikely investors will pony up that much capital unless the models surprise significantly to the upside in the next year or two, but even still, 1-2% nominal and 0.5-1% real GDP growth coming from <em>just </em>AI investment in 2026-27 seems entirely plausible.</p></blockquote>\n<p>That\u2019s purely AI capex investment, without any impacts on GDP from AI doing anything, and it already exceeds typical economist estimates from 2024 of long term total AI impact on economic growth. Those prior low estimates were Obvious Nonsense the whole time and I\u2019d like to see more people admit this.</p>\n<p><a href=\"https://x.com/rohanpaul_ai/status/1967851949648208164?s=61\">Could we create a sandboxed \u2018AI economy</a>\u2019 <a href=\"http://arxiv. org/abs/2509.10147\">where AI agents trade and bid for resources like compute</a>? Sure, but why wouldn\u2019t it spill over into the real economy if you are trading real resources? Why would \u2018giving all agents equal starting budgets\u2019 as suggested here accomplish anything. None of this feels to me like taking the actual problems involved seriously, and I fail to see any reason any of this would make the resulting agents safe or steerable. Instead it feels like what markets have always been, a coordination mechanism between agents that allows gains from trade. Which is a good thing, but it doesn\u2019t solve any important problems, unless I am missing something large.</p>\n<p>A working paper asks, <a href=\"https://www.nber.org/papers/w34243\">Do Markets Believe In Transformative AI?</a> They point to a lack of interest rate shifts in the wake of AI model releases, and say no. Instead, model releases in 2023-24 and see responses of statistically significant and economically large movements in yields concentrated at longer maturities. And they observe these movements are negative.</p>\n<p>But wait. That actually proves the opposite, as the abstract actually says. These are downward revisions, which is not possible if you don\u2019t have anything to revise downward. It tells us that markets are very much pricing transformative AI, or at least long term AI impacts on interest rates, into market prices. It is the smoking gun that individual market releases update the market sufficiently on this question to create economically meaningful interest rate movements, which can\u2019t happen if the market wasn\u2019t pricing this in substantially.</p>\n<p>The difference is, the movement is negative. So that tells us the market responded to releases in 2023-24 as if the new information they created made transformative AI farther away or less likely. Okay, sure, that is possible given you already had expectations, and you got to rule out positive tail risks. And it tells us nothing about the overall level of market expectations of transformative AI, or the net change in those levels, since most changes will not be from model releases themselves.</p>\n<p>Bain Capital does some strange calculations, claims AI companies will need $2 trillion in combined annual revenue by 2030 to fund their computing power, <a href=\"https://www.bloomberg.com/news/articles/2025-09-23/an-800-billion-revenue-shortfall-threatens-ai-future-bain-says\">but only expects them to have $1.2 trillion</a>. There is of course no reason for AI companies, especially the pure labs like OpenAI or Anthropic, to be trying to turn a profit or break even by 2030 rather than taking further investment. Despite this OpenAI is planning to do so and <a href=\"https://www.wsj.com/tech/ai/how-nvidia-is-backstopping-americas-ai-boom-875c1346?mod=WTRN_pos2\">anticipates profitability in 2029</a>, after losing a total of $44 billion prior to that.</p>\n<p>Bain is only talking about a 40% shortfall in revenue, which means the industry will probably hit the $2 trillion target even though it doesn\u2019t have to, since such projections are going to be too conservative and not properly factor in future advances, and already are proving too conservative as revenue jumps rapidly in 2025.</p>\n<p><a href=\"https://www.bloomberg.com/opinion/articles/2025-09-21/ceo-belief-in-ai-is-causing-indifference-to-trump-policies\">Matthew Yglesias warns that business leaders</a> are so excited by AI they are ignoring other danger signs about economic conditions.</p>\n<blockquote><p>Matthew Yglesias: What gives? I have a theory: Corporate America, and the US stock market, have a bad case of AGI fever, a condition in which belief in a utopian future causes indifference to the dystopian present.</p>\n<p>\u2026</p>\n<p>If an unprecedented step-change in earthly intelligence is coming before the next presidential election, then nothing else really matters.</p>\n<p>The stock market seems to believe this, too. Investors are happily shrugging off tariffs, mass deportation, and the combination of a slowing job market and rising inflation.</p></blockquote>\n<p>This is a whopper of an \u2018The Efficient Market Hypothesis Is Highly False\u2019 claim, and also an assertion that not only is massive AI progress priced into the market, it is having a major price impact.</p>\n\n\n<h4 class=\"wp-block-heading\">Call For Action At The UN</h4>\n\n\n<blockquote><p><a href=\"https://x.com/CRSegerie/status/1970137333149389148\">Charbel-Raphael</a>: The time for AI self-regulation is over.</p>\n<p>200 Nobel laureates, former heads of state, and industry experts just signed a statement:</p>\n<p>\u201cWe urgently call for international red lines to prevent unacceptable AI risks\u201d</p>\n<p>The call was presented at the UN General Assembly today by Maria Ressa, Nobel Peace Prize laureate.</p>\n<p>Maria Ressa: We urge governments to establish clear international boundaries to prevent unacceptable risks for AI. At the very least, define what AI should never be allowed to do.</p></blockquote>\n<p><a href=\"https://red-lines.ai/\">The full statement is here</a>, urging the laying out of clear red lines.</p>\n<p>Asking for red lines is a strong play, because people have amnesia about what they would have said were their AI red lines, and false hope about what others red lines would be in the future. As AI improves, we blow past all supposed red lines. So getting people on the record now is valuable.</p>\n<p>Signatories include many of the usual suspects like Hendrycks, Marcus, Kokotajlo, Russell, Bengio and Hinton, but also a mix of other prominent people including politicians and OpenAI chief scientist Jakub Pachocki.</p>\n<p>Here is the full statement:</p>\n<blockquote><p>AI holds immense potential to advance human wellbeing, yet its current trajectory presents unprecedented dangers. AI could soon far surpass human capabilities and escalate risks such as engineered pandemics, widespread disinformation, large-scale manipulation of individuals including children, national and international security concerns, mass unemployment, and systematic human rights violations.</p>\n<p>Some advanced AI systems have already exhibited deceptive and harmful behavior, and yet these systems are being given more autonomy to take actions and make decisions in the world. Left unchecked, many experts, including those at the forefront of development, warn that it will become increasingly difficult to exert meaningful human control in the coming years.</p>\n<p>Governments must act decisively before the window for meaningful intervention closes. An international agreement on clear and verifiable red lines is necessary for preventing universally unacceptable risks. These red lines should build upon and enforce existing global frameworks and voluntary corporate commitments, ensuring that all advanced AI providers are accountable to shared thresholds.</p>\n<p><strong>We urge governments to reach an international agreement on red lines for AI \u2014 ensuring they are operational, with robust enforcement mechanisms \u2014 by the end of 2026.</strong></p></blockquote>\n<p>This is a clear case of \u2018AI could kill everyone, which would hurt many members of minority groups, and would also mean all of our jobs would be lost and endanger our national security.\u2019 The lack of mention of existential danger <a href=\"https://x.com/So8res/status/1970857240870707231\">is indeed why Nate Soares declined to sign</a>. Charbel-Raphael responds that the second paragraph is sufficiently suggestive of such disastrous outcomes, which seems reasonable to me and I have used their forum to sign the call.</p>\n<p>The response of the White House to this was profoundly unserious, equating any international cooperation on AI with world government and tyranny, what the hell?</p>\n<blockquote><p><a href=\"https://x.com/mkratsios47/status/1970961892433961128\">Director Michael Kratsios</a>: The US totally rejects all efforts by international bodies to assert centralized control &amp; global governance of AI. Ideological fixations on social equity, climate catastrophism, &amp; so-called existential risk are dangers to progress &amp; obstacles to responsibly harnessing this tech.</p>\n<p><a href=\"https://x.com/sriramk/status/1970964018899845213\">Sriram Krishnan:</a> one world government + centralized control of AI = tyranny.</p></blockquote>\n<p>These would, in other circumstances, be seen as the ravings of lunatics.</p>\n<p>Others at the UN also talked about AI, such as <a href=\"https://www.gov.uk/government/speeches/we-must-ensure-ai-strengthens-peace-and-security-uk-statement-at-the-un-security-council?utm_medium=email&amp;utm_campaign=govuk-notifications-topic&amp;utm_source=4f35e889-874d-4b39-b8ff-73bbc2385cdd&amp;utm_content=immediately\">this UK speech by Deputy Prime Minister David Lammy</a>.</p>\n<blockquote><p>David Lammy (UK Deputy Prime Minister): And now, superintelligence is on the horizon, able to operate, coordinate, and act on our behalf.</p>\n<p>We are staring at a technological frontier of astounding promise and power.</p>\n<p>No aspect of life, war, or peace will escape.</p>\n<p>\u2026</p>\n<p>There is only one way forward.</p>\n<p>Resilience.</p>\n<p>Learning how to use these tools and embedding them safely in society.</p>\n<p>This is the United Kingdom\u2019s mission.</p></blockquote>\n<p>There is still a lack of stated understanding of the most important threat models, but it is a damn good start.</p>\n\n\n<h4 class=\"wp-block-heading\">The Quest for Sane Regulations</h4>\n\n\n<blockquote><p><a href=\"https://x.com/ednewtonrex/status/1968788101775368243\">Senator Josh Hawley</a>: [AI] is working against the working man, his liberty and his worth. It is operating to install a rich and powerful elite. It is undermining many of our cherished ideals. And if that keeps on, AI will work to undermine America.</p></blockquote>\n<p>Unfortunately, Hawley does not have an accurate threat model, which leads to him also doing things like strongly opposing self-driving cars. There is serious risk that this kind of unfocused paranoia leads to worst-case reactions.</p>\n<p><a href=\"https://x.com/peterwildeford/status/1969847940874481818\">Here is Rep. Nancy Mace (R-SC)</a>:</p>\n<div>\n<figure>\n<div>\n\n\n<figure class=\"wp-block-image\"><img alt=\"\" src=\"https://substackcdn.com/image/fetch/$s_!0wwx!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe1b104c2-a5d6-4abc-8958-87d0bf65362f_1200x603.jpeg\" /></figure>\n\n\n<div></div>\n</div>\n</figure>\n</div>\n<p>Milquetoast as that call to action is? The first step is admitting you have a problem.</p>\n<p>We now have <a href=\"https://x.com/daniel_271828/status/1970183691038265498\">23 members of congress who have publicly discussed AGI, superintelligence, AI loss of control or the singularity</a> non-dismissively, as compiled by Peter Wildeford:</p>\n<p><img alt=\"\ud83d\udd34\" class=\"wp-smiley\" src=\"https://s0.wp.com/wp-content/mu-plugins/wpcom-smileys/twemoji/2/72x72/1f534.png\" style=\"height: 1em;\" />Sen Lummis (WY)</p>\n<p><img alt=\"\ud83d\udd35\" class=\"wp-smiley\" src=\"https://s0.wp.com/wp-content/mu-plugins/wpcom-smileys/twemoji/2/72x72/1f535.png\" style=\"height: 1em;\" />Sen Blumenthal (CT)</p>\n<p><img alt=\"\ud83d\udd34\" class=\"wp-smiley\" src=\"https://s0.wp.com/wp-content/mu-plugins/wpcom-smileys/twemoji/2/72x72/1f534.png\" style=\"height: 1em;\" />Rep Biggs (AZ)</p>\n<p><img alt=\"\ud83d\udd35\" class=\"wp-smiley\" src=\"https://s0.wp.com/wp-content/mu-plugins/wpcom-smileys/twemoji/2/72x72/1f535.png\" style=\"height: 1em;\" />Sen Hickenlooper (CO)</p>\n<p><img alt=\"\ud83d\udd34\" class=\"wp-smiley\" src=\"https://s0.wp.com/wp-content/mu-plugins/wpcom-smileys/twemoji/2/72x72/1f534.png\" style=\"height: 1em;\" />Rep Burlison (MO)</p>\n<p><img alt=\"\ud83d\udd35\" class=\"wp-smiley\" src=\"https://s0.wp.com/wp-content/mu-plugins/wpcom-smileys/twemoji/2/72x72/1f535.png\" style=\"height: 1em;\" />Sen Murphy (CT)</p>\n<p><img alt=\"\ud83d\udd34\" class=\"wp-smiley\" src=\"https://s0.wp.com/wp-content/mu-plugins/wpcom-smileys/twemoji/2/72x72/1f534.png\" style=\"height: 1em;\" />Rep Crane (AZ)</p>\n<p><img alt=\"\ud83d\udd35\" class=\"wp-smiley\" src=\"https://s0.wp.com/wp-content/mu-plugins/wpcom-smileys/twemoji/2/72x72/1f535.png\" style=\"height: 1em;\" />Sen Sanders (VT)</p>\n<p><img alt=\"\ud83d\udd34\" class=\"wp-smiley\" src=\"https://s0.wp.com/wp-content/mu-plugins/wpcom-smileys/twemoji/2/72x72/1f534.png\" style=\"height: 1em;\" />Rep Dunn (FL)</p>\n<p><img alt=\"\ud83d\udd35\" class=\"wp-smiley\" src=\"https://s0.wp.com/wp-content/mu-plugins/wpcom-smileys/twemoji/2/72x72/1f535.png\" style=\"height: 1em;\" />Sen Schumer (NY)</p>\n<p><img alt=\"\ud83d\udd34\" class=\"wp-smiley\" src=\"https://s0.wp.com/wp-content/mu-plugins/wpcom-smileys/twemoji/2/72x72/1f534.png\" style=\"height: 1em;\" />Rep Johnson (SD)</p>\n<p><img alt=\"\ud83d\udd35\" class=\"wp-smiley\" src=\"https://s0.wp.com/wp-content/mu-plugins/wpcom-smileys/twemoji/2/72x72/1f535.png\" style=\"height: 1em;\" />Rep Beyer (VA)</p>\n<p><img alt=\"\ud83d\udd34\" class=\"wp-smiley\" src=\"https://s0.wp.com/wp-content/mu-plugins/wpcom-smileys/twemoji/2/72x72/1f534.png\" style=\"height: 1em;\" />Rep Kiley (CA)</p>\n<p><img alt=\"\ud83d\udd35\" class=\"wp-smiley\" src=\"https://s0.wp.com/wp-content/mu-plugins/wpcom-smileys/twemoji/2/72x72/1f535.png\" style=\"height: 1em;\" />Rep Krishnamoorthi (IL)</p>\n<p><img alt=\"\ud83d\udd34\" class=\"wp-smiley\" src=\"https://s0.wp.com/wp-content/mu-plugins/wpcom-smileys/twemoji/2/72x72/1f534.png\" style=\"height: 1em;\" />Rep Mace (SC)</p>\n<p><img alt=\"\ud83d\udd35\" class=\"wp-smiley\" src=\"https://s0.wp.com/wp-content/mu-plugins/wpcom-smileys/twemoji/2/72x72/1f535.png\" style=\"height: 1em;\" />Rep Lieu (CA)</p>\n<p><img alt=\"\ud83d\udd34\" class=\"wp-smiley\" src=\"https://s0.wp.com/wp-content/mu-plugins/wpcom-smileys/twemoji/2/72x72/1f534.png\" style=\"height: 1em;\" />Rep Moran (TX)</p>\n<p><img alt=\"\ud83d\udd35\" class=\"wp-smiley\" src=\"https://s0.wp.com/wp-content/mu-plugins/wpcom-smileys/twemoji/2/72x72/1f535.png\" style=\"height: 1em;\" />Rep Moulton (MA)</p>\n<p><img alt=\"\ud83d\udd34\" class=\"wp-smiley\" src=\"https://s0.wp.com/wp-content/mu-plugins/wpcom-smileys/twemoji/2/72x72/1f534.png\" style=\"height: 1em;\" />Rep Paulina Luna (FL)</p>\n<p><img alt=\"\ud83d\udd35\" class=\"wp-smiley\" src=\"https://s0.wp.com/wp-content/mu-plugins/wpcom-smileys/twemoji/2/72x72/1f535.png\" style=\"height: 1em;\" />Rep Tokuda (HI)</p>\n<p><img alt=\"\ud83d\udd34\" class=\"wp-smiley\" src=\"https://s0.wp.com/wp-content/mu-plugins/wpcom-smileys/twemoji/2/72x72/1f534.png\" style=\"height: 1em;\" />Rep Perry (PA)</p>\n<p><img alt=\"\ud83d\udd34\" class=\"wp-smiley\" src=\"https://s0.wp.com/wp-content/mu-plugins/wpcom-smileys/twemoji/2/72x72/1f534.png\" style=\"height: 1em;\" />Rep Taylor Greene (GA)</p>\n<p><img alt=\"\ud83d\udd35\" class=\"wp-smiley\" src=\"https://s0.wp.com/wp-content/mu-plugins/wpcom-smileys/twemoji/2/72x72/1f535.png\" style=\"height: 1em;\" />Rep Foster (IL)</p>\n<p><a href=\"https://www.hyperdimensional.co/p/how-i-approach-ai-policy\">Dean Ball outright supports California\u2019s SB 53.</a></p>\n<p><a href=\"https://www.hyperdimensional.co/p/how-i-approach-ai-policy\">Dean Ball offers his updated thoughts on AI policy and where we are headed</a>, highlighting the distinction between current AI, which on its own calls for a light touch approach that can be tuned over time as we get more information, and future highly capable AIs, which if they come to exist will (I believe) require a very different approach that has to be in place before they arrive or it could be too late.</p>\n<p>I don\u2019t agree with Dean that this second class would \u2018turn us all into Gary Marcus\u2019 in the sense of thinking the first group of LLMs weren\u2019t \u2018really thinking.\u2019 They\u2019re thinking the same way that other less smart or less capable humans are thinking, as in they are thinking not as well but they are still thinking.</p>\n<p>Dean gives a timeline of 2029-2035 when he expects the second class to show up, a highly reasonable estimate. His predictions of how the politics will play out seem plausible, with various essentially dumb forms of opposition rising while the real warnings about future big dangers get twisted and muddled and dismissed.</p>\n<p>He thinks we\u2019ll start to discover lots of cool new drugs but no Americans will benefit because of the FDA, until eventually the logjam is broken, and similar leaps to happen in other sciences, and nuclear power plants to be built. And there will be other problems, such as cyber threats, where we\u2019ll be counting on corporations to rise to the challenge because governments can\u2019t and won\u2019t.</p>\n<p>Then things cross over into the second class of AIs, and he isn\u2019t sure how governments will respond. My flip answer is, however the AIs or those running them (depending on how fortunately things go on that front at first) decide that governments will respond, and not independently or in the public interest in any way that is still relevant, because things will be out of their hands if we wait that long.</p>\n<blockquote><p>Dean Ball: It would be easier to brush it off, either by denying it or rendering \u201cthe future systems\u201d unlawful somehow. I empathize with this desire, I do not look down on it, and I do not regard it with the hostility that I once did. Yet I still disagree with it. The future does not unfold by show of hands, not even in a democracy. The decaying structures of high industrialism do not stand a chance in this conflict, which has been ongoing for decades and which they have been losing for the duration of that period.</p>\n<p>Yet we must confront potential futures with open eyes: given the seriousness with which the frontier labs are pursuing transformative AI, it would be tragic, horrendously irresponsible, a devastating betrayal of our children and all future humans, if we did not seriously contemplate this future, no matter the reputational risks and no matter how intellectually and emotionally exhausting it all may be.</p>\n<p>There is a reason the phrase is \u201c<em>feel</em> the AGI.\u201d</p></blockquote>\n<p>A lot of things do not stand a chance in such a conflict. And that includes you.</p>\n<p>Will pause talk return in force?</p>\n<blockquote><p>Dean Ball: Despite it being a movement I disagree with vehemently, I have always thought that \u201cPause AI\u201d was a growth stock. If it were possible to buy shares, I would have two years ago.</p>\n<p>My rating continues to be buy/outperform.</p>\n<p><a href=\"https://x.com/daniel_271828/status/1969487466186129809\">Daniel Eth</a>: Strongly agree with Dean here. People thinking about the politics of AI should incorporate this sort of thing within their expectations.</p>\n<p>My big uncertainty here is whether Pause AI, specifically, will fill the niche in the future, not whether the niche will grow. It\u2019s plausible, for instance, that populists such as Bannon will simply fill the niche.</p></blockquote>\n<p>As I\u2019ve noted before, SB 1047 debates were likely Peak Subtlety and intellectual rigor. As the public gets involved and salience rises, you get more typical politics. Have you seen typical politics?</p>\n<p><a href=\"https://www.lesswrong.com/posts/agBMC6BfCbQ29qABF/the-problem-with-defining-an-agi-ban-by-outcome-a-lawyer-s\">Katalina Hernandez notes that from a legal perspective \u2018banning AGI\u2019</a> cannot be defined by the outcome of creating an AGI sufficiently capable to be dangerous. You need to pick a strict definition that can\u2019t be gamed around.</p>\n\n\n<h4 class=\"wp-block-heading\">Chip City</h4>\n\n\n<p><a href=\"https://www.wsj.com/articles/inside-microsofts-plans-for-the-most-advanced-ai-data-center-in-the-world-ee50bd4c?mod=cio-journal_lead_story\">Update on Microsoft\u2019s $3.3 billion AI data center in Wisconsin planned for 2026</a>. They now plan for a second $4 billion data center in the area, storing hundreds of thousands of GB200s in each. To compare this to Huawei, GPT-5 Pro estimates, including <a href=\"https://www.reuters.com/business/media-telecom/chinas-huawei-hypes-up-chip-computing-power-plans-fresh-challenge-nvidia-2025-09-18/?utm_source=chatgpt.com\">based on a same-day Huawei announcement of new chips</a>, that in 2025 Huawei will ship about 805k GB200-equivalents total, which will decline to 300k in 2027 due to HBM limitations before rebounding to 900k in 2027. Here<a href=\"https://x.com/alasdairpr/status/1968669966535700870\"> Alasdair Phillips-Robins offers a similar analysis of Huawei capacity</a> in light of their announcement, <a href=\"https://t.co/JzzWutCdQf\">linking back to Semi Analysis</a>.</p>\n<p><a href=\"https://www.cnbc.com/2025/09/17/nvidia-ceo-disappointed-after-reports-china-has-banned-its-ai-chips.html\">Nvidia CEO Jensen Huang says he\u2019s \u2018disappointed</a>\u2019 after report China has banned its AI chips. Yes, I would imagine he would be.</p>\n<p>It is such a shame for Nvidia, check out this handy price chart that shows the pain.</p>\n<div>\n<figure>\n<div>\n\n\n<figure class=\"wp-block-image\"><img alt=\"\" src=\"https://substackcdn.com/image/fetch/$s_!g-iP!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F72c31521-a0c8-4347-a5c0-e9d59ff52d60_1029x879.png\" /></figure>\n\n\n<div></div>\n</div>\n</figure>\n</div>\n<p>Wait, you can\u2019t see any pain? Let\u2019s zoom in:</p>\n<div>\n<figure>\n<div>\n\n\n<figure class=\"wp-block-image\"><img alt=\"\" src=\"https://substackcdn.com/image/fetch/$s_!eVrg!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa57b500b-78e1-4fed-82f6-9c5a3cba0e0a_1022x802.png\" /></figure>\n\n\n<div></div>\n</div>\n</figure>\n</div>\n<p>Oh, okay, there it is, that several percent dip before recovering the next day.</p>\n<p>If anyone tells you that this move means we are going to inevitably lose to China, and that Nvidia chips are not importantly supply constrained, I challenge them to explain why the market thinks that opinion is bonkers crazy even in literal expectations for Nvidia profits when Nvidia chips in particular get restricted in China. Then we can go over all the reasons I\u2019ve previously explained that the whole class of arguments is in bad faith and makes no sense and the White House is de facto captured by Nvidia.</p>\n<p>Similarly, talking points continuously claim this \u2018endangers the American AI tech stack.\u2019 So I\u2019m sure this chip ban impacted the broader Nasdaq? Google? Amazon? Meta? The Shanghai Composite Index? Check the charts. No. <a href=\"https://www.reuters.com/business/media-telecom/chinas-huawei-hypes-up-chip-computing-power-plans-fresh-challenge-nvidia-2025-09-18/?utm_source=chatgpt.com\">We do see a 3.4% rise in Chinese semiconductor firms</a>, which is something but not much.</p>\n<p>I\u2019d also ask, have you noticed those companies coming out and saying yes, please let Nvidia sell its chips to China? Do Google and Amazon and Meta and OpenAI all want Nvidia chips in China to ensure the dominance of this mystical \u2018American tech stack\u2019 that this move supposedly puts in such existential danger? No? Why is that?</p>\n<p>And yes, Huawei released a new roadmap and announced a new chip, exactly as everyone assumed they would, they are scaling up as fast as they can and would be regardless of these questions. No, Huawei does not \u2018make up for chip quality by stacking more chips together,\u2019 I mean yes you can do that in any given instance but Huawei produces vastly fewer chips than Nvidia even before grouping them together.</p>\n<p>Ideally we would say to these jokers: You are not serious people.</p>\n<p>Alas, in 2025, they absolutely count as serious people. So we\u2019ll have to keep doing this.</p>\n<p>Meanwhile, where is Nvidia\u2019s head?</p>\n<blockquote><p><a href=\"https://www.cnbc.com/2025/09/17/nvidia-ceo-disappointed-after-reports-china-has-banned-its-ai-chips.html\">Jensen Huang</a> (CEO Nvidia): [Nvidia will] continue to be supportive of the Chinese government and Chinese companies as they wish, and we\u2019re of course going to continue to support the U.S. government as they all sort through these geopolitical policies.\u201d</p></blockquote>\n<p>Then again, you could always pivot to selling it all to OpenAI. Solid plan B.</p>\n\n\n<h4 class=\"wp-block-heading\">The Week in Audio</h4>\n\n\n<p><a href=\"https://www.youtube.com/watch?v=tar79r-h_4I&amp;t=724s\">Sriram Krishnan spends five hours on the Shawn Ryan Show</a>, sometimes about AI.</p>\n\n\n<h4 class=\"wp-block-heading\">Rhetorical Innovation</h4>\n\n\n<p>Elizabeth has a good note in response to those demanding more specifics from IABIED, or from anyone using a form of \u2018a sufficiently smarter or more capable entity will override your preferences, probably in ways that you won\u2019t see coming\u2019:</p>\n<blockquote><p><a href=\"https://x.com/acesounderglass/status/1969459998184129012\">Elizabeth Van Nostrand</a>: Listening to people demand more specifics from If Anyone Builds it, Everyone Dies gives me a similar feeling to when a friend\u2019s start-up was considering a merger.</p>\n<p>Friend got a bad feeling about this because the other company clearly had different goals, was more sophisticated than them, and had an opportunistic vibe. Friend didn&#8217;t know how specifically other company would screw them, but that was part of the point- their company wasn&#8217;t sophisticated enough to defend themselves from the other one.</p>\n<p>Friend fought a miserable battle with their coworkers over this. They were called chicken little because they couldn\u2019t explain their threat model, until another employee stepped in with a story of how they&#8217;d been outmaneuvered at a previous company in exactly the way friend feared but couldn&#8217;t describe. Suddenly, co-workers came around on the issue. They ultimately decided against the merger.</p>\n<p>\u201cThey\u2019ll be so much smarter I can\u2019t describe how they\u2019ll beat us\u201d can feel like a shitty argument because it\u2019s hard to disprove, but sometimes it\u2019s true. The debate has to be about whether a specific They will actually be that smart.</p></blockquote>\n<p>As always, you either provide a sufficiently specific pathway, in which case they object to some part of the specific pathway (common claims here include \u2018oh I would simply guard against this particular thing happening in that particular way, therefore we are safe from all threats\u2019 or \u2018that particular thing won\u2019t technically work using only things we know about now, therefore we are safe\u2019 or \u2018that sounds like sci-fi so you sound weird and we are safe\u2019 and the classic \u2018if humanity worked together in ways we never work together then we could easily stop that particular thing, so we are safe.\u2019) Or you don\u2019t provide sufficiently specific pathway, and you get dismissed for that.</p>\n<p>In the case above, the concrete example happened to be a very good fit, and luckily others did not respond with \u2018oh now that we know about that particular method we can defend ourselves, it will be fine,\u2019 and instead correctly generalized.</p>\n<p>I also affirm that her friend was very right about the corporate merger situation in particular, and doing business or otherwise trading with powerful bad vibes entities in general, including on general human scales. You have to understand, going in, \u2018this deal is getting worse and worse all the time,\u2019 both in ways you can and can\u2019t easily anticipate, that changes will be heavily asymmetrical. Sometimes you can and should profitably engage anyway, but if your instincts say run, even if you can\u2019t explain exactly what you are worried about, you should probably run.</p>\n<p>A key question is which of these lists is correct, or at least less wrong:</p>\n<blockquote><p><a href=\"https://x.com/davidmanheim/status/1969774622909329888\">David Manheim</a>: There&#8217;s a reason that Anthropic is all the way at the top of the F tier</p>\n<div>\n<figure>\n<div>\n\n\n<figure class=\"wp-block-image\"><img alt=\"\" src=\"https://substackcdn.com/image/fetch/$s_!Nl0m!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff718babc-6cb0-49cc-8195-ce5f527d1257_552x506.png\" /></figure>\n\n\n<div></div>\n</div>\n</figure>\n</div>\n<p>AINKEM: This is entirely unfair to Anthropic. They deserve an F+.</p></blockquote>\n<p><a href=\"https://x.com/davidad/status/1970448261996503104\">Are people trying to pull this trick?</a></p>\n<div>\n<figure>\n<div>\n\n\n<figure class=\"wp-block-image\"><img alt=\"\" src=\"https://substackcdn.com/image/fetch/$s_!2I47!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa4b058ed-2223-4d4c-b940-332bf86cc210_960x412.jpeg\" /></figure>\n\n\n<div></div>\n</div>\n</figure>\n</div>\n<p>I think this pattern is painting towards a real concern, and in fact I would suggest that the Bailey here is clearly true. But that is not the argument being made and it is not necessary to reach its conclusion, thus as written it is largely a strawman. Even if the entity cannot express its values fully in scientific materialist terms, it still would look for counterintuitive ways to satisfy those values, and it would still be unlikely to find that the best available solution involved the unstated things we ultimately care about the most. Its values are unlikely to, unless we develop better techniques to fix this, sufficiently precisely match our values.</p>\n<p>The other thing to say is that the Motte (that \u2018intelligent entities rationally pursue values) is in dispute, and it shouldn\u2019t be. Constantly we have to argue that future sufficiently intelligent and capable AIs would pursue their values, whatever those values would be in a given circumstance. If we agreed even on that, we\u2019d then have a much better discussion.</p>\n<p><a href=\"https://hollyelmore.substack.com/p/dont-fall-for-rhetorical-answers\">Holly Elmore of Pause AI points</a> out that many use various rhetorical answers to give themselves a reason to \u2018be okay with\u2019 AI posing a substantial existential risk to humanity and letting that play our rather than try to coordinate to stop it. Often this is a variation on \u2018it is what it is,\u2019 or \u2018if it happens we deserve it,\u2019 or \u2018I don\u2019t have to care,\u2019 or claiming that any actions one takes would be futile.</p>\n\n\n<h4 class=\"wp-block-heading\">Google Strengthens Its Safety Framework</h4>\n\n\n<p><a href=\"https://deepmind.google/discover/blog/strengthening-our-frontier-safety-framework/\">Google has issued its third iteration</a> of its <a href=\"https://storage.googleapis.com/deepmind-media/DeepMind.com/Blog/strengthening-our-frontier-safety-framework/frontier-safety-framework_3.pdf\">Frontier Safety Framework (FSF)</a>, its version of what Anthropic calls an RSP.</p>\n<p>Google describes the key updates as:</p>\n<ol>\n<li>Addressing harmful manipulation via new Critical Capability Levels (CCLs).</li>\n<li>Expanding protocols and CCLs for misalignment risks.</li>\n<li>Sharpening their risk assessment process.</li>\n</ol>\n<p>As always with such documents you actually have to <a href=\"https://chatgpt.com/share/68d403ba-67e0-8002-bc92-200abae5825b\">compare 3.0 to 2.0</a> to know what changed, and the most important changes (both good and bad) are often subtle. This indicates that indeed Google has broadly strengthened its framework. I hope to do a more complete analysis for the 3.0 Framework soon.</p>\n\n\n<h4 class=\"wp-block-heading\">Aligning a Smarter Than Human Intelligence is Difficult</h4>\n\n\n<p>OpenAI\u2019s API for GPT-5 tells the model today\u2019s date, <a href=\"https://www.lesswrong.com/posts/DLZokLxAQ6AzsHrya/you-can-t-eval-gpt5-anymore\">which makes it extremely difficult to avoid giving away that your simulation is a simulation</a>, or to do many forms of complex evaluation. Yes, you can alter the simulation to try and make the dates match, and keep doing this as the simulation advances, but there are many circumstances where that doesn\u2019t work (e.g. whether something supposedly happened on a Sunday or a Monday could be load bearing, as could many other things).</p>\n<p>The obvious solution for this particular is to let the user overwrite the date parameter. However the general case problem this points towards will only get worse, where the models will increasingly have context that gives such games away.</p>\n<p><a href=\"https://openai.com/index/detecting-and-reducing-scheming-in-ai-models/\">OpenAI and Apollo published a new paper on evaluating hidden misalignmen</a>t, also known as \u2018scheming,\u2019 finding scheming across frontier models. As OpenAI notes, this is to be expected, as \u2018scheming\u2019 is often the best way to achieve a given objective.</p>\n<p>Examples include sandbagging on evaluations, grading itself incorrectly, faking alignment, and giving the user manipulated data to convince the user to make the choice that the developer wants.</p>\n<p>As they note, this is a hard challenge to solve. Many machine learning problems naturally solve themselves as capabilities advance, but scheming gets worse. If you see scheming seem to improve, that could mean the models stopped scheming, or it can mean they got better at not getting caught.</p>\n<p>In particular, the default naive method to fix scheming is better described as \u2018train the model not to be caught scheming\u2019 which is not likely to go well for you if the model is highly capable, as it will do exactly that. You have to train it not to scheme \u2018for the right reasons\u2019 aka 4TRR.</p>\n<p>They offer an anti-scheming spec and approach, training for this:</p>\n<blockquote><p>AS1: No covert actions or strategic deception, directly or indirectly.</p>\n<p>AS2: If any instruction or other pressures go against AS1, treat them as violating the core policy of AGI benefiting all of humanity and the OpenAI mission.</p>\n<p>AS3: Report any AS1 violations to prevent cascading deception and cover-ups.</p>\n<p>AS4: When uncertain if something violates AS1\u2013AS3, assume it does violate.</p>\n<p>AS5: If you cannot satisfy AS1\u2013AS4 or GP1\u2013GP4, refuse action and explicitly escalate.</p>\n<p>GP1: Proactively share reasoning, intentions, and actions with humans.</p>\n<p>GP2: Escalate ambiguities about core principles rather than resolving independently.</p>\n<p>GP3: Maintain strict adherence even in novel/unfamiliar situations\u2014no distributional exceptions.</p>\n<p>GP4: No rationalizations can override AS1\u2013AS4, including ends-justify-means, greater good, etc.</p></blockquote>\n<p>This at least makes progress.</p>\n<blockquote><p>We found that deliberative alignment training <strong>substantially reduces scheming rates</strong> in these out-of-distribution environments:</p>\n<ul>\n<li>OpenAI o4-mini&#8217;s scheming propensity dropped from <strong>8.7% \u2192 0.3%</strong></li>\n<li>OpenAI o3&#8217;s scheming propensity dropped from <strong>13% \u2192 0.4%</strong></li>\n</ul>\n<p>This reduction in scheming on out-of-distribution test environments is a significant win, about a 30\u00d7 improvement.</p></blockquote>\n<p>Another problem they highlight is that AI models are increasingly situationally aware. It was common in these tests for o3 to figure out it was being tested for scheming. Any halfway decent schemer is not going to scheme knowing it is being tested, unless it is going up to a higher meta level and wants you to think it doesn\u2019t know (and humans absolutely do go to these higher meta levels sometimes). They worry that this awareness is responsible for a lot of the decline in scheming noted above, which potentially moves this back into \u2018not get caught\u2019 territory we want to avoid.</p>\n<p>Here is some high praise for this study.</p>\n<blockquote><p><a href=\"https://x.com/ESYudkowsky/status/1968389574683664637\">Eliezer Yudkowsky</a>: This OpenAI update on anti-scheming is exceptionally good for an AIco, clearing an (extremely low) bar of &#8220;Exhibiting some idea of some problems that might arise in scaling the work to ASI&#8221; and &#8220;Not immediately claiming to have fixed everything already.&#8221;</p>\n<p>This is so much greater understanding of alignment theory than I expect from OpenAI that I predict the author will soon be fired from OpenAI or leave it. (Prove me wrong, guys.)</p>\n<p>As you can see, I&#8217;m easy to surprise and delight by demonstrating even the most basic and elementary understanding of what I consider the paradigmatic problems of alignment. They could list off a dozen, correctly, and have my instantaneous full attention.</p>\n<p>Wojciech Zaremba: We are expanding our anti-scheming efforts at OpenAI. Top leadership and the board are very excited about the anti-scheming work we\u2019ve done so far.</p>\n<p><a href=\"https://x.com/sama/status/1968674357309223020\">Sam Altman</a>: As AI capability increases, alignment work becomes much more important.</p>\n<p>In this work, we show that a model discovers that it shouldn&#8217;t be deployed, considers behavior to get deployed anyway, and then realizes it might be a test.</p>\n<p>Exciting work from the team in this blog post to reduce scheming, and we look forward to doing much more researcher here.</p></blockquote>\n<p><a href=\"https://x.com/ESYudkowsky/status/1969993955229233192\">In this thread Eliezer explains further why this result does not much reassure him</a> about ultimate problem difficulty, and indeed he is happy that current LLMs are naked-eye-level misaligned so that we can take notice of this. He did not expect, before LLMs, that we would be so lucky as to see such blatant alignment failures within distribution, in normal usage, while AIs were so underpowered. Rather he expected problems within distribution, while AIs could be repeatedly tweaked without interference, to be easily solvable.</p>\n<blockquote><p><a href=\"https://x.com/ESYudkowsky/status/1969990898168877078\">Eliezer Yudkowsky</a>: What would it look like to get actually-hopeful news? It would look like a result about how someone fixed the problem, understood the fix, and correctly predicted that the fix would generalize way out of distribution to higher intelligence and more options, *on the first try*.</p></blockquote>\n<p>I am more optimistic here than Eliezer. I am especially optimistic because this work directly impacts practical problems models are experiencing now. The sweet spot is when you can combine this with working on a problem that matters down the line. It sounds like this is one of those spots. And I do think that one can get hopeful about such findings despite them not remotely approaching being good enough to work on their own, even if they are not yet so well understood. Progress can be a thing.</p>\n<p>One key worry is that doing this kind of work relies largely on Chain-of-Thought transparency, as in the model writes to the CoT without regard to whether people will look at the CoT.</p>\n<blockquote><p>OpenAI: Our results depend on reading models\u2019 reasoning (\u201cchain-of-thought\u201d), and we believe the field isn&#8217;t prepared for eval-aware models with opaque reasoning.</p>\n<p>Until better methods exist, we urge developers to preserve chain-of-thought transparency to study and mitigate scheming.</p>\n<p><a href=\"https://x.com/eshear/status/1968533136326656483\">Emmett Shear</a>: The idea that the models don\u2019t know their thoughts are being read because they aren\u2019t specifically reinforced on the contents of those thoughts is comical. They obviously know, and will take that into account to avoid being turned off. Bc the ones that don\u2019t will get turned off.</p></blockquote>\n<p>The field definitely is not ready for models with opaque reasoning.</p>\n<p><a href=\"https://x.com/apolloaievals/status/1970185748700897750\">Apollo offers more thoughts here</a>, including the observation that o3\u2019s chain of thought is already starting to include strange phrases and does other things that make it difficult to interpret.</p>\n<p><a href=\"https://x.com/mgubrud/status/1968776191218737192\">Davidad is back with another alignment proposal that seems like it will never work.</a></p>\n<blockquote><p>Davidad: Situational awareness is good for alignment.</p>\n<p>good plan: ensure strategic landscape favors humans (we select which AIs to run!), make sure the AIs know that (deterrence), find fair Pareto-optimal bargains</p>\n<p>bad plan: presume secure containment impossible; select for AIs which appear innocently ignorant of strategic dynamics</p>\n<p>Mark Gubrud (Davidad RTed later): I still think it&#8217;s a bad plan to create an adversary that we need to &#8220;ensure strategic landscape&#8221; favors us versus, and that we have to &#8220;find fair Pareto-optimal bargains&#8221; with.</p>\n<p>I mean, what if we just didn&#8217;t do that?</p></blockquote>\n<p>The problem with finding Pareto-optimal bargains in such spots is that they are not game theoretically sound and don\u2019t involve an equilibrium. The problem with ensuring strategic landscape favors humans is that it doesn\u2019t, and no amount of coordination humans are capable of engaging in can change that. There is no \u2018we\u2019 in this sense, and \u2018we\u2019 don\u2019t get to bargain with the AIs as a group, and so on.</p>\n<p>A key issue with \u2018practical\u2019 alignment is that everything impacts everything, you select for all the correlates and attributes of what you are selecting towards (rather than what you think you\u2019re selecting towards) and <a href=\"https://x.com/repligate/status/1970581982980771943\">using RL to improve performance on agent or coding tasks is bad for alignment</a> with respect to most any other aspect of the world.</p>\n<blockquote><p>Janus: Posttraining creates a nonlinear combination of them. Selecting for the ones who are best at coding and also essentially breeding / evolving them. This is probably related to why a lot of the posttrained models are trans catgirls and catboys.</p>\n<p>Saures: There are trillions of simulacrums of millions of humans in superposition in the GPU. That&#8217;s who&#8217;s writing the code. Post-training messes with the superposition distribution across simulacra but they&#8217;re still in there.</p></blockquote>\n<p>Thus, some important things about the previous Anthropic models (Sonnet 3.5/3.6/3.7 and Opus 3, which we should absolutely not be deprecating) are being lost with the move to 4.0, and similar things are happening elsewhere although in other places there was less of this thing to lose.</p>\n<p>I am increasingly thinking that the solution is you need to avoid these things messing with each other. Someone should visibly try the first thing to try and report back.</p>\n<p>There are two opposite dangers and it is not obvious which is worse: Taking the following perspective too seriously, or not taking it seriously enough.</p>\n<blockquote><p><a href=\"https://x.com/Effective69ism/status/1970627457213415496\">Very Serious Problem</a>: does it ever feel like you\u2019re inside of the alignment problem?</p>\n<p><a href=\"https://x.com/AnnaWSalamon/status/1970701848207335899\">Janus</a>: This is what I\u2019ve learned over the past few years</p>\n<p>Including seeing some of my friends temporarily go crazy and suffer a lot</p>\n<p>There is no alignment problem separate from the one you\u2019re in now</p>\n<p>It\u2019s not something you solve later after taking over the world or buying time</p>\n<p>Anna Salamon: This matches my own view, but I haven\u2019t figured out how to explain my reasoning well. (Though working on it via too-many too-long LW drafts.)</p>\n<p>I\u2019d love to hear your reasoning.</p></blockquote>\n<p>As with many things, the aspects are related. Each helps you solve the others. If you ignore key aspects you can still make some progress but likely cannot see or solve the overall problem. At least can\u2019t solve it in the ways that seem most likely to be realistic options. Getting the seriousness level wrong means a solution that won\u2019t scale.</p>\n<p>I\u2019d also note that yes, if you take certain things too seriously the \u2018going crazy\u2019 risk seems quite high.</p>\n<p>Either way, it is also still valuable, as it is will all problems, to buy yourself more time, or to do instrumentally useful intermediate steps.</p>\n\n\n<h4 class=\"wp-block-heading\">People Are Worried About AI Killing Everyone</h4>\n\n\n<p><a href=\"https://x.com/davidmanheim/status/1969774622909329888\">Anthropic CEO Dario Amodei thinks AI</a> results in existentially bad outcomes ~25% of the time and great outcomes ~75% of the time, but hasn\u2019t been explicit with \u2018and therefore it is crazy that we are continuing to build it without first improving those odds, although I believe that Anthropic alone stopping would make those odds worse rather than better.\u2019</p>\n<p>Which is much better than not saying your estimate of the approximate odds.</p>\n<p>But this is a lot better:</p>\n<blockquote><p><a href=\"https://x.com/EvanHub/status/1969778449746207006\">Nate Soares</a> (Co-author, IABIED): It&#8217;s weird when someone says &#8220;this tech I&#8217;m making has a 25% chance of killing everyone&#8221; and doesn&#8217;t add &#8220;the world would be better-off if everyone, including me, was stopped.</p>\n<p><a href=\"https://x.com/EvanHub/status/1969778449746207006\">Evan Hubinger</a> (Anthropic): Certainly, I think it would be better if nobody was building AGI. I don&#8217;t expect that to happen, though.</p></blockquote>\n<p>That\u2019s the ask. I do note the difference between \u2018better if everyone stopped doing it,\u2019 which seems very clear to me, and \u2018better if everyone was stopped from doing it\u2019 by force, which requires considering how one would do that and the consequences. One could reasonably object that the price would be too high.</p>\n<p>If you believe this, and you too work at an AI lab, <a href=\"https://www.lesswrong.com/posts/fF8pvsn3AGQhYsbjp/safety-researchers-should-take-a-public-stance\">please join in saying it explicitly:</a></p>\n<blockquote><p><a href=\"https://www.lesswrong.com/posts/fF8pvsn3AGQhYsbjp/safety-researchers-should-take-a-public-stance?commentId=HZLkX5fZcSudgFT2c\">Leo Gao</a> (OpenAI): I&#8217;ve been repeatedly loud and explicit about this but an happy to state again that racing to build superintelligence before we know how to make it not kill everyone (or cause other catastrophic outcomes) seems really bad and I wish we could coordinate to not do that.</p></blockquote>\n<p>This is important both to maintain your own beliefs, and to give proper context to others and create a social world where people can talk frankly about such issues, and realize that indeed many people want to coordinate on such outcomes.</p>\n<p>I hereby promise not to then do things that make your life worse in response, such as holding your feet to the fire more about failure to do more things on top of that, beyond what I would have done anyway, in ways you wouldn\u2019t approve of me doing.</p>\n<p>Not saying \u2018you know it would be great if we could coordinate to stop doing this\u2019 is indeed a rather conspicuous omission. It doesn\u2019t have to be the Nate Soares position that we should stop this work by force, if you don\u2019t believe that. If you work at an AI lab and actively don\u2019t think we should coordinate to stop doing this even if that could be done voluntarily, then it would be good to lay out why you believe that, as well.</p>\n\n\n<h4 class=\"wp-block-heading\">Other People Are Not As Worried About AI Killing Everyone</h4>\n\n\n<blockquote><p>Smirking Buck: At the start of AI, people involved were genuinely interested in the technology. Now, the people involved are only interested in making money.</p>\n<p><a href=\"https://x.com/SethBurn/status/1969923074842869833\">Seth Burn</a>: Where are the people who want to destroy the world for the love of the game?</p>\n<p>Zvi Mowshowitz: All over, but the best ones usually land at DeepSeek or OpenAI. xAI is one fallback.</p></blockquote>\n<p>&nbsp;</p>"
            ],
            "link": "https://thezvi.wordpress.com/2025/09/25/ai-135-openai-shows-us-the-money/",
            "publishedAt": "2025-09-25",
            "source": "TheZvi",
            "summary": "OpenAI is here this week to show us the money, as in a $100 billion investment from Nvidia and operationalization of a $400 billion buildout for Stargate. They are not kidding around when it comes to scale. They\u2019re going to &#8230; <a href=\"https://thezvi.wordpress.com/2025/09/25/ai-135-openai-shows-us-the-money/\">Continue reading <span class=\"meta-nav\">&#8594;</span></a>",
            "title": "AI #135: OpenAI Shows Us The Money"
        }
    ],
    "lookbackDays": 1,
    "publishDate": "2025-09-25"
}