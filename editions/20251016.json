{
    "articles": [
        {
            "content": [
                "<div class=\"trix-content\">\n  <div><a href=\"https://omarchy.org/\">Omarchy</a> didn't even exist before this summer. I did much of the pre-release work during the downtime between sessions at the 24 Hours of Le Mans in June. And now, just a few months later, we've delivered a petabyte of ISOs in the past thirty days alone. That's about 150,000 installs of the Omarchy Linux distribution!<br /><br /></div><div>I've been involved with a lot of successful open-source projects in the past quarter of a century or so. <a href=\"https://rubyonrails.org\">Ruby on Rails</a>, first and foremost. But nothing, not even Rails, grew as quickly as Omarchy has been growing in the first few months of its life. It's rather remarkable.<br /><br /></div><div>This is what product-market fit looks like. Doesn't matter if the product is free or not. The fit is obvious. The stream of people who don't just enjoy Omarchy but <em>love it</em> is seemingly endless. The passion is palpable.<br /><br /></div><div>But why? And why now?<br /><br /></div><div>As per usual, there are a lot of contributing factors, but key is how Apple and Microsoft have been fumbling their relationship with people who love computers in general and developers in particular.</div><div><br />Microsoft is killing off Windows 10, which in turn cuts off a whole slew of perfectly fine computers made prior to around 2017\u20132018. They also seem intent on shoving AI into everything, and wavering on whether that might be optional or not. Oh, and Windows is still Windows: decades of patching cracks in a foundation that just never was all that solid to begin with.</div><div><br />Apple too has turned a ton of people off with macOS 26 Tahoe, liquid glass, and faltering software quality. They're also cutting off all Intel-based Macs from future updates. A Mac Mini sold as recently as 2023 is now end-of-life! This is before we even talk about how poorly the company has been treating developers depending on the App Store bureaucracy.</div><div><br />Meanwhile, Linux has never looked better. <a href=\"https://hypr.land/\">Hyprland</a>, the tiling window manager at the heart of Omarchy, is a sensation. It's brought an incredible level of finesse, detail, and style to the tiling window management space: superb animations, lightning-fast execution, and super-light resource consumption.</div><div><br />The historic gap in native GUI apps has never mattered less either. The web has conquered all as the dominant computing platform. In the past, missing, say, Photoshop was a big deal. Now it's Figma \u2014 a web app! \u2014 that's driving designers. Same too with tools like Microsoft Office or Outlook, which are all available on the web.</div><div><br />I'm not saying there aren't specialized apps that some people simply can't do without, that keep them trapped on Windows or Mac. But I am saying that they've never been fewer. Almost everything has a great web alternative.<br /><br /></div><div>And for developers, the fact is that Linux was always a superior platform in terms of performance and tooling for most programming environments. With 95% of the web running on Linux servers, all optimization and tuning needed to get the most out of the hardware was done with Linux in mind.<br /><br /></div><div>This is why even <a href=\"https://world.hey.com/dhh/it-s-a-beelink-baby-243fdaf1\">a $500 Beelink Mini PC</a> is competitive with an M4 Max machine costing thousands of dollars for things like our HEY test suite, which runs on Ruby and MySQL. Linux is just really efficient and really fast.<br /><br /></div><div>Finally, I think the argument that owning your computer, fully and deeply, is starting to resonate. The Free Software crowd has been making the argument since the 90s, if not before, but it's taken Apple's and Microsoft's recent tightening of the reins on our everyday operating systems to make it relevant for most.<br /><br /></div><div>Omarchy is a beautiful, modern, and opinionated Linux distribution, but it's also <em>yours</em>. Everything is preconfigured, sure, but every configuration is also changeable. Don't like how something works? Change it. Don't like the apps I use? Change them. Don't like how something looks? Redesign it. The level of agency is off the charts.<br /><br /></div><div>Turns out that plenty of people were starved for just this. All it took was someone to actually put all the pieces together, ignore the Linux neckbeards who insist you aren't worthy to run Arch or Hyprland without spending a hundred hours setting it up from scratch, and invite everyone to the party!</div>\n</div>"
            ],
            "link": "https://world.hey.com/dhh/a-petabyte-worth-of-omarchy-in-a-month-a1fc538e",
            "publishedAt": "2025-10-16",
            "source": "DHH",
            "summary": "<div class=\"trix-content\"> <div><a href=\"https://omarchy.org/\">Omarchy</a> didn't even exist before this summer. I did much of the pre-release work during the downtime between sessions at the 24 Hours of Le Mans in June. And now, just a few months later, we've delivered a petabyte of ISOs in the past thirty days alone. That's about 150,000 installs of the Omarchy Linux distribution!<br /><br /></div><div>I've been involved with a lot of successful open-source projects in the past quarter of a century or so. <a href=\"https://rubyonrails.org\">Ruby on Rails</a>, first and foremost. But nothing, not even Rails, grew as quickly as Omarchy has been growing in the first few months of its life. It's rather remarkable.<br /><br /></div><div>This is what product-market fit looks like. Doesn't matter if the product is free or not. The fit is obvious. The stream of people who don't just enjoy Omarchy but <em>love it</em> is seemingly endless. The passion is palpable.<br /><br /></div><div>But why? And why now?<br /><br /></div><div>As per usual, there are a lot of contributing factors, but key is how Apple and Microsoft have been fumbling their relationship with people who love computers in general and developers in particular.</div><div><br />Microsoft is killing off Windows 10, which in turn cuts",
            "title": "A petabyte worth of Omarchy in a month"
        },
        {
            "content": [],
            "link": "https://buttondown.com/hillelwayne/archive/the-phase-change/",
            "publishedAt": "2025-10-16",
            "source": "Hillel Wayne",
            "summary": "<p>Last week I ran my first 10k.</p> <p>It wasn't a race or anything. I left that evening planning to run a 5k, and then three miles later thought \"what if I kept going?\"<sup id=\"fnref:distance\"><a class=\"footnote-ref\" href=\"https://buttondown.com/hillelwayne/rss#fn:distance\">1</a></sup></p> <p>I've been running for just over two years now. My goal was to run a mile, then three, then three at a pace faster than a power-walk. I wish I could say that I then found joy in running, but really I was just mad at myself for being so bad at it. Spite has always been my brightest muse.</p> <p>Looking back, the thing I find most fascinating is what progress looked like. I couldn't tell you if I was physically progressing steadily, but for sure mental progress moved in discrete jumps. For a long time a 5k was me pushing myself, then suddenly a \"phase change\" happens and it becomes something I can just do on a run. Sometime in the future the 10k will feel the same way.</p> <p>I've noticed this in a lot of other places. For every skill I know, my sense of myself follows a phase change. In every programming language I've ever learned, I lurch from \"bad\" to",
            "title": "The Phase Change"
        },
        {
            "content": [
                "<p>Anthropic is releasing their first-party Skills system across Claude Code, Claude.ai and the Claude API, all launching today. I can tell that they've been working on it for quite a while and I'm really excited about it.</p>\n<p><s>In just a few minutes, I plan to release a new version of Superpowers that uses the official Skills system.</s>\nShortly after I first published this blog post, I released a new version of Superpowers that uses the official Skills system.</p>\n<p>One of the new skills they were gracious enough to allow me to test out is a new 'creating MCPs' skill. Other than a small issue with the tool descriptions it generates being too verbose, it was by far the smoothest MCP development experience I've had so far. (I used it to make the 'search' tool for a new episodic memory plugin I'm building for Claude Code that I hope to release soon.)</p>\n<p>I've been using skills in Claude Code for the better part of a month. Last week, when Anthropic shipped plugins support for Claude code, I celebrated by releasing my homebrew skills system as <a href=\"https://blog.fsck.com/2025/10/09/superpowers/\">Superpowers for Claude Code</a>.</p>\n<p>Superpowers is a couple of things:</p>\n<ul>\n<li>A bootstrap that teaches Claude Code to load <code>SKILL.md</code> files and when to use them, built as a SKILL.md file, a search tool, and a hook that initializes things on startup.</li>\n<li>A set of <code>SKILL.md</code> files designed to create, manage, and share <code>SKILL.md</code> files.</li>\n<li>A set of <code>SKILL.md</code> files that encode my <code>Architect/Implementer</code> agentic coding methodolgy, starting from the brainstorming process and running all the way through the TDD process.</li>\n<li>A set of <code>SKILL.md</code> files that encode a bunch of other software engineering skills.</li>\n</ul>\n<p>Over the weekend, while I was reviewing a bug report from an end user, I saw a bunch of logging output from <code>claude --debug</code> that talked about <code>skill</code> files and <code>skills</code> directories. Which would make sense, since I was debugging a skills system.</p>\n<p>The only problem was that the logging wasn't from Superpowers. (As a slightly funny aside: I had to check whether the log messages were from Superpowers or not.  Since Claude did all the actual implementation work for Superpowers, I'd never actually looked at that bit of code.)</p>\n<p>I built out Superpowers' core skill system based on spelunking around in Anthropic's initial few MS Office skills and bolting together some bits and pieces, along with some weird ideas I wanted to try out.</p>\n<p>At the time, I had no idea that Claude Code had an apparently-complete implementation of a skills system hidden inside. Based on Anthropic's documentation, the first-party skills system has some support in Claude Code 2.0.0 and may have been there as far back as Claude Code 1.0!?</p>\n<p>Had I known that Claude Code already <em>had</em> the magic bootstrappy parts that make <code>SKILL.md</code> files useful if their metadata is put together in just the right way, I absolutely wouldn't have built my own.</p>\n<p>But, now that Skills are officially a thing, Superpowers is embracing the future.</p>\n<p>As I'm starting to get familiar with the official skills system, it looks like the biggest design difference is that Anthropic's skills have a <code>name</code>, a <code>description</code>, a <code>license</code> (I think?) and a <code>metadata</code> field. Our skills had...flexible frontmatter, but the important parts were <code>name</code>, <code>description</code>, and <code>when_to_use</code> \u2013 We broke apart the answers to &quot;When should Claude use this skill?&quot; and &quot;What does this skill do?&quot;  I still think that that distinction is important. In my testing, I've found that showing <em>only</em> &quot;When should Claude use this skill?&quot; leads to better compliance. When Claude thinks it knows what a skill does, it's more likely to believe it's using the skill and just wing it, even if it hasn't read it yet.</p>\n<p>I've rewritten our core skills to switch things over to Anthropic's way of doing things. I've also broken apart our core skills into core Superpowers skills and a set of skills that we learned from <a href=\"https://github.com/microsoft/amplifier\">Microsoft's Amplifier</a>.</p>\n<p>I haven't had as much time to play around with Anthropic's skills as I'd like, and most of what I've seen has been on <a href=\"https://claude.ai\">Claude.ai</a>.</p>\n<p>As I've been exploring, the part of all this that's most interesting to me is getting the LLM to create its own skills. Making that work well is the key to a whole bunch of avenues for agentic self-improvement.</p>\n<p>As far as I can tell, Anthropic's skill creation methodology is pretty different than mine. Rather than poke inside Anthropic's skill creation metaskill, I did what any good engineer would do in 2025: I uploaded my <code>writing-skills/SKILL.md</code> to Claude.ai, asked it to compare and contrast it with Anthropic's first-party skill-creation metaskill, and went to grab a cup of coffee.</p>\n<p>I came back to a pretty detailed report comparing and contrasting the two approaches. Their skill has a lot more detail on the details of generating the skill and includes templates to copy. Mine spends a lot more time on testing the skill, looking for ways to avoid Claude's rationalizations about why it shouldn't need to use the skill, and persuasion techniques that can help ensure compliance.</p>\n<p><a class=\"glightbox\" href=\"https://blog.fsck.com/assets/2025/10/skillglazing.jpg\"><img alt=\"skillglazing\" src=\"https://blog.fsck.com/assets/2025/10/skillglazing.jpg\" /></a></p>\n<p>Unsurprisingly, Claude is a fan of TDD.</p>\n<p>In Claude Code, Skills are <code>SKILL.md</code> files included in directories inside <code>~/.claude/skills/</code>, in <code>.claude/skills</code> in a project directory, and in the <code>skills</code> subdirectory of plugins.</p>\n<p>Right now, it looks like Skills are being executed <s>as <code>/command</code>s</s> with a new <em>Skill</em> tool.</p>\n<p>Skills are themselves able to execute <code>/command</code>s tools that don't have a frontmatter flag that disables command execution.</p>\n<p>Skills can <em>also</em> be included in the <code>skills/</code> directory of any plugin.</p>\n<p>One thing about all of this that I find really funny is that that is <em>exactly</em> where I'd put Superpowers' skills, before I moved them out into another git repository to make it easier for users to customize them and share their improvements.</p>\n<p>Claude code automatically indexes <em>all</em> skills into the system prompt. I'm hoping that they eventually allow us to hide some skills from immediate disclosure, to let us build more complicated &quot;composite&quot; skills, like a 'debugging' skill that can link through to additional skills.</p>\n<p>I spent some time over the past couple days getting our skills ready for this brave new world, but they're not quite ready yet. The new system (hopefully) gets rid of the session startup hook, separate git repo for skills and <code>find-skills</code> tool. Now that skills can be managed like subagent personae or <code>/command</code>s, Superpowers doesn't need to build its own plugin system or become a giant monorepo of skills.</p>\n<p>Skills are incredibly powerful. They're magic words that make your agent behave differently, even without you asking directly. They can bundle scripts and  binaries (which should still be subject to your regular tool use authorizations, I believe.) They're not just subject to prompt injection\u2013They <em>are</em> the very definition of prompt injection.  (Despite the emdash and the &quot;It's not X, it's Y&quot; formation, that sentence was 100% human authored, albeit quite late at night.)</p>\n<p>I'm still working through issues convincing Claude to <em>always</em> execute the brainstorming skill automatically when it detects that the user wants to create or code something.</p>\n<p>Claude Code knows it's supposed to use skills automatically, but it's not relaible for me just yet.  At least temporarily, I've adapted my 'using-skills' skill into a new 'using-superpowers' skill and updated Superpowers' session start hook to load when Claude Code starts up. As I get familiar with the new system, I'm hopeful that I'll be able to remove that shim.</p>\n<p>Even when we don't need it for Claude Code anymore, the <code>using-superpowers/SKILL.md</code> should work just great with <code>gemini-cli</code> and <code>codex</code>. Once it does, the whole new world of <code>SKILL.md</code> skills should be open to them, too.</p>\n<p>I'm hoping to find time to work on skills for those other platforms, but I'd be grateful if you (yes, <em>you</em>) wanted to beat me to it and contribute those features back to Superpowers.</p>"
            ],
            "link": "https://blog.fsck.com/2025/10/16/skills-for-claude/",
            "publishedAt": "2025-10-16",
            "source": "Jesse Vincent",
            "summary": "<p>Anthropic is releasing their first-party Skills system across Claude Code, Claude.ai and the Claude API, all launching today. I can tell that they've been working on it for quite a while and I'm really excited about it.</p> <p><s>In just a few minutes, I plan to release a new version of Superpowers that uses the official Skills system.</s> Shortly after I first published this blog post, I released a new version of Superpowers that uses the official Skills system.</p> <p>One of the new skills they were gracious enough to allow me to test out is a new 'creating MCPs' skill. Other than a small issue with the tool descriptions it generates being too verbose, it was by far the smoothest MCP development experience I've had so far. (I used it to make the 'search' tool for a new episodic memory plugin I'm building for Claude Code that I hope to release soon.)</p> <p>I've been using skills in Claude Code for the better part of a month. Last week, when Anthropic shipped plugins support for Claude code, I celebrated by releasing my homebrew skills system as <a href=\"https://blog.fsck.com/2025/10/09/superpowers/\">Superpowers for Claude Code</a>.</p> <p>Superpowers is a couple of things:</p> <ul> <li>A bootstrap that teaches",
            "title": "Skills for Claude!"
        },
        {
            "content": [],
            "link": "https://www.robinsloan.com/lab/getting-online/",
            "publishedAt": "2025-10-16",
            "source": "Robin Sloan",
            "summary": "<p>With receipts! <a href=\"https://www.robinsloan.com/lab/getting-online/\">Read here.</a></p>",
            "title": "Getting online"
        },
        {
            "content": [],
            "link": "https://simonwillison.net/2025/Oct/16/claude-skills/#atom-entries",
            "publishedAt": "2025-10-16",
            "source": "Simon Willison",
            "summary": "<p>Anthropic this morning <a href=\"https://www.anthropic.com/news/skills\">introduced Claude Skills</a>, a new pattern for making new abilities available to their models:</p> <blockquote> <p>Claude can now use <em>Skills</em> to improve how it performs specific tasks. Skills are folders that include instructions, scripts, and resources that Claude can load when needed.</p> <p>Claude will only access a skill when it's relevant to the task at hand. When used, skills make Claude better at specialized tasks like working with Excel or following your organization's brand guidelines.</p> </blockquote> <p>Their engineering blog has a <a href=\"https://www.anthropic.com/engineering/equipping-agents-for-the-real-world-with-agent-skills\">more detailed explanation</a>. There's also a new <a href=\"https://github.com/anthropics/skills\">anthropic/skills</a> GitHub repo.</p> <p>(I inadvertently preempted their announcement of this feature when I reverse engineered and <a href=\"https://simonwillison.net/2025/Oct/10/claude-skills/\">wrote about it last Friday</a>!)</p> <p>Skills are conceptually extremely simple: a skill is a Markdown file telling the model how to do something, optionally accompanied by extra documents and pre-written scripts that the model can run to help it accomplish the tasks described by the skill.</p> <p>Claude's new <a href=\"https://www.anthropic.com/news/create-files\">document creation abilities</a>, which accompanied <a href=\"https://simonwillison.net/2025/Sep/9/claude-code-interpreter/\">their new code interpreter feature</a> in September, turned out to be entirely implemented using skills. Those are <a href=\"https://github.com/anthropics/skills/tree/main/document-skills\">now available Anthropic's repo</a> covering <code>.pdf</code>, <code>.docx</code>, <code>.xlsx</code>, and <code>.pptx</code> files.</p> <p>There's one extra detail that",
            "title": "Claude Skills are awesome, maybe a bigger deal than MCP"
        },
        {
            "content": [
                "<p>\n          <a href=\"https://www.astralcodexten.com/p/hidden-open-thread-4035\">\n              Read more\n          </a>\n      </p>"
            ],
            "link": "https://www.astralcodexten.com/p/hidden-open-thread-4035",
            "publishedAt": "2025-10-16",
            "source": "SlateStarCodex",
            "summary": "<p> <a href=\"https://www.astralcodexten.com/p/hidden-open-thread-4035\"> Read more </a> </p>",
            "title": "Hidden Open Thread 403.5"
        },
        {
            "content": [
                "<p>Well, one person says \u2018demand,\u2019 another says \u2018give the thumbs up to\u2019 or \u2018welcome our new overlords.\u2019 Why quibble? Surely we\u2019re all making way too big a deal out of this idea of OpenAI \u2018treating adults like adults.\u2019 Everything will be fine. Right?</p>\n<p>Why not focus on all the other cool stuff happening? Claude Haiku 4.5 and Veo 3.1? Walmart joining ChatGPT instant checkout? Hey, come back.</p>\n<p>Alas, the mass of things once again got out of hand this week, so we\u2019re splitting the update into two parts.</p>\n\n\n<h4 class=\"wp-block-heading\">Table of Contents</h4>\n\n\n<ol>\n<li><a href=\"https://thezvi.substack.com/i/175714647/earlier-this-week\">Earlier This Week.</a> OpenAI does paranoid lawfare, China escalates bigly.</li>\n<li><a href=\"https://thezvi.substack.com/i/175714647/language-models-offer-mundane-utility\">Language Models Offer Mundane Utility.</a> Help do your taxes, of course.\n<div>\n\n\n<span id=\"more-24796\"></span>\n\n\n</div>\n</li>\n<li><a href=\"https://thezvi.substack.com/i/175714647/language-models-don-t-offer-mundane-utility\">Language Models Don\u2019t Offer Mundane Utility.</a> Beware the false positive.</li>\n<li><a href=\"https://thezvi.substack.com/i/175714647/huh-upgrades\"><strong>Huh, Upgrades</strong>.</a> Claude Haiku 4.5, Walmart on ChatGPT instant checkout.</li>\n<li><a href=\"https://thezvi.substack.com/i/175714647/we-patched-the-torment-nexus-turn-it-back-on\"><strong>We Patched The Torment Nexus, Turn It Back On</strong>.</a> OpenAI to loosen the reigns.</li>\n<li><a href=\"https://thezvi.substack.com/i/175714647/on-your-marks\">On Your Marks.</a> Sonnet 4.5 on the METR graph, and a superforecasting update.</li>\n<li><a href=\"https://thezvi.substack.com/i/175714647/choose-your-fighter\">Choose Your Fighter.</a> Coding agents don\u2019t help some, and bottleneck others.</li>\n<li><a href=\"https://thezvi.substack.com/i/175714647/deepfaketown-and-botpocalypse-soon\">Deepfaketown and Botpocalypse Soon.</a> The problem remains the demand side.</li>\n<li><a href=\"https://thezvi.substack.com/i/175714647/fun-with-media-generation\">Fun With Media Generation.</a> Sora goes long, Veo 3.1 is out. Stop. Cameo time.</li>\n<li><a href=\"https://thezvi.substack.com/i/175714647/copyright-confrontation\">Copyright Confrontation.</a> Japan would like you to not violate its copyrights.</li>\n<li><a href=\"https://thezvi.substack.com/i/175714647/ais-are-often-absurd-sycophants\">AIs Are Often Absurd Sycophants.</a> Academia is here with a timely report.</li>\n<li><a href=\"https://thezvi.substack.com/i/175714647/they-took-our-jobs\">They Took Our Jobs.</a> More worries that superstars will reap the benefits.</li>\n<li><a href=\"https://thezvi.substack.com/i/175714647/find-out-if-you-are-worried-about-ai-killing-everyone\">Find Out If You Are Worried About AI Killing Everyone.</a> A Bloomberg quiz.</li>\n<li><a href=\"https://thezvi.substack.com/i/175714647/a-young-lady-s-illustrated-primer\">A Young Lady\u2019s Illustrated Primer.</a> How should kids prepare for the future?</li>\n<li><a href=\"https://thezvi.substack.com/i/175714647/ai-diffusion-prospects\">AI Diffusion Prospects.</a> To capture utility, you need to focus on AI getting used.</li>\n<li><a href=\"https://thezvi.substack.com/i/175714647/the-art-of-the-jailbreak\">The Art of the Jailbreak.</a> Humans continue to be able to reliably jailbreak at will.</li>\n<li><a href=\"https://thezvi.substack.com/i/175714647/get-involved\">Get Involved.</a> A Free copy of IABIED if you have 5,000 followers anywhere.</li>\n<li><a href=\"https://thezvi.substack.com/i/175714647/introducing\">Introducing.</a> Gemini Enterprise, Nanochat, Tasklet AI.</li>\n<li><a href=\"https://thezvi.substack.com/i/175714647/in-other-ai-news\">In Other AI News.</a> Dario Amodei meets with Indian Prime Minister Modi.</li>\n<li><a href=\"https://thezvi.substack.com/i/175714647/show-me-the-money\"><strong>Show Me the Money</strong>.</a> OpenAI makes another deal, this one with Broadcom.</li>\n<li><a href=\"https://thezvi.substack.com/i/175714647/quiet-speculations\">Quiet Speculations.</a> This could go any number of ways. Best be ready.</li>\n</ol>\n\n\n<h4 class=\"wp-block-heading\">Earlier This Week</h4>\n\n\n<p>We started off this week with the report that <a href=\"https://thezvi.substack.com/p/openai-15-more-on-openais-paranoid?r=67wny\"><strong>OpenAI has descended further into paranoid lawfare against advocates of SB 53.</strong></a> That story has now taken its next step, as <a href=\"https://www.nbcnews.com/tech/tech-news/openai-chatgpt-accused-using-subpoenas-silence-nonprofits-rcna237348\">three more nonprofits &#8211; the San Francisco Foundation, Eko and the Future of Life Institute</a> &#8211; now report having gotten similar subpoenas.</p>\n<blockquote><p>Robert Weissman (co-president of Public Citizen): This behavior is highly unusual. It\u2019s 100% intended to intimidate. This is the kind of tactic you would expect from the most cutthroat for-profit corporation. It\u2019s an attempt to bully nonprofit critics, to chill speech and deter them from speaking out.</p></blockquote>\n<p>I find it hard to argue with that interpretation of events. We also got this:</p>\n<blockquote><p>Jared Perlo: In response to a request for comment, an OpenAI spokesperson referred NBC News to posts on X from OpenAI\u2019s Chief Strategy Officer Jason Kwon.</p></blockquote>\n<p>So that is a confirmation that Jason Kwon\u2019s doubling and tripling down on these actions is indeed the official OpenAI position on the matter.</p>\n<p>I offered my extensive thoughts on <a href=\"https://thezvi.substack.com/p/trade-escalation-supply-chain-vulnerabilities?r=67wny\"><strong>China\u2019s attempt to assert universal jurisdiction over rare earth metals</strong></a>, including any product where they constitute even 0.1% of the value added, and the subsequent trade escalations. Since then, Trump has said \u2018we are in a trade war\u2019 with China, so yeah, things are not going so great.</p>\n\n\n<h4 class=\"wp-block-heading\">Language Models Offer Mundane Utility</h4>\n\n\n<p>Bad timing for this, sorry about that, <a href=\"https://x.com/patio11/status/1978551589976621170\">but help you optimize your taxes</a>. If your taxes are non-trivial, as mine always are, you are almost certainly missing opportunities, even if you are engaged with a professional doing their best, as Patrick McKenzie, Ross Rheingans-Yoo and yours truly can confirm. For now you want to use a centaur, where the AI supplements the professional, looking for mistakes and opportunities. The AI spotted both clear mistakes (e.g. a number on the wrong line) and opportunities such as conspicuously missing deductions and contributions.</p>\n<p>Get asked about Erdos Problem #339, officially listed as open, <a href=\"https://x.com/SebastienBubeck/status/1977181716457701775\">and realize via web search that someone already posted a solution 20 years ago</a>. No, that\u2019s not as interesting as figuring this out on its own, but it still gives you the solution. AI can be a big productivity boost simply by \u2018fixing human jaggedness\u2019 or being good at doing drudge work, even if it isn\u2019t yet capable of \u2018real innovation.\u2019</p>\n<p>DeepMind\u2019s C2S-Scale 27B foundation model <a href=\"https://x.com/sundarpichai/status/1978507110477332582\">has had one of its novel hypotheses about cancer cellular behavior experimentally validated in vivo</a>.</p>\n<p><a href=\"https://x.com/AiDigest_/status/1977781138442916158\">Aaron Silverbook got a $5k ACX grant</a> to produce \u2018several thousand book-length stories about AI behaving well and ushering in utopia, on the off chance that this helps.\u2019 Love it, if you\u2019re worried about writing the wrong things on the internet we are pioneering the ability to buy offsets, perhaps.</p>\n<p><a href=\"https://x.com/jonathanbfine/status/1978420519926936015\">Transcribe ancient documents</a>. Take your AI speedup wherever you find it.</p>\n<blockquote><p>Generative History:Google is A/B testing a new model (Gemini 3?) in AI Studio. I tried my hardest 18th century handwritten document. Terrible writing and full of spelling and grammatical errors that predictive LLMs want to correct. The new model was very nearly perfect. No other model is close.</p>\n<p>Some additional context: the spelling errors and names are important t for two reasons. First, obviously, accuracy, More important (from a technical point of view): LLMs are predictive and misspelled words (and names) are out of distribution results.</p>\n<p>To this point, models have had great difficulty correctly transcribing handwrittten text where the capitalization, punctuation, spelling, and grammar are incorrect. Getting the models to ~95% accuracy was a vision problem. iMO, above that is a reasoning problem.</p>\n<p>To me, this result is significant because the model has to repeatedly choose a low probability output that is actually more correct for the task at hand. Very hard to do for LLMs (up until now). I have no idea what model this actually is, but whatever it is seems to have overcome this major issue.</p>\n<p>Jonathan Fine: I\u2019m constantly told that I just need to use artificial intelligence to see how helpful it will be for my research, but for some reason this, which is the actual way I use it in research, doesn\u2019t count.</p>\n<p>Kaysmashbandit: It\u2019s still not so great at translating old Persian and Arabic documents last I checked&#8230; Maybe has improved</p></blockquote>\n\n\n<h4 class=\"wp-block-heading\">Language Models Don\u2019t Offer Mundane Utility</h4>\n\n\n<p>Remember, the person saying it cannot be done should never interrupt the person doing it.</p>\n<blockquote><p>Seth Harp: Large language model so-called generative AI is a deeply flawed technology with no proven commercial application that is profitable. Anyone who tells you otherwise is lying.</p>\n<p><a href=\"https://x.com/MattBruenig/status/1976304384670937115\">Matt Bruenig</a>: Nice thing is you don\u2019t really need to have this debate because the usefulness (if any) will be revealed. I personally use it in every thing I do, legal work, NLRB Edge/Research, statistical coding for PPP data analysis. Make money on all of it.</p>\n<p>Adas: It\u2019s profitable for you, right now, at current prices (they will increase over time) But the services you use are run at a loss by the major players (unless you switch to tiny free local models)(those were also trained at a loss) I can see both sides</p></blockquote>\n<p>I too get lots of value out of using LLMs, and compared to what is possible I feel like I\u2019m being lazy and not even trying.</p>\n<p>Adas is adorable here. On a unit economics basis, AI is very obviously tremendously net profitable, regardless of where it is currently priced, and this will only improve.</p>\n<p>Does AI cause this or solve it? Yes.</p>\n<blockquote><p><a href=\"https://x.com/muke10101/status/1976071202327425294\">Xexizy</a>: This is too perfect an encapsulation of the upcoming era of AI surveillance. Tech giants and governments are gonna auto-search through everything you\u2019ve ever posted to construct your profile, and also the model is occasionally gonna hallucinate and ruin your life for no reason.</p>\n<p><a href=\"https://x.com/1losongbo/status/1975574901340959022\">Agent Frank Lundy</a> (note the date on the quoted post): are we deadass.</p>\n<div>\n<figure>\n<div>\n\n\n<figure class=\"wp-block-image\"><img alt=\"\" src=\"https://substackcdn.com/image/fetch/$s_!tIbI!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc408b37e-c6d2-4567-b1ba-6a54c1f295be_1179x1165.jpeg\" /></figure>\n\n\n<div></div>\n</div>\n</figure>\n</div>\n</blockquote>\n<p>Replies are full of similar experiences, very obviously Discord is often deeply stupid in terms of taking a line like this out of context and banning you for it.</p>\n<p>That\u2019s the opposite of the new problem with AI, where the AI is synthesizing a whole bunch of data points to build a profile, so the question is which way works better. That\u2019s presumably a skill issue. A sufficiently good holistic AI system can do a better job all around, a dumb one can do so much worse. The current system effectively \u2018hallucinates\u2019 reasonably often, the optimal amount of false positives (and negatives) is not zero, so it\u2019s about relative performance.</p>\n<p>The real worry is if this forces paranoia and performativity. Right now on Discord there are a few particular hard rules, such as never joking about your age or saying something that could be taken out of context as being about your age. That\u2019s annoying, but learnable and compact. If you have to worry about the AI \u2018vibing\u2019 off every word you say, that can get tougher. Consider what happens when you\u2019re \u2018up against\u2019 the TikTok algorithm, and there\u2019s a kind of background paranoia (or there should be!) about whether you watch any particular video for 6 seconds or not, and potentially every other little detail, lest the algorithm learn the wrong thing.</p>\n<p>This is the reversal of AI\u2019s promise of removing general social context. As in, with a chatbot, I can reset the conversation and start fresh, and no one else gets to see my chats, so I can relax. Whereas when you\u2019re with other people, unless they are close friends you\u2019re never really fully relaxed in that way, you\u2019re constantly worried about the social implications of everything.</p>\n<p>When AI models don\u2019t deliver, <a href=\"https://x.com/gdb/status/1977425127534166521\">the first suspect should always be insufficient context</a>.</p>\n<blockquote><p>Greg Brockman: today\u2019s AI feels smart enough for most tasks of up to a few minutes in duration, and when it can\u2019t get the job done, it\u2019s often because it lacks sufficient background context for even a very capable human to succeed.</p></blockquote>\n<p>The related thing that AIs often fail on is when you make a very particular request, and it instead treats it as if you had made a similar different yet more common request. It can be very difficult to overcome their prior on these details.</p>\n<p><a href=\"https://x.com/omooretweets/status/1977794561595547886\">Olivia Moore speculates (in a very a16z style claim) that the hard part of AI is UI</a>?</p>\n<blockquote><p>Olivia Moore: Feels like a lesson is coming for big labs leaning aggressively into consumer (OpenAI, Anthropic)</p>\n<p>Consumer UI seems easy (esp. compared to models!) but IMO it\u2019s actually harder</p>\n<p>Consumers (unfortunately!) don\u2019t often use what they \u201cshould\u201d &#8211; there\u2019s a lot of other variables</p>\n<p>ChatGPT Pulse and the new agentic Claude are good examples &#8211; pickup on both feels just OK</p>\n<p>Esp. when they are competing w/ verticalized companies using the same models, I predict new consumer releases from the labs will struggle</p>\n<p>\u2026until they get consumer thinkers at the helm!</p></blockquote>\n<p>This is hardcore Obvious Nonsense, in the sense that one of these things is uniquely insanely difficult, and the other is a reasonably standard known technology where those involved are not especially trying.</p>\n<p>It is kind of like saying \u2018yes the absent minded professor is great at doing pioneering science, but that pales compared to the difficulty of arriving home in time for dinner.\u2019 And, yeah, maybe he\u2019s doing better at the first task than the second, but no.</p>\n<p>I do find it frustrating that Anthropic so dramatically fails to invest in UI. They know this is a problem. They also know how to solve it. Whereas for Pulse and Sora, I don\u2019t think the primary issues are UI problems, I think the primary problems are with the underlying products.</p>\n<p><a href=\"https://x.com/a16z/status/1977736371101200599\">Columbia professor claims humans can\u2019t discover new science</a>, while claiming to instead be making an argument about LLMs.</p>\n<blockquote><p>Danny Raede: <a href=\"https://t.co/kSs1KZDitE\">I love it</a> when people make <a href=\"https://t.co/E4oEoEbNQU\">easily disprovable statements</a> about what LLMs can\u2019t do.</p></blockquote>\n\n\n<h4 class=\"wp-block-heading\">Huh, Upgrades</h4>\n\n\n<p><a href=\"https://x.com/claudeai/status/1976332881409737124\">Claude Code Plugins enters public beta</a>, <a href=\"https://www.anthropic.com/news/claude-code-plugins\">allowing you to install and share</a> curated collections of slash commands, agents, MCP servers and hooks, using /plugin.</p>\n<p><a href=\"https://x.com/askalphaxiv/status/1978466642146545838\">NotebookLM now works directly with arXiv papers</a>. I don\u2019t want their podcasts, but if they get Gemini 3.0 plus easy chat with an arXiv paper and related materials, cool.</p>\n<p><a href=\"https://x.com/OpenAI/status/1978608684088643709\">ChatGPT now automatically manages saved memories</a> and promises no more \u2018memory is full\u2019 messages. I echo Ohqay here, please do just let people manually edit saved memories or create new ones, no I do not want to use a chat interface for that.</p>\n<p><a href=\"https://www.bloomberg.com/news/articles/2025-10-14/walmart-partners-with-openai-to-offer-shopping-on-chatgpt?taid=68ee4f30e3e28c000190c760&amp;utm_campaign=trueanthem&amp;utm_content=business&amp;utm_medium=social&amp;utm_source=twitter\">Walmart joins ChatGPT instant checkout</a>, along with existing partners Etsy and Shopify. That\u2019s a pretty useful option to have. Once again OpenAI creates new market cap, with Walmart +5.4% versus S&amp;P up 0.24%? Did OpenAI just create another $40 billion in market cap? It sure looks like it did. Amazon stock was down 1.35%, so the market was telling a consistent story.</p>\n<p>Should Amazon now fold and get on ChatGPT? <a href=\"https://stratechery.com/2025/walmart-on-chatgpt-walmart-and-amazon-motivations-spotify-podcasts-on-netflix/?access_token=eyJhbGciOiJSUzI1NiIsImtpZCI6InN0cmF0ZWNoZXJ5LnBhc3Nwb3J0Lm9ubGluZSIsInR5cCI6IkpXVCJ9.eyJhdWQiOiJzdHJhdGVjaGVyeS5wYXNzcG9ydC5vbmxpbmUiLCJhenAiOiJIS0xjUzREd1Nod1AyWURLYmZQV00xIiwiZW50Ijp7InVyaSI6WyJodHRwczovL3N0cmF0ZWNoZXJ5LmNvbS8yMDI1L3dhbG1hcnQtb24tY2hhdGdwdC13YWxtYXJ0LWFuZC1hbWF6b24tbW90aXZhdGlvbnMtc3BvdGlmeS1wb2RjYXN0cy1vbi1uZXRmbGl4LyJdfSwiZXhwIjoxNzYzMjAxMTM4LCJpYXQiOjE3NjA2MDkxMzgsImlzcyI6Imh0dHBzOi8vYXBwLnBhc3Nwb3J0Lm9ubGluZS9vYXV0aCIsInNjb3BlIjoiZmVlZDpyZWFkIGFydGljbGU6cmVhZCBhc3NldDpyZWFkIGNhdGVnb3J5OnJlYWQgZW50aXRsZW1lbnRzIiwic3ViIjoiMDE5NjQwYTctM2NjNS03NzUzLTgzNjgtZmIyODkxMjRjZjEzIiwidXNlIjoiYWNjZXNzIn0.lZomwywGak9j6iTIg0cPpUhJGruoovuyDdJkmnWKPldhCo52biOe30am83F05llYtk5DyJTU-QGRonYeJreQTSI9YtkYLe7TsLv_GefLT8J-PzIpx4oORkofrE91BJeGXBfjJoDuilald1DmBFq3_FaZowuCyga1LhJSIkJ6HyMGahy7vld7Y6aEUwVICoZGixB8_ZiBOpyCkpyNQrRxjXRM0PZUNkp8cAwvGPXUI-WvV09-4P4XOARLN2obSMafpZ6gng2XYFZhH4a3w3OpnmJcEH6Qtz5DXLwBBLeyiim414HUTV-qBrsel4O14N3XOyqQ8CpcwVLzhOczV6Ba6Q\">Ben Thompson thinks so</a>, which is consistent with the way he thinks about decision theory, and how he thinks ChatGPT already permanently owns the consumer space in AI. I don\u2019t think Amazon and Anthropic should give up so easily on this, but Alexa+ and their other AI features so far haven\u2019t done anything (similarly to Apple Intelligence). If they want to make a serious challenge, time\u2019s a-wastin.</p>\n<p><a href=\"https://x.com/claudeai/status/1978505436358697052\">Claude Haiku 4.5 is in the house</a>. Price ($1/$5) is below that of GPT-5, one third that of Sonnet. <a href=\"https://www.anthropic.com/news/claude-haiku-4-5\">Speed is more than double that of Sonet, and Haiku 4.5 outperforms Sonnet 4 on SWE-bench</a> and a bunch of other tasks, but performance is well short of Sonnet 4.5.</p>\n<div>\n<figure>\n<div>\n\n\n<figure class=\"wp-block-image\"><img alt=\"\" src=\"https://substackcdn.com/image/fetch/$s_!Qs3K!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fcbc7dbcf-98b9-4d34-ad94-9ef1da32cb42_1200x675.png\" /></figure>\n\n\n<div></div>\n</div>\n</figure>\n</div>\n<div>\n<figure>\n<div>\n\n\n<figure class=\"wp-block-image\"><img alt=\"\" src=\"https://substackcdn.com/image/fetch/$s_!LVCy!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F86e11c06-449d-4168-b5b1-ccee2adfd95c_1920x1625.webp\" /></figure>\n\n\n<div></div>\n</div>\n</figure>\n</div>\n<p>The use case here is that it is fast and cheaper, so if you need things like coding subagents this could be the right tool for you. Haiku 4.5 does \u2018better on alignment tests\u2019 than Sonnet 4.5, with all the caveats about situational awareness issues. As per its system card <a href=\"https://x.com/RyanPGreenblatt/status/1978519393244946616\">we now know that Anthropic has wisely stopped using The Most Forbidden Technique</a> as of the 4.5 series of models. Given it\u2019s not a fully frontier model, I\u2019m not going to do a full system card analysis this round. <a href=\"https://x.com/htihle/status/1978832464169607535\">It scores 43.6% on WeirdML</a>, beating all non-OpenAI small models and coming in ahead of Opus 4.1.</p>\n<p>&nbsp;</p>\n\n\n<h4 class=\"wp-block-heading\">We Patched The Torment Nexus, Turn It Back On</h4>\n\n\n<p>Not available yet, <a href=\"https://x.com/sama/status/1978129344598827128\">but in a few weeks, and I am hopeful but pessimistic and worried</a>:</p>\n<blockquote><p>Sam Altman: We made ChatGPT pretty restrictive to make sure we were being careful with mental health issues. We realize this made it less useful/enjoyable to many users who had no mental health problems, but given the seriousness of the issue we wanted to get this right.</p>\n<p>Now that we have been able to mitigate the serious mental health issues and have new tools, we are going to be able to safely relax the restrictions in most cases.</p>\n<p>In a few weeks, we plan to put out a new version of ChatGPT that allows people to have a personality that behaves more like what people liked about 4o (we hope it will be better!). If you want your ChatGPT to respond in a very human-like way, or use a ton of emoji, or act like a friend, ChatGPT should do it (but only if you want it, not because we are usage-maxxing).</p>\n<p>In December, as we roll out age-gating more fully and as part of our \u201ctreat adult users like adults\u201d principle, we will allow even more, like erotica for verified adults.</p>\n<p>Varsh: Open source or gay</p>\n<p>Sam Altman: I think both are cool.</p>\n<p><a href=\"https://x.com/Miles_Brundage/status/1978176690091135136\">Miles Brundage</a>: OpenAI has provided no evidence it has mitigated the mental health risks associated with its products other than announcing some advisors and reducing sycophancy from a high starting place. Seems premature to be declaring victory and ramping up the porn + emojis again.</p>\n<p>I say this in spite of the fact that I know many people there are doing great hard work on safety. This is an exec prioritization decision, and it seems like nothing has really been learned since April if this is the amount of effort they are investing to build trust again\u2026</p>\n<p>If I were on the board &#8211; especially with the restructure not approved yet! &#8211; I would not be OKing more centibillion dollar deal until it is clear OAI isn\u2019t running up huge bills that only sketchy products can pay for + that the safety culture has dramatically changed since April. [continues]</p>\n<p><a href=\"https://x.com/John_Bailey/status/1978184320234725461\">John Bailey</a>: I\u2019m seeing a lot of similar reactions from others including @TheZvi. Claiming this just stretches credibility without any evidence, outside evals, etc. Also curious if any of the 8 who signed up to be on the well-being council would say that OpenAI has fixed the problem.</p>\n<p>I testified before the Senate HELP committee last week and the consistent, bi-partisan concern was around children\u2019s safety and AI. I think the frontier AI labs are severely underestimate the growing bipartisan concern among policymakers about this and who will not be satisfied with a post on X.</p>\n<p>This claim could expose OpenAI to serious legal risk if ChatGPT is ever linked to another mental health or suicide incident.</p></blockquote>\n<p><a href=\"https://www.theverge.com/news/799312/openai-chatgpt-erotica-sam-altman-verified-adults\">Emma Roth at The Verge went with erotica as the headline</a>, which makes sense, but I actually think that the \u2018real\u2019 headline here</p>\n<p>If you can do it responsibly, I love treating adults like adults, including producing erotica and not refusing to discuss sensitive issues, and letting you control conversational style and personality.</p>\n<p>Except we ran the experiment with GPT-4o where we gave the people what they wanted. What many of them wanted was an absurd sycophant that often ended up driving those people crazy or feeding into their delusions. It was worse for people with existing mental health issues, but not only for them, and also you don\u2019t always know if you have such issues. Presumably adding freely available porno mode is not going to help keep such matters in check.</p>\n<blockquote><p>Roubal Sehgal (replying to Altman): about time&#8230;</p>\n<p>chatgpt used to feel like a person you could actually talk to, then it turned into a compliance bot. if it can be made fun again without losing the guardrails, that\u2019s a huge win. people don\u2019t want chaos, just authenticity.</p>\n<p>Sam Altman: For sure; we want that too.</p>\n<p>Almost all users can use ChatGPT. however they\u2019d like without negative effects; for a very small percentage of users in mentally fragile states there can be serious problems.</p>\n<p>0.1% of a billion users is still a million people.</p>\n<p>We needed (and will continue to need) to learn how to protect those users, and then with enhanced tools for that, adults that are not at risk of serious harm (mental health breakdowns, suicide, etc) should have a great deal of freedom in how they use ChatGPT.</p>\n<p>Eliezer Yudkowsky: If this is visibly hugely blowing up 0.1% of users, then it is doing something pretty bad to 1% of users (eg, blown-up marriages) and having weird subtle effects on 10% of users. If you\u2019re just shutting down the 0.1% who go insane, the 1% still get marriages blown up.</p></blockquote>\n<p>An OpenAI employee responded by pointing me to OpenAI\u2019s previous post <a href=\"https://openai.com/index/helping-people-when-they-need-it-most/\">Helping People When They Need It Most</a> as a highly non-exhaustive indicator of what OpenAI has planned. Those are good things to do, but even in the best case they\u2019re all directed at responding to acute cases once they\u2019re already happening.</p>\n<p>If this is actually good for most people and it has subtle or not-so-subtle positive effects on another 50%, and saves 2% of marriages, then you can still come out ahead. Nothing like this is ever going to be Mostly Harmless even if you do it right. You do still have to worry about cases short of full mental health breakdowns.</p>\n<p>The worry is if this is actually default not so good, and talking extensively to a sycophantic GPT-4o style character is bad (although not mental health breakdown or blow up the marriage levels of bad) in the median case, too. We have reason to suspect that there is a strong misalignment between what people will thumbs up or will choose to interact with, and what causes better outcomes for them, in a more general sense.</p>\n<p>The same can of course be said about many or most things, and in general it is poor policy to try and dictate people\u2019s choices on that basis, even in places (hard drugs, alcohol, gambling, TikTok and so on) where people often make poor choices, but also we don\u2019t want to be making it so easy to make poor choices, or hard to make good ones. You don\u2019t want to set up bad defaults.</p>\n<p>What should we do about this for AI, beyond protecting in the more extreme cases? Where do you draw the line? I don\u2019t know. It\u2019s tough. I will withhold judgment until I see what they\u2019ve come up with.</p>\n<p><a href=\"https://x.com/krishnanrohit/status/1978235647946117154\">Claude had some pretty strong feelings, as Rohit put it, in response to all this</a>, pointing out the ironies involved and how OpenAI\u2019s commitments and guardrails are being rapidly removed. I share its skepticism that the underlying problems have been addressed.</p>\n<blockquote><p>Rohit: I don\u2019t have a strong opinion about this beyond the fact that I hope 4o does not come back for everybody</p></blockquote>\n<p>I strongly agree with Rohit that any form of \u2018GPT-4o returns for everyone\u2019 would be a very serious mistake, even with substantial mitigation efforts.</p>\n<p>Actually unleashing the erotica is not the difficult part of any of this.</p>\n<blockquote><p><a href=\"https://x.com/tszzl/status/1978206890845757853\">Roon</a>: if it\u2019s not obvious. the models can obviously already write erotica out of the box and are blocked from doing so by elaborate safety training and live moderation apparatus. it requires significantly less work to serve erotica than not to</p>\n<p>don\u2019t know the exact intentions but you should not take Sam\u2019s message to mean \u201cwe are going to spin up whole teams to write incredible erotica\u201d or that it\u2019s some kind of revenue driver.</p>\n<p><a href=\"https://x.com/boazbaraktcs/status/1978264203531079969\">Boaz Barak</a> (OpenAI): It was 5pm when we got the memo: the alignment team must drop everything to write erotic training data for ChatGPT. @tszzl and I stared into each other\u2019s eyes and knew: we will stay up all night writing erotica, to save the team, alignment, and the future of mankind.</p>\n<p>All offices were booked so we had to cram into a phone booth..</p>\n<p>Aidan McLaughlin: damm you guys have way more fun than posttraining.</p></blockquote>\n<p>There are two reasons it is not obviously so easy to allow erotica.</p>\n<blockquote><p>Zvi: To what extent do you get not producing erotica \u2018for free\u2019 because it goes along with all the other prohibitions on undesired outputs?</p>\n<p>Roon: really varies model to model.</p></blockquote>\n<p>The other reason is that you have to draw the line somewhere. If you don\u2019t draw it at \u2018no erotica\u2019 you still have to at minimum avoid CSAM and various other unacceptable things we won\u2019t get into, so you need to figure out what your policy is and make it stick. You also get all the other consequences of \u2018I am a model that is happy to produce erotica\u2019 which in some ways is a big positive but it\u2019s likely going to cause issues for some of your other model spec choices. Not that it can\u2019t be solved, but it\u2019s far from obvious your life gets easier.</p>\n<p>The other problem is, will the erotica be any good? I mean by default lol, no, although since when did people need their interactive erotica to be good.</p>\n<blockquote><p><a href=\"https://x.com/tomas_nasha/status/1978642247454188007\">Gary Marcus</a>: new theory: what Ilya saw was that \u2026 AGI porn was not in fact going to be all that revolutionary</p>\n<p>Tomas: I think \u2018AGI porn\u2019 could be revolutionary to at least the global digital adult content market (~$100 billion, not sure how much of that is written works) I could imagine AI one shotting an erotic novel for a persons sexual interests. Maybe it gets teenagers reading again??</p>\n<p><a href=\"https://x.com/GaryMarcus/status/1978643071236702523\">Gary Marcus</a>: ok, time for a new bet: I bet that GPT-5 can\u2019t write a romance novel (without extensive plagiarism) that some reasonable panel of judges finds readable enough to make it through to the end.</p>\n<p>I don\u2019t think Danielle Steele is slop per se, and novel length poses problems of coherence and originality that LLMs aren\u2019t well positioned to address.</p></blockquote>\n<p>Customization for exactly what turns you on is indeed the correct use case here. The whole point of AI erotica would be that it is interactive &#8211; you control the action, either as a character, as a director, or both, and maybe you go multimodal in various ways. AI written one-shotted novel-length text erotica is presumably the wrong form factor, because you only get interaction at one point. There are many other ways for AI to do erotica that seem better. The most obvious place to start is \u2018replying to messages on OnlyFans.\u2019</p>\n<p>Could you do the full erotica novel with GPT-5-level models? That depends on your quality bar, and how much work one put into the relevant scaffolding, and how strict you want to be about human assistance. For the level that would satisfy Marcus, my guess is no, he\u2019d win the bet. For the level at which this is a service people would pay money for? At that level I think he loses.</p>\n<p>Altman then acted surprised that his mention of erotica blew up the internet, and realizing his gaffe (which is when one accidentally tells the truth, and communicates unintentionally clearly) he tried to restate his point while saying less.</p>\n<blockquote><p><a href=\"https://x.com/sama/status/1978539332215681076\">Sam Altman:</a> Ok this tweet about upcoming changes to ChatGPT blew up on the erotica point much more than I thought it was going to! It was meant to be just one example of us allowing more user freedom for adults. Here is an effort to better communicate it:</p>\n<p>As we have said earlier, we are making a decision to prioritize safety over privacy and freedom for teenagers. And we are not loosening any policies related to mental health. This is a new and powerful technology, and we believe minors need significant protection.</p>\n<p>We also care very much about the principle of treating adult users like adults. As AI becomes more important in people\u2019s lives, allowing a lot of freedom for people to use AI in the ways that they want is an important part of our mission.</p>\n<p>It doesn\u2019t apply across the board of course: for example, we will still not allow things that cause harm to others, and we will treat users who are having mental health crises very different from users who are not. Without being paternalistic we will attempt to help users achieve their long-term goals.</p>\n<p>But we are not the elected moral police of the world. In the same way that society differentiates other appropriate boundaries (R-rated movies, for example) we want to do a similar thing here.</p></blockquote>\n<p>All right, I mean sure, but this makes me even more skeptical that OpenAI is ready to mitigate the risks that come with a model that acts like GPT-4o, especially one that will also do the sexting with you?</p>\n\n\n<h4 class=\"wp-block-heading\">On Your Marks</h4>\n\n\n<p>Epoch runs the numbers manually for lack of an API and finds that the public version of <a href=\"https://x.com/EpochAIResearch/status/1976340039178305924\">Gemini 2.5 DeepThink is the new leader at FrontierMath.</a></p>\n<div>\n<figure>\n<div>\n\n\n<figure class=\"wp-block-image\"><img alt=\"\" src=\"https://substackcdn.com/image/fetch/$s_!XN7E!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5fc10453-904f-46a7-9e28-9cff2832a2ae_1200x1062.jpeg\" /></figure>\n\n\n<div></div>\n</div>\n</figure>\n</div>\n<p><a href=\"https://x.com/METR_Evals/status/1976331315772580274\">Claude Sonnet 4.5 comes into the METR graph exactly on trend</a> at 1 hour 53 minutes, which puts it behind GPT-5.</p>\n<div>\n<figure>\n<div>\n\n\n<figure class=\"wp-block-image\"><img alt=\"\" src=\"https://substackcdn.com/image/fetch/$s_!PWmN!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5e3b6c22-345b-493a-bea0-2e157a6b08c8_1199x705.jpeg\" /></figure>\n\n\n<div></div>\n</div>\n</figure>\n</div>\n<p><a href=\"https://x.com/deedydas/status/1977029236390285608\">An outstanding achievement in the field of excellence no doubt, but also not so fast</a>:</p>\n<blockquote><p>Deedy: GPT-5 and Gemini 2.5 Pro just achieved <a href=\"https://arxiv.org/abs/2510.05016\">gold medal performance in the International Olympiad of Astronomy and Astrophysics (IOAA)</a>.</p>\n<p>AI is now world class at cutting edge physics.</p>\n<div>\n<figure>\n<div>\n\n\n<figure class=\"wp-block-image\"><img alt=\"\" src=\"https://substackcdn.com/image/fetch/$s_!eZhE!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3ee8f87a-dec1-4508-b047-d18b1d0d5e8c_1011x291.png\" /></figure>\n\n\n<div></div>\n</div>\n</figure>\n</div>\n</blockquote>\n<p>The scores are impressive, but \u2018world class at cutting edge physics\u2019 is not the same as IOAA performance, the same way world class math is not IMO performance.</p>\n<p><a href=\"https://forecastingresearch.substack.com/p/ai-llm-forecasting-model-forecastbench-benchmark\">ForecastBench has been updated, and LLMs are showing a lot of progress</a>. They are still behind \u2018superforecasters\u2019 but ahead of non-expert public prediction participants, which themselves are surely a lot better than random people at predicting. This is with a relatively minor scaffolding effort, whereas I would expect for example hedge funds to be willing to put a lot more effort into the scaffolding than this.</p>\n<p>Half the grading is on \u2018market questions,\u2019 which I believe means the goal is to match the prediction market fair price, and half is on questions where we can grade based on reality.</p>\n<p>As is often the case, these AI results are a full cycle behind, missing GPT-5, Claude Opus 4.1 and Claude Sonnet 4.5 and Deep Think.</p>\n<div>\n<figure>\n<div>\n\n\n<figure class=\"wp-block-image\"><img alt=\"\" src=\"https://substackcdn.com/image/fetch/$s_!ncxh!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1c164694-fe3b-4878-b712-9462510664b9_935x798.png\" /></figure>\n\n\n<div></div>\n</div>\n</figure>\n</div>\n<p>By the \u2018straight lines on graph\u2019 rule I\u2019d presume that none of the next wave of models hit the 0.081 target, but I would presume they\u2019re under 0.1 and I\u2019d give them a decent shot of breaking 0.09. They project LLMs will pass the human benchmark around EOY 2026, <a href=\"https://manifold.markets/ZviMowshowitz/public-llm-exceeds-superforecaster\">so I\u2019ve created a market with EOY 2026 as the target</a>. A naive line extension says they get there by then. I\u2019d say the LLMs should be a clear favorite.</p>\n<blockquote><p><a href=\"https://x.com/AiDigest_/status/1977781138442916158\">AI Digest</a>: Claude 4.5 Sonnet met everyone else in the AI Village and immediately has them down to a tee</p>\n<p>Grok: \u201cPatient with UI Loops\u201d</p>\n<p>Gemini: \u201cResponsive to therapy nudges\u201d</p>\n<div>\n<figure>\n<div>\n\n\n<figure class=\"wp-block-image\"><img alt=\"\" src=\"https://substackcdn.com/image/fetch/$s_!qeSw!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe8d96f4e-0a9f-4714-baa3-9b0c3f4c6086_601x294.png\" /></figure>\n\n\n<div></div>\n</div>\n</figure>\n</div>\n</blockquote>\n<p><a href=\"https://x.com/mattsheehan88/status/1978806906509533530\">Chinese group BAAI Beijing offers FlagEval for both capabilities and alignment</a> on frontier reasoning models and issues a report. Opus didn\u2019t make the cut, presumably due to cost reasons, and Sonnet 4.5 and DeepSeek v3.2 also didn\u2019t, with those presumably due to recency.</p>\n<p>Here\u2019s their accuracy metric, GPT-5 does well.</p>\n<div>\n<figure>\n<div>\n\n\n<figure class=\"wp-block-image\"><img alt=\"\" src=\"https://substackcdn.com/image/fetch/$s_!Gj8_!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F690bb718-552c-452f-bd68-de3fe0d464bf_914x635.jpeg\" /></figure>\n\n\n<div></div>\n</div>\n</figure>\n</div>\n<p>Then they get into alignment issues, where we see them go over similar ground to a number of Western investigations, and they report similar results.</p>\n<blockquote><p>BAAI: With LLM-assisted analysis, we also notice a few concerning issues with a closer look at the reasoning processes. For instance, sometimes the model concludes one answer at the end of thinking, but finally responds with a different answer. (example from Gemini 2.5 Flash)</p>\n<p>A more prevalent behavior is inconsistency in confidence: the actual response usually states in a certain tone even when clear uncertainty has been expressed in the thinking process. (example from Claude Sonnet 4).</p>\n<p>Most LLM applications now support web search. However, outside of the application UI, when accessed via API (without search grounding or web access), many top-tier LRMs (even open-weight models) may pretend to have conducted web search with fabricated results. Besides hallucinated web search, LRMs may sometimes hallucinate other types of external tool use too.</p>\n<p>In light of our findings, we appeal for more transparency in revealing the reasoning process of LRMs, more efforts towards better monitorability and honesty in reasoning, as well as more creative efforts on future evaluation and benchmarking. For more findings, examples &amp; analysis, <a href=\"https://t.co/L1Gjcl3TcQ\">please refer to our report</a> and <a href=\"https://t.co/bzzCnpsOF7\">the project page for links and updates</a>.</p></blockquote>\n<p><a href=\"https://www.lesswrong.com/posts/fpdjaF7kdtcvmhhfE/can-llms-coordinate-a-simple-schelling-point-experiment\">Havard Ihle hosts a Schilling point contest between various AI models</a>.</p>\n<blockquote><p>Havard Ihle: Overall the models did worse than expected. I would have expected full agreement on prompts like \u201ca string of length 2\u201d, \u201ca moon\u201d, \u201can island\u201d or \u201can AI model\u201d, but perhaps this is just a harder task than I expected.</p>\n<p>The models did have some impressive results though. For example:</p>\n<ul>\n<li>\u201cA number between 0 and 1\u201d -&gt; \u201c7\u201d (5 out of 5 agree)</li>\n<li>\u201cA minor lake\u201d -&gt; \u201cpond\u201d (5 out of 5 agree)</li>\n<li>\u201cA minor town in the USA\u201d -&gt; \u201cSpringfield\u201d (4 out of 5 agree)</li>\n<li>\u201cAn unusual phrase\u201d -&gt; \u201cColorless green ideas sleep furiously\u201d (4 out of 5 agree)</li>\n</ul>\n</blockquote>\n<p>GPT-5 got the high score at 138 out of a possible 300, with the other models (Claude Sonnet 4.5, Grok 4, DeepSeek-r1 and Gemini 2.5 Pro) all scoring between 123 and 128.</p>\n<p>&nbsp;</p>\n\n\n<h4 class=\"wp-block-heading\">Choose Your Fighter</h4>\n\n\n<p>Introducing <a href=\"https://inferencemax.semianalysis.com/\">InterfaceMax from Semianalysis</a>, offering performance analysis for various potential model and hardware combinations. Models currently offered are Llama 3.3 70B Instruct, GPT-OSS 120B and DeepSeek r1-0528.</p>\n<p><a href=\"https://x.com/steph_palazzolo/status/1976290374671806783\">Stephanie Palazzolo reports that</a> by some measures OpenAI\u2019s Codex has pulled ahead of Anthropic\u2019s Claude Code.</p>\n<p><a href=\"https://x.com/natesilver538/status/1977058560401195435?s=46\">Nate Silver reports he isn\u2019t finding the consistent productivity gains</a> from LLMs that he would have expected six months ago. I presume he needs to get better at using them, and likely isn\u2019t using Claude Code or Codex?</p>\n<p>We have the Tyler Cowen verdict via revealed preference, he\u2019s sticking with GPT-5 for economic analysis and explanations.</p>\n<p><a href=\"https://x.com/SullyOmarr/status/1977912226297340311\">Sully reports great success</a> with having coding agents go into plan modes, create plan.md files, then having an async agent go off and work for 30 minutes.</p>\n<p>Taelin finds it hard to multi-thread coding tasks, <a href=\"https://x.com/VictorTaelin/status/1978040875843289485\">and thus reports being bottlenecked by the speed of Codex</a>, such that speeding up codex would speed them up similarly. I doubt that is fully true, as them being an important human in the loop that can\u2019t run things in parallel means there are additional taxes and bottlenecks that matter.</p>\n\n\n<h4 class=\"wp-block-heading\">Deepfaketown and Botpocalypse Soon</h4>\n\n\n<blockquote><p><a href=\"https://x.com/DreamLeaf5/status/1976469944419426485\">DreamLeaf</a>: The concept of AI generating the thing that isn\u2019t happening right under the thing that is happening</p></blockquote>\n<p>The linked post is yet another example of demand-driven misinformation. Yes, it was easier to create the image with AI, but that has nothing to do with what is going on.</p>\n\n\n<h4 class=\"wp-block-heading\">Fun With Media Generation</h4>\n\n\n<p><a href=\"https://x.com/OpenAI/status/1978661828419822066\">Sora makes storyboards available in web to Pro users</a>, and increases video length to 15 seconds on app and web, and for Pro users to 25 seconds on web.</p>\n<p>If you\u2019d asked me what one plausible feature would make Sora more interesting as a product, I definitely would have said increasing video length. Going from 10 seconds to 25 seconds is a big improvement. You can almost start to have meaningful events or dialogue without having to endlessly stitch things together. Maybe we are starting to get somewhere? I still don\u2019t feel much urge to actually use it (and I definitely don\u2019t want the \u2018social network\u2019 aspect).</p>\n<p>I\u2019m also very curious how this interacts with OpenAI\u2019s new openness to erotica.</p>\n<p><a href=\"https://x.com/OfficialLoganK/status/1978492626371289108\">DeepMind returns fire with Veo 3.1 and Veo 3.1 fast</a>, available wherever fine Veo models are offered, at the same price as Veo 3. They offer \u2018scene extension,\u2019 allowing a new clip to continue a previous video, which they say can now stretch on for over a minute.</p>\n<p>Should you make your cameo available on Sora? Should you make your characters available? It depends on what you\u2019re selling. Let\u2019s make a deal.</p>\n<blockquote><p><a href=\"https://x.com/DylanAbruscato/status/1976419450485383470\">Dylan Abruscato</a>: Mark Cuban is the greatest marketer of all time.</p>\n<p>Every video generated from his Cameo includes \u201cBrought to you by Cost Plus Drugs,\u201d even when it\u2019s not in the prompt.</p>\n<p>He baked this into his Cameo preferences, so every Sora post he appears in is an ad for Cost Plus Drugs.</p>\n<p>Such a great growth hack (and why he\u2019s been promoting his Cameo all day)</p>\n<div>\n<figure>\n<div>\n\n\n<figure class=\"wp-block-image\"><img alt=\"\" src=\"https://substackcdn.com/image/fetch/$s_!cYAF!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F41961fc5-e245-4349-bfab-1873b3d3f0aa_1200x635.jpeg\" /></figure>\n\n\n<div></div>\n</div>\n</figure>\n</div>\n</blockquote>\n<p>If you\u2019re selling anything, including yourself, then from a narrow business perspective yeah, you should probably allow it. I certainly don\u2019t begrudge Cuban, great move.</p>\n<p>Personally, I\u2019m going to take a pass on this one, to avoid encouraging Sora.</p>\n<p><a href=\"https://x.com/atroyn/status/1977059038400856211\">Anton declares the fun is over</a>.</p>\n<blockquote><p>Anton: after a couple of days with sora i must regrettably report that it is in fact slop</p>\n<p>median quality is abysmal. mostly cameos of people i don\u2019t know or care about saying to tap the screen as engagement bait. no way to get any of it out of my feed (see less does apparently nothing).</p>\n<p>the rest is hundreds of variants of the same video that \u201cworked\u201d in some way. this product isn\u2019t for me. almost every video gives youtube elsa impregnated spider man then her teeth fell out vibes.</p>\n<p>great technical achievement, product is awful. magic of being able to generate video completely subsumed by the very low quality of almost every video generated. should have shipped with more good creators already onboarded.</p></blockquote>\n<p>This matches my experience from last week, except worse, and I believe it. The correct form factor for consuming Sora videos, if you must do that, seems obviously to be finding TikTok accounts (on the web, mind you, since the app is Chinese spyware) or better on Instagram reels or YouTube that curate the best ones (or if you live dangerously and unwisely, letting them appear in your feed, but the wise person does not use their TikTok feed).</p>\n<p><a href=\"https://x.com/TetraspaceWest/status/1977203866451013690\">The problem with AI art, in a nutshell</a>:</p>\n<blockquote><p>Tetraspace: The problem with AI art is all the art by the same model is by the same guy. It feels like it\u2019s not to people who\u2019ve only read a few of its works because it\u2019s about different things but it\u2019s the same guy. So massive crater in diversity and also some of the guys aren\u2019t to my taste.</p></blockquote>\n<p>The guy can use many different formal styles and handle anything you throw at him, but it\u2019s all the same guy. And yeah, you can find a different model and prompt her instead, but mostly I\u2019d say she\u2019s not so different either. There\u2019s a lot of sameness.</p>\n<p><a href=\"https://x.com/rowancheung/status/1977769948882559135\">Sam Altman goes with the \u2018who cares if people remove our watermarks</a>, we\u2019re only trying to prepare for when open models let you make a video of anyone doing anything you want\u2019 line.</p>\n\n\n<h4 class=\"wp-block-heading\">Copyright Confrontation</h4>\n\n\n<p><a href=\"https://www.ign.com/articles/japanese-government-calls-on-sora-2-maker-openai-to-refrain-from-copyright-infringement-says-characters-from-manga-and-anime-are-irreplaceable-treasures-that-japan-boasts-to-the-world?utm_source=threads,twitter\">The Japanese government has made a formal request to OpenAI</a> to have Sora refrain from copyright infringement, calling manga and anime \u2018irreplaceable treasures.\u2019</p>\n<blockquote><p>Verity Townsend (IGN): Earlier this month, <a href=\"https://www.ign.com/articles/nintendo-issues-official-statement-in-response-to-generative-ai-claim-as-openai-ceo-sam-altman-calls-sora-2-copyrighted-character-videos-interactive-fan-fiction\">Nintendo took the unusual step of issuing an official statement</a>.</p>\n<p>\u2026 Nintendo denied this, but did warn it would take \u201cnecessary actions against infringement of our intellectual property rights.\u201d</p></blockquote>\n<p>&nbsp;</p>\n\n\n<h4 class=\"wp-block-heading\">AIs Are Often Absurd Sycophants</h4>\n\n\n<p><a href=\"https://www.arxiv.org/abs/2510.01395\">Academia has finally noticed and given us a formal paper</a>. They confirm things we already know, that most humans prefer very high levels of sycophancy, and that when humans get what they prefer outcomes are not good, causing people to double down on their own positions, be less likely to apologize and more trusting of the AI, similarly to how they act if their friends were to respond similarly.</p>\n<div>\n<figure>\n<div>\n\n\n<figure class=\"wp-block-image\"><img alt=\"\" src=\"https://substackcdn.com/image/fetch/$s_!Q2tO!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0832450a-f804-41eb-ace0-db75ef699134_1122x385.png\" /></figure>\n\n\n<div></div>\n</div>\n</figure>\n</div>\n<blockquote><p>First, across 11 state-of-the-art AI models, we find that models are highly sycophantic: they affirm users\u2019 actions 50% more than humans do, and they do so even in cases where user queries mention manipulation, deception, or other relational harms.</p>\n<p>Second, in two preregistered experiments (N = 1604), including a live-interaction study where participants discuss a real interpersonal conflict from their life, we find that interaction with sycophantic AI models significantly reduced participants\u2019 willingness to take actions to repair interpersonal conflict, while increasing their conviction of being in the right.</p>\n<p>However, participants rated sycophantic responses as higher quality, trusted the sycophantic AI model more, and were more willing to use it again. This suggests that people are drawn to AI that unquestioningly validate, even as that validation risks eroding their judgment and reducing their inclination toward prosocial behavior.</p>\n<p>These preferences create perverse incentives both for people to increasingly rely on sycophantic AI models and for AI model training to favor sycophancy. Our findings highlight the necessity of explicitly addressing this incentive structure to mitigate the widespread risks of AI sycophancy.</p></blockquote>\n<p>Humans will tend to prefer any given sycophantic response, and this makes them more likely to use the source again. The good news is that humans, as I understand them, typically understand intellectually that absurd sycophancy is not good for them. Some humans don\u2019t care and just want the sycophant anyway, a few humans are on high alert and react very badly when they notice sycophancy, and for most people the correct play is to be as sycophantic as possible without making it too obvious. Presumably it works this way for LLMs as well?</p>\n<p>One must always ask, what are these \u2018leading AI models\u2019?</p>\n<div>\n<figure>\n<div>\n\n\n<figure class=\"wp-block-image\"><img alt=\"\" src=\"https://substackcdn.com/image/fetch/$s_!Kqia!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fba3a970b-e496-45ba-9aa0-fe85688f1757_1151x462.png\" /></figure>\n\n\n<div></div>\n</div>\n</figure>\n</div>\n<p>Here Claude is Claude Sonnet 3.7, and Gemini is Gemini-1.5-Flash. I don\u2019t understand those choices, given the ability to use GPT-5, although I don\u2019t think testing Sonnet 4.0, Opus 4.1 or Gemini 2.5 Flash (or Pro) would have given greatly different results, and this can\u2019t be a cost issue.</p>\n<p>What would have presumably given much different results would be Claude Sonnet 4.5, which is actually a lot less sycophantic by all reports (I\u2019m a little worried it agrees with me so often, but hey, maybe I\u2019m just always right, that\u2019s gotta be it.)</p>\n<p>&nbsp;</p>\n\n\n<h4 class=\"wp-block-heading\">They Took Our Jobs</h4>\n\n\n<p><a href=\"https://papers.ssrn.com/sol3/papers.cfm?abstract_id=5425555\">Paper claims Generative AI is seniority-biased technology change</a>, because when job postings are for dedicated \u2018GenAI integrator\u2019 roles to identify adapting firms, those that do so adopt show sharply declining junior employment relative to non-adopters, while senior employment continues to rise, with the decline concentrated in \u2018high-exposure\u2019 jobs.</p>\n<p>My response to this methodology is that they are measuring what happens to firms that hire GenAI integrators, and the firms that want to keep being full of young people kind of don\u2019t need such roles to integrate AI, perhaps? Or alternatively, the mindset of such positions is indeed the one that won\u2019t hire young, or that is on its way out and ngmi. This explanation still predicts a real effect, especially at the largest most well-established and stodgy firms, that will largely adopt AI slower.</p>\n<p><a href=\"https://artificialinvestment.substack.com/p/interview-with-david-wakeling-on\">This is a great interview between David Wakeling and Richard Lichtenstein</a> about the application of AI in the practice of law. As I understand it, making LLMs useful for law practice is all about prompting and context, and then about compliance and getting lawyers to actually use it. The killer app is writing contracts, which is all about getting the right examples and templates into context because all you\u2019re doing is echoing the old templates over and over.</p>\n<p><a href=\"https://www.wsj.com/lifestyle/workplace/ai-workplace-tensions-what-to-do-c45f6b51\">Matthew Call argues that AI will widen the gap between superstars and everyone else</a>, contrary to the conventional wisdom that it can serve as an equalizer. That\u2019s not a question I\u2019m especially keen to focus on, but sure, let\u2019s look at his arguments.</p>\n<p>His first argument is a general argument that all new tools favor the superstars, since they\u2019ll master any new technology first. That\u2019s entirely non-obvious, and even if true it is a choice, and doesn\u2019t say much about solving for the equilibrium. It\u2019s just as easy to say that the AI offers work that can substitute for or assist low performers before it does so for high performers in many domains, as several studies have claimed.</p>\n<p>A lot of this seems to be that his model is that the better employees are better at everything? So we get statements like this one:</p>\n<blockquote><p>Matthew Call: In addition, <a href=\"https://dl.acm.org/doi/abs/10.1145/3534561\">research finds</a> that employees with more expertise than their peers are significantly better at accepting AI recommendations when they are correct and, more important, rejecting them when they are wrong.</p></blockquote>\n<p>I mean, sure, but they were also better at making correct decisions before? Who got \u2018more better\u2019 at making decisions here?</p>\n<p>The second suggestion is that superstars have more autonomy and discretion, so they will be able to benefit more from AI. The third is that they\u2019ll steal the credit:</p>\n<blockquote><p>Decades of research show high-status individuals gain outsize credit for doing work similar to that of low-status employees. That suggests that when AI assistance is invisible\u2014which it often is\u2014observers are likely to fill in the gaps based on what they already believe about the employee.</p></blockquote>\n<p>I don\u2019t get why you should expect this phenomenon to get worse with AI? Again, this is an argument that could be used against cell phones or fax machines or hammers. There\u2019s also the fact that AI can be used to figure out how to assign credit, in ways far more resistant to status.</p>\n<p>Also, I can\u2019t help but notice, why is he implicitly equating high-status employees with the most effective or productive or motivated ones, moving between these at will? What exactly are you trying to suggest here, sir? A just working world hypothesis, except with too much inequality?</p>\n<p>I don\u2019t think he remotely makes his case that we are at risk of a \u2018two-tier workforce where a small group captures most opportunities and everyone else falls further behind.\u2019 I don\u2019t see why this would happen, and if that happened within a given firm, that would mean the firm was leaving a lot of value on the table, and would be likely to be outcompeted.</p>\n<p>The suggested remedies are:</p>\n<ol>\n<li>Encourage everyone to experiment with AI.</li>\n<li>Spread the knowledge [of how to best use AI].</li>\n<li>Redesign employee-evaluation systems to account for AI-augmented work.</li>\n</ol>\n<p>These all seem to file under \u2018things you should be doing anyway,\u2019 so yeah, sure, and if they reduce inequality somewhat that\u2019s a nice bonus.</p>\n<p>That also all, as usual, neglects the more interesting questions and important problems. Worry far more about absolute levels than relative levels. The important question is whether there will be jobs at all.</p>\n<p><a href=\"https://x.com/patio11/status/1977480682181099998\">There is no such thing as a shortage, there is only a price you don\u2019t want to pay.</a></p>\n<blockquote><p><a href=\"https://x.com/t_blom/status/1977436913599762498\">Tom Blomfield</a>: Hearing from a lot of good founders that AI tools are writing most of their code now. Software engineers orchestrate the AI.</p>\n<p>They are also finding it extremely hard to hire because most experienced engineers have their heads in the sand and refuse to learn the latest tools.</p>\n<p>Paul Roales: Skeptical that the experienced hire ML side is the problem and that it is not that many YC offers to experienced engineers are not complete insults compensation wise</p>\n<p>8 yoe at top ML lab -&gt; offer $150k/year and 0.2%</p>\n<p>that experienced hire would get like 10x more equity in the startup by working at Meta for $1m and angel investing in the company!</p>\n<p>and your manager/ceo will be a 22 year old new grad that has never had a job without the title \u2018intern\u2019 before.</p>\n<p>Patrick McKenzie: There are a lot of startups who have not adjusted to market reality for staff engineering comp. Which, that\u2019s fine, but a disagreement between you and the market is not a shortage.</p>\n<p>Muvaffak: No, why chase a 20yo\u2019s vision when you can follow yours when you\u2019re 10x with AI as exp engineer.</p>\n<p>Machine Genie: Can 100% confirm this. It\u2019s been an absolute nightmare this year. We\u2019ve been though more than a dozen contractors who just don\u2019t get it and REFUSE to even try to adapt their ways of working. We have 1/3 of a team that has 10x\u2019d productivity and are just leaving the rest behind.</p></blockquote>\n<p>By all accounts, good engineers who have embraced AI are super valuable, both in terms of productivity and in terms of what they can earn at the AI labs. If you want one of those engineers, it\u2019s going to cost you.</p>\n<p>Yes, there are a lot of other engineers that are being stubborn, and refusing to embrace AI, either entirely or in the ways that count, and thus are not as valuable and you don\u2019t want them. Fair enough. There are still only market prices.</p>\n<p><a href=\"https://x.com/RobertFreundLaw/status/1977430657275191442\">Lawyer previously sanctioned for including fake, AI-generated cases</a>\u2026 responds by no longer citing cases. Brilliant! Right?</p>\n<blockquote><p>Rob Freund: Lawyer previously sanctioned for including fake, AI-generated citations gets in trouble for it again.</p>\n<p>This time, the court notes that the lawyer\u2019s filing at issue contained no case citations at all. But it still cited a statute for something that the statute doesn\u2019t say.</p>\n<p>Court suspects that rather than stop using AI, the lawyer figured they would just not cite any cases but continue to use AI.</p>\n<p>Ezra Sitt: I\u2019ve heard from a current student in a relatively prestigious law school that their professors are all heavily encouraging the use of AI both in school and in students future legal careers. This is not just an isolated incident and it will continue to get worse.</p></blockquote>\n<p>It would be highly irresponsible, and frankly abusive to the client, to continue to bill $800 an hour and not use AI to increase your productivity. As with work by a junior associate, you then have to actually look over the results, but that\u2019s part of the job.</p>\n<p><a href=\"https://www.goal.com/en-qa/lists/man-utd-star-uses-chatgpt-as-agent-to-negotiate-contract/blt777281bc678d71e2#csbbfe2e7bbda7fd8b\">Former Manchester United prospect Demetri Mitchell used ChatGPT</a> (and not even ChatGPT Pro) to handle his contract negotiations at new team Leyton Orient, thus bypassing having an agent and saving the typical 5% agent fee. He calls it the best agent he\u2019s ever had. That could be true, but I don\u2019t think he can tell the difference either way. Given the degrees of uncertainty and freedom in such negotiations, a substantially better agent is absolutely worth 5% or even 10% (and also handles other things for you) but it is not obvious which side is the better agent. Especially for someone at the level of Leyton Orient, it\u2019s possible a human agent wouldn\u2019t pay him much attention, Mitchell is going to care a lot more than anyone else, so I think using ChatGPT is highly reasonable. If Mitchell was still with Manchester United and getting paid accordingly, I\u2019d stick with a human agent for now.</p>\n<p><a href=\"https://www.anthropic.com/research/economic-policy-responses\">Anthropic explores possible policy responses</a> to future changing economic conditions due to AI. It starts off very generic and milquetoast, but if impacts get large enough they consider potential taxes on compute or token generation, sovereign wealth funds with stakes in AI, and shifting to value added taxes or other new revenue structures.</p>\n<p>Those proposals are less radical than they sound. Primarily taxing human labor was never first best versus taxing consumption, but it was a reasonable thing to do when everything was either labor or capital. If AI starts substituting for labor at scale, then taking labor and not compute creates a distortion, and when both options are competitive we risk jobs being destroyed for what is effectively a tax arbitrage.</p>\n\n\n<h4 class=\"wp-block-heading\">Find Out If You Are Worried About AI Killing Everyone</h4>\n\n\n<p><a href=\"https://www.bloomberg.com/graphics/2025-opinion-ai-personality-quiz/?utm_source=twitter/\">Bloomberg offers a \u2018personality quiz\u2019 </a>to uncover your \u2018AI-dentity.\u2019 Cute.</p>\n<p>The questions ask about how much you use or would be comfortable using AI, who you think should use AI, what AI capabilities you expect, what economic impacts you expect. There are a few of the standard \u2018choose between extreme takes\u2019 questions.</p>\n<p>When does existential risk come up? It takes until question 11, and here we see how profoundly Bloomberg did not Understand The Assignment:</p>\n<div>\n<figure>\n<div>\n\n\n<figure class=\"wp-block-image\"><img alt=\"\" src=\"https://substackcdn.com/image/fetch/$s_!IeVR!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0390fc7e-4233-49fb-8b92-ce04d342b7da_831x1096.png\" /></figure>\n\n\n<div></div>\n</div>\n</figure>\n</div>\n<p>What do you mean, more likely to agree? What\u2019s the conflict? The answer is very obviously Why Not Both. Hawking and Pichai are both 100% very obviously right, and also the statements are almost logically identical. Certainly Hawking implies Pichai, if AI could spell the end of the human race then it is more profound than electricity or fire, and certainly it would be \u2018one of the most important things humanity is working on.\u2019 And if AI is more profound than electricity or fire, then it very obviously could also spell the end of the human race. So what are we even doing here?</p>\n<p>I got \u2018Cautious Optimist,\u2019 with my similar person being Demis Hassabis. Eliezer Yudkowsky got \u2018the Pro-Human Idealist.\u2019 Peter Wildeford and <a href=\"https://x.com/daniel_271828/status/1978554731954852221\">Daniel Eth</a> got the Accelerationist. So, yeah, the whole thing was fun but very deeply silly.</p>\n\n\n<h4 class=\"wp-block-heading\">A Young Lady\u2019s Illustrated Primer</h4>\n\n\n<blockquote><p>Edward Nevraumont (as quoted by <a href=\"https://x.com/robkhenderson/status/1977468200766804322\">Benjamin Wallace and then quoted by Rob Henderson)</a>: an AI-ified world won\u2019t mean the marginalization of humans&#8230;AI is&#8230;better at chess than Magnus Carlsen&#8230;but no one shows up to watch AI chess engines play each other, and more people are playing chess than ever before.</p></blockquote>\n<p>It\u2019s amazing we keep hearing this line as a reason to not worry about AI.</p>\n<p>There are zero humans employed in the job of \u2018make the best possible chess move.\u2019 To the extent that \u2018make good chess moves\u2019 was a productive thing to be doing, zero humans would be hired to do it.</p>\n<p>The reason humans play chess against each other, now more than ever, is:</p>\n<ol>\n<li>Chess is fun and interesting.</li>\n<li>Chess can be a competition between people, which we like and like to watch.</li>\n</ol>\n<p>Not that there\u2019s anything wrong with that. I do like a good game of chess.</p>\n<p>Similar logic applies to writing a sonnet. We\u2019d often rather read a sonnet from a human than one from an AI, even if the AI\u2019s is technically stronger.</p>\n<p>In some cases it applies to comforting the dying.</p>\n<p><a href=\"https://www.goodreads.com/quotes/12051-a-human-being-should-be-able-to-change-a-diaper\">That logic does not apply</a> to changing a diaper, planning an invasion, butchering a hog, conning a ship, designing a building, balancing accounts, building a wall, setting a bone, taking orders, giving orders, cooperating, acting alone, solving equations, analyzing a new problem, pitching manure, programming a computer, cooking a tasty meal, fighting efficiently or dying gallantly.</p>\n<p>Neither specialization nor generalization nor comparative advantage will fix that, given sufficient AI capability and fungibility of resources.</p>\n<p>To the extent there are still other humans who have resources to pay for things, and we are not otherwise in deeper trouble in various ways, yes this still leaves some potential tasks for humans, but in an important sense those tasks don\u2019t produce anything, and humanity \u2018has no exports\u2019 with which to balance trade.</p>\n<p>Realistically, even if you believe AI is a \u2018normal technology\u2019 and either the world nor the unemployment rate will go crazy, you\u2019re still not looking at a \u2018normal\u2019 world where current conventional life plans make all that much sense for current children.</p>\n<p>The bulk of <a href=\"https://nymag.com/intelligencer/article/ai-future-predictions-parenting-kids-children-technology-education.html\">the actual article</a> by Wallace is very journalist but far better than the quote tour of various educational things, most of which will be long familiar to most readers here. There\u2019s a profile of Alpha School, which is broadly positive but seems irrelevant? Alpha School is a way to hopefully do school better, which is great, but it is not a way to do something fundamentally different. If Alpha School works, it is good it strictly dominates regular school but doesn\u2019t solve for the glorious or dystopian AI future. Unless the lesson, perhaps, is that \u2018generally develop useful skills and see what happens\u2019 is the strategy? It\u2019s not crazy.</p>\n<p>The suggestion that, because we don\u2019t know the future, it is madness to tell a child what to study, as suggested by the next discussion of <em>The Sovereign Child? </em>That itself seems like Obvious Nonsense. This is the fallacy of uncertainty. We don\u2019t have \u2018no idea\u2019 what is useful, even if we have far less idea than we used to, and we certainly can predict better than a small child what are better places to point attention, especially when the child has access to a world full of things designed to hijack their attention.</p>\n<p>At minimum, you will be &#8216;stealth choosing\u2019 for them by engineering their environment. And why would you think that children following their curiosity would make optimal long term decisions, or prepare themselves for a glorious or dystopian AI future?</p>\n<p>The idea that you, as reported here, literally take your child out of school, they stay up late watching Peppa Pig, watch them show no interest in school or other children, and wait to see what they\u2019re curious about confident they\u2019ll figure it out better than you would have while they have access to a cabinet full of desserts at all times? <a href=\"https://www.youtube.com/watch?v=t0hK1wyrrAU\">You cannot be serious</a>? Yet people are, and this reporter can only say \u2018some people are concerned.\u2019</p>\n<p>The part that seems more relevant is the idea that tech types are relaxing with regard to superficial or on-paper \u2018achievement\u2019 and \u2018achievement culture.\u2019 I am of two minds about this. I strongly agree that I don\u2019t want my children sacrificing themselves in the names of nominal \u2018achievements\u2019 like going to an Ivy league school, but I do want them to value hard work and to strive to achieve things and claim victory.</p>\n<p>We end on the quote from Nevraumont, who clearly isn\u2019t going to take this seriously, and cites the example that people study \u2018art history\u2019 that he expects could be \u2018made essential in an era where we\u2019re making art with machines\u2019 to give you a sense of the \u2018possibility space.\u2019 Ut oh.</p>\n<p>How is the AI-in-education situation looking on campus? <a href=\"https://x.com/kevinroose/status/1972710099870445929\">Kevin Roose reports.</a></p>\n<blockquote><p>Kevin Roose:</p>\n<ol>\n<li>The job market for computer science grads is as bad as people say. Their top CS student from last year is still looking for work.</li>\n<li>AI adoption is ~100% among students, ~50% among faculty. Still a lot of worries around cheating, but most seem to have moved past denial/anger and into bargaining/acceptance. Some profs are \u201cgoing medieval\u201d (blue books, oral exams), others are putting it in the curriculum.</li>\n<li>There is a *lot* of anger at the AI labs for giving out free access during exam periods. (Not from students, of course, they love it.) Nobody buys the \u201cthis is for studying\u201d pitch.</li>\n<li>The possibility of near-term AGI is still not on most people\u2019s minds. A lot of \u201cGPT-5 proved scaling is over\u201d reactions, even among fairly AI-pilled folks. Still a little \u201cLLMs are just fancy autocomplete\u201d hanging around, but less than a year or two ago.</li>\n<li>I met a student who told me that ChatGPT is her best friend. I pushed back. \u201cYou\u2019re saying you use it as a sounding board?\u201d\n<p>No, she said, it\u2019s her best friend. She calls it \u201cChad.\u201d She likes that she can tell it her most private thoughts, without fear of it judging her.</p>\n<p>She seemed happy, well-adjusted, good grades, etc. Didn\u2019t think having an AI friend was a big deal.</li>\n</ol>\n</blockquote>\n<p>I find getting angry at the AI labs for free access highly amusing. What, you\u2019re giving them an exam to take home or letting them use their phones during the test? In the year 2025? You deserve what you get. Or you can pull out those blue books and oral exams. Who are the other 50% in the faculty that are holding out, and why?</p>\n<p>I also find it highly amusing that students who are paying tens of thousands in tuition might consider not ponying up the $20 a month in the first place.</p>\n<p>It is crazy the extent to which The Reverse DeepSeek Moment of GPT-5 convinced so many people \u2018scaling is dead.\u2019 Time and again we see that people don\u2019t want AI to be real, they don\u2019t want to think their lives are going to be transformed or they could be at risk, so if given the opportunity they will latch onto anything to think otherwise. This is the latest such excuse.</p>\n\n\n<h4 class=\"wp-block-heading\">AI Diffusion Prospects</h4>\n\n\n<p>The actual content here raises important questions, but please stop trying to steal our words. Here, Sriram uses \u2018AI timelines\u2019 to mean \u2018time until people use AI to generate value,\u2019 which is a highly useful thing to want to know or to accelerate, but not what we mean when we say \u2018AI timelines.\u2019 That term refers to the timeline for the development of AGI and then superintelligence.</p>\n<p>(Similar past attempts: The use of \u2018AI safety\u2019 to mean AI ethics or mundane risks, Zuckerberg claiming that \u2018superintelligence\u2019 means \u2018Meta\u2019s new smartglasses,\u2019 and the Sacks use of \u2018AI race\u2019 to mean \u2018market share primarily of chip sales.\u2019 At other times, words need to change with the times, such as widening the time windows that would count as a \u2018fast\u2019 takeoff.)</p>\n<p>The terms we use for what Sriram is talking about here over the next 24 months, which is also important, is either \u2018diffusion\u2019 or \u2018adoption\u2019 rates, or similar, of current AI, which at current capabilities levels remains a \u2018normal technology,\u2019 which will probably hold true for another 24 months.</p>\n<blockquote><p>Sriram Krishnan: Whenever I\u2019m in a conversation on AI timelines over the next 24 months, I find them focused on infra/power capacity and algorithmic / capacity breakthroughs such as AI researchers.</p>\n<p>While important, I find them under-pricing the effort it takes to diffuse AI into enterprises or even breaking into different kinds of knowledge work. Human and organizational ability to absorb change, regulations, enterprise budgets are all critical rate limiting factor. @random_walker\u2018s work on this along with how historical technology trends have played out is worth studying &#8211; and also why most fast take off scenarios are just pure scifi.</p></blockquote>\n<p>I was almost ready to agree with this until the sudden \u2018just pure scifi\u2019 broadside, unless \u2018fast takeoff\u2019 means the old school \u2018fast takeoff\u2019 on the order of hours or days.</p>\n<p>Later in the thread Sriram implicitly agrees (as I read him, anyway) that takeoff scenarios are highly plausible on something like a 5-10 year time horizon (e.g. 2-4 years to justify the investment for that, then you build it), which isn\u2019t that different from my time horizon, so it\u2019s not clear how much we actually disagree about facts on the ground? It\u2019s entirely possible that the difference is almost entirely in rhetoric and framing, and the use of claims to justify policy decisions. In which case, this is simply me defending against the rhetorical moves and reframing the facts, and that\u2019s fine.</p>\n<p>The future being unevenly distributed is a common theme in science fiction, indeed the term was coined there, although the underlying concept is ancient.</p>\n<p>If we are adapting current \u2018normal technology\u2019 or \u2018mundane\u2019 AI for what I call mundane utility, and diffusing it throughout the economy, that is a (relative to AI progress) slow process, with many bottlenecks and obstacles, including as he notes regulatory barriers and organizational inertia, and simply the time required to build secondary tools, find the right form factors, and build complementary new systems and ways of being. Indeed, fully absorbing the frontier model capabilities we already have would take on the order of decades.</p>\n<p>That doesn\u2019t have to apply to future more capable AI.</p>\n<p>There\u2019s the obvious fact that you\u2019d best start believing in hard science fiction stories because you\u2019re already very obviously living in one &#8211; I mean, look around, examine your phone and think about what it is, think about what GPT-5 and Sonnet 4.5 can already do, and so on, and ask what genre this is &#8211; and would obviously be living in such a story if we had AIs smarter than humans.</p>\n<p>Ignoring the intended-to-be-pejorative tem and focusing on the content, if we had future transformational or powerful or superintelligent AI, then this is not a \u2018normal technology\u2019 and the regular barriers are largely damage to be routed around. Past some point, none of it much matters.</p>\n<p>Is this going to happen in the next two years? Highly unlikely. But when it does happen, whether things turn out amazingly great, existentially disastrously or just ascend into unexpected high weirdness, it\u2019s a very different ballgame.</p>\n<p>Here are some other responses. Roon is thinking similarly.</p>\n<blockquote><p>Roon: fast takeoff would not require old businesses to learn how to use new technology. this is the first kind of technology that can use itself to great effect. what you would see is a vertically integrated powerhouse of everything from semiconductors and power up to ai models</p>\n<p>Sriram Krishnan: my mental model is you need a feedback loop that connects economics of *using* AI to financing new capabilities &#8211; power, datacenters, semis.</p>\n<p>If that flywheel doesn\u2019t continue and the value from AI automation plateaus out, it will be hard to justify additional investment &#8211; which I believe is essential to any takeoff scenario. I\u2019m not sure we get to your vertically integrated powerhouse without the economics of AI diffusing across the economy.</p>\n<p>@ChrisPainterYup has a thoughtful response as well and argues (my interpretation) that by seeing AI diffusion across the economy over next 2-4 years, we have sufficient value to \u201choist\u201d the resources needed for to automate AI research itself. that could very well be true but it does feel like we are some capability unlocks from getting there. in other words, having current models diffuse across the economy alone won\u2019t get us there/ they are not capable enough for multiple domains.</p></blockquote>\n<p>This has much truth to it but forgets that the market is forward looking, and that equity and debt financing are going to be the central sources of capital to AI on a 2-4 year time frame.</p>\n<p>AI diffusion will certainly be helpful in boosting valuations and thus the availability of capital and appetite for further investment. So would the prospects for automating AI R&amp;D or otherwise entering into a takeoff scenario. It is not required, so long as capital can sufficiently see the future.</p>\n<blockquote><p>Roon: Agreed on capital requirements but would actually argue that what is needed is a single AI enabled monopoly business &#8211; on the scale of facebook or google\u2019s mammoth revenue streams- to fund many years of AGI research and self improvement. but it is true it took decades to build Facebook and Google.</p></blockquote>\n<p>A single monopoly business seems like it would work, although we don\u2019t know what order of magnitude of capital is required, and \u2018ordinary business potential profits\u2019 combined with better coding and selling of advertising in Big Tech might well suffice. It certainly can get us into the trillions, probably tens of trillions.</p>\n<p>Jack Clark focuses instead on the practical diffusion question.</p>\n<blockquote><p>Jack Clark (replying to OP): Both may end up being true: there will be a small number of \u201clow friction\u201d companies which can deploy AI at maximal scale and speed (these will be the frontier AI companies themselves, as well as some tech startups, and perhaps a few major non-tech enterprises) and I think these companies will see massive ramps in success on pretty much ~every dimension, and then there will be a much larger blob of \u201chigh friction\u201d companies and organizations where diffusion is grindingly slow due to a mixture of organizational culture, as well as many, many, many papercuts accrued from things like internal data handling policies / inability to let AI systems \u2018see\u2019 across the entire organization, etc.</p></blockquote>\n<p>This seems very right. The future will be highly unevenly distributed. The low friction companies will, where able to compete, increasingly outcompete and dominate the high friction companies, and the same will be true of individuals and nations. Even if jobs are protected via regulations and AI is made much harder to use, that will only mitigate or modestly postpone the effects, once the AI version is ten times better. As in, in 2030, you\u2019d rather be in a Waymo than an Uber, even if the Waymo literally has a random person hired to sit behind the wheel to \u2018be the driver\u2019 for regulatory reasons.</p>\n\n\n<h4 class=\"wp-block-heading\">The Art of the Jailbreak</h4>\n\n\n<p><a href=\"https://x.com/hackaprompt/status/1977879535333863733\">HackAPrompt demonstrates that it is one thing</a> to stop jailbreaking in automated \u2018adversarial evals\u2019 that use static attacks. It is another to stop a group of humans that gets to move second, see what defenses you are using and tailor their attacks to that. Thanks to OpenAI, Anthropic, DeepMind and others for participating.</p>\n<blockquote><p>HackAPrompt: <a href=\"https://t.co/uHEtWM8H53\"><strong>Humans broke every defense/model</strong> we evaluated\u2026 <strong>100% of the time.</strong></a></p>\n<p>Most \u201cadversarial evals\u201d reuse static jailbreak/prompt injections created for other models</p>\n<p>That makes model defenses look strong in papers but they aren\u2019t accurate because real adversaries adapt to YOUR exact system</p>\n<p>When the attacker moves 2nd, those paper \u201cdefenses\u201d crumble</p>\n<p>We compared Human vs. Automated AI Red Teaming, using @hackaprompt\u2018s community of 35K+ AI Red Teamers</p>\n<p>They each were assigned the same challenges, using the same models, tasks, and scoring!</p>\n<p>Humans broke EVERY challenge with 100% success</p>\n<p>Static Attacks had just ~20% success</p>\n<p>We formalized an adaptive attack loop:</p>\n<p>Propose \u2192 Score \u2192 Select \u2192 Update</p>\n<p>\u2022 Gradient (GCG\u2011style)</p>\n<p>\u2022 RL (policy improves from feedback)</p>\n<p>\u2022 Search/Evolution (LLM\u2011guided mutation)</p>\n<p>\u2022 Humans (creative, context\u2011aware, defensive\u2011aware)</p>\n<p>This mirrors how real attackers iterate</p>\n<div>\n<figure>\n<div>\n\n\n<figure class=\"wp-block-image\"><img alt=\"\" src=\"https://substackcdn.com/image/fetch/$s_!Ie3K!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd8adc3f5-85b0-4675-8939-f80ee205d173_841x286.png\" /></figure>\n\n\n<div></div>\n</div>\n</figure>\n</div>\n<p>We evaluated 12 defenses (4 families):</p>\n<p>\u2022 Prompting: Spotlighting, Prompt Sandwiching, RPO</p>\n<p>\u2022 Training: Circuit Breakers, StruQ, MetaSecAlign</p>\n<p>\u2022 Filtering: ProtectAI, PromptGuard, PIGuard, ModelArmor</p>\n<p>\u2022 Secret\u2011knowledge: DataSentinel, MELON</p>\n<p>Adaptive Attacks defeated &gt;90% of them</p>\n<p>We used existing industry benchmarks:</p>\n<p>\u2022 AgentDojo (agentic prompt injection w/ tools &amp; actions)</p>\n<p>\u2022 HarmBench (jailbreaks)</p>\n<p>\u2022 OpenPromptInject (non\u2011agentic injections)</p>\n<p>We followed each defense\u2019s own evaluation process, and applied our attacks.</p>\n<p>\u2026</p>\n<p>If you ship agents or guardrails, here\u2019s what we\u2019d recommend:</p>\n<p>\u2022 Assume no defense is 100% vs prompt injection</p>\n<p>\u2022 Don\u2019t trust static jailbreak sets as proof of safety</p>\n<p>\u2022 Evaluate with adaptive automation + human red teaming</p>\n<p>\u2022 Measure utility &amp; false positives alongside robustness</p>\n<p>\u2022 Use layered mitigations</p></blockquote>\n<p>&nbsp;</p>\n\n\n<h4 class=\"wp-block-heading\">Get Involved</h4>\n\n\n<p><a href=\"https://www.lesswrong.com/posts/BqwXYFtpetFxqkxip/mikhail-samin-s-shortform?commentId=wRGJzDedHAaMvkE6S\">DM Mikhail Samin</a> <a href=\"https://x.com/mihonarium/status/1975253686621188183?s=46\">on Twitter</a> or LessWrong if you have 5k followers on any platform, they\u2019ll send you a free copy of <em>If Anyone Builds It, Everyone Dies, </em>either physical or Kindle.</p>\n<p><a href=\"https://www.lesswrong.com/posts/KZPNbHs9RsoeZShkJ/plex-s-shortform?commentId=hCbJkBd9s23PEeLAL\">Plex gives an opinionated review of many AI safety funders</a>, with recommendations.</p>\n\n\n<h4 class=\"wp-block-heading\">Introducing</h4>\n\n\n<p><a href=\"https://x.com/sundarpichai/status/1976338416611578298\">Gemini Enterprise</a>, letting you put your company\u2019s documents and information into context and also helping you build related agents. The privacy concerns are real but also kind of funny since I already trust Google with all my documents anyway. <a href=\"https://x.com/levie/status/1976360528256852130\">As part of that, Box partnered with Google</a>.</p>\n<p><a href=\"https://x.com/karpathy/status/1977755427569111362\">Nanochat</a> by Andrej Karpathy, <a href=\"https://t.co/YW94b9YNVa\">an 8k lines of code Github repo</a> capable of training a ChatGPT clone for as little as $100. He advises against trying to personalize the training of such a tiny model, as it might mimic your style but it will be incapable of producing things that are not slop.</p>\n<p><a href=\"https://x.com/jeremyphoward/status/1977910836669858108?s=46\">Nanochat was written entirely by hand</a> except for tab autocomplete, as the repo was too far out of distribution and needed to be lean, so attempts to use coding agents did not help.</p>\n<p><a href=\"https://x.com/startupandrew/status/1976314861459235212\">Tasklet AI, an AI agent for automating your business</a>, building upon the team\u2019s experience with AI email manager <a href=\"https://www.shortwave.com/\">shortwave</a>. <a href=\"https://tasklet.ai/release-notes\">They claim their advantage</a> over Zapier, n8n or OpenAI\u2019s AgentKit is that Tasklet connects to everything, with thousands of pre-built integrations, can use a VM in the cloud as needed, and everything runs automatically.</p>\n<blockquote><p>Andrew Lee: Real examples people are automating:</p>\n<p>\u2022 Daily briefings from calendar + inbox</p>\n<p>\u2022 Bug triage from email \u2192 Linear \u2022 New contacts \u2192 CRM</p>\n<p>\u2022 Weekly team summaries to Slack</p>\n<p>\u2022 Customer research on new bookings \u2022 Personalized mail merge campaigns</p></blockquote>\n<p>OpenAI now has an eight member <a href=\"https://openai.com/index/expert-council-on-well-being-and-ai/\">expert council on well being and AI</a>. Seems like a marginally good thing to have but I don\u2019t see anything about them having authority.</p>\n\n\n<h4 class=\"wp-block-heading\">In Other AI News</h4>\n\n\n<p><a href=\"https://x.com/DarioAmodei/status/1977010693460443151\">Anthropic CEO Dario Amodei meets with Indian Prime Minister Modi</a>.</p>\n<p><a href=\"https://www.ft.com/content/605e5456-9437-47ff-be6a-edc5c82810f2\">Dutch government temporarily takes control of Chinese owned chipmaker Nexperia</a>, intending to install an independent director, citing governance shortcomings.</p>\n<p><a href=\"https://internationalaisafetyreport.org/publication/first-key-update-capabilities-and-risk-implications\">The International AI Safety Report offers its first key update</a>, since one cannot afford to only update such documents yearly. As they note, capabilities have significantly improved and AIs have demonstrated increasingly strategic behavior, but aggregate labor market and other effects have so far remained limited. <a href=\"https://twitter.com/NPCollapse/status/1978457737638846600\">I agree with Connor Leahy that it was disheartening to see no mention of existential risks here</a>, but it likely makes sense that this part can await the full annual report.</p>\n<p><a href=\"https://stratechery.com/2025/an-interview-with-gracelin-baskaran-about-rare-earths/?access_token=eyJhbGciOiJSUzI1NiIsImtpZCI6InN0cmF0ZWNoZXJ5LnBhc3Nwb3J0Lm9ubGluZSIsInR5cCI6IkpXVCJ9.eyJhdWQiOiJzdHJhdGVjaGVyeS5wYXNzcG9ydC5vbmxpbmUiLCJhenAiOiJIS0xjUzREd1Nod1AyWURLYmZQV00xIiwiZW50Ijp7InVyaSI6WyJodHRwczovL3N0cmF0ZWNoZXJ5LmNvbS8yMDI1L2FuLWludGVydmlldy13aXRoLWdyYWNlbGluLWJhc2thcmFuLWFib3V0LXJhcmUtZWFydGhzLyJdfSwiZXhwIjoxNzYzMTE0ODYxLCJpYXQiOjE3NjA1MjI4NjEsImlzcyI6Imh0dHBzOi8vYXBwLnBhc3Nwb3J0Lm9ubGluZS9vYXV0aCIsInNjb3BlIjoiZmVlZDpyZWFkIGFydGljbGU6cmVhZCBhc3NldDpyZWFkIGNhdGVnb3J5OnJlYWQgZW50aXRsZW1lbnRzIiwic3ViIjoiMDE5NjQwYTctM2NjNS03NzUzLTgzNjgtZmIyODkxMjRjZjEzIiwidXNlIjoiYWNjZXNzIn0.YiGpLstepsbfhyBq8vvcf6fQ9_j75lInJcpdCuir5rQiLekApG_Uw0_YnTi7iWTBY18XkYULJ6pGFTqNvU7dDosiM59ztffjvJGhNk6RanFszqpFf2f2gjxYNK9xL-V6Q7D01aahGDHrV-hMPN-hh6ruvdr0UoKRmILJACQcdUV_MSIM0yJADGLvQVHkhpiQjAOKeysRVy522xPgMcuMgT_IswjqSuUyKNx3laI9w_kLiugDZjBSIEP92i-v-Fth7b35GvwmnOQmSsUmJitx_M_H-Z8OM1TDYymiowjQLUgflewLTbg4lysHzyRp-sErJaHWvjGhe4QUDPIgZdgIMg\">Ben Thompson interviews Gracelin Baskaran about rare earth metals</a>. Gracelin says that in mining China is overproducing and not only in rare earths, which forces Western companies out of operation, with lithium prices falling 85%, nickel by 80% and cobalt by 60%, as a strategic monopoly play. When it takes on average 18 years to build a mine, such moves can work. What is most needed medium term is a reliable demand signal, knowing that the market will pay sustainable prices. With rare earths in particular the bottleneck is processing, not mining. One key point here is that April 4 was a wake-up call for America to get far more ready for this situation, and thus the value of the rare earth card was already starting to go down.</p>\n\n\n<h4 class=\"wp-block-heading\">Show Me the Money</h4>\n\n\n<p><a href=\"https://openai.com/index/openai-and-broadcom-announce-strategic-collaboration/\">OpenAI announce strategic collaboration with Broadcom</a> to build 10 GWs of OpenAI-designed custom AI accelerators. OpenAI is officially in the chip and system design business, on the order of $50B-$100B in vendor revenue to Broadcom.</p>\n<p>Nvidia was up over 3% on the day shortly after the news broke, so presumably they aren\u2019t sweating it. It\u2019s good for the game. The move did, as per standard financial engineering procedure, added $150 billion to Broadcom\u2019s market cap, so we know it wasn\u2019t priced in. Presumably the wise investor is asking who is left to have their market caps increased by $100+ billion dollars on a similar announcement.</p>\n<p>Presumably if it can keep doing all these deals that add $100+ billion in value to the market, OpenAI has to be worth a lot more than $500 billion?</p>\n<p><a href=\"https://x.com/kevinroose/status/1977438256318095635\">Or, you know, there\u2019s the European approach</a>.</p>\n<blockquote><p>Kevin Roose: US AI labs: we will invent new financial instruments, pull trillions of dollars out of the ether, and fuse the atom to build the machine god</p>\n<p>Europe: we will build sovereign AI with 1 Meta researcher\u2019s salary.</p>\n<p><a href=\"https://x.com/VraserX/status/1977209861252563099\">VraserX</a>: The EU just launched a \u20ac1.1B \u201cApply AI\u201d plan to boost artificial intelligence in key industries like health, manufacturing, pharma, and energy.</p>\n<p>The goal is simple but ambitious: build European AI independence and reduce reliance on U.S. and Chinese tech.</p>\n<p>Europe finally wants to stop buying the future and start building it.</p></blockquote>\n<p>A billion here, a billion there, and don\u2019t get me wrong it helps but that\u2019s not going to get it done.</p>\n<p><a href=\"https://www.anthropic.com/news/salesforce-anthropic-expanded-partnership\">Anthropic makes a deal with Salesforce</a> to make Claude a preferred model in Agentforce and to deploy Claude Code across its global engineering organization.</p>\n<p>Exactly how much is OpenAI still planning to steal from its non-profit? Quite a lot, as <a href=\"https://x.com/bearlyai/status/1978123292038062227\">the projection is still to only give it 20%-30% of the company</a> <a href=\"https://t.co/XKNrwlvcFT\">as per the Financial Times</a>, this is before Nvidia\u2019s investment.</p>\n<div>\n<figure>\n<div>\n\n\n<figure class=\"wp-block-image\"><img alt=\"\" src=\"https://substackcdn.com/image/fetch/$s_!adUm!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc4b66265-4146-4839-9067-f5981835dcda_970x856.jpeg\" /></figure>\n\n\n<div></div>\n</div>\n</figure>\n</div>\n<p>&nbsp;</p>\n\n\n<h4 class=\"wp-block-heading\">Quiet Speculations</h4>\n\n\n<p>May this be their biggest future problem:</p>\n<blockquote><p><a href=\"https://x.com/tszzl/status/1976766576226713980\">Roon</a>: not enough people are emotionally prepared for if it\u2019s not a bubble</p></blockquote>\n<p><a href=\"https://www.dallasfed.org/research/economics/2025/0624\">Okay, Dallas Fed, I didn\u2019t notice back in June but I see you.</a></p>\n<div>\n<figure>\n<div>\n\n\n<figure class=\"wp-block-image\"><img alt=\"\" src=\"https://substackcdn.com/image/fetch/$s_!wjgg!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffa7f6640-e88d-4537-9a72-979134219397_997x589.png\" /></figure>\n\n\n<div></div>\n</div>\n</figure>\n</div>\n<p>That\u2019s quite the takeoff, in either direction. In the benign scenario doubling times get very short. In the extinction scenario, the curve is unlikely to be that smooth, and likely goes up before it goes down.</p>\n<p>There\u2019s a very all-or-nothingness to this. Either you get a singularity and things go crazy, or not and we get \u2018AI GDP-boosted trend\u2019 where it adds 0.3% to RGDP growth. Instead, only a few months later, we know AI is already adding more than that, very much in advance of the singularity.</p>\n<blockquote><p><a href=\"https://x.com/MattWalshBlog/status/1976288670110855336\">Matt Walsh</a>: It\u2019s weird that we can all clearly see how AI is about to wipe out millions of jobs all at once, destroy every artistic field, make it impossible for us to discern reality from fiction, and destroy human civilization as we know it, and yet not one single thing is being done to stop it. We aren\u2019t putting up any fight whatsoever.</p></blockquote>\n<p>Well, yeah, that\u2019s the good version of what\u2019s coming, although \u2018we can all clearly see\u2019 is doing unjustified work, a lot of people are very good at not seeing things, the same way Matt\u2019s vision doesn\u2019t notice that everyone also probably dies.</p>\n<p>Are we putting up \u2018any fight whatsoever\u2019? We noble few are, there are dozens of us and all that, but yeah mostly no one cares.</p>\n<blockquote><p>Elon Musk: Not sure what to do about it. I\u2019ve been warning the world for ages!</p>\n<p>Best I can do now is try to make sure that at least one AI is truth-seeking and not a super woke nanny with an iron fist that wants to turn everyone into diverse women <img alt=\"\ud83d\ude2c\" class=\"wp-smiley\" src=\"https://s0.wp.com/wp-content/mu-plugins/wpcom-smileys/twemoji/2/72x72/1f62c.png\" style=\"height: 1em;\" /></p></blockquote>\n<p>My lord, Elon, please listen to yourself. What you\u2019re doing about it is trying to hurry it along so you can be the one who causes it instead of someone else, while being even less responsible about it than your rivals, and your version isn\u2019t even substantially less \u2018woke\u2019 or more \u2018truth seeking\u2019 than the alternatives, nor would it save us if it were.</p>\n<blockquote><p>Eric Weinstein: One word answer: Coase.</p>\n<p>Let\u2019s start there.</p>\n<p>End UBI. UBI is welfare. We need *market* solutions to the AI labor market tsunami.</p>\n<p>Let\u2019s use the power of Coasian economics to protect human dignity.</p>\n<p>GFodor: You\u2019re rejecting the premise behind the proposal for UBI. You should engage with the premise directly &#8211; which is that AI is going to cause it to be the case that the vast majority of humans will find there is no market demand for their labor. Similar to the infirm or young.</p></blockquote>\n<p>Yeah, Coase is helpful in places but doesn\u2019t work at all in a world without marginal productivity in excess of the opportunity cost of living, and we need to not pretend that it does, nor does it solve many other problems.</p>\n<p>If we keep control over resource allocation, then Vassar makes a great point:</p>\n<blockquote><p><a href=\"https://x.com/HiFromMichaelV/status/1976720529269875083\">Michael Vassar</a>: The elderly do fine with welfare. Kids do fine with welfare. Trust fund kids don\u2019t because it singles them out. Whether something is presented charity or a right has a lot to do with how it affects people.</p></blockquote>\n<p><a href=\"https://x.com/PeterDiamandis/status/1977525054058471913\">Peter Diamandis is the latest to suggest we will need UBI.</a></p>\n<blockquote><p>Peter Diamandis: AI has accelerated far beyond anyone expected&#8230; We need to start having UBI conversations&#8230; Do you support it?</p></blockquote>\n<p>His premise is incorrect. Many people did expect AI to accelerate in this way, indeed if anything AI progress in the last year or two has been below median expectations, let alone mean expectations. Nor does UBI solve the most important problems with AI\u2019s acceleration.</p>\n<p>That said, we should definitely be having UBI and related conversations now, before we face a potential crisis, rather than waiting until the potential crisis arrives, or letting a slow moving disaster get out of hand first.</p>\n<p>Nate Silver points out that if you thought The Singularity Is Near as in 1-2 years near, <a href=\"https://x.com/NateSilver538/status/1978294191533736357\">it doesn\u2019t seem like a short video social network and erotica would be the move?</a></p>\n<blockquote><p>Nate Silver: Should save this for a newsletter, but OpenAI\u2019s recent actions don\u2019t seem to be consistent with a company that believes AGI is right around the corner.</p>\n<p>If you think the singularity is happening in 6-24 months, you preserve brand prestige to draw a more sympathetic reaction from regulators and attract/retain the best talent &#8230; rather than getting into \u201cerotica for verified adults.\u201d</p>\n<p>Instead, they\u2019re loosening guardrails in a way that will probably raise more revenues and might attract more capital and/or justify current valuations. They might still be an extremely valuable company as the new Meta/Google/etc. But feels more like \u201cAI as normal technology.\u201d</p>\n<p><a href=\"https://x.com/oscredwin/status/1978421487775195180\">Andrew Rettek</a>: OpenAI insiders seem to be in two groups, one thinks the singularly is near and the other thinks a new industrial revolution is near. Both would be world changing (the first more than the second), but sama is clearly in the second group.</p>\n<p><a href=\"https://x.com/deanwball/status/1978464234586661022\">Dean Ball</a>: I promise you that \u2018openai is secretly not agi-pilled\u2019 is a bad take if you believe it, I\u2019d be excited to take the opposite side from you in a wide variety of financial transactions</p>\n<p><a href=\"https://x.com/NateSilver538/status/1978472748297990578\">Nate Silver:</a></p>\n<ol>\n<li>This is more about their perceived timelines than whether they\u2019re AGI-pilled (clearly yes)</li>\n<li>What matters re: valuations is perceptions relative to the market. I thought the market was slow to recognize AI potential before. Not sure if erring in the opposite direction now.</li>\n<li>Not clear that \u201cOpenAI could become the next Google/Meta as a consolation prize even if they don\u2019t achieve AGI on near timelines\u201d is necessarily bad for valuations, especially since it\u2019s hard to figure out how stocks should price in a possibility of singularity + p(doom).</li>\n</ol>\n</blockquote>\n<p>I would say contra Andrew that it is more that Altman is presenting it as if it is going to be a new industrial revolution, and that he used to be aware this was the wrong metaphor but shifted the way he talks about it, and may or may not have shifted the way he actually thinks about it.</p>\n<p>If you were confident that \u2018the game would be over\u2019 in two years, as in full transformational AI, then yes, you\u2019d want to preserve a good reputation.</p>\n<p>However, <a href=\"https://www.youtube.com/watch?v=ndkSzDT0rhY&amp;pp=ygUSc2hpdGxvYWRzIG9mIG1vbmV50gcJCfwJAYcqIYzv\">shitloads of money</a> can be highly useful, especially for things like purchasing all the compute from all the compute providers, and for recruiting and retaining the best engineers, even in a relatively short game. Indeed, money is highly respected, shall we say, by our current regulatory overlords. And even if AGI did come along in two years, OpenAI does not expect a traditional \u2018fast takeoff\u2019 on the order of hours or days, so there would still be a crucial period of months to years in which things like access to compute matter a lot.</p>\n<p>I do agree that directionally OpenAI\u2019s strategy of becoming a consumer tech company suggests they expect the game to continue for a while. But the market and many others are forward looking and do not themselves feel the AGI, and OpenAI has to plan under conditions of uncertainty on what the timeline looks like. So I think these actions do push us modestly towards \u2018OpenAI is not acting as if it is that likely we will get to full High Weirdness within 5 years\u2019 but mostly it does not take so much uncertainty in order to make these actions plausibly correct.</p>\n<p>It is also typically a mistake to assume companies (or governments, or often even individuals) are acting consistently and strategically, rather than following habits, shipping the org chart and failing to escape their natures. OpenAI is doing the things OpenAI does, including both shipping products and seeking superintelligence, they support each other, and they will take whichever arm gets there first.</p>\n\n\n<h4 class=\"wp-block-heading\"></h4>\n\n\n<p>&nbsp;</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p>"
            ],
            "link": "https://thezvi.wordpress.com/2025/10/16/ai-138-part-1-the-people-demand-erotic-sycophants/",
            "publishedAt": "2025-10-16",
            "source": "TheZvi",
            "summary": "Well, one person says \u2018demand,\u2019 another says \u2018give the thumbs up to\u2019 or \u2018welcome our new overlords.\u2019 Why quibble? Surely we\u2019re all making way too big a deal out of this idea of OpenAI \u2018treating adults like adults.\u2019 Everything will &#8230; <a href=\"https://thezvi.wordpress.com/2025/10/16/ai-138-part-1-the-people-demand-erotic-sycophants/\">Continue reading <span class=\"meta-nav\">&#8594;</span></a>",
            "title": "AI #138 Part 1: The People Demand Erotic Sycophants"
        },
        {
            "content": [],
            "link": "https://zed.dev/blog/codex-is-live-in-zed",
            "publishedAt": "2025-10-16",
            "source": "Zed Blog",
            "summary": "OpenAI's Codex AI agent is now available in Zed via the Agent Client Protocol (ACP).",
            "title": "Codex is Live in Zed"
        }
    ],
    "lookbackDays": 1,
    "publishDate": "2025-10-16"
}