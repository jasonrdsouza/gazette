{
    "articles": [
        {
            "content": [],
            "link": "https://overreacted.io/how-to-fix-any-bug/",
            "publishedAt": "2025-10-21",
            "source": "Dan Abramov",
            "summary": "The joys of vibecoding.",
            "title": "How to Fix Any Bug"
        },
        {
            "content": [],
            "link": "https://buttondown.com/hillelwayne/archive/modal-editing-is-a-weird-historical-contingency/",
            "publishedAt": "2025-10-21",
            "source": "Hillel Wayne",
            "summary": "<p class=\"empty-line\" style=\"height: 16px; margin: 0px !important;\"></p> <p>A while back my friend <a href=\"https://morepablo.com/\" target=\"_blank\">Pablo Meier</a> was reviewing some 2024 videogames and wrote <a href=\"https://morepablo.com/2025/03/games-of-2024.html\" target=\"_blank\">this</a>:</p> <blockquote> <p>I feel like some artists, if they didn't exist, would have the resulting void filled in by someone similar (e.g. if Katy Perry didn't exist, someone like her would have). But others don't have successful imitators or comparisons (thinking Jackie Chan, or Weird Al): they are irreplaceable. </p> </blockquote> <p>He was using it to describe auteurs but I see this as a property of opportunity, in that \"replaceable\" artists are those who work in bigger markets. Katy Perry's market is large, visible and obviously (but not <em>easily</em>) exploitable, so there are a lot of people who'd compete in her niche. Weird Al's market is unclear: while there were successful parody songs in the past, it wasn't clear there was enough opportunity there to support a superstar.</p> <p>I think that modal editing is in the latter category. Vim is now very popular and has spawned numerous successors. But its key feature, <strong>modes</strong>, is not obviously-beneficial, to the point that if Bill Joy didn't make vi (vim's direct predecessor) fifty years ago I don't think",
            "title": "Modal editing is a weird historical contingency we have through sheer happenstance"
        },
        {
            "content": [],
            "link": "https://blog.nelhage.com/post/regex-crosswords-z3/",
            "publishedAt": "2025-10-21",
            "source": "Nelson Elhage",
            "summary": "For a while now, I&rsquo;ve been fascinated by Z3 and by SMT solving more broadly. While on pat leave recently, I was reminded of the existence of regular-expression crossword puzzles, and allowed myself to get nerdsniped by writing a Z3-backed solver. I expected to spend perhaps an afternoon cranking out a quick solver; I ended up getting sucked into understanding and debugging Z3 performance, and learning far more about Z3 and about SMT than I expected.",
            "title": "Solving Regex Crosswords with Z3"
        },
        {
            "content": [],
            "link": "https://www.robinsloan.com/lab/demons-of-streaming/",
            "publishedAt": "2025-10-21",
            "source": "Robin Sloan",
            "summary": "<p>An old arrangement. <a href=\"https://www.robinsloan.com/lab/demons-of-streaming/\">Read here.</a></p>",
            "title": "The demons of streaming"
        },
        {
            "content": [
                "<p><strong>I.</strong></p><p>In my 2019 post <a href=\"https://slatestarcodex.com/2019/09/18/too-much-dark-money-in-almonds/\">Too Much Dark Money In Almonds</a>, I asked: why is there so little money in politics?</p><p>During the 2018 election, Americans - candidates, parties, PACs, and small donors like you - spent a combined $5 billion pushing their preferred candidates. Although that sounds like a lot of money, Americans spent $12 billion on almonds that same year. Why the imbalance? The oil industry has strong political opinions, and they make $500 billion per year. Do they really think electing oil-friendly politicians isn&#8217;t worth 2% of revenue?</p><p>We debated how this could be. Some of the discussion proved prescient - I asked if maybe Elon Musk should buy some kind of social media property. But we never found a good answer, and the implied question remained open: if some billionaire wanted to spend an actually relevant percent of his net worth on politics, could he just take over everything?</p><p>I recently talked to some Silicon Valley political consultants who updated me on the status of this issue: Marc Andreessen tried this in 2024 and it basically worked. Now he is trying it a second time, it will probably work again, and Marc Andreessen will probably own every politician twice over.</p><p><strong>II.</strong></p><p>First, the backstory: pre-Andreessen, you could divide SuperPACs into three categories:</p><ol><li><p>Partisan groups with names like 'Democrats Should Win The Senate, Inc'</p></li><li><p>AIPAC</p></li><li><p>Everyone else</p></li></ol><p><strong>The partisan groups</strong> have lots of money but little distortionary effect. Democratic machines try to elect Democrats, Republican machines try to elect Republicans, but they don't push their chosen candidates towards any specific position besides the ones that play well with voters. They are, so to speak, priced in.</p><p><strong>AIPAC</strong> is a single-issue PAC aimed at supporting Israel, which is orders of magnitude more effective than any other political organization. Their advantage stems from the nature of political donations, which come in two types. \"Hard money\" is money given directly to candidates; strict campaign finance limits it to $7000 per donor. \"Soft money\" comes from SuperPACs and can evade most campaign finance laws; it can pay for ads but can't fund candidates directly. Candidates prefer hard money to soft money, but it's harder to get; a single billionaire can provide unlimited soft money, but you need a wide donor base to acquire hard money. But not too wide! When millions of waitresses and bartenders gave Bernie Sanders $25 each, that was impressive grassroots support - but each of those $25 checks only went 1/280th as far as one person giving the $7000 max, and these people were hard to corral and coordinate for downballot causes. AIPAC's natural constituency, (((Middle Eastern democracy supporters))), are at the exact sweet spot of moderately numerous, moderately well-off, and very committed. This gives AIPAC unparalleled access to hard money, compared to other groups that are more reliant on single billionaires or legions of poor people. But also, AIPAC fights hard. If some random Congressman is anti-Israel, AIPAC will swoop down on their race in Middle Of Nowhere, Missouri and pour $10 million into electing their opponent. By now everyone knows this, and the mere threat of AIPAC action is enough to keep politicians in line. </p><p><strong>Everyone else</strong> includes other industry groups, labor groups, and activist cadres.  Probably on aggregate these people are destroying America, but as individual organizations they're miniscule compared to the first two categories. The biggest of these is a real estate group 25-50% the size of AIPAC that nobody's ever heard of.</p><p>The average PAC strategy is this: when the incumbent will obviously win, donate money to the incumbent. When there's a tight race, donate money to both sides.</p><p>Why does this first prong of this strategy work? If the incumbent will definitely win, why are they selling out for more cash?</p><p>Safe-seat Congressmen want more hard money for a pretty good reason: they can transfer it to other politicians or the party apparatus in exchange for goodwill that can be cashed in later for leadership positions. </p><p>Safe-seat Congressmen want more soft money because . . .  the consultants I talked to didn&#8217;t have a great answer here. One ventured that he had seen Democrats in D + 30 states with 0.000% chance of losing run themselves ragged raising more and more money. Just as Substack bloggers may reload their browser again and again watching the likes and restacks come in, so politicians will reload their campaign metrics panel watching the flow of donations. Any politician who&#8217;s survived long enough to matter is a little bit paranoid and will never truly accept that their safe seat is safe. These people aren't corrupt. They're not spending the money on campaign Lamborghinis. They don't even necessarily have some future campaign they're saving it for. They're just addicted to fundraising.</p><p>And why does the second prong work? Why does donating to a Congressman buy their goodwill if you also donated an equal amount to their opponent?</p><p>Part of the answer is the same as above: it can buy leadership positions, it can satisfy an irrational addiction to money. But another part is that politicians don&#8217;t like thinking of donations as a corrupt <em>quid pro quo</em>. The AIPAC strategy, where you know the PAC will fund your opponent if you don't do what they want, is something of an exception. Usually it's just - you have a random bill on toilet regulation or something in front of you. A bunch of randos want to call you and give their advice. But you see that Americans For Innovative Toilets donated $3295 during your last campaign (and maybe also gave something to your opponent, but whatever, everyone does it). This catches your attention. So you make sure to take their call first, and listen the longest.</p><p>This still doesn't entirely make sense to me. But it's how all PACs (except AIPAC and the machines) operated until 2024.</p><p><strong>III.</strong></p><p>In 2024, the crypto industry raised the stakes.</p><p>Let's put numbers on all of this. In that year, AIPAC raised $87 million. The real estate group that usually plays runner-up raised $20 million.</p><p>Marc Andreessen&#8217;s new crypto PAC, Fairshake, raised $260 million. Just a totally unheard-of amount of money for a single industry.</p><p>How did they do it? In some sense, this isn't surprising. In case you haven't heard, crypto went up a lot. Many people in the industry got rich. A16Z, Marc Andreessen's crypto-heavy venture capital firm, says they invested $8 billion into crypto. Coinbase, the biggest US crypto company, is valued at $85 billion. The richest crypto billionaires have 10-to-11 digit net worths. And government regulation is potentially an existential threat to crypto. So in some sense, it's the least surprising thing in the world that they could scrounge up $250 million to save their multi-hundred-billion-dollar industry. The only reason it's remarkable is that for some reason which I still haven't figured out, nobody else - not the oil industry, not the firearms industry, not the defense industry - ever tried this before.</p><p>So how did the crypto industry pull this together? Andreessen personally donated $40 - $50 million (remember, the second-biggest industry PAC, real estate, raised only $20 million total from all donors, personal and business). Again, this isn't a crazy proportion of his net worth: he has $2 billion, so a $50 million expense hardly forces him back to ramen. It's just that no other billionaire of his stature is even in the game. </p><p>Then his cofounder Ben Horowitz donated another $40 million. Then two big crypto companies (Coinbase and Ripple, both with A16Z links) donated another $40 - 50 million each. As the saying goes, sooner or later it all adds up to real money.</p><p>Anyway, they won overwhelmingly. They combined the business-as-usual strategy of donating to safe incumbents and both sides of close races, with the AIPAC strategy of picking a few big opponents of their cause and airdropping massive sums on their rivals. For example, Representative Katie Porter (D-California) was an Elizabeth Warren ally and cryptocurrency critic. When she ran for Senate, Fairshake dropped $10 million into attack ads against her in the primaries - more than most candidates' total spending. The attack ads didn't say she was bad on crypto - something that approximately no voters care about. They were just normal attack ads on whatever aspect of her policy and personality focus groups said she was most vulnerable on (in practice, an accusation that she mistreated her Congressional staff). She lost badly, coming in third place. Although nobody can prove she wouldn't have lost anyway, conventional wisdom was that crypto had successfully made its point. <a href=\"https://www.sfgate.com/tech/article/crypto-katie-porter-senate-campaign-19829644.php\">According to SFGate</a>:</p><blockquote><p>An unnamed political operative told the magazine: &#8220;Porter was a perfect choice because she let crypto declare, &#8216;If you are even slightly critical of us, we won&#8217;t just kill you&#8212;we&#8217;ll kill your f&#8212;king family, we&#8217;ll end your career.&#8217; From a political perspective, it was a masterpiece.&#8221; The scare campaign appears to have worked. The House of Representatives passed a pro-crypto bill, with bipartisan support, in May. Candidates with Fairshake&#8217;s support won their primaries in 85% of cases, the New Yorker wrote. Now, neither presidential candidate wants to run astray of the industry: Donald Trump spoke at a crypto conference, and Kamala Harris signaled her support. And Porter is forced out of Congress.</p></blockquote><p>These are all important signs that crypto&#8217;s bet is paying off, but I think I know what metric the crypto barons themselves are watching, and if anything it&#8217;s even more bullish:</p><div class=\"captioned-image-container\"><figure><a class=\"image-link image2 is-viewable-img\" href=\"https://substackcdn.com/image/fetch/$s_!bWaN!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F13d918d0-5918-4bf6-9dee-0a798c76ae82_706x499.png\" target=\"_blank\"><div class=\"image2-inset\"><source type=\"image/webp\" /><img alt=\"\" class=\"sizing-normal\" height=\"392.9801699716714\" src=\"https://substackcdn.com/image/fetch/$s_!bWaN!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F13d918d0-5918-4bf6-9dee-0a798c76ae82_706x499.png\" width=\"556\" /><div class=\"image-link-expand\"><div class=\"pencraft pc-display-flex pc-gap-8 pc-reset\"><div class=\"pencraft pc-reset icon-container restack-image\"><svg class=\"lucide lucide-refresh-cw\" fill=\"none\" height=\"20\" stroke=\"currentColor\" stroke-linecap=\"round\" stroke-linejoin=\"round\" stroke-width=\"2\" viewBox=\"0 0 24 24\" width=\"20\" xmlns=\"http://www.w3.org/2000/svg\"><path d=\"M3 12a9 9 0 0 1 9-9 9.75 9.75 0 0 1 6.74 2.74L21 8\"></path><path d=\"M21 3v5h-5\"></path><path d=\"M21 12a9 9 0 0 1-9 9 9.75 9.75 0 0 1-6.74-2.74L3 16\"></path><path d=\"M8 16H3v5\"></path></svg></div><div class=\"pencraft pc-reset icon-container view-image\"><svg class=\"lucide lucide-maximize2 lucide-maximize-2\" fill=\"none\" height=\"20\" stroke=\"currentColor\" stroke-linecap=\"round\" stroke-linejoin=\"round\" stroke-width=\"2\" viewBox=\"0 0 24 24\" width=\"20\" xmlns=\"http://www.w3.org/2000/svg\"><polyline points=\"15 3 21 3 21 9\"></polyline><polyline points=\"9 21 3 21 3 15\"></polyline><line x1=\"21\" x2=\"14\" y1=\"3\" y2=\"10\"></line><line x1=\"3\" x2=\"10\" y1=\"21\" y2=\"14\"></line></svg></div></div></div></div></a><figcaption class=\"image-caption\">Red arrow represents the 2024 election.</figcaption></figure></div><p>Crypto titans had many valid complaints. The Biden administration&#8217;s crypto regulation policy was arbitrary and punitive, and occasionally skirted the border of illegality. It genuinely harmed innovation and held back important industries like remittances, digital payments, and (of course) prediction markets. As a crypto bag-holder myself, I can&#8217;t complain about all the beautiful verdant green on the chart above. Still, winning this hard is maybe a little humiliating. Does the government really need a <a href=\"https://www.whitehouse.gov/presidential-actions/2025/03/establishment-of-the-strategic-bitcoin-reserve-and-united-states-digital-asset-stockpile/\">strategic Bitcoin reserve?</a> Should it really <a href=\"https://www.theblock.co/post/368631/us-government-data-public-blockchains\">release economic data on three different blockchains?</a> Must we really have <a href=\"https://foxbaltimore.com/news/nation-world/washington-dc-president-donald-trump-bitcoin-statue-federal-reserve-interest-rate-cut-decision-us-capitol-building-cryptocurrency-wall-street-financial-markets-digital\">a twelve foot high golden statue of Trump holding a Bitcoin in front of the US Capitol?</a> We&#8217;re exploring bold new territory here.</p><div class=\"captioned-image-container\"><figure><a class=\"image-link image2 is-viewable-img\" href=\"https://substackcdn.com/image/fetch/$s_!eZlq!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Faa435a03-c505-4414-8be8-4a3b76dfad12_738x612.png\" target=\"_blank\"><div class=\"image2-inset\"><source type=\"image/webp\" /><img alt=\"\" class=\"sizing-normal\" height=\"398.0487804878049\" src=\"https://substackcdn.com/image/fetch/$s_!eZlq!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Faa435a03-c505-4414-8be8-4a3b76dfad12_738x612.png\" width=\"480\" /><div class=\"image-link-expand\"><div class=\"pencraft pc-display-flex pc-gap-8 pc-reset\"><div class=\"pencraft pc-reset icon-container restack-image\"><svg class=\"lucide lucide-refresh-cw\" fill=\"none\" height=\"20\" stroke=\"currentColor\" stroke-linecap=\"round\" stroke-linejoin=\"round\" stroke-width=\"2\" viewBox=\"0 0 24 24\" width=\"20\" xmlns=\"http://www.w3.org/2000/svg\"><path d=\"M3 12a9 9 0 0 1 9-9 9.75 9.75 0 0 1 6.74 2.74L21 8\"></path><path d=\"M21 3v5h-5\"></path><path d=\"M21 12a9 9 0 0 1-9 9 9.75 9.75 0 0 1-6.74-2.74L3 16\"></path><path d=\"M8 16H3v5\"></path></svg></div><div class=\"pencraft pc-reset icon-container view-image\"><svg class=\"lucide lucide-maximize2 lucide-maximize-2\" fill=\"none\" height=\"20\" stroke=\"currentColor\" stroke-linecap=\"round\" stroke-linejoin=\"round\" stroke-width=\"2\" viewBox=\"0 0 24 24\" width=\"20\" xmlns=\"http://www.w3.org/2000/svg\"><polyline points=\"15 3 21 3 21 9\"></polyline><polyline points=\"9 21 3 21 3 15\"></polyline><line x1=\"21\" x2=\"14\" y1=\"3\" y2=\"10\"></line><line x1=\"3\" x2=\"10\" y1=\"21\" y2=\"14\"></line></svg></div></div></div></div></a><figcaption class=\"image-caption\">Give me your degens, your risk-seeking. Your huddled masses, yearning to bet free.</figcaption></figure></div><p><strong>IV.</strong></p><p>&#8230;and we&#8217;ll be exploring it a whole lot more, very soon.</p><p>Last month, the AI industry <a href=\"https://www.marketingaiinstitute.com/blog/ai-super-pac\">announced a new SuperPAC</a> called &#8220;Leading The Future&#8221; (a dumb name, but, in their defense, &#8220;AIPAC&#8221; was already taken). They start with $200 million in seed funding, led by a $50 million donation by Andreessen Horowitz, and another $50 million from OpenAI co-founder Greg Brockman.</p><p>(Why Brockman and not Altman, or OpenAI as a corporation? Because most people don&#8217;t know who Brockman is, so this keeps OpenAI&#8217;s hands clean. I imagine Altman going into a meeting, pointing at Brockman, and saying &#8220;I&#8217;m famous, you&#8217;re not, please cough up $50 million of your own money for the cause.&#8221;)</p><p>On the same day, Meta announced their own SuperPAC, <a href=\"https://finance.yahoo.com/news/meta-spend-tens-millions-pro-175939046.html\">Mobilizing Economic Transformation Across</a> (META) California. Why two PACs? Opinions differ; one person told me that it lets the general PAC avoid the negative associations that Facebook has gathered over the years, but <a href=\"https://archive.is/Cvt3P\">the Verge thinks</a> that maybe everyone else in tech hates Zuckerberg too much to work with him. Meta has committed to spending &#8220;tens of millions&#8221;.</p><p>Most likely, the new PAC will use the playbook pioneered by crypto: destroy any candidate who dares support regulations on AI, by funding attack ads that don&#8217;t mention AI in any way and, at best, briefly mention the name &#8220;Leading The Future&#8221;.</p><p>Just the Andreessen/Brockman SuperPAC, without any help from Meta, is already twice as rich as AIPAC. Their existence sends a clear message: we are going to crush any politician who tries to regulate AI.</p><p><strong>V.</strong></p><p>&#8230;unless someone stops them.</p><p>Leading The Future still only has 2% as much money as the almond industry. The tiny scale of US political spending is dangerous insofar as it means that one or two billionaires willing to go all-in can distort the national landscape. But it also makes it possible to oppose them. Certainly if you can get one or two billionaires of your own - but it might even be within the range of a committed group of ordinary people. Not waiters and bartenders, maybe. But if safe AI supporters were as committed as Israel supporters, they could probably make something happen.</p><p>For a long time, the AI safety movement has underperformed politically. Effective altruism includes thousands of well-off people committed to spending 10% of their income on improving the world. If a thousand of them gave $7K each to political candidates, that would be $7 million of campaign-finance-compliant hard money - about as much as anyone can gather for anything. Hard money buys more influence per dollar than soft money, so this could be a big deal. All you&#8217;d need is the right people to coordinate it. </p><p>So far, this has been slow going. Partly it&#8217;s because in the early 2020s, people affiliated with FTX took point on this effort; when FTX imploded, it not only took its incipient political infrastructure with it, but poisoned the well for future efforts. And partly it&#8217;s because EAs overlearned the lesson of the early 2010s, when we spoke out against AI capabilities efforts so &#8220;effectively&#8221; that a bunch of people thought &#8220;wow, AI capabilities companies must be a really big deal, maybe I should found one!&#8221;; the resulting institutional scar tissue biased us towards staying quiet about our concerns. </p><p>Still, I wouldn&#8217;t be writing this if the consultants and activists weren&#8217;t gearing up for a bigger fight. They asked me to include some action items for readers who want to participate:</p><ul><li><p>Email <strong>aisafetypolitics@gmail.com</strong> to connect to the people organizing this effort and talk with them about what you can do, including potential future donation opportunities. </p></li><li><p><strong><a href=\"https://secure.actblue.com/donate/boresai?refcode=acx\">Donate to support Alex Bores</a></strong>. Bores is a New York state representative who authored <a href=\"https://newsletter.safe.ai/p/ai-safety-newsletter-57-the-raise\">the RAISE Act</a>, one of the two most exciting state-level AI safety bills. Earlier today, he announced he will be running for Congress, and is a likely target for Andreessen&#8217;s SuperPAC. This might be the most impactful AI policy giving opportunity for a while - if he&#8217;s seen to get a large stream of pro-AI-safety money, this might defuse people&#8217;s worries and demonstrate that our side can hold their own - and donations today (his first day in the race) are especially impactful. You can <a href=\"https://ericneyman.wordpress.com/2025/10/20/consider-donating-to-alex-bores-author-of-the-raise-act/\">read a longer case for Bores here</a>. Remember that donating to Democratic candidates may affect your career opportunities (eg make it harder to work in a Republican administration) or get you on political spammers&#8217; mailing lists.</p></li><li><p>(Eventually) donate to support Scott Wiener. Wiener is a California state representative who authored SB 53, the other exciting state-level AI safety bill. He <a href=\"https://www.politico.com/news/2025/10/17/nancy-pelosi-faces-primary-challenge-from-california-state-lawmaker-00613787\">is widely expected</a> to run for Congress in San Francisco. I&#8217;ll keep you updated if/when this happens.</p></li></ul><p>And watch Open Threads in case I announce other things in this category.</p>"
            ],
            "link": "https://www.astralcodexten.com/p/tech-pacs-are-closing-in-on-the-almonds",
            "publishedAt": "2025-10-21",
            "source": "SlateStarCodex",
            "summary": "<p><strong>I.</strong></p><p>In my 2019 post <a href=\"https://slatestarcodex.com/2019/09/18/too-much-dark-money-in-almonds/\">Too Much Dark Money In Almonds</a>, I asked: why is there so little money in politics?</p><p>During the 2018 election, Americans - candidates, parties, PACs, and small donors like you - spent a combined $5 billion pushing their preferred candidates. Although that sounds like a lot of money, Americans spent $12 billion on almonds that same year. Why the imbalance? The oil industry has strong political opinions, and they make $500 billion per year. Do they really think electing oil-friendly politicians isn&#8217;t worth 2% of revenue?</p><p>We debated how this could be. Some of the discussion proved prescient - I asked if maybe Elon Musk should buy some kind of social media property. But we never found a good answer, and the implied question remained open: if some billionaire wanted to spend an actually relevant percent of his net worth on politics, could he just take over everything?</p><p>I recently talked to some Silicon Valley political consultants who updated me on the status of this issue: Marc Andreessen tried this in 2024 and it basically worked. Now he is trying it a second time, it will probably work again, and Marc Andreessen will probably own every politician twice",
            "title": "Tech PACs Are Closing In On The Almonds"
        },
        {
            "content": [
                "<p>Some podcasts are self-recommending on the \u2018yep, I\u2019m going to be breaking this one down\u2019 level. This was very clearly one of those. So here we go.</p>\n<p>As usual for podcast posts, the baseline bullet points describe key points made, and then the nested statements are my commentary.</p>\n<p>If I am quoting directly I use quote marks, otherwise assume paraphrases.</p>\n<p>Rather than worry about timestamps, <a href=\"https://www.youtube.com/watch?v=lXUZvyajciY\">I\u2019ll use YouTube\u2019s</a> section titles, as it\u2019s not that hard to <a href=\"https://www.youtube.com/redirect?event=video_description&amp;redir_token=QUFFLUhqa1VlLXBpLWZEZmJIdG1SWlZFeExxTHdYOHlPUXxBQ3Jtc0ttZjhrSWx4NUJVWlVVY21jS0oyZ0NYWVJ5RWZFanZ4VEFZbzduYVZpalpmdkI3M1oxTmZWN19NYy0waVluekYzME9LRXoxMXl1Mlg5bmpsQjhXeElRSm1zakpTcXQtazh0UGFrVmtoWWFDNFg5TVMyRQ&amp;q=https%3A%2F%2Fdwarkesh.substack.com%2Fp%2Fandrej-karpathy&amp;v=lXUZvyajciY\">find things via the transcript</a> as needed.</p>\n<p>This was a fun one in many places, interesting throughout, frustrating in similar places to where other recent Dwarkesh interviews have been frustrating. It gave me a lot of ideas, some of which might even be good.</p>\n<div>\n\n\n<span id=\"more-24806\"></span>\n\n\n</div>\n<div>\n<div>\n<div>\n<div>Double click to interact with video</div>\n</div>\n</div>\n</div>\n\n\n<h4 class=\"wp-block-heading\">AGI Is Still a Decade Away</h4>\n\n\n<ol>\n<li>Andrej calls this the \u2018decade of agents\u2019 contrary to (among others who have said it) the Greg Brockman declaration that 2025 is the \u2018year of agents,\u2019 as there is so much work left to be done. Think of AI agents as employees or interns, that right now mostly can\u2019t do the things due to deficits of intelligence and context.\n<ol>\n<li>I agree that 2025 as the year of the agent is at least premature.</li>\n<li>You can defend the 2025 claim if you focus on coding, Claude Code and Codex, but I think even there it is more confusing than helpful as a claim.</li>\n<li>I also agree that we will be working on improving agents for a long time.</li>\n<li>2026 might be the proper \u2018year of the agent\u2019 as when people start using AI agents for a variety of tasks and getting a bunch of value from them, but they will still have a much bigger impact on the world in 2027, and again in 2028.</li>\n<li>On the margin and especially outside of coding, I think context and inability to handle certain specific tasks (especially around computer use details) are holding things back right now more than intelligence. A lot of it seems eminently solvable quickly in various ways if one put it in the work.</li>\n</ol>\n</li>\n<li>Dwarkesh points to lack of continual learning or multimodality, but notes it\u2019s hard to tell how long it will take. Andrej says \u2018well I have 15 years of prediction experience and intuition and I average things out and it feels like a decade to me.\u2019\n<ol>\n<li>A decade seems like an eternity to me on this.</li>\n<li>If it\u2019s to full AGI it is slow but less crazy. So perhaps this is Andrej saying that to count as an agent for this the AI needs to essentially be AGI.</li>\n</ol>\n</li>\n<li>AI has had a bunch of seismic shifts, Andrej has seen at least two and they seem to come with regularity. Neural nets used to be a niche thing before AlexNet but they were still trained per-task, the focus on Atari and other games was a mistake because you want to interact with the \u2018real world\u2019. Then LLMs. The common mistake was trying to \u201cget the full thing too early\u201d and especially aiming at agents too soon.\n<ol>\n<li>The too soon thing seems true and important. You can\u2019t unlock capabilities in a useful way until you have the groundwork and juice for them.</li>\n<li>Once you do have the groundwork and juice, they tend to happen quickly, without having to do too much extra work.</li>\n<li>In general, seems like if something is super hard to do, better if you wait?</li>\n<li>However you can with focused effort make a lot of progress beyond what you\u2019d get at baseline, even if that ultimately stalls out, as seen by the Atari and universe examples.</li>\n</ol>\n</li>\n<li>Dwarkesh asks what about the Sutton perspective, should you be able to throw an AI out there into the world the way you would a human or animal and just work with and \u2018grow up\u2019 via sensory data? Andrej points to his response to Sutton, that biological brains work via a very different process, we\u2019re building ghosts not animals, although we should make them more \u2018animal-like\u2019 over time. But animals don\u2019t do what Sutton suggests, they use an evolutionary outer loop. Animals only use RL for non-intelligence tasks, things like motor skills.\n<ol>\n<li>I think humans do use RL on intelligence tasks? My evidence for this is that when I use this model of humans it seems to make better predictions, both about others and about myself.</li>\n<li>Humans are smarter about this than \u2018pure RL\u2019 of course, including being the meta programmer and curating their own training data.</li>\n</ol>\n</li>\n<li>Dwarkesh contrasts pre-training with evolution in that evolution compacts all info into 3 GB of DNA, thus evolution is closer to finding a lifetime learning algorithm. Andrej agrees there is miraculous compression in DNA and that it includes learning algorithms, but we\u2019re not here to build animals, only useful things, and they\u2019re \u2018crappy\u2019 but what know how to build are the ghosts. Dwarkesh says evolution does not give us knowledge, it gives us the algorithm to find knowledge a la Sutton.\n<ol>\n<li>Dwarkesh is really big on the need for continual (or here he says \u2018lifetime\u2019) learning and the view that it is importantly distinct from what RL does.</li>\n<li>I\u2019m not convinced. As Dario points out, in theory you can put everything in the context window. You can do a lot better on memory and imitating continual learning than that with effort, and we\u2019ve done remarkably little on such fronts.</li>\n<li>The actual important difference to me is more like sample efficiency. I see ways around that problem too, but am not putting them in this margin.</li>\n<li>I reiterate that evolution actually does provide a lot of knowledge, actually, or the seeds to getting specific types of knowledge, using remarkably few bits of data to do this. If you buy into too much \u2018blank slate\u2019 you\u2019ll get confused.</li>\n</ol>\n</li>\n<li>Andrej draws a distinction between the neural net picking up all the knowledge in its training data versus it becoming more intelligent, and often you don\u2019t even want the knowledge, we rely on it too much, and this is part of why agents are bad at \u201cgoing off the data manifold of what exists on the internet.\u201d We want the \u201ccognitive core.\u201d\n<ol>\n<li>I buy that you want to minimize the compute costs associated with carrying lots of extra information, so for many tasks you want a Minimum Viable Knowledge Base. I don\u2019t buy that knowledge tends to get in the way. If it does, then Skill Issue.</li>\n<li>More knowledge seems hard to divorce fully from more intelligence. A version of me that was abstractly \u2018equally smart,\u2019 but which knew far less, might technically have the same Intelligence score on the character sheet, but a lot lower Wisdom and would effectively be kind of dumb. See young people.</li>\n<li>I\u2019m skeptical about a single \u2018cognitive core\u2019 for similar reasons.</li>\n</ol>\n</li>\n<li>Dwarkesh reiterates in-context learning as \u2018the real intelligence\u2019 as distinct from gradient descent. Andrej agrees it\u2019s not explicit, it\u2019s \u201cpattern completion within a token window\u201d but notes there\u2019s tons of patterns on the internet that get into the weights, and it\u2019s possible in-context learning runs a small gradient descent loop inside the neural network. Dwarkesh asks, \u201cwhy does it feel like with in-context learning we\u2019re getting to this continual learning, real intelligence-like thing? Whereas you don\u2019t get the analogous feeling just from pre-training.\u201d\n<ol>\n<li>My response would basically again be sample efficiency, and the way we choose to interact with LLMs being distinct from the training? I don\u2019t get this focus on (I kind of want to say fetishization of?) continual learning as a distinct thing. It doesn\u2019t feel so distinct to me.</li>\n</ol>\n</li>\n<li>Dwarkesh asks, how much of the information from training gets stored in the model? He compares KV cache of 320 kilobytes to a full 70B model trained on 15 trillion tokens. Andrej thinks models get a \u2018hazy recollection\u2019 of what happened in training, the compression is dramatic to get 15T tokens into 70B parameters.\n<ol>\n<li>Is it that dramatic? Most tokens don\u2019t contain much information, or don\u2019t contain new information. In some ways 0.5% (70B vs. 15T) is kind of a lot. It depends on what you care about. If you actually had to put it all in the 320k KV Cache that\u2019s a lot more compression.</li>\n<li>As Andrej says, it\u2019s not enough, so you get much more precise answers about texts if you have the full text in the context window. Which is also true if you ask humans about the details of things that mostly don\u2019t matter.</li>\n</ol>\n</li>\n<li>What part about human intelligence have we most failed to replicate? Andrej says \u2018a lot of it\u2019 and starts discussing physical brain components causing \u201cthese cognitive deficits that we all intuitively feel when we talk to them models.\u201d\n<ol>\n<li>I feel like that\u2019s a type mismatch. I want to know what capabilities are missing, not which physical parts of the brain? I agree that intuitively some capabilities are missing, but I\u2019m not sure how essential this is, and as Andrej suggests we shouldn\u2019t be trying to build an analog of a human.</li>\n</ol>\n</li>\n<li>Dwarkesh turns back to continual learning, asks if it will emerge spontaneously if the model gets the right incentives. Andrej says no, that sleep does this for humans where \u2018the context window sometimes sticks around\u2019 and there\u2019s no natural analog, but we want a way to do this, and points to sparse attention.\n<ol>\n<li>I\u2019m not convinced we know how the sleep or \u2018sticking around\u2019 thing works, clearly there is something going on somewhere.</li>\n<li>I agree this won\u2019t happen automatically under current techniques, but we can use different techniques, and again I\u2019m getting the Elle Woods \u2018what, like it\u2019s hard?\u2019 reaction to all this, where \u2018hard\u2019 is relative to problem importance.</li>\n</ol>\n</li>\n<li>Andrej kind of goes Lindy, pointing to <a href=\"https://en.wikipedia.org/wiki/Translational_symmetry\">translation invariance</a> to expect algorithmic and other changes at a similar rate to the past, and pointing to the many places he says we\u2019d need gains in order to make further progress, that various things are \u2018all surprisingly equal,\u2019 it needs to improve \u2018across the board.\u2019\n<ol>\n<li>Is this the crux, the fundamental disagreement about the future, in two ways?</li>\n<li>The less important one is the idea that progress requires all of [ABCDE] to make progress. That seems wrong to me. Yes, you are more efficient if you make progress more diffusely under exponential scaling laws, but you can still work around any given deficit via More Dakka.</li>\n<li>As a simple proof by hypothetical counterexample, suppose I held one of his factors (e.g. architecture, optimizer, loss function) constant matching GPT-3, but could apply modern techniques and budgets to the others. What do I get?</li>\n<li>More importantly, Andrej is denying the whole idea that technological progress here or in general is accelerating, or will accelerate. And that seems deeply wrong on multiple levels?</li>\n<li>For this particular question, progress has been rapid, investments of all kinds have been huge, and already we are seeing AI directly accelerate AI progress substantially, a process that will accelerate even more as AI gets better, even if it doesn\u2019t cross into anything like full automated AI R&amp;D or a singularity, and we keep adding more ways to scale. It seems rather crazy to expect 2025 \u2192 2035 to be similar to 2015 \u2192 2025 in AI, on the level of \u2018wait, you\u2019re suggesting what?\u2019</li>\n<li>In the longer arc of history, if we\u2019re going to go there, we see a clear acceleration of time. So we have the standard several billion years to get multicellular life, several hundred million years to get close to human intelligence, several hundred thousand to million years to get agriculture and civilization, several thousand years to get the industrial revolution, several hundred years to get the information age, several dozen years to get AI to do anything useful on the general intelligence front, several ones of years to go from \u2018anything useful at all\u2019 to GPT-5 and Sonnet 4.5 being superhuman in many domains already.</li>\n<li>I think Andrej makes better arguments for relatively long (still remarkably short!) timelines later, but him invoking this gives me pause.</li>\n</ol>\n</li>\n</ol>\n\n\n<h4 class=\"wp-block-heading\">LLM Cognitive Deficits</h4>\n\n\n<ol>\n<li>Andrej found LLMs of little help when assembling his new repo nanochat, which is a an 8k-line set of all the things you need for a minimal ChatGPT clone. He still used autocomplete, but vibe coding only works with boilerplate stuff. In particular, the models \u2018remember wrong\u2019 from all the standard internet ways of doing things, that he wasn\u2019t using. For example, he did his own version of a DDP container inside the code, and the models couldn\u2019t comprehend that and kept trying to use DDP instead. Whereas he only used vibe coding for a few boilerplate style areas.\n<ol>\n<li>I\u2019ve noticed this too. LLMs will consistently make the same mistakes, or try to make the same changes, over and over, to match their priors.</li>\n<li>It\u2019s a reasonable prior to think things like \u2018oh almost no one would ever implement a version of DDP themselves,\u2019 the issue is that they aren\u2019t capable of being told that this happened and having this overcome that prior.</li>\n</ol>\n</li>\n<li>\u201cI also feel like it\u2019s annoying to have to type out what I want in English because it\u2019s too much typing. If I just navigate to the part of the code that I want, and I go where I know the code has to appear and I start typing out the first few letters, autocomplete gets it and just gives you the code. This is a very high information bandwidth to specify what you want.\u201d\n<ol>\n<li>As a writer this resonates so, so much. There are many tasks where in theory the LLM could do it for me, but by the time I figure out how to get the LLM to do it for me, I might as well have gone and done it myself.</li>\n<li>Whereas the autocomplete in gmail is actually good enough that it\u2019s worth my brain scanning it to see if it\u2019s what I wanted to type (or on occasion, a better version).</li>\n</ol>\n</li>\n<li>Putting it together: LLMs are very good at code that has been written many times before, and poor at code that has not been written before, in terms of the structure and conditions behind the code. Code that has been written before on rare occasions is in between. The modes are still amazing, and can often help. On the vibe coding: \u201cI feel like the industry is making too big of a jump and is trying to pretend like this is amazing, and it\u2019s not. It\u2019s slop.\u201d\n<ol>\n<li>There\u2019s a big difference between the value added when you can successfully vibe code large blocks of code, versus when you can get answers to questions, debugging notes and stuff like that.</li>\n<li>The second category can still be a big boost to productivity, including to AI R&amp;D, but isn\u2019t going to go into crazy territory or enter into recursion mode.</li>\n<li>I presume Andrej is in a position where his barrier for \u2018not slop\u2019 is super high and the problems he works on are unusually hostile as well.</li>\n<li>I do think these arguments are relevant evidence for longer timelines until crazy happens, that we risk overestimating the progress made on vibe coding.</li>\n</ol>\n</li>\n<li>Andrej sees all of computing as a big recursive self-improvement via things like code editors and syntax highlighting and even data checking and search engines, in a way that is continuous with AI. Better autocomplete is the next such step. We\u2019re abstracting, but it is slow.\n<ol>\n<li>One could definitely look at it this way. It\u2019s not obvious what that reframing pushes one towards.</li>\n</ol>\n</li>\n</ol>\n\n\n<h4 class=\"wp-block-heading\">RL Is Terrible</h4>\n\n\n<ol>\n<li>How should we think about humans being able to build a rich world model from interactions with the environment, without needing final reward? Andrej says they don\u2019t do RL, they do something different, whereas RL is terrible but everything else we\u2019ve tried has been worse. All RL can do is check the final answers, and say \u2018do more of this\u2019 when it works. A human would evaluate parts of the process, an LLM can\u2019t and won\u2019t do this.\n<ol>\n<li>So yeah, RL is like democracy. Fair enough.</li>\n<li>Why can\u2019t we set up LLMs to do the things human brains do here? Not the exact same thing, but something built on similar principles?</li>\n<li>I mean it seems super doable to me, but if you want me to figure out how to do it or actually try doing it the going rate is at least $100 million. Call me.</li>\n</ol>\n</li>\n<li>Dwarkesh does ask why, or at least about process supervision. Andrej says it is tricky how to do that properly, how do you assign credit to partial solutions? Labs are trying to use LLM judges but this is actually subtle, and you\u2019ll run into adversarial examples if you do it for too long. It finds out that dhdhdhdh was an adversarial example so it starts outputting that, or whatever.\n<ol>\n<li>So then you\u2026 I mean I presume the next 10 things I would say here have already been tried and they fail but I\u2019m not super confident in that.</li>\n</ol>\n</li>\n<li>So train models to be more robust? Find the adversarial examples and fix them one at a time won\u2019t work, there will always be another one.\n<ol>\n<li>Certainly \u2018find the adversarial examples and fix them one at a time\u2019 is an example of \u2018how to totally fail OOD or at the alignment problem,\u2019 you would need a way to automatically spot when you\u2019re invoking one.</li>\n</ol>\n</li>\n</ol>\n\n\n<h4 class=\"wp-block-heading\">How Do Humans Learn?</h4>\n\n\n<ol>\n<li>What about the thing where humans sleep or daydream, or reflect? Is there some LLM analogy? Andrej says basically no. When an LLM reads a book it predicts the next token, when a human does they do synthetic data generation, talk about it with their friends, manipulate the info to gain knowledge. But doing this with LLMs is nontrivial, for reasons that are subtle and hard to understand, and if you generate synthetic data to train on that makes the model worse, because the examples are silently collapsed, similar to how they know like 3 total jokes. LLMs don\u2019t retain entropy, and we don\u2019t know how to get them to retain it. \u201cI guess what I\u2019m saying is, say we have a chapter of a book and I ask an LLM to think about it, it will give you something that looks very reasonable. But if I ask it 10 times, you\u2019ll notice that all of them are the same. Any individual sample will look okay, but the distribution of it is quite terrible.\u201d\n<ol>\n<li>I wish Andrej\u2019s answer here was like 5 minutes longer. Or maybe 50 minutes.</li>\n<li>In general, I\u2019m perhaps not typical, but I\u2019d love to hear the \u2018over your head\u2019 version where he says a bunch of things that gesture in various directions, and it\u2019s up to you whether you want to try and understand it.</li>\n<li>I mean from the naive perspective this has \u2018skill issue\u2019 written all over it, and there\u2019s so many things I would want to try.</li>\n</ol>\n</li>\n<li>\u201cI think that there\u2019s possibly no fundamental solution to this. I also think humans collapse over time. These analogies are surprisingly good. Humans collapse during the course of their lives. This is why children, they haven\u2019t overfit yet&#8230; We end up revisiting the same thoughts. We end up saying more and more of the same stuff, and the learning rates go down, and the collapse continues to get worse, and then everything deteriorates.\u201d\n<ol>\n<li>I feel this.</li>\n<li>That means both in myself, and in my observations of others.</li>\n<li>Mode collapse in humans is evolutionarily and strategically optimal, under conditions of aging and death. If you\u2019re in exploration, pivot to exploitation.</li>\n<li>We also have various systems to fight this and pivot back to exploration.</li>\n<li>One central reason humans get caught in mode collapse, when we might not want that it, is myopia and hyperbolic discounting.</li>\n<li>Another is, broadly speaking, \u2018liquidity or solvency constraints.\u2019</li>\n<li>A third would be commitments, signaling, loyalty and so on.</li>\n<li>If we weren\u2019t \u2018on the clock\u2019 due to aging, which both cuts the value of exploration and also raises the difficulty of it, I think those of us who cared could avoid mode collapse essentially indefinitely.</li>\n<li>Also I notice [CENSORED] which has obvious deep learning implications?</li>\n</ol>\n</li>\n<li>Could dreaming be a way to avoid mode collapse by going out of distribution?\n<ol>\n<li>I mean, maybe, but the price involved seems crazy high for that.</li>\n<li>I worry that we\u2019re using \u2018how humans do it\u2019 as too much of a crutch.</li>\n</ol>\n</li>\n<li>Andrej notes you should always be seeking entropy in your life, suggesting talking to other people.\n<ol>\n<li>There are lots of good options. I consume lots of text tokens.</li>\n</ol>\n</li>\n<li>What\u2019s up with children being great at learning, especially things like languages, but terrible at remembering experiences or specific information? LLMs are much better than humans at memorization, and this can be a distraction.\n<ol>\n<li>I\u2019m not convinced this is actually true?</li>\n<li>A counterpoint is that older people learn harder things, and younger people, especially young children, simply cannot learn those things at that level, or would learn them a lot slower.</li>\n<li>Another counterpoint is that a lot of what younger humans learn is at least somewhat hard coded into the DNA to be easier to learn, and also are replacing nothing which helps you move a lot faster and seem to be making a lot more progress.</li>\n<li>Languages are a clear example of this. I say this as someone with a pretty bad learning disability for languages, who has tried very hard to pick up various additional languages and failed utterly.</li>\n<li>A third counterpoint is that children really do put a ton of effort into learning, often not that efficiently (e.g. rewatching and rereading the same shows and books over and over, repeating games and patterns and so on), to get the information they need. Let your children play, but that\u2019s time intensive. Imagine what adults can and do learn when they truly have no other responsibilities and go all-in on it.</li>\n</ol>\n</li>\n<li>How do you solve model collapse? Andrej doesn\u2019t know, the models be collapsed, and Dwarkesh points out RL punishes output diversity. Perhaps you could regularize entropy to be higher, it\u2019s all tricky.</li>\n<li>Andrej says state of the art models have gotten smaller, and he still thinks they memorized too much and we should seek a small cognitive core.\n<ol>\n<li>He comes back to this idea that knowing things is a disadvantage. I don\u2019t get it. I do buy that smaller models are more efficient, especially with inference scaling, and so this is the best practical approach for now.</li>\n<li>My prediction is that the cognitive core hypothesis is wrong, and that knowledge and access to diverse context is integral to thinking, especially high entropy thinking. I don\u2019t think a single 1B model is going to be a good way to get any kind of conversation you want to have.</li>\n<li>There are people who have eidetic memories. They can have a hard time taking advantage because working memory remains limited, and they don\u2019t filter for the info worth remembering or abstracting out of them. So there\u2019s some balance at some point, but I definitely feel like remembering more things than I do would be better? And that I have scary good memory and memorization in key points, such as ability (for a time, anyway) to recall the exact sequence of entire Magic games and tournaments, which is a pattern you also see from star athletes &#8211; you ask Steve Curry or Lebron James and they can tell you every detail of every play.</li>\n</ol>\n</li>\n<li>Most of the internet tokens are total garbage, stock tickers, symbols, huge amounts of slop, and you basically don\u2019t want that information.\n<ol>\n<li>I\u2019m not sure you don\u2019t want that information? It\u2019s weird. I don\u2019t know enough to say. Obviously it would not be hard to filter such tokens out at this point, so they must be doing something useful. I\u2019m not sure it\u2019s due to memorization, but I also don\u2019t see why the memorization would hurt.</li>\n</ol>\n</li>\n<li>They go back and forth over the size of the supposed cognitive core, Dwarkesh asks why not under 1 billion, Andrej says you probably need a billion knobs and he\u2019s already contrarian being that low.\n<ol>\n<li>Whereas yeah, I think 1 billion is not enough and this is the wrong approach entirely unless you want to e.g. do typical simple things within a phone.</li>\n</ol>\n</li>\n</ol>\n\n\n<h4 class=\"wp-block-heading\">AGI Will Blend Into 2% GDP Growth</h4>\n\n\n<p>Wait what?</p>\n<p>Note: The 2% number doesn\u2019t actually come up until the next section on ASI.</p>\n<ol>\n<li>How to measure progress? Andrej doesn\u2019t like education level as a measure of AI progress (I agree), he\u2019s also not a fan of the famous METR horizon length graph and is tempted to reject the whole question. He\u2019s sticking with AGI as &#8216;can do any economically valuable task at human performance or better.\u2019\n<ol>\n<li>And you\u2019re going to say having access to \u2018any economically valuable (digital) task at human performance or better\u2019 only is +2% GDP growth? Really?</li>\n<li>You have to measure something you call AI progress, since you\u2019re going to manage it. Also people will ask constantly and use it to make decisions. If nothing else, you need an estimate of time to AGI.</li>\n</ol>\n</li>\n<li>He says only 10%-20% of the economy is \u2018only knowledge work.\u2019\n<ol>\n<li>I asked Sonnet. McKinsey 2012 finds knowledge work accounted for 31 percent of all workers in America in 2010. Sonnet says 30%-35% pure knowledge work, 12%-17% pure manual, the rest some hybrid, split the rest in half, you get 60% knowledge work by task, but the knowledge work typically is about double the economic value of the non-knowledge work, so we\u2019re talking on the order of 75% of all economic value.</li>\n<li>How much would this change Andrej\u2019s other estimates, given this is more than triple his estimate?</li>\n</ol>\n</li>\n<li>Andrej points to the famous predictions of automating radiology, and suggests what we\u2019ll do more often is have AI do 80% of the volume, then delegate 20% to humans.\n<ol>\n<li>Okay, sure, that\u2019s a kind of intermediate step, we might do that for some period of time. If so, let\u2019s say that for 75% of economic value we have the AI provide 60% of the value, assuming the human part is more valuable. So it\u2019s providing 45% of all economic value if composition of &#8216;labor including AI\u2019 does not change.</li>\n<li>Except of course if half of everything now has marginal cost epsilon (almost but not quite zero), then there will be a large shift in composition to doing more of those tasks.</li>\n</ol>\n</li>\n<li>Dwarkesh compares radiologists to early Waymos where they had a guy in the front seat that never did anything so people felt better, and similarly if an AI can do 99% of a job the human doing the 1% can still be super valuable because bottleneck. Andrej points out radiology turns out to be a bad example for various reasons, suggests call centers.\n<ol>\n<li>If you have 99 AI tasks and 1 human task, and you can\u2019t do the full valuable task without all 100 actions, then in some sense the 1 human task is super valuable.</li>\n<li>In another sense, it\u2019s really not, especially if any human can do it and there is now a surplus of humans available. Market price might drop quite low.</li>\n<li>Wages don\u2019t go up as you approach 99% AI, as Dwarkesh suggests they could, unless you\u2019re increasingly bottlenecked on available humans due to a Jevons Paradox situation or hard limit on supply, both of which are the case in radiology, or this raises required skill levels. This is especially true if you\u2019re automating a wide variety of tasks and there is less demand for labor.</li>\n</ol>\n</li>\n<li>Dwarkesh points out that we don\u2019t seem to be on an AGI paradigm, we\u2019re not seeing large productivity improvements for consultants and accountants. Whereas coding was a perfect fit for a first task, with lots of ready-made places to slot in an AI.\n<ol>\n<li>Skill issue. My lord, skill issue.</li>\n<li>Current LLMs can do accounting out of the box, they can automate a large percentage of that work, and they can enable you to do your own accounting. If you\u2019re an accountant and not becoming more productive? That\u2019s on you.</li>\n<li>That will only advance as AI improves. A true AGI-level AI could very obviously do most accounting tasks on its own.</li>\n<li>Consultants should also be getting large productivity boosts on the knowledge work part of their job, including learning things, analyzing things and writing reports and so on. To the extent their job is to sell themselves and convince others to listen to them, AI might not be good enough yet.</li>\n<li>Andrej asks about automating creating slides. If AI isn\u2019t helping you create slides faster, I mean, yeah, skill issue, or at least scaffolding issue.</li>\n</ol>\n</li>\n<li>Dwarkesh says Andy Matuschak tried 50 billion things to get LLMs to write good spaced repetition prompts, and they couldn\u2019t do it.\n<ol>\n<li>I do not understand what went wrong with the spaced repetition prompts. Sounds like a fun place to bang one\u2019s head for a while and seems super doable, although I don\u2019t know what a good prompt would look like as I don\u2019t use spaced repetition.</li>\n<li>To me, this points towards skill issues, scaffolding issues and time required to git gud and solve for form factors as large barriers to AI value unlocks.</li>\n</ol>\n</li>\n</ol>\n\n\n<h4 class=\"wp-block-heading\">ASI</h4>\n\n\n<ol>\n<li>What about superintelligence? \u201cI see it as a progression of automation in society. Extrapolating the trend of computing, there will be a gradual automation of a lot of things, and superintelligence will an extrapolation of that. We expect more and more autonomous entities over time that are doing a lot of the digital work and then eventually even the physical work some amount of time later. Basically I see it as just automation, roughly speaking.\u201d\n<ol>\n<li>That\u2019s\u2026 not ASI. That\u2019s intelligence denialism. AI as normal technology.</li>\n<li>I took a pause here. It\u2019s worth sitting with this for a bit.</li>\n<li>Except it kind of isn\u2019t, when you hear what he says later? It\u2019s super weird.</li>\n</ol>\n</li>\n<li>Dwarkesh pushes back: \u201cBut automation includes the things humans can already do, and superintelligence implies things humans can\u2019t do.\u201d Andrej gives a strange answer: \u201cBut one of the things that people do is invent new things, which I would just put into the automation if that makes sense.\u201d\n<ol>\n<li>No, it doesn\u2019t make sense? I\u2019m super confused what \u2018just automation\u2019 is supposed to meaningfully indicate?</li>\n<li>If what we are automating is \u2018being an intelligence\u2019 then everything AI ever does is always \u2018just automation\u2019 but that description isn\u2019t useful.</li>\n<li>Humans can invest and do new things but superintelligence can invent and do new things that are in practice not available to humans, \u2018invent new things\u2019 is not the relevant natural category here.</li>\n</ol>\n</li>\n<li>Andrej worries about a gradual loss of control and understanding of what is happening, and thinks this is the most likely outcome. Multiple competing entities, initially competing on behalf of people, that gradually become more autonomous, some go rogue, others fight them off. They still get out of control.\n<ol>\n<li>No notes, really. That\u2019s the baseline scenario if we solve a few other impossible-level problems (or get extremely lucky that they\u2019re not as hard as they look to me) along the way.</li>\n<li>Andrej doesn\u2019t say \u2018unless\u2019 here, or offer a solution or way to prevent this.</li>\n<li>Missing mood?</li>\n</ol>\n</li>\n<li>Dwarkesh asks, will we see an intelligence explosion if we have a million copies of you running in parallel super fast? Andrej says yes, but best believe in intelligence explosions because you\u2019re already living in one and have been for decades, that\u2019s why GDP grows, this is all continuous with the existing hyper-exponential trend, previous techs also didn\u2019t make GDP go up much, everything was slow diffusion.\n<ol>\n<li>It\u2019s so weird to say \u2018oh, yeah, the million copies of me sped up a thousand times would just be more of the same slow growth trends, ho hum, intelligence explosion,\u2019 \u201cit\u2019s just more automation.\u201d</li>\n</ol>\n</li>\n<li>\u201cWe\u2019re still going to have an exponential that\u2019s going to get extremely vertical. It\u2019s going to be very foreign to live in that kind of an environment.\u201d \u2026 \u201cYes, my expectation is that it stays in the same [2% GDP growth rate] pattern.\u201d\n<ol>\n<li>I\u2026 but\u2026 um\u2026 I\u2026 what?</li>\n<li>Don\u2019t you have to pick a side? He seems to keep trying to have his infinite cakes and eat them too, both an accelerating intelligence explosion and then magically GDP growth stays at 2% like it\u2019s some law of nature.</li>\n</ol>\n</li>\n<li>\u201cSelf-driving as an example is also computers doing labor. That\u2019s already been playing out. It\u2019s still business as usual.\u201d\n<ol>\n<li>Self-driving is a good example of slow diffusion of the underlying technology for various reasons. It\u2019s been slow going, and mostly isn\u2019t yet going.</li>\n<li>This is a clear example of an exponential that hasn\u2019t hit you yet. Self-driving cars are Covid-19 in January 2020, except they\u2019re a good thing.</li>\n<li>A Fermi estimate for car trips in America per week is around 2 billion, or for rideshares about 100 million per week.</li>\n<li>Waymo got to 100,000 weekly rides in August 2024, was at 250,000 weekly rides in April 2025, we don\u2019t yet have more recent data but <a href=\"https://manifold.markets/JoshYou/how-many-paid-driverless-waymo-ride\">this market</a> estimates roughly 500,000 per week by year end. That\u2019s 0.5% of taxi rides. The projection for end of year 2026 says maybe 1.5 million rides per week, 1.5%.</li>\n<li>Currently the share of non-taxi rides that are full self-driving is essentially zero, maybe 0.2% of trips have meaningful self driving components.</li>\n<li>So very obviously, for now, this isn\u2019t going to show up in the productivity or GDP statistics overall, or at least not directly, although I do think this is a non-trivial rise in productivity and lived experience in areas where Waymos are widely available for those who use it, most importantly in San Francisco.</li>\n</ol>\n</li>\n<li>Karpathy keeps saying this will all be gradual capabilities gains and gradual diffusion, with no discrete jump. He suggests you would need some kind of overhang being unlocked such as a new energy source to see a big boost.\n<ol>\n<li>I don\u2019t know how to respond to someone who thinks we\u2019re in an intelligence explosion, but refuses to include any form of such feedback into their models.</li>\n<li>That\u2019s not shade, that\u2019s me literally not knowing how to respond.</li>\n<li>It\u2019s very strange to not expect any overhangs to be unlocked. That\u2019s saying that there aren\u2019t going to be any major technological ideas that we have missed.</li>\n<li>His own example is an energy source. If all ASI did was unlock a new method of cheap, safe, clean, unlimited energy, let\u2019s say a design for fusion power plants, that were buildable in any reasonable amount of time, that alone would disrupt the GDP growth trend.</li>\n</ol>\n</li>\n</ol>\n<p>I won\u2019t go further into the same GDP growth or intelligence explosion arguments I seem to discuss in many Dwarkesh Patel podcast posts. I don\u2019t think Andrej has a defensible position here, in the sense that he is doing some combination of denying the premise of AGI/ASI, not taking into account its implications in some places while acknowledging the same dynamics in others.</p>\n<p>Most of all, this echoes the common state of the discourse on such questions, which seems to involve:</p>\n<ol>\n<li>You, the overly optimistic fool, say AGI will arrive in 2 years, or 5 years, and you say that when it happens it will be a discrete event and then everything changes.\n<ol>\n<li>There is also you, the alarmist, saying this would kill everyone, cause us to lose control or otherwise stand risk of being a bad thing.</li>\n</ol>\n</li>\n<li>I, the wise world weary realist, say AGI will only arrive in 10 years, and it will be a gradual, continuous thing with no discrete jumps, facing physical bottlenecks and slow diffusion.</li>\n<li>So therefore we won\u2019t see a substantial change to GDP growth, your life will mostly seem normal, there\u2019s no risk of extinction or loss of control, and so on, building sufficiently advanced technology of minds smarter, faster, cheaper and more competitive than ourselves along an increasing set of tasks will go great.</li>\n<li>Alternatively, I, the proper cynic, realize AI is simply a \u2018normal technology\u2019 and it\u2019s \u2018just automation of some tasks\u2019 and they will remain \u2018mere tools\u2019 and what are you getting on about, let\u2019s go build some economic models.</li>\n</ol>\n<p>I\u2019m fine with those who expect to at first encounter story #2 instead of story #1.</p>\n<p>Except it totally, absolutely does not imply #3. Yes, these factors can slow things down, and 10 years are more than 2-5 years, but 10 years is still not that much time, and a continuous transition ends up in the same place, and tacking on some years for diffusion also ends up in the same place. It buys you some time, which we might be able to use well, or we might not, but that\u2019s it.</p>\n<p>What about story #4, which to be clear is not Karpathy\u2019s or Patel\u2019s? It\u2019s possible that AI progress stalls out soon and we get a normal technology, but I find it rather unlikely and don\u2019t see why we should expect that. I think that it is quite poor form to treat this as any sort of baseline scenario.</p>\n\n\n<h4 class=\"wp-block-heading\">Evolution of Intelligence and Culture</h4>\n\n\n<ol>\n<li>Dwarkesh pivots to Nick Lane. Andrej is surprised evolution found intelligence and expects it to be a rare event among similar worlds. Dwarkesh suggests we got \u2018squirrel intelligence\u2019 right after the oxygenation of the atmosphere, which Sutton said was most of the way to human intelligence, yet human intelligence took a lot longer. They go over different animals and their intelligences. You need things worth learning but not worth hardcoding.</li>\n<li>Andrej notes LLMs don\u2019t have a culture, suggests it could be a giant scratchpad.\n<ol>\n<li>The backrooms? Also LLMs can and will have a culture because anything on the internet can become their context and training data. We already see this, with LLMs basing behaviors off observations of other prior LLMs, in ways that are often undesired.</li>\n</ol>\n</li>\n<li>Andrej mentions self-play, says that he thinks the models can\u2019t create culture because they\u2019re \u2018still kids.\u2019 Savant kids, but still kids.\n<ol>\n<li>Kids create culture all the time.</li>\n<li>No, seriously, I watch my own kids create culture.</li>\n<li>I\u2019m not saying they in particular created a great culture, but there\u2019s no question they\u2019re creating culture.</li>\n</ol>\n</li>\n</ol>\n\n\n<h4 class=\"wp-block-heading\">Why Self Driving Took So Long</h4>\n\n\n<ol>\n<li>Andrej was at Tesla leading self-driving from 2017 to 2022. Why did self-driving take a decade? Andrej says it isn\u2019t done. It\u2019s a march of nines (of reliability). Waymo isn\u2019t economical yet, Tesla\u2019s approach is more scalable, and to be truly done would mean people wouldn\u2019t need a driver\u2019s license anymore. But he agrees it is \u2018kind of real.\u2019\n<ol>\n<li>Kind of? I mean obviously self-driving can always improve, pick up more nines, get smoother, get faster, get cheaper. Waymo works great, and the economics will get there.</li>\n<li>Andrej is still backing the Tesla approach, and maybe they will make fools of us all but for now I do not see it.</li>\n</ol>\n</li>\n<li>They draw parallels to AI and from AI to previous techs. Andrej worries we may be overbuilding compute, he isn\u2019t sure, says he\u2019s bullish on the tech but a lot of what he sees on Twitter makes no sense and is about fundraising or attention.\n<ol>\n<li>I find it implausible that we are overbuilding compute, but it is possible, and indeed if it was not possible then we would be massively underbuilding.</li>\n</ol>\n</li>\n<li>\u201cI\u2019m just reacting to some of the very fast timelines that people continue to say incorrectly. I\u2019ve heard many, many times over the course of my 15 years in AI where very reputable people keep getting this wrong all the time. I want this to be properly calibrated, and some of this also has geopolitical ramifications and things like that with some of these questions. I don\u2019t want people to make mistakes in that sphere of things. I do want us to be grounded in the reality of what technology is and isn\u2019t.\u201d\n<ol>\n<li>Key quote.</li>\n<li>Andrej is not saying AGI is far in any normal person sense, or that its impact will be small, as he says he is bullish on the technology.</li>\n<li>What Andrej is doing is pushing back on the even faster timelines and bigger expectations that are often part of his world. Which is totally fair play.</li>\n<li>That has to be kept in perspective. If Andrej is right the future will blow your mind, it will go crazy.</li>\n<li>Where the confusion arises is where Andrej then tries to equate his timelines and expectations with calm and continuity, or extends those predictions forward in ways that don\u2019t make sense to me.</li>\n<li>Again, I see similar things with many others e.g. the communications of the White House\u2019s Sriram Krishnan, saying AGI is far, but if you push far means things like 10 years. Which is not that far.</li>\n<li>I think Andrej\u2019s look back has a similar issue of perspective. Very reputable people keep predicting specific AI accomplishments on timelines that don\u2019t happen, sure, that\u2019s totally a thing. But is AI underperforming the expectations of reputable optimists? I think progress in AI in general in the last 15 years, certainly since 2018 and the transformer, has been absolutely massive compared to general expectations, of course there were (and likely always will be) people saying \u2018AGI in three years\u2019 and that didn\u2019t happen.</li>\n</ol>\n</li>\n</ol>\n\n\n<h4 class=\"wp-block-heading\">Future Of Education</h4>\n\n\n<ol>\n<li>Dwarkesh asks about <a href=\"https://eurekalabs.ai/\">Eureka Labs</a>. Why not AI research? Andrej says he\u2019s not sure he could improve what the labs are doing. He\u2019s afraid of a <em>WALL-E </em>or <em>Idiocracy </em>problem where humans are disempowered and don\u2019t do things. He\u2019s trying to build Starfleet Academy.\n<ol>\n<li>I think he\u2019s right to be worried about disempowerment, but looking to education as a solution seems misplaced here? Education is great, all for it, but it seems highly unlikely it will \u2018turn losses into wins\u2019 in this sense.</li>\n<li>The good news is Andrej definitely has fully enough money so he can do whatever he wants, and it\u2019s clear this stuff is what he wants.</li>\n</ol>\n</li>\n<li>Dwarkesh Patel hasn\u2019t seen Star Trek.\n<ol>\n<li>Can we get this fixed, please?</li>\n<li>I propose a podcast which is nothing but Dwarkesh Patel watching Star Trek for the first time and reacting.</li>\n</ol>\n</li>\n<li>Andrej thinks AI will fundamentally change education, and it\u2019s still early. Right now you have an LLM, you ask it questions, that\u2019s already super valuable but it still feels like slop, he wants an actual tutor experience. He learned Korean from a tutor 1-on-1 and that was so much better than a 10-to-1 class or learning on the internet. The tutor figured out where he was as a student, asked the right questions, and no LLM currently comes close. Right now they can\u2019t.\n<ol>\n<li>Strongly agreed on all of that.</li>\n</ol>\n</li>\n<li>His first class is LLM-101-N, with Nanochat as the capstone.\n<ol>\n<li>This raises the question of whether a class is even the right form factor at all for this AI world. Maybe it is, maybe it isn\u2019t?</li>\n</ol>\n</li>\n<li>Dwarkesh points out that if you can self-probe well enough you can avoid being stuck. Andrej contrasts LLM-101-N with his CS231n at Stanford on deep learning, that LLMs really empower him and help him go faster. Right now he\u2019s hiring faculty but over time some TAs can become AIs.</li>\n<li>\u201cI often say that pre-AGI education is useful. Post-AGI education is fun. In a similar way, people go to the gym today. We don\u2019t need their physical strength to manipulate heavy objects because we have machines that do that. They still go to the gym. Why do they go to the gym? Because it\u2019s fun, it\u2019s healthy, and you look hot when you have a six-pack. It\u2019s attractive for people to do that in a very deep, psychological, evolutionary sense for humanity. Education will play out in the same way. You\u2019ll go to school like you go to the gym.\u201d</li>\n<li>\u201cIf you look at, for example, aristocrats, or you look at ancient Greece or something like that, whenever you had little pocket environments that were post-AGI in a certain sense, people have spent a lot of their time flourishing in a certain way, either physically or cognitively. I feel okay about the prospects of that. If this is false and I\u2019m wrong and we end up in a <em>WALL-E</em> or <em>Idiocracy</em> future, then I don\u2019t even care if there are Dyson spheres. This is a terrible outcome. I really do care about humanity. Everyone has to just be superhuman in a certain sense.\u201d\n<ol>\n<li>(on both quotes) So, on the one hand, yes, mostly agreed, if you predicate this on the post-AGI post-useful-human-labor world where we can\u2019t do meaningful productive work and also get to exist and go to the gym and go around doing our thing like this is all perfectly normal.</li>\n<li>On the other hand, it\u2019s weird to expect things to work out like that, although I won\u2019t reiterate why, except to say that if you accept that the humans are now learning for fun then I don\u2019t think this jives with a lot of Andrej\u2019s earlier statements and expectations.</li>\n<li>If you\u2019re superhuman in this sense, that\u2019s cool, but if you\u2019re less superhuman than the competition, then does it do much beyond being cool? What are most people going to choose to do with it? What is good in life? What is the value?</li>\n<li>This all gets into much longer debates and discussions, of course.</li>\n</ol>\n</li>\n<li>\u201cI think there will be a transitional period where we are going to be able to be in the loop and advance things if we understand a lot of stuff. In the long-term, that probably goes away.\u201d\n<ol>\n<li>Okay, sure, there will be a transition period of unknown length, but that doesn\u2019t as they say solve for the equilibrium.</li>\n<li>I don\u2019t expect that transition period to last very long, although there are various potential values for very long.</li>\n</ol>\n</li>\n<li>Dwarkesh asks about teaching. Andrej says everyone should learn physics early, since early education is about booting up a brain. He looks for first or second order terms of everything. Find the core of the thing and understand it.\n<ol>\n<li>Our educational system is not about booting up brains. If it was, it would do a lot of things very differently. Not that we should let this stop us.</li>\n</ol>\n</li>\n<li>Curse of knowledge is a big problem, if you\u2019re an expert in a field often you don\u2019t know what others don\u2019t know. Could be helpful to see other people\u2019s dumb questions that they ask an LLM?</li>\n<li>From Dwarkesh: \u201cAnother trick that just works astoundingly well. If somebody writes a paper or a blog post or an announcement, it is in 100% of cases that just the narration or the transcription of how they would explain it to you over lunch is way more, not only understandable, but actually also more accurate and scientific, in the sense that people have a bias to explain things in the most abstract, jargon-filled way possible and to clear their throat for four paragraphs before they explain the central idea. But there\u2019s something about communicating one-on-one with a person which compels you to just say the thing.\u201d\n<ol>\n<li>Love it. Hence we listen to and cover podcasts, too.</li>\n<li>I think this is because in a conversation you don\u2019t have to be defensible or get judged or be technically correct, you don\u2019t have to have structure that looks good, and you don\u2019t have to offer a full explanation.</li>\n<li>As in, you can gesture at things, say things without justifications, watch reactions, see what lands, fill in gaps when needed, and yeah, \u2018just say the thing.\u2019</li>\n<li>That\u2019s (a lot of) why it isn\u2019t the abstract, plus habit, it isn\u2019t done that way because it isn\u2019t done that way.</li>\n</ol>\n</li>\n</ol>\n\n\n<h4 class=\"wp-block-heading\">Reactions</h4>\n\n\n<p><a href=\"https://x.com/peterwildeford/status/1979954980942500299\">Peter Wildeford offers his one page summary</a>, which I endorse as a summary.</p>\n<p><a href=\"https://x.com/sriramk/status/1979350310209622049\">Sriram Krishnan highlights part of the section on education</a>, which I agree was excellent, and recommends the overall podcast highly.</p>\n<p><a href=\"https://x.com/karpathy/status/1979644538185752935\">Andrej Karpathy offered his post-podcast reactions here</a>, including a bunch of distillations, highlights and helpful links.</p>\n<p>Here\u2019s his summary on the timelines question:</p>\n<blockquote><p>Andrej Karpathy: Basically my AI timelines are about 5-10X pessimistic w.r.t. what you\u2019ll find in your neighborhood SF AI house party or on your twitter timeline, but still quite optimistic w.r.t. a rising tide of AI deniers and skeptics</p></blockquote>\n<p>Those house parties must be crazy, as must his particular slice of Twitter. He has AGI 10 years away and he\u2019s saying that\u2019s 5-10X pessimistic. Do the math.</p>\n<p>My slice currently overall has 4-10 year expectations. The AI 2027 crowd has some people modestly shorter, but even they are now out in 2029 or so I think.</p>\n<p>That\u2019s how it should work, evidence should move the numbers back and forth, and if you had a very aggressive timeline six months or a year ago recent events should slow your roll. You can say \u2018those people were getting ahead of themselves and messed up\u2019 and that\u2019s a reasonable perspective, but I don\u2019t think it was obviously a large mistake given what we knew at the time.</p>\n<blockquote><p><a href=\"https://x.com/peterwildeford/status/1979910600022012219\">Peter Wildeford</a>: I\u2019m desperate for a worldview where we agree both are true:</p>\n<p>&#8211; current AI is slop and the marketing is BS, but</p>\n<p>&#8211; staggering AI transformation (including extinction) is 5-20 years out, this may not be good by default, and thus merits major policy action now</p></blockquote>\n<p>I agree with the second point (with error bars). The first point I would rate as \u2018somewhat true.\u2019 Much of the marketing is BS and much of the output is slop, no question, but much of it is not on either front and the models are already extremely helpful to those who use them.</p>\n<blockquote><p><a href=\"https://x.com/peterwildeford/status/1980318096184209532\">Peter Wildeford</a>: If the debate truly has become</p>\n<p>&#8211; \u201cAGI is going to take all the jobs in just two years\u201d vs.</p>\n<p>&#8211; \u201cno you idiot, don\u2019t buy the hype, AI is really slop, it will take 10-20 years before AGI automates all jobs (and maybe kill us)\u201d</p>\n<p>&#8230;I feel like we have really lost the big picture here</p>\n<div>\n<figure>\n<div>\n\n\n<figure class=\"wp-block-image\"><img alt=\"\" src=\"https://substackcdn.com/image/fetch/$s_!MWnL!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F38247902-63b4-4b0e-ade3-7518196ef22d_500x567.png\" /></figure>\n\n\n<div></div>\n</div>\n</figure>\n</div>\n<p>[meme credit: Darth thromBOOzyt]</p></blockquote>\n<p>Similarly, the first position here is obviously wrong, and the second position could be right on the substance but has one hell of a Missing Mood, 10-20 years before all jobs get automated is kind of the biggest thing that happened in the history of history even if the process doesn\u2019t kill or diempower us.</p>\n<blockquote><p><a href=\"https://x.com/robertskmiles/status/1980055989828370440\">Rob Miles</a>: It\u2019s strange that the \u201canti hype\u201d position is now \u201cAGI is one decade away\u201d. That&#8230; would still be a very alarming situation to be in? It\u2019s not at all obvious that that would be enough time to prepare.</p></blockquote>\n<p><a href=\"https://x.com/johncoogan/status/1980332643809173662\">It\u2019s so crazy the amount to which vibes can supposedly shift</a> when objectively nothing has happened and even the newly expressed opinions aren\u2019t so different from what everyone was saying before, it\u2019s that now we\u2019re phrasing it as \u2018this is long timelines\u2019 as opposed to \u2018this is short timelines.\u2019</p>\n<blockquote><p>John Coogan: It\u2019s over. Andrej Karpathy popped the AI bubble. It\u2019s time to rotate out of AI stocks and focus on investing in food, water, shelter, and guns. AI is fake, the internet is overhyped, computers are pretty much useless, even the steam engine is mid. We\u2019re going back to sticks and stones.</p>\n<p>Obviously it\u2019s not actually that bad, but the general tech community is experiencing whiplash right now after the Richard Sutton and Andrej Karpathy appearances on Dwarkesh. Andrej directly called the code produced by today\u2019s frontier models \u201cslop\u201d and estimated that AGI was around 10 years away. Interestingly this lines up nicely with Sam Altman\u2019s \u201cThe Intelligence Age\u201d blog post from September 23, 2024, where he said \u201cIt is possible that we will have superintelligence in a few thousand days (!); it may take longer, but I\u2019m confident we\u2019ll get there.\u201d</p>\n<p>I read this timeline to mean a decade, which is what people always say when they\u2019re predicting big technological shifts (see space travel, quantum computing, and nuclear fusion timelines). This is still earlier than Ray Kurzweil\u2019s 2045 singularity prediction, which has always sounded on the extreme edge of sci-fi forecasting, but now looks bearish.</p></blockquote>\n<p>Yep, I read Altman as ~10 years there as well. Except that Altman was approaching that correctly as \u2018quickly, there\u2019s no time\u2019 rather than \u2018we have all the time in the world.\u2019</p>\n<blockquote><p>There\u2019s a whole chain of AGI-soon bears who feel vindicated by Andrej\u2019s comments and the general vibe shift. Yann LeCun, Tyler Cowen, and many others on the side of \u201cprogress will be incremental\u201d look great at this moment in time.</p>\n<p>This George Hotz quote from a Lex Fridman interview in June of 2023 now feels way ahead of the curve, at the time: \u201cWill GPT-12 be AGI? My answer is no, of course not. Cross-entropy loss is never going to get you there. You probably need reinforcement learning in fancy environments to get something that would be considered AGI-like.\u201d</p>\n<p>Big tech companies can\u2019t turn on a dime on the basis of the latest Dwarkesh interview though. Oracle is building something like $300 billion in infrastructure over the next five years.</p></blockquote>\n<p>It\u2019s so crazy to think a big tech company would think \u2018oops, it\u2019s over, Dwarkesh interviews said so\u2019 and regret or pull back on investment, also yeah it\u2019s weird that Amazon was up 1.6% while AWS was down.</p>\n<blockquote><p><a href=\"https://x.com/DanielleFong/status/1980360367646863782\">Danielle Fong</a>: aws down, amazon up</p>\n<p>nvda barely sweating</p>\n<p>narrative bubbles pop more easily than market bubbles</p>\n<div>\n<figure>\n<div>\n\n\n<figure class=\"wp-block-image\"><img alt=\"\" src=\"https://substackcdn.com/image/fetch/$s_!ghgN!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbf0e9fa8-4a06-4402-b74d-afa955004161_1200x797.jpeg\" /></figure>\n\n\n<div></div>\n</div>\n</figure>\n</div>\n</blockquote>\n<p>Why would you give Hotz credit for \u2018GPT-12 won\u2019t be AGI\u2019 here, when the timeline for GPT-12 (assuming GPT-11 wasn\u2019t AGI, so we\u2019re not accelerating releases yet) is something like 2039? Seems deeply silly. And yet here we are. Similarly, people supposedly \u2018look great\u2019 when others echo previous talking points? In my book, you look good based on actual outcomes versus predictions, not when others also predict, unless you are trading the market.</p>\n<p><a href=\"https://x.com/liron/status/1979918522357522592\">I definitely share the frustration Liron had here</a>:</p>\n<blockquote><p>Liron Shapira: Dwarkesh asked Karpathy about the Yudkowskian observation that exponential economic growth to date has been achieved with *constant* human-level thinking ability.</p>\n<p>Andrej acknowledged the point but said, nevertheless, he has a strong intuition that 2% GDP growth will hold steady.</p>\n<p>Roon: correction, humanity has achieved superexponential economic growth to date</p>\n<p>Liron: True.</p>\n<div>\n<figure>\n<div>\n\n\n<figure class=\"wp-block-image\"><img alt=\"\" src=\"https://substackcdn.com/image/fetch/$s_!yayA!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F51f7360d-b46f-4ae8-8852-f8da7fd532ec_1200x485.jpeg\" /></figure>\n\n\n<div></div>\n</div>\n</figure>\n</div>\n</blockquote>\n<p>In short, I don\u2019t think a reasonable extrapolation from above plus AGI is ~2%.</p>\n<p>But hey, that\u2019s the way it goes. It\u2019s been a fun one.</p>\n<p>&nbsp;</p>\n\n\n<h4 class=\"wp-block-heading\"></h4>"
            ],
            "link": "https://thezvi.wordpress.com/2025/10/21/on-dwarkesh-patels-podcast-with-andrej-karpathy/",
            "publishedAt": "2025-10-21",
            "source": "TheZvi",
            "summary": "Some podcasts are self-recommending on the \u2018yep, I\u2019m going to be breaking this one down\u2019 level. This was very clearly one of those. So here we go. As usual for podcast posts, the baseline bullet points describe key points made, &#8230; <a href=\"https://thezvi.wordpress.com/2025/10/21/on-dwarkesh-patels-podcast-with-andrej-karpathy/\">Continue reading <span class=\"meta-nav\">&#8594;</span></a>",
            "title": "On Dwarkesh Patel\u2019s Podcast With Andrej Karpathy"
        }
    ],
    "lookbackDays": 1,
    "publishDate": "2025-10-21"
}