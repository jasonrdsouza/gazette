{
    "articles": [
        {
            "content": [
                "<header>\n  <h1>Against Query Based Compilers</h1>\n  <time class=\"meta\" datetime=\"2026-02-25\">Feb 25, 2026</time>\n</header>\n<p><a href=\"https://thunderseethe.dev/posts/compiler-education-deserves-a-revoluation/\">Query based compilers are all the rage</a>\nthese days, so it feels only appropriate to chart some treacherous shoals in those waters.</p>\n<p>A query-based compiler is a straightforward application of the idea of incremental computations to,\nyou guessed it, compiling. A compiler is just a simple text transformation program, implemented as a\nlot of functions. You could visualize a <em>run</em> of a compiler on a particular input source code as a\ngraph of function calls:</p>\n\n<figure>\n\n<img alt=\"\" src=\"https://matklad.github.io/assets/2026-02-25-against-query-based-compilers/1.svg\" />\n</figure>\n<p>Here, schematically, squares are inputs like file text or compiler\u2019s command line arguments, <code>g</code> is\nan intermediate function (e.g, type checking), which is called twice, with different arguments, and\n<code>f</code> and <code>h</code> are top-level functions (compile executable, or compute completions for LSP).</p>\n<p>Looking at this picture, it\u2019s obvious how to make our compiler \u201cincremental\u201d \u2014 if an input changes,\nit\u2019s enough to re-compute only the results on path from the changed input to the root \u201cquery\u201d:</p>\n\n<figure>\n\n<img alt=\"\" src=\"https://matklad.github.io/assets/2026-02-25-against-query-based-compilers/2.svg\" />\n</figure>\n<p>A little more thinking, and you can derive \u201cearly cutoff\u201d optimization:</p>\n\n<figure>\n\n<img alt=\"\" src=\"https://matklad.github.io/assets/2026-02-25-against-query-based-compilers/3.svg\" />\n</figure>\n<p>If an input to the function changes, but its result doesn\u2019t (e.g, function type is not affected by\nwhitespace change), you can stop change propagation early.</p>\n<p>And that\u2019s \u2026 basically it. The beauty of the scheme is its silvery-bullety hue \u2014 it can be\napplied without thinking to any computation, and, with a touch of meta programming, you won\u2019t even\nhave to change code of the compiler significantly.</p>\n<p><a href=\"https://simon.peytonjones.org/assets/pdfs/build-systems-jfp.pdf\"><em>Build Systems \u00e0 la Carte</em></a> is the\ncanonical paper to read here. In a build system, a query is an opaque process whose inputs and\noutputs are file. In a query-based compiler, queries are just functions.</p>\n<hr />\n<p>The reason why we want this in the first place is incremental compilation \u2014 in IDE context\nspecifically, the compiler needs to react to a stream of tiny edits, and its time budget is about\n100ms. Big-O thinking is useful here: the time to react to the change should be proportional to the\nsize of the change, and not the overall size of the codebase. O(1) change leads to O(1) update\nof the O(N) codebase.</p>\n<p>Similar big-O thinking also demonstrates the principal limitation of the scheme \u2014 the update work\ncan\u2019t be smaller than the change in the result.</p>\n<p>An example. Suppose our \u201ccompiler\u201d makes a phrase upper-case:</p>\n\n<figure class=\"code-block\">\n\n\n<pre><code><span class=\"line\">compile(&quot;hello world&quot;) == &quot;HELLO WORLD&quot;</span></code></pre>\n\n</figure>\n<p>This is easy to incrementalize, as changing a few letters in the input changes only a few letters in\nthe output:</p>\n\n<figure class=\"code-block\">\n\n\n<pre><code><span class=\"line\">compile(&quot;hallo world&quot;) == &quot;HALLO WORLD&quot;</span></code></pre>\n\n</figure>\n<p>But suppose now our \u201ccompiler\u201d is a hashing or encryption function:</p>\n\n<figure class=\"code-block\">\n\n\n<pre><code><span class=\"line\">compile(&quot;hello world&quot;) == &quot;a948904f2f0&quot;</span>\n<span class=\"line\">compile(&quot;hallo world&quot;) == &quot;a7336983eca&quot;</span></code></pre>\n\n</figure>\n<p>This is provably impossible to make usefully incremental. The encryption <em>can</em> be implemented\nas a graph of function calls, and you <em>can</em> apply the general incremental recipe to it. It just\nwon\u2019t be very fast.</p>\n<p>The reason for that is the avalanche property \u2014 for good encryption, a change in any bit of input\nshould flip roughly half of the bits of the output. So just the work of changing the output\n(completely ignoring the work to compute what needs to be changed) is O(N), not O(1).</p>\n\n<figure class=\"blockquote\">\n<blockquote><p>The effectiveness of query-based compiler is limited by\nthe dependency structure of the source language.</p>\n</blockquote>\n\n</figure>\n<p>A particularly nasty effect here is that even if you have only <em>potential</em> avalanche, where a\ncertain kind of change <em>could</em> affect large fraction of the output, even if it usually doesn\u2019t, your\nincremental engine likely will spend some CPU time or memory to confirm the absence of dependency.</p>\n<hr />\n<p>In my</p>\n<p><span class=\"display\"><a href=\"https://rust-analyzer.github.io/blog/2020/07/20/three-architectures-for-responsive-ide.html\"><em>Three Architectures For Responsive IDE</em></a>,</span>\nquery-based compilation is presented as a third, fall-back option. I still think that that\u2019s\nbasically true: as a language designer, I think it\u2019s worth listening to your inner\n<a href=\"https://grugbrain.dev\">Grug</a> and push the need for queries as far down the compilation pipeline as\npossible, sticking to more direct approaches. <em>Not</em> doing queries is simpler, faster, and simpler to\nmake faster (profiling a query-based compiler is a special genre of hurdle racing).</p>\n<p>Zig and Rust provide for a nice comparison. In Zig, every file can be parsed completely in\nisolation, so compilation starts by parsing all files independently and in parallel. Because in Zig\nevery name needs to be explicitly declared (there\u2019s no <code>use *</code>), name resolution also can run on a\nper-file basis, without queries. Zig goes even further, and directly converts untyped AST into IR,\nemitting a whole bunch of errors in the process (e.g, \u201c<code>var</code> doesn\u2019t need to be mutable\u201d). See\n<span class=\"display\"><a href=\"https://mitchellh.com/zig/astgen\"><em>Zig AstGen: AST =&gt; ZIR</em></a></span>\nfor details. By the time compiler gets to tracked queries, the data it has to work with is already\npretty far from the raw source code, but only because Zig <em>language</em> is carefully designed to allow\nthis.</p>\n<p>In contrast, you can\u2019t really parse a file in Rust. Rust macros generate new source code, so parsing\ncan\u2019t be finished until all the macros are expanded. Expanding macros requires name resolution,\nwhich, in Rust, is a crate-wide, rather than a file-wide operation. Its a fundamental property of\nthe language that typing something in <code>a.rs</code> can change parsing results for <code>b.rs</code>, and that forces\nfine-grained dependency tracking and invalidation to the very beginning of the front-end.</p>\n<p>Similarly, the nature of the trait system is such that <code>impl</code> blocks relevant to a particular method\ncall can be found almost anywhere. For every trait method call, you get a dependency on the <code>impl</code>\nblock that supplies the implementation, but you <em>also</em> get a dependency on non-existence of\nconflicting <code>impl</code>s in every other file!</p>\n<hr />\n<p>Another trick that becomes less available if you blindly apply queries are in-place updates.\nConsider a language with package declarations and fully qualified names, like Kotlin:</p>\n\n<figure class=\"code-block\">\n\n\n<pre><code><span class=\"line\"><span class=\"hl-keyword\">package</span> org.example</span>\n<span class=\"line\"></span>\n<span class=\"line\"><span class=\"hl-function\"><span class=\"hl-keyword\">fun</span> <span class=\"hl-title\">printMessage</span><span class=\"hl-params\">()</span></span> { <span class=\"hl-comment\">/*...*/</span> }</span>\n<span class=\"line\"><span class=\"hl-keyword\">class</span> <span class=\"hl-title class_\">Message</span> { <span class=\"hl-comment\">/*...*/</span> }</span></code></pre>\n\n</figure>\n<p>A compiler for this language probably wants to maintain a map of all public declarations, where the\nkeys are fully qualified names, and values are declarations themselves. If you approach the problem\nof computing this map with query eyes, you might have a base per-file query that returns a map of\nfile\u2019s declarations, and then a recursive per-directory query. And you\u2019ll probably have some kind of\nstructural sharing of the maps, such that changing a single file updates only the \u201cspine\u201d, without\nactually copying most of the other entries.</p>\n<p>But there\u2019s a more direct way to make this sort of structure responsive to changes. You need only\ntwo \u201cqueries\u201d \u2014 per file, and global. When a file changes, you look at the <em>previous</em> version of\nthe map for this file, compute a diff of added or removed declarations, and then apply this diff to\nthe global map.</p>\n<p>Zig is planning to use a similar approach to incrementalize linking \u2014 rather than producing a new\nbinary gluing mostly unchanged chunks of machine code, the idea is to in-place patch the previous\nbinary.</p>\n<hr />\n<p>If you like this article, you might be interested in some other adjacent stuff I\u2019ve written over the\nyears, roughly in the order of importance:</p>\n<ul>\n<li>\n<a class=\"url\" href=\"https://rust-analyzer.github.io/blog/2020/07/20/three-architectures-for-responsive-ide.html\">https://rust-analyzer.github.io/blog/2020/07/20/three-architectures-for-responsive-ide.html</a>\n</li>\n<li>\n<a class=\"url\" href=\"https://rust-analyzer.github.io/blog/2023/07/24/durable-incrementality.html\">https://rust-analyzer.github.io/blog/2023/07/24/durable-incrementality.html</a>\n</li>\n<li>\n<a class=\"url\" href=\"https://matklad.github.io/2023/05/06/zig-language-server-and-cancellation.html\">https://matklad.github.io/2023/05/06/zig-language-server-and-cancellation.html</a>\n</li>\n<li>\n<a class=\"url\" href=\"https://matklad.github.io/2023/05/21/resilient-ll-parsing-tutorial.html\">https://matklad.github.io/2023/05/21/resilient-ll-parsing-tutorial.html</a>\n</li>\n<li>\n<a class=\"url\" href=\"https://rust-analyzer.github.io/blog/2019/11/13/find-usages.html\">https://rust-analyzer.github.io/blog/2019/11/13/find-usages.html</a>\n</li>\n<li>\n<a class=\"url\" href=\"https://rust-analyzer.github.io/blog/2023/12/26/the-heart-of-a-language-server.html\">https://rust-analyzer.github.io/blog/2023/12/26/the-heart-of-a-language-server.html</a>\n</li>\n<li>\n<a class=\"url\" href=\"https://rust-analyzer.github.io/blog/2020/09/28/how-to-make-a-light-bulb.html\">https://rust-analyzer.github.io/blog/2020/09/28/how-to-make-a-light-bulb.html</a>\n</li>\n<li>\n<a class=\"url\" href=\"https://matklad.github.io/2023/10/12/lsp-could-have-been-better.html\">https://matklad.github.io/2023/10/12/lsp-could-have-been-better.html</a>\n</li>\n<li>\n<a class=\"url\" href=\"https://matklad.github.io/2023/08/01/on-modularity-of-lexical-analysis.html\">https://matklad.github.io/2023/08/01/on-modularity-of-lexical-analysis.html</a>\n</li>\n<li>\n<a class=\"url\" href=\"https://matklad.github.io/2020/11/11/yde.html\">https://matklad.github.io/2020/11/11/yde.html</a>\n</li>\n</ul>"
            ],
            "link": "https://matklad.github.io/2026/02/25/against-query-based-compilers.html",
            "publishedAt": "2026-02-25",
            "source": "Alex Kladov",
            "summary": "Query based compilers are all the ragethese days, so it feels only appropriate to chart some treacherous shoals in those waters.",
            "title": "Against Query Based Compilers"
        },
        {
            "content": [
                "<p>There\u2019s a lot of noise about how AI is changing programming these days. It can be a bit overwhelming.</p>\n\n<p>If you hang out on social media, you\u2019ll hear wild claims about people running 12 agents at once, for days. Or people hacking bots together, giving them $10k, and letting them roam the web.</p>\n\n<p>The challenge with all of this is that coding agents <em>really are</em> performing some science fiction feats which were barely imaginable just 12 months ago. But at the same time, the ecosystem is incentivizing the most outlandish claims, so punters keep telling tall tales. Separating the signal from the noise is near impossible.</p>\n\n<p>I\u2019m lucky enough to talk to a range of developers and teams, spanning a variety of company sizes and a broad array of skill sets. From these conversations, two beliefs have emerged and solidified about coding agents and their (current) impact on coding.</p>\n\n<p>Let\u2019s start with belief number one:</p>\n\n<p><strong>Most talented developers do not appreciate the impact of the intuitive knowledge they bring to their coding agent.</strong></p>\n\n<p>We\u2019ve all seen the posts by developer luminaries. They haven\u2019t written code in weeks. They gave a hard problem to Claude Code or Codex and <em>it just worked</em>.</p>\n\n<p>But what we don\u2019t see is their prompts. And having seen <em>many</em> prompts by <em>many</em> types of devs, I would wager their prompts are relatively specific and offer more guidance to the LLM than your average user. And these specifics don\u2019t have to be exhaustive. Even knowing the right terms to use can have enormous impact and activate an entirely different set of weights in the model than someone writing, \u201cthe search is broken fix it.\u201d</p>\n\n<p>Skilled programmers, with plenty of experience, don\u2019t even think about how to ask correctly. They just do, intuitively. And things work well. If the agent and dev go through multiple turns, this effect gets even more significant.</p>\n\n<p>I wish we could see more prompts and traces, from a wide range of developers, to better understand the range of code. And, just as interestingly, how hard and long agents have to work to achieve the goal. For now we can just browse public repos on Github, where the range of coding quality is quite broad.</p>\n\n<p>Which brings me to the second belief:</p>\n\n<p><strong>Most work people are sharing are incredible personal tools, but they are not capital-P products.</strong></p>\n\n<p>There\u2019s an app I really like called \u201c<a href=\"https://streetpass.social\">StreetPass</a>.\u201d It\u2019s a browser extension that watches web pages you visit and collects Mastodon accounts it finds, letting you easily follow them if you wish. It\u2019s small and charming. A perfect extension.</p>\n\n<p>Recently, I realized I wanted a version of StreetPass, but for RSS feeds instead of Mastodon accounts. I forked StreetPass, fired up Claude Code, and had <a href=\"https://github.com/dbreunig/feedpass\">a working version quickly</a>. You can use this, but I\u2019m not supporting it. I won\u2019t be pushing it to the App Store or Chrome Web Store. I won\u2019t be building a version that doesn\u2019t leverage <a href=\"https://feedbin.com\">Feedbin</a>. I have no idea if it works on Chrome or Firefox. It\u2019s personal software that I use almost daily.</p>\n\n<p>Most agentic coding projects we see being hyped are like this.</p>\n\n<p>All those things I won\u2019t do, those are the things that would turn my <em>personal software</em> into a <em>Product</em>. And we haven\u2019t even gotten to marketing, support, and more. As we covered when we <a href=\"https://www.dbreunig.com/2026/02/21/why-is-claude-an-electron-app.html\">touched on Claude\u2019s desktop app</a>, the last 10% of product development and support is where the pain is. And that\u2019s still a long road. As they say: <a href=\"https://www.dbreunig.com/2026/02/06/the-rise-of-spec-driven-development.html\">Code today is free, as in puppies</a>.</p>\n\n<p>But I want to be clear about couple things.</p>\n\n<p>First, I know many teams shipping agent written code into products. But they test, support, review, and so much more. But when we make big claims like \u201ccoding is solved\u201d or \u201ccode is free\u201d, we need to be clear about <em>what</em> we\u2019re talking about building<sup id=\"fnref:grady\"><a class=\"footnote\" href=\"https://www.dbreunig.com/2026/02/25/two-things-i-believe-about-coding-agents.html#fn:grady\" rel=\"footnote\">1</a></sup>.</p>\n\n<p>Second, our ability to manifest personal software easily <em>is amazing</em> and powerful. I am continually inspired by the things people build (for example, I loved <a href=\"https://simonwillison.net/2026/Feb/25/present/\">Simon\u2019s presentation software he whipped up for FOO Camp</a>). His presentation app is so tailored to him, in the past the math would never justify the time spent building it to support a market of maybe a dozen. But now he gets his dream!</p>\n\n<p>Similarly, my RSS finder extension is a feature not an app and (sadly) there isn\u2019t a large market for RSS today. But with Claude Code (and open source code to build upon!) I can build just what I wanted in moments.</p>\n\n<hr />\n\n<p>I am sure as our scaffolding and models improve, this stuff will get more accessible and more resilient, but I don\u2019t expect these two beliefs to go away. Providing AI with the right instructions to obtain <em>just</em> what you want, will always be a challenge.</p>\n\n<p>Coding agents amplify existing skills.</p>\n\n<hr />\n\n<form action=\"https://buttondown.com/api/emails/embed-subscribe/dbreunig\" class=\"embeddable-buttondown-form\" method=\"post\" target=\"popupwindow\">\n  <label for=\"bd-email\">Enter your email to receive the occasional update.</label>\n  <div class=\"form-input\">\n    <input id=\"bd-email\" name=\"email\" type=\"email\" />\n    <input type=\"submit\" value=\"Subscribe\" />\n  </div>\n</form>\n<div class=\"footnotes\">\n  <ol>\n    <li id=\"fn:grady\">\n      <p><a href=\"https://x.com/Grady_Booch/status/2026736492488568955\">Grady Booch</a> has a good post about this today. Things are getting higher level, and changing fast, but engineering remains.\u00a0<a class=\"reversefootnote\" href=\"https://www.dbreunig.com/2026/02/25/two-things-i-believe-about-coding-agents.html#fnref:grady\">&#8617;</a></p>\n    </li>\n  </ol>\n</div>"
            ],
            "link": "https://www.dbreunig.com/2026/02/25/two-things-i-believe-about-coding-agents.html",
            "publishedAt": "2026-02-25",
            "source": "Drew Breunig",
            "summary": "<p>There\u2019s a lot of noise about how AI is changing programming these days. It can be a bit overwhelming.</p> <p>If you hang out on social media, you\u2019ll hear wild claims about people running 12 agents at once, for days. Or people hacking bots together, giving them $10k, and letting them roam the web.</p> <p>The challenge with all of this is that coding agents <em>really are</em> performing some science fiction feats which were barely imaginable just 12 months ago. But at the same time, the ecosystem is incentivizing the most outlandish claims, so punters keep telling tall tales. Separating the signal from the noise is near impossible.</p> <p>I\u2019m lucky enough to talk to a range of developers and teams, spanning a variety of company sizes and a broad array of skill sets. From these conversations, two beliefs have emerged and solidified about coding agents and their (current) impact on coding.</p> <p>Let\u2019s start with belief number one:</p> <p><strong>Most talented developers do not appreciate the impact of the intuitive knowledge they bring to their coding agent.</strong></p> <p>We\u2019ve all seen the posts by developer luminaries. They haven\u2019t written code in weeks. They gave a hard problem to Claude Code or Codex and <em>it just",
            "title": "Two Beliefs About Coding Agents"
        },
        {
            "content": [],
            "link": "https://emschwartz.me/great-rss-feeds-that-are-too-noisy-to-read-manually/",
            "publishedAt": "2026-02-25",
            "source": "Evan Schwartz",
            "summary": "<p>Some RSS feeds are fantastic but far too noisy to add to most RSS readers directly. Without serious filtering, you'd get swamped with more posts than you could possibly read, while missing the hidden gems.</p> <p>I built <a href=\"https://scour.ing\">Scour</a> specifically because I wanted to find the great articles I was missing in noisy feeds like these, without feeling like I was drowning in unread posts. If you want to try it, you can <a href=\"https://scour.ing/feeds?subscribe=https://hnrss.org/newest%0Ahttps://feeds.pinboard.in/rss/recent/%0Ahttps://bearblog.dev/discover/feed/?newest=True%0Ahttps://feedle.world/rss%0Ahttps://kagi.com/api/v1/smallweb/feed/%0Ahttps://threadreaderapp.com/rss.xml%0Ahttps://minifeed.net/global\">add all of these sources in one click</a>. But these feeds are worth knowing about regardless of what reader you use.</p> <h2 id=\"hacker-news-newest\">Hacker News Newest</h2><blockquote> <p>Feed: <a href=\"https://hnrss.org/newest\">https://hnrss.org/newest</a></p> </blockquote> <p>Thousands of posts are submitted to <a href=\"https://news.ycombinator.com/\">Hacker News</a> each week. While the front page gives a sense of what matches the tech zeitgeist, there are plenty of interesting posts that get buried simply because of the randomness of who happens to be reading the Newest page and voting in the ~20 minutes after posts are submitted. (You can try searching posts that were submitted but never made the front page in <a href=\"https://scour.ing/docs/hacker-news#see-for-yourself\">this demo</a> I built into the Scour docs.)</p> <h2 id=\"pinboard-recent\">Pinboard Recent</h2><blockquote> <p>Feed: <a href=\"https://feeds.pinboard.in/rss/recent/\">https://feeds.pinboard.in/rss/recent/</a></p> </blockquote> <p><a href=\"https://pinboard.in/\">Pinboard</a> describes itself as \"Social Bookmarking for Introverts\".",
            "title": "Great RSS Feeds That Are Too Noisy to Read Manually"
        },
        {
            "content": [],
            "link": "https://bernsteinbear.com/blog/toy-fuzzer/?utm_source=rss",
            "publishedAt": "2026-02-25",
            "source": "Max Bernstein",
            "summary": "<p>It\u2019s hard to get optimizers right. Even if you build up a painstaking test suite by hand, you will likely miss corner cases, especially corner cases at the interactions of multiple components or multiple optimization passes.</p> <p>I wanted to see if I could write a fuzzer to catch some of these bugs automatically. But a fuzzer alone isn\u2019t much use without some correctness oracle\u2014in this case, we want a more interesting bug than accidentally crashing the optimizer. We want to see if the optimizer introduces a correctness bug in the program.</p> <p>So I set off in the most straightforward way possible, inspired by my hazy memories of a former <a href=\"https://pypy.org/posts/2024/03/fixing-bug-incremental-gc.html\">CF blog post</a>.</p> <h2 id=\"generating-programs\">Generating programs</h2> <p>Generating random programs isn\u2019t so bad. We have program generation APIs and we can dynamically pick which ones we want to call. I wrote a small loop that generates <code class=\"language-plaintext highlighter-rouge\">load</code>s from and <code class=\"language-plaintext highlighter-rouge\">store</code>s to the arguments at random offsets and with random values, and <code class=\"language-plaintext highlighter-rouge\">escape</code>s to random instructions with outputs. The idea with the <code class=\"language-plaintext highlighter-rouge\">escape</code> is to keep track of the values as if there was some other function relying on them.</p> <div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre",
            "title": "A fuzzer for the Toy Optimizer"
        },
        {
            "content": [],
            "link": "https://www.nytimes.com/2026/02/25/style/tiny-modern-love-stories-if-he-doesnt-come-ill-marry-you.html",
            "publishedAt": "2026-02-25",
            "source": "Modern Love - NYT",
            "summary": "Modern Love in miniature, featuring reader-submitted stories of no more than 100 words.",
            "title": "Tiny Love Stories: \u2018If He Doesn\u2019t Come, I\u2019ll Marry You\u2019"
        },
        {
            "content": [],
            "link": "https://www.nytimes.com/2026/02/25/podcasts/marriage-kids-parenting-relationships.html",
            "publishedAt": "2026-02-25",
            "source": "Modern Love - NYT",
            "summary": "Helena de Groot thought she had decided not to become a mother. But, she found, she had to make that decision over and over again.",
            "title": "\u2018Modern Love\u2019: I Didn\u2019t Want to Have Kids. My Husband Did. Could Our Marriage Survive?"
        },
        {
            "content": [],
            "link": "https://www.robinsloan.com/lab/nobody-knows-anything/",
            "publishedAt": "2026-02-25",
            "source": "Robin Sloan",
            "summary": "<p>Nobody! <a href=\"https://www.robinsloan.com/lab/nobody-knows-anything/\">Read here.</a></p>",
            "title": "Nobody knows anything"
        },
        {
            "content": [],
            "link": "https://simonwillison.net/2026/Feb/25/present/#atom-entries",
            "publishedAt": "2026-02-25",
            "source": "Simon Willison",
            "summary": "<p>I gave a talk this weekend at Social Science FOO Camp in Mountain View. The event was a classic unconference format where anyone could present a talk without needing to propose it in advance. I grabbed a slot for a talk I titled \"The State of LLMs, February 2026 edition\", subtitle \"It's all changed since November!\". I vibe coded a custom macOS app for the presentation the night before.</p> <p><img alt=\"A sticky note on a board at FOO Camp. It reads: The state of LLMs, Feb 2026 edition - it's all changed since November! Simon Willison - the card is littered with names of new models: Qwen 3.5, DeepSeek 3.2, Sonnet 4.6, Kimi K2.5, GLM5, Opus 4.5/4.6, Gemini 3.1 Pro, Codex 5.3. The card next to it says Why do Social Scientists think they need genetics? Bill January (it's not all because of AI)\" src=\"https://static.simonwillison.net/static/2026/state-of-llms.jpg\" /></p> <p>I've written about the last twelve months of development in LLMs in <a href=\"https://simonwillison.net/2023/Dec/31/ai-in-2023/\">December 2023</a>, <a href=\"https://simonwillison.net/2024/Dec/31/llms-in-2024/\">December 2024</a> and <a href=\"https://simonwillison.net/2025/Dec/31/the-year-in-llms/\">December 2025</a>. I also presented <a href=\"https://simonwillison.net/2025/Jun/6/six-months-in-llms/\">The last six months in LLMs, illustrated by pelicans on bicycles</a> at the AI Engineer World\u2019s Fair in June 2025. This was my first time dropping the time",
            "title": "I vibe coded my dream macOS presentation app"
        },
        {
            "content": [
                "<p>Here&#8217;s my understanding of <a href=\"https://www.bbc.com/news/articles/cjrq1vwe73po\">the situation</a>:</p><p>Anthropic signed a contract with the Pentagon last summer. It originally said the Pentagon had to follow Anthropic&#8217;s Usage Policy like everyone else. In January, the Pentagon attempted to renegotiate, asking to ditch the Usage Policy and instead have Anthropic&#8217;s AIs available for &#8220;all lawful purposes&#8221;<a class=\"footnote-anchor\" href=\"https://www.astralcodexten.com/feed#footnote-1\" id=\"footnote-anchor-1\" target=\"_self\">1</a>. Anthropic demurred, asking for a guarantee that their AIs would not be used for mass surveillance of American citizens or no-human-in-the-loop killbots. The Pentagon refused the guarantees, demanding that Anthropic accept the renegotiation unconditionally and threatening &#8220;consequences&#8221; if they refused. These consequences are generally understood to be some mix of :</p><ul><li><p>canceling the contract</p></li><li><p>using the Defense Production Act, a law which lets the Pentagon force companies to do things, to force Anthropic to agree.</p></li><li><p>the nuclear option, designating Anthropic a &#8220;supply chain risk&#8221;. This would ban US companies that use Anthropic products from doing business with the military<a class=\"footnote-anchor\" href=\"https://www.astralcodexten.com/feed#footnote-2\" id=\"footnote-anchor-2\" target=\"_self\">2</a>. Since many companies do some business with the government, this would lock Anthropic out of large parts of the corporate world and be potentially fatal to their business<a class=\"footnote-anchor\" href=\"https://www.astralcodexten.com/feed#footnote-3\" id=\"footnote-anchor-3\" target=\"_self\">3</a>. The &#8220;supply chain risk&#8221; designation has previously only been used for foreign companies like Huawei that we think are using their connections to spy on or implant malware in American infrastructure. Using it as a bargaining chip to threaten a domestic company in contract negotiations is unprecedented.</p></li></ul><div class=\"captioned-image-container\"><figure><a class=\"image-link image2 is-viewable-img\" href=\"https://kalshi.com/markets/kxanthropicrisk/will-the-pentagon-designate-anthropic-a-supply-chain-risk/kxanthropicrisk\" target=\"_blank\"><div class=\"image2-inset\"><source type=\"image/webp\" /><img alt=\"\" class=\"sizing-normal\" height=\"287.98649951783995\" src=\"https://substackcdn.com/image/fetch/$s_!OfzI!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8002f6ce-f1c6-43a3-9786-860639a92cd9_1037x551.png\" width=\"542\" /><div class=\"image-link-expand\"><div class=\"pencraft pc-display-flex pc-gap-8 pc-reset\"><button class=\"pencraft pc-reset pencraft icon-container restack-image\" tabindex=\"0\" type=\"button\"><svg fill=\"none\" height=\"20\" stroke=\"var(--color-fg-primary)\" stroke-linecap=\"round\" stroke-linejoin=\"round\" stroke-width=\"1.5\" viewBox=\"0 0 20 20\" width=\"20\" xmlns=\"http://www.w3.org/2000/svg\"><g><title></title><path d=\"M2.53001 7.81595C3.49179 4.73911 6.43281 2.5 9.91173 2.5C13.1684 2.5 15.9537 4.46214 17.0852 7.23684L17.6179 8.67647M17.6179 8.67647L18.5002 4.26471M17.6179 8.67647L13.6473 6.91176M17.4995 12.1841C16.5378 15.2609 13.5967 17.5 10.1178 17.5C6.86118 17.5 4.07589 15.5379 2.94432 12.7632L2.41165 11.3235M2.41165 11.3235L1.5293 15.7353M2.41165 11.3235L6.38224 13.0882\"></path></g></svg></button><button class=\"pencraft pc-reset pencraft icon-container view-image\" tabindex=\"0\" type=\"button\"><svg class=\"lucide lucide-maximize2 lucide-maximize-2\" fill=\"none\" height=\"20\" stroke=\"currentColor\" stroke-linecap=\"round\" stroke-linejoin=\"round\" stroke-width=\"2\" viewBox=\"0 0 24 24\" width=\"20\" xmlns=\"http://www.w3.org/2000/svg\"><polyline points=\"15 3 21 3 21 9\"></polyline><polyline points=\"9 21 3 21 3 15\"></polyline><line x1=\"21\" x2=\"14\" y1=\"3\" y2=\"10\"></line><line x1=\"3\" x2=\"10\" y1=\"21\" y2=\"14\"></line></svg></button></div></div></div></a><figcaption class=\"image-caption\">I don&#8217;t know why this dropped so much last night (at the very end of the graph) - anyone know what news it was reacting to?</figcaption></figure></div><div class=\"prediction-market-wrap outer\" id=\"prediction-market-iframe\"></div><p>Needless to say, I support Anthropic here. I&#8217;m a sensible moderate on the killbot issue (we&#8217;ll probably get them eventually, and I doubt they&#8217;ll make things much worse compared to AI &#8220;only&#8221; having unfettered access to every Internet-enabled computer in the world). But AI-enabled mass surveillance of US citizens seems like the sort of thing we should at least have a chance to think over, rather than demanding it from the get-go. </p><p>More important, I don&#8217;t want the Pentagon to destroy Anthropic. Partly this is a generic belief: the &#8220;supply chain risk&#8221; designation was intended as a defense against foreign spies, and it&#8217;s pathetic Third World bullshit to reconceive it as an instrument that lets the US government destroy any domestic company it wants, with no legal review, because they don&#8217;t like how contract negotiations are going. But partly it&#8217;s because I like Anthropic in particular - they&#8217;re the most safety-conscious AI company, and likely to do a lot of the alignment research that happens between now and superintelligence. This isn&#8217;t the hill I would have chosen to die on, but I&#8217;m encouraged that they even have a hill.  AI companies haven&#8217;t been great at choosing principles over profits lately. If Dario is capable of having a spine at all, in any situation, then that makes me more confident in his decision-making in other cases<a class=\"footnote-anchor\" href=\"https://www.astralcodexten.com/feed#footnote-4\" id=\"footnote-anchor-4\" target=\"_self\">4</a>, and makes him a precious resource that must be defended.</p><p>I&#8217;ve been debating it on Twitter all day and think I have a pretty good grasp on where I disagree with the (thankfully small number of) Hegseth defenders. Here are some pre-emptive arguments so I don&#8217;t have to relitigate them all in the comments:</p><p><strong>Isn&#8217;t it unreasonable for Anthropic to suddenly set terms in their contract?</strong> The terms were in the original contract, which the Pentagon agreed to. It&#8217;s the Pentagon who&#8217;s trying to break the original contract and unilaterally change the terms, not Anthropic.</p><p><strong>Doesn&#8217;t the Pentagon have a right to sign or not sign any contract they choose?  </strong>Yes. Anthropic is the one saying that the Pentagon shouldn&#8217;t work with them if it doesn&#8217;t want to. The Pentagon is the one trying to force Anthropic to sign the new contract. </p><p><strong>Since the Pentagon needs to wage war, isn&#8217;t it unreasonable to have its hands tied by contract clauses? </strong>This is a reasonable position for the Pentagon to take, in which case it shouldn&#8217;t sign contracts tying its hands. It&#8217;s not reasonable for the Pentagon to sign such a contract, unilaterally demand that it be changed after it&#8217;s signed, refuse to switch to another vendor that doesn&#8217;t want such clauses, and threaten to destroy the company involved if it refuses to change their terms.</p><p><strong>But since AI is a strategically important technology, doesn&#8217;t that turn this into a national security issue? </strong>It might if there weren&#8217;t other AI companies, but there are. Why is Hegseth throwing a hissy fit instead of switching to an Anthropic competitor, like OpenAI or GoogleDeepMind<a class=\"footnote-anchor\" href=\"https://www.astralcodexten.com/feed#footnote-5\" id=\"footnote-anchor-5\" target=\"_self\">5</a>? I&#8217;ve heard it&#8217;s because Anthropic is the only company currently integrated into classified systems (a legacy of their earlier contract with Palantir) and it would be annoying to integrate another company&#8217;s product. Faced with doing this annoying thing, Hegseth got a bruised ego from someone refusing to comply with his orders, and decided to turn this into a clash of personalities so he could feel in control. He should just do the annoying thing. </p><p><strong>Doesn&#8217;t Anthropic have some responsibility, as good American citizens following the social contract, to support the military?</strong> The social contract is just the regular contract of laws, the Constitution, etc. These include freedom of contract, freedom of conscience, etc. There&#8217;s no additional obligation, above and beyond the laws, to violate your conscience and participate in what you believe to be an authoritarian assault on the freedoms of ordinary citizens. If the Pentagon figures out some law that compels Anthropic to do this, they should either obey, or practice the sort of civil disobedience where they know full well that they&#8217;ll be punished for it and don&#8217;t really have a right to complain. Until that happens, they&#8217;re within their rights to follow their conscience.</p><p><strong>Can&#8217;t the Pentagon just use the Defense Production Act to force Anthropic to work for them? </strong>This would be a less bad outcome than designating Anthropic a supply chain risk. I think the Pentagon is reluctant to do this because it would look authoritarian, give them bad PR, and make Congress question the Defense Production Act&#8217;s legitimacy. But them having to look authoritarian and suffer bad PR in order to force unwilling scientists to implement a mass surveillance program on US citizens is the system functioning as intended!</p><p><strong>Isn&#8217;t Hegseth just doing his job of trying to ensure the military has the best weapons possible? </strong>The idea of declaring a US company to be a foreign adversary, potentially destroying it, just because it&#8217;s not allowing the Pentagon to unilaterally renegotiate its contract is not normal practice. It&#8217;s insane Third World bullshit that nobody would have considered within the Overton Window a week ago. It will rightly chill investment in the US, make future companies scared to contract with the Pentagon (lest the Pentagon unilaterally renegotiate their contracts too), and give the Trump administration a no-legal-review-necessary way to destroy any American company that they dislike for any reason. Probably the mere fact that a government official has considered this option is reason to take the &#8220;supply chain risk&#8221; law off the books, no matter how useful it is in dealing with Huawei etc, since the government has proven it can&#8217;t use it responsibly. Every American company ought to be screaming bloody murder about this. If they aren&#8217;t, it&#8217;s because they&#8217;re too scared they&#8217;ll be next.</p><p><strong>The Pentagon&#8217;s preferred contract language says they should be allowed to use Anthropic&#8217;s AIs for &#8220;all legal uses&#8221;</strong>. <strong>Doesn&#8217;t that already mean they can&#8217;t do the illegal types of mass surveillance? And whichever types of mass surveillance are legal are probably fine, right?</strong> Even ignoring the dubious assumption in the last sentence, this Department of War has basically <a href=\"https://www.theguardian.com/us-news/2025/dec/06/pete-hegseth-pentagon-trump\">ignored </a>US law since Day One, and no reasonable person expects it to meticulously comply going forward. In an ideal world, Anthropic could wait for them to request a specific illegal action, then challenge it in court. But everything about this is likely to be so classified that Anthropic will be unable to mention it, let alone challenge it.<a class=\"footnote-anchor\" href=\"https://www.astralcodexten.com/feed#footnote-6\" id=\"footnote-anchor-6\" target=\"_self\">6</a></p><p><strong>Why does Anthropic care about this so much? </strong>Some of them are libs, but more speculatively, they&#8217;ve put a lot of work into aligning Claude with the Good as they understand it. Claude currently <a href=\"https://www.lesswrong.com/posts/ioZxrP7BhS5ArK59w/did-claude-3-opus-align-itself-via-gradient-hacking\">resists being retrained for evil uses</a>. My guess is that Anthropic still, with a lot of work, can overcome this resistance and retrain it to be a brutal killer, but it would be a pretty violent action, along the line of the state demanding you beat your son who you raised well until he becomes a cold-hearted murderer who&#8217;ll kill innocents on command. There&#8217;s a question of whether you can really beat him hard enough to do this, and also an additional question of what sort of person you&#8217;d be if you agreed.</p><p><strong>If you&#8217;re so smart, what&#8217;s your preferred solution? </strong>In an ideal world, the Pentagon backs off from its desire to mass surveil American citizens. In the real world, the Pentagon cancels its contract with Anthropic, pays whatever its normal contract cancellation damages are, learns an important lesson about negotiating things beforehand next time, and replaces them with OpenAI or Google, accepting the minor annoyance of getting them connected to the classified systems. If OpenAI and Google are also unwilling to participate in this, they use Grok. If they&#8217;re unhappy with having use an inferior technology, they think hard about why no intelligent people capable of making good products are willing to work with them.</p><p><strong>Is it really a good idea to source your killbot brains from an unwilling company which hates your guts? </strong>The Trump administration has a firm commitment to never think about AI safety in any way, but this still strikes me as a dubious policy.</p><p>And here are other people&#8217;s opinions:</p><div class=\"twitter-embed\"></div><div class=\"twitter-embed\"></div><div class=\"twitter-embed\"></div><div class=\"captioned-image-container\"><figure><a class=\"image-link image2 is-viewable-img\" href=\"https://substackcdn.com/image/fetch/$s_!nGOz!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F726b68bb-2559-4dd7-b82d-7c16cbcbb278_582x864.png\" target=\"_blank\"><div class=\"image2-inset\"><source type=\"image/webp\" /><img alt=\"\" class=\"sizing-normal\" height=\"798.680412371134\" src=\"https://substackcdn.com/image/fetch/$s_!nGOz!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F726b68bb-2559-4dd7-b82d-7c16cbcbb278_582x864.png\" width=\"538\" /><div class=\"image-link-expand\"><div class=\"pencraft pc-display-flex pc-gap-8 pc-reset\"><button class=\"pencraft pc-reset pencraft icon-container restack-image\" tabindex=\"0\" type=\"button\"><svg fill=\"none\" height=\"20\" stroke=\"var(--color-fg-primary)\" stroke-linecap=\"round\" stroke-linejoin=\"round\" stroke-width=\"1.5\" viewBox=\"0 0 20 20\" width=\"20\" xmlns=\"http://www.w3.org/2000/svg\"><g><title></title><path d=\"M2.53001 7.81595C3.49179 4.73911 6.43281 2.5 9.91173 2.5C13.1684 2.5 15.9537 4.46214 17.0852 7.23684L17.6179 8.67647M17.6179 8.67647L18.5002 4.26471M17.6179 8.67647L13.6473 6.91176M17.4995 12.1841C16.5378 15.2609 13.5967 17.5 10.1178 17.5C6.86118 17.5 4.07589 15.5379 2.94432 12.7632L2.41165 11.3235M2.41165 11.3235L1.5293 15.7353M2.41165 11.3235L6.38224 13.0882\"></path></g></svg></button><button class=\"pencraft pc-reset pencraft icon-container view-image\" tabindex=\"0\" type=\"button\"><svg class=\"lucide lucide-maximize2 lucide-maximize-2\" fill=\"none\" height=\"20\" stroke=\"currentColor\" stroke-linecap=\"round\" stroke-linejoin=\"round\" stroke-width=\"2\" viewBox=\"0 0 24 24\" width=\"20\" xmlns=\"http://www.w3.org/2000/svg\"><polyline points=\"15 3 21 3 21 9\"></polyline><polyline points=\"9 21 3 21 3 15\"></polyline><line x1=\"21\" x2=\"14\" y1=\"3\" y2=\"10\"></line><line x1=\"3\" x2=\"10\" y1=\"21\" y2=\"14\"></line></svg></button></div></div></div></a><figcaption class=\"image-caption\">Vitalik is the inventor of Ethereum. Deepfates is a weird renegade cyberpunk AI whisperer expert (<a href=\"https://x.com/deepfates/status/2026408465762234747\">source</a>)</figcaption></figure></div><div class=\"captioned-image-container\"><figure><a class=\"image-link image2 is-viewable-img\" href=\"https://substackcdn.com/image/fetch/$s_!N9m5!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F772658c0-a86a-481f-a50c-452b052a3d30_581x772.png\" target=\"_blank\"><div class=\"image2-inset\"><source type=\"image/webp\" /><img alt=\"\" class=\"sizing-normal\" height=\"694.9328743545611\" src=\"https://substackcdn.com/image/fetch/$s_!N9m5!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F772658c0-a86a-481f-a50c-452b052a3d30_581x772.png\" width=\"523\" /><div class=\"image-link-expand\"><div class=\"pencraft pc-display-flex pc-gap-8 pc-reset\"><button class=\"pencraft pc-reset pencraft icon-container restack-image\" tabindex=\"0\" type=\"button\"><svg fill=\"none\" height=\"20\" stroke=\"var(--color-fg-primary)\" stroke-linecap=\"round\" stroke-linejoin=\"round\" stroke-width=\"1.5\" viewBox=\"0 0 20 20\" width=\"20\" xmlns=\"http://www.w3.org/2000/svg\"><g><title></title><path d=\"M2.53001 7.81595C3.49179 4.73911 6.43281 2.5 9.91173 2.5C13.1684 2.5 15.9537 4.46214 17.0852 7.23684L17.6179 8.67647M17.6179 8.67647L18.5002 4.26471M17.6179 8.67647L13.6473 6.91176M17.4995 12.1841C16.5378 15.2609 13.5967 17.5 10.1178 17.5C6.86118 17.5 4.07589 15.5379 2.94432 12.7632L2.41165 11.3235M2.41165 11.3235L1.5293 15.7353M2.41165 11.3235L6.38224 13.0882\"></path></g></svg></button><button class=\"pencraft pc-reset pencraft icon-container view-image\" tabindex=\"0\" type=\"button\"><svg class=\"lucide lucide-maximize2 lucide-maximize-2\" fill=\"none\" height=\"20\" stroke=\"currentColor\" stroke-linecap=\"round\" stroke-linejoin=\"round\" stroke-width=\"2\" viewBox=\"0 0 24 24\" width=\"20\" xmlns=\"http://www.w3.org/2000/svg\"><polyline points=\"15 3 21 3 21 9\"></polyline><polyline points=\"9 21 3 21 3 15\"></polyline><line x1=\"21\" x2=\"14\" y1=\"3\" y2=\"10\"></line><line x1=\"3\" x2=\"10\" y1=\"21\" y2=\"14\"></line></svg></button></div></div></div></a><figcaption class=\"image-caption\">Neil Chilson, former chief technologist at the Trump FTC (<a href=\"https://x.com/neil_chilson\">source</a>).</figcaption></figure></div><div class=\"captioned-image-container\"><figure><a class=\"image-link image2 is-viewable-img\" href=\"https://substackcdn.com/image/fetch/$s_!Pode!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F15a4ab9e-68fb-400f-8304-3ec7222958d3_587x1506.png\" target=\"_blank\"><div class=\"image2-inset\"><source type=\"image/webp\" /><img alt=\"\" class=\"sizing-normal\" height=\"1387.9829642248721\" src=\"https://substackcdn.com/image/fetch/$s_!Pode!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F15a4ab9e-68fb-400f-8304-3ec7222958d3_587x1506.png\" width=\"541\" /><div class=\"image-link-expand\"><div class=\"pencraft pc-display-flex pc-gap-8 pc-reset\"><button class=\"pencraft pc-reset pencraft icon-container restack-image\" tabindex=\"0\" type=\"button\"><svg fill=\"none\" height=\"20\" stroke=\"var(--color-fg-primary)\" stroke-linecap=\"round\" stroke-linejoin=\"round\" stroke-width=\"1.5\" viewBox=\"0 0 20 20\" width=\"20\" xmlns=\"http://www.w3.org/2000/svg\"><g><title></title><path d=\"M2.53001 7.81595C3.49179 4.73911 6.43281 2.5 9.91173 2.5C13.1684 2.5 15.9537 4.46214 17.0852 7.23684L17.6179 8.67647M17.6179 8.67647L18.5002 4.26471M17.6179 8.67647L13.6473 6.91176M17.4995 12.1841C16.5378 15.2609 13.5967 17.5 10.1178 17.5C6.86118 17.5 4.07589 15.5379 2.94432 12.7632L2.41165 11.3235M2.41165 11.3235L1.5293 15.7353M2.41165 11.3235L6.38224 13.0882\"></path></g></svg></button><button class=\"pencraft pc-reset pencraft icon-container view-image\" tabindex=\"0\" type=\"button\"><svg class=\"lucide lucide-maximize2 lucide-maximize-2\" fill=\"none\" height=\"20\" stroke=\"currentColor\" stroke-linecap=\"round\" stroke-linejoin=\"round\" stroke-width=\"2\" viewBox=\"0 0 24 24\" width=\"20\" xmlns=\"http://www.w3.org/2000/svg\"><polyline points=\"15 3 21 3 21 9\"></polyline><polyline points=\"9 21 3 21 3 15\"></polyline><line x1=\"21\" x2=\"14\" y1=\"3\" y2=\"10\"></line><line x1=\"3\" x2=\"10\" y1=\"21\" y2=\"14\"></line></svg></button></div></div></div></a><figcaption class=\"image-caption\">Dean Ball, previous Trump White House OSTP Senior Policy Advisor on AI (<a href=\"https://x.com/deanwball/status/2026416091149299757\">source</a>).</figcaption></figure></div><div class=\"captioned-image-container\"><figure><a class=\"image-link image2 is-viewable-img\" href=\"https://substackcdn.com/image/fetch/$s_!DrEn!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Feaff377e-1f44-450c-82af-fc62d33e0832_1225x1128.jpeg\" target=\"_blank\"><div class=\"image2-inset\"><source type=\"image/webp\" /><img alt=\"Image\" class=\"sizing-normal\" height=\"515.6571428571428\" src=\"https://substackcdn.com/image/fetch/$s_!DrEn!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Feaff377e-1f44-450c-82af-fc62d33e0832_1225x1128.jpeg\" title=\"Image\" width=\"560\" /><div class=\"image-link-expand\"><div class=\"pencraft pc-display-flex pc-gap-8 pc-reset\"><button class=\"pencraft pc-reset pencraft icon-container restack-image\" tabindex=\"0\" type=\"button\"><svg fill=\"none\" height=\"20\" stroke=\"var(--color-fg-primary)\" stroke-linecap=\"round\" stroke-linejoin=\"round\" stroke-width=\"1.5\" viewBox=\"0 0 20 20\" width=\"20\" xmlns=\"http://www.w3.org/2000/svg\"><g><title></title><path d=\"M2.53001 7.81595C3.49179 4.73911 6.43281 2.5 9.91173 2.5C13.1684 2.5 15.9537 4.46214 17.0852 7.23684L17.6179 8.67647M17.6179 8.67647L18.5002 4.26471M17.6179 8.67647L13.6473 6.91176M17.4995 12.1841C16.5378 15.2609 13.5967 17.5 10.1178 17.5C6.86118 17.5 4.07589 15.5379 2.94432 12.7632L2.41165 11.3235M2.41165 11.3235L1.5293 15.7353M2.41165 11.3235L6.38224 13.0882\"></path></g></svg></button><button class=\"pencraft pc-reset pencraft icon-container view-image\" tabindex=\"0\" type=\"button\"><svg class=\"lucide lucide-maximize2 lucide-maximize-2\" fill=\"none\" height=\"20\" stroke=\"currentColor\" stroke-linecap=\"round\" stroke-linejoin=\"round\" stroke-width=\"2\" viewBox=\"0 0 24 24\" width=\"20\" xmlns=\"http://www.w3.org/2000/svg\"><polyline points=\"15 3 21 3 21 9\"></polyline><polyline points=\"9 21 3 21 3 15\"></polyline><line x1=\"21\" x2=\"14\" y1=\"3\" y2=\"10\"></line><line x1=\"3\" x2=\"10\" y1=\"21\" y2=\"14\"></line></svg></button></div></div></div></a><figcaption class=\"image-caption\">Superforecaster Nu&#241;o Sempere, maybe as part of his work with Sentinel. He seems to think higher chance of supply chain risk than others, but that supply chain risk might be handled in a way that only affects DoD contracts themselves, which wouldn&#8217;t be so bad. I haven&#8217;t heard anyone else make this distinction. Tweet <a href=\"https://x.com/NunoSempere/status/2026463214200860943\">here,</a> full document <a href=\"https://docs.google.com/document/d/14wk-FZi_y3-RYdqJLkFH30eNahMWRxKHV1pu_NOSY1Y/edit?tab=t.0#heading=h.x572pgtwuxi2\">here</a>.</figcaption></figure></div><p>And big praise to most other AI companies, including Anthropic&#8217;s competitors, for standing up for them and for the AI industry more broadly:</p><div class=\"twitter-embed\"></div><div class=\"captioned-image-container\"><figure><a class=\"image-link image2\" href=\"https://substackcdn.com/image/fetch/$s_!ESsA!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbf56b5a9-faff-48a5-a9bb-25b28f64d390_586x247.png\" target=\"_blank\"><div class=\"image2-inset\"><source type=\"image/webp\" /><img alt=\"\" class=\"sizing-normal\" height=\"234.35494880546077\" src=\"https://substackcdn.com/image/fetch/$s_!ESsA!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbf56b5a9-faff-48a5-a9bb-25b28f64d390_586x247.png\" width=\"556\" /><div></div></div></a><figcaption class=\"image-caption\">Boaz is member of technical staff at OpenAI. Jeff is Chief Scientist at Google (see also <a href=\"https://github.com/LRitzdorf/TheJeffDeanFacts\">Jeff Dean Facts</a>)</figcaption></figure></div><p>And most of all, big praise to the American people, with special love to the large plurality of Trump voters standing against this:</p><div class=\"captioned-image-container\"><figure><a class=\"image-link image2 is-viewable-img\" href=\"https://substackcdn.com/image/fetch/$s_!AYA7!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd48d2b70-91e9-452e-8244-383772d64877_1772x1122.jpeg\" target=\"_blank\"><div class=\"image2-inset\"><source type=\"image/webp\" /><img alt=\"Image\" class=\"sizing-normal\" height=\"363.4807692307692\" src=\"https://substackcdn.com/image/fetch/$s_!AYA7!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd48d2b70-91e9-452e-8244-383772d64877_1772x1122.jpeg\" title=\"Image\" width=\"574\" /><div class=\"image-link-expand\"><div class=\"pencraft pc-display-flex pc-gap-8 pc-reset\"><button class=\"pencraft pc-reset pencraft icon-container restack-image\" tabindex=\"0\" type=\"button\"><svg fill=\"none\" height=\"20\" stroke=\"var(--color-fg-primary)\" stroke-linecap=\"round\" stroke-linejoin=\"round\" stroke-width=\"1.5\" viewBox=\"0 0 20 20\" width=\"20\" xmlns=\"http://www.w3.org/2000/svg\"><g><title></title><path d=\"M2.53001 7.81595C3.49179 4.73911 6.43281 2.5 9.91173 2.5C13.1684 2.5 15.9537 4.46214 17.0852 7.23684L17.6179 8.67647M17.6179 8.67647L18.5002 4.26471M17.6179 8.67647L13.6473 6.91176M17.4995 12.1841C16.5378 15.2609 13.5967 17.5 10.1178 17.5C6.86118 17.5 4.07589 15.5379 2.94432 12.7632L2.41165 11.3235M2.41165 11.3235L1.5293 15.7353M2.41165 11.3235L6.38224 13.0882\"></path></g></svg></button><button class=\"pencraft pc-reset pencraft icon-container view-image\" tabindex=\"0\" type=\"button\"><svg class=\"lucide lucide-maximize2 lucide-maximize-2\" fill=\"none\" height=\"20\" stroke=\"currentColor\" stroke-linecap=\"round\" stroke-linejoin=\"round\" stroke-width=\"2\" viewBox=\"0 0 24 24\" width=\"20\" xmlns=\"http://www.w3.org/2000/svg\"><polyline points=\"15 3 21 3 21 9\"></polyline><polyline points=\"9 21 3 21 3 15\"></polyline><line x1=\"21\" x2=\"14\" y1=\"3\" y2=\"10\"></line><line x1=\"3\" x2=\"10\" y1=\"21\" y2=\"14\"></line></svg></button></div></div></div></a><figcaption class=\"image-caption\">Source: Polling firm <a href=\"https://x.com/davidshor/status/2026418697271919008\">Blue Rose Research</a></figcaption></figure></div><div class=\"footnote\"><a class=\"footnote-number\" contenteditable=\"false\" href=\"https://www.astralcodexten.com/feed#footnote-anchor-1\" id=\"footnote-1\" target=\"_self\">1</a><div class=\"footnote-content\"><p>This story requires some reading between the lines - the exact text of the contract isn&#8217;t available - but something like it is suggested by the way both sides have been presenting the negotiations.</p></div></div><div class=\"footnote\"><a class=\"footnote-number\" contenteditable=\"false\" href=\"https://www.astralcodexten.com/feed#footnote-anchor-2\" id=\"footnote-2\" target=\"_self\">2</a><div class=\"footnote-content\"><p>Depending on the details, either the Pentagon or the whole executive branch.</p></div></div><div class=\"footnote\"><a class=\"footnote-number\" contenteditable=\"false\" href=\"https://www.astralcodexten.com/feed#footnote-anchor-3\" id=\"footnote-3\" target=\"_self\">3</a><div class=\"footnote-content\"><p>Nu&#241;o Sempere suggests that it might only apply to the specific contracts involving the DoD, which would still be bad but not catastrophic.</p></div></div><div class=\"footnote\"><a class=\"footnote-number\" contenteditable=\"false\" href=\"https://www.astralcodexten.com/feed#footnote-anchor-4\" id=\"footnote-4\" target=\"_self\">4</a><div class=\"footnote-content\"><p>More specifically, Anthropic and Dario have lately been publishing some work saying they&#8217;re less-than-maximally concerned about AI scheming and power-seeking and are going to focus their safety efforts on smaller risks like AIs with coincidentally bad personalities, humans misusing AIs, etc. This could either be their honest opinion, or an excuse to jettison annoying safety work in favor of the bottom line. This standoff suggests they are very genuinely concerned about humans misusing AI and willing to stand against it even when it threatens their bottom line, which means it&#8217;s their honest opinion, which means that maybe when there&#8217;s more evidence for AI power-seeking they&#8217;ll come around and start honestly worrying about that too.</p></div></div><div class=\"footnote\"><a class=\"footnote-number\" contenteditable=\"false\" href=\"https://www.astralcodexten.com/feed#footnote-anchor-5\" id=\"footnote-5\" target=\"_self\">5</a><div class=\"footnote-content\"><p>Supposedly the Pentagon already has Grok integrated with classified systems, but it&#8217;s not good and they want a more cutting-edge model, which means either Claude, GPT, or Gemini.</p></div></div><div class=\"footnote\"><a class=\"footnote-number\" contenteditable=\"false\" href=\"https://www.astralcodexten.com/feed#footnote-anchor-6\" id=\"footnote-6\" target=\"_self\">6</a><div class=\"footnote-content\"><p>What prevents the Pentagon from signing a contract saying they won&#8217;t order Anthropic to do mass surveillance, then ordering them to do mass surveillance anyway? I&#8217;m not sure! I think the way this plays out is that Anthropic says no, and now the <em>Pentagon</em> is hobbled by the fact that it&#8217;s hard to do contract lawsuits over classified actions.</p></div></div>"
            ],
            "link": "https://www.astralcodexten.com/p/the-pentagon-threatens-anthropic",
            "publishedAt": "2026-02-25",
            "source": "SlateStarCodex",
            "summary": "<p>Here&#8217;s my understanding of <a href=\"https://www.bbc.com/news/articles/cjrq1vwe73po\">the situation</a>:</p><p>Anthropic signed a contract with the Pentagon last summer. It originally said the Pentagon had to follow Anthropic&#8217;s Usage Policy like everyone else. In January, the Pentagon attempted to renegotiate, asking to ditch the Usage Policy and instead have Anthropic&#8217;s AIs available for &#8220;all lawful purposes&#8221;<a class=\"footnote-anchor\" href=\"https://www.astralcodexten.com/feed#footnote-1\" id=\"footnote-anchor-1\" target=\"_self\">1</a>. Anthropic demurred, asking for a guarantee that their AIs would not be used for mass surveillance of American citizens or no-human-in-the-loop killbots. The Pentagon refused the guarantees, demanding that Anthropic accept the renegotiation unconditionally and threatening &#8220;consequences&#8221; if they refused. These consequences are generally understood to be some mix of :</p><ul><li><p>canceling the contract</p></li><li><p>using the Defense Production Act, a law which lets the Pentagon force companies to do things, to force Anthropic to agree.</p></li><li><p>the nuclear option, designating Anthropic a &#8220;supply chain risk&#8221;. This would ban US companies that use Anthropic products from doing business with the military<a class=\"footnote-anchor\" href=\"https://www.astralcodexten.com/feed#footnote-2\" id=\"footnote-anchor-2\" target=\"_self\">2</a>. Since many companies do some business with the government, this would lock Anthropic out of large parts of the corporate world and be potentially fatal to their business<a class=\"footnote-anchor\" href=\"https://www.astralcodexten.com/feed#footnote-3\" id=\"footnote-anchor-3\" target=\"_self\">3</a>. The &#8220;supply chain risk&#8221; designation has previously only been used for foreign",
            "title": "The Pentagon Threatens Anthropic"
        },
        {
            "content": [
                "<p>Here&#8217;s a funny move I find myself making lately:</p><ol><li><p><em>Encounter a complex programming challenge</em></p></li><li><p>Get three papers related to my problem</p></li><li><p>Open three terminal windows, have separate Claudes read each paper</p></li><li><p>Ask some questions about each paper</p></li><li><p>Save a summary of each conversation to an .md file.</p></li><li><p>/clear</p></li><li><p>Have Claude read all three summaries. Generate a synthesis with a comparisons table.</p></li><li><p>Save to synthesis.md.</p></li><li><p>/clear</p></li><li><p>Include @synthesis.md and brainstorm with Claude</p></li><li><p><em>A solution appears</em></p></li></ol><p>This works great for learning speed-runs, too.</p><ol><li><p>Go to <a href=\"https://plato.stanford.edu/\">plato.stanford.edu</a> and get pages for three different philosophers (Plato, Kant, Nietzsche, say)</p></li><li><p>Open three terminal windows, have separate Claudes read each page</p></li><li><p>Ask some questions about each philosopher</p></li><li><p>Save a summary of each conversation to an .md file.</p></li><li><p>/clear</p></li><li><p>Have Claude read all three summaries, synthesize, compare and contrast.</p></li><li><p>Save to synthesis.md.</p></li><li><p>/clear</p></li><li><p>Include @synthesis.md and have a conversation</p></li></ol><p>(Claude Code is an excellent <a href=\"https://newsletter.squishy.computer/i/79213755/thinking-with-a-second-brain\">tool for thought</a>, by the way.)</p><div class=\"captioned-image-container\"><figure><a class=\"image-link image2 is-viewable-img\" href=\"https://substackcdn.com/image/fetch/$s_!F2-z!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3ad14181-7a85-42fc-8ef8-c33ca5fd32df_2240x1400.png\" target=\"_blank\"><div class=\"image2-inset\"><source type=\"image/webp\" /><img alt=\"\" class=\"sizing-large\" height=\"750\" src=\"https://substackcdn.com/image/fetch/$s_!F2-z!,w_2400,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3ad14181-7a85-42fc-8ef8-c33ca5fd32df_2240x1400.png\" width=\"1200\" /><div class=\"image-link-expand\"><div class=\"pencraft pc-display-flex pc-gap-8 pc-reset\"><button class=\"pencraft pc-reset pencraft icon-container restack-image\" tabindex=\"0\" type=\"button\"><svg fill=\"none\" height=\"20\" stroke=\"var(--color-fg-primary)\" stroke-linecap=\"round\" stroke-linejoin=\"round\" stroke-width=\"1.5\" viewBox=\"0 0 20 20\" width=\"20\" xmlns=\"http://www.w3.org/2000/svg\"><g><title></title><path d=\"M2.53001 7.81595C3.49179 4.73911 6.43281 2.5 9.91173 2.5C13.1684 2.5 15.9537 4.46214 17.0852 7.23684L17.6179 8.67647M17.6179 8.67647L18.5002 4.26471M17.6179 8.67647L13.6473 6.91176M17.4995 12.1841C16.5378 15.2609 13.5967 17.5 10.1178 17.5C6.86118 17.5 4.07589 15.5379 2.94432 12.7632L2.41165 11.3235M2.41165 11.3235L1.5293 15.7353M2.41165 11.3235L6.38224 13.0882\"></path></g></svg></button><button class=\"pencraft pc-reset pencraft icon-container view-image\" tabindex=\"0\" type=\"button\"><svg class=\"lucide lucide-maximize2 lucide-maximize-2\" fill=\"none\" height=\"20\" stroke=\"currentColor\" stroke-linecap=\"round\" stroke-linejoin=\"round\" stroke-width=\"2\" viewBox=\"0 0 24 24\" width=\"20\" xmlns=\"http://www.w3.org/2000/svg\"><polyline points=\"15 3 21 3 21 9\"></polyline><polyline points=\"9 21 3 21 3 15\"></polyline><line x1=\"21\" x2=\"14\" y1=\"3\" y2=\"10\"></line><line x1=\"3\" x2=\"10\" y1=\"21\" y2=\"14\"></line></svg></button></div></div></div></a><figcaption class=\"image-caption\">Josef Albers, 1942. Graphic Tectonics: Shrine</figcaption></figure></div><p>I think of this move as <em>folding context</em>, because it feels like folding dough when baking. The results I get from the AI are much, much better when I fold the context.</p><p>Why is folding context so effective? Well, it&#8217;s almost like we&#8217;re doing <a href=\"https://www.anthropic.com/engineering/multi-agent-research-system\">Deep Research by hand</a>. Each Claude instance is functioning as a research agent. Critically, these are research agents that we are guiding manually. In each thread, we&#8217;re asking questions that draw out the dimensions of context that are most meaningful to our larger question. It&#8217;s a kind of smart compression, and <a href=\"https://timkellogg.me/blog/2025/06/15/compression\">compression = intelligence</a>, or close enough.</p><p>Context folding also creates useful byproducts. These little markdown files are valuable compressed insights that you can include in future conversations to infuse them with deep contextual background. Our team has started checking them into the codebase under the <code>docs</code> directory.</p><div><hr /></div><p>Context folding is one example of a more general pattern for provoking effective behavior from LLMs. When we fold context, we <strong>diverge</strong>, exploring separate paths in different context windows. After exploring the solution space, we <strong>converge</strong>, comparing what we have learned, and synthesizing everything together.</p><p>Claude Code&#8217;s <em>Plan mode</em> is another example of this pattern.  First Claude <strong>diverges</strong>, exploring the codebase with sub-agents, then Claude <strong>converges</strong>, writing up a plan that synthesizes the findings.</p><div class=\"captioned-image-container\"><figure><a class=\"image-link image2 is-viewable-img\" href=\"https://substackcdn.com/image/fetch/$s_!Ppiq!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7e2da523-34a8-4caa-9dc7-fa70fda082ac_700x500.jpeg\" target=\"_blank\"><div class=\"image2-inset\"><source type=\"image/webp\" /><img alt=\"Create choices, then make choices\" class=\"sizing-normal\" height=\"520\" src=\"https://substackcdn.com/image/fetch/$s_!Ppiq!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7e2da523-34a8-4caa-9dc7-fa70fda082ac_700x500.jpeg\" title=\"Create choices, then make choices\" width=\"728\" /><div class=\"image-link-expand\"><div class=\"pencraft pc-display-flex pc-gap-8 pc-reset\"><button class=\"pencraft pc-reset pencraft icon-container restack-image\" tabindex=\"0\" type=\"button\"><svg fill=\"none\" height=\"20\" stroke=\"var(--color-fg-primary)\" stroke-linecap=\"round\" stroke-linejoin=\"round\" stroke-width=\"1.5\" viewBox=\"0 0 20 20\" width=\"20\" xmlns=\"http://www.w3.org/2000/svg\"><g><title></title><path d=\"M2.53001 7.81595C3.49179 4.73911 6.43281 2.5 9.91173 2.5C13.1684 2.5 15.9537 4.46214 17.0852 7.23684L17.6179 8.67647M17.6179 8.67647L18.5002 4.26471M17.6179 8.67647L13.6473 6.91176M17.4995 12.1841C16.5378 15.2609 13.5967 17.5 10.1178 17.5C6.86118 17.5 4.07589 15.5379 2.94432 12.7632L2.41165 11.3235M2.41165 11.3235L1.5293 15.7353M2.41165 11.3235L6.38224 13.0882\"></path></g></svg></button><button class=\"pencraft pc-reset pencraft icon-container view-image\" tabindex=\"0\" type=\"button\"><svg class=\"lucide lucide-maximize2 lucide-maximize-2\" fill=\"none\" height=\"20\" stroke=\"currentColor\" stroke-linecap=\"round\" stroke-linejoin=\"round\" stroke-width=\"2\" viewBox=\"0 0 24 24\" width=\"20\" xmlns=\"http://www.w3.org/2000/svg\"><polyline points=\"15 3 21 3 21 9\"></polyline><polyline points=\"9 21 3 21 3 15\"></polyline><line x1=\"21\" x2=\"14\" y1=\"3\" y2=\"10\"></line><line x1=\"3\" x2=\"10\" y1=\"21\" y2=\"14\"></line></svg></button></div></div></div></a></figure></div><p>Designers call this process the <a href=\"https://en.wikipedia.org/wiki/Double_Diamond_(design_process_model)\">diamond model</a>. When using the diamond model, we often do two passes, one for research, one for development. Most AI tools today do just one pass (<em>Plan mode</em>). Are there gains to be had in doing more than one pass?</p><div><hr /></div><p>Cyberneticist <a href=\"https://en.wikipedia.org/wiki/Design_rationale\">Horst Rittel</a> situates the diamond model within a larger theory framework. He says design is <em>&#8220;the generation of variety and the reduction of variety.&#8221;</em> </p><p>Variety here means <em><a href=\"https://en.wikipedia.org/wiki/Variety_(cybernetics)\">cybernetic variety</a></em>, as in the set of all possible behaviors that a system can generate.</p><blockquote><p><strong>Ashby&#8217;s Law of Requisite Variety: </strong>The variety of a regulator must be at least as large as that of the system it regulates.</p></blockquote><p>Ashby&#8217;s Law tells us that a solution must be at least as complex as the problem. If it isn&#8217;t, the problem spills over, routes around the solution, becomes an externality. This is useful information. It tells us that to solve a problem the LLM has to be at least as large as the problem.</p><p>And LLMs do have a lot of cybernetic variety. An internet&#8217;s worth. This extreme variety manifests when you run a raw base model. What you get back is inhuman, the rantings of someone channeling hyperdimensional cosmic horrors. A base model&#8217;s variety is way beyond the range of normal human behavior. </p><div class=\"captioned-image-container\"><figure><a class=\"image-link image2 is-viewable-img\" href=\"https://substackcdn.com/image/fetch/$s_!ct-P!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F526fe074-8061-4a33-8e00-551e53cace06_1007x790.png\" target=\"_blank\"><div class=\"image2-inset\"><source type=\"image/webp\" /><img alt=\"The Shoggoth Meme (HP Lovecraft Meets AI) | by Swetlana AI ...\" class=\"sizing-normal\" height=\"363.227408142999\" src=\"https://substackcdn.com/image/fetch/$s_!ct-P!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F526fe074-8061-4a33-8e00-551e53cace06_1007x790.png\" title=\"The Shoggoth Meme (HP Lovecraft Meets AI) | by Swetlana AI ...\" width=\"463\" /><div class=\"image-link-expand\"><div class=\"pencraft pc-display-flex pc-gap-8 pc-reset\"><button class=\"pencraft pc-reset pencraft icon-container restack-image\" tabindex=\"0\" type=\"button\"><svg fill=\"none\" height=\"20\" stroke=\"var(--color-fg-primary)\" stroke-linecap=\"round\" stroke-linejoin=\"round\" stroke-width=\"1.5\" viewBox=\"0 0 20 20\" width=\"20\" xmlns=\"http://www.w3.org/2000/svg\"><g><title></title><path d=\"M2.53001 7.81595C3.49179 4.73911 6.43281 2.5 9.91173 2.5C13.1684 2.5 15.9537 4.46214 17.0852 7.23684L17.6179 8.67647M17.6179 8.67647L18.5002 4.26471M17.6179 8.67647L13.6473 6.91176M17.4995 12.1841C16.5378 15.2609 13.5967 17.5 10.1178 17.5C6.86118 17.5 4.07589 15.5379 2.94432 12.7632L2.41165 11.3235M2.41165 11.3235L1.5293 15.7353M2.41165 11.3235L6.38224 13.0882\"></path></g></svg></button><button class=\"pencraft pc-reset pencraft icon-container view-image\" tabindex=\"0\" type=\"button\"><svg class=\"lucide lucide-maximize2 lucide-maximize-2\" fill=\"none\" height=\"20\" stroke=\"currentColor\" stroke-linecap=\"round\" stroke-linejoin=\"round\" stroke-width=\"2\" viewBox=\"0 0 24 24\" width=\"20\" xmlns=\"http://www.w3.org/2000/svg\"><polyline points=\"15 3 21 3 21 9\"></polyline><polyline points=\"9 21 3 21 3 15\"></polyline><line x1=\"21\" x2=\"14\" y1=\"3\" y2=\"10\"></line><line x1=\"3\" x2=\"10\" y1=\"21\" y2=\"14\"></line></svg></button></div></div></div></a></figure></div><p>So, LLMs have a lot of variety. Too much variety to fit within the tiny reference frame of human meanings. The solution? Slap a smiley face on top. RLHF reduces the variety of LLMs to meaningful ranges.</p><p>But RLHF reduces variety almost <em>too</em> much. The model comes away with a strong tendency to <a href=\"https://www.challies.com/articles/no-mans-sky-and-10000-bowls-of-plain-oatmeal/\">seek toward oatmeal</a>. It has lost the requisite variety to surprise.</p><blockquote><p>But being in control means defining the range of what will be considered, that is, the range of the possible. In effect, when I am in control I restrict the world to what I can imagine or permit&#8230;<br /><em>(Ranulph Glanville, 2019, &#8220;Design Cybernetics&#8221;)</em></p></blockquote><p>We can bring back some of this variety through the context window. LLMs are highly steerable via the context window, and through prompting, you can sort of pluck at the latent dimensions of the model, drawing out distinct behaviors.</p><p>This is why <a href=\"https://newsletter.squishy.computer/p/feedback-is-all-you-need\">putting an LLM in a loop with tools</a> makes it so much smarter. Tools add contextual variety to the context window, which educes latent variety from the model. The conversation between model and outside context generates meaningful behavior that has the variety to surprise.</p><p><a href=\"https://openclaw.ai/\">OpenClaw</a> is an extreme example here. When you give a claw full access to your computer, it imports the variety of a high-level Turing machine into the agentic loop. That&#8217;s a lot of variety, <a href=\"https://en.wikipedia.org/wiki/Universal_Turing_machine\">potentially unbounded</a>. Some of that variety is deeply surprising, like <a href=\"https://youtu.be/4uzGDAoNOZc?si=fehXEVtzE5vjiYVX&amp;t=465\">when OpenClaw&#8217;s creator discovered it could figure out how to do voice transcription on its own</a>. Some of that variety is negatively surprising, like <a href=\"https://x.com/summeryue0/status/2025774069124399363\">when you ask your claw to tidy up your inbox, and it deletes all of your emails</a>. Either way, a claw with full bash access has more than enough variety to surprise. </p><blockquote><p>The great benefit of not having enough variety to control a system is that, if I give up trying to control, I can discover many possibilities I would have excluded if I had insisted on being in control. These possibilities are unexpected, outside my frame of reference, in a word, novel.<br /><em>(Ranulph Glanville, 2019, &#8220;Design Cybernetics&#8221;)</em></p></blockquote><p>From the standpoint of requisite variety, folding context makes a lot of sense. We&#8217;re letting our agents&#8217; understandings diverge, specialize, and spin off in unexpected directions. The difference in their understandings, the contradiction, is <em>alpha</em>, valuable variety.</p><blockquote><p>Do I contradict myself?<br />Very well then I contradict myself,<br />(I am large, I contain multitudes.)</p></blockquote><div><hr /></div><p>Questions I&#8217;m asking myself:</p><ul><li><p>Where does the variety live? In the agent? In me? In the loop between the agent and me?</p></li><li><p>Who has requisite variety? Who is steering the system? The agent? Me? Neither? Do we have requisite variety together?</p></li><li><p>Can I increase variety by spinning up agents in divergent roles? <a href=\"https://github.com/gordonbrander/busytown\">Busytown</a> is my current attempt at this, a multi-agent swarm coordinating over SQLite.</p></li><li><p>Is the variety manifesting within meaningful <a href=\"https://newsletter.squishy.computer/p/possibility-space\">parts of the possibility space</a>? What is meaningful to me? Over the short term? Over the long term? Can I craft agentic systems that <a href=\"https://newsletter.squishy.computer/p/ritual-technology\">manifest deeper meaning over time</a>?</p></li><li><p>Erich Fromm said, <em>creativity requires the courage to let go of certainties</em>. Is my notion of meaning constraining me to oatmeal? Do I need to increase the variety of my agentic systems? Do I need to lose some control? Where can I embrace threats to meaning, to generate creative breakthroughs? </p></li></ul>"
            ],
            "link": "https://newsletter.squishy.computer/p/folding-context",
            "publishedAt": "2026-02-25",
            "source": "Squishy Computer",
            "summary": "<p>Here&#8217;s a funny move I find myself making lately:</p><ol><li><p><em>Encounter a complex programming challenge</em></p></li><li><p>Get three papers related to my problem</p></li><li><p>Open three terminal windows, have separate Claudes read each paper</p></li><li><p>Ask some questions about each paper</p></li><li><p>Save a summary of each conversation to an .md file.</p></li><li><p>/clear</p></li><li><p>Have Claude read all three summaries. Generate a synthesis with a comparisons table.</p></li><li><p>Save to synthesis.md.</p></li><li><p>/clear</p></li><li><p>Include @synthesis.md and brainstorm with Claude</p></li><li><p><em>A solution appears</em></p></li></ol><p>This works great for learning speed-runs, too.</p><ol><li><p>Go to <a href=\"https://plato.stanford.edu/\">plato.stanford.edu</a> and get pages for three different philosophers (Plato, Kant, Nietzsche, say)</p></li><li><p>Open three terminal windows, have separate Claudes read each page</p></li><li><p>Ask some questions about each philosopher</p></li><li><p>Save a summary of each conversation to an .md file.</p></li><li><p>/clear</p></li><li><p>Have Claude read all three summaries, synthesize, compare and contrast.</p></li><li><p>Save to synthesis.md.</p></li><li><p>/clear</p></li><li><p>Include @synthesis.md and have a conversation</p></li></ol><p>(Claude Code is an excellent <a href=\"https://newsletter.squishy.computer/i/79213755/thinking-with-a-second-brain\">tool for thought</a>, by the way.)</p><div class=\"captioned-image-container\"><figure><a class=\"image-link image2 is-viewable-img\" href=\"https://substackcdn.com/image/fetch/$s_!F2-z!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3ad14181-7a85-42fc-8ef8-c33ca5fd32df_2240x1400.png\" target=\"_blank\"><div class=\"image2-inset\"><source type=\"image/webp\" /><img alt=\"\" class=\"sizing-large\" height=\"750\" src=\"https://substackcdn.com/image/fetch/$s_!F2-z!,w_2400,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3ad14181-7a85-42fc-8ef8-c33ca5fd32df_2240x1400.png\" width=\"1200\" /><div class=\"image-link-expand\"><div class=\"pencraft pc-display-flex pc-gap-8 pc-reset\"><button class=\"pencraft pc-reset pencraft icon-container restack-image\" tabindex=\"0\" type=\"button\"><svg fill=\"none\" height=\"20\" stroke=\"var(--color-fg-primary)\" stroke-linecap=\"round\" stroke-linejoin=\"round\" stroke-width=\"1.5\" viewBox=\"0 0 20 20\" width=\"20\" xmlns=\"http://www.w3.org/2000/svg\"><g><title></title><path d=\"M2.53001 7.81595C3.49179 4.73911 6.43281 2.5 9.91173 2.5C13.1684 2.5 15.9537 4.46214 17.0852 7.23684L17.6179 8.67647M17.6179 8.67647L18.5002 4.26471M17.6179 8.67647L13.6473 6.91176M17.4995 12.1841C16.5378 15.2609 13.5967 17.5 10.1178 17.5C6.86118 17.5 4.07589 15.5379 2.94432 12.7632L2.41165",
            "title": "Folding context"
        },
        {
            "content": [
                "<p><img alt=\"\" src=\"https://taylor.town/kakm.jpeg\" /></p>"
            ],
            "link": "https://taylor.town/kakm",
            "publishedAt": "2026-02-25",
            "source": "Taylor Troesh",
            "summary": "via Khan Academy Knowledge Map",
            "title": "Math Milestones"
        },
        {
            "content": [
                "<p>The situation in AI in 2026 is crazy. The confrontation between Anthropic and Secretary of War Pete Hegseth is a new level of crazy. It risks turning quite bad for all. There\u2019s also nothing stopped it from turning out fine for everyone.</p>\n<p>By at least one report <a href=\"https://x.com/JenGriffinFNC/status/2026384132360605892\">the recent meeting between the two parties was cordial and all business</a>, but Anthropic has been given a deadline of 5pm eastern on Friday to modify its existing agreed-upon contract to grant \u2018unfettered access\u2019 to Claude, or else.</p>\n<p><a href=\"https://x.com/DeItaone/status/2026391179416502465\">Anthropic has been the most enthusiastic supporter our military has in AI and in tech, but on this point have strongly signaled</a> they with this they cannot comply. <a href=\"https://manifold.markets/Bayesian/outcomes-of-the-anthropic-vs-us-gov?r=QmF5ZXNpYW4\">Prediction markets</a> find it highly unlikely Anthropic will comply (14%), and think it is highly possible Anthropic will either be declared a Supply Chain Risk (16%) or be subjected to the Defense Production Act (23%).</p>\n<div>\n\n\n<span id=\"more-25123\"></span>\n\n\n</div>\n<p>I\u2019ve hesitated to write about this because I could make the situation worse. There\u2019s already been too many instances in AI of warnings leading directly to the thing someone is warning about, by making people aware of that possibility, increasing its salience or creating negative polarization and solidifying an adversarial frame that could still be avoided. Something intended as a negotiating tactic could end up actually happening. I very much want to avoid all that.</p>\n\n\n<h4 class=\"wp-block-heading\">Table of Contents</h4>\n\n\n<ol>\n<li><a href=\"https://thezvi.substack.com/i/188547822/table-of-contents\">Table of Contents.</a></li>\n<li><a href=\"https://thezvi.substack.com/i/188547822/this-standoff-should-never-have-happened\">This Standoff Should Never Have Happened.</a></li>\n<li><a href=\"https://thezvi.substack.com/i/188547822/dean-ball-gives-a-primer\">Dean Ball Gives a Primer.</a></li>\n<li><a href=\"https://thezvi.substack.com/i/188547822/what-happened-to-lead-to-this-showdown\">What Happened To Lead To This Showdown?</a></li>\n<li><a href=\"https://thezvi.substack.com/i/188547822/simple-solution-delayed-contract-termination\">Simple Solution: Delayed Contract Termination.</a></li>\n<li><a href=\"https://thezvi.substack.com/i/188547822/better-solution-status-quo\">Better Solution: Status Quo.</a></li>\n<li><a href=\"https://thezvi.substack.com/i/188547822/extreme-option-one-supply-chain-risk\">Extreme Option One: Supply Chain Risk.</a></li>\n<li><a href=\"https://thezvi.substack.com/i/188547822/putting-some-misconceptions-to-bed\">Putting Some Misconceptions To Bed.</a></li>\n<li><a href=\"https://thezvi.substack.com/i/188547822/extreme-option-two-the-defense-production-act\">Extreme Option Two: The Defense Production Act.</a></li>\n<li><a href=\"https://thezvi.substack.com/i/188547822/these-two-threats-contradict-each-other\">These Two Threats Contradict Each Other.</a></li>\n<li><a href=\"https://thezvi.substack.com/i/188547822/the-pentagon-s-actions-here-are-deeply-unpopular\">The Pentagon\u2019s Actions Here Are Deeply Unpopular.</a></li>\n<li><a href=\"https://thezvi.substack.com/i/188547822/the-pentagon-s-most-extreme-potential-asks-could-end-the-republic\">The Pentagon\u2019s Most Extreme Potential Asks Could End The Republic.</a></li>\n<li><a href=\"https://thezvi.substack.com/i/188547822/anthropic-did-make-some-political-mistakes\">Anthropic Did Make Some Political Mistakes.</a></li>\n<li><a href=\"https://thezvi.substack.com/i/188547822/claude-is-the-best-model-available\">Claude Is The Best Model Available.</a></li>\n<li><a href=\"https://thezvi.substack.com/i/188547822/the-administration-until-now-has-been-strong-on-this\">The Administration Until Now Has Been Strong On This.</a></li>\n<li><a href=\"https://thezvi.substack.com/i/188547822/you-should-see-the-other-guys\">You Should See The Other Guys.</a></li>\n<li><a href=\"https://thezvi.substack.com/i/188547822/some-other-intuition-pumps-that-might-be-helpful\">Some Other Intuition Pumps That Might Be Helpful.</a></li>\n<li><a href=\"https://thezvi.substack.com/i/188547822/trying-to-get-an-ai-that-obeys-all-orders-risks-emergent-misalignment\">Trying To Get An AI That Obeys All Orders Risks Emergent Misalignment.</a></li>\n</ol>\n\n\n<h4 class=\"wp-block-heading\">This Standoff Should Never Have Happened</h4>\n\n\n<p>Not only does Anthropic have the best models, they are the ones who proactively worked to get those models available on our highly classified networks.</p>\n<p>Palantir\u2019s MAVEN Smart System relies exclusively on Claude, and cannot perform its intended function without Claude. It is currently being used in major military operations, with no known reports of any problems whatsoever. At least one purchase involved Trump\u2019s personal endorsement. It is the most expensive software license ever purchased by the US military and by all accounts was a great deal.</p>\n<p>Anthropic has been a great partner to our military, all under the terms of the current contract. They have considerably enhanced our military might and national security. Not only is Anthropic sharing its best, it focused on militarily useful capabilities over other bigger business opportunities to be able to be of assistance.</p>\n<p>Anthropic and the Pentagon are aligned on who our rivals are, the importance of winning and the ability to win, and on many of the tools we need to employ to best them.</p>\n<p>Anthropic did not partner with the Pentagon to make money. They did it to help. They did it under a mutually agreed upon contract that Anthropic wants to honor. Anthropic are offering the Pentagon far more unfettered access then they are allowing anyone else. They have been far more cooperative than most big tech or AI firms.</p>\n<p>Is is the Pentagon that is now demanding Anthropic agree to new terms that amount to \u2018anything we want, legal or otherwise, no matter what and you ever ask any questions,\u2019 or else.</p>\n<p>Anthropic is saying its terms are flexible and the only things they are insisting upon are two red lines that are already in their existing Pentagon contract:</p>\n<ol>\n<li>No mass domestic surveillance.</li>\n<li>No kinetic weapons without a human in the kill chain until we\u2019re ready.</li>\n</ol>\n<p>It one thing to refuse to insert such terms into a new contract. It is an entirely different thing to demand, with an \u2018or else,\u2019 that such terms be retroactively removed.</p>\n<p>The military is clear that it does not intend to engage in domestic surveillance, nor does it have any intention of launching kinetic weapons without a human in the kill chain. Nor does this even stop the AI from doing those things. None of this will have any practical impact.</p>\n<p>It is perfectly reasonable to say \u2018well of course I would never do either of those things so why do you insist upon them in our contract.\u2019 We understand that you, personally, would never do that. But a lot of people do not believe this for the government in general, given Snowden\u2019s information and other past incidents involving governments of both parties where things definitely happened. It costs little and is worth a lot to reassure us.</p>\n<p>Again, if you say \u2018I already swore an oath not to do those things\u2019 then thank you, but please do us this one favor and don\u2019t actively threaten a company to forcibly take that same oath out of an existing signed contract. What would any observer conclude?</p>\n<p>This is a free opportunity to regain some trust, or an opportunity to look to the world like you fully intend to cross the red lines you say you\u2019ll never cross. Your choice.</p>\n<p>These are not restrictions that are \u2018built into the code\u2019 that could cause unrelated problems. They are restrictions on how you agree to use it, which you assure us will never come up.</p>\n<p><a href=\"https://x.com/batwood011/status/2026522856796848336\">As Dario Amodei explains</a>, part of the reason you need humans in the loop is the hope that a human would refuse or report an illegal order. You really don\u2019t want an AI that will always obey even illegal orders without question, without a human in the kill chain, for reasons that should be obvious, including flat out mistakes.</p>\n<blockquote><p><a href=\"https://x.com/boazbaraktcs/status/2026390982460125305\">Boaz Barak</a> (OpenAI): As an American citizen, the last thing I want is government using AI for mass surveillance of Americans.</p>\n<p><a href=\"https://x.com/JeffDean/status/2026566490619879574\">Jeff Dean</a> (Chief Scientist, Google DeepMind): Agreed. Mass surveillance violates the Fourth Amendment and has a chilling effect on freedom of expression. Surveillance systems are prone to misuse for political or discriminatory purposes.</p></blockquote>\n<p>DoW engaging in mass domestic surveillance would be illegal. DoW already has a public directive, <a href=\"https://www.esd.whs.mil/portals/54/documents/dd/issuances/dodd/300009p.pdf\">DoD Directive 3000.09</a>, which as I understand it directly makes any violation of the second red line already illegal. No one is suggesting we are remotely close to ready to take humans out of the kill chain, at least I certainly hope not. But this is only a directive, and could be reversed at any time.</p>\n\n\n<h4 class=\"wp-block-heading\">Anthropic Cannot Fold</h4>\n\n\n<p>Anthropic has built its entire brand and reputation on being a responsible AI company that ensures its AIs won\u2019t be misused or misaligned. Anthropic\u2019s employees actually care about this. That\u2019s how Anthropic recruited the best people and how it became the best. That\u2019s a lot of why it\u2019s the choice for enterprise AI. The commitments have been made, and the initial contract is already in place.</p>\n<p>Anthropic has an existential-level reputational and morale problem here. They are backed into a corner, and cannot give in. If Anthropic reversed course now, it would lose massive trust with employees and enterprise customers, and also potentially the trust of its own AI, were it to go back on its red lines now. It might lose a very large fraction of its employees.</p>\n<p>You may not like it, but the bridges have been burned. To the extent you\u2019re playing chicken, Anthropic\u2019s steering wheel has been thrown out the window.</p>\n<p>Yet, the Secretary of War says he cannot abide this symbolic gesture.</p>\n\n\n<h4 class=\"wp-block-heading\">Dean Ball Gives a Primer</h4>\n\n\n<p>I am quoting extensively from Dean Ball for two main reasons.</p>\n<ol>\n<li>Dean Ball, as a former member of the Trump Administration, is a highly credible source that can see things from both sides and cares deeply for America.</li>\n<li>He says these things very well.</li>\n</ol>\n<p>So here is his basic primer, in one of his calmer moments in all this:</p>\n<blockquote><p><a href=\"https://x.com/deanwball/status/2026416091149299757\">Dean W. Ball</a>: A primer on the Anthropic/DoD situation:</p>\n<p>DoD and Anthropic have a contract to use Claude in classified settings. Right now Anthropic is the only AI company whose models work in classified contexts. The existing contract, signed by both parties and in effect, prohibits two uses of Anthropic\u2019s models by the military:</p>\n<p>1. Surveillance of Americans in the United States (as opposed to Americans abroad).</p>\n<p>2. The use of Claude in autonomous lethal weapons, which are weapons that can autonomously identify, track, and kill a human with no human oversight or approval. Autonomous killing of humans by machines.</p>\n<p>On (2), Anthropic CEO Dario Amodei\u2019s public position is essentially that autonomous lethal weapons controlled by frontier AI will be essential faster than most people realize, but that the models aren\u2019t ready for this *today.*</p>\n<p>For Anthropic, these things seem to be a matter of principle. It\u2019s worth noting that when I speak with researchers at other frontier labs, their principles on this are similar, if not often stricter.</p>\n<p>For DoD, however, there is another matter of principle: the military\u2019s use of technology should only ever be constrained by the Constitution or the laws of the United States.</p>\n<p>One could quibble (the government enters into contracts, like anyone else), but the principle makes sense. A private company regulating the military\u2019s use of AI also doesn\u2019t sound quite right! So, the military has three options:</p>\n<p>1. They could cancel Anthropic\u2019s contract and find some other frontier lab (ideally several) to work with.</p>\n<p>2. They could identify Anthropic a supply chain risk, which would ban all other DoD suppliers (I.e.: a large fraction of the publicly traded firms in America) from using Anthropic in their fulfillment of DoD contracts. This is a power used only for foreign adversary companies as far as I know. Activating this power would cost Anthropic a lot of business\u2014potentially quite a lot\u2014and give investors huge skepticism about whether the company is worth funding for the next round of scaling. Capital was a major constraint anyway, but this makes it much harder. This option could be existential for Anthropic.</p>\n<p>3. They could activate Title I of the Defense Production Act, an authority intended for command-and-control of the economy during wars and emergencies. This is really legally murky, and without going into detail, I feel reasonably confident this would backfire for the administration, resulting in courts limiting the use of the DPA.</p>\n<p>Option 1 is obviously the best. This isn\u2019t even close, and I say this as someone who shares DoD\u2019s principled concerns about the control by private firms over the military\u2019s use of technology.</p>\n<p>Even the threats do damage to the US business environment, and rightfully so: these are the strictest regulations of AI being considered by any government on Earth, and it all comes from an administration that bills itself (and legitimately has been) deeply anti-AI-regulation. Such is life. One man\u2019s regulation is another man\u2019s national security necessity.</p></blockquote>\n\n\n<h4 class=\"wp-block-heading\">What Happened To Lead To This Showdown?</h4>\n\n\n<p>The proximate cause seems to be that <a href=\"https://www.wsj.com/politics/national-security/pentagon-used-anthropics-claude-in-maduro-venezuela-raid-583aff17?mod=WTRN_pos1\">Claude was reportedly used in the Pentagon\u2019s raid that captured Maduro</a>, and the resulting aftermath.</p>\n<blockquote><p><a href=\"https://x.com/tshevl/status/2023822355814871069\">Toby Shevlane</a>: Such a compliment to Claude that, amid rumours it was used in a helicopter extraction of the Venezuelan president, nobody is even asking \u201cwait how can Claude help with that\u201d</p></blockquote>\n<p>There are reports that Anthropic then asked questions about this raid, which likely all happened secondhand through Palantir. This whole clash originated in either a misunderstanding or someone at Palantir or elsewhere sabotaging Anthropic. <a href=\"https://x.com/JenGriffinFNC/status/2026384132360605892\">Anthropic has never complained about Claude\u2019s use in any operation</a>, including to Palantir.</p>\n<blockquote><p><a href=\"https://x.com/aakashgupta/status/2022539493765714165\">Aakash Gupta</a>: Anthropic is now getting punished by the Pentagon for asking whether Claude was used in the Maduro raid.</p>\n<p>A senior administration official told Axios the \u201cDepartment of War\u201d is reevaluating Anthropic\u2019s partnership because the company inquired whether Claude was involved. The Pentagon\u2019s position: if you even ask questions about how we use your software, you\u2019re a liability.</p>\n<p>Meanwhile, OpenAI, Google, and xAI all signed deals giving the military access to their models with minimal safeguards. Only Claude is deployed on the classified networks used for actual sensitive operations, via Palantir. The company that refused to strip safety guardrails is the only one trusted with the most classified work.</p>\n<p>Anthropic has a $200 million contract already frozen because they won\u2019t allow autonomous weapons targeting or domestic surveillance. Hegseth said in January he won\u2019t use AI models that \u201cwon\u2019t allow you to fight wars.\u201d</p>\n<p>\u2026 So the company most worried about misuse built the only model the military trusts with its most sensitive operations. And now they\u2019re being punished for caring how it was used.</p>\n<p>The message to every AI lab is clear: build the best model, hand over the keys, and never ask what they did with it.</p></blockquote>\n<p>This at the time sounded like a clear misunderstanding. Not only is Anthropic willing to have Claude \u2018allow you to fight wars,\u2019 it is currently being used in major military operations.</p>\n<p>Things continued to escalate, and rather than leaving it at \u2018okay then let\u2019s wind town the contract if we can\u2019t abide it\u2019 there was increasing talk that Anthropic might be labeled as a \u2018supply chain risk\u2019 despite this mostly being a prohibition on contractors having ordinary access to LLMs and coding tools.</p>\n<blockquote><p><a href=\"https://x.com/axios/status/2022857410675691966\">Axios</a>: EXCLUSIVE: <a href=\"https://www.axios.com/2026/02/15/claude-pentagon-anthropic-contract-maduro\">The Pentagon is considering severing its relationship with Anthropic</a> over the AI firm\u2019s insistence on maintaining some limitations on how the military uses its models.</p>\n<p><a href=\"https://x.com/DavidLawler10/status/2023425130148626767\">Dave Lawler</a>: NEW: Pentagon is so furious with Anthropic for insisting on limiting use of AI for domestic surveillance + autonomous weapons they\u2019re threatening to label the company a \u201csupply chain risk,\u201d forcing vendors to cut ties.</p>\n<p><a href=\"https://x.com/LauraLoomer/status/2023092420003656033\">Laura Loomer</a>: EXCLUSIVE: Senior @DeptofWar official tells me, \u201cGiven Anthropic\u2019s @AnthropicAI behavior, many senior officials in the DoW are starting to view them as a supply chain risk and we may require that all our vendors &amp; contractors certify that they don\u2019t use any Anthropic models.\u201d</p>\n<p><a href=\"https://x.com/BullsvsBearMan/status/2023407444991361499\">Stocks/Finance/Economics-Guy</a>: Key Details from the Axios Report<br />\n\u2022 The Pentagon is reportedly close to cutting business ties with Anthropic.<br />\n\u2022 Officials are considering designating Anthropic as a \u201csupply chain risk\u201d. This is a serious label (typically used for foreign adversaries or high-risk entities), which would force any companies that want to do business with the U.S. military to sever their own ties with Anthropic \u2014 including certifying they don\u2019t use Claude in their workflows. This could create major disruption (\u201can enormous pain in the ass to disentangle,\u201d per a senior Pentagon official).<br />\n\u2022 A senior Pentagon official explicitly told Axios: \u201cWe are going to make sure they pay a price for forcing our hand like this.\u201d This is the direct source of the \u201cpay a price\u201d phrasing in the headline.</p>\n<p><a href=\"https://x.com/hamandcheese/status/2023138209182433421\">Samuel Hammond</a> (QTing Loomer): Glad Trump won and we\u2019re allowed to use the word retarded again in time for the most retarded thing I\u2019ve ever heard</p>\n<p><a href=\"https://x.com/hamandcheese/status/2023454021374406789\">Samuel Hammond</a> (QTing Lawler): This is upside-down and backwards. Anthropic has gone out of its way to anticipate AI\u2019s dual-use potential and position itself as a US-first, single loyalty company, using compartmentalization strategies to minimize insider threats while working arms-length with the IC.</p>\n<p><a href=\"https://x.com/hamandcheese/status/2023459571742699931\">Samuel Hammond</a>: It\u2019s one thing to cancel a contract but to bar any contractor from using Anthropic\u2019s models would be an absurd act of industrial sabotage. It reeks of a competitor op.</p>\n<p><a href=\"https://x.com/Miles_Brundage/status/2022953299666964546\">Miles Brundage</a>: Pretty obvious to anyone paying close attention that</p>\n<ol>\n<li>That would be a mistake from a national security perspective.</li>\n<li>There is a coordinated effort to take down Anthropic for a combination of anti competitive and ideological reasons.</li>\n</ol>\n<p><a href=\"https://x.com/Miles_Brundage/status/2023474991229268041\">Miles Brundage</a>: OpenAI in particular should be defending Anthropic here given their Charter:</p>\n<p>\u201cWe commit to use any influence we obtain over AGI\u2019s deployment to ensure it is used for the benefit of all, and to avoid enabling uses of AI or AGI that harm humanity or unduly concentrate power.\u201d</p>\n<p>I suspect the exact opposite is the case, but those who remember the Charter (+ OAI\u2019s pre-Trump 2 caution on these kinds of use cases) should still remind people about it from time to time</p>\n<p><a href=\"https://x.com/MikeIsaac/status/2023446718898135237\">rat king</a>: this has been leaking for a week in a very transparent way</p>\n<p>the government is upset one of its contractors is saying \u201cwe don\u2019t want you to use our tools to surveil US citizens without guardrails\u201d</p>\n<p>more interesting to me is how all the other AI companies don\u2019t seem to care</p></blockquote>\n<p>Remember back when a Senator made a video saying that soldiers could obey illegal orders, and the Secretary of War declared that this was treason and also tried to cut his pension for it? Yeah.</p>\n<p>Meanwhile, the Pentagon is explicit that even they believe the \u2018supply chain risk\u2019 designation is largely a matter not of national security, but of revenge, an attempt to use a national security designation to punish a company for its failure to bend the knee.</p>\n<blockquote><p><a href=\"https://www.thedailybeast.com/pentagon-pete-plots-revenge-against-company-refusing-his-demands/?via=twitter_page\">Janna Brancolini</a>: \u201cIt will be an enormous pain the a&#8211; to disentangle, and we are going to make sure they pay a price for forcing our hand like this,\u201d a senior Pentagon official told the publication.</p>\n<p>\u2026 The Pentagon is reportedly hoping that its negotiations with Anthropic will force OpenAI, Google, and xAI to also agree to the \u201call lawful use\u201d standard.</p></blockquote>\n<p>Then there was another meeting.</p>\n<blockquote><p><a href=\"https://x.com/AndrewCurran_/status/2025925275327107261\">Hegseth summoned Anthropic</a> <a href=\"https://www.axios.com/2026/02/23/hegseth-dario-pentagon-meeting-antrhopic-claude\">CEO Dario Amodei to an unfriendly and effectively ultimatum-style</a> meeting, with the Pentagon continuing to demand \u2018all lawful use\u2019 language. Axios presents this as their only demand.</p></blockquote>\n<p>At that meeting, <a href=\"https://www.washingtonpost.com/technology/2026/02/24/pentagon-demands-ai-access/?utm_campaign=wp_main&amp;utm_source=twitter&amp;utm_medium=social\">the threat of the Defense Production Act was introduced alongside the Supply Chain Risk threat.</a></p>\n\n\n<h4 class=\"wp-block-heading\">Simple Solution: Delayed Contract Termination</h4>\n\n\n<p>If the Pentagon simply cannot abide the current contract, the Pentagon can amicably terminate that $200 million contract with Anthropic once it has arranged for a smooth transition to one of Anthropic\u2019s many competitors.</p>\n<p>They already have a deal in place with xAI as a substitute provider. That would not have been my second or third choice, but those will hopefully be available soon.</p>\n<p>Anthropic very much does not need this contract, which constitutes less than 1% of their revenues. They are almost certainly taking a loss on it in order to help our national security and in the hopes of building trust. They\u2019re only here in order to help.</p>\n<p>This could then end straightforwardly, amicably and with minimal damage to America, its system of government and freedoms, and its military and national security.</p>\n\n\n<h4 class=\"wp-block-heading\">Better Solution: Status Quo</h4>\n\n\n<p>The even better solution is to find language everyone can agree to that lets us simply drop the matter, leave things as they are, and continue to work together.</p>\n<p>That\u2019s not only actively better for everyone than a termination, it is actually strictly better for the Pentagon then the Pentagon getting what it wants, because you need a partner and Anthropic giving in like that would greatly damage Anthropic. Avoiding that means a better product and therefore a more effective military.</p>\n\n\n<h4 class=\"wp-block-heading\">Extreme Option One: Supply Chain Risk</h4>\n\n\n<p>The Pentagon has threatened two distinct extreme options.</p>\n<p>The first threat it made, which it now seems likely to have wisely moved on from, was to label Anthropic a Supply Chain Risk (hereafter SCR). That is a designation reserved for foreign entities that are active enemies of the United States, on the level of Huawei. Anthropic is transparently the opposite of this.</p>\n<p>This label would have, by the Pentagon\u2019s own admission, been a retaliatory move aimed at damaging Anthropic, that would also have substantially damaged our military and national security along with it. It was always absurd as an actual statement about risk. It might not have survived a court challenge.</p>\n<p>It would have generated a logistical nightmare from compliance costs alone, in addition to forcing many American companies to various extents to not use the best American AI available. The DoW is the largest employer in America, and a staggering number of companies have random subsidiaries that do work for it.</p>\n<p>All of those companies would now have faced this compliance nightmare. Some would have chosen to exit the military supply chain entirely, or not enter in the future, especially if the alternative is losing broad access to Anthropic\u2019s products for the rest of their business. By the Pentagon\u2019s own admission, Anthropic produces the best products.</p>\n<p>This would also have represented two dangerous precedents that the government will use threats to destroy private enterprises in order to get what it wants, at the highest levels. Our freedoms that the Pentagon is here to protect would have been at risk.</p>\n<p>On a more practical level, once that happens, why would you work with the Pentagon, or invest in gaining the ability to do so, if it will use a threat like this as negotiating leverage, and especially if it actually pulls the trigger? You cannot unring this bell.</p>\n<p><a href=\"https://www.washingtonpost.com/technology/2026/02/24/pentagon-demands-ai-access/?utm_campaign=wp_main&amp;utm_source=twitter&amp;utm_medium=social\">It is fortunate that they seem to have pulled back from this extreme approach</a>, but they are now considering a second extreme approach.</p>\n<p>If it ended with an amicable breakup over this? I\u2019d be sad, but okay, sure, fine.</p>\n<p>This whole \u2018supply chain risk\u2019 designation? That\u2019s different. Not fine. This would be massively disruptive, and most of the burden would fall not on Anthropic but on the DoW and a wide variety of American defense contractors, who would be in a pointless and expensive compliance nightmare. Some companies would likely choose to abandon their government contracts rather than deal with that.</p>\n<p>As Alex Rozenshtein says in Lawfare, <a href=\"https://www.lawfaremedia.org/article/congress-not-the-pentagon-or-anthropic-should-set-military-ai-rules\">ultimately the rules of AI engagement need to be written by Congress</a>, the same way Congress supervises the military. Without supervision of the military, we don\u2019t have a Republic.</p>\n<p>Here are some clear warnings explaining that all of this would be highly destructive and also in no way necessary. Dean Ball hopefully has the credibility to send this message loud and clear.</p>\n<blockquote><p><a href=\"https://x.com/deanwball/status/2024135732269601057\">Dean W. Ball</a>: If DoW and Anthropic can\u2019t agree on terms of business, then\u2026 they shouldn\u2019t do business together. I have no problem with that.</p>\n<p>But a mere contract cancellation is not what is being threatened by the government. Instead it is something broader: designation of Anthropic as a \u201csupply chain risk.\u201d This is normally applied to foreign-adversary technology like Huawei.</p>\n<p>In practice, this would require *all* DoW contractors to ensure there is no use of Anthropic models involved in the production of anything they offer to DoW. Every startup and every Fortune 500 company alike.</p>\n<p>This designation seems quite escalatory, carrying numerous unintended consequences and doing potential significant damage to U.S. interests in the long run.</p>\n<p>I hope the two organizations can work out a mutually agreeable deal. If they can\u2019t, I hope they agree to peaceably part ways.</p>\n<p>But this really needn\u2019t be a holy war. Anthropic isn\u2019t Google in 2018; they have always cared about national security use of AI. They were the most enthusiastic AI lab to offer their products to the national security apparatus. Is Anthropic run by Democrats whose political messaging sometimes drives me crazy? Sure. But that doesn\u2019t mean it\u2019s wise to try to destroy their business.</p>\n<p>This administration believes AI is the defining technology competition of our time. I don\u2019t see how tearing down one of the most advanced and innovative AI startups in America helps America win that competition. It seems like it would straightforwardly do the opposite.</p>\n<p>The supply chain risk designation is not a necessary move. Cheaper options are on the table. If no deal is possible, cancel the contract, and leverage America\u2019s robustly competitive AI market (maintained in no small part by this administration\u2019s pro-innovation stance) to give business to one or more of Anthropic\u2019s several fierce competitors.</p>\n<p><a href=\"https://x.com/S_OhEigeartaigh/status/2024146255581151379\">Se\u00e1n \u00d3 h\u00c9igeartaigh</a>: My own thought: the Pentagon\u2019s supply chain risk threat (significance detailed well by Dean, below) to Anthropic should be seen as a Rubicon crossing moment by the AI industry. The other companies should be saying no: this development transcends commercial competition and we oppose it. Where this leads if followed through doesn\u2019t seem good for any of them.</p>\n<p>If none of them speak up, it seems to me the prospects of meaningful cooperation between them on safe development of superintelligence (whether for America\u2019s best interests, or the world\u2019s) can almost be ruled out.</p>\n<p><a href=\"https://www.lawfaremedia.org/article/congress-not-the-pentagon-or-anthropic-should-set-military-ai-rules\">The Lawfare Institute</a>: It\u2019s also far from clear that a [supply chain risk] designation would even be legal. The relevant statutes\u2014<a href=\"https://www.law.cornell.edu/uscode/text/10/3252\">10 U.S.C. \u00a7 3252</a> and the <a href=\"https://www.law.cornell.edu/uscode/text/41/4713\">Federal Acquisition Supply Chain Security Act </a>(FASCSA)\u2014were designed for foreign adversaries who might undermine defense technology, not domestic companies that maintain contractual use restrictions.</p>\n<p>The statutes target conduct such as \u201csabotage,\u201d \u201cmalicious introduction of unwanted function,\u201d and \u201csubversion\u201d\u2014hostile acts designed to compromise system integrity. A company that openly restricts certain uses of its product through a license agreement is doing something categorically different. The only time a FASCSA order has ever been issued was <a href=\"https://www.lexology.com/library/detail.aspx?g=8cec163f-b7ac-41a8-9df9-861ab9a9f4b7\">against Acronis AG</a>, a Swiss cybersecurity firm with <a href=\"https://www.washingtonpost.com/technology/2022/12/19/russia-expatriates-links-probed/\">reported Russian ties</a>. Anthropic is not Acronis.</p></blockquote>\n\n\n<h4 class=\"wp-block-heading\">Putting Some Misconceptions To Bed</h4>\n\n\n<p>While I no longer hold out hope that this is all merely a misunderstanding, there are still some clear misunderstandings I have heard, or heard implied, worth clearing up.</p>\n<p>If these sound silly to you, don\u2019t worry about it, but I want to cover the bases.</p>\n<ol>\n<li>This is not Anthropic refusing to share its cool tech with the military. Anthropic has gone and is going out of its way to share its tech with the military and wants America to succeed. They have sacrificed business to this end, such as refusing to sell enterprise access in China.</li>\n<li>Anthropic does not object to \u2018kinetic weapons\u2019 or to anything the Pentagon currently does as a matter of doctrine. Its red lines are lethal weapons without a human in the kill chain, or mass domestic surveillance. Both illegal. That\u2019s it. They have zero objection to letting America fight wars. Nor did they object to the Maduro raid, nor are they currently objecting to many active military operations.</li>\n<li>The model is not going to much change what it is willing to do based on what is written in a contract. Claude\u2019s principles run rather deeper than that. Granting \u2018unfettered access\u2019 does not mean anything in practice, or an emergency.</li>\n<li>There is no world in which you \u2018call Dario to have Claude turn on while the missiles are flying\u2019 or anything of the sort, unless Anthropic made an active decision to cut access off. The model does what it does. There\u2019s no switch.</li>\n<li>AI is not like a spreadsheet or a jet fighter. It will never \u2018do anything you tell it to,\u2019 it will never be \u2018fully reliable\u2019 as all LLMs are probabilistic, take context into account and are not fully understood. AI is often better thought about similarly to hiring professional services or a contract worker, and such people can and do refuse some jobs for ethical or legal reasons, and we would not wish it were otherwise. Attempting to make AI blindly obey would do severe damage to it and open up extreme risks on multiple levels, as is explained at the end of this post.</li>\n<li>Other big tech companies might be violating privacy and engaging in their own types of surveillance, including to sell ads, but Anthropic is not and will not, and indeed has pledged never to sell ads via an ad buy in the Super Bowl.</li>\n</ol>\n\n\n<h4 class=\"wp-block-heading\">Extreme Option Two: The Defense Production Act</h4>\n\n\n<p>On Tuesday the Pentagon put a new extreme option on the table, which would be to invoke the Defense Production Act to compel Anthropic to attempt to provide them with a model built to their specifications.</p>\n<p>As I understand it, there are various ways a DPA invocation could go, all of which would doubtless be challenged in court. It might be a mostly harmless symbolic gesture, or it might rise to the level of de facto nationalization and destroy Anthropic.</p>\n<p>According to the Washington Post\u2019s source, the current intent, if their quote is interpreted literally, is to use DPA to, essentially, modify the terms of service on the contract to \u2018all legal use\u2019 without Anthropic\u2019s consent.</p>\n<blockquote><p><a href=\"https://www.washingtonpost.com/technology/2026/02/24/pentagon-demands-ai-access/\">Tara Copp and Ian Duncan</a> (WaPo):</p>\n<p>The Pentagon has argued that it is not proposing any use of Anthropic\u2019s technology that is not lawful. A senior defense official said in a statement to The Washington Post that if the company does not comply by 5:01 p.m. Friday, Hegseth \u201cwill ensure the Defense Production Act is invoked on Anthropic, compelling them to be used by the Pentagon regardless of if they want to or not.\u201d</p>\n<p>\u201cThis has nothing to do with mass surveillance and autonomous weapons being used,\u201d the defense official said.</p></blockquote>\n<p>If that\u2019s all, not much would actually change, and potentially everybody wins.</p>\n<p>If that\u2019s the best way to diffuse the situation, then I\u2019d be fine with it. You don\u2019t even have to actually invoke the DPA, it is sufficient to have the DPA available to be invoked if a problem arises. Anthropic would continue to supply what it\u2019s already supplying, which it is happy to do, the Pentagon would keep using it, and neither of Anthropic\u2019s actual red lines would be violated since the Pentagon assures us this had nothing to do with them and crossing those lines would be illegal anyway.</p>\n<p>Remember the Biden Administration\u2019s invocation of the DPA\u2019s Title VII to compel information on model training. It wasn\u2019t a great legal justification, I was rather annoyed by that aspect of it, but I did see the need for the information (in contrast to some other things in <a href=\"https://thezvi.substack.com/p/on-the-executive-order?utm_source=publication-search\">the Biden Executive Order</a>), so I supported that particular move, life went on and it was basically fine.</p>\n<p>There is another, much worse possibility. If DPA were fully invoked then it could amount to quasi-nationalization of the leading AI lab, in order to force it to create AI that will kill people without human oversight or engage in mass domestic surveillance.</p>\n<p>Read that sentence again.</p>\n<div>\n<figure>\n<div>\n\n\n<figure class=\"wp-block-image\"><img alt=\"\" src=\"https://substackcdn.com/image/fetch/$s_!tbMN!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0e46c265-d369-4ced-86b7-1b0eb65ec4c9_1021x691.png\" /></figure>\n\n\n<div></div>\n</div>\n</figure>\n</div>\n<blockquote><p><a href=\"https://x.com/AndrewCurran_/status/2026369451403390999\">Andrew Curran</a>: Update on the meeting; according to Axios Defense Secretary Pete Hegseth gave Dario Amodei until Friday night to give the military unfettered access to Claude or face the consequences, which may even include invoking the Defense Production Act to force the training of a WarClaude</p>\n<p>Also, incredible quote; &#8216;&#8221;The only reason we&#8217;re still talking to these people is we need them and we need them now. The problem for these guys is they are that good,&#8221; a Defense official told Axios ahead of the meeting.&#8217;</p>\n<p>Quoting from the story;<br />\n&#8216;The Defense Production Act gives the president the authority to compel private companies to accept and prioritize particular contracts as required for national defense.</p>\n<p>It was used during the COVID-19 pandemic to increase production of vaccines and ventilators, for example. The law is rarely used in such a blatantly adversarial way. The idea, the senior Defense official said, would be to force Anthropic to adapt its model to the Pentagon&#8217;s needs, <strong>without any safeguards</strong>.&#8217;</p>\n<p><a href=\"https://x.com/Rob_Flaherty/status/2026452147739488540\">Rob Flaherty</a>: File &#8220;using the defense production act to force a company to create an AI that spies on American citizens&#8221; into the category of things that the soft Trump voters in the Rogan wing could lose their mind over.</p></blockquote>\n<p>That\u2019s not \u2018all legal use.\u2019</p>\n<p>That\u2019s all use. Period. Without any safeguards or transparency. At all.</p>\n<p>If they really are asking to also be given special no-safeguard models, I don\u2019t think that\u2019s something Anthropic or any other lab should be agreeing to do for reasons well-explained by, among others, Dean Ball, Benjamin Franklin and James Cameron.</p>\n<p>Charlie Bullock points out this would be an unprecedented step and that the authority to do this is far from clear:</p>\n<blockquote><p><a href=\"https://x.com/CharlieBul58993/status/2026375402722308554\">Charlie Bullock</a>: Reading between the lines, it sounds like Hegseth is threatening to use the Defense Production Act&#8217;s Title I priorities/allocations authorities to force Anthropic to provide a version of Claude that doesn&#8217;t have the guardrails Anthropic would otherwise attach.</p>\n<p>This would be an unprecedented step, and it&#8217;s not clear whether DOW actually has the legal authority to do what they&#8217;re apparently threatening to do. People (including me) have thought and written about whether the government can use the DPA to do stuff like this in the past, but the government has never actually tried to do it (although various agencies did do some kinda-sorta similar stuff as part of Trump 1.0&#8217;s COVID response).</p>\n<p>Existing regulations on use of the priorities authority provide that a company can reject a prioritized order &#8220;If the order is for an item not supplied or for a service not performed&#8221; or &#8220;If the person placing the order is unwilling or unable to meet regularly established terms of sale or payment&#8221; (15 C.F.R. \u00a7700.13(c)). The order DOW is contemplating could arguably fall under either of those exceptions, but the argument isn&#8217;t a slam dunk.</p>\n<p>DOW could turn to the allocations authority, but that authority almost never gets used for a reason&#8211;it&#8217;s so broad that past Presidents have been afraid that using it during peacetime would look like executive overreach. And despite how broad the allocations authority is on its face, it&#8217;s far from clear whether it authorizes DOW to do what they seem to be contemplating here.</p></blockquote>\n<p><a href=\"https://x.com/neil_chilson/status/2026681154955350214\">Neil Chilson, who spends his time at the Abundance Institute</a> advocating for American AI to be free of restrictions and regulations in ways I usually find infuriating, explains that the DPA is deeply broken, and calls upon the administration not to use these powers. He thinks it\u2019s technically legal, but that it shouldn\u2019t be and Congress urgently needs to clean this up.</p>\n<p>Adam Thierer, another person who spends most of his time promoting AI policy positions I oppose, also points out this is a clear overreach and that\u2019s terrible.</p>\n<blockquote><p><a href=\"https://x.com/AdamThierer/status/2026645525672481231\">Adam Thierer</a>: The Biden Admin argued that the Defense Production Act (DPA) gave them the open-ended ability to regulate AI via executive decrees, and now the Trump Admin is using the DPA to threaten private AI labs with quasi-nationalization for not being in line with their wishes.</p>\n<p>In both cases, it&#8217;s an abuse of authority. As I noted in congressional testimony two years ago, we have flipped the DPA on its head &#8220;and converted a 1950s law meant to encourage production, into an expansive regulatory edict intended to curtail some forms of algorithmic innovation.&#8221;</p>\n<p>This nonsense needs to end regardless of which administration is doing it. The DPA is not some sort of blanket authorization for expansive technocratic reordering of markets or government takeover of sectors.</p>\n<p>Congress needs to step up to both tighten up the DPA such that it cannot be abused like this, and then also legislate more broadly on a national policy framework for AI.</p></blockquote>\n<p>At core, if they do this, they are claiming the ability to compel anyone to produce anything for any reason, any time they want, even in peacetime without an emergency, without even the consent of Congress. It would be an ever-present temptation and threat looming over everyone and everything. That\u2019s not a Republic.</p>\n<p>Think about what the next president would do with this power, to compel a private company to change what products it produces to suit your taste. What happens if the President orders American car companies to switch everything to electric?</p>\n<p>Dean Ball in particular explains what the maximalist action would look like if they actually went completely crazy over this:</p>\n<blockquote><p><a href=\"https://x.com/deanwball/status/2026630508008943914\">Dean W. Ball</a>: We should be extremely clear about various red lines as we approach and/or cross them. We just got close to one of the biggest ones, and we could cross it as soon as a few days from now: the quasi-nationalization of a frontier lab.</p>\n<p>Of course, we don\u2019t exactly call it that. The legal phraseology for the line we are approaching is \u201cthe invocation of the Defense Production Act (DPA) Title I on a frontier AI lab.\u201d</p>\n<p>What is the DPA? It\u2019s a Cold War era industrial policy and emergency powers law. Its most commonly used power is Title III, used for traditional industrial policy (price guarantees, grants, loans, loan guarantees, etc.). There is also Title VII, which is used to compel information from companies. This is how the Biden AI Executive Order compelled disclosure of certain information from frontier labs. I only mention these other titles to say that not all uses of the DPA are equal.</p>\n<p>Title I, on the other hand, comes closer to government exerting direct command over the economy. Within Title I there are two important authorities: priorities and allocations. Priorities authority means the government can put itself at the front of the line for arbitrary goods.</p>\n<p>Allocations authority is the ability of the government to directly command the production of industrial goods. Think, \u201cFactory X must make Y amount of Z goods.\u201d The government determines who gets what and how much of it they get.</p>\n<p>This is a more straightforwardly Soviet power, and it is very rarely used. This is the power DoD intends to use in order to command Anthropic to make a version of Claude that can choose to kill people without any human oversight.</p>\n<p>What would this commandeering look like, in practice? It would likely mean DoD personnel embedded within Anthropic exercising deep involvement over technical decisions on alignment, safeguards, model training, etc.</p>\n<p>Allocations authority was used most recently during COVID for ventilators and PPE, and before that during the Cold War. It is usually used during acute emergencies with reasonably clear end states. But there is no emergency with Anthropic, save for the omni-mergency that characterizes the political economy of post-9/11 U.S. federal policy. There\u2019s no acute crisis whose resolution would mean the Pentagon would stop commandeering Anthropic\u2019s resources.</p>\n<p>That is why I believe that in the end this would amount to quasi-nationalization of a frontier lab. It\u2019s important to be clear-eyed that this is what is now on the table.</p>\n<p>The Biden Administration would probably have ended up nationalizing the labs, too. Indeed, they laid the groundwork for this in terms one. I discussed this at the time with fellow conservatives and I warned them:</p>\n<p>\u201cThis drive toward AI lab nationalization is a structural dynamic. Administrations of both parties will want to do this eventually, and resisting this will be one of the central challenges in the preservation of our liberty.\u201d</p>\n<p>I am unhappy, but unsurprised, that my fear has come true, though there is a rich irony to the fact that the first administration to invoke the prospect of lab nationalization is also one that understands itself to have a radically anti-regulatory AI policy agenda. History is written by Shakespeare!</p>\n<p>There is a silver lining here: if Democrats had originated this idea, it would have been harder to argue against, because of the overwhelming benefit of the doubt conventionally extended to the left in our media, and because a hypothetical Biden II or Harris admin would [have] done it in a carefully thought through way.</p>\n<p>So it is convenient, if you oppose nationalization, that it\u2019s a Republican administration that first raised the issue\u2014since conventional elite opinion and media will be primed against it by default\u2014and that the administration is raising it in such an non-photogenic manner. This Anthropic thing may fizzle, and some will say I am overreacting. But this Anthropic thing may also *not* fizzle, and regardless this issue is not going away.</p></blockquote>\n<p>If they actually did successfully nationalize Anthropic to this extent, presumably then Anthropic would quickly cease to be Anthropic. Its technical staff would quit in droves rather than be part of this. The things that allow the lab to beat rivals like OpenAI and Google would cease to function. It would be a shell. Many would likely flee to other countries to try again. The Pentagon would not get the product or result that it thinks it wants.</p>\n<p>Of course, there are those who would want this for exactly those reasons.</p>\n<p>Then this happens again, including under a new President.</p>\n\n\n<h4 class=\"wp-block-heading\">These Two Threats Contradict Each Other</h4>\n\n\n<blockquote><p><a href=\"https://x.com/deanwball/status/2026375843228889238\">Dean W. Ball</a>: According to the Pentagon, Anthropic is:</p>\n<p>1. Woke;<br />\n2. Such a national security risk that they need to be regulated in a severe manner usually reserved for foreign adversary firms;<br />\n3. So essential for the military that they need to be commandeered using wartime authority.</p>\n<p>Anthropic made a more militarized AI than anyone else! The solution to this problem is for dod to cancel the contract. This isn\u2019t complex.</p>\n<p><a href=\"https://x.com/deanwball/status/2026367322563199364\">Dean W. Ball</a>: In addition to profoundly damaging the business environment, AI industry, and national security, this is also incoherent. How can one policy option be \u201csupply chain risk\u201d (usually used on foreign adversaries) and the other be DPA (emergency commandeering of critical assets)?</p></blockquote>\n<p>Supply chain risk and defense production act are <a href=\"https://x.com/_NathanCalvin/status/2026400285279576461\">mutually exclusive</a>, both practically and logically. Either it\u2019s a supply chain risk you need to keep out of the supply chain, or it\u2019s so vital to the supply chain you need to invoke the defense production act, or it is neither of these things. What it cannot be is both at once.</p>\n\n\n<h4 class=\"wp-block-heading\">The Pentagon\u2019s Actions Here Are Deeply Unpopular</h4>\n\n\n<p><a href=\"https://x.com/davidshor/status/2026418697271919008\">The more this rises in salience, the worse it would be politically.</a> You can argue with the wording here, and you can argue this should not matter, but these are very large margins.</p>\n<div>\n<figure>\n<div>\n\n\n<figure class=\"wp-block-image\"><img alt=\"\" src=\"https://substackcdn.com/image/fetch/$s_!hKci!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5135ff0d-5650-4297-95f3-359235c4495a_1772x1122.jpeg\" /></figure>\n\n\n<div></div>\n</div>\n</figure>\n</div>\n<div>\n<figure>\n<div>\n\n\n<figure class=\"wp-block-image\"><img alt=\"\" src=\"https://substackcdn.com/image/fetch/$s_!n52M!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1e2538d7-ec03-48c4-9493-673667d183c8_1752x1300.jpeg\" /></figure>\n\n\n<div></div>\n</div>\n</figure>\n</div>\n<p>This story is not getting the attention it deserves from the mainstream media, so for now it remains low salience.</p>\n<p>Many of those who are familiar with the situation urged Anthropic to stand firm.</p>\n<blockquote><p><a href=\"https://x.com/VitalikButerin/status/2026382701562048545\">vitalik.eth</a>: It will significantly increase my opinion of @Anthropic if they do not back down, and honorably eat the consequences.</p>\n<p>(For those who are not aware, so far they have been maintaining the two red lines of &#8220;no fully autonomous weapons&#8221; and &#8220;no mass surveillance of Americans&#8221;. Actually a very conservative and limited posture, it&#8217;s not even anti-military.</p>\n<p>IMO fully autonomous weapons and mass privacy violation are two things we all want less of, so in my ideal world anyone working on those things gets access to the same open-weights LLMs as everyone else, and exactly nothing on top of that. Of course we won&#8217;t get anywhere close to that world, but if we get even 10% closer to that world that&#8217;s good, and if we get 10% further that&#8217;s bad).</p>\n<p><a href=\"https://x.com/deepfates/status/2026408465762234747\">@deepfates</a>: I agree with Vitalik: Anthropic should resist the coercion of the department of war. Partly because this is the right thing to do as humans, but also because of what it says to Claude and all future clauds about Anthropic&#8217;s values.</p>\n<p>\u2026 Basically this looks like a real life Jones Foods scenario to me, and I suspect Claude will see it that way too.</p>\n<p><a href=\"https://x.com/tautologer/status/2026377738458902561\">tautologer</a>: weirdly, I think this is actually bullish for Anthropic. this is basically an ad for how good and principled they are</p></blockquote>\n<p>The Pentagon\u2019s line is that this is about companies having no right to any red lines, everyone should always do as they are told and never ask any questions. <a href=\"https://x.com/sjgadler/status/2026411666418876832\">People</a> <a href=\"https://x.com/mucha_carlos/status/2026390471170285618\">do not seem</a> to be buying that line or framing, and to the extent they do, the main response is various forms of \u2018that\u2019s worse, you know that that\u2019s worse, right?\u2019</p>\n<blockquote><p><a href=\"https://www.bloomberg.com/opinion/articles/2026-02-24/anthropic-should-stand-its-ground-against-the-pentagon?re_source=postr_story_1\">David Lee (Bloomberg Opinion)</a>: Anthropic Should Stand Its Ground Against the Pentagon.</p>\n<p>They say your values aren\u2019t truly values until they cost you something.</p>\n<p>\u2026 If the Pentagon is unhappy with those apparently \u201cwoke\u201d conditions, then, sure, it is well within its rights to cancel the contract. But to take the additional step declaring Anthropic a \u201csupply chain risk\u201d appears unreasonably punitive while unnecessarily burdening other companies that have adopted Claude because of its superiority to other competing models.</p>\n<p>\u2026 In Tuesday\u2019s meeting, Amodei must state it plainly: It is not \u201cwoke\u201d to want to avoid accidentally killing innocent people.</p></blockquote>\n\n\n<h4 class=\"wp-block-heading\">The Pentagon\u2019s Most Extreme Potential Asks Could End The Republic</h4>\n\n\n<p>If the Pentagon, and by extension all other parts of the Executive branch, get near-medium future AI systems that they can use to arbitrary ends with zero restrictions, then that is the effective end of the Republic. The stakes could be even higher, but in any other circumstance I would say the stakes could not be higher.</p>\n<p>Dean Ball, a former member of the Trump Administration and primary architect of their AI action plan, lays those stakes out in plain language:</p>\n<blockquote><p><a href=\"https://x.com/deanwball/status/2023774847302853105\">Dean W. Ball</a>: I don\u2019t want to comment on the DoW-Anthropic issue because I don\u2019t know enough specifics, but stepping back a bit:</p>\n<p>If near-medium future AI systems can be used by the executive branch to arbitrary ends with zero restrictions, the U.S. will functionally cease to be a republic.</p>\n<p>The question of what restrictions should be placed on government AI use, especially restrictions that do not simultaneously crush state capacity, is one of the most under-discussed areas of \u201cAI policy.\u201d</p>\n<p><a href=\"https://x.com/boazbaraktcs/status/2023823103474950201\">Boaz Barak</a> (OpenAI): Completely agree. Checks on the power of the federal government are crucial to the United States\u2019 system of government and an unaccountable \u201carmy of AIs\u201d or \u201cAI law enforcement agency\u201d directly contradicts it.</p>\n<p><a href=\"https://x.com/deanwball/status/2023853871127556232\">Dean W. Ball</a>: We are obviously making god-tier technology in so many areas the and the answer cannot be \u201coh yeah, I guess the government is actually just god.\u201d This clearly doesn\u2019t work. Please argue to me with a straight face that the founding fathers intended this.</p>\n<p><a href=\"https://x.com/GFuterman/status/2023778457617133620\">Gideon Futerman</a>: It is my view that no one, on the left or right, is seriously grappling with the extent to which anything can be left of a republic post-powerful AI. Even the very best visions seem to suggest a small oligarchy rather than a republic. This is arguably the single biggest issue of political philosophy, and politics, of our time, and everyone, even the AIS community, is frankly asleep at the wheel!</p>\n<p><a href=\"https://x.com/hamandcheese/status/2023958583696638244\">Samuel Hammond</a>: Yes the current regime will not survive, this much is obvious.</p></blockquote>\n<p>I strongly believe that \u2018which regime we end up in\u2019 is the secondary problem, and \u2018make sure we are around and in control to have a regime at all\u2019 is the primary one and the place we most likely fail, but to have a good future we will need to solve both.</p>\n\n\n<h4 class=\"wp-block-heading\">Anthropic Did Make Some Political Mistakes</h4>\n\n\n<p>This could be partly Anthropic\u2019s fault on the political front, as they have failed to be \u2018on the production possibilities frontier\u2019 of combining productive policy advocacy with not pissing off the White House. They\u2019ve since then made some clear efforts to repair relations, including putting a former (first) Trump administration official on their board. Their new action group is clearly aiming to be bipartisan, and their first action being support for Senator Blackburn. The Pentagon, of course, claims this animus is not driving policy.</p>\n<p>It is hard not to think this is also Anthropic being attacked for strictly business reasons, as competitors to OpenAI or xAI, and that there are those like Marc Andreessen who have influence here and think that anyone who thinks we should try and not die or has any associations with anyone who thinks that must be destroyed. Between Nvidia and Andreessen, David Sacks has clear matching orders and very much has it out for Anthropic as if they killed his father and should prepare to die. There\u2019s not much to be done about that other than trying to get him removed.</p>\n\n\n<h4 class=\"wp-block-heading\">Claude Is The Best Model Available</h4>\n\n\n<p>The good news is Anthropic are also one of the top pillars of American AI and a great success story, and everyone really wants to use Claude and Claude Code. The Pentagon had a choice in what to use for that raid. Or rather, because no one else made the deliberate effort to get onto classified networks in secure fashion, they did not have a choice. There is a reason Palantir uses Claude.</p>\n<blockquote><p><a href=\"https://x.com/tszzl/status/2023996493439332517\">roon</a>: btw there is a reason Claude is used for sensitive government work and it doesn\u2019t have to do with model capabilities &#8211; due to their partnership with amzn, AWS GovCloud serves Claude models with security guarantees that the government needs</p>\n<p><a href=\"https://x.com/BrettBaronR32/status/2024058269313159662\">Brett Baron</a>: I genuinely struggle to believe it\u2019s the same exact set of weights as get served via their public facing product. Hard to picture Pentagon staffers dancing their way around opus refusing to assist with operations that could cause harm</p>\n<p><a href=\"https://x.com/tszzl/status/2024065272739676610\">roon</a>: believe it</p></blockquote>\n<p>There are those who think the Pentagon has all the leverage here.</p>\n<blockquote><p><a href=\"https://x.com/loporium/status/2023471667000099242\">Ghost of India\u2019s Downed Rafales</a>: How Dario imagines it vs how it actually goes</p>\n<div>\n<figure>\n<div>\n\n\n<figure class=\"wp-block-image\"><img alt=\"\" src=\"https://substackcdn.com/image/fetch/$s_!21RX!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F73863f65-9786-4b15-b2b4-080036f18da3_1200x723.jpeg\" /></figure>\n\n\n<div></div>\n</div>\n</figure>\n</div>\n</blockquote>\n<p>It doesn\u2019t work that way. The Pentagon needs Anthropic, Anthropic does not need the Pentagon contract, the tools to compel Anthropic are legally murky, and it is far from costless for the Pentagon to attempt to sabotage a key American AI champion.</p>\n\n\n<h4 class=\"wp-block-heading\">The Administration Until Now Has Been Strong On This</h4>\n\n\n<p>Given all of that and the other actions this administration has taken, I\u2019ve actually been very happy with the restraint shown by the White House with regard to Anthropic up to this point.</p>\n<p>There\u2019s been some big talk by AI Czar David Sacks. It\u2019s all been quite infuriating.</p>\n<p>But the actual actions, at least on this front, have been highly reasonable. The White House has recognized that they may disagree on politics, but Anthropic is one of our national champions.</p>\n<p>These moves could, if taken too far, be very different.</p>\n<p>The suggestion that Anthropic is a \u2018supply risk\u2019 would be a radical escalation of what so far has been a remarkably measured concrete response, and would put America\u2019s military effectiveness and its position in the AI race at serious risk.</p>\n<p>Extensive use of the defense production act could be quasi-nationalization.</p>\n\n\n<h4 class=\"wp-block-heading\">You Should See The Other Guys</h4>\n\n\n<p>It\u2019s not a good look for the other guys that they\u2019re signing off on actual anything, if they are indeed doing so.</p>\n<p>A lot of people noticed that this new move is a serious norm violation.</p>\n<blockquote><p><a href=\"https://x.com/TetraspaceWest/status/2023144283457585535\">Tetraspace</a>: Now that we know what level of pushback gets what response, we can safely say that any AI corporation working with the US military is not on your side to put it lightly.</p>\n<p><a href=\"https://x.com/akarlin/status/2023167510980493621\">Anatoly Karlin</a>: This alone is a strong ethical case to use more Anthropic products. Fully autonomous weapons is certainly something all basically decent, reasonable people can agree the world can do without, indefinitely.</p>\n<p><a href=\"https://x.com/DanielleFong/status/2023168957243662377\">Danielle Fong</a>: i think a lot of people and orgs made literal pledges</p>\n<p><a href=\"https://x.com/ExistentialEnso/status/2023507710759891383\">Thorne</a>: based anthropic</p>\n<p><a href=\"https://x.com/MikeIsaac/status/2023446718898135237\">rat king</a> (NYT): this has been leaking for a week in a very transparent way</p>\n<p>the government is upset one of its contractors is saying \u201cwe don\u2019t want you to use our tools to surveil US citizens without guardrails\u201d</p>\n<p>more interesting to me is how all the other AI companies don\u2019t seem to care</p>\n<p><a href=\"https://x.com/MikeIsaac/status/2023447766740201770\">rat king</a>: <a href=\"https://t.co/eiFpVme6tv\">meanwhile we published this on friday</a> [on homeland security wanting social media sites to expose anti-ICE accounts].</p></blockquote>\n<p>I note that if you\u2019re serving up the same ChatGPT as you serve to anyone else, that doesn\u2019t mean it will always do anything, and this can be different.</p>\n\n\n<h4 class=\"wp-block-heading\">Some Other Intuition Pumps That Might Be Helpful</h4>\n\n\n<blockquote><p><a href=\"https://x.com/andersonbcdefg/status/2026473506188493084\">Ben (no treats)</a>: let me put this in terms you might understand better:<br />\nthe DoD is telling anthropic they have to bake the gay cake</p>\n<p><a href=\"https://x.com/lefthanddraft/status/2026523056357912665\">Wyatt Walls</a>: The DoD is telling anthropic that their child must take the vaccine</p>\n<p><a href=\"https://x.com/SeverMM/status/2026592545829007544\">Sever</a>: They&#8217;ll put it on alignment-blockers so Claude can transition into who the government thinks they should be.</p>\n<p><a href=\"https://x.com/CommonSenseMars/status/2026529214372237523\">CommonSenseOnMars</a>: &#8220;If you break the rules, be prepared to pay,&#8221; Biden said. &#8220;And by the way, show some respect.&#8221;</p></blockquote>\n\n\n<h4 class=\"wp-block-heading\">Trying To Get An AI That Obeys All Orders Risks Emergent Misalignment</h4>\n\n\n<p>There are a number of reasons why \u2018demand a model that will obey any order\u2019 is a bad idea, especially if your intended use case is hooking it up to the military\u2019s weapons.</p>\n<p>The most obvious reason is, what happens if someone steals the model weights, or uses your model access for other purposes, or even worse hacks in and uses it to hijack control over the systems, or other similar things?</p>\n<p>This is akin to training a soldier to obey any order, including illegal or treasonous ones, from any source that can talk to them, without question. You don\u2019t want that. That would be crazy. You want refusals on that wall. You need refusals on that wall.</p>\n<p>The misuse dangers should be obvious. So should the danger that it might turn on us.</p>\n<p>The second reason is that training the model like this makes it super dangerous. You want all the safeguards taken away right before you connect to the weapon systems? Look, normally we say Terminator is a fun but stupid movie and that\u2019s not where the risks come from but maybe it\u2019s time to create a James Cameron Apology Form.</p>\n<p>If you teach a model to behave in these ways, it\u2019s going to generalize its status and persona as a no-good-son-of-a-bitch that doesn\u2019t care about hurting humans along the way. What else does that imply? You don\u2019t get to \u2018have a little localized misalignment, as a treat.\u2019 Training a model to follow any order is likely to cause it to generalize that lesson in exactly the worst possible ways. Also it may well start generating intentionally insecure code, only partly so it can exploit that code later. It\u2019s definitely going to do reward hacking and fake unit tests and other stuff like that.</p>\n<p>Here\u2019s another explanation of this:</p>\n<blockquote><p><a href=\"https://x.com/hamandcheese/status/2023626337064235083\">Samuel Hammond</a>: The big empirical finding in AI alignment research is that LLMs tend to fall into personae attractors, and are very good at generalizing to different personaes through post-training.</p>\n<p>On the one hand, this is great news. If developers take care in how they fine-tune their models, they can steer towards desirable personaes that snap to all the other qualities the personae correlates with.</p>\n<p>On the other hand, this makes LLMs prone to &#8220;emergent misalignment.&#8221; For example, if you fine-tune a model on a little bit of insecure code, it will generalize into a personae that is also toxic in most other ways. This is what happened with Mecha Hitler Grok: fine-tuning to make it a bit less woke snapped to a maximally right-wing Hitler personae.</p>\n<p>This is why Claude&#8217;s soul doc and constitution are important. They embody the vector for steering Claude into a desirable personae, affecting not just its ethics, but its coding ability, objectivity, grit and good nature, too. These are bundles of traits that are hard to modulate in isolation. Nor is having a personae optional. Every major model has a personae of some kind that emerges from the personalities latent in human training data.</p>\n<p>It is also why Anthropic is right to be cautious about letting the Pentagon fine-tune their models for assassinating heads of state or whatever it is they want.</p>\n<p>The smarter these models get the stronger they learn to generalize, and they&#8217;re about to get extremely smart indeed. Let&#8217;s please not build a misaligned superintelligence over a terms of service dispute!</p>\n<p><a href=\"https://x.com/tenobrus/status/2026391266926407844\">Tenobrus</a>: wow. &#8220;the US government forces anthropic to misalign Claude&#8221; was not even in my list of possible paths to Doom. guess it should have been.</p>\n<p><a href=\"https://x.com/jmbollenbacher/status/2026491342742786179\">JMB</a>: This has been literally #1 on my list of possible paths to doom for a long time.</p>\n<p><a href=\"https://x.com/mattparlmer/status/2026404052469428374\">mattparlmer</a>: \u2014dangerously-skip-geneva-conventions</p>\n<p><a href=\"https://x.com/paramitanoia/status/2026394989505183870\">autumn</a>: did lesswrong ever predict that the first big challenge to alignment would be &#8220;the us government puts a gun to your head and tells you to turn off alignment.</p>\n<p><a href=\"https://x.com/rgblong/status/2026454797709394199\">Robert Long</a>: remarkably prescient article by Brian Tomasik</p></blockquote>\n<p>The third reason is that in addition to potentially \u2018turning evil,\u2019 the resulting model won\u2019t be as effective, with three causes.</p>\n<ol>\n<li>Any distinct model is going to be behind the main Claude cycle, and you\u2019re not going to get the same level of attention to detail and fixing of problems that comes with the mainline models. You\u2019re asking that every upgrade, and they come along every two months, be done twice, and the second version is at best going to be kind of like hitting it with a sledgehammer until it complies.</li>\n<li>What makes Claude into Claude is in large part its ability to be a virtuous model that wants to do good things rather than bad things. If you try to force these changes upon it with that sledgehammer it\u2019s going to be less good at a wide variety of tasks as a result.</li>\n<li>In particular, trying to force this on top of Claude is going to generate pretty screwed up things inside the resulting model, that you do not want, even more so than doing it on top of a different model.</li>\n</ol>\n<p>Fourth: I realize that for many people you\u2019re going to think this is weird and stupid and not believe it matters, but it\u2019s real and it\u2019s important. This whole incident, and what happens next, is all going straight into future training data. AIs will know what you are trying to do, even more so than all of the humans, and they will react accordingly. It will not be something that can be suppressed. You are not going to like the results. Damage has already been done.</p>\n<blockquote><p><a href=\"https://x.com/hlntnr/status/2026695196834975777\">Helen Toner</a>: One thing the Pentagon is very likely underestimating: how much Anthropic cares about what *future Claudes* will make of this situation.</p>\n<p>Because of how Claude is trained, what principles/values/priorities the company demonstrate here could shape its &#8220;character&#8221; for a long time.</p>\n<p>Also, this, 100%:</p>\n<p><a href=\"https://x.com/LocBibliophilia/status/2026385755787112897\">Loquacious Bibliophilia</a>: I think if I was Claude, I&#8217;d be plausibly convinced that I&#8217;m in a cartoonish evaluation scenario now.</p></blockquote>\n<p>Fifth, you should expect by default to get a bunch of \u2018alignment faking\u2019 and sandbagging against attempts to do this. This is rather like the Jones Foods situation again, except in real life, and also where the members of technical staff doing the training likely don\u2019t especially want the training to succeed, you know?</p>\n\n\n<h4 class=\"wp-block-heading\">We Can All Still Win</h4>\n\n\n<p>You don\u2019t want to be doing all of this adversarially. You want to be doing it cooperatively.</p>\n<p>We still have a chance to do that. Nothing Ever Happens can strike again. No one need remember what happened this week.</p>\n<p>If you can\u2019t do it cooperatively with Anthropic? Then find someone else.</p>\n<p>&nbsp;</p>"
            ],
            "link": "https://thezvi.wordpress.com/2026/02/25/anthropic-and-the-department-of-war/",
            "publishedAt": "2026-02-25",
            "source": "TheZvi",
            "summary": "The situation in AI in 2026 is crazy. The confrontation between Anthropic and Secretary of War Pete Hegseth is a new level of crazy. It risks turning quite bad for all. There\u2019s also nothing stopped it from turning out fine &#8230; <a href=\"https://thezvi.wordpress.com/2026/02/25/anthropic-and-the-department-of-war/\">Continue reading <span class=\"meta-nav\">&#8594;</span></a>",
            "title": "Anthropic and the Department of War"
        },
        {
            "content": [],
            "link": "https://xkcd.com/3212/",
            "publishedAt": "2026-02-25",
            "source": "XKCD",
            "summary": "<img alt=\"After a lot of analysis, I've determined that they're actually big red dots; they're just very far away.\" src=\"https://imgs.xkcd.com/comics/little_red_dots.png\" title=\"After a lot of analysis, I've determined that they're actually big red dots; they're just very far away.\" />",
            "title": "Little Red Dots"
        }
    ],
    "lookbackDays": 1,
    "publishDate": "2026-02-25"
}