{
    "articles": [
        {
            "content": [],
            "link": "https://harper.blog/notes/2025-05-20_b02caa989153_these-are-the-parents-that-rai/",
            "publishedAt": "2025-05-21",
            "source": "Harper Reed",
            "summary": "<p>These are the parents that raised me</p> <figure> <img alt=\"image_1.jpg\" height=\"1199\" src=\"https://harper.blog/notes/2025-05-20_b02caa989153_these-are-the-parents-that-rai/image_1.jpg\" width=\"1799\" /> </figure> <hr /> <p>Thank you for using RSS. I appreciate you. <a href=\"mailto:harper&#64;modest.com\">Email me</a></p>",
            "title": "Note #247"
        },
        {
            "content": [
                "<p>If you want to get paid for abstract analysis that is not mainly organized around current cultural or political fights, academia is pretty much the only game in town. So I am quite grateful that academia exists, and has included me.</p><p>But I do have a complaint. In most areas of life, activities are typically justifies in some detail in terms of the accepted purpose of that area. E.g., hospitals save lives, businesses serve customers, roads support travel, armies deter fights, and so on. But though the accepted purpose of academic research is to either answer deep important questions, or to help non-academics somehow, academics rarely explicitly justify their work in such terms.</p><p>For example, polls found that these goals best explain ~7% of <a href=\"https://x.com/robinhanson/status/1924849402960613420\">which</a> research projects academics pick, and ~9% of <a href=\"https://x.com/robinhanson/status/1924889179156013506\">which</a> papers/projects academics approve via peer review. Such choices are instead explained 32% and 58% respectively by topic/methods being in fashion. The remaining 62% of project choice is explained by building on prior work/skills, and the remaining 33% of peer review choice is explained by work showing impressive abilities.</p><p>You can also check this yourself by asking individual academics to explain how their work could plausibly contribute to answering deep important questions, or to non-academic value. Most will be surprised by the question, having never been asked, and answer poorly. </p><p>Yes, in principle the fashions that drive these choices could themselves be driven by processes that induce fashion to track deep important questions and non-academic value. But I&#8217;ve been in academia for four decades now and I just don&#8217;t see this. Changes to academic fashion, like other fashion, instead mostly results from individuals competing to gain their usual selfish rewards.</p><p>The arts are the other main area of life where specialists poorly justify their specific actions in terms of the usual area purposes. So if the arts are mostly about showing off personal abilities, and abilities to judge such abilities, likely so is academia. </p>"
            ],
            "link": "https://www.overcomingbias.com/p/my-complaint-re-academia",
            "publishedAt": "2025-05-21",
            "source": "Robin Hanson",
            "summary": "If you want to get paid for abstract analysis that is not mainly organized around current cultural or political fights, academia is pretty much the only game in town.",
            "title": "My Complaint Re Academia"
        },
        {
            "content": [],
            "link": "https://simonwillison.net/2025/May/21/chatgpt-new-memory/#atom-entries",
            "publishedAt": "2025-05-21",
            "source": "Simon Willison",
            "summary": "<p>Last month ChatGPT got a major upgrade. As far as I can tell the closest to an official announcement was <a href=\"https://twitter.com/OpenAI/status/1910378768172212636\">this tweet from @OpenAI</a>:</p> <blockquote> <p>Starting today [April 10th 2025], memory in ChatGPT can now reference all of your past chats to provide more personalized responses, drawing on your preferences and interests to make it even more helpful for writing, getting advice, learning, and beyond.</p> </blockquote> <p>This <a href=\"https://help.openai.com/en/articles/8590148-memory-faq\">memory FAQ</a> document has a few more details, including that this \"Chat history\" feature is currently only available to paid accounts:</p> <blockquote> <p> Saved memories and Chat history are offered only to Plus and Pro accounts. Free\u2011tier users have access to Saved memories only.</p> </blockquote> <p>This makes a <em>huge</em> difference to the way ChatGPT works: it can now behave as if it has recall over prior conversations, meaning it will be continuously customized based on that previous history.</p> <p>It's effectively collecting a <strong>dossier</strong> on our previous interactions, and applying that information to every future chat.</p> <p>It's closer to how many (most?) users intuitively guess it would work - surely an \"AI\" can remember things you've said to it in the past?</p> <p>I wrote about this common misconception last year in <a",
            "title": "I really don't like ChatGPT's new memory dossier"
        },
        {
            "content": [
                "<p>\n          <a href=\"https://www.astralcodexten.com/p/hidden-open-thread-3825\">\n              Read more\n          </a>\n      </p>"
            ],
            "link": "https://www.astralcodexten.com/p/hidden-open-thread-3825",
            "publishedAt": "2025-05-21",
            "source": "SlateStarCodex",
            "summary": "<p> <a href=\"https://www.astralcodexten.com/p/hidden-open-thread-3825\"> Read more </a> </p>",
            "title": "Hidden Open Thread 382.5"
        },
        {
            "content": [
                "<p>Five years later, we can&#8217;t stop talking about COVID. Remember lockdowns? The conflicting guidelines about masks - don&#8217;t wear them! Wear them! Maybe wear them! School closures, remote learning, learning loss, something about teachers&#8217; unions. That one Vox article on how worrying about COVID was anti-Chinese racism. The time Trump sort of half-suggested injecting disinfectants. Hydroxychloroquine, ivermectin, fluvoxamine, Paxlovid. Those jerks who tried to pressure you into getting vaccines, or those other jerks who wouldn&#8217;t get vaccines even though it put everyone else at risk. Anthony Fauci, Pierre Kory, Great Barrington, Tomas Pueyo, Alina Chan. Five years later, you can open up any news site and find continuing debate about all of these things.</p><p>The only thing about COVID nobody talks about anymore is the 1.2 million deaths.</p><p>That&#8217;s 1.2 million American deaths. Globally it&#8217;s officially 7 million, unofficially 20 - 30 million. But 1.2 million American deaths is still a lot. It&#8217;s more than Vietnam plus 9/11 plus every mass shooting combined - in fact, more than ten times all those things combined. It was the single highest-fatality event in American history, beating the previous record-holder - the US Civil War - by over 50%. All these lives seem to have fallen into oblivion too quietly to be heard over the noise of Lab Leak Debate #35960381.</p><p>Maybe it&#8217;s because they were mostly old people? Old people have already lived a long life, nobody can get too surprised about them dying. But although only a small fraction of COVID deaths were young people, a small fraction of a large number can still be large: the pandemic killed 250,000 &lt;65-year-old Americans, wiping out enough non-seniors to populate Salt Lake City. More military-age young men died in COVID than in Iraq/Afghanistan. Even the old people were somebody&#8217;s spouse or parent or grandparent; many should have had a good 5 - 10 years left.</p><p>Usually I&#8217;m the one arguing that we have to do cost-benefit analysis, that it&#8217;s impractical and incoherent to value every life at infinity billion dollars. And indeed, <a href=\"https://www.astralcodexten.com/p/lockdown-effectiveness-much-more\">most lockdown-type measures look</a> marginal on a purely economic analysis, and utterly fail one that includes hedonic costs. Rejecting some safety measures even though they saved lives was probably the right call. Still, I didn&#8217;t want to win <em>this</em> hard. People are saying things like &#8220;COVID taught us that scientists will always exaggerate how bad things will be.&#8221; I think if we&#8217;d known at the beginning of COVID that it would kill 1.2 million Americans, people would have thought that whatever warnings they were getting, or panicky responses were being proposed, were -  if anything - understated.<a class=\"footnote-anchor\" href=\"https://www.astralcodexten.com/feed#footnote-1\" id=\"footnote-anchor-1\" target=\"_self\">1</a></p><p>Rather than rescue this with appeals to age or some other variable making these deaths not count, I think we should think of it as a bias, fueled by two things. First, dead people can&#8217;t complain about their own deaths, so there are no sympathetic victims writing their sob stories for everyone to see<a class=\"footnote-anchor\" href=\"https://www.astralcodexten.com/feed#footnote-2\" id=\"footnote-anchor-2\" target=\"_self\">2</a>. Second, <a href=\"https://slatestarcodex.com/2014/12/17/the-toxoplasma-of-rage/\">controversy sells</a>. We fight over lockdowns, lab leaks, long COVID, and vaccines, all of which have people arguing both sides, and all of which let us feel superior to our stupid and evil enemies. But there&#8217;s no &#8220;other side&#8221; to 1.2 million deaths. Thinking about them doesn&#8217;t let you feel superior to anyone - just really sad.</p><p>This is the same point I try to make in <a href=\"https://www.astralcodexten.com/p/in-continued-defense-of-effective\">my writings on charity</a>. A million lives is a statistic, but some random annoying controversial thing that captures the public interest is alive and salient - it&#8217;s easier to remember a story about a charity that turned out to be corrupt, or offensive, or just cringe, compared to the one that saved 1,000 or 10,000 or 100,000 lives. Even the people who <em>do </em>remember the 10,000 lives have to fight to avoid both-sidesing it - &#8220;Well, this charity saved 10,000 lives, but that charity said something cringe on Twitter, so overall it&#8217;s kind of a wash&#8221;. In the end people average out the whole subject to &#8220;Wait, you support charities? But didn&#8217;t you hear about that one that turned out to be corrupt? Can&#8217;t believe you&#8217;d be into something like that.&#8221;</p><p>I freely admit I don&#8217;t know where I&#8217;m going with this. If you ask what you should do differently upon being reminded that 1.2 million Americans died during COVID, I won&#8217;t have an answer - there&#8217;s no gain from scheduling ten minutes to be sad each morning on Google Calendar. I&#8217;m not recommending you do anything differently, just remarking how weird it is that this doesn&#8217;t automatically come up more of its own accord.</p><div class=\"footnote\"><a class=\"footnote-number\" contenteditable=\"false\" href=\"https://www.astralcodexten.com/feed#footnote-anchor-1\" id=\"footnote-1\" target=\"_self\">1</a><div class=\"footnote-content\"><p>I&#8217;m being weirdly hypocritical or self-contradictory here. If people had known at the beginning that 1.2 million people would have died, they would have proposed policies much stricter than what actually happened - and I think those policies would have been wrong. But in the real world, it&#8217;s as if two opposite mistakes cancelled out - one, where people <a href=\"https://www.scientificamerican.com/article/psychology-of-taboo-tradeoff/\">demand we choose lives over any amount of money</a> when they&#8217;re explicitly making the comparison, and a second where people never make the comparison because they just sort of ignore any number of real-world deaths.</p></div></div><div class=\"footnote\"><a class=\"footnote-number\" contenteditable=\"false\" href=\"https://www.astralcodexten.com/feed#footnote-anchor-2\" id=\"footnote-2\" target=\"_self\">2</a><div class=\"footnote-content\"><p>People might complain about their relatives dying, but I think you&#8217;re more likely to get told to &#8220;read the room&#8221; when complaining that your grandma died at 75 than when complaining that you lost your job or suffered learning loss or something.</p></div></div>"
            ],
            "link": "https://www.astralcodexten.com/p/the-other-covid-reckoning",
            "publishedAt": "2025-05-21",
            "source": "SlateStarCodex",
            "summary": "<p>Five years later, we can&#8217;t stop talking about COVID. Remember lockdowns? The conflicting guidelines about masks - don&#8217;t wear them! Wear them! Maybe wear them! School closures, remote learning, learning loss, something about teachers&#8217; unions. That one Vox article on how worrying about COVID was anti-Chinese racism. The time Trump sort of half-suggested injecting disinfectants. Hydroxychloroquine, ivermectin, fluvoxamine, Paxlovid. Those jerks who tried to pressure you into getting vaccines, or those other jerks who wouldn&#8217;t get vaccines even though it put everyone else at risk. Anthony Fauci, Pierre Kory, Great Barrington, Tomas Pueyo, Alina Chan. Five years later, you can open up any news site and find continuing debate about all of these things.</p><p>The only thing about COVID nobody talks about anymore is the 1.2 million deaths.</p><p>That&#8217;s 1.2 million American deaths. Globally it&#8217;s officially 7 million, unofficially 20 - 30 million. But 1.2 million American deaths is still a lot. It&#8217;s more than Vietnam plus 9/11 plus every mass shooting combined - in fact, more than ten times all those things combined. It was the single highest-fatality event in American history, beating the previous record-holder - the US Civil War - by over 50%. All these lives seem to have",
            "title": "The Other COVID Reckoning"
        },
        {
            "content": [
                "<span class=\"thumbnail\"><img alt=\"\" class=\"attachment-thumbnail size-thumbnail wp-post-image\" height=\"108\" src=\"https://content.wolfram.com/sites/43/2025/05/imaginingbiggerbrains-news.png\" width=\"128\" /></span><h2 id=\"cats-dont-talk\">Cats Don\u2019t Talk</h2>\n<p>We humans have perhaps 100 billion neurons in our brains. But what if we had many more? Or what if the AIs we built effectively had many more? What kinds of things might then become possible? At 100 billion neurons, we know, for example, that compositional language of the kind we humans use is possible. At the 100 million or so neurons of a cat, it doesn\u2019t seem to be. But what would become possible with 100 trillion neurons? And is it even something we could imagine understanding? </p>\n<p>My purpose here is to start exploring such questions, informed by what we\u2019ve seen in recent years in neural nets and LLMs, as well as by what we now know about the <a href=\"https://writings.stephenwolfram.com/category/computational-science/\">fundamental nature of computation</a>, and about neuroscience and the operation of actual brains (like the one that\u2019s writing this, imaged here): </p>\n<p><img alt=\"What If We Had Bigger Brains? Imagining Minds beyond Ours\" height=\"244\" src=\"https://content.wolfram.com/sites/43/2025/05/sw05202025catscimg1.png\" title=\"What If We Had Bigger Brains? Imagining Minds beyond Ours\" width=\"611\" /><span id=\"more-68380\"></span></p>\n<p>One suggestive point is that as artificial neural nets have gotten bigger, they seem to have successively passed a sequence of thresholds in capability:</p>\n<div>\n<div class=\"wolfram-c2c-wrapper writtings-c2c_above\"> <img alt=\"\" height=\"144\" src=\"https://content.wolfram.com/sites/43/2025/05/sw05202025catsbimg1.png\" title=\"\" width=\"423\" /> </div>\n</p></div>\n<p>So what\u2019s next? No doubt there\u2019ll be things like humanoid robotic control that have close analogs in what we humans already do. But what if we go far beyond the ~10<sup>14</sup> connections that our human brains have? What qualitatively new kinds of capabilities might there then be? </p>\n<p>If this was about \u201ccomputation in general\u201d then there wouldn\u2019t really be much to talk about. The <a href=\"https://www.wolframscience.com/nks/chap-12--the-principle-of-computational-equivalence/\">Principle of Computational Equivalence</a> implies that beyond some low threshold computational systems can generically produce behavior that corresponds to computation that\u2019s as sophisticated as it can ever be. And indeed that\u2019s the kind of thing we see both in lots of abstract settings, and in the natural world. </p>\n<p>But the point here is that we\u2019re not dealing with \u201ccomputation in general\u201d. We\u2019re dealing with the kinds of computations that brains fundamentally do. And the essence of these seems to have to do with taking in large amounts of sensory data and then coming up with what amount to decisions about what to do next. </p>\n<p>It\u2019s not obvious that there\u2019d be any reasonable way to do this. The world at large is full of <a href=\"https://www.wolframscience.com/nks/chap-12--the-principle-of-computational-equivalence#sect-12-6--computational-irreducibility\">computational irreducibility</a>\u2014where the only general way to work out what will happen in a system is just to run the underlying rules for that system step by step and see what comes out:</p>\n<div>\n<div class=\"wolfram-c2c-wrapper writtings-c2c_above\"> <img alt=\"\" height=\"201\" src=\"https://content.wolfram.com/sites/43/2025/05/sw05202025catsdimg4.png\" title=\"\" width=\"531\" /> </div>\n</p></div>\n<p>And, yes, there are plenty of questions and issues for which there\u2019s essentially no choice but to <a href=\"https://writings.stephenwolfram.com/2024/03/can-ai-solve-science/#the-hard-limit-of-computational-irreducibility\">do this irreducible computation</a>\u2014just as there are plenty of cases where <a href=\"https://writings.stephenwolfram.com/2023/02/what-is-chatgpt-doing-and-why-does-it-work/\">LLMs need</a> to <a href=\"https://writings.stephenwolfram.com/2023/03/chatgpt-gets-its-wolfram-superpowers/\">call on</a> our <a href=\"https://www.wolfram.com/language/\">Wolfram Language</a> computation system to get computations done. But brains, for the things most important to them, somehow seem to routinely manage to \u201cjump ahead\u201d without in effect simulating every detail. And what makes this possible is the fundamental fact that within any system that shows overall computational irreducibility there must inevitably be an infinite number of \u201cpockets of computational reducibility\u201d, in effect associated with \u201csimplifying features\u201d of the behavior of the system. </p>\n<p>It\u2019s these \u201cpockets of reducibility\u201d that brains exploit to be able to successfully \u201cnavigate\u201d the world for their purposes in spite of its \u201cbackground\u201d of computational irreducibility. And in these terms things like the progress of science (and technology) can basically be thought of as the identification of progressively more pockets of computational reducibility. And we can then imagine that the capabilities of bigger brains could revolve around being able to \u201chold in mind\u201d more of these pockets of computational reducibility. </p>\n<p>We can think of brains as fundamentally serving to <a href=\"https://writings.stephenwolfram.com/2023/12/observer-theory/\">\u201ccompress\u201d the complexity of the world</a>, and extract from it just certain features\u2014associated with pockets of reducibility\u2014that we care about. And for us a key manifestation of this is the idea of concepts, and of language that uses them. At the level of raw sensory input we might see many detailed images of some category of thing\u2014but language lets us describe them all just in terms of one particular symbolic concept (say \u201crock\u201d).</p>\n<p>In a rough first approximation, we can imagine that there\u2019s a direct correspondence between concepts and words in our language. And it\u2019s then notable that human languages all tend to have perhaps 30,000 common words (or word-like constructs). So is that scale the result of the size of our brains? And could bigger brains perhaps deal with many more words, say millions or more? </p>\n<p>\u201cWhat could all those words be about?\u201d we might ask. After all, our everyday experience makes it seem like our current 30,000 words are quite sufficient to describe the world as it is. But in some sense this is circular: we\u2019ve invented the words we have because they\u2019re what we need to describe the aspects of the world we care about, and want to talk about. There will always be more features of, say, the natural world that we could talk about. It\u2019s just that we haven\u2019t chosen to engage with them. (For example, we could perfectly well invent words for all the detailed patterns of clouds in the sky, but those patterns are not something we currently feel the need to talk in detail about.)</p>\n<p>But given our current set of words or concepts, is there \u201cclosure\u201d to it? Can we successfully operate in a \u201cself-consistent slice of <a href=\"https://writings.stephenwolfram.com/2023/07/generative-ai-space-and-the-mental-imagery-of-alien-minds/#the-notion-of-interconcept-space\">concept space</a>\u201d or will we always find ourselves needing new concepts? We might think of <a href=\"https://writings.stephenwolfram.com/2023/03/will-ais-take-all-our-jobs-and-end-human-history-or-not-well-its-complicated/#generalized-economics-and-the-concept-of-progress\">new concepts as being associated with intellectual progress</a> that we choose to pursue or not. But insofar as the \u201coperation of the world\u201d is computationally irreducible it\u2019s basically inevitable that we\u2019ll eventually be confronted with things that cannot be described by our current concepts.</p>\n<p>So why is it that the number of concepts (or words) isn\u2019t just always increasing? A fundamental reason is abstraction. Abstraction takes collections of potentially large numbers of specific things (\u201ctiger\u201d, \u201clion\u201d, &#8230;) and allows them to be described \u201cabstractly\u201d in terms of a more general thing (say, \u201cbig cats\u201d). And abstraction is useful if it\u2019s possible to make collective statements about those general things (\u201call big cats have&#8230;\u201d), in effect providing a consistent \u201chigher-level\u201d way of thinking about things. </p>\n<p>If we imagine concepts as being associated with particular pockets of reducibility, the phenomenon of abstraction is then a reflection of the existence of networks of these pockets. And, yes, such networks can themselves show computational irreducibility, which can then have its own pockets of reducibility, etc. </p>\n<p>So what about (artificial) neural nets? It\u2019s routine to \u201clook inside\u201d these, and for example see the possible patterns of activation at a given layer based on a range of possible (\u201creal-world\u201d) inputs. We can then think of these patterns of activation as <a href=\"https://writings.stephenwolfram.com/2023/02/what-is-chatgpt-doing-and-why-does-it-work/#the-concept-of-embeddings\">forming points in a \u201cfeature space\u201d</a>. And typically we\u2019ll be able to see clusters of these points, which we can potentially identify as \u201cemergent concepts\u201d that we can view as having been \u201cdiscovered\u201d by the neural net (or rather, its training). Normally there won\u2019t be existing words in human languages that correspond to most of these concepts. They represent pockets of reducibility, but not ones that we\u2019ve identified, and that are captured by our typical 30,000 or so words. And, yes, even in today\u2019s neural nets, there can easily be millions of \u201cemergent concepts\u201d.</p>\n<p>But will these be useful abstractions or concepts, or merely \u201cincidental examples of compression\u201d not connected to anything else? The construction of neural nets implies that a pattern of \u201cemergent concepts\u201d at one layer will necessarily feed into the next layer. But the question is really whether the concept can somehow be useful \u201cindependently\u201d\u2014not just at this particular place in the neural net.</p>\n<p>And indeed the most obvious everyday use for words and concepts\u2014and language in general\u2014is for communication: for \u201ctransferring thoughts\u201d from one mind to another. Within a brain (or a neural net) there are all kinds of complicated patterns of activity, different in each brain (or each neural net). But a fundamental role that concepts, words and language play is to define a way to \u201cpackage up\u201d certain features of that activity in a form that can be robustly transported between minds, somehow inducing \u201ccomparable thoughts\u201d in all of them. </p>\n<p>The transfer from one mind to another can never be precise: in going from the pattern of activity in one brain (or neural net) to the pattern of activity in another, there\u2019ll always be translation involved. But\u2014at least up to a point\u2014one can expect that the \u201cmore that\u2019s said\u201d the more faithful a translation can be. </p>\n<p>But what if there\u2019s a bigger brain, with more \u201cemergent concepts\u201d inside? Then to communicate about them at a certain level of precision we might need to use more words\u2014if not a fundamentally richer form of language. And, yes, while dogs seem to understand isolated words (\u201csit\u201d, \u201cfetch\u201d, &#8230;), we, with our larger brains, can deal with compositional language in which we can in effect construct an infinite range of meanings by combining words into phrases, sentences, etc. </p>\n<p>At least as we currently imagine it, language defines a certain model of the world, based on some finite collection of primitives (words, concepts, etc.). The existence of computational irreducibility tells us that such a model can never be complete. Instead, the model has to \u201capproximate things\u201d based on the \u201cnetwork of pockets of reducibility\u201d that the primitives in the language effectively define. And insofar as a bigger brain might in essence be able to make use of a larger network of pockets of reducibility, it can then potentially support a more precise model of the world.</p>\n<p>And it could then be that if we look at such a brain and what it does, it will inevitably seem closer to the kind of \u201cincomprehensible and irreducible computation\u201d that\u2019s characteristic of so many abstract systems, and systems in nature. But it could also be that in being a \u201cbrain-like construct\u201d it\u2019d necessarily tap into computational reducibility in such a way that\u2014with the formalism and abstraction we\u2019ve built\u2014we\u2019d still meaningfully be able to talk about what it can do. </p>\n<p>At the outset we might have thought any attempt for us to \u201cunderstand minds beyond ours\u201d would be like asking a cat to understand algebra. But somehow the universality of the concepts of computation that we now know\u2014with their ability to address the deepest foundations of physics and other fields\u2014makes it seem more plausible we might now be in a position to meaningfully discuss minds beyond ours. Or at least to discuss the rather more concrete question of what brains like ours, but bigger than ours, might be able to do. </p>\n<h2 id=\"how-brains-seem-to-work\">How Brains Seem to Work</h2>\n<p>As we\u2019ve mentioned, at least in a rough approximation, the role of brains is to turn large amounts of sensory input into small numbers of decisions about what to do. But how does this happen? </p>\n<p>Human brains continually receive input from a few million \u201csensors\u201d, mostly associated with photoreceptors in our eyes and touch receptors in our skin. This input is processed by a total of about 100 billion neurons, each responding in a few milliseconds, and mostly organized into a handful of layers. There are altogether perhaps 100 trillion connections between neurons, many quite long range. At any given moment, a few percent of neurons (i.e. perhaps a billion) are firing. But in the end, all that activity seems to feed into particular structures in the lower part of the brain that in effect \u201ctake a majority vote\u201d a few times a second to determine what to do next\u2014in particular with the few hundred \u201cactuators\u201d our bodies have. </p>\n<p>This basic picture seems to be more or less the same in all higher animals. The total number of neurons scales roughly with the number of \u201cinput sensors\u201d (or, in a first approximation, the surface area of the animal\u2014i.e. volume<sup>2/3</sup>\u2014which determines the number of touch sensors). The fraction of brain volume that consists of connections (\u201cwhite matter\u201d) as opposed to main parts of neurons (\u201cgray matter\u201d) increases as a power of the number of neurons. The largest brains\u2014like ours\u2014have a roughly nested pattern of folds that presumably reduce average connection lengths. Different parts of our brains have characteristic functions (e.g. motor control, handling input from our eyes, generation of language, etc.), although there seems to be enough universality that other parts can usually learn to take over if necessary. And in terms of overall performance, animals with smaller brains generally seem to react more quickly to stimuli. </p>\n<p>So what was it that made brains originally arise in biological evolution? Perhaps it had to do with giving animals a way to decide where to go next as they moved around. (Plants, which don\u2019t move around, don\u2019t have brains.) And perhaps it\u2019s because animals can\u2019t \u201cgo in more than one direction at once\u201d that brains seem to have the fundamental feature of generating a single stream of decisions. And, yes, this is probably why we have a <a href=\"https://writings.stephenwolfram.com/2021/03/what-is-consciousness-some-new-perspectives-from-our-physics-project/\">single thread of &#8220;conscious experience&#8221;</a>, rather than a whole collection of experiences associated with the activities of all our neurons. And no doubt it\u2019s also what we leverage in the construction of language\u2014and in communicating through a one-dimensional sequence of tokens. </p>\n<p>It\u2019s notable how similar our description of brains is to the basic <a href=\"https://writings.stephenwolfram.com/2023/02/what-is-chatgpt-doing-and-why-does-it-work/\">operation of large language models</a>: an LLM processes input from its \u201ccontext window\u201d by feeding it through large numbers of artificial neurons organized in layers\u2014ultimately taking something like a majority vote to decide what token to generate next. There are differences, however, most notably that whereas brains routinely intersperse learning and thinking, current LLMs separate training from operation, in effect \u201clearning first\u201d and \u201cthinking later\u201d.</p>\n<p>But almost certainly the core capabilities of both brains and neural nets <a href=\"https://writings.stephenwolfram.com/2024/08/whats-really-going-on-in-machine-learning-some-minimal-models/\">don&#8217;t depend much</a> on the details of their biological or architectural structure. It matters that there are many inputs and few outputs. It matters that there\u2019s irreducible computation inside. It matters that the systems are trained on the world as it is. And, finally, it matters how \u201cbig\u201d they are, in effect relative to the \u201cnumber of relevant features of the world\u201d.</p>\n<p>In artificial neural nets, and presumably also in brains, memory is encoded in the<br />\nstrengths (or \u201cweights\u201d) of connections between neurons. And at least in neural nets it seems that the number of tokens (of textual data) that can reasonably be \u201cremembered\u201d is a few times the number of weights. (With current methods, the number of computational operations of training needed to achieve this is roughly the product of the total number of weights and the total number of tokens.) If there are too few weights, what happens is that the <a href=\"https://writings.stephenwolfram.com/2024/08/whats-really-going-on-in-machine-learning-some-minimal-models/#:~:text=pretty%20much%20the%20smallest%20that%20will%20work\">&#8220;memory&#8221; gets fuzzy</a>, with details of the fuzziness reflecting details of the structure of the network. </p>\n<p>But what\u2019s crucial\u2014for both neural nets and brains\u2014is not so much to remember specifics of training data, but rather to just &#8220;<a href=\"https://writings.stephenwolfram.com/2023/02/what-is-chatgpt-doing-and-why-does-it-work/#models-for-human-like-tasks\">do something reasonable</a>&#8221; for a wide range of inputs, regardless of whether they\u2019re in the training data. Or, in other words, to generalize appropriately from training data.</p>\n<p>But what is \u201cappropriate generalization\u201d? As a practical matter, it tends to be \u201cgeneralization that aligns with what we humans would do\u201d. And it\u2019s then a remarkable fact that artificial neural nets with fairly simple architectures can successfully do generalizations in a way that\u2019s roughly aligned with human brains. So why does this work? Presumably it\u2019s because there are universal features of \u201cbrain-like systems\u201d that are close enough between human brains and neural nets. And once again it\u2019s important to emphasize that what\u2019s happening in both cases seems distinctly weaker than \u201cgeneral computation\u201d. </p>\n<p> A feature of \u201cgeneral computation\u201d is that it can potentially involve unbounded amounts of time and storage space. But both brains and typical neural nets have just a fixed number of neurons. And although both brains and LLMs in effect have an \u201couter loop\u201d that can \u201crecycle\u201d output to input, it\u2019s limited. </p>\n<p>And at least when it comes to brains, a key feature associated with this is the limit on \u201cworking memory\u201d, i.e. memory that can readily be both read and written \u201cin the course of a computation\u201d. Bigger and more developed brains typically seem to support larger amounts of working memory. Adult humans can remember perhaps 5 or 7 \u201cchunks\u201d of data in working memory; for young children, and other animals, it\u2019s less. Size of working memory (as we\u2019ll discuss later) seems to be important in things like language capabilities. And the fact that it\u2019s limited is no doubt one reason we can\u2019t generally \u201crun code in our brains\u201d. </p>\n<p>As we try to reflect on what our brains do, we\u2019re most aware of our stream of conscious thought. But that represents just a tiny fraction of all our neural activity. Most of the activity is much less like \u201cthought\u201d and much more like typical processes in nature, with lots of elements seemingly \u201cdoing their own thing\u201d. We might think of this as an \u201cocean of unconscious neural activity\u201d, from which a \u201c<a href=\"https://writings.stephenwolfram.com/2021/05/the-problem-of-distributed-consensus/\">thread of consensus thought</a>\u201d is derived. Usually\u2014much like in an artificial neural net\u2014it\u2019s difficult to find much regularity in that \u201cunconscious activity\u201d. Though when one trains oneself enough to get to the point of being able to \u201cdo something without thinking about it\u201d, that presumably happens by organizing some part of that activity. </p>\n<p>There\u2019s always a question of what kinds of things we can learn. We can\u2019t overcome computational irreducibility. But how broadly can we handle what\u2019s computationally reducible? Artificial neural nets show a certain genericity in their operation: although some specific architectures are more efficient than others, it doesn\u2019t seem to matter much whether the input they\u2019re fed is images or text or numbers, or whatever. And for our brains it\u2019s probably the same\u2014though what we\u2019ve normally experienced, and learned from, are the specific kinds of input the come from our eyes, ears, etc. And from these, we\u2019ve ended up recognizing certain types of regularities\u2014that we\u2019ve then used to guide our actions, set up our environment, etc. </p>\n<p>And, yes, this plugs into certain pockets of computational reducibility in the world. But there\u2019s always further one could go. And how that might work with brains bigger than ours is at the core of what we\u2019re trying to discuss here.</p>\n<h2 id=\"language-and-beyond\">Language and Beyond</h2>\n<p>At some level we can view our brains as serving to take the complexity of the world and extract from it a compressed representation that our finite minds can handle. But what is the structure of that representation? A central aspect of it is that it ignores many details of the original input (like particular configurations of pixels). Or, in other words, it effectively <a href=\"https://writings.stephenwolfram.com/2023/12/observer-theory/\">equivalences many different inputs together</a>.</p>\n<p>But how then do we describe that equivalence class? Implementationally, say in a neural net, the equivalence class might <a href=\"https://www.wolframscience.com/nks/p275/\">correspond to an attractor</a> to which many different initial conditions all evolve. In terms of the detailed pattern of activity in the neural net the attractor will typically be very hard to describe. But on a larger scale we can potentially just think of it as some kind of robust construct that represents a class of things\u2014or what in terms of our process of thought we might describe as a \u201cconcept\u201d.</p>\n<p>At the lowest level there\u2019s all sorts of complicated neural activity in our brains\u2014most of it mired in computational irreducibility. But the \u201cthin thread of conscious experience\u201d that we extract from this we can for many purposes treat as being made up of higher-level \u201cunits of thought\u201d, or essentially \u201cdiscrete concepts\u201d.</p>\n<p>And, yes, it\u2019s certainly our typical human experience that robust constructs\u2014and particularly ones from which other constructs can be built\u2014will be discrete. In principle one can imagine that there could be things like \u201crobust continuous spaces of concepts\u201d (\u201ccat and dog and everything in between\u201d). But we don&#8217;t have anything like the computational paradigm that shows us a consistent universal way that such things could fit together (there\u2019s <a href=\"https://www.wolframscience.com/nks/notes-12-4--continuous-computation/\">no robust analog of computation theory for real numbers</a>, for example). And somehow the success of the computational paradigm\u2014potentially all the way down to the foundations of the physical universe\u2014doesn\u2019t seem to leave much room for anything else.</p>\n<p>So, OK, let\u2019s imagine that we can represent our thread of conscious experience in terms of concepts. Well, that\u2019s close to saying that we\u2019re using language. We\u2019re \u201cpackaging up\u201d the details of our neural activity into \u201crobust elements\u201d which we can think of as concepts\u2014and which are represented in language essentially by words. And not only does this \u201cpackaging\u201d into language give a robust way for different brains to communicate; it also gives a single brain a robust way to \u201cremember\u201d and \u201credeploy\u201d thoughts.</p>\n<p>Within one brain one could imagine that one might be able to remember and \u201cthink\u201d directly in terms of detailed low-level neural patterns. But no doubt the \u201cneural environment\u201d inside a brain is continually changing (not least because of its stream of sensory input). And so the only way to successfully \u201cpreserve a thought\u201d across time is presumably to \u201cpackage it up\u201d in terms of robust elements, or essentially in terms of language. In other words, if we\u2019re going to be able to consistently \u201cthink a particular thought\u201d we probably have to formulate it in terms of something robust\u2014like concepts.</p>\n<p>But, OK, individual concepts are one thing. But language\u2014or at least human language\u2014is based on putting together concepts in structured ways. One might take a noun (\u201ccat\u201d) and qualify it with an adjective (\u201cblack\u201d) to form a phrase that\u2019s in effect a finer-grained version of the concept represented by the noun. And in a rough approximation one can think of language as formed from trees of nested phrases like this. And insofar as the phrases are independent in their structure (i.e. \u201c<a href=\"https://www.wolframscience.com/nks/notes-10-12--context-free-languages/\">context free</a>\u201d), we can parse such language by recursively understanding each phrase in turn\u2014with the constraint that we can\u2019t do it if the nesting goes too deep for us to hold the necessary stack of intermediate steps in our working memory.</p>\n<p>An important feature of ordinary human language is that it\u2019s ultimately presented in a sequential way. Even though it may consist of a nested tree of phrases, the words that are the leaves of that tree are spoken or written in a one-dimensional sequence. And, yes, the fact that this is how it works is surely closely connected to the fact that our brains construct a single thread of conscious experience. </p>\n<p>In the actuality of the <a href=\"https://reference.wolfram.com/language/ref/entity/Language.html\">few thousand human languages</a> currently in use, there is considerable superficial diversity, but also considerable fundamental commonality. For example, the same parts of speech (noun, verb, etc.) typically show up, as do concepts like \u201csubject\u201d and \u201cobject\u201d. But the details of how words are put together, and how things are indicated, can be fairly different. Sometimes nouns have case endings; sometimes there are separate prepositions. Sometimes verb tenses are indicated by annotating the verb; sometimes with extra words. And sometimes, for example, what would usually be whole phrases can be smooshed together into single words.</p>\n<p>It\u2019s not clear to what extent commonalities between languages are the result of shared history, and to what extent they\u2019re consequences either of the particulars of our human sensory experience of the world, or the particular construction of our brains. It\u2019s not too hard to get <a href=\"https://writings.stephenwolfram.com/2024/03/can-ai-solve-science/#identifying-computational-reducibility\">something like concepts to emerge</a> in experiments on training neural nets to pass data through a \u201cbottleneck\u201d that simulates a \u201cmind-to-mind communication channel\u201d. But how compositionality or grammatical structure might emerge is not clear.</p>\n<p>OK, but so what might change if we had bigger brains? If neural nets are a guide, one obvious thing is that we should be able to deal directly with a larger number of \u201cdistinct concepts\u201d, or words. So what consequences would this have? Presumably one\u2019s language would get \u201cgrammatically shallower\u201d, in the sense that what would otherwise have had to be said with nested phrases could now be said with individual words. And presumably this would tend to lead to \u201cfaster communication\u201d, requiring fewer words. But it would likely also lead to more rigid communication, with less ability to tweak shades of meaning, say by changing just a few words in a phrase. (And it would presumably also require longer training, to learn what all the words mean.)</p>\n<p>In a sense we have a preview of what it\u2019s like to have more words whenever we deal with specialized versions of existing language, aimed say at particular technical fields. There are additional words of \u201cjargon\u201d available, that make certain things \u201cfaster to say\u201d (but require longer to learn). And with that jargon comes a certain rigidity, in saying easily only what the jargon says, and not something slightly different. </p>\n<p>So how else could language be different with a bigger brain? With larger working memory, one could presumably have more deeply nested phrases. But what about more sophisticated grammatical structures, say ones that aren&#8217;t &#8220;context free&#8221;, in the sense that different nested phrases can\u2019t be parsed separately? My guess is that this quickly devolves into requiring arbitrary computation\u2014and runs into computational irreducibility. In principle it\u2019s perfectly possible to have any program as the \u201cmessage\u201d one communicates. But if one has to run the program to \u201cdetermine its meaning\u201d, that\u2019s in general going to involve computational irreducibility.</p>\n<p>And the point is that with our assumptions about what \u201cbrain-like systems\u201d do, that\u2019s something that\u2019s out of scope. Yes, one can construct a system (even with neurons) that can do it. But not with the \u201csingle thread of decisions from sensory input\u201d workflow that seems characteristic of brains. (There are finer gradations one could consider\u2014like languages that are <a href=\"https://www.wolframscience.com/nksonline/page-938d/\">context sensitive</a> but don\u2019t require general computation. But the Principle of Computational Equivalence strongly suggests that the separation between nested context-free systems and ones associated with arbitrary computation <a href=\"https://www.wolframscience.com/nks/notes-12-4--intermediate-degrees/\">is very thin</a>, and there doesn\u2019t seem to be any particular reason to expect that the capabilities of a bigger brain would land right there.)</p>\n<p>Said another way: the Principle of Computational Equivalence says it\u2019s easy to have a system that can deal with arbitrary computation. It\u2019s just that such a system is not \u201cbrain like\u201d in its behavior; it\u2019s more like a typical system we see in nature. </p>\n<p>OK, but what other \u201cadditional features\u201d can one imagine, for even roughly \u201cbrain-like\u201d systems? One possibility is to go beyond the idea of a single thread of experience, and to consider a <a href=\"https://writings.stephenwolfram.com/2021/09/multicomputation-a-fourth-paradigm-for-theoretical-science/#the-formal-structure-of-multicomputation\">multiway system</a> in which threads of experience can branch and merge. And, yes, this is what we imagine happens at a low level in the physical universe, particularly in connection with <a href=\"https://writings.stephenwolfram.com/2024/10/on-the-nature-of-time/#multiple-threads-of-time\">quantum mechanics</a>. And indeed it\u2019s perfectly possible to imagine, for example, a \u201cquantum-like\u201d LLM system in which one generates a graph of different textual sequences. But just \u201cscaling up the number of neurons\u201d in a brain, without changing the overall architecture, won\u2019t get to this. We have to have a different, multiway architecture. Where we have a \u201cgraph of consciousness\u201d rather than a \u201cstream of consciousness\u201d, and where, in effect, we\u2019re \u201cthinking a graph of thoughts\u201d, notably with thoughts themselves being able to branch and merge. </p>\n<p>In our practical use of language, it\u2019s most often communicated in spoken or written form\u2014effectively as a one-dimensional sequence of tokens. But in math, for example, it\u2019s common to have a <a href=\"https://www.stephenwolfram.com/publications/mathematical-notation-past-future/\">certain amount of 2D structure</a>, and in general there are also all sorts of specialized (usually technical) diagrammatic representations in use, often based on using graphs and networks\u2014as we\u2019ll discuss in more detail below. </p>\n<p>But what about general pictures? Normally it\u2019s difficult for us to produce these. But in <a href=\"https://writings.stephenwolfram.com/2023/07/generative-ai-space-and-the-mental-imagery-of-alien-minds/\">generative AI systems it&#8217;s basically easy</a>. So could we then imagine directly \u201ccommunicating mental images\u201d from one mind to another? Maybe as a practical matter some neural implant in our brain could aggregate neural signals from which a displayed image could be generated. But is there in fact something coherent that could be extracted from our brains in this way? Perhaps that can only happen after \u201cconsensus is formed\u201d, and we\u2019ve reduced things to a much thinner \u201cthread of experience\u201d. Or, in other words, perhaps the only robust way for us to \u201cthink about images\u201d is in effect to reduce them to discrete concepts and language-like representations.</p>\n<p>But perhaps if we \u201chad the hardware\u201d to display images directly from our minds it\u2019d be a different story. And it\u2019s sobering to imagine that perhaps the reason cats and dogs don\u2019t appear to have compositional language is just that they don\u2019t \u201chave the hardware\u201d to talk like we do (and it\u2019s too laborious for them to \u201ctype with their paws\u201d, etc.). And, by analogy, that if we \u201chad the hardware\u201d for displaying images, we\u2019d discover we could also \u201cthink very differently\u201d. </p>\n<p>Of course, in some small ways we do have the ability to \u201cdirectly communicate with images\u201d, for example in our use of gestures and body language. Right now, these seem like largely ancillary forms of communication. But, yes, it\u2019s conceivable that with bigger brains, they could be more.</p>\n<p>And when it comes to other animals the story can be different. <a href=\"https://www.wolframscience.com/nks/notes-12-10--animal-communication/\">Cuttlefish are notable</a> for dynamically producing elaborate patterns on their skin\u2014giving them in a sense the hardware to \u201ccommunicate in pictures\u201d. But so far as one can tell, they produce just a small number of distinct patterns\u2014and certainly nothing like a \u201cpictorial generalization of compositional language\u201d. (In principle one could imagine that \u201cgeneralized cuttlefish\u201d could do things like \u201cdynamically run cellular automata on their skin\u201d, just like all sorts of animals <a href=\"https://www.wolframscience.com/nks/chap-8--implications-for-everyday-systems#sect-8-7--biological-pigmentation-patterns\">&#8220;statically&#8221; do in the process of growth or development</a>. But to decode such patterns\u2014and thereby in a sense enable \u201ccommunicating in programs\u201d\u2014would typically require irreducible amounts of computation that are beyond the capabilities of any standard brain-like system.)</p>\n<h2 id=\"sensors-and-actuators\">Sensors and Actuators</h2>\n<p>We humans have raw inputs coming into our brains from a few million sensors distributed across our usual senses of touch, sight, hearing, taste and smell (together with balance, temperature, hunger, etc.). In most cases the detailed sensor inputs are not independent; in a typical visual scene, for example, neighboring pixels are highly correlated. And it doesn\u2019t seem to take many layers of neurons in our brains to distill our typical sensory experience from pure pieces of \u201craw data\u201d to what we might view as \u201cmore independent features\u201d. </p>\n<p>Of course there\u2019ll usually be much more in the raw data than just those features. But the \u201cfeatures\u201d typically correspond to aspects of the data that we\u2019ve \u201clearned are useful to us\u201d\u2014normally connected to pockets of computational reducibility that exist in the environment in which we operate. Are the features we pick out all we\u2019ll ever need? In the end, we typically want to derive a small stream of decisions or actions from all the data that comes in. But how many \u201cintermediate features\u201d do we need to get \u201cgood\u201d decisions or actions?</p>\n<p>That really depends on two things. First, what our decisions and actions are like. And second, what our raw data is like. Early in the history of our species, everything was just about \u201cindigenous human experience\u201d: what the natural world is like, and what we can do with our bodies. But as soon as we were dealing with technology, that changed. And in today\u2019s world we\u2019re constantly exposed, for example, to visual input that comes not from the natural world, but, say, from digital displays. </p>\n<p>And, yes, we often try to arrange our \u201cuser experience\u201d to align with what\u2019s familiar from the natural world (say by having objects that stay unchanged when they\u2019re moved across the screen). But it doesn\u2019t have to be that way. And indeed it\u2019s easy\u2014even with simple programs\u2014to generate for example visual images very different from what we\u2019re used to. And in many such cases, it\u2019s very hard for us to \u201ctell what\u2019s going on\u201d in the image. Sometimes it\u2019ll just \u201clook too complicated\u201d. Sometimes it\u2019ll seem like it has pieces we should recognize, but we don\u2019t:</p>\n<div>\n<div class=\"wolfram-c2c-wrapper writtings-c2c_above\"> <img alt=\"\" height=\"270\" src=\"https://content.wolfram.com/sites/43/2025/05/sw05192025sensorsimg1.png\" title=\"\" width=\"675\" /> </div>\n</p></div>\n<p>When it\u2019s \u201cjust too complicated\u201d, that\u2019s often a reflection of computational irreducibility. But when there are pieces we might \u201cthink we should recognize\u201d, that can be a reflection of pockets of reducibility we\u2019re just not familiar with. If we imagine a space of possible images\u2014as we can readily produce with generative AI\u2014there will be some that correspond to concepts (and words) we\u2019re familiar with. But the vast majority will effectively lie in \u201c<a href=\"https://writings.stephenwolfram.com/2023/07/generative-ai-space-and-the-mental-imagery-of-alien-minds/\">interconcept space</a>\u201d: places where we could have concepts, but don\u2019t, at least yet:</p>\n<div>\n<div class=\"wolfram-c2c-wrapper writtings-c2c_above\"> <img alt=\"\" height=\"220\" src=\"https://content.wolfram.com/sites/43/2025/05/sw05192025sensorsimg2a.png\" title=\"\" width=\"684\" /> </div>\n</p></div>\n<p>So what could bigger brains do with all this? Potentially they could handle more features, and more concepts. Full computational irreducibility will always in effect ultimately overpower them. But when it comes to handling pockets of reducibility, they\u2019ll presumably be able to deal with more of them. So in the end, it\u2019s very much as one might expect: a bigger brain should be able to track more things going on, \u201csee more details\u201d, etc. </p>\n<p>Brains of our size seem like they are in effect sufficient for \u201cindigenous human experience\u201d. But with technology in the picture, it\u2019s perfectly possible to \u201coverload\u201d them. (Needless to say, technology\u2014in the form of filtering, data analysis, etc.\u2014can also reduce that overload, in effect taking raw input and bringing our actual experience of it closer to something \u201cindigenous\u201d.)</p>\n<p>It\u2019s worth pointing out that while two brains of a given size might be able to \u201cdeal with the same number of features or concepts\u201d, those features or concepts might be different. One brain might have learned to talk about the world in terms of one set of primitives (such as certain basic colors); another in terms of a different set of primitives. But if both brains are sampling \u201cindigenous human experience\u201d in similar environments one can expect that it should be possible to translate between these descriptions\u2014just as it is generally possible to translate between things said in different human languages.</p>\n<p>But what if the brains are effectively sampling \u201cdifferent slices of reality\u201d? What if one\u2019s using technology to convert different physical phenomena to forms (like images) that we can \u201cindigenously\u201d handle? Perhaps we\u2019re sensing different electromagnetic frequencies; perhaps we\u2019re sensing molecular or chemical properties; perhaps we\u2019re sensing something like fluid motion. The kinds of features that will be \u201cuseful\u201d may be quite different in these different modalities. Indeed, even something as seemingly basic as the notion of an \u201cobject\u201d may not be so relevant if our sensory experience is effectively of continuous fluid motion.</p>\n<p>But in the end, what\u2019s \u201cuseful\u201d will depend on what we can do. And once again, it depends on whether we\u2019re dealing with \u201cpure humans\u201d (who can\u2019t, for example, move like octopuses) or with humans \u201caugmented by technology\u201d. And here we start to see an issue that relates to the basic capabilities of our brains.</p>\n<p>As \u201cpure humans\u201d, we have certain \u201cactuators\u201d (basically in the form of muscles) that we can \u201cindigenously\u201d operate. But with technology it\u2019s perfectly possible for us to use quite different actuators in quite different configurations. And as a practical matter, with brains like ours, we may not be able to make them work.</p>\n<p>For example, while humans can control helicopters, they never managed to control quadcopters\u2014at least not <a href=\"https://writings.stephenwolfram.com/2012/10/kids-arduinos-and-quadricopters/\">until digital flight controllers could do most of the work</a>. In a sense there were just too many degrees of freedom for brains like ours to deal with. Should bigger brains be able to do more? One would think so. And indeed one could imagine testing this with artificial neural nets. In millipedes, for example, their actual brains seem to support only a couple of <a href=\"https://www.wolframscience.com/nksonline/page-1011f/\">patterns of motion of their legs</a> (roughly, same phase vs. opposite phase). But one could imagine that with a bigger brain, all sorts of other patterns would become possible. </p>\n<p>Ultimately, there are two issues at stake here. The first is having a brain be able to \u201cindependently address\u201d enough actuators, or in effect enough degrees of freedom. The second is having a brain be able to control those degrees of freedom. And for example with mechanical degrees of freedom there are again essentially issues of computational irreducibility. Looking at the space of possible configurations\u2014say of millipede legs\u2014does one effectively just have to trace the path to find out if, and how, <a href=\"https://writings.stephenwolfram.com/2022/06/games-and-puzzles-as-multicomputational-systems/\">one can get from one configuration to another</a>? Or are there instead pockets of reducibility, associated with regularities in the space of configurations, that let one \u201cjump ahead\u201d and figure this out without tracing all the steps? It\u2019s those pockets of reducibility that brains can potentially make use of.</p>\n<p>When it comes to our everyday \u201cindigenous\u201d experience of the world, we are used to certain kinds of computational reducibility, associated for example with familiar natural laws, say about motion of objects. But what if we were dealing with different experiences, <a href=\"https://www.wolframscience.com/nks/notes-10-13--biological-forms-of-perception/\">associated with different senses</a>? </p>\n<p>For example, imagine (as with dogs) that our sense of smell was better developed than our sense of sight\u2014as reflected by more nerves coming into our brains from our noses than our eyes. Our description of the world would then be quite different, based for example not on geometry revealed by the line-of-sight arrival of light, but instead by the delivery of odors through fluid motion and diffusion\u2014not to mention the probably-several-hundred-dimensional space of odors, compared to the red, green, blue space of colors. Once again there would be features that could be identified, and \u201cconcepts\u201d that could be defined. But those might only be useful in an environment \u201cbuilt for smell\u201d rather than one \u201cbuilt for sight\u201d. </p>\n<p>And in the end, how many concepts would be useful? I don\u2019t think we have any way to know. But it certainly seems as if one can be a successful \u201csmell-based animal\u201d with a smaller brain (presumably supporting fewer concepts) than one needs as a successful \u201csight-based animal\u201d. </p>\n<p>One feature of \u201cnatural senses\u201d is that they tend to be spatially localized: an animal basically senses things only where it is. (We\u2019ll discuss the case of social organisms later.) But what if we had access to a distributed array of sensors\u2014say associated with IoT devices? The \u201ceffective laws of nature\u201d that one could perceive would then be different. Maybe there would be regularities that could be captured by a small number of concepts, but it seems more likely that the story would be more complicated, and that in effect one would \u201cneed a bigger brain\u201d to be able to keep track of what\u2019s going on, and make use of whatever pockets of reducibility might exist.</p>\n<p>There are somewhat similar issues if one imagines changing the timescales for sensory input. Our perception of space, for example, depends on the fact that light travels fast enough that in the milliseconds it takes our brain to register the input, we\u2019ve already received light from everything that\u2019s around us. But if our brains operated a million times faster (as digital electronics does) we\u2019d instead be registering individual photons. And while our brains might aggregate these to something like what we ordinarily perceive, there may be all sorts of other (e.g. quantum optics) effects that would be more obvious. </p>\n<h2 id=\"abstraction\">Abstraction</h2>\n<p>The more abstractly we try to think, the harder it seems to get. But would it get easier if we had bigger brains? And might there perhaps be fundamentally <a href=\"https://writings.stephenwolfram.com/2025/01/who-can-understand-the-proof-a-window-on-formalized-mathematics/#what-about-a-higher-level-abstraction\">higher levels of abstraction</a> that we could reach\u2014but only if we had bigger brains. </p>\n<p>As a way to approach such questions, let\u2019s begin by talking a bit about the history of the phenomenon of abstraction. We might already say that basic perception involves some abstraction, capturing as it does a filtered version of the world as it actually is. But perhaps we reach a different level when we start to ask \u201cwhat if?\u201d questions, and to imagine how things in the world could be different than they are. </p>\n<p>But somehow when it comes to us humans, it seems as if the greatest early leap in abstraction was the invention of language, and the explicit delineation of concepts that could be quite far from our direct experience. The earliest written records tend to be rather matter of fact, mostly recording as they do events and transactions. But already there are plenty of signs of abstraction. <a href=\"https://writings.stephenwolfram.com/2021/05/how-inevitable-is-the-concept-of-numbers/\">Numbers independent of what they count.</a> Things that should happen in the future. The concept of money. </p>\n<p>There seems to be a certain pattern to the development of abstraction. One notices that some category of things one sees many times can be considered similar, then one \u201cpackages these up\u201d into a concept, often described by a word. And in many cases, there\u2019s a certain kind of self amplification: once one has a word for something (as a modern example, say \u201cblog\u201d), it becomes easier for us to think about the thing, and we tend to see it or make it more often in the world around us. But what really makes abstraction take off is when we start building a whole tower of it, with one abstract concept recursively being based on others.</p>\n<p>Historically this began quite slowly. And perhaps it was seen first in theology. There were glimmerings of it in things like early (syllogistic) logic, in which one started to be able to talk about the form of arguments, independent of their particulars. And then there was mathematics, where computations could be done just in terms of numbers, independent of where those numbers came from. And, yes, while there were tables of \u201craw computational results\u201d, numbers were usually discussed in terms of what they were numbers of. And indeed when it came to things like measures of weight, it took until surprisingly modern times for there to be an absolute, abstract notion of weight, independent of whether it was a weight of figs or of wool. </p>\n<p>The development of algebra in the early modern period can be considered an important step forward in abstraction. Now there were formulas that could be manipulated abstractly, without even knowing what particular numbers <em>x</em> stood for. But it would probably be fair to say that there was a <a href=\"https://writings.stephenwolfram.com/2022/03/the-physicalization-of-metamathematics-and-its-implications-for-the-foundations-of-mathematics/#some-historical-and-philosophical-background\">major acceleration in abstraction in the 19th century</a>\u2014with the <a href=\"https://writings.stephenwolfram.com/2020/12/combinators-and-the-story-of-computation/#what-is-mathematics-and-logic-made-of\">development of formal systems</a> that could be discussed in \u201cpurely symbolic form\u201d independent of what they might (or might not) \u201cactually represent\u201d. </p>\n<p>And it was from this tradition that <a href=\"https://www.wolframscience.com/nks/notes-11-3--history-of-universality/\">modern notions of computation emerged</a> (and indeed particularly ones associated with symbolic computation that I personally have extensively used). But the most obvious area in which towers of abstraction have been built is mathematics. One might start with numbers (that could count things). But soon one\u2019s on to variables, functions, spaces of functions, category theory\u2014and a zillion other constructs that abstractly build on each other.</p>\n<p>The great value of abstraction is that it allows one to think about large classes of things all at once, instead of each separately. But how do those abstract concepts fit together? The issue is that often it\u2019s in a way that\u2019s very remote from anything about which we have direct experience from our raw perception of the world. Yes, we can define concepts about transfinite numbers or higher categories. But they don\u2019t immediately relate to anything we\u2019re familiar with from our everyday experience. </p>\n<p>As a practical matter one can often get a sense of how high something is on the tower of abstraction by seeing how much one has to explain to build up to it from \u201craw experiential concepts\u201d. Just sometimes it turns out that actually, once one hears about a certain seemingly \u201chighly abstract\u201d concept, one can actually explain it surprisingly simply, without going through the whole historical chain that led to it. (A notable example of this is the concept of universal computation\u2014which arose remarkably late in human intellectual history, but is now quite easy to explain, albeit particularly given its actual widespread embodiment in technology.) But the more common case is that there\u2019s no choice but to explain a whole tower of concepts.</p>\n<p>At least in my experience, however, when one actually thinks about \u201chighly abstract\u201d things, one does it by making analogies to more familiar, more concrete things. The analogies may not be perfect, but they provide scaffolding which allows our brains to take what would otherwise be quite inaccessible steps. </p>\n<p>At some level any abstraction is a reflection of a pocket of computational reducibility. Because if a useful abstraction can be defined, what it means is that it\u2019s possible to say something in a \u201csummarized\u201d or reduced way, in effect \u201cjumping ahead\u201d, without going through all the computational steps or engaging with all the details. And one can then think of towers of abstraction as being like networks of pockets of computational reducibility. But, yes, it can be hard to navigate these. </p>\n<p>Underneath, there\u2019s lots of computational irreducibility. And if one is prepared to \u201cgo through all the steps\u201d one can often \u201cget to an answer\u201d without all the \u201cconceptual difficulty\u201d of complex abstractions. But while computers can often readily \u201cgo through all the steps\u201d, brains can\u2019t. And that\u2019s in a sense why we have to use abstraction. But inevitably, even if we\u2019re using abstraction, and the pockets of computational reducibility associated with it, there\u2019ll be shadows of the computational irreducibility underneath. And in particular, if we try to \u201cexplore everything\u201d, our network of pockets of reducibility will inevitably \u201cget complicated\u201d, and ultimately also be mired in computational irreducibility, albeit with \u201chigher-level\u201d constructs than in the computational irreducibility underneath. </p>\n<p>No finite brain will ever be able to \u201cgo all the way\u201d, but it starts to seem likely that a bigger brain will be able to \u201creach further\u201d in the network of abstraction. But what will it find there? How does the character of abstraction change when we take it further? We\u2019ll be able to discuss this a bit more concretely when we talk about computational language below. But perhaps the main thing to say now is that\u2014at least in my experience\u2014most higher abstractions don\u2019t feel as if they\u2019re \u201cstructurally different\u201d once one understands them. In other words, most of the time, it seems as if the same patterns of thought and reasoning that one\u2019s applied in many other places can be applied there too, just to different kinds of constructs. </p>\n<p>Sometimes, though, there seem to be exceptions. <a href=\"https://www.wolframscience.com/nks/chap-2--the-crucial-experiment#sect-2-2--the-need-for-a-new-intuition\">Shocks to intuition</a> that seem to separate what one\u2019s now thinking about from anything one\u2019s thought before. And, for example, for me this happened when I started <a href=\"https://www.wolframscience.com/nks/chap-2--the-crucial-experiment/\">looking broadly at the computational universe</a>. I had always assumed that simple rules would lead to simple behavior. But many years ago I discovered that in the computational universe this isn\u2019t true (hence computational irreducibility). And this led to a whole different paradigm for thinking about things. </p>\n<p>It feels a bit like in <a href=\"https://www.wolframscience.com/metamathematics/\">metamathematics</a>. Where one can imagine one type of abstraction associated with different constructs out of which to form theorems. But where somehow there\u2019s another level associated with different ways to build new theorems, or indeed whole spaces of theorems. Or to build proofs from proofs, or proofs from proofs of proofs, etc. But the remarkable thing is that there seems to be an ultimate construct that encompasses it all: <a href=\"https://writings.stephenwolfram.com/2021/11/the-concept-of-the-ruliad/\">the ruliad</a>. </p>\n<p>We can describe the ruliad as the entangled limit of all possible computations. But we can also describe it as the limit of all possible abstractions. And it seems to lie underneath all physical reality, as well as all possible mathematics, etc. But, we might ask, how do brains relate to it?</p>\n<p>Inevitably, it\u2019s full of computational irreducibility. And looked at as a whole, brains can\u2019t get far with it. But the key idea is to think about how brains as they are\u2014with all their various features and limitations\u2014will \u201cparse\u201d it. And what I\u2019ve argued is that what \u201cbrains as they are\u201d will perceive about the ruliad are the core laws of physics (and mathematics) as we know them. In other words, it\u2019s because brains are the way they are that we perceive the laws of physics that we perceive.</p>\n<p>Would it be different for bigger brains? Not if they\u2019re the \u201csame kind of brains\u201d. Because <a href=\"https://writings.stephenwolfram.com/2023/12/observer-theory/\">what seems to matter for the core laws of physics are really just two properties of observers</a>. First, that they\u2019re computationally bounded. And second, that they believe they are persistent in time, and have a single thread of experience through time. And both of these seem to be core features of what makes brains \u201cbrain-like\u201d, rather than just arbitrary computational systems. </p>\n<p>It\u2019s a remarkable thing that just these features are sufficient to make core laws of physics inevitable. But if we want to understand more about the physics we\u2019ve constructed\u2014and the laws we\u2019ve deduced\u2014we probably have to understand more about what we\u2019re like as observers. And indeed, <a href=\"https://writings.stephenwolfram.com/2023/12/observer-theory/#what-we-assume-about-ourselves\">as I&#8217;ve argued elsewhere</a>, even our physical scale (much bigger than molecules, much smaller than the whole universe) is for example important in giving us the particular experience (and laws) of physics that we have. </p>\n<p>Would this be different with bigger brains? Perhaps a little. But anything that something brain-like can do pales in comparison to the computational irreducibility that exists in the ruliad and in the natural world. Nevertheless, with every new pocket of computational reducibility that\u2019s reached we get some new abstraction about the world, or in effect, some new law about how the world works. </p>\n<p>And as a practical matter, each such abstraction can allow us to build a whole collection of new ways of thinking about the world, and making things in the world. It\u2019s challenging to trace this arc. Because in a sense it\u2019ll all be about \u201cthings we never thought to think about before\u201d. <a href=\"https://writings.stephenwolfram.com/2023/03/will-ais-take-all-our-jobs-and-end-human-history-or-not-well-its-complicated/#will-there-be-anything-left-for-the-humans-to-do\">Goals we might define for ourselves</a> that are built on a tower of abstraction, far away from what we might think of as \u201cindigenous human goals\u201d.</p>\n<p>It\u2019s important to realize that there won\u2019t just be one tower of abstraction that can be built. There\u2019ll inevitably be an infinite network of pockets of computational reducibility, with each path leading to a different specific tower of abstraction. And indeed the abstractions we have pursued reflect the particular arc of human intellectual history. Bigger brains\u2014or AIs\u2014have many possible directions they can go, each one defining a different path of history.</p>\n<p>One question to ask is to what extent reaching higher levels of abstraction is a matter of education, and to what extent it requires additional intrinsic capabilities of a brain. It is, I suspect, a mixture. Sometimes it\u2019s really just a question of knowing \u201cwhere that pocket of reducibility is\u201d, which is something we can learn from education. But sometimes it\u2019s a question of navigating a network of pockets, which may only be possible when brains reach a certain level of \u201ccomputational ability\u201d. </p>\n<p>There\u2019s another thing to discuss, <a href=\"https://writings.stephenwolfram.com/2023/03/will-ais-take-all-our-jobs-and-end-human-history-or-not-well-its-complicated/#preparing-for-an-ai-world\">related to education</a>. And that\u2019s the fact that over time, more and more \u201cdistinct pieces of knowledge\u201d get built up in our civilization. There was perhaps a time in history when a brain of our size could realistically commit to memory at least the basics of much of that knowledge. But today that time has long passed. Yes, abstraction in effect compresses what one needs to know. But the continual addition of new and seemingly important knowledge, across countless specialties, makes it impossible for brains of our size to keep up.</p>\n<p>Plenty of that knowledge is, though, quite siloed in different areas. But sometimes there are \u201cgrand analogies\u201d to make\u2014say <a href=\"https://writings.stephenwolfram.com/2024/12/foundations-of-biological-evolution-more-results-more-surprises/#fitness-functions-based-on-aspect-ratio\">pulling an idea from relativity theory and applying it to biological evolution</a>. In a sense such analogies reveal new abstractions\u2014but to make them requires knowledge that spans many different areas. And that\u2019s a place where bigger brains\u2014or AIs\u2014can potentially do something that\u2019s in a fundamental way \u201cbeyond us\u201d. </p>\n<p>Will there always be such \u201cgrand analogies\u201d to make? The general growth of knowledge is inevitably a computationally irreducible process. And within it there will inevitably be pockets of reducibility. But how often in practice will one actually encounter \u201clong-range connections\u201d across \u201cknowledge space\u201d? As a specific example one can look at metamathematics, where such connections are manifest in theorems that <a href=\"https://www.wolframscience.com/metamathematics/uniformity-and-motion-in-metamathematical-space/\">link seemingly different areas of mathematics</a>. And this example leads one to realize that at some deep level grand analogies are in a sense inevitable. In the context of the ruliad, one can think of different domains of knowledge as corresponding to different parts. But the nature of the ruliad\u2014encompassing as it does everything that is computationally possible\u2014inevitably imbues it with a certain homogeneity, which implies that (as the Principle of Computational Equivalence might suggest) there must ultimately be a correspondence between different areas. In practice, though, this correspondence may be at a very \u201catomic\u201d (or \u201cformal\u201d) level, far below the kinds of descriptions (based on pockets of reducibility) that we imagine brains normally use.</p>\n<p>But, OK, will it always take an \u201cexpanding brain\u201d to keep up with the \u201cexpanding knowledge\u201d we have? Computational irreducibility guarantees that there\u2019ll always in principle be \u201cnew knowledge\u201d to be had\u2014separated from what\u2019s come before by irreducible amounts of computation. But then there\u2019s the question of <a href=\"https://writings.stephenwolfram.com/2023/03/will-ais-take-all-our-jobs-and-end-human-history-or-not-well-its-complicated/\">whether in the end we&#8217;ll care about it</a>. After all, it could be that the knowledge we can add is so abstruse that it will never affect any practical decisions we have to make. And, yes, to some extent that\u2019s true (which is why only some tiny fraction of the Earth\u2019s population will care about what I\u2019m writing here). But another consequence of computational irreducibility is that there will always be \u201csurprises\u201d\u2014and those can eventually \u201cpush into focus\u201d even what at first seems like arbitrarily obscure knowledge. </p>\n<h2 id=\"computational-language\">Computational Language</h2>\n<p>Language in general\u2014and compositional language in particular\u2014is arguably the greatest invention of our species. But is it somehow \u201cthe top\u201d\u2014the highest possible representation of things? Or if, for example, we had bigger brains, is there something beyond it that we could reach? </p>\n<p>Well, in some very formal sense, yes, compositional language (at least in idealized form) is \u201cthe top\u201d. Because\u2014at least if it\u2019s allowed to include utterances of any length\u2014then in some sense it can in principle encode arbitrary, <a href=\"https://www.wolframscience.com/nks/chap-11--the-notion-of-computation/\">universal computations</a>. But this really isn\u2019t true in any useful sense\u2014and indeed to apply ordinary compositional language in this way would require doing computationally irreducible computations.</p>\n<p>So we return to the question of what might in practice lie beyond ordinary human language. I wondered about this for a long time. But in the end I realized that the most important clue is in a sense right in front of me: the <a href=\"https://writings.stephenwolfram.com/2019/05/what-weve-built-is-a-computational-language-and-thats-very-important/\">concept of computational language</a>, that I\u2019ve spent much of my life exploring. </p>\n<p>It\u2019s worth saying at the outset that the way computational language plays out for computers and for brains is somewhat different, and in some respects complementary. In computers you might specify something as a <a href=\"https://www.wolfram.com/language/\">Wolfram Language</a> symbolic expression, and then the \u201cmain action\u201d is to evaluate this expression, potentially running a long computation to find out what the expression evaluates to.</p>\n<p>Brains aren\u2019t set up to do long computations like this. For them a Wolfram Language expression is something to use in effect as a \u201crepresentation of a thought\u201d. (And, yes, that\u2019s an important distinction between the computational language concept of Wolfram Language, and standard \u201cprogramming languages\u201d, which are intended purely as a way to tell a computer what to do, not a way to represent thoughts.)</p>\n<p>So what kinds of thoughts can we readily represent in our computational language? There are ones involving explicit numbers, or mathematical expressions. There are ones involving cities and chemicals, and other real-world entities. But then there are higher-level ones, that in effect describe more abstract structures.</p>\n<p>For example, there\u2019s <tt><a href=\"http://reference.wolfram.com/language/ref/NestList.html\">NestList</a></tt>, which gives the result of nesting any operation, here named <em>f</em>:</p>\n<div>\n<div class=\"wolfram-c2c-wrapper writtings-c2c_above\"> <img alt=\"\" height=\"44\" src=\"https://content.wolfram.com/sites/43/2025/05/sw05192025computationalbimg1.png\" title=\"\" width=\"458\" /> </div>\n</p></div>\n<p>At the outset, it\u2019s not obvious that this would be a useful thing to do. But in fact it\u2019s a <a href=\"https://www.wolfram.com/language/elementary-introduction/3rd-ed/27-applying-functions-repeatedly.html\">very successful abstraction</a>: there are lots of functions <em>f</em> for which one wants to do this.</p>\n<p>In the development of ordinary human language, words tend to get introduced when they\u2019re useful, or, in other words, when they express things one often wants to express. But somehow in human language the words one gets tend to be more concrete. Maybe they describe something that directly happens to objects in the world. Maybe they describe our impression of a human mental state. Yes, one can make rather vague statements like \u201cI\u2019m going to do something to someone\u201d. But human language doesn\u2019t normally \u201cgo meta\u201d, doing things like <tt>NestList</tt> where one\u2019s saying that one wants to take some \u201cdirect statement\u201d and in effect \u201cwork with the statement\u201d. In some sense, human language tends to \u201cwork with data\u201d, applying a simple analog of code to it. Our computational language can \u201cwork with code\u201d as \u201craw material\u201d.</p>\n<p>One can think about this as a \u201chigher-order function\u201d: a function that operates not on data, but on functions. And one can keep going, dealing with functions that operate on functions that operate on functions, and so on. And at every level one is increasing the generality\u2014and abstraction\u2014at which one is working. There may be many specific functions (a bit analogous to verbs) that operate on data (a bit analogous to nouns). But when we talk about operating on functions themselves we can potentially have just a single function (like <tt>NestList</tt>) that operates, quite generally, on many functions. In ordinary language, we might call such things \u201cmetaverbs\u201d, but they aren\u2019t something that commonly occurs.</p>\n<p>But what makes them possible in computational language? Well, it\u2019s taking the computational paradigm seriously, and representing everything in computational terms: objects, actions, etc. In Wolfram Language, it\u2019s that we can represent <a href=\"https://www.wolfram.com/language/elementary-introduction/3rd-ed/33-expressions-and-their-structure.html\">everything as a symbolic expression</a>. Arrays of numbers (or countries, or whatever) are symbolic expressions. Graphics are symbolic expressions. Programs are symbolic expressions. And so on. </p>\n<p>And given this uniformity of representation it becomes feasible\u2014and natural\u2014to do higher-order operations, that in effect manipulate symbolic structure without being concerned about what the structure might represent. At some level we can view this as leading to the ultimate abstraction embodied in the ruliad, where in a sense \u201ceverything is pure structure\u201d. But in practice in Wolfram Language we try to <a href=\"https://writings.stephenwolfram.com/2010/10/the-poetry-of-function-naming/\">&#8220;anchor&#8221; what we&#8217;re doing to known concepts</a> from ordinary human language\u2014so that we use names for things (like <tt>NestList</tt>) that are derived from common English words.</p>\n<p>In some formal sense this isn\u2019t necessary. Everything can be \u201cpurely structural\u201d, as it is not only in the ruliad but also in <a href=\"https://writings.stephenwolfram.com/2020/12/combinators-a-centennial-view/\">constructs like combinators</a>, where, say, the operation of addition can be represented by:</p>\n<div>\n<div class=\"wolfram-c2c-wrapper writtings-c2c_above\"> <img alt=\"\" height=\"13\" src=\"https://content.wolfram.com/sites/43/2025/05/sw05192025computationalimg2.png\" title=\"\" width=\"232\" /> </div>\n</p></div>\n<p>Combinators have been around for more than a century. But they are almost <a href=\"https://writings.stephenwolfram.com/2020/12/combinators-and-the-story-of-computation/\">impenetrably difficult for most humans to understand</a>. Somehow they involve too much \u201cpure abstraction\u201d, not anchored to concepts we \u201chave a sense of\u201d in our brains. </p>\n<p>It\u2019s been interesting for me to observe over the years what it\u2019s taken for people (including myself) to come to terms with the kind of higher-order constructs that exist in the Wolfram Language. The typical pattern is that over the course of months or years one gets used to lots of specific cases. And only after that is one able\u2014often in the end rather quickly\u2014to \u201cget to the next level\u201d and start to use some generalized, higher-order construct. But normally one can in effect only \u201cgo one level at a time\u201d. After one groks one level of abstraction, that seems to have to \u201csettle\u201d for a while before one can go on to the next one.</p>\n<p>Somehow it seems as if one is gradually \u201cfeeling out\u201d a certain amount of computational irreducibility, to learn about a new pocket of reducibility, that one can eventually use to \u201cthink in terms of\u201d. </p>\n<p>Could \u201chaving a bigger brain\u201d speed this up? Maybe it\u2019d be useful to be able to remember more cases, and perhaps get more into \u201cworking memory\u201d. But I rather suspect that combinators, for example, are in some sense fundamentally beyond all brain-like systems. It\u2019s much as the Principle of Computational Equivalence suggests: one quickly \u201cascends\u201d to things that are as computationally sophisticated as anything\u2014and therefore inevitably involve computational irreducibility. There are only certain specific setups that remain within the computationally bounded domain that brain-like systems can deal with.</p>\n<p>Of course, even though they can\u2019t directly \u201crun code in their brains\u201d, humans\u2014and LLMs\u2014can perfectly well <a href=\"https://www.wolfram.com/resources/tools-for-AIs\">use Wolfram Language as a tool</a>, getting it to actually run computations. And this means they can readily \u201cobserve phenomena\u201d that are computationally irreducible. And indeed in the end it\u2019s very much the same kind of thing observing such phenomena in the abstract computational universe, and in the \u201creal\u201d physical universe. And the point is that in both cases, brain-like systems will pull out only certain features, essentially corresponding to pockets of computational reducibility. </p>\n<p>How do things like higher-order functions relate to this? At this point it\u2019s not completely clear. Presumably in at least some sense there are hierarchies of higher-order functions that capture certain kinds of regularities that can be thought of as associated with networks of computational reducibility. And it\u2019s conceivable that category theory and its higher-order generalizations are relevant here. In category theory one imagines applying sequences of functions (\u201cmorphisms\u201d) and it\u2019s a foundational assumption that the effect of any sequence of functions can also be represented by just a single function\u2014which seems tantamount to saying that one can always \u201cjump ahead\u201d, or in other words, that everything one\u2019s dealing with is computationally reducible. Higher-order category theory then effectively extends this to higher-order functions, but always with what seem like assumptions of computational reducibility.</p>\n<p>And, yes, this all seems highly abstract, and difficult to understand. But does it really need to be, or is there some way to \u201cbring it down\u201d to a level that\u2019s close to everyday human thinking? It\u2019s not clear. But in a sense the core art of computational language design (that I\u2019ve <a href=\"https://livestreams.stephenwolfram.com/category/live-ceoing/\">practiced so assiduously</a> for nearly half a century) is precisely to take things that at first might seem abstruse, and somehow cast them into an accessible form. And, yes, this is something that\u2019s about as intellectually challenging as anything\u2014because in a sense it involves continually trying to \u201cfigure out what\u2019s really going on\u201d, and in effect \u201cdrilling down\u201d to get to the foundations of everything. </p>\n<p>But, OK, when one gets there, how simple will things be? Part of that depends on how much computational irreducibility is left when one reaches what one considers to be \u201cthe foundations\u201d. And part in a sense depends on the extent to which one can \u201cfind a bridge\u201d between the foundations and something that\u2019s familiar. Of course, what\u2019s \u201cfamiliar\u201d can change. And indeed over the four decades that I\u2019ve been developing the Wolfram Language quite a few things (particularly in areas like functional programming) that at first seemed abstruse and unfamiliar have begun to seem more familiar. And, yes, it\u2019s taken the collective development and dissemination of the relevant ideas to achieve that. But now it \u201cjust takes education\u201d; it doesn\u2019t \u201ctake a bigger brain\u201d to deal with these things.</p>\n<p>One of the core features of the Wolfram Language is that it represents everything as a symbolic expression. And, yes, symbolic expressions are formally able to represent any kind of computational structure. But beyond that, the important point is that they\u2019re somehow set up to <a href=\"https://www.wolframscience.com/nks/chap-10--processes-of-perception-and-analysis#sect-10-12--human-thinking\">be a match for how brains work</a>. </p>\n<p>And in particular, symbolic expressions can be thought of \u201cgrammatically\u201d as consisting of nested functions that form a tree-like structure; effectively a more precise version of the typical kind of grammar that we find in human language. And, yes, just as we manage to understand and generate human language with a limited working memory, so (at least at the grammatical level) we can do the same thing with computational language. In other words, in dealing with Wolfram Language we\u2019re leveraging our faculties with human language. And that\u2019s why Wolfram Language can serve as such an effective bridge between the way we think about things, and what\u2019s computationally possible.</p>\n<p>But symbolic expressions represented as trees aren\u2019t the only conceivable structures. It\u2019s also possible to have symbolic expressions where the elements are nodes on a graph, and the graph can even have loops in it. Or one can go further, and start talking, for example, about the <a href=\"https://writings.stephenwolfram.com/2020/04/finally-we-may-have-a-path-to-the-fundamental-theory-of-physics-and-its-beautiful/#what-is-space\">hypergraphs that appear in our Physics Project</a>. But the point is that brain-like systems have a hard time processing such structures. Because to keep track of what\u2019s going on they in a sense have to keep track of multiple \u201cthreads of thought\u201d. And that\u2019s not something individual brain-like systems as we current envision them can do. </p>\n<h2 id=\"many-brains-together-the-formation-of-society\">Many Brains Together: The Formation of Society</h2>\n<p>As we\u2019ve discussed several times here, it seems to be a key feature of brains that they create a single \u201cthread of experience\u201d. But what would it be like to have multiple threads? Well, we actually have a very familiar example of that: what happens when we have a whole collection of people (or other animals).</p>\n<p>One could imagine that biological evolution might have produced animals whose brains maintain multiple simultaneous threads of experience. But somehow it has ended up instead restricting each animal to just one thread of experience\u2014and getting multiple threads by having multiple animals. (Conceivably creatures like octopuses may actually in some sense support multiple threads within one organism.) </p>\n<p>Within a single brain it seems important to always \u201ccome to a single, definite conclusion\u201d\u2014say to determine where an animal will \u201cmove next\u201d. But what about in a collection of organisms? Well, there\u2019s still some kind of coordination that will be important to the fitness of the whole population\u2014perhaps even something as direct as moving together as a herd or flock. And in a sense, just as all those different neuron firings in one brain get collected to determine a \u201cfinal conclusion for what to do\u201d, so similarly the conclusions of many different brains have to be <a href=\"https://writings.stephenwolfram.com/2023/03/will-ais-take-all-our-jobs-and-end-human-history-or-not-well-its-complicated/#governance-in-an-ai-world\">collected to determine a coordinated outcome</a>.</p>\n<p>But how can a coordinated outcome arise? Well, there has to be communication of some sort between organisms. Sometimes it\u2019s rather passive (just watch what your neighbor in a herd or flock does). Sometimes it\u2019s something more elaborate and active\u2014like language. But is that the best one can do? One might imagine that there could be some kind of \u201ctelepathic coordination\u201d, in which the raw pattern of neuron firings is communicated from one brain to another. But as we\u2019ve argued, such communication cannot be expected to be robust. To achieve robustness, one must \u201cpackage up\u201d all the internal details into some standardized form of communication (words, roars, calls, etc.) that one can expect can be \u201cfaithfully unpacked\u201d and in effect \u201cunderstood\u201d by other, suitably similar brains. </p>\n<p>But it\u2019s important to realize that the very possibility of such standardized communication in effect requires coordination. Because somehow what goes on in one brain has to be aligned with what goes on in another. And indeed the way that\u2019s maintained is precisely through continual communication. </p>\n<p>So, OK, how might bigger brains affect this? One possibility is that they might enable more complex social structures. There are plenty of animals with fairly small brains that successfully form \u201call do the same thing\u201d flocks, herds and the like. But the larger brains of primates seem to allow more complex \u201ctribal\u201d structures. Could having a bigger brain let one successfully maintain a larger social structure, in effect remembering and handling larger numbers of social connections? Or could the actual forms of these connections be more complex? While human social connections seem to be at least roughly captured by social networks represented as ordinary graphs, maybe bigger brains would for example routinely require hypergraphs.</p>\n<p>But in general we can say that language\u2014or standardized communication of some form\u2014is deeply connected to the existence of a \u201ccoherent society\u201d. For without being able to exchange something like language there\u2019s no way to align the members of a potential society. And without coherence between members something like language won\u2019t be useful. </p>\n<p>As in so many other situations, one can expect that the detailed interactions between members of a society will show all sorts of computational irreducibility. And insofar as one can identify \u201cthe will of society\u201d (or, for that matter, the \u201ctide of history\u201d), it represents a pocket of computational reducibility in the system.</p>\n<p>In human society there is a considerable tendency (though it\u2019s often not successful) to try to maintain a single \u201cthread of society\u201d, in which, at some level, everyone is supposed to act more or less the same. And certainly that\u2019s an important simplifying feature in allowing brains like ours to \u201cnavigate the social world\u201d. Could bigger brains do something more sophisticated? As in other areas, one can imagine a whole network of regularities (or pockets of reducibility) in the structure of society, perhaps connected to a whole tower of \u201chigher-order social abstractions\u201d, that only brains bigger than ours can comfortably deal with. (\u201cJust being friends\u201d might be a story for the \u201csmall brained\u201d. With bigger brains one might instead have patterns of dependence and connectivity that can only be represented in complicated graph theoretic ways.)</p>\n<h2 id=\"minds-beyond-ours\">Minds beyond Ours</h2>\n<p>We humans have a tremendous tendency to think\u2014or at least hope\u2014that our minds are somehow \u201cat the top\u201d of what\u2019s possible. But with what we know now about computation and how it operates in the natural world it\u2019s pretty <a href=\"https://www.wolframscience.com/nks/chap-12--the-principle-of-computational-equivalence#sect-12-12--historical-perspectives\">clear this isn&#8217;t true</a>. And indeed it seems as if it\u2019s precisely a limitation in the \u201ccomputational architecture\u201d of our minds\u2014and brains\u2014that leads to that most cherished feature of our existence that we <a href=\"https://writings.stephenwolfram.com/2021/03/what-is-consciousness-some-new-perspectives-from-our-physics-project/\">characterize as \u201cconscious experience\u201d</a>. </p>\n<p>In the natural world at large, computation is in some sense happening quite uniformly, everywhere. But our brains seem to be set up to do computation in a more directed and more limited way\u2014taking in large amounts of sensory data, but then filtering it down to a small stream of actions to take. And, yes, one can remove this \u201climitation\u201d. And while the result may lead to more computation getting done, it doesn\u2019t lead to something that\u2019s \u201ca mind like ours\u201d.</p>\n<p>And indeed in what we\u2019ve done here, we\u2019ve tended to be very conservative in how we imagine \u201cextending our minds\u201d. We\u2019ve mostly just considered what might happen if our brains were scaled up to have more neurons, while basically maintaining the same structure. (And, yes, animals physically bigger than us already have larger brains\u2014as did Neanderthals\u2014but what we really need to look at is size of brain relative to size of the animal, or, in effect \u201camount of brain for a given amount of sensory input\u201d.)</p>\n<p>A certain amount about what happens with different scales of brains is already fairly clear from looking at different kinds of animals, and at things like their apparent lack of human-like language. But now that we have artificial neural nets that do remarkably human-like things we\u2019re in a position to get a more systematic sense of what different scales of \u201cbrains\u201d can do. And indeed we\u2019ve seen a sequence of \u201ccapability thresholds\u201d passed as neural nets get larger.</p>\n<p>So what will bigger brains be able to do? What\u2019s fairly straightforward is that they\u2019ll presumably be able to take larger amounts of sensory input, and generate larger amounts of output. (And, yes, the sensory input could come from existing modalities, or new ones, and the outputs could go to existing \u201cactuators\u201d, or new ones.) As a practical matter, the more \u201cdata\u201d that has to be processed for a brain to \u201ccome to a decision\u201d and generate an output, the slower it\u2019ll probably be. But as brains get bigger, so presumably will the size of their working memory\u2014as well as the number of distinct \u201cconcepts\u201d they can \u201cdistinguish\u201d and \u201cremember\u201d. </p>\n<p>If the same overall architecture is maintained, there\u2019ll still be just a single \u201cthread of experience\u201d, associated with a single \u201cthread of communication\u201d, or a single \u201cstream of tokens\u201d. At the size of brains we have, we can deal with compositional language in which \u201cconcepts\u201d (represented, basically, as words) can have at least a certain depth of qualifiers (corresponding, say, to adjectival phrases). As brain size increases, we can expect there can both be more \u201craw concepts\u201d\u2014allowing fewer qualifiers\u2014as well as more working memory to deal with more deeply nested qualifiers. </p>\n<p>But is there something qualitatively different that can happen with bigger brains? Computational language (and particularly my experience with the Wolfram Language) gives some indications, the most notable of which is the idea of \u201cgoing meta\u201d and using \u201chigher-order constructs\u201d. Instead of, say, operating directly on \u201craw concepts\u201d with (say, \u201cverb-like\u201d) \u201cfunctions\u201d, we can imagine higher-order functions that operate on functions themselves. And, yes, this is something of which we see powerful examples in the Wolfram Language. But it feels as if we could somehow go further\u2014and make this more routine\u2014if our brains in a sense had \u201cmore capacity\u201d.</p>\n<p>To \u201cgo meta\u201d and \u201cuse higher-order constructs\u201d is in effect a story of abstraction\u2014and of taking many disparate things and abstracting to the point where one can \u201ctalk about them all together\u201d. The world at large is full of complexity\u2014and computational irreducibility. But in essence what makes \u201cminds like ours\u201d possible is that there are pockets of computational reducibility to be found. And those pockets of reducibility are closely related to being able to successfully do abstraction. And as we build up towers of abstraction we are in effect navigating through networks of pockets of computational reducibility. </p>\n<p>The progress of knowledge\u2014and the fact that we\u2019re educated about it\u2014lets us get to a certain level of abstraction. And, one suspects, the more capacity there is in a brain, the further it will be able to go. </p>\n<p>But where will it \u201cwant to go\u201d? The world at large\u2014full as it is with computational irreducibility, along with infinite numbers of pockets of reducibility\u2014leaves infinite possibilities. And it is largely the coincidence of our particular history that defines the path we have taken. </p>\n<p>We often identify our \u201csense of purpose\u201d with the path we will take. And perhaps the definiteness of our belief in purpose is related to the particular feature of brains that leads us to concentrate \u201ceverything we\u2019re thinking\u201d down into just a single stream of decisions and action. </p>\n<p>And, yes, as we\u2019ve discussed, one could in principle imagine \u201cmultiway minds\u201d with multiple \u201cthreads of consciousness\u201d operating at once. But we humans (and individual animals in general) don\u2019t seem to have those. Of course, in collections of humans (or other animals) there are still inevitably multiple \u201cthreads of consciousness\u201d \u2014and it\u2019s things like language that \u201cknit together\u201d those threads to, for example, make a coherent society.</p>\n<p>Quite what that \u201cknitting\u201d looks like might change as we scale up the size of brains. And so, for example, with bigger brains we might be able to deal with \u201chigher-order social structures\u201d that would seem alien and incomprehensible to us today.</p>\n<p>So what would it be like to interact with a \u201cbigger brain\u201d? Inside, that brain might effectively use many more words and concepts than we know. But presumably it could generate at least a rough (\u201cexplain-like-I\u2019m-5\u201d) approximation that we\u2019d be able to understand. There might well be all sorts of abstractions and \u201chigher-order constructs\u201d that we are basically blind to. And, yes, one is reminded of something like a dog listening to a human conversation about philosophy\u2014and catching only the occasional \u201csit\u201d or \u201cfetch\u201d word. </p>\n<p>As we\u2019ve discussed several times here, if we remove our restriction to \u201cbrain-like\u201d operation (and in particular to deriving a small stream of decisions from large amounts of sensory input) we\u2019re thrown into the domain of general computation, where computational irreducibility is rampant, and we can\u2019t in general expect to say much about what\u2019s going on. But if we maintain \u201cbrain-like operation\u201d, we\u2019re instead in effect navigating through \u201cnetworks of computational reducibility\u201d, and we can expect to talk about things like concepts, language and towers of abstraction.</p>\n<p>From a foundational point of view, we can imagine any mind as in effect being at a particular place in <a href=\"https://writings.stephenwolfram.com/2021/11/the-concept-of-the-ruliad/\">the ruliad</a>. When minds communicate, they are effectively exchanging the rulial analog of particles\u2014robust concepts that are somehow unchanged as they propagate within the ruliad. So what would happen if we had bigger brains? In a sense it\u2019s a surprisingly \u201cmechanical\u201d story: a bigger brain\u2014encompassing more concepts, etc.\u2014in effect just occupies a larger region of rulial space. And the presence of abstraction\u2014perhaps learned from a whole arc of intellectual history\u2014can lead to more expansion in rulial space. </p>\n<p>And in the end it seems that \u201cminds beyond ours\u201d can be characterized by how large the regions of the ruliad they occupy are. (Such minds are, in some very literal rulial sense, more \u201cbroad minded\u201d.) So what is the limit of all this? Ultimately, it\u2019s a \u201cmind\u201d that spans the whole ruliad, and in effect incorporates all possible computations. But in some fundamental sense this is not a mind like ours, not least because by \u201cbeing everything\u201d it \u201cbecomes nothing\u201d\u2014and one can no longer identify it as having a coherent \u201cthread of individual existence\u201d.</p>\n<p>And, yes, the overall thrust of what we\u2019ve been saying applies just as well to \u201cAI minds\u201d as to biological ones. If we remove restrictions like being set up to generate the next token, we\u2019ll be left with a neural net that\u2019s just \u201cdoing computation\u201d, with no obvious \u201cmind-like purpose\u201d in sight. But if we make neural nets do typical \u201cbrain-like\u201d tasks, then we can expect that they too will find and navigate pockets of reducibility. We may well not recognize what they\u2019re doing. But insofar as we can, then inevitably we\u2019ll mostly be sampling the parts of \u201cminds beyond ours\u201d that are aligned with \u201cminds like ours\u201d. And it\u2019ll take progress in our whole human intellectual edifice to be able to fully appreciate what it is that minds beyond ours can do.</p>\n<p style=\"font-size: 90%;\"><i>Thanks for recent discussions about topics covered here in particular to Richard Assar, Joscha Bach, Kovas Boguta, Thomas Dullien, Dugan Hammock, Christopher Lord, Fred Meinberg, Nora Popescu, Philip Rosedale, Terry Sejnowski, Hikari Sorensen, and James Wiles.</i></p>"
            ],
            "link": "https://writings.stephenwolfram.com/2025/05/what-if-we-had-bigger-brains-imagining-minds-beyond-ours/",
            "publishedAt": "2025-05-21",
            "source": "Stephen Wolfram",
            "summary": "<span class=\"thumbnail\"><img alt=\"\" class=\"attachment-thumbnail size-thumbnail wp-post-image\" height=\"108\" src=\"https://content.wolfram.com/sites/43/2025/05/imaginingbiggerbrains-news.png\" width=\"128\" /></span>Cats Don\u2019t Talk We humans have perhaps 100 billion neurons in our brains. But what if we had many more? Or what if the AIs we built effectively had many more? What kinds of things might then become possible? At 100 billion neurons, we know, for example, that compositional language of the kind we humans [&#8230;]",
            "title": "What If We Had Bigger Brains? Imagining Minds beyond Ours"
        },
        {
            "content": [],
            "link": "https://xkcd.com/3092/",
            "publishedAt": "2025-05-21",
            "source": "XKCD",
            "summary": "<img alt=\"169 is a baker's gross.\" src=\"https://imgs.xkcd.com/comics/bakers_units.png\" title=\"169 is a baker's gross.\" />",
            "title": "Baker's Units"
        }
    ],
    "lookbackDays": 1,
    "publishDate": "2025-05-21"
}