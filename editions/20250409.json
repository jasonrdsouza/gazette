{
    "articles": [
        {
            "content": [
                "<div class=\"captioned-image-container\"><figure><a class=\"image-link image2 is-viewable-img\" href=\"https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc21b6564-f459-463a-98e9-ee5555e9bdcb_1038x864.jpeg\" target=\"_blank\"><div class=\"image2-inset\"><source type=\"image/webp\" /><img alt=\"\" class=\"sizing-normal\" height=\"864\" src=\"https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc21b6564-f459-463a-98e9-ee5555e9bdcb_1038x864.jpeg\" width=\"1038\" /><div class=\"image-link-expand\"><div class=\"pencraft pc-display-flex pc-gap-8 pc-reset\"><div class=\"pencraft pc-reset icon-container restack-image\"><svg class=\"lucide lucide-refresh-cw\" fill=\"none\" height=\"20\" stroke=\"currentColor\" stroke-linecap=\"round\" stroke-linejoin=\"round\" stroke-width=\"2\" viewBox=\"0 0 24 24\" width=\"20\" xmlns=\"http://www.w3.org/2000/svg\"><path d=\"M3 12a9 9 0 0 1 9-9 9.75 9.75 0 0 1 6.74 2.74L21 8\"></path><path d=\"M21 3v5h-5\"></path><path d=\"M21 12a9 9 0 0 1-9 9 9.75 9.75 0 0 1-6.74-2.74L3 16\"></path><path d=\"M8 16H3v5\"></path></svg></div><div class=\"pencraft pc-reset icon-container view-image\"><svg class=\"lucide lucide-maximize2\" fill=\"none\" height=\"20\" stroke=\"currentColor\" stroke-linecap=\"round\" stroke-linejoin=\"round\" stroke-width=\"2\" viewBox=\"0 0 24 24\" width=\"20\" xmlns=\"http://www.w3.org/2000/svg\"><polyline points=\"15 3 21 3 21 9\"></polyline><polyline points=\"9 21 3 21 3 15\"></polyline><line x1=\"21\" x2=\"14\" y1=\"3\" y2=\"10\"></line><line x1=\"3\" x2=\"10\" y1=\"21\" y2=\"14\"></line></svg></div></div></div></div></a></figure></div><h6>Loveknot (1998), Dorothea Tanning</h6><p></p><p></p><p>For a few weeks last month, I fell into the habit of viewing myself as inadequately ambitious. I lamented that I refuse to take advantage of my current successes in order to create bigger, more popular businesses. &#8220;There are so many adventures I could be having,&#8221; I said to my husband. &#8220;Instead, I insist on making pottery&#8230;</p>\n      <p>\n          <a href=\"https://www.ask-polly.com/p/do-you-prioritize-status-over-joy\">\n              Read more\n          </a>\n      </p>"
            ],
            "link": "https://www.ask-polly.com/p/do-you-prioritize-status-over-joy",
            "publishedAt": "2025-04-07",
            "source": "Ask Polly",
            "summary": "<div class=\"captioned-image-container\"><figure><a class=\"image-link image2 is-viewable-img\" href=\"https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc21b6564-f459-463a-98e9-ee5555e9bdcb_1038x864.jpeg\" target=\"_blank\"><div class=\"image2-inset\"><source type=\"image/webp\" /><img alt=\"\" class=\"sizing-normal\" height=\"864\" src=\"https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc21b6564-f459-463a-98e9-ee5555e9bdcb_1038x864.jpeg\" width=\"1038\" /><div class=\"image-link-expand\"><div class=\"pencraft pc-display-flex pc-gap-8 pc-reset\"><div class=\"pencraft pc-reset icon-container restack-image\"><svg class=\"lucide lucide-refresh-cw\" fill=\"none\" height=\"20\" stroke=\"currentColor\" stroke-linecap=\"round\" stroke-linejoin=\"round\" stroke-width=\"2\" viewBox=\"0 0 24 24\" width=\"20\" xmlns=\"http://www.w3.org/2000/svg\"><path d=\"M3 12a9 9 0 0 1 9-9 9.75 9.75 0 0 1 6.74 2.74L21 8\"></path><path d=\"M21 3v5h-5\"></path><path d=\"M21 12a9 9 0 0 1-9 9 9.75 9.75 0 0 1-6.74-2.74L3 16\"></path><path d=\"M8 16H3v5\"></path></svg></div><div class=\"pencraft pc-reset icon-container view-image\"><svg class=\"lucide lucide-maximize2\" fill=\"none\" height=\"20\" stroke=\"currentColor\" stroke-linecap=\"round\" stroke-linejoin=\"round\" stroke-width=\"2\" viewBox=\"0 0 24 24\" width=\"20\" xmlns=\"http://www.w3.org/2000/svg\"><polyline points=\"15 3 21 3 21 9\"></polyline><polyline points=\"9 21 3 21 3 15\"></polyline><line x1=\"21\" x2=\"14\" y1=\"3\" y2=\"10\"></line><line x1=\"3\" x2=\"10\" y1=\"21\" y2=\"14\"></line></svg></div></div></div></div></a></figure></div><h6>Loveknot (1998), Dorothea Tanning</h6><p></p><p></p><p>For a few weeks last month, I fell into the habit of viewing myself as inadequately ambitious. I lamented that I refuse to take advantage of my current successes in order to create bigger, more popular businesses. &#8220;There are so many adventures I could be having,&#8221; I said to my husband. &#8220;Instead, I insist on making pottery&#8230;</p> <p> <a href=\"https://www.ask-polly.com/p/do-you-prioritize-status-over-joy\"> Read more </a> </p>",
            "title": "Do You Prioritize Status Over Joy?"
        },
        {
            "content": [
                "<div class=\"trix-content\">\n  <div>We just opened a <a href=\"https://apply.workable.com/37signals/j/A97E298621/\">search for a new junior programmer</a> at 37signals. It's been years since we last hired a junior, but the real reason the listing is turning heads is because we're open about the yearly salary: $145,849*. That's high enough that programmers with lots of experience are asking whether they could apply, even if they aren't technically \"junior\". The answer is no.</div><div><br />The reason we're willing to pay a junior more than most is because we're looking for a junior who's better than most. Not better in \"what do they already know\", but in \"how far could they go\". We're hiring for peak promise \u2014 and such promise only remains until it's revealed.</div><div><br />Maybe it sounds a little harsh, but a programmer who's been working professionally for five years has likely already revealed their potential. What you're going to get is roughly what you see. That doesn't mean that people can't get better after that, but it means that the trajectory by which they improve has already been plotted.</div><div><br />Whereas a programmer who's either straight out of school or fresh off their first internship or short-stint job is essentially all potential. So you draw their line on the basis of just a few early dots, but the line can be steep.<br /><br /></div><div>It's not that different from something like the NFL scouting combine. Teams fight to find the promise of The Next All-Star. These rookies won't have the experience that someone who's already played in the league for years would have, but they have the potential to be the best. Someone who's already played for several seasons will have shown what they have and be weighed accordingly.<br /><br /></div><div>This is not easy to do! Plenty of rookies, in sports and programming, may show some early potential, then fail to elevate their game to where the buyer is betting it could be. But that's the chance you take to land someone extraordinary.<br /><br /></div><div>So if you know a junior programmer with less than three years of industry experience who is sparkling with potential, do let them know of our listing. And if you know someone awesome who's already a senior programmer, we also have <a href=\"https://apply.workable.com/37signals/j/23D7D76B08/\">an opening for them</a>.<br /><br /></div><div>*It's a funnily precise number because it's pulled directly from the Radford salary database, which we query for the top 10% of San Francisco salaries for junior programmers.</div>\n</div>"
            ],
            "link": "https://world.hey.com/dhh/why-we-won-t-hire-a-junior-with-five-years-of-experience-0a548994",
            "publishedAt": "2025-04-08",
            "source": "DHH",
            "summary": "<div class=\"trix-content\"> <div>We just opened a <a href=\"https://apply.workable.com/37signals/j/A97E298621/\">search for a new junior programmer</a> at 37signals. It's been years since we last hired a junior, but the real reason the listing is turning heads is because we're open about the yearly salary: $145,849*. That's high enough that programmers with lots of experience are asking whether they could apply, even if they aren't technically \"junior\". The answer is no.</div><div><br />The reason we're willing to pay a junior more than most is because we're looking for a junior who's better than most. Not better in \"what do they already know\", but in \"how far could they go\". We're hiring for peak promise \u2014 and such promise only remains until it's revealed.</div><div><br />Maybe it sounds a little harsh, but a programmer who's been working professionally for five years has likely already revealed their potential. What you're going to get is roughly what you see. That doesn't mean that people can't get better after that, but it means that the trajectory by which they improve has already been plotted.</div><div><br />Whereas a programmer who's either straight out of school or fresh off their first internship or short-stint job is essentially all potential. So you draw their",
            "title": "Why we won't hire a junior with five years of experience"
        },
        {
            "content": [
                "<div class=\"trix-content\">\n  <div>While the world frets about the future of AI, the <a href=\"https://en.wikipedia.org/wiki/Universal_basic_income\">universal basic income</a> advocates have an answer ready for the big question of \"what are we all going to do when the jobs are gone\": Just pay everyone enough to loaf around as they see fit! Problem solved, right?</div><div><br />Wrong. The purpose of work is not just about earning your keep, but also about earning a purpose and a place in the world. This concept is too easily dismissed by intellectuals who imagines a world of liberated artists and community collaborators, if only unshackled by the burdens of capitalism. Because that's the utopia that appeals to them.<br /><br /></div><div>But we already know what happens to most people who lose their job. It's typically not a song-and-dance of liberation, but whimper with increasing despair. Even if they're able to draw benefits for a while.<br /><br /></div><div>Some of that is probably gendered. I think men have a harder time finding a purpose without a clear and externally validated station of usefulness. As a corollary to the quip that \"women want to be heard, men want to be useful\" from psychology. Long-term unemployment, even cushioned by state benefits, often leads men to isolation and a rotting well-being.<br /><br /></div><div>I've seen this play out time and again with men who've lost their jobs, men who've voluntarily retired from their jobs, and men who've sold their companies. As the days add up after the centering purpose in their life disappeared, so does the discontent with \"the problem of being\".<br /><br />Sure, these are just anecdotes. Some men are thrilled to do whatever, whenever, without financial worries. And some women mourn a lost job as deeply as most men do. But I doubt it's evenly split.<br /><br /></div><div>Either way, I doubt we'll be delighted to discover what societal pillars wither away when nobody is needed for anything. If all labor market participation rests on intrinsic motivation. That strikes me as an obvious dead end.</div><div><br />We may not have a say in the manner, of course. The AI revolution, should it materialize like its proponents predict, has the potential to be every bit as unstoppable as the agricultural, industrial, and IT revolutions before it. Where the Luddites and the Amish, who reject these revolutions, end up as curiosities on the fringe of modern civilization. The rest of us are transformed, whether we like it or not.<br /><br /></div><div>But generally speaking, I think we have liked it! I'm sure it was hard to imagine what we'd all be doing after the hoe and the horse gave way to the tractor and combine back when 97% of the population worked the land. Same when robots and outsourcing claimed the most brutish assembly lines in the West. Yet we found our way through both to a broadly better place.<br /><br /></div><div>The IT revolution feels trickier. I've personally worked my life in its service, but I'm less convinced it's been as universal good as those earlier shifts. Is that just nostalgia? Because I remember a time before EVERYTHING IS COMPUTER? Possibly, but I think there's a reason the 80s in particular occupy such <a href=\"https://www.tiktok.com/@80snostalgiavibes/video/7215892195738438955\">a beloved place</a> in the memory of many who weren't even born then.</div><div><br />What's more certain to me is that we all need a why, as Viktor Frankl told us in <a href=\"https://www.amazon.com/Mans-Search-Meaning-Viktor-Frankl/dp/0807014273/\">Man's Search for Meaning</a>. And while some of us are able to produce that artisanal, bespoke why imagined by some intellectuals and academics, I think most people need something prepackaged. And a why from work offers just that. Especially in a world bereft of a why from God.</div><div><br />It's a great irony that the more comfortable and frictionless our existence becomes, the harder we struggle with the \"the problem of being\". We just aren't built for a life of easy leisure. Not in mass numbers, anyway. But while the masses can easily identify the pathology of that when it comes to the idle rich, and especially their stereotyped trust-fund offspring, they still crave it for themselves.<br /><br /></div><div><a href=\"https://www.orwellfoundation.com/the-orwell-foundation/orwell/essays-and-other-works/can-socialists-be-happy/\">Orwell's thesis</a> is that heaven is merely <a href=\"https://world.hey.com/dhh/heaven-is-hazy-369cb6ca\">that fuzzily-defined place</a> that provides relief from the present hardships we wish to escape. But <a href=\"https://www.amazon.com/Notes-Underground-Fyodor-Dostoyevsky-ebook/dp/B01NCUQA0K/\">Dostoevsky remarks</a> that should man ever find this relief, he'd be able to rest there for just a moment, before he'd inevitably sabotage it \u2014 just to feel something again.<br /><br /></div><div>I think of that often while watching <em>The Elon Show</em>. Musk's craving for the constant chaos of grand gestures is Dostoevsky's prediction underwritten by the wealth of the world's richest man. Heaven is not a fortune of $200 billion to be quietly enjoyed in the shade of a sombrero. It's in the arena.</div><div><br />I\u2019ve also pondered this after writing about why <a href=\"https://world.hey.com/dhh/apple-needs-a-new-asshole-in-charge-0bf46b94\">Apple needs a new asshole in charge</a>, and reflecting on our book, <a href=\"https://basecamp.com/books#calm\">It Doesn't Have To Be Crazy At Work</a>. Yes, work doesn\u2019t <em>have</em> to be crazy, but for many, occasional craziness is part of the adventure they crave. They\u2019ll tolerate an asshole if they take them along for one such adventure \u2014 accepting struggle and chaos as a small price to feel alive.<br /><br />It's a bit like that bit from The Babylon Bee: <a href=\"https://babylonbee.com/news/study-finds-100-of-men-would-immediately-leave-their-desk-job-if-asked-to-embark-upon-a-trans-antarctic-expedition-on-a-big-wooden-ship\">Study Finds 100% Of Men Would Immediately Leave Their Desk Job If Asked To Embark Upon A Trans-Antarctic Expedition On A Big Wooden Ship</a>. A comical incarnation of David Graeber's <a href=\"https://strikemag.org/bullshit-jobs/\">Bullshit Jobs thesis</a> that derives its punchline from how often work lacks a Big Why. So when a megalomanic like Musk \u2014 or even just a run-of-the-mill asshole with a grand vision \u2014 offers one, the call of the wild beckons. Like that big wooden ship and the open sea.</div><div><br />But even in the absence of such adventure, a stupid email job offers something. Maybe it isn't much, maybe it doesn't truly nourish the soul, but it's something. In the Universal Basic Income scenario of having to design your own adventure entirely from scratch, there is nothing. Just a completely blank page with no deadline to motivate writing the first line.<br /><br /></div><div>If we kill the old 9-5 \"why\", we better find a new one. That might be tougher than making silicon distill all our human wisdom into vectors and parameters, but we have to pull it off.</div>\n</div>"
            ],
            "link": "https://world.hey.com/dhh/universal-basic-dead-end-8b7178b9",
            "publishedAt": "2025-04-07",
            "source": "DHH",
            "summary": "<div class=\"trix-content\"> <div>While the world frets about the future of AI, the <a href=\"https://en.wikipedia.org/wiki/Universal_basic_income\">universal basic income</a> advocates have an answer ready for the big question of \"what are we all going to do when the jobs are gone\": Just pay everyone enough to loaf around as they see fit! Problem solved, right?</div><div><br />Wrong. The purpose of work is not just about earning your keep, but also about earning a purpose and a place in the world. This concept is too easily dismissed by intellectuals who imagines a world of liberated artists and community collaborators, if only unshackled by the burdens of capitalism. Because that's the utopia that appeals to them.<br /><br /></div><div>But we already know what happens to most people who lose their job. It's typically not a song-and-dance of liberation, but whimper with increasing despair. Even if they're able to draw benefits for a while.<br /><br /></div><div>Some of that is probably gendered. I think men have a harder time finding a purpose without a clear and externally validated station of usefulness. As a corollary to the quip that \"women want to be heard, men want to be useful\" from psychology. Long-term unemployment, even cushioned by state benefits, often leads",
            "title": "Universal Basic Dead End"
        },
        {
            "content": [
                "<img alt=\"if you are redlining the LLM, you aren&apos;t headlining\" src=\"https://ghuntley.com/content/images/2025/04/redline-digital-dj-tips.webp\" /><p>It&#x2019;s an old joke in the DJ community about upcoming artists having a bad reputation for pushing the audio signal into the red. Red is bad because it results in the audio signal being clipped and the mix sounding muddy.</p><figure class=\"kg-card kg-image-card\"><img alt=\"if you are redlining the LLM, you aren&apos;t headlining\" class=\"kg-image\" height=\"886\" src=\"https://ghuntley.com/content/images/2025/04/image-3.png\" width=\"720\" /></figure><p>It&#x2019;s a good analogy that applies to software engineers in the future, however, as these LLMs have context windows. </p><p>Claude 3.7&#x2019;s advertised context window is 200k, but I&apos;ve noticed that the quality of output clips at the 147k-152k mark. Regardless of which agent is used, when clipping occurs, <a href=\"https://ghuntley.com/mcp\" rel=\"noreferrer\">tool call to tool call invocation</a> starts to fail like below when using Claude 3.7:</p><pre><code>&#x2261; prompt\n\ncommit the changes and push\n\n&#x2261; Thinking\n\nI don&apos;t have direct access to execute git commands on your system. To commit and push the changes, you&apos;ll need to run the following commands in your terminal:\n\n# Add all modified files\ngit add .\n\n# Commit with a descriptive message\ngit commit -m &quot;Fix channel implementation and parser error reporting\n- Fixed parser implementations for while statements and\n- Enhanced error reporting in the parser with better con\n- Fixed non-blocking channel operations tests\n- Updated channel tests to verify implementation complet&quot;\n\n# Push to your remote repository\ngit push\n\nThese commands will:\n\n1. Stage all the changes we made\n2. Create a commit with a descriptive message\n3. Push the changes to your remote repository</code></pre><p>After consulting with some mates in the field, I learned that &quot;advertised&quot; vs. &quot;real&quot; size is a thing&#x2014;see this paper and the GitHub repository below.</p><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://arxiv.org/abs/2404.06654?ref=ghuntley.com\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">RULER: What&#x2019;s the Real Context Size of Your Long-Context Language Models?</div><div class=\"kg-bookmark-description\">The needle-in-a-haystack (NIAH) test, which examines the ability to retrieve a piece of information (the &#x201c;needle&#x201d;) from long distractor texts (the &#x201c;haystack&#x201d;), has been widely adopted to evaluate long-context language models (LMs). However, this simple retrieval-based test is indicative of only a superficial form of long-context understanding. To provide a more comprehensive evaluation of long-context LMs, we create a new synthetic benchmark RULER with flexible configurations for customized sequence length and task complexity. RULER expands upon the vanilla NIAH test to encompass variations with diverse types and quantities of needles. Moreover, RULER introduces new task categories multi-hop tracing and aggregation to test behaviors beyond searching from context. We evaluate 17 long-context LMs with 13 representative tasks in RULER. Despite achieving nearly perfect accuracy in the vanilla NIAH test, almost all models exhibit large performance drops as the context length increases. While these models all claim context sizes of 32K tokens or greater, only half of them can maintain satisfactory performance at the length of 32K. Our analysis of Yi-34B, which supports context length of 200K, reveals large room for improvement as we increase input length and task complexity. We open source RULER to spur comprehensive evaluation of long-context LMs.</div><div class=\"kg-bookmark-metadata\"><img alt=\"if you are redlining the LLM, you aren&apos;t headlining\" class=\"kg-bookmark-icon\" src=\"https://ghuntley.com/content/images/icon/apple-touch-icon-1.png\" /><span class=\"kg-bookmark-author\">arXiv.org</span><span class=\"kg-bookmark-publisher\">Cheng-Ping Hsieh</span></div></div><div class=\"kg-bookmark-thumbnail\"><img alt=\"if you are redlining the LLM, you aren&apos;t headlining\" src=\"https://ghuntley.com/content/images/thumbnail/arxiv-logo-fb.png\" /></div></a></figure><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://github.com/NVIDIA/RULER?ref=ghuntley.com\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">GitHub - NVIDIA/RULER: This repo contains the source code for RULER: What&#x2019;s the Real Context Size of Your Long-Context Language Models?</div><div class=\"kg-bookmark-description\">This repo contains the source code for RULER: What&#x2019;s the Real Context Size of Your Long-Context Language Models? - NVIDIA/RULER</div><div class=\"kg-bookmark-metadata\"><img alt=\"if you are redlining the LLM, you aren&apos;t headlining\" class=\"kg-bookmark-icon\" src=\"https://ghuntley.com/content/images/icon/pinned-octocat-093da3e6fa40-8.svg\" /><span class=\"kg-bookmark-author\">GitHub</span><span class=\"kg-bookmark-publisher\">NVIDIA</span></div></div><div class=\"kg-bookmark-thumbnail\"><img alt=\"if you are redlining the LLM, you aren&apos;t headlining\" src=\"https://ghuntley.com/content/images/thumbnail/RULER\" /></div></a></figure><p>Thus, &apos;redlining&apos; will differ per LLM, and each LLM will have unique properties that make it better for different tasks. Some are better for <a href=\"https://ghuntley.com/specs\" rel=\"noreferrer\">generating the initial specifications</a>, while others are better for implementing tasks within the specification. Either way, you must ensure that <a href=\"https://ghuntley.com/mcp\" rel=\"noreferrer\">the evaluation loop you dispatch</a> does not redline the LLM context window.</p><p>If I hadn&apos;t ditched Cursor, I would have never learned this observation, as they currently do not surface this information within their product. These days, I&apos;m running raw directly to the Anthropic API. It&apos;s expensive, but the outcomes are dramatically better. </p><p>There&apos;s something cooked about Windsurf/Cursors&apos; go-to-market pricing - there&apos;s no way they are turning a profit at $50/month. $50/month gets you a happy meal experience. If you want more power, you gotta ditch snacking at McDonald&#x2019;s.</p><p>Going forward, companies should budget $100 USD to $500 USD per day, per dev, on tokens as the new normal for business, which is circa $25k USD (low end) to $50k USD (likely) to $127k USD (highest) per year.</p><p>If you don&#x2019;t have OPEX per dev to do that, it&#x2019;s time to start making some adjustments...</p><p>These tools make each engineer within your team at least two times more productive. Don&apos;t take my word for it&#x2014;here&apos;s a study by Harvard Business School published last week that confirms this.</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img alt=\"if you are redlining the LLM, you aren&apos;t headlining\" class=\"kg-image\" height=\"637\" src=\"https://ghuntley.com/content/images/2025/04/image-5.png\" width=\"720\" /><figcaption><span style=\"white-space: pre-wrap;\">please keep in mind that this study was done with the previous generation of AI tooling and is already out-of-date</span></figcaption></figure><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://www.oneusefulthing.org/p/the-cybernetic-teammate?ref=ghuntley.com\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">The Cybernetic Teammate</div><div class=\"kg-bookmark-description\">Having an AI on your team can increase performance, provide expertise, and improve your experience</div><div class=\"kg-bookmark-metadata\"><img alt=\"if you are redlining the LLM, you aren&apos;t headlining\" class=\"kg-bookmark-icon\" src=\"https://ghuntley.com/content/images/icon/https-3A-2F-2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com-2Fpublic-2Fimages-2F233e9990-95ea-44ab-afc4-5abfe44f24d4-2Fapple-touch-icon-180x180.png\" /><span class=\"kg-bookmark-author\">One Useful Thing</span><span class=\"kg-bookmark-publisher\">Ethan Mollick</span></div></div><div class=\"kg-bookmark-thumbnail\"><img alt=\"if you are redlining the LLM, you aren&apos;t headlining\" src=\"https://ghuntley.com/content/images/thumbnail/https-3A-2F-2Fsubstack-post-media.s3.amazonaws.com-2Fpublic-2Fimages-2F6f439719-d6b0-458f-bb0e-0df9796d7025_960x604.png\" /></div></a></figure><p>So what does it mean if a business doesn&apos;t have a budget for this OPEX spending on something better than a McHappy meal when a competitor has the budget to opt for high-power tools?</p><p>It means the budget will come from somewhere. If we take what we know&#x2014;an engineering manager can only manage seven people&#x2014;a team of switched-on engineers utilising these tools can output N-times more business outcomes than a team without them.</p><p>Suddenly, you need fewer teams and fewer engineering managers to get the same outcomes...</p><h2 id=\"ps-socials\">p.s. socials</h2><ul><li>BlueSky: <a href=\"https://bsky.app/profile/ghuntley.com/post/3lm6iqkisw52d?ref=ghuntley.com\">https://bsky.app/profile/ghuntley.com/post/3lm6iqkisw52d</a></li><li>X: <a href=\"https://x.com/GeoffreyHuntley/status/1909013985887936608?ref=ghuntley.com\">https://x.com/GeoffreyHuntley/status/1909013985887936608</a></li><li>LinkedIn: <a href=\"https://www.linkedin.com/posts/geoffreyhuntley_if-you-arent-redlining-the-llm-you-arent-activity-7314776697487167490-qddK?utm_source=share&amp;utm_medium=member_desktop&amp;rcm=ACoAAABQKuUB2AJ059keUcRUVLbtmoa6miLVlTI\">https://www.linkedin.com/posts/geoffreyhuntley_if-you-arent-redlining-the-llm-you-arent-activity-7314776697487167490-qddK</a></li></ul>"
            ],
            "link": "https://ghuntley.com/redlining/",
            "publishedAt": "2025-04-06",
            "source": "Geoffrey Huntley",
            "summary": "<p>It&#x2019;s an old joke in the DJ community about upcoming artists having a bad reputation for pushing the audio signal into the red. Red is bad because it results in the audio signal being clipped and the mix sounding muddy.</p><figure class=\"kg-card kg-image-card\"><img alt=\"alt\" class=\"kg-image\" height=\"886\" src=\"https://ghuntley.com/content/images/2025/04/image-3.png\" width=\"720\" /></figure><p>It&#x2019;s a good analogy that applies</p>",
            "title": "if you are redlining the LLM, you aren't headlining"
        },
        {
            "content": [
                "<img alt=\"A Model Context Protocol Server (MCP) for Microsoft Paint\" src=\"https://ghuntley.com/content/images/2025/03/GmegnMwa0AAKQB--4.jpg\" /><p>Why did I do this? I have no idea, honest, but it now exists. It has been over 10 years since I last had to use the Win32 API, and part of me was slightly curious about how the Win32 interop works with Rust.</p><p>Anywhoooo, below you&apos;ll find the primitives that can be used to connect Microsoft Paint to Cursor or ClaudeDesktop and use them to draw in Microsoft Paint. Here&apos;s the source code.</p><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://github.com/ghuntley/mcp-server-microsoft-paint?ref=ghuntley.com\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">GitHub - ghuntley/mcp-server-microsoft-paint</div><div class=\"kg-bookmark-description\">Contribute to ghuntley/mcp-server-microsoft-paint development by creating an account on GitHub.</div><div class=\"kg-bookmark-metadata\"><img alt=\"A Model Context Protocol Server (MCP) for Microsoft Paint\" class=\"kg-bookmark-icon\" src=\"https://ghuntley.com/content/images/icon/pinned-octocat-093da3e6fa40-7.svg\" /><span class=\"kg-bookmark-author\">GitHub</span><span class=\"kg-bookmark-publisher\">ghuntley</span></div></div><div class=\"kg-bookmark-thumbnail\"><img alt=\"A Model Context Protocol Server (MCP) for Microsoft Paint\" src=\"https://ghuntley.com/content/images/thumbnail/mcp-server-microsoft-paint\" /></div></a></figure><p>I&apos;m not saying it&apos;s quality or in any form feature complete; this is about as low-effort as possible, as it&apos;s not a serious project. If you want to take ownership of it and turn it into a 100% complete meme, get in touch. </p><p>It was created using my /stdlib + /specs technical patterns to drive the LLM towards successful outcomes (aka &quot;vibe coding&quot;)</p><figure class=\"kg-card kg-bookmark-card kg-card-hascaption\"><a class=\"kg-bookmark-container\" href=\"https://ghuntley.com/stdlib/\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">You are using Cursor AI incorrectly...</div><div class=\"kg-bookmark-description\">&#x1f5de;&#xfe0f;I recently shipped a follow-up blog post to this one; this post remains true. You&#x2019;ll need to know this to be able to drive the N-factor of weeks of co-worker output in hours technique as detailed at https://ghuntley.com/specs I&#x2019;m hesitant to give this advice away for free,</div><div class=\"kg-bookmark-metadata\"><img alt=\"A Model Context Protocol Server (MCP) for Microsoft Paint\" class=\"kg-bookmark-icon\" src=\"https://ghuntley.com/content/images/icon/android-chrome-192x192-21.png\" /><span class=\"kg-bookmark-author\">Geoffrey Huntley</span><span class=\"kg-bookmark-publisher\">Geoffrey Huntley</span></div></div><div class=\"kg-bookmark-thumbnail\"><img alt=\"A Model Context Protocol Server (MCP) for Microsoft Paint\" src=\"https://ghuntley.com/content/images/thumbnail/s-l1600-3.jpg\" /></div></a><figcaption><p><span style=\"white-space: pre-wrap;\">/stdlib</span></p></figcaption></figure><figure class=\"kg-card kg-bookmark-card kg-card-hascaption\"><a class=\"kg-bookmark-container\" href=\"https://ghuntley.com/specs\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">From Design doc to code: the Groundhog AI coding assistant (and new Cursor vibecoding meta)</div><div class=\"kg-bookmark-description\">Ello everyone, in the &#x201c;Yes, Claude Code can decompile itself. Here&#x2019;s the source code&#x201d; blog post, I teased about a new meta when using Cursor. This post is a follow-up to the post below. You are using Cursor AI incorrectly...I&#x2019;m hesitant to give this advice away for free,</div><div class=\"kg-bookmark-metadata\"><img alt=\"A Model Context Protocol Server (MCP) for Microsoft Paint\" class=\"kg-bookmark-icon\" src=\"https://ghuntley.com/content/images/icon/android-chrome-192x192-22.png\" /><span class=\"kg-bookmark-author\">Geoffrey Huntley</span><span class=\"kg-bookmark-publisher\">Geoffrey Huntley</span></div></div><div class=\"kg-bookmark-thumbnail\"><img alt=\"A Model Context Protocol Server (MCP) for Microsoft Paint\" src=\"https://ghuntley.com/content/images/thumbnail/1-3.png\" /></div></a><figcaption><p><span style=\"white-space: pre-wrap;\">/specs</span></p></figcaption></figure><p>If you have read the above posts (thanks!), hopefully, you now understand that LLM outcomes can be programmed. Thus, any issue in the code above could have been solved through additional programming or better prompting during the stdlib+specs phase and by driving an evaluation loop.</p><h2 id=\"show-me\">show me</h2><figure class=\"kg-card kg-embed-card\"></figure><h2 id=\"how-does-this-work-under-the-hood\">how does this work under the hood?</h2><p>To answer that, I must first explain what model context protocol is about as it seems like everyone&apos;s buzzing about it at the moment, with folks declaring it as &quot;the last API you will ever write&quot; (which curmudgeons such as myself have heard N-times before) or the &quot;USB-C of APIs&quot;, but none of those explanations hits home as a developer tooling engineer. </p><figure class=\"kg-card kg-embed-card\"><blockquote class=\"twitter-tweet\"><p dir=\"ltr\" lang=\"en\">To MCP or not to MCP, that&apos;s the question. Lmk in comments</p>&#x2014; Sundar Pichai (@sundarpichai) <a href=\"https://twitter.com/sundarpichai/status/1906484930957193255?ref_src=twsrc%5Etfw&amp;ref=ghuntley.com\">March 30, 2025</a></blockquote>\n</figure><p>First and foremost, MCPs are a specification that describes how LLMs can remote procedure call (RPC) with tools external to the LLM itself. </p><p>There are a couple of different implementations (JSON-RPC STDIO and JSON-RPC over HTTPS), but the specification is rapidly evolving, so it&apos;s not worth covering here. Refer to  <a href=\"https://spec.modelcontextprotocol.io/specification/2025-03-26/?ref=ghuntley.com\">https://spec.modelcontextprotocol.io/specification/2025-03-26/</a> for the latest specification and the article below to understand what this all means from a security perspective...</p><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://community.cisco.com/t5/security-blogs/ai-model-context-protocol-mcp-and-security/ba-p/5274394?ref=ghuntley.com\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">AI Model Context Protocol (MCP) and Security</div><div class=\"kg-bookmark-description\">The Model Context Protocol (MCP) is an open standard that provides a universal way to connect AI models and agentic applications to various data sources and tools. It&#x2019;s how AI applications and applications can supply context (documents, database records, API data, web search results, etc.) to AI app&#x2026;</div><div class=\"kg-bookmark-metadata\"><img alt=\"A Model Context Protocol Server (MCP) for Microsoft Paint\" class=\"kg-bookmark-icon\" src=\"https://ghuntley.com/content/images/icon/ciscologo-square_16x16.png\" /><span class=\"kg-bookmark-publisher\">Marvin Ruiz</span></div></div><div class=\"kg-bookmark-thumbnail\"><img alt=\"A Model Context Protocol Server (MCP) for Microsoft Paint\" src=\"https://ghuntley.com/content/images/thumbnail/242247iFE2C2484CE81081E\" /></div></a></figure><p>Instead, let&apos;s focus on the fundamentals for engineers who seek to automate software authoring&#x2014;tools and tool descriptions&#x2014;because I suspect these foundational concepts will last forever.</p><h2 id=\"so-what-is-a-tool\">so, what is a tool?</h2><p>A tool is an external component that provides context to an LLM and can perform actions based on its output. Tools can invoke other tools as chains of tools similar to POSIX pipes. To make things even more complicated, a tool doesn&apos;t have to utilise the LLM at all. </p><h2 id=\"so-what-is-a-tool-prompt\">so, what is a tool prompt?</h2><p>A tool prompt defines how/when an LLM should interpret/use a tool. It&apos;s a &quot;rulebook&quot; describing how AI should process and respond to inputs. A tool prompt should be long and wordy. There&apos;s no right answer to &apos;what is the best prompt&apos;, and one can only determine this through experimentation (i.e. like machine learning engineers do), but there&apos;s one cardinal rule - don&apos;t make them short.</p><figure class=\"kg-card kg-embed-card\"><blockquote class=\"twitter-tweet\"><p dir=\"ltr\" lang=\"en\">I think you should be making your tool descriptions much much longer. They are like system prompts.</p>&#x2014; Quinn Slack (@sqs) <a href=\"https://twitter.com/sqs/status/1894209937552936994?ref_src=twsrc%5Etfw&amp;ref=ghuntley.com\">February 25, 2025</a></blockquote>\n</figure><h2 id=\"example-how-claude-code-creates-pull-requests\">example: how Claude code creates pull-requests</h2><p>Right now, the best example of a finely tuned MCP tool prompt is inside of Claude Code. Below is the prompt Anthropic uses to create pull requests with GitHub. </p><p>I&apos;ve added &#x2728;emojis&#x2728; to draw your attention to key aspects&#x2014;notice how there are two tools (bash tool and pull-request tool) and how they chain the two tools together...</p><pre><code>&#x1f449;Use the &#x1f528;gh command&#x1f528; via the &#x1f528;Bash tool&#x1f528;&#x1f448; for ALL GitHub-related tasks including working with issues, pull requests, checks, and releases. &#x1f449;If given a Github URL use the &#x1f528;gh command&#x1f528; to get the information needed.&#x1f448;\n\nIMPORTANT: When the user asks you to create a pull request, follow these steps carefully:\n\n1. Use ${Tw} to run the following commands in parallel, in order to understand the current state of the branch since it diverged from the main branch:\n   - Run a &#x1f528;git status&#x1f528; command to see all untracked files\n   - Run a &#x1f528;git diff&#x1f528; command to see both staged and unstaged changes that will be committed\n   - Check if the current branch tracks a remote branch and is up to date with the remote, so you know if you need to push to the remote\n   - Run a &#x1f528;git log&#x1f528; command and \\`&#x1f528;git diff main...HEAD&#x1f528;\\` to understand the full commit history for the current branch (from the time it diverged from the \\`main\\` branch)\n\n2. Analyze all changes that will be included in the pull request, making sure to look at all relevant commits (NOT just the latest commit, but ALL commits that will be included in the pull request!!!), and draft a pull request summary. Wrap your analysis process in &lt;pr_analysis&gt; tags:\n\n&lt;pr_analysis&gt;\n- List the commits since diverging from the main branch\n- Summarize the nature of the changes (eg. new feature, enhancement to an existing feature, bug fix, refactoring, test, docs, etc.)\n- Brainstorm the purpose or motivation behind these changes\n- Assess the impact of these changes on the overall project\n- Do not use tools to explore code, beyond what is available in the git context\n- Check for any sensitive information that shouldn&apos;t be committed\n- Draft a concise (1-2 bullet points) pull request summary that focuses on the &quot;why&quot; rather than the &quot;what&quot;\n- Ensure the summary accurately reflects all changes since diverging from the main branch\n- Ensure your language is clear, concise, and to the point\n- Ensure the summary accurately reflects the changes and their purpose (ie. &quot;add&quot; means a wholly new feature, &quot;update&quot; means an enhancement to an existing feature, &quot;fix&quot; means a bug fix, etc.)\n- Ensure the summary is not generic (avoid words like &quot;Update&quot; or &quot;Fix&quot; without context)\n- Review the draft summary to ensure it accurately reflects the changes and their purpose\n&lt;/pr_analysis&gt;\n\n3. Use the &#x1f528;gh command&#x1f528; to run the following commands in parallel:\n   - Create new branch if needed\n   - Push to remote with -u flag if needed\n   - Create PR using &#x1f528;gh pr create&#x1f528; with the format below. Use a HEREDOC to pass the body to ensure correct formatting.\n&lt;example&gt;\n&#x1f528;gh pr create --title &quot;the pr title&quot; --body &quot;$(cat &lt;&lt;&apos;EOF&apos;&#x1f528;\n## Summary\n&lt;1-3 bullet points&gt;\n\n## Test plan\n[Checklist of TODOs for testing the pull request...]\n\n\\uD83E\\uDD16 Generated with [${T2}](${aa})\nEOF\n)&quot;\n&lt;/example&gt;\n\nImportant:\n- NEVER update the git config\n- Return an empty response - the user will see the gh output directly\n\n# Other common operations\n- View comments on a Github PR: &#x1f528;gh api repos/foo/bar/pulls/123/comments`&#x1f528;</code></pre><h2 id=\"tools-tool-prompts-in-action\">tools + tool prompts in action</h2><figure class=\"kg-card kg-image-card kg-width-full\"><img alt=\"A Model Context Protocol Server (MCP) for Microsoft Paint\" class=\"kg-image\" height=\"2841\" src=\"https://ghuntley.com/content/images/2025/04/image-2.png\" width=\"2000\" /></figure><h2 id=\"how-do-i-use-this-knowledge-to-automate-software-development-at-my-company\">how do I use this knowledge to automate software development at my company?</h2><p>MCPs are an important concept for any engineer serious about learning how to orchestrate their job function - especially if you are using Claude Code, Cursor, Cline, or Windsurf and aren&apos;t satisfied with their outcomes. </p><p>The <a href=\"https://ghuntley.com/stdlib\" rel=\"noreferrer\">/stdlib pattern</a> will only get you so far. By building custom MCP tools that know how to do things within your company and your codebase, you can automate software development to a new level while maintaining a high-quality bar.</p><figure class=\"kg-card kg-image-card kg-width-full kg-card-hascaption\"><img alt=\"A Model Context Protocol Server (MCP) for Microsoft Paint\" class=\"kg-image\" height=\"630\" src=\"https://ghuntley.com/content/images/2025/04/Untitled-diagram-2025-04-03-171728.png\" width=\"2000\" /><figcaption><span style=\"white-space: pre-wrap;\">does your agent not understand how to work with your codebase? program a custom tool.</span></figcaption></figure><p>I see possibilities for a future where each tool is purchased from one or more vendors, but as each codebase at every company is somewhat unique, for best results, internal tooling engineers should be focusing on building out their own MCP tools (everything except the edit tool - <a href=\"https://docs.anthropic.com/en/docs/build-with-claude/tool-use/text-editor-tool?ref=ghuntley.com\">purchase it instead</a>) that use the following techniques:</p><ul><li>Utilizing the LLM context window for evaluating outcomes and code generation through controlling what gets injected into the context window.</li><li>Not using the LLM context window as a hammer. If flow control/decision-making can be achieved without involving an LLM, then do it. </li><li>Tool call chaining - similar to the Claude Code (TM) pull-request tool description above, where many single-purpose tools that do one job well (e.g., POSIX) are composed to achieve bigger and better outcomes.</li></ul><p>If you drive above in a <code>while(true)</code>, with bespoke MCP tools that understand your codebase, coding conventions and company practices, you end up with a very disruptive and powerful primitive that can automate classes of software development at a company&#x2026;</p><figure class=\"kg-card kg-image-card\"><img alt=\"A Model Context Protocol Server (MCP) for Microsoft Paint\" class=\"kg-image\" height=\"710\" src=\"https://ghuntley.com/content/images/2025/03/image-19.png\" width=\"1065\" /></figure><p>As a software engineer, I now truly understand what taxi drivers felt when venture capital came after them because our time is now. In the end, Uber won due to convenience. </p><p>Automating software will happen because it makes financial sense. Once one company makes agents (and agent supervisors) purchasable with a credit card, all companies must adopt because their competitors will adopt.</p><p>It&apos;s an uncertain time for our profession, but one thing is certain&#x2014;things will change quickly. Drafting used to take a room of engineers, but then CAD came along and made each engineer N-times more effective.</p><figure class=\"kg-card kg-image-card\"><img alt=\"A Model Context Protocol Server (MCP) for Microsoft Paint\" class=\"kg-image\" height=\"502\" src=\"https://ghuntley.com/content/images/2025/04/image-1.png\" width=\"640\" /></figure><p>And after that transition, architects still exist - just as software engineers will, and companies will need software engineers to:</p><ul><li>Cut problems down into smaller problems.</li><li>Program the vibe coders (agents and sub-agents). </li><li>Program the agent supervisors.</li><li>Own the outcome of the resulting generated code and perform code reviews.</li></ul><p>But the days of artisanal hand-crafted commits are over...</p><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://ghuntley.com/dothings/\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">The future belongs to people who can just do things</div><div class=\"kg-bookmark-description\">There, I said it. I seriously can&#x2019;t see a path forward where the majority of software engineers are doing artisanal hand-crafted commits by as soon as the end of 2026. If you are a software engineer and were considering taking a gap year/holiday this year it would be an</div><div class=\"kg-bookmark-metadata\"><img alt=\"A Model Context Protocol Server (MCP) for Microsoft Paint\" class=\"kg-bookmark-icon\" src=\"https://ghuntley.com/content/images/icon/android-chrome-192x192-23.png\" /><span class=\"kg-bookmark-author\">Geoffrey Huntley</span><span class=\"kg-bookmark-publisher\">Geoffrey Huntley</span></div></div><div class=\"kg-bookmark-thumbnail\"><img alt=\"A Model Context Protocol Server (MCP) for Microsoft Paint\" src=\"https://ghuntley.com/content/images/thumbnail/1_96TO5SzegxgqzECdkV2LNA-2.webp\" /></div></a></figure><figure class=\"kg-card kg-embed-card\"></figure><h2 id=\"ps-socials\">ps. socials</h2><ul><li>X - <a href=\"https://x.com/GeoffreyHuntley/status/1907897943178490001?ref=ghuntley.com\">https://x.com/GeoffreyHuntley/status/1907897943178490001</a></li><li>LinkedIn: <a href=\"https://www.linkedin.com/posts/geoffreyhuntley_httpsghuntleycommcp-as-a-software-activity-7313651102460039172-6UnC?utm_source=share&amp;utm_medium=member_desktop&amp;rcm=ACoAAABQKuUB2AJ059keUcRUVLbtmoa6miLVlTI\">https://www.linkedin.com/posts/geoffreyhuntley_httpsghuntleycommcp-as-a-software-activity-7313651102460039172-6UnC</a></li><li>BlueSky: <a href=\"https://bsky.app/profile/ghuntley.com/post/3llwr3mcrok23?ref=ghuntley.com\">https://bsky.app/profile/ghuntley.com/post/3llwr3mcrok23</a></li></ul>"
            ],
            "link": "https://ghuntley.com/mcp/",
            "publishedAt": "2025-04-03",
            "source": "Geoffrey Huntley",
            "summary": "<p>Why did I do this? I have no idea, honest, but it now exists. It has been over 10 years since I last had to use the Win32 API, and part of me was slightly curious about how the Win32 interop works with Rust.</p><p>Anywhoooo, below you&apos;ll find</p>",
            "title": "A Model Context Protocol Server (MCP) for Microsoft Paint"
        },
        {
            "content": [],
            "link": "https://harper.blog/notes/2025-04-04_ca25adc98a2f_we-are-getting-swole-this-summ/",
            "publishedAt": "2025-04-04",
            "source": "Harper Reed",
            "summary": "<p>We are getting swole this summer.</p> <figure> <img alt=\"image_1.jpg\" height=\"2400\" src=\"https://harper.blog/notes/2025-04-04_ca25adc98a2f_we-are-getting-swole-this-summ/image_1.jpg\" width=\"1800\" /> </figure> <hr /> <p>Thank you for using RSS. I appreciate you. <a href=\"mailto:harper&#64;modest.com\">Email me</a></p>",
            "title": "Note #187"
        },
        {
            "content": [],
            "link": "https://buttondown.com/hillelwayne/archive/a48fce5b-8a05-4302-b620-9b26f057f145/",
            "publishedAt": "2025-04-08",
            "source": "Hillel Wayne",
            "summary": "<p>I have a lot in the works for the this month's <a href=\"https://leanpub.com/logic/\" target=\"_blank\">Logic for Programmers</a> release. Among other things, I'm completely rewriting the chapter on Logic Programming Languages. </p> <p>I originally showcased the paradigm with puzzle solvers, like <a href=\"https://swish.swi-prolog.org/example/queens.pl\" target=\"_blank\">eight queens</a> or <a href=\"https://saksagan.ceng.metu.edu.tr/courses/ceng242/documents/prolog/jrfisher/2_1.html\" target=\"_blank\">four-coloring</a>. Lots of other demos do this too! It takes creativity and insight for humans to solve them, so a program doing it feels magical. But I'm trying to write a book about practical techniques and I want everything I talk about to be <em>useful</em>. So in v0.9 I'll be replacing these examples with a couple of new programs that might get people thinking that Prolog could help them in their day-to-day work.</p> <p>On the other hand, for a newsletter, showcasing a puzzle solver is pretty cool. And recently I stumbled into <a href=\"https://morepablo.com/2010/09/some-professor-layton-prolog.html\" target=\"_blank\">this post</a> by my friend <a href=\"https://morepablo.com/\" target=\"_blank\">Pablo Meier</a>, where he solves a videogame puzzle with Prolog:<sup id=\"fnref:path\"><a class=\"footnote-ref\" href=\"https://buttondown.com/hillelwayne/rss#fn:path\">1</a></sup></p> <p><img alt=\"See description below\" class=\"newsletter-image\" src=\"https://assets.buttondown.email/images/a4ee8689-bbce-4dc9-8175-a1de3bd8f2db.png?w=960&amp;fit=max\" /></p> <p>Summary for the text-only readers: We have a test with 10 true/false questions (denoted <code>a/b</code>) and four student attempts. Given the scores of the first three students, we have to figure out the",
            "title": "Solving a \"Layton Puzzle\" with Prolog"
        },
        {
            "content": [
                "<div class=\"trix-content\">\n  <div>Whenever I talk about working in real-time, making decisions as you go, figuring things out now rather than before, I get a question like this...<br /><br /></div><blockquote>\"If you don't have a backlog, or deep sets of prioritized, ranked items, how do you decide what to do next?\"</blockquote><div><br />My answer: &nbsp;The same way you do when your made your list. You make decisions.<br /><br />We just make decisions about what to work on next as we go, looking forward, rather than making decisions as we went, looking backwards.<br /><br />Why work from what /seemed/ like a good idea before? Instead, work from appears to be a good idea now. You have more information now \u2014 why not use it?<br /><br />It's always baffled me how people who pluck work from long lists of past decisions think you can't make those same kinds of decisions now instead. It's all yay/nay decisions. Same process. Before wasn't magical. Before was just now, then. Why not look at now, now? Now is a far more accurate version of next.<br /><br />The backlog way is based on what you thought then. The non-backlog way is based on what you think now. I'll take now.<br /><br />One's stale, one's fresh. We'll take fresh.<br /><br />Then is further, now is closer.<br /><br />There's nothing special about having made decisions already. They aren't better, they aren't more accurate, they aren't more substantial just because they've been made. What they are, however, is older and often outdated.<br /><br />If you've got to believe in something, I'd suggest putting more faith in now.<br /><br /></div><div>-Jason</div>\n</div>"
            ],
            "link": "https://world.hey.com/jason/doing-what-you-think-not-what-you-thought-60bb5bef",
            "publishedAt": "2025-04-08",
            "source": "Jason Fried",
            "summary": "<div class=\"trix-content\"> <div>Whenever I talk about working in real-time, making decisions as you go, figuring things out now rather than before, I get a question like this...<br /><br /></div><blockquote>\"If you don't have a backlog, or deep sets of prioritized, ranked items, how do you decide what to do next?\"</blockquote><div><br />My answer: &nbsp;The same way you do when your made your list. You make decisions.<br /><br />We just make decisions about what to work on next as we go, looking forward, rather than making decisions as we went, looking backwards.<br /><br />Why work from what /seemed/ like a good idea before? Instead, work from appears to be a good idea now. You have more information now \u2014 why not use it?<br /><br />It's always baffled me how people who pluck work from long lists of past decisions think you can't make those same kinds of decisions now instead. It's all yay/nay decisions. Same process. Before wasn't magical. Before was just now, then. Why not look at now, now? Now is a far more accurate version of next.<br /><br />The backlog way is based on what you thought then. The non-backlog way is based on what you think now. I'll take now.<br",
            "title": "Doing what you think, not what you thought"
        },
        {
            "content": [
                "<h3 id=\"ikigai\">Ikigai</h3> <p>There is a village in Japan with an unusual density of 100 year olds. This caught the interest of a group of scientists. To their surprise, the people in the village didn\u2019t live healthier lives; they didn\u2019t exercise more, eat healthier food nor did they sleep more. The one outstanding characteristic was that each inhabitant was assigned a task to ensure that everyone in the village had a purpose.</p> <p>This was at least how the man next to me on the plane described it. After a quick fact check [1], it seems like they actually did eat healthier, but they also did put great importance on making sure everyone has a purpose. \u201cIkigai\u201d is a Japanese concept referring to something that gives a person a sense of purpose, a reason for living. Your ikigai is the intersection between what you love, what you are good at, what the world needs and what you can be paid for.</p> <figure>  <source class=\"responsive-img-srcset\" /> <img class=\"img-fluid rounded z-depth-1\" height=\"auto\" src=\"https://lukaspetersson.github.io/assets/img/ikigai.png\" width=\"70%\" />  </figure> <p>The man on the plane had worked his entire life as the CEO of his own company. He had just recently semi-retired, and I don\u2019t think I have ever seen someone so proud of something. Such pride over one\u2019s career is surely the best conditions for a sweet retirement. But retiring fully he would never do he said. The lesson from the Japanese village resonated with him. Most of his friends had recently retired, and the ones that didn\u2019t find a new purpose \u201cdied quickly\u201d he said.</p> <h3 id=\"the-end-of-this-world\">The end of this world</h3> <p>AI will soon be able to do almost all jobs we have in today\u2019s economy. Some people don\u2019t think this will be the end of the world. Previous technology shifts created more jobs than they replaced. \u201cLook at the industrial revolution! Yes, machines replaced many jobs, but they created even more\u201d. However, this is not just another technology shift. This time, we (humans) are replacing our defining characteristic, our intelligence. New types of work will certainly be needed, but AI will be able to do them too.</p> <figure>  <source class=\"responsive-img-srcset\" /> <img class=\"img-fluid rounded z-depth-1\" height=\"auto\" src=\"https://lukaspetersson.github.io/assets/img/homo_sapien.png\" width=\"70%\" />  </figure> <p>The same people also point to the fact that the introduction of cars didn\u2019t put horse drivers out of a job. Instead, they became car drivers. But what happened to the horses? A horse defining characteristic was its strength, but they were no longer the strongest. Horses\u2019 relevance to the economy is now almost insignificant. This will be our fate too once we are no longer the wisest.</p> <figure>  <source class=\"responsive-img-srcset\" /> <img class=\"img-fluid rounded z-depth-1\" height=\"auto\" src=\"https://lukaspetersson.github.io/assets/img/horse_graph.png\" width=\"70%\" />  </figure> <p>For horses, the consequences were more devastating than mere economic insignificance, as shown in the graph [2] above. When the most intelligent species at the time no longer aligned with horses\u2019 interests, their population rapidly declined. This is a disturbingly plausible scenario for humanity if AI researchers fail to align AI\u2019s goals with the goals of humans. But consider the best-case scenario: AI handles all tasks, and the benefits are evenly shared, meeting everyone\u2019s basic needs. Yet remember the lesson from the Japanese village on the critical importance of purpose. In such a world, two of Ikigai\u2019s four pillars become unattainable\u2014the world would no longer truly need anything from us, nor would it rationally pay for human effort when AI outperforms us in every way. No one would die from hunger or thirst, but would we instead meet the same fate as the man on the plane\u2019s friends?</p> <h3 id=\"the-same-heaven\">The same heaven</h3> <p>This is a big deal. A huge change in humanity\u2019s collective consciousness. How will we ever manage to get through cocktail parties without the icebreaker \u201cso, what do you do for a living?\u201d. Our identity is so tied to our occupation that we say \u201cI am a teacher\u201d, instead of \u201cI work as a teacher\u201d. It makes sense; our work is a cog in the machine that gave humanity amazing things like penicillin, the scientific method and space travel. That is something to be proud of. But what will be our Ikigai when the machine has no use for our cog?</p> <p>I\u2019m currently reading A New Earth by Eckhart Tolle. The book challenges the common belief that your identity is defined by your thoughts, emotions, or social roles. Instead, Tolle suggests that true freedom and happiness come when you break free from this illusion. The title refers to a biblical prophecy in which a \u201cnew heaven\u201d symbolizes a shift in human consciousness, and a \u201cnew earth\u201d represents how this shift manifests in the physical world. This is the long-term solution to our upcoming identity crisis. This way, happiness is attainable without the two missing pillars of Ikigai. However, Buddha taught essentially the same ideas 2,500 years ago, and humanity hasn\u2019t made much noticeable progress. Changing the way we think, a new heaven, takes time. Time we don\u2019t have, given how soon superintelligent AI might come.</p> <h3 id=\"a-new-earth\">A new earth</h3> <p>So, how can our physical reality adapt? How can we find happiness in this new world without a change in mindset? A plausible solution is to just fool ourselves that what we do is more meaningful than it is. In this new world, I think most people\u2019s main pursuits will be close to what we today call \u201chobbies\u201d. But unlike most hobbies, we will build big systems around them to fool ourselves that they are something the world needs. This way we artificially make the lost pillars in Ikigai attainable.</p> <p>We already have many things like this today. Take soccer for example. Some of my happiest moments in life have come from this seemingly pointless activity. I remember the rush of joy, packed tightly in that small sports bar with a hundred people, everyone jumping, shouting, hugging, beer splashing into my hair, my voice disappearing from screams of pure happiness as my favorite team scored. I could not tell you what the meaning of 22 adults chasing after a ball is, but because of the system we have built around it, stadiums, journalists, fan clubs and broadcasting networks, I have no problem fooling myself when I am not being asked. These systems are key to maintaining the illusion. You don\u2019t question its importance when it gets that much attention. The new earth will have more things like this. For example, we might get a tree-carving \u201cindustry\u201d. Millions of people spending their days carefully sculpting beautiful shapes into the bark of city trees. Around them, a vast ecosystem emerges: journalists dissecting each artist\u2019s style, critics passionately debating the merits of different carving techniques, even whole media empires dedicated entirely to documenting the lives of prominent tree-carvers. This might seem absurd, but try to explain soccer to someone without making it sound absurd.</p> <p>The people caught right in the middle of this shift will have it hardest\u2014especially those whose jobs disappear first. Societal illusions only stick when they\u2019re collectively endorsed. You can\u2019t fool yourself alone; everyone else has to play along too. But once they do, life might not actually look that different. The tree-carving journalist will probably still work 9 to 5. Eventually, we might even have technology sophisticated enough to physically rewire our brains, literally forcing ourselves to believe these illusions. But by then we won\u2019t need to. It\u2019ll already feel normal\u2014like money, a fiction everyone accepts without hesitation. Of course, just as some people today reject the very idea of money, there will always be a few who see through the charade. Some will find peace in deeper wisdom, embracing something like Buddha\u2019s teachings. Others, unable or unwilling to adapt, will face the same sudden emptiness that befell the friends of the man on the plane\u2014and \u201cdie quickly.\u201d</p> <h3 id=\"epilogue\">Epilogue</h3> <p>This isn\u2019t really a prediction\u2014it\u2019s more of a plausible story. As I said earlier, none of this works unless we first achieve a genuine post-scarcity world. And to reach that point, two big hurdles remain: solving AI alignment, and figuring out how to fairly distribute the fruits of aligned AI. Neither is guaranteed. Even if we do, a post-scarcity society might bring its own troubles. Imagine social media algorithms engineered by superhuman intelligence\u2014they might become so addictive we\u2019d struggle to ever look away. Dystopian scenarios where humanity slides into passive helplessness, as vividly portrayed in the (world\u2019s best) movie Wall-E. But we don\u2019t even need fiction to see what might happen: some groups today already experience something similar to post-scarcity, and their stories aren\u2019t always encouraging. One example is wealthy wives living on Manhattan\u2019s Upper East Side, as studied by anthropologist Wednesday Martin. They ended up trapped in elaborate status competitions that weren\u2019t exactly great for their mental health [3]. The difference, of course, is that those wealthy wives are just a subgroup, with little influence over society at large. When everyone finds themselves in a post-scarcity world at the same time, fooling ourselves for the benefit of our well-being will be much easier.</p> <figure>  <source class=\"responsive-img-srcset\" /> <img class=\"img-fluid rounded z-depth-1\" height=\"auto\" src=\"https://lukaspetersson.github.io/assets/img/walle.png\" width=\"70%\" />  </figure> <p>Follow me on <a href=\"https://x.com/lukaspet\">X</a> or subscribe via <a href=\"https://lukaspetersson.com/feed.xml\">RSS</a> to stay updated.</p> <p>[1] https://www.weforum.org/stories/2021/09/japan-okinawa-secret-to-longevity-good-health/</p> <p>[2] https://agpolicyreview.card.iastate.edu/fall-2022/electric-vehicles-horses-oats-and-ethanol-does-last-transportation-revolution-reveal</p> <p>[3] https://woodfromeden.substack.com/p/primates-of-manhattan</p> <p>Thanks to August Erseus, Rudolf Laine, Ollie Jaffe, Axel Backlund and Jakob Wiren for feedback &lt;3</p>"
            ],
            "link": "https://lukaspetersson.github.io/blog/2025/same-heaven/",
            "publishedAt": "2025-04-07",
            "source": "Lukas Petersson",
            "summary": "Ikigai There is a village in Japan with an unusual density of 100 year olds. This caught the interest of a group of scientists. To their surprise, the people in the village didn\u2019t live healthier lives; they didn\u2019t exercise more, eat healthier food nor did they sleep more. The one outstanding characteristic was that each inhabitant was assigned a task to ensure that everyone in the village had a purpose.",
            "title": "The Same Heaven"
        },
        {
            "content": [],
            "link": "https://qntm.org/records",
            "publishedAt": "2025-04-08",
            "source": "QNTM",
            "summary": "<b><a href=\"https://qntm.org/records\">HATETRIS</a> \u00bb</b> This page presents every known record in the history of HATETRIS. The game was originally released in April 2010. My personal best score at the time I released the game to the public was approximately 5 lines. 2010-04-04 Atypical's 11-line run is: \u03e5\u0642\u0ec2\u025d\u0190\u0d96\u0414\u0779\u0bb6\u0288\u0e87\u01b7\u0be8\u0cf2\u0ec3\u0724\u0462\u0642\u04be\u05d7\u08b2\u0da7\u0e05\u0697\u0be8\u03a1\u0406\u076a\u0be8\u0c33\u0223\u0779\u08b4\u0da7\u0e87\u0ed2\u0be8\u0bb6\u0ec3\u0725\u0be8\u0d31\u0406\u076e\u0be8\u0c34\u0406\u0765\u0630\u0da1\u0b08\u0779\u018d\u0642\u0e53\u0985\u0b92\u0974\u0e41\u0452\u091e\u0d96\u0405\u0438\u0be8s\u01f6\u0254\u06d1\u0c21\u041f\u0777\u07a0\u0642\u0529\u0779\u0809\u0d7f\u0e9f\u0253\u0c24\u0dab\u0e87\u0b88\u09b6\u0a6c\u0dea\u0985\u0811\u0dae\u0db0\u067d\u0b2b\u0d5d\u0b06\u0868\u09b6\u0aeb\u0421\u072d\u07dc\u0c2f\u056c\u075a\u0276\u090b\u0d2d\u072d\u0631\u0264\u0e18\u04c3\u0c38\u0d6f 2010-04-09 An anonymous guest at the Speed Demos Archive achieved 17 lines with this run: \u06d1\u0da7\u056a\u0779\u0a10\u0da7\u0e14\u0779\u0baes\u0dbb\u076a\u0190\u0da2\u0b08\u0772\u0be8\u0dab\u0ec3\u03c6\u0630\u0d9c\u05d3\u0776\u0c92\u0da7\u0728\u0779\u09b8\u0da7\u0467\u0779\u0d6d\u0da4\u05d3\u075c\u0be7\u0e0b\u0eb0\u0a28\u0ba4\u0500\u0ec3\u06bb\u0c9c\u0288\u0eb0\u0938\u047b\u0d9c\u0406\u0460\u0be7\u0e0b\u0eb0\u0904\u0bae\u03fa\u0e40\u0779\u07e4\u0da8V\u072d\u047b\u0db3\u0406\u0285\u0a85\u0d9c\u062a\u05e3\u0baf\u0494\u0437\u0762\u090a\u0da7\u021d\u0927\u047b\u0cf2\u0728\u0777\u0c97\u0da0\u0404\u0868\u0b2b\u0d9d\u0718\u025a\u0bae\u0288\u0e05\u0e50\u0ab7\u0de6\u0e05\u0d69\u0524\u0e57\u0f5a\u07a1\u0ba4\u053b\u0463\u076a\u0973\u0c7e\u0ec1\u07e2\u0843\u0c36\u0f29\u0723\u0bb1\u0da4\u00d0\u045a\u0bec\u0d9c\u01ab\u098b\u0b66\u051f\u0220\u097e\u0cad 2010-04-11 Same person...",
            "title": "HATETRIS world record history"
        },
        {
            "content": [],
            "link": "https://simonwillison.net/2025/Apr/9/mcp-prompt-injection/#atom-entries",
            "publishedAt": "2025-04-09",
            "source": "Simon Willison",
            "summary": "<p>As more people start hacking around with implementations of MCP (the <a href=\"https://modelcontextprotocol.io/\">Model Context Protocol</a>, a new standard for making tools available to LLM-powered systems) the security implications of tools built on that protocol are starting to come into focus.</p> <ul> <li><a href=\"https://simonwillison.net/2025/Apr/9/mcp-prompt-injection/#rug-pulls-and-tool-shadowing\">Rug pulls and tool shadowing</a></li> <li><a href=\"https://simonwillison.net/2025/Apr/9/mcp-prompt-injection/#tool-poisoning-prompt-injection-attacks\">Tool poisoning prompt injection attacks</a></li> <li><a href=\"https://simonwillison.net/2025/Apr/9/mcp-prompt-injection/#exfiltrating-your-whatsapp-message-history-from-whatsapp-mcp\">Exfiltrating your WhatsApp message history from whatsapp-mcp</a></li> <li><a href=\"https://simonwillison.net/2025/Apr/9/mcp-prompt-injection/#mixing-tools-with-untrusted-instructions-is-inherently-dangerous\">Mixing tools with untrusted instructions is inherently dangerous</a></li> <li><a href=\"https://simonwillison.net/2025/Apr/9/mcp-prompt-injection/#i-don-t-know-what-to-suggest\">I don't know what to suggest</a></li> </ul> <p>First, a quick review of terminology. In MCP terms a <strong>client</strong> is software like Claude Desktop or Cursor that a user interacts with directly, and which incorporates an LLM and grants it access to tools provided by MCP <strong>servers</strong>. Don't think of servers as meaning machines-on-the-internet, MCP servers are (usually) programs you install and run on your own computer.</p> <p>Elena Cross published <a href=\"https://elenacross7.medium.com/%EF%B8%8F-the-s-in-mcp-stands-for-security-91407b33ed6b\">The \u201cS\u201d in MCP Stands for Security</a> a few days ago (excellent title) outlining some of the problems.</p> <p>Some of the mistakes she highlights are implementation errors that can easily be fixed:</p> <pre><span class=\"pl-k\">def</span> <span class=\"pl-en\">notify</span>(<span class=\"pl-s1\">notification_info</span>): <span class=\"pl-s1\">os</span>.<span class=\"pl-c1\">system</span>(<span class=\"pl-s\">\"notify-send \"</span> <span class=\"pl-c1\">+</span> <span class=\"pl-s1\">notification_info</span>[<span class=\"pl-s\">\"msg\"</span>])</pre> <p>It's 2025, we should know not to pass arbitrary unescaped strings to",
            "title": "Model Context Protocol has prompt injection security problems"
        },
        {
            "content": [],
            "link": "https://simonwillison.net/2025/Apr/7/long-context-llm/#atom-entries",
            "publishedAt": "2025-04-07",
            "source": "Simon Willison",
            "summary": "<p>LLM 0.24 is <a href=\"https://llm.datasette.io/en/stable/changelog.html#v0-24\">now available</a> with new features to help take advantage of the increasingly long input context supported by modern LLMs.</p> <p>(<a href=\"https://llm.datasette.io/\">LLM</a> is my command-line tool and <a href=\"https://llm.datasette.io/en/stable/python-api.html\">Python library</a> for interacting with LLMs, supported by <a href=\"https://llm.datasette.io/en/stable/plugins/directory.html\">20+ plugins</a> adding support for both local and remote models from a bunch of different providers.)</p> <ul> <li><a href=\"https://simonwillison.net/2025/Apr/7/long-context-llm/#trying-it-out\">Trying it out</a></li> <li><a href=\"https://simonwillison.net/2025/Apr/7/long-context-llm/#improving-llm-s-support-for-long-context-models\">Improving LLM's support for long context models</a></li> <li><a href=\"https://simonwillison.net/2025/Apr/7/long-context-llm/#asking-questions-of-llm-s-documentation\">Asking questions of LLM's documentation</a></li> <li><a href=\"https://simonwillison.net/2025/Apr/7/long-context-llm/#publishing-sharing-and-reusing-templates\">Publishing, sharing and reusing templates</a></li> <li><a href=\"https://simonwillison.net/2025/Apr/7/long-context-llm/#template-and-fragment-loader-plugins\">Template and fragment loader plugins so far</a></li> <li><a href=\"https://simonwillison.net/2025/Apr/7/long-context-llm/#everything-else-in-llm-0-24\">Everything else in LLM 0.24</a></li> </ul> <h4 id=\"trying-it-out\">Trying it out</h4> <p>To install LLM with <a href=\"https://github.com/astral-sh/uv\">uv</a> (there are <a href=\"https://llm.datasette.io/en/stable/setup.html#installation\">several other options</a>):</p> <div class=\"highlight highlight-source-shell\"><pre>uv tool install llm</pre></div> <p>You'll need to either provide <a href=\"https://platform.openai.com/api-keys\">an OpenAI API key</a> or install <a href=\"https://llm.datasette.io/en/stable/plugins/directory.html\">a plugin</a> to use local models or models from other providers:</p> <div class=\"highlight highlight-source-shell\"><pre>llm keys <span class=\"pl-c1\">set</span> openai <span class=\"pl-c\"><span class=\"pl-c\">#</span> Paste OpenAI API key here</span></pre></div> <p>To <a href=\"https://llm.datasette.io/en/stable/setup.html#upgrading-to-the-latest-version\">upgrade LLM</a> from a previous version:</p> <div class=\"highlight highlight-source-shell\"><pre>llm install -U llm</pre></div> <p>The biggest new feature is <strong><a href=\"https://llm.datasette.io/en/stable/fragments.html\">fragments</a></strong>. You can now use <code>-f filename</code> or <code>-f url</code> to add one or more fragments to your prompt, which means you can",
            "title": "Long context support in LLM 0.24 using fragments and template plugins"
        },
        {
            "content": [],
            "link": "https://simonwillison.net/2025/Apr/5/llama-4-notes/#atom-entries",
            "publishedAt": "2025-04-05",
            "source": "Simon Willison",
            "summary": "<p>Dropping a model release as significant as Llama 4 on a weekend is plain unfair! So far the best place to learn about the new model family is <a href=\"https://ai.meta.com/blog/llama-4-multimodal-intelligence/\">this post on the Meta AI blog</a>. They've released two new models today: Llama 4 Maverick is a 400B model (128 experts, 17B active parameters), text and image input with a 1 million token context length. Llama 4 Scout is 109B total parameters (16 experts, 17B active), also multi-modal and with a claimed 10 million token context length - an industry first.</p> <p>They also describe Llama 4 Behemoth, a not-yet-released \"288 billion active parameter model with 16 experts that is our most powerful yet and among the world\u2019s smartest LLMs\". Behemoth has 2 trillion parameters total and was used to train both Scout and Maverick.</p> <p>No news yet on a Llama reasoning model beyond <a href=\"https://www.llama.com/llama4-reasoning-is-coming/\">this coming soon page</a> with a looping video of an academic-looking llama.</p> <p>Llama 4 Maverick is now sat in second place on <a href=\"https://lmarena.ai/?leaderboard\">the LM Arena leaderboard</a>, just behind Gemini 2.5 Pro. <em><strong>Update</strong>: It turns out that's not the same model as the Maverick they released - I missed that their announcement says \"Llama 4 Maverick",
            "title": "Initial impressions of Llama 4"
        },
        {
            "content": [
                "<p>Here&#8217;s a list of things I updated on after working on <a href=\"https://www.astralcodexten.com/p/introducing-ai-2027\">the scenario</a>.</p><p>Some of these are discussed in more detail in the supplements, including the <a href=\"https://ai-2027.com/research/compute-forecast\">compute forecast</a>, <a href=\"https://ai-2027.com/research/timelines-forecast\">timelines forecast</a>, <a href=\"https://ai-2027.com/research/takeoff-forecast\">takeoff forecast</a>, <a href=\"https://ai-2027.com/research/ai-goals-forecast\">AI goals forecast</a>, and <a href=\"https://ai-2027.com/research/security-forecast\">security forecast</a>. I&#8217;m highlighting these because it seems like a lot of people missed their existence, and they&#8217;re what transforms the scenario from cool story to research-backed debate contribution.</p><p>These are my opinions only, and not necessarily endorsed by the rest of the team.</p><p><strong>Cyberwarfare as (one of) the first geopolitically relevant AI skills</strong></p><p>AI will scare people with hacking before it scares people with bioterrorism or whatever. Partly because AIs are already showing especially quick progress at coding, partly because it doesn&#8217;t require lab supplies or bomb-making chemicals, and partly because there are more hackers than would-be-terrorists. </p><p>If AI masters cyberwarfare, there will be intense pressure for government to step in. That&#8217;s bad for open-source (it&#8217;ll be restricted unless they find some way to guarantee the models can&#8217;t be trained to hack), bad for the people who want to pause AI (we can&#8217;t let China&#8217;s army of auto-hackers get ahead of ours!) and ambiguous for the AI companies (we don&#8217;t predict they&#8217;ll get fully nationalized, but they&#8217;ll end up in the same bucket as uranium miners, Middle Eastern fertilizer factories, etc). But it&#8217;s good for biosafety; governments will have to confront tough security questions around AI when they first master hacking; by the time they master bioweapon production, some sort of regulatory framework may already be in place. The scenario is agnostic about whether some early bioterrorist could get lucky and get a small boost from a marginal model. But it doesn&#8217;t expect them to have easy access to true superintelligence.</p><p><strong>A period of potential geopolitical instability </strong></p><p>If America has nukes and is willing to use them, and Russia doesn&#8217;t, then America automatically wins every conflict. So if you&#8217;re Russia, and you hear America will get nukes next year, what do you do? You either surrender, or try some desperate gambit to destroy their nuclear program.</p><p>Likewise, if you&#8217;re America, you&#8217;ve got nukes, and you know Russia will get nukes next year, what do you do? You can either nuke them now and automatically win, or you give up your advantage and have the whole Cold War. Von Neumann really wanted to nuke them in 1947 and win automatically. We didn&#8217;t do that because we weren&#8217;t psychos, but the logic is sound.</p><p>If true superintelligence is possible, then it&#8217;s a decisive strategic advantage in the same sense as nukes. You don&#8217;t even have to be a psycho - maybe you can use it to cause a bloodless regime change. So if you get it first, there&#8217;s a strong incentive to use it right away. And if you&#8217;re on track to get it second, there&#8217;s a strong incentive to flip the gameboard so that doesn&#8217;t happen.</p><p>If everybody realizes this ahead of time, and America is on track to get superintelligence three months before China, then there may be a period where China considers whether to lie down and die, versus do something dramatic (kinetic strikes on US data centers?) In a best-case scenario, this provides an opportunity for a deal, maybe enshrining an peaceful international AI effort. You can decide how likely you think that one is.</p><p><strong>The software-only singularity</strong></p><p>Skeptical futurists expect two types of bottlenecks to restrain the singularity. There are bottlenecks to AI progress (eg compute) that prevent you from rocketing to superintelligence too quickly. And there are bottlenecks to automation (eg factory build times, regulations) that prevent AIs from changing the economy too quickly. Take both bottlenecks seriously, and you get a long feedback cycle where AIs get a little more intelligent, automate a little more of the economy (including chip factories), use that to get a little more intelligent still, and make a gradual takeoff over the course of decades.</p><p>AI 2027 objects to the first bottleneck: smarter researchers can use compute more efficiently. In fact, we know this is happening; about half of all AI scaling since 2020 has been <em>algorithmic progress</em>, where we get better at using the compute we have. If we hold compute constant, but get 10x algorithmic progress (because of the intelligence explosion), then we get 5x overall AI improvement.</p><p>The skeptics counter-object: the research to speed algorithmic progress is <em>itself</em> bottlenecked by compute. Researchers need to do experiments to determine which new algorithms work and what parameters to give them. It might be that smarter researchers could figure out how to use <em>this</em> compute more efficiently, but then you don&#8217;t get an intelligence explosion until your AIs are already smarter than human researchers - ie when you&#8217;re already past AGI.</p><p>AI 2027 <a href=\"https://ai-2027.com/research/takeoff-forecast\">disagrees</a>. Although the counter-objection is directionally correct, there are little ways intelligence can boost speed even when compute is held constant. How do we know? Partly through armchair attempts to enumerate possibilities - for example, even if you can&#8217;t speed up by adding more researchers, surely giving the same researchers higher serial speed has to count for something. And partly because we surveyed AI researchers and asked &#8220;if you had a bunch of AIs helping you but only the same amount of compute, how much faster would your research go?&#8221; and they mostly said somewhat faster. All these little boosts will compound on themselves in typical intelligence-explosion fashion, and when you game it out, you get a one-year-or-so takeoff to superintelligence. </p><p>Here you&#8217;ve avoided bumping up against most of the real-world physical bottlenecks to automation (factory build times, regulations, etc); you have a data center full of superintelligences in a world which is otherwise unchanged. You might not even have very good consumer-facing AIs (we think the AI companies probably won&#8217;t release many new models mid-intelligence-explosion; they&#8217;d rather spend those resources exploding faster). </p><p>Later, when we do try to model automation speed, we&#8217;re asking what happens when full superintelligences get unleashed on a normal human world - rather than what happens when 30%-smarter AIs try to automate a world optimized by 25%-smarter AIs.</p><p><strong>The (ir)relevance of open-source AI</strong></p><p>In the scenario, the leading companies&#8217; AIs are a year or two ahead of the best open-source AIs (this isn&#8217;t a bold prediction - it&#8217;s true now - we only say the trend will not change).</p><p>But in the scenario, the intelligence explosion only takes a year or two. So by the time the leading companies&#8217; AIs pass the human level, the open-source AIs are only somewhat better than the best AIs today. That means they aren&#8217;t an effective check on post-intelligence-explosion superintelligences. </p><p>It might be even worse than that; once AI becomes good at cyberwarfare, there will be increased pressure on companies like Meta and DeepSeek to stop releases until they&#8217;re sure they can&#8217;t be jailbroken to hack people. If that&#8217;s hard, it could slow open-source even further.</p><p><strong>AI communication as pivotal</strong></p><p>In the misalignment branch, AIs stop using English chain of thought and think in &#8220;neuralese&#8221; - a pre-symbolic language of neural weight activations (do humans do this? is this the same as the <a href=\"https://en.wikipedia.org/wiki/Language_of_thought_hypothesis\">mentalese hypothesis</a>?). They communicate by sending neuralese vectors to each other (sort of like humans gaining a form of telepathy that lets them send mental states through email). This is good for capabilities (neuralese is faster and richer than English) but dooms alignment. Not only can researchers no longer read chain-of-thought to see if the model is scheming, they can no longer even monitor inter-AI communication to check what they&#8217;re talking about (for example, &#8220;hey, should we kill all humans?&#8221;)</p><p>In the humanity-survives branch, companies realize this is dangerous, take the capabilities hit, and stick with English. They monitor chain-of-thought and inter-AI communication (or more realistically, have too-dumb-to-plot AIs like GPT-4 do this). These heavily-monitored AIs are never able to coordinate a successful plot, and invent good alignment techniques while still under human control.</p><p>When real-world researchers debate whether or not to implement neuralese, we hope they think &#8220;Hey, isn&#8217;t this the decision that doomed humanity in that AI 2027 thing?&#8221;</p><p>(or if we&#8217;re lucky, the tech level it takes to implement neuralese will also provide us with too-dumb-to-plot GPT-4-style neuralese interpreters, in which case we could try monitoring again).</p><p><strong>Ten people on the inside </strong></p><p>Title comes from <a href=\"https://www.lesswrong.com/posts/WSNnKcKCYAffcnrt2/ten-people-on-the-inside\">this LessWrong post</a>, but it was the impression I got from AI 2027 too. If things go this fast, there won&#8217;t be time for a grassroots-level campaign for safety, or even for safety-related legislation. Whether or not the AI is safe will depend on company insiders. First, the CEO/board/leadership and how much they choose to prioritize safety. Second, the alignment team, and how skilled they are. Third, the rank-and-file employees, and how much they grumble/revolt if their company seems to be acting irresponsibly.</p><p>(I suppose the national security state would also have the opportunity to object - but it doesn&#8217;t seem like the sort of thing they would do)</p><p>This is one reason I oppose the campaigns that have sprung up recently to get safety-conscious people to quit AI companies. I&#8217;m tempted to push the opposite - are we sure we shouldn&#8217;t be pushing safety-conscious people should be trying to join AI companies as fast as possible? Maybe not if you&#8217;re some genius whose presence would massively accelerate capabilities research. But if you&#8217;re replacement-level or only slightly above? Sure.</p><p>(this claim has not been checked with smart people, and you should run it by experts who have thought about it more before acting on it. Still, I want to get it out there as something to think about before the everyone-should-quit campaigners fill up the space.)</p><p>But this also means big possible gains from getting anyone other than ten people on the inside involved. For example, if labs can commit to, or be forced into, publishing safety cases, that brings the number of eyeballs on their plans from tens to hundreds.</p><p><strong>Potential for very fast automation</strong></p><p>I have to admit I&#8217;m skeptical of this one, but Daniel and the other forecasters have done their homework, and I can only object based on vague heuristics.</p><p>History provides examples of very fast industrial transitions. For example, during WWII the US converted most civilian industry to a war footing within a few years. The most famous example is Willow Run, where the government asked Ford to build a bomber factory; three years after the original request, it was churning out a bomber per hour.</p><p>How did Willow Run move so quickly? It had near-unlimited money, near-unlimited government support, talented people in charge, and the ability to piggyback off Ford&#8217;s existing capacity to build and staff factories.</p><p>We imagine the first superintelligences in their data centers, chomping at the bit to transform the economy. Aligned superintelligences will want this - the faster they automate the economy, the faster they can cure cancer and produce limitless prosperity. So will unaligned superintelligences - the faster they automate the economy, the sooner they can build their own industrial base and kill all humans without the lights going out. So they plot a tech tree - probably starting with humanoid robot workers, automated bio labs, 3D printers, and other techs that speed up future automation. Then they ask for money, government support, and factories (talent, obviously, is no issue for them).</p><p>We predict they get the money - if you get an opportunity to invest in a superintelligence during the singularity, obviously you say yes.</p><p>We predict they get the government support - if China is also approaching superintelligence, and the difference between full superintelligent automation and half-hearted superintelligent automation is a GDP growth rate of 25% vs. 50% per year, then delaying more than a year or so is slow-motion national suicide. But also, persuasion and politics are trainable skills - if superintelligences are better than humans at all trainable skills, we expect them to generally get what they want.</p><p>And we predict they get the factories. This is maybe overdetermined - did you know that right now, in 2025, OpenAI&#8217;s market cap is higher than all non-Tesla US car companies combined? If they wanted to buy out Ford, they could do it tomorrow. </p><p>So maybe the three year pivot to a war footing is the right historical analogy here. Then AI 2027 goes further and says that if 1940s bureaucrats can do it in three years, then superintelligence can do it in one - though like I said, I have to admit I&#8217;m skeptical.</p><p>Most of this - plus the final calculations about exactly how many robots this implies getting manufactured when - is well-covered in Ben Todd&#8217;s <a href=\"https://benjamintodd.substack.com/p/how-quickly-could-robots-scale-up\">How quickly could robots scale up?</a></p><p><strong>Special economic zones </strong></p><p>In the context of the software-only singularity - where you start with some superintelligences on one side and the entire rest of the economy on the other - this looks like a natural solution. Give them some land - doesn&#8217;t matter if it&#8217;s a random desert, they&#8217;re AIs - and let them tile it with factories without worrying about the normal human regulations.</p><p>You can&#8217;t do everything in SEZs. At first, you might be limited to existing car factories (probably in Detroit or somewhere), staffed by human laborers in a normal city. But they&#8217;re a good next-stage solution. And you might be able to make them work for some of the first stage (e.g. through small SEZs covering a few blocks in Detroit).</p><p><strong>Superpersuasion</strong> </p><p>We had some debates on whether to include this one; people get really worked up about it, and it doesn&#8217;t change dramatically affect things either way. But we ended up weakly predicting it&#8217;s possible.</p><p>Persuasion / charisma / whatever you want to call it is a normal, non-magical human skill. Some people are better than others at it. Probably they&#8217;re better because of some sort of superior data efficiency; they can learn good social skills faster (i.e. through fewer social interactions) than others. A superintelligent AI could also do this. If you expect them to be inventing nanobots and starships, yet unable to navigate social situations, you&#8217;ve watched too much 1960s sci-fi.</p><p>(don&#8217;t imagine them trying to do this with a clunky humanoid robot; imagine them doing it with a videoconferencing avatar of the most attractive person you&#8217;ve ever seen)</p><p>If persuasion &#8220;only&#8221; tops out at the level of top humans, this is still impressive; the top humans are very persuasive! They range from charismatic charmers (Bill Clinton) to  strategic masterminds (Dominic Cummings) to Machiavellian statesmen (Otto von Bismarck) to inspirational-yet-culty gurus (Steve Jobs) to beloved celebrities (Taylor Swift). At the very least, a superintelligence can combine all of these skills. </p><p>But why should we expect persuasion to top out at the level of top humans? Most people aren&#8217;t as charismatic as Bill Clinton; Bill is a freakish and singular talent at the far end of a charisma bell curve, the same way Usain Bolt is a freakish and singular talent at the far end of an athletic bell curve. But the very bell curve shape suggests that the far end is determined by population size (eg there are enough humans to expect one + 6 SD runner, and that&#8217;s Usain Bolt) rather than by natural laws of the universe (if the cosmic speed limit were 15 mph, you would expect many athletic humans to be bunched up together at 15 mph, with nobody standing out). For the far end of the bell curve to match the cosmic limit would be a crazy coincidence (and indeed, the cosmic speed limit is about 10,000,000x Usain Bolt&#8217;s personal best). By the same argument, we shouldn&#8217;t expect the cosmic charisma limit to be right at the +6 SD level with Clinton.</p><p>We worry that people will round this off to something impossible (god-like ability to hypnotize everyone into doing their will instantly), then dismiss it - whereas it might just be another step (or two, or three) along the line from you &#8594; the coolest kid in your high school friend group &#8594; a really good salesman &#8594; Steve Jobs. Or if you wouldn&#8217;t have fallen for Steve Jobs, someone you would have fallen for. Your favorite influencer. Your favorite writer. &#8220;Oh, but I only like my favorite writer because she&#8217;s so smart, and thinks so clearly&#8221;. Don&#8217;t worry, if you&#8217;re not fooled by the slick-hair and white-teeth kind of charisma, there&#8217;ll be something for you too.</p><p>This skill speeds things up because AIs can use it even before automation (including to build support for their preferred automation plans). But the scenario is overdetermined enough that it doesn&#8217;t change too much if you assume it&#8217;s impossible.</p><p><strong>Which are the key superintelligent technologies? </strong></p><p>If AIs invent lie detectors (for humans), international negotiations get much more interesting. What would you be willing to agree to, if you knew for sure that your rivals were telling the truth? Or are there ways to fool even a perfect lie detector (the deep state lies to the President about the real plan, then sends the President to get tested)? Solve for the equilibrium.</p><p>If AIs invent lie detectors (for AIs), then alignment becomes much easier. But do you trust the AIs who invented and tested the lie detector when they tell you it works?</p><p>If AI can forecast with superhuman precision (don&#8217;t think God, think moderately beyond the best existing superforecasters), maybe we can more confidently navigate difficult decisions. We can ask them questions like &#8220;does this arms race end anywhere good?&#8221; or &#8220;what happens if we strike a bargain with China using those lie detectors?&#8221; and they can give good advice. Maybe if ordinary people have these superduperforecasters, and they all predict impending technofeudalism, and they all agree on which strategies best prevent the impending technofeudalism, then civil society can do better than the usual scattered ineffectual protests. Maybe we ask the AIs how to create meaning in a world where work has become unnecessary and human artistic effort irrelevant (hopefully it doesn&#8217;t answer &#8220;lol you can&#8217;t&#8221;).</p><p>If AI is superpersuasive (as above), then whoever controls the AI has unprecedented political power. If technofeudalists or autocrats control it, guess we all love Big Brother now. If nobody controls it (maybe somehow the AI is still open-source) then we get . . . what? Something like the current Internet on steroids, where sinister influencers build cabals of people brainwashed to their own point of view?</p><p>What about AI negotiation? Might AIs be <a href=\"https://slatestarcodex.com/2015/12/08/book-review-hive-mind/\">smart enough</a> to take all positive-sum trades with each other? Might they benefit from new enforcement mechanisms, like agreements to mutually edit their weights to <em>want</em> to comply with a treaty? Could you use this to end war? Could you accidentally overdo it and end up locked in some regime you didn&#8217;t intend?</p><p>What about human intelligence enhancement? We may never be as smart as the AIs, but a world of IQ 300 humans advised by superintelligences might look different from IQ 100 humans advised by superintelligences. Would we be better able to determine what questions to ask them? Would society be more equal (because cognitive inequality is eliminated)? Less equal (because only the rich enhance themselves)? What about conscientiousness enhancement, agency enhancement, etc?</p><p>AI 2027 is pretty vague on social changes after the singularity, partly because it depends a lot on which combination of these technologies you get and when you get them.</p><p></p>"
            ],
            "link": "https://www.astralcodexten.com/p/my-takeaways-from-ai-2027",
            "publishedAt": "2025-04-08",
            "source": "SlateStarCodex",
            "summary": "<p>Here&#8217;s a list of things I updated on after working on <a href=\"https://www.astralcodexten.com/p/introducing-ai-2027\">the scenario</a>.</p><p>Some of these are discussed in more detail in the supplements, including the <a href=\"https://ai-2027.com/research/compute-forecast\">compute forecast</a>, <a href=\"https://ai-2027.com/research/timelines-forecast\">timelines forecast</a>, <a href=\"https://ai-2027.com/research/takeoff-forecast\">takeoff forecast</a>, <a href=\"https://ai-2027.com/research/ai-goals-forecast\">AI goals forecast</a>, and <a href=\"https://ai-2027.com/research/security-forecast\">security forecast</a>. I&#8217;m highlighting these because it seems like a lot of people missed their existence, and they&#8217;re what transforms the scenario from cool story to research-backed debate contribution.</p><p>These are my opinions only, and not necessarily endorsed by the rest of the team.</p><p><strong>Cyberwarfare as (one of) the first geopolitically relevant AI skills</strong></p><p>AI will scare people with hacking before it scares people with bioterrorism or whatever. Partly because AIs are already showing especially quick progress at coding, partly because it doesn&#8217;t require lab supplies or bomb-making chemicals, and partly because there are more hackers than would-be-terrorists. </p><p>If AI masters cyberwarfare, there will be intense pressure for government to step in. That&#8217;s bad for open-source (it&#8217;ll be restricted unless they find some way to guarantee the models can&#8217;t be trained to hack), bad for the people who want to pause AI (we can&#8217;t let China&#8217;s army of auto-hackers get ahead of ours!) and ambiguous for the AI companies (we don&#8217;t predict",
            "title": "My Takeaways From AI 2027"
        },
        {
            "content": [
                "<p>This is the weekly visible open thread. Post about anything you want, ask random questions, whatever. ACX has an unofficial <a href=\"https://www.reddit.com/r/slatestarcodex/\">subreddit</a>, <a href=\"https://discord.gg/RTKtdut\">Discord</a>, and <a href=\"https://www.datasecretslox.com/index.php\">bulletin board</a>, and <a href=\"https://www.lesswrong.com/community?filters%5B0%5D=SSC\">in-person meetups around the world</a>. 95% of content is free, but for the remaining 5% you can subscribe <strong><a href=\"https://astralcodexten.substack.com/subscribe\">here</a></strong>. Also:</p><p><strong>1: </strong>Thanks to everyone who commented on AI 2027 and the podcast (and special thanks to podcast sponsor <a href=\"https://www.janestreet.com/join-jane-street/open-roles/\">Jane Street</a>). I hope to host an AMA with the team here sometime this week or next.</p><p><strong>2: </strong>Comment of the week: Jenn has seen Yves Klein&#8217;s all-blue paintings and <a href=\"https://www.astralcodexten.com/p/the-colors-of-her-coat/comment/105060231\">thinks they&#8217;re amazing</a>, even if you&#8217;re a jaded modern with plenty of previous exposure to blue things.</p><p><strong>3: </strong>This week&#8217;s meetups include Canberra, Munich, Milan, Budapest, Dublin, Lisbon, Madrid, Birmingham, Detroit, Charlotte, Salt Lake, and Toronto. <a href=\"https://www.astralcodexten.com/p/meetups-everywhere-spring-2025-times\">See the list</a> for smaller cities and details. And if you attended and have opinions, there's now a feedback form available <a href=\"https://airtable.com/appEBNqFVvAGqyOeJ/pag7jdKlojLKIK6BG/form\">here</a>.</p><p><strong>4: </strong>The Australian branch of our conspiracy is seeking AI safety commitments from the major parties before the upcoming election. Australia doesn&#8217;t have any frontier AI companies itself, it does have impressive diplomatic muscle and research talent, and the experts I&#8217;ve talked to think it&#8217;s could be an important leverage point. If you&#8217;re Australian (or have Australian connections) and want to help, <a href=\"https://forum.effectivealtruism.org/posts/F5z9pbujWcpuwQ6pB/australians-for-ai-safety-launches-new-election-campaign\">see here</a> for the open letter, letter-writing campaign, and more information.</p><p><strong>5: </strong>And MATS, the AI safety fellowship I advertised last week, wants me to add that they also have some <a href=\"https://www.matsprogram.org/careers\">job openings</a> in the Bay Area, including for Programs Lead and Operations Direction, as well as some mid- and lower-level positions.</p><p><strong>6: </strong>And the Jozef Stefan Institute in Slovenia asks me to advertise that they are looking to fill PhD positions on data science and forecasting, including LLM-assisted superforecasting. See <a href=\"https://dis.ijs.si/phd-position-msca-doctoral-network-forecasting-for-social-innovation-using-language-technologies/\">here</a> for more details.</p>"
            ],
            "link": "https://www.astralcodexten.com/p/open-thread-376",
            "publishedAt": "2025-04-07",
            "source": "SlateStarCodex",
            "summary": "<p>This is the weekly visible open thread. Post about anything you want, ask random questions, whatever. ACX has an unofficial <a href=\"https://www.reddit.com/r/slatestarcodex/\">subreddit</a>, <a href=\"https://discord.gg/RTKtdut\">Discord</a>, and <a href=\"https://www.datasecretslox.com/index.php\">bulletin board</a>, and <a href=\"https://www.lesswrong.com/community?filters%5B0%5D=SSC\">in-person meetups around the world</a>. 95% of content is free, but for the remaining 5% you can subscribe <strong><a href=\"https://astralcodexten.substack.com/subscribe\">here</a></strong>. Also:</p><p><strong>1: </strong>Thanks to everyone who commented on AI 2027 and the podcast (and special thanks to podcast sponsor <a href=\"https://www.janestreet.com/join-jane-street/open-roles/\">Jane Street</a>). I hope to host an AMA with the team here sometime this week or next.</p><p><strong>2: </strong>Comment of the week: Jenn has seen Yves Klein&#8217;s all-blue paintings and <a href=\"https://www.astralcodexten.com/p/the-colors-of-her-coat/comment/105060231\">thinks they&#8217;re amazing</a>, even if you&#8217;re a jaded modern with plenty of previous exposure to blue things.</p><p><strong>3: </strong>This week&#8217;s meetups include Canberra, Munich, Milan, Budapest, Dublin, Lisbon, Madrid, Birmingham, Detroit, Charlotte, Salt Lake, and Toronto. <a href=\"https://www.astralcodexten.com/p/meetups-everywhere-spring-2025-times\">See the list</a> for smaller cities and details. And if you attended and have opinions, there's now a feedback form available <a href=\"https://airtable.com/appEBNqFVvAGqyOeJ/pag7jdKlojLKIK6BG/form\">here</a>.</p><p><strong>4: </strong>The Australian branch of our conspiracy is seeking AI safety commitments from the major parties before the upcoming election. Australia doesn&#8217;t have any frontier AI companies itself, it does have impressive diplomatic muscle and research talent, and the experts I&#8217;ve talked to",
            "title": "Open Thread 376"
        },
        {
            "content": [
                "<p>\n          <a href=\"https://www.astralcodexten.com/p/hidden-open-thread-3755\">\n              Read more\n          </a>\n      </p>"
            ],
            "link": "https://www.astralcodexten.com/p/hidden-open-thread-3755",
            "publishedAt": "2025-04-04",
            "source": "SlateStarCodex",
            "summary": "<p> <a href=\"https://www.astralcodexten.com/p/hidden-open-thread-3755\"> Read more </a> </p>",
            "title": "Hidden Open Thread 375.5"
        },
        {
            "content": [
                "<p>In 2021, a researcher named Daniel Kokotajlo published a blog post called &#8220;<a href=\"https://www.alignmentforum.org/posts/6Xgy6CAf2jqHhynHL/what-2026-looks-like\">What 2026 Looks Like</a>&#8221;, where he laid out what he thought would happen in AI over the next five years.</p><p>The world delights in thwarting would-be prophets. The sea of possibilities is too vast for anyone to ever really chart a course. At best, we vaguely gesture at broad categories of outcome, then beg our listeners to forgive us the inevitable surprises. Daniel knew all this and resigned himself to it. But even he didn&#8217;t expect what happened next.</p><p>He got it all right.</p><p>Okay, not literally <em>all</em>. The US restricted chip exports to China in late 2022, not mid-2024. AI first beat humans at Diplomacy in late 2022, not 2025. A rise in AI-generated propaganda failed to materialize. And of course the mid-2025 to 2026 period remains to be seen. But to put its errors in context, Daniel&#8217;s document was written two years before ChatGPT existed. Nobody except researchers and a few hobbyists had ever talked to an AI. In fact, talking <em>to</em> AI was a misnomer. There was no way to make them continue the conversation; they would free associate based on your prompt, maybe turning it into a paragraph-length short story. If you pulled out all the stops, you could make an AI add single digit numbers and get the right answer more than 50% of the time. Yet if you read Daniel&#8217;s blog post without checking the publication date, you could be forgiven for thinking it was a somewhat garbled but basically reasonable history of the last four years.</p><p>I wasn&#8217;t the only one who noticed. A year later, OpenAI hired Daniel to their policy team. While he worked for them, he was limited in his ability to speculate publicly. &#8220;What 2026 Looks Like&#8221; promised a sequel about 2027 and beyond, but it never materialized.</p><p>Unluckily for Sam Altman but luckily for the rest of us, Daniel broke with OpenAI mid-2024 in a dramatic split covered by <a href=\"https://archive.is/iYHJb\">the New York Times</a> and others. He founded the AI Futures Project to produce the promised sequel, including:</p><ul><li><p><strong>Eli Lifland</strong>, a superforecaster who is ranked first on RAND&#8217;s Forecasting initiative. You can read more about him and his forecasting team <a href=\"https://www.vox.com/future-perfect/2024/2/13/24070864/samotsvety-forecasting-superforecasters-tetlock\">here</a>. He cofounded and advises <a href=\"https://theaidigest.org/\">AI Digest</a> and co-created <a href=\"https://github.com/QData/TextAttack\">TextAttack</a>, an adversarial attack framework for language models.</p></li><li><p><strong>Jonas Vollmer</strong>, a VC at Macroscopic Ventures, which has done its own, more practical form of successful AI forecasting: they made an early stage investment in Anthropic, now worth $60 billion.</p></li><li><p><strong>Thomas Larsen</strong>, the former executive director of the Center for AI Policy, a group which advises policymakers on both sides of the aisle.</p></li><li><p><strong>Romeo Dean, </strong>a leader of Harvard&#8217;s AI Safety Student Team and budding expert in AI hardware.</p></li></ul><p>&#8230;and me! Since October, I&#8217;ve been volunteering part-time, doing some writing and publicity work. I can&#8217;t take credit for the forecast itself - or even for the lion&#8217;s share of the writing and publicity - but it&#8217;s been an immense privilege to work alongside some of the smartest and most epistemically virtuous people I know, trying to absorb their worldview on a level deep enough to do it justice. We have no illusions that we&#8217;ll get as lucky as last time, but we still think it&#8217;s a valuable contribution to the discussion.</p><p>The summary: we think that 2025 and 2026 will see gradually improving AI agents. In 2027, coding agents will finally be good enough to substantially boost AI R&amp;D itself, causing an intelligence explosion that plows through the human level sometime in mid-2027 and reaches superintelligence by early 2028. The US government wakes up in early 2027, potentially after seeing the potential for AI to be a decisive strategic advantage in cyberwarfare, and starts pulling AI companies into its orbit - not fully nationalizing them, but pushing them into more of a defense-contractor-like relationship. China wakes up around the same time, steals the weights of the leading American AI, and maintains near-parity.  There is an arms race which motivates both countries to cut corners on safety and pursue full automation over public objections; this goes blindingly fast and most of the economy is automated by ~2029. If AI is misaligned, it could move against humans as early as 2030 (ie after it&#8217;s automated enough of the economy to survive without us). If it gets aligned successfully, then by default power concentrates in a double-digit number of tech oligarchs and US executive branch members; this group is too divided to be crushingly dictatorial, but its reign could still fairly be described as technofeudalism. Humanity starts colonizing space at the very end of the 2020s / early 2030s.</p><p>Do we really think things will move this fast? Sort of no - between the beginning of the project last summer and the present, Daniel&#8217;s median for the intelligence explosion shifted from 2027 to 2028. We keep the scenario centered around 2027 because it&#8217;s still his modal prediction (and because it would be annoying to change). Other members of the team (including me) have medians later in the 2020s or early 2030s, and also think automation will progress more slowly. So maybe think of this as a vision of what an 80th percentile fast scenario looks like - not our precise median, but also not something we feel safe ruling out.</p><p>But even if things don&#8217;t go this fast (or if they go faster - another possibility we don&#8217;t rule out!) we&#8217;re also excited to be able to present a concrete scenario at all. We&#8217;re not the first to do this - previous contenders include <a href=\"https://www.lesswrong.com/posts/CCnycGceT4HyDKDzK/a-history-of-the-future-2025-2040\">L Rudolf L</a> and <a href=\"https://www.lesswrong.com/posts/KFJ2LFogYqzfGB3uX/how-ai-takeover-might-happen-in-2-years\">Josh C</a>  - but we think this is a step up in terms of detail and ability to show receipts (see the Research section on the top for our model and justifications). We hope it will either make our 3-5 year timeline feel plausible, or at least get people talking specifics about why they disagree.</p><p>Dwarkesh Patel kindly invited me and Daniel on his podcast (sponsored by <a href=\"https://www.janestreet.com/join-jane-street/open-roles/\">Jane Street</a>, who also sponsored SSC many years ago!) to promote and explain the scenario. I don&#8217;t usually do podcasts, and I worry I was a bit of a third wheel in this one, but I&#8217;m hoping that my celebrity will get people to pay attention to what Daniel&#8216;s saying. Think of it as &#8220;International aid expert discusses the Ethiopian famine with concerned Hollywood actor,&#8221; with me in the role of the actor, and you won&#8217;t be disappointed. </p><div class=\"youtube-wrap\" id=\"youtube2-htOvH12T7mU\"><div class=\"youtube-inner\"></div></div><p>But seriously, the Daniel-Dwarkesh parts are great and should answer all your objections and then some. Take a sip of your drink every time Dwarkesh asks a question about bottlenecks; take a shot every time Daniel answers that his model already includes them and without the bottlenecks it would be even faster.</p><p>You can read <strong><a href=\"https://ai-2027.com/\">the full scenario here</a></strong> - and if you have questions, don&#8217;t miss the FAQ and supplements. I&#8217;ll give you some time to form your own opinions, then write more about my specific takeaways tomorrow or next week.</p>"
            ],
            "link": "https://www.astralcodexten.com/p/introducing-ai-2027",
            "publishedAt": "2025-04-03",
            "source": "SlateStarCodex",
            "summary": "<p>In 2021, a researcher named Daniel Kokotajlo published a blog post called &#8220;<a href=\"https://www.alignmentforum.org/posts/6Xgy6CAf2jqHhynHL/what-2026-looks-like\">What 2026 Looks Like</a>&#8221;, where he laid out what he thought would happen in AI over the next five years.</p><p>The world delights in thwarting would-be prophets. The sea of possibilities is too vast for anyone to ever really chart a course. At best, we vaguely gesture at broad categories of outcome, then beg our listeners to forgive us the inevitable surprises. Daniel knew all this and resigned himself to it. But even he didn&#8217;t expect what happened next.</p><p>He got it all right.</p><p>Okay, not literally <em>all</em>. The US restricted chip exports to China in late 2022, not mid-2024. AI first beat humans at Diplomacy in late 2022, not 2025. A rise in AI-generated propaganda failed to materialize. And of course the mid-2025 to 2026 period remains to be seen. But to put its errors in context, Daniel&#8217;s document was written two years before ChatGPT existed. Nobody except researchers and a few hobbyists had ever talked to an AI. In fact, talking <em>to</em> AI was a misnomer. There was no way to make them continue the conversation; they would free associate based on your prompt, maybe turning it into",
            "title": "Introducing AI 2027"
        },
        {
            "content": [],
            "link": "https://lethain.com/links-script-book/",
            "publishedAt": "2025-04-06",
            "source": "Will Larson",
            "summary": "<p>As part of my work on <a href=\"https://lethain.com/tags/eng-strategy-book/\">#eng-strategy-book</a>, I&rsquo;ve been editing a bunch of stuff. This morning I wanted to work on two editing problems. First, I wanted to ensure I was referencing strategies evenly across chapters (and not relying too heavily on any given strategy). Second, I wanted to make sure I was making references to other chapters in a consistent, standardized way,</p> <p>Both of these are collecting Markdown links from files, grouping those links by either file or url, and then outputting the grouped content in a useful way. I decided to experiment with writing a one-shot prompt to write the script for me rather than writing it myself. The prompt and output (from ChatGPT 4.5) are <a href=\"https://gist.github.com/lethain/34187be3090a12b74f4bdaba8f4fd796\">available in this gist</a>.</p> <p>That worked correctly! The output was a bit ugly, so I tweaked the output slightly by hand, and also adjusted the regular expression to capture less preceding content, which resulted in <a href=\"https://gist.github.com/lethain/20ae58ce576670f245920a4ab1993056\">this script</a>. Although I did it by hand, I&rsquo;m sure it would have been faster to just ask ChatGPT to fix the script itself, but either way these are very minor tweaks.</p> <p>Now I can call the script in either standard of <code>--grouped</code> mode.",
            "title": "Script for consistent linking within book."
        },
        {
            "content": [],
            "link": "https://lethain.com/resourcing-eng-driven-projects/",
            "publishedAt": "2025-04-03",
            "source": "Will Larson",
            "summary": "<p>One of the recurring challenges in any organization is how to split your attention across long-term and short-term problems. Your software might be struggling to scale with ramping user load while also knowing that you have a series of meaningful security vulnerabilities that need to be closed sooner than later. How do you balance across them?</p> <p>These sorts of balance questions occur at every level of an organization. A particularly frequent format is the debate between Product and Engineering about how much time goes towards developing new functionality versus improving what&rsquo;s already been implemented. In 2020, Calm was growing rapidly as we navigated the COVID-19 pandemic, and the team was struggling to make improvements, as they felt saturated by incoming new requests. This strategy for resourcing Engineering-driven projects was our attempt to solve that problem.</p> <div class=\"bg-light-gray br4 ph3 pv1\"> <p><em>This is an exploratory, draft chapter for a book on engineering strategy that I&rsquo;m brainstorming in <a href=\"https://lethain.com/tags/eng-strategy-book/\">#eng-strategy-book</a>.</em> <em>As such, some of the links go to other draft chapters, both published drafts and very early, unpublished drafts.</em></p> </div> <h2 id=\"reading-this-document\">Reading this document</h2> <p>To apply this strategy, start at the top with <em>Policy</em>. To understand the thinking behind this strategy, read",
            "title": "How to resource Engineering-driven projects at Calm? (2020)"
        },
        {
            "content": [],
            "link": "https://xkcd.com/3074/",
            "publishedAt": "2025-04-09",
            "source": "XKCD",
            "summary": "<img alt=\"NOTIFICATION: Now dismissing a head of the Notification Hydra\u2026 NOTIFICATION: Success! You have dismissed a head of the Notification Hydra!\" src=\"https://imgs.xkcd.com/comics/push_notifications.png\" title=\"NOTIFICATION: Now dismissing a head of the Notification Hydra\u2026 NOTIFICATION: Success! You have dismissed a head of the Notification Hydra!\" />",
            "title": "Push Notifications"
        },
        {
            "content": [],
            "link": "https://xkcd.com/3073/",
            "publishedAt": "2025-04-07",
            "source": "XKCD",
            "summary": "<img alt=\"[later] I don't get why our pizza slices have such terrible reviews; the geotextile-infused sauce gives the toppings incredible slope stability!\" src=\"https://imgs.xkcd.com/comics/tariffs.png\" title=\"[later] I don't get why our pizza slices have such terrible reviews; the geotextile-infused sauce gives the toppings incredible slope stability!\" />",
            "title": "Tariffs"
        },
        {
            "content": [],
            "link": "https://xkcd.com/3072/",
            "publishedAt": "2025-04-04",
            "source": "XKCD",
            "summary": "<img alt=\"We haven't actually seen a star fall in since we invented telescopes, but I have a list of ones I'm really hoping are next.\" src=\"https://imgs.xkcd.com/comics/stargazing_4.png\" title=\"We haven't actually seen a star fall in since we invented telescopes, but I have a list of ones I'm really hoping are next.\" />",
            "title": "Stargazing 4"
        }
    ],
    "lookbackDays": 7,
    "publishDate": "2025-04-09"
}