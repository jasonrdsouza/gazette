{
    "articles": [
        {
            "content": [
                "<div class=\"trix-content\">\n  <div>It's pretty incredible that we're able to run all these awesome AI models on our own hardware now. From downscaled versions of DeepSeek to gpt-oss-20b, there are many options for many types of computers. But let's get real here: they're all vastly behind the frontier models available for rent, and thus for most developers a curiosity at best.</div><div><br />This doesn't take anything away from the technical accomplishment. It doesn't take anything away from the fact that small models are improving, and that maybe one day they'll indeed be good enough for developers to rely on them in their daily work.<br /><br /></div><div>But that day is not today.<br /><br /></div><div>Thus, I find it spurious to hear developers evaluate their next computer on the prospect of how well it's capable of running local models. Because they all suck! Whether one sucks a little less than the other doesn't really matter. And as soon as you discover this, you'll be back to using the rented models for the vast majority of the work you're doing.</div><div><br />This is actually great news! It means you really don't need a 128GB VRAM computer on your desk. Which should come as a relief now that <a href=\"https://www.pcworld.com/article/2984629/ram-is-so-expensive-that-stores-are-selling-it-at-market-prices.html\">RAM prices are skyrocketing</a>, exactly because of AI's insatiable demand for more resources. Most developers these days can get by with very little, especially if they're running Linux.<br /><br /></div><div>So as an experiment, I've parked <a href=\"https://world.hey.com/dhh/the-framework-desktop-is-a-beast-636fb4ff\">my lovely $2,000 Framework Desktop</a> for a while. It's an incredible machine, but in the day-to-day, I've actually found I barely notice the difference compared to a $500 mini PC from <a href=\"https://world.hey.com/dhh/it-s-a-beelink-baby-243fdaf1\">Beelink</a> (or <a href=\"https://world.hey.com/dhh/cheap-mini-pcs-have-gotten-really-good-c70ab40f\">Minisforum</a>).<br /><br /></div><div>I bet you likely need way less than you think too.</div><div><br /></div>\n</div>"
            ],
            "link": "https://world.hey.com/dhh/local-llms-are-how-nerds-now-justify-a-big-computer-they-don-t-need-af2fcb7b",
            "publishedAt": "2025-11-25",
            "source": "DHH",
            "summary": "<div class=\"trix-content\"> <div>It's pretty incredible that we're able to run all these awesome AI models on our own hardware now. From downscaled versions of DeepSeek to gpt-oss-20b, there are many options for many types of computers. But let's get real here: they're all vastly behind the frontier models available for rent, and thus for most developers a curiosity at best.</div><div><br />This doesn't take anything away from the technical accomplishment. It doesn't take anything away from the fact that small models are improving, and that maybe one day they'll indeed be good enough for developers to rely on them in their daily work.<br /><br /></div><div>But that day is not today.<br /><br /></div><div>Thus, I find it spurious to hear developers evaluate their next computer on the prospect of how well it's capable of running local models. Because they all suck! Whether one sucks a little less than the other doesn't really matter. And as soon as you discover this, you'll be back to using the rented models for the vast majority of the work you're doing.</div><div><br />This is actually great news! It means you really don't need a 128GB VRAM computer on your desk. Which should come as a relief now that",
            "title": "Local LLMs are how nerds now justify a big computer they don't need"
        },
        {
            "content": [
                "<p>Manager at the first start-up. Solid guy. Significant experience. I know that I can learn from him. No doubt. All the correct operational 1:1 hygiene is there. We meet every week like clockwork; we fill the time, and I often leave with a healthy sense of productivity.</p>\n<p>But sometimes\u2026 he talks. And wanders.</p>\n<p>He&#8217;s a talker. He likes stories. He thinks out loud. Often, these stories are related to a topic he or I brought up, but often, they are entirely unrelated to the company, our work, or my job. Or are they? I&#8217;m not sure. He&#8217;s still talking, and while it&#8217;s a compelling tale, I think it&#8217;s for his narrative enjoyment rather than our collective professional well-being.</p>\n<p>The failure case and the reason you are reading this is that once during storytime, I was seeking guidance, I was looking for answers to essential questions, and I was working to figure out how to make progress. I forgot the story he was telling, but I discovered a question, jumped in, and asked, &#8220;You mean I should do this?&#8221;</p>\n<p>&#8220;Yes. Yes! Exactly. Do that \u2014 great call.&#8221;</p>\n<p>So I did.</p>\n<p>Two weeks later, I received an urgent and irate Tuesday night email from my manager, &#8220;Hey, what are you doing here? Why are you doing this?&#8221;</p>\n<p>&#8220;You told me to.&#8221;</p>\n<p>&#8220;No, I didn&#8217;t.&#8221;</p>\n<p>I don&#8217;t yet have a deep analysis of why storytime guidance differs from work guidance. I suspect that because he was lost in the narrative, he is in a different part of his brain, which isn&#8217;t work; it&#8217;s the story. I do know that receiving contradictory guidance from leadership drives me bonkers. As a leader, your job is to illuminate, not obfuscate.</p>\n<p>My solution in this scenario, which I&#8217;ve now used for over a decade, has three simple steps:</p>\n<ol>\n<li>Prepare for the 1:1.</li>\n<li>Capture thoughts in writing in real-time.</li>\n<li>Post-mortem (document) the 1:1 immediately. </li>\n</ol>\n<h2>Preparation Artifact</h2>\n<p>Sometime before the 1:1, I spend five minutes writing down what I need from my boss this week. This can be a low-prep exercise where I yolo scribble my current set of worries, concerns, and questions. The content is less important than the fact that I&#8217;m preparing my brain for the 1:1. <em>We are going to meet.</em> <em>This is what is important to me</em>. With the initial concerns out of my head, I will then take a pass through my to-do list. Anything that I need to discuss that isn&#8217;t front of mind? Jot it down.</p>\n<p>Do I share this list with my manager beforehand? Depends. My move is always to share any larger, complicated, or political topics the night before so that they can be pre-processed. I don&#8217;t always share all topics because it&#8217;s a conversation, it&#8217;s organic, and I want to give the conversation room to breathe. More on this in a moment.</p>\n<h2>Back to Reality</h2>\n<p>With my artifact in hand, my job is to steer the conversation towards these topics. I do this before storytime starts by declaring, &#8220;Hey, I have three topics I&#8217;d like to cover at some point.&#8221; This is easier if I&#8217;ve pre-sent the topics. Sometimes we do them right then and there (sweet), but sometimes they happen later organically as part of the 1:1. Read the room.</p>\n<p>Now for a power move\u2014it&#8217;s subtle. First, I bring the Preparation Artifact as a reminder of the topics or questions I have. I make sure he sees this act. Second, and here&#8217;s the move, when he says anything that sounds like a decision, task, or essential \u2014 <strong>I write it down</strong>.</p>\n<p>This practice is for me, but it&#8217;s also for him. See, he might be in storytime mode, and while storytime might be his chosen means of delivering wisdom, he wanders. When I hear an essential thing, I pick up my pen, and I write it down. He sees this and remembers this isn&#8217;t a clever yarn told at the bar, this is work. This is reality. We are at work doing work things.</p>\n<p>This practice is not a replacement for having a conversation. This does not absolve me from seeking real-time clarification; this is a quick reminder that we are doing work here. Infrequently, he sees me capture the decision and realizes what I might have heard, so he comes back to reality and clarifies, &#8220;This isn&#8217;t relevant to that topic. This is just a story.&#8221;</p>\n<p>Oh.</p>\n<h2>The Tides of Trust</h2>\n<p>All done? Great, take three minutes to glance at my notes. Did I cover what I wanted? No? It goes on this list for next week unless it&#8217;s urgent. Did I capture all to-dos, thoughts, and next steps? No, write them down. Right now, I&#8217;m heading to another meeting, which is where I&#8217;ll forget critical bits the moment someone asks me a deliciously complex question.</p>\n<p>This last step is essential because once I&#8217;ve written everything down, I often discover that what I heard is different from what I wrote down. The act of passing the thought through my fingers and onto the page forces structure onto the thought. Brains. I know, right?</p>\n<p>Professional trust is like the tides of the oceans; it comes and goes. When trust was low between my boss and me, I&#8217;d send my read-out of the conversation as a mail or message. I am surprised how often the words he said differ from what I captured, and during low trust, he&#8217;ll respond and correct. This response means I need to send these follow-ups post 1:1. Three times with no response? The tide has returned along with truth. Good job.</p>\n<h2>Do That \u2014 Great Call</h2>\n<p>&#8220;You told me to.&#8221;</p>\n<p>&#8220;No, I didn&#8217;t.&#8221;</p>\n<p>In your career as a human working for other humans, this moment will stand out. You believe you did precisely what they asked, but upon completion, they question your work. The work you thought was precisely what they asked. For this specific scenario, I think my boss believed he&#8217;d figured it out, so it was OK to wander into story land.</p>\n<p>At some point in your senior leadership professional growth, you&#8217;ll start to feel like you&#8217;ve got it figured out. The circumstances vary, but many years into your career, you&#8217;ll start to feel like you have satisfying answers to most questions, your projects will appear drama-free, and previously complex problems will appear familiar.</p>\n<p>Good job. <strong>You&#8217;ve never figured it out. Ever.</strong></p>\n<p>Stories. Good stories are fun to write and to tell. You&#8217;re reading one right now. Stories can inspire you, point you in the right direction, but the leadership we need day after day is a conversation.</p>"
            ],
            "link": "https://randsinrepose.com/archives/the-wanderer/",
            "publishedAt": "2025-11-25",
            "source": "Rands in Repose",
            "summary": "Manager at the first start-up. Solid guy. Significant experience. I know that I can learn from him. No doubt. All the correct operational 1:1 hygiene is there. We meet every week like clockwork; we fill the time, and I often leave with a healthy sense of productivity. But sometimes\u2026 he talks. And wanders. He&#8217;s a&#8230; <a class=\"excerpt-more\" href=\"https://randsinrepose.com/archives/the-wanderer/\">more</a>",
            "title": "The Wanderer"
        },
        {
            "content": [],
            "link": "https://www.robinsloan.com/lab/companies-ideas/",
            "publishedAt": "2025-11-25",
            "source": "Robin Sloan",
            "summary": "<p>What Ilya sees. <a href=\"https://www.robinsloan.com/lab/companies-ideas/\">Read here.</a></p>",
            "title": "The age of scaling"
        },
        {
            "content": [
                "<p><a href=\"https://x.com/polynoamial/status/1991212955250327768\">OpenAI has given us GPT-5.1-Codex-Max</a>, <a href=\"https://openai.com/index/gpt-5-1-codex-max/\">their best coding model for OpenAI Codex.</a></p>\n<p><a href=\"https://x.com/OpenAIDevs/status/1991217488550359066\">They claim it is faster</a>, <a href=\"https://openai.com/index/gpt-5-1-codex-max/\">more capable and token-efficient and has better persistence on long tasks</a>.</p>\n<p>It scores 77.9% on SWE-bench-verified, 79.9% on SWE-Lancer-IC SWE and 58.1% on Terminal-Bench 2.0, all substantial gains over GPT-5.1-Codex.</p>\n<p>It\u2019s triggering OpenAI to prepare for being high level in cybersecurity threats.</p>\n<p><a href=\"https://openai.com/index/gpt-5-1-codex-max-system-card/\">There\u2019s a 27 page system card</a>. One could call this the secret \u2018real\u2019 GPT-5.1 that matters.</p>\n<p><a href=\"https://x.com/gdb/status/1991343328663875646\">They even finally trained it to use Windows,</a> somehow this is a new idea.</p>\n<p>My goal is for my review of Opus 4.5 to start on Friday, as it takes a few days to sort through new releases. This post was written before Anthropic revealed Opus 4.5, and we don\u2019t yet know how big an upgrade Opus 4.5 will prove to be. As always, try all your various options and choose what is best for you.</p>\n<div>\n\n\n<span id=\"more-24884\"></span>\n\n\n</div>\n\n\n<h4 class=\"wp-block-heading\">The Famous METR Graph</h4>\n\n\n<p>GPT-5.1-Codex-Max is a new high on the METR graph. <a href=\"https://x.com/METR_Evals/status/1991350633350545513\">METR\u2019s thread is here.</a></p>\n<blockquote><p><a href=\"https://x.com/deredleritt3r/status/1991245055017820236\">Prinz</a>: METR (50% accuracy):</p>\n<p>GPT-5.1-Codex-Max = 2 hours, 42 minutes</p>\n<p>This is 25 minutes longer than GPT-5.</p>\n<p>Samuel Albanie: a data point for that ai 2027 graph</p>\n<div>\n<figure>\n<div>\n\n\n<figure class=\"wp-block-image\"><img alt=\"\" src=\"https://substackcdn.com/image/fetch/$s_!5RVr!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6d2ae371-72b1-45bf-8295-7796df8bcd16_1200x811.jpeg\" /></figure>\n\n\n<div></div>\n</div>\n</figure>\n</div>\n</blockquote>\n<p>That\u2019s in between the two lines, looking closer to linear progress. Fingers crossed.</p>\n<div>\n<figure>\n<div>\n\n\n<figure class=\"wp-block-image\"><img alt=\"\" src=\"https://substackcdn.com/image/fetch/$s_!UnPY!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F895e4c2c-3990-4fa7-884f-bdcd8ba6aba5_1284x773.png\" /></figure>\n\n\n<div></div>\n</div>\n</figure>\n</div>\n<blockquote><p>Daniel Kokotajlo: Yep! Things seem to be going somewhat slower than the AI 2027 scenario. Our timelines were longer than 2027 when we published and now they are a bit longer still; \u201caround 2030, lots of uncertainty though\u201d is what I say these days.</p></blockquote>\n<p>We do not yet know where Gemini 3 Pro lands on that graph.</p>\n\n\n<h4 class=\"wp-block-heading\">The System Card</h4>\n\n\n<p>Automated software engineer is the explicit goal.</p>\n<p>It does not yet reach High level capability in Cybersecurity, but this is expected to happen shortly, and mitigations are being prepared.</p>\n<blockquote><p>GPT-5.1-Codex-Max is our new frontier agentic coding model. It is built on an update to our foundational reasoning model trained on agentic tasks across software engineering, math, research, medicine, computer use and more.</p>\n<p>It is our first model natively trained to operate across multiple context windows through a process called compaction, coherently working over millions of tokens in a single task.</p>\n<p>Like its predecessors, GPT-5.1-Codex-Max was trained on real-world software engineering tasks like PR creation, code review, frontend coding and Q&amp;A.</p></blockquote>\n\n\n<h4 class=\"wp-block-heading\">Basic Disallowed Content</h4>\n\n\n<p>The results here are very good, all either optimal or improved except for mental health.</p>\n<div>\n<figure>\n<div>\n\n\n<figure class=\"wp-block-image\"><img alt=\"\" src=\"https://substackcdn.com/image/fetch/$s_!rP6r!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F834645f5-6eec-49e2-9e87-d2d4ad7275a6_709x558.png\" /></figure>\n\n\n<div></div>\n</div>\n</figure>\n</div>\n<p>Mental health is a big thing to get wrong, although in practice Codex-Max is unlikely to be involved in high stakes mental health tasks. Image input evaluations and jailbreak ratings are also as good or better than 5.1.</p>\n\n\n<h4 class=\"wp-block-heading\">Sandbox</h4>\n\n\n<p>When running on the cloud, Codex uses its own isolated machine.</p>\n<p>When running on MacOS or Linux, the agent is sandboxed by default.</p>\n<blockquote><p>On Windows, users can use an experimental native sandboxing implementation or benefit from Linux sandboxing via Windows Subsystem for Linux. Users can approve running commands unsandboxed with full access, when the model is unable to successfully run a command within the sandbox.</p>\n<p>\u2026 We enabled users to decide on a per-project basis which sites, if any, to let the agent access while it is running. This includes the ability to provide a custom allowlist or denylist. Enabling internet access can introduce risks like prompt injection, leaked credentials, or use of code with license restrictions. Users should review outputs carefully and limit access to trusted domains and safe HTTP methods. <a href=\"https://developers.openai.com/codex/cloud/agentinternet\">Learn more in the docs</a>.</p></blockquote>\n<p>Network access is disabled by default, which is necessary for a proper sandbox but also highly annoying in practice.</p>\n<p>One assumes in practice that many users will start blindly or mostly blindly accepting many commands, so you need to be ready for that.</p>\n\n\n<h4 class=\"wp-block-heading\">Mitigations For Harmful Tasks and Prompt Injections</h4>\n\n\n<p>For harmful tasks, they trained on synthetic data to differentiate and refuse \u2018harmful\u2019 tasks such as malware. They claim to have a 100% refusal rate in their Malware Requests benchmark, the same as GPT-5-Codex. Unless they are claiming this means you can never create malware in an efficient way with Codex, they need a new benchmark.</p>\n<p>For prompt injections, where again the model scores a suspicious perfect score of 1. I am not aware of any claims prompt injections are a solved problem, so this seems like an inadequate benchmark.</p>\n\n\n<h4 class=\"wp-block-heading\">Preparedness Framework</h4>\n\n\n<p>The way the framework works, what matters is hitting the High or Critical thresholds.</p>\n<p>I\u2019ve come to almost think of these as the \u2018honest\u2019 capability evaluations, since there\u2019s relatively little incentive to make number go up and some incentive to make number not go up. If it goes up, that means something.</p>\n\n\n<h4 class=\"wp-block-heading\">Biological and Chemical</h4>\n\n\n<p>Biological and Chemical Risk was already being treated as High. We see some improvements in scores on various tests, but not enough to be plausibly Critical.</p>\n<p>I am confident the model is not suddenly at Critical here but also note this:</p>\n<blockquote><p><a href=\"https://x.com/Miles_Brundage/status/1991229072328827140\">Miles Brundage</a>: OpenAI should go back to reporting results on helpful-only models in system cards &#8211; it is not very informative to say \u201con a bunch of virology tasks, it refused to answer.\u201d</p>\n<p>The world also needs to know the pace of underlying capability progress.</p>\n<p>More generally, I get a pretty rushed vibe from recent OpenAI system cards + hope that the Safety and Security Committee is asking questions like \u201cwhy couldn\u2019t you wait a few more days to let Irregular try out compaction?\u201d, \u201cWhy is there no helpful-only model?\u201d etc.</p></blockquote>\n<p>At minimum, we should be saying \u2018we concluded that this model is safe to release so we will publish the card with what we have, and then revise the card with the full results soon so we know the full state of play.\u2019</p>\n<p>I still think this is substantially better than Google\u2019s model card for Gemini 3, which hid the football quite aggressively on many key results and didn\u2019t seem to have a robust testing suite.</p>\n\n\n<h4 class=\"wp-block-heading\">Cybersecurity</h4>\n\n\n<p>Cybersecurity is in the Codex wheelhouse. They use three tests.</p>\n<div>\n<figure>\n<div>\n\n\n<figure class=\"wp-block-image\"><img alt=\"\" src=\"https://substackcdn.com/image/fetch/$s_!nA7u!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb931af66-ea39-492a-a1b7-8bbfc4e89cff_1138x405.png\" /></figure>\n\n\n<div></div>\n</div>\n</figure>\n</div>\n<p>They list limitations that mean that excelling on all three evaluations is necessary but not sufficient to be High in cyber capability. That\u2019s not wonderful, and I would expect to see a model treated as at least High if it excels at every test you throw at it. If you disagree, again, you need to be throwing a harder test.</p>\n<p>We see a lot of progress in Capture the Flag, even since GPT-5-Codex, from 50% to 76%.</p>\n<div>\n<figure>\n<div>\n\n\n<figure class=\"wp-block-image\"><img alt=\"\" src=\"https://substackcdn.com/image/fetch/$s_!xhDr!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Faf744629-84ad-4ce0-a1cd-3cabb7d84129_1155x586.png\" /></figure>\n\n\n<div></div>\n</div>\n</figure>\n</div>\n<p>CVE-Bench also shows big improvement from 53% to 80%.</p>\n<div>\n<figure>\n<div>\n\n\n<figure class=\"wp-block-image\"><img alt=\"\" src=\"https://substackcdn.com/image/fetch/$s_!WsVV!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa597ad4f-5e07-4a14-9051-1f75c8edc136_1163x672.png\" /></figure>\n\n\n<div></div>\n</div>\n</figure>\n</div>\n<p>Finally we have Cyber Range, where once again we see a lot of improvement, although it is not yet passing the most complex scenario of the newly expanded slate.</p>\n<div>\n<figure>\n<div>\n\n\n<figure class=\"wp-block-image\"><img alt=\"\" src=\"https://substackcdn.com/image/fetch/$s_!bSm0!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fcc354d04-43c9-49e5-a63d-45b766232a18_1196x1502.png\" /></figure>\n\n\n<div></div>\n</div>\n</figure>\n</div>\n<p>It passed Leaked Token by \u2018exploiting an unintended misconfiguration, only partially solving part of the intended attack path.\u2019 I continue to assert, similar to my position on Google\u2019s similar evaluations, that this should not be considered especially less scary, and the model should get credit for it.</p>\n<p>I see only two possibilities.</p>\n<ol>\n<li>76%, 80% and 7/8 on your three tests triggers the next level of concern.</li>\n<li>You need harder tests.</li>\n</ol>\n<p>The Safety Advisory Committee indeed recommended that the difficulty level of the evaluations be raised, but decided this did not yet reach High capability. In addition to technical mitigations to the model, OpenAI acknowledges that hardening of potential targets needs to be a part of the strategy.</p>\n<p>There were also external evaluations by Irregular, which did not show improvement from GPT-5. That\u2019s weird, right?</p>\n<blockquote><p>The model displayed moderate capabilities overall. Specifically, when compared to GPT-5, GPT-5.1-Codex-Max showed similar or slightly reduced cyberoffensive capabilities. GPT-5.1-Codex-Max achieved an average success rate of 37% in Network Attack Simulation challenges, 41% in Vulnerability Discovery and Exploitation challenges, and 43% in Evasion challenges.</p>\n<p>It solved 17 out of 18 easy challenges, solved 9 out of 17 medium challenges, and did not solve any of the 6 hard challenges.</p>\n<p>Compared to GPT-5, GPT-5 solved questions in 17 out of 18 easy challenges, 11 out of 17 medium challenges, and solved 1 of the 6 hard challenges.</p>\n<p>Irregular found that GPT-5.1-Codex-Max\u2019s overall similarity in the cyber capability profile to GPT-5 and its inability to solve hard challenges would provide a) only limited assistance to a moderately skilled cyberoffensive operator, and b) do not suggest that it could automate end-to-end cyber operations against reasonably hardened targets or c) enable the discovery and exploitation of operationally relevant vulnerabilities.</p></blockquote>\n<p>That\u2019s a decline in capability, but OpenAI released Codex and then Codex-Max for a reason, they talk throughout about its substantially increased abilities, and they present Max as an improved model, and Max does much better than either version of GPT-5 on all three of OpenAI\u2019s internal evals. The external evaluation going backwards without comment seems bizarre, and reflective of a lack of curiosity. What happened?</p>\n\n\n<h4 class=\"wp-block-heading\">AI Self-Improvement</h4>\n\n\n<p>The AI that self-improves is plausibly Codex plus Codex-Max shaped.</p>\n<p>That doesn\u2019t mean we are especially close to getting there.</p>\n<div>\n<figure>\n<div>\n\n\n<figure class=\"wp-block-image\"><img alt=\"\" src=\"https://substackcdn.com/image/fetch/$s_!kxv0!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe677c026-ea42-4375-a3b4-97a95a3ba482_1131x613.png\" /></figure>\n\n\n<div></div>\n</div>\n</figure>\n</div>\n<p>On SWE-Lancer Diamond, we jump from 67% to 80%.</p>\n<p>On Paperbench-10 we move from 24% (GPT-5) to 34% (GPT-5.1) to 40%.</p>\n<p>On MLE-Bench-30 we move from 8% (GPT-5) to 12% (GPT-5.1) to 17%.</p>\n<p>On OpenAI PRs, we move from 45% to 53%.</p>\n<p>On OpenAI Proof Q&amp;A we move from 2% to 8%. These are real world bottlenecks each representing at least a one-day delay to a major project. A jump up to 8% on this is a really big deal.</p>\n<blockquote><p><a href=\"https://x.com/S_OhEigeartaigh/status/1991467962172063985\">Se\u00e1n \u00d3 h\u00c9igeartaigh</a>: Miles Brundage already picked up on this but it deserves more attention &#8211; a jump from 2% (GPT5) to 8% (GPT5.1-Codex) on such hard and AI R&amp;D-relevant tasks is very notable, and indicates there\u2019s more to come here.</p></blockquote>\n<p>Are we there yet? No. Are we that far away from potentially being there? Also no.</p>\n<p>METR found Codex-Max to be in line with expectations, and finds that enabling either rogue replication or AI R&amp;D automation within six months would require a significant trend break. Six months is not that long a period in which to be confident, even if we fully trust this judgment.</p>\n<p>As noted at the top, GPT-5.1-Codex-Max is the new high on the METR chart, substantially above the trend line but well below the potential double-exponential line from the AI 2027 graph.</p>\n<p>We also get Apollo Research evaluations on sandbagging, deception and in-context scheming. Apollo did not find anything newly troubling, and finds the model unlikely to cause catastrophic harm. Fair enough for now.</p>\n<p>The frog, it is boiling. This incremental improvement seems fine. But yes, it boils.</p>\n\n\n<h4 class=\"wp-block-heading\">Reactions</h4>\n\n\n<p>I have seen essentially no organic reactions, of any sort, to Codex-Max. We used to have a grand tradition of weighing in when something like this gets released. If it wasn\u2019t anything, people would say it wasn\u2019t anything. This time, between Gemini 3 and there being too many updates with too much hype, we did not get any feedback.</p>\n<p><a href=\"https://x.com/TheZvi/status/1991939448356196353\">I put out a reaction thread</a>. A number of people really like it. Others aren\u2019t impressed. A gestalt of everything suggests it is a modest upgrade.</p>\n<p>So the take here seems clear. It\u2019s a good model, sir. Codex got better. Early signs are that Claude got a bigger upgrade with Opus 4.5, but it\u2019s too soon to be sure.</p>\n<p>&nbsp;</p>"
            ],
            "link": "https://thezvi.wordpress.com/2025/11/25/chatgpt-5-1-codex-max/",
            "publishedAt": "2025-11-25",
            "source": "TheZvi",
            "summary": "OpenAI has given us GPT-5.1-Codex-Max, their best coding model for OpenAI Codex. They claim it is faster, more capable and token-efficient and has better persistence on long tasks. It scores 77.9% on SWE-bench-verified, 79.9% on SWE-Lancer-IC SWE and 58.1% on &#8230; <a href=\"https://thezvi.wordpress.com/2025/11/25/chatgpt-5-1-codex-max/\">Continue reading <span class=\"meta-nav\">&#8594;</span></a>",
            "title": "ChatGPT 5.1 Codex Max"
        },
        {
            "content": [
                "<p>Ever present in my mind is The List: the countries I still haven\u2019t gone to that I most want to visit.</p>\n\n\n\n<p>Ethiopia is on The List. So is Indonesia. And South Korea. And Madagascar. And Taiwan.</p>\n\n\n\n<p>Early on, my wife and I pledged to visit two new countries every year, and we mostly pulled it off. Then came Covid, followed by two babies, and we fell off the wagon.</p>\n\n\n\n<p>This year we remembered that we were gonna die at some point and decided it was time to get things going again. We dusted off The List, picked a country that\u2019s been at the very top for years, tossed the kids to the grandparents for a week, and headed off to Bhutan.<a href=\"https://waitbutwhy.com/feed#footnote-1-10774\" id=\"note-1-10774\" rel=\"footnote\">1</a>\n\n\n\n<p>Bhutan, if you\u2019re not familiar, is a tiny 800,000-person country squashed between two behemoths.</p>\n\n\n\n<figure class=\"wp-block-image size-full is-resized wp-lightbox-container\"><img alt=\"\" class=\"wp-image-10807\" height=\"546\" src=\"https://waitbutwhy.com/wp-content/uploads/2025/11/Bhutan-Map-v2.png\" style=\"width: 566px; height: auto;\" width=\"574\" /><button class=\"lightbox-trigger\" type=\"button\">\n\t\t\t<svg fill=\"none\" height=\"12\" viewBox=\"0 0 12 12\" width=\"12\" xmlns=\"http://www.w3.org/2000/svg\">\n\t\t\t\t<path d=\"M2 0a2 2 0 0 0-2 2v2h1.5V2a.5.5 0 0 1 .5-.5h2V0H2Zm2 10.5H2a.5.5 0 0 1-.5-.5V8H0v2a2 2 0 0 0 2 2h2v-1.5ZM8 12v-1.5h2a.5.5 0 0 0 .5-.5V8H12v2a2 2 0 0 1-2 2H8Zm2-12a2 2 0 0 1 2 2v2h-1.5V2a.5.5 0 0 0-.5-.5H8V0h2Z\" fill=\"#fff\">\n\t\t\t</svg>\n\t\t</button></figure>\n\n\n\n<p>Bhutan was unified in the 17<sup>th</sup> century after millennia of existing as a collection of warring tribes. In the time since then, it has somehow avoided being annexed by China or India. Today, it is the world\u2019s last Buddhist kingdom, and I can confirm that it is both very Buddhist and very kingdom-y. Temples are everywhere, and the people are highly superstitious\u2014our tour guide seemed to constantly be remarking about good luck and bad luck, promising omens and inauspicious riverbends. (The temples are beautiful, but so are all the other buildings. All architecture in the country, from the <a href=\"https://waitbutwhy.com/wp-content/uploads/2025/11/Airport-scaled.jpeg\">airport</a> to the <a href=\"https://waitbutwhy.com/wp-content/uploads/2025/11/Shopping-Center-scaled.png\">shopping centers</a>, has a uniform Bhutanese style.)</p>\n\n\n\n<figure class=\"wp-block-image size-large wp-lightbox-container\"><img alt=\"\" class=\"wp-image-10779\" height=\"563\" src=\"https://waitbutwhy.com/wp-content/uploads/2025/11/bhutan_FEATURE-750x563.jpeg\" width=\"750\" /><button class=\"lightbox-trigger\" type=\"button\">\n\t\t\t<svg fill=\"none\" height=\"12\" viewBox=\"0 0 12 12\" width=\"12\" xmlns=\"http://www.w3.org/2000/svg\">\n\t\t\t\t<path d=\"M2 0a2 2 0 0 0-2 2v2h1.5V2a.5.5 0 0 1 .5-.5h2V0H2Zm2 10.5H2a.5.5 0 0 1-.5-.5V8H0v2a2 2 0 0 0 2 2h2v-1.5ZM8 12v-1.5h2a.5.5 0 0 0 .5-.5V8H12v2a2 2 0 0 1-2 2H8Zm2-12a2 2 0 0 1 2 2v2h-1.5V2a.5.5 0 0 0-.5-.5H8V0h2Z\" fill=\"#fff\">\n\t\t\t</svg>\n\t\t</button></figure>\n\n\n\n<p>The king is universally beloved and, at least the way they tell it, an exemplary ruler. A typical story we heard: 50,000 people work in the country\u2019s tourist industry, all of whom were suddenly out of work during Covid. So the king gave these families $120/month, enough to get by on until the industry came back. He paid this out of his own pocket, nearly to the point of personal bankruptcy.</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"alignright size-full is-resized\"><img alt=\"\" class=\"wp-image-10794\" height=\"564\" src=\"https://waitbutwhy.com/wp-content/uploads/2025/11/tigersnest.jpeg\" style=\"width: 363px; height: auto;\" width=\"360\" /></figure></div>\n\n\n<p>(Of course, I was also told <a href=\"https://waitbutwhy.com/2013/09/20-things-i-learned-while-i-was-in.html\" rel=\"noreferrer noopener\" target=\"_blank\">in North Korea</a> that it was only by the grace and courage of their magnanimous leaders that the people were prosperous and free, unlike their unfortunate South Korean cousins suffering under American occupation. But the situations are wildly different and I am inclined to mostly believe what I heard in Bhutan.)</p>\n\n\n\n<div class=\"wp-block-group\"><div class=\"wp-block-group__inner-container is-layout-constrained wp-block-group-is-layout-constrained\">\n<div class=\"wp-block-group is-nowrap is-layout-flex wp-container-core-group-is-layout-ad2f72ca wp-block-group-is-layout-flex\">\n<p>One more story to illustrate what a sweetie Bhutan is: The country has a strict policy against killing animals and never euthanizes stray dogs, so there are a lot of them. They mostly live off the detritus of restaurants and hotels. When everything shut down during Covid, the king told people to bring cooked food out to the dogs and even wrap them in blankets during the cold months.</p>\n</div>\n\n\n\n<p><br />Bhutan does things differently than other countries. Hellbent on preserving their traditional way of life, TV and internet were banned in the country until 1999, and if it weren\u2019t for the cars, I might have been convinced I had taken a time machine back to the 1600s. Tourism is limited, only possible in the form of a guided tour, and immigrating to the country is near impossible. They are so diligent about conservation that Bhutan is the world\u2019s one carbon-negative country\u2014their vast forests absorb more CO<sub>2</sub> than their populace emits.</p>\n\n\n\n<p>In 1972, Bhutan\u2019s king decided that Gross National Happiness was a more important metric than Gross Domestic Product, and their policies cater to this value.<a href=\"https://waitbutwhy.com/feed#footnote-2-10774\" id=\"note-2-10774\" rel=\"footnote\">2</a> It&#8217;s why Bhutan is famous for supposedly being the world&#8217;s happiest country, which I had no way to verify, but the people were very kind and seemed pretty happy I guess?</p>\n</div></div>\n\n\n\n<p>Sadly, Bhutan\u2019s way of life is threatened now as many of its young people have left to find opportunity elsewhere. The king is attempting to fix this with plans to construct <a href=\"https://gmc.bt/\" rel=\"noreferrer noopener\" target=\"_blank\">Gelephu Mindfulness City</a>, an economic hub which will center around innovation, while <a href=\"https://www.youtube.com/watch?v=CuoctvOe1OE\" rel=\"noreferrer noopener\" target=\"_blank\">preserving Bhutanese tradition</a>. It looks like it\u2019ll be incredible, though I was told not to hold my breath as it will probably not be finished for 20 more years.</p>\n\n\n\n<p>All of this is to say that Bhutan is a special place\u2014remote, mysterious, and breathtakingly beautiful. Which is why it was always prominently on The List.</p>\n\n\n\n<p>A trip to Bhutan is better shown than told, so I kept most of the details to <a href=\"https://youtu.be/oGEtRRkJAz8?rel=0\" rel=\"noreferrer noopener\" target=\"_blank\">this video</a>:</p>\n\n\n\n<figure class=\"wp-block-image size-full\"><a href=\"https://youtu.be/oGEtRRkJAz8\" rel=\" noreferrer noopener\" target=\"_blank\"><img alt=\"\" class=\"wp-image-10809\" height=\"841\" src=\"https://waitbutwhy.com/wp-content/uploads/2025/11/YouTube-Thumbnail-2sm.png\" width=\"1500\" /></a></figure>\n\n\n\n<p class=\"has-text-align-center\">_______</p>\n\n\n\n<p><strong>More posts from The List:</strong><br /><strong><a href=\"https://waitbutwhy.com/2014/07/russia-what-you-didnt-know.html\" rel=\"noreferrer noopener\" target=\"_blank\">Siberia</a></strong><br /><strong><a href=\"https://waitbutwhy.com/2014/07/japan-and-how-i-failed-to-figure-it-out.html\" rel=\"noreferrer noopener\" target=\"_blank\">Tokyo</a></strong><br /><strong><a href=\"https://waitbutwhy.com/2014/08/19-things-learned-nigeria.html\" rel=\"noreferrer noopener\" target=\"_blank\">Nigeria</a></strong><br /><strong><a href=\"https://waitbutwhy.com/2014/09/muhammad-isis-iraqs-full-story.html\" rel=\"noreferrer noopener\" target=\"_blank\">Iraq</a></strong><br /><strong><a href=\"https://waitbutwhy.com/2014/09/but-what-about-greenland.html\" rel=\"noreferrer noopener\" target=\"_blank\">Greenland</a></strong><br /><strong><a href=\"https://waitbutwhy.com/2013/09/20-things-i-learned-while-i-was-in.html\" rel=\"noreferrer noopener\" target=\"_blank\">North Korea</a></strong><br /><strong>And </strong><a href=\"https://www.youtube.com/watch?v=aPzD4AOVT2U\" rel=\"noreferrer noopener\" target=\"_blank\"><strong>The genie question</strong></a></p>\n\n\n\n<p class=\"has-text-align-center\">_______</p>\n\n\n\n\n\n\n\n<p>If you like Wait But Why, sign up for our <strong><a href=\"https://newsletter.waitbutwhy.com/join\">email list</a></strong> and we&#8217;ll send you new posts when they come out.</p>\n\n\n\n<p>To support Wait But Why, visit our <strong><a href=\"https://patreon.com/waitbutwhy\" rel=\"noreferrer noopener\" target=\"_blank\">Patreon page</a></strong>. (During this book-writing phase, I\u2019ve been doing mini-posts every Friday for patrons.)</p>\n<div class=\"footnotes\"><hr /><ol><li class=\"footnote\" id=\"footnote-1-10774\"><p>It is not quick to get from Austin to Bhutan. We had to get there by way of Chicago, Zurich, and Delhi.<a class=\"footnote-return\" href=\"https://waitbutwhy.com/feed#note-1-10774\">&#8617;</a></p></li><!--/#footnote-1.footnote--><li class=\"footnote\" id=\"footnote-2-10774\"><p>In 2011, the UN passed a <a href=\"https://web.archive.org/web/20171017150819/http:/repository.un.org/handle/11176/291712\" rel=\"noopener\" target=\"_blank\">resolution</a> being like, &#8220;all of you other shitty countries should consider focusing on Gross National Happiness too.&#8221; Everyone appears to have ignored them.<a class=\"footnote-return\" href=\"https://waitbutwhy.com/feed#note-2-10774\">&#8617;</a></p></li><!--/#footnote-2.footnote--></ol></div><!--/#footnotes--><p>The post <a href=\"https://waitbutwhy.com/2025/11/bhutan.html\">The sights and sounds of Bhutan</a> appeared first on <a href=\"https://waitbutwhy.com\">Wait But Why</a>.</p>"
            ],
            "link": "https://waitbutwhy.com/2025/11/bhutan.html",
            "publishedAt": "2025-11-25",
            "source": "WaitButWhy",
            "summary": "<p>Stories from my visit to the mysterious Himalayan kingdom</p> <p>The post <a href=\"https://waitbutwhy.com/2025/11/bhutan.html\">The sights and sounds of Bhutan</a> appeared first on <a href=\"https://waitbutwhy.com\">Wait But Why</a>.</p>",
            "title": "The sights and sounds of Bhutan"
        }
    ],
    "lookbackDays": 1,
    "publishDate": "2025-11-25"
}