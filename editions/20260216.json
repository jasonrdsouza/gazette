{
    "articles": [
        {
            "content": [
                "<header>\n  <h1>Diagnostics Factory</h1>\n  <time class=\"meta\" datetime=\"2026-02-16\">Feb 16, 2026</time>\n</header>\n<p>In\n<span class=\"display\"><a href=\"https://matklad.github.io/2025/11/06/error-codes-for-control-flow.html\"><em>Error Codes For Control Flow</em></a>,</span>\nI explained that Zig\u2019s strongly-typed error codes solve the \u201chandling\u201d half of error management,\nleaving \u201creporting\u201d to the users. Today, I want to describe my personal default approach to\nthe reporting problem, that is, showing the user a useful error message.</p>\n<p>The approach is best described in the negative: <em>avoid</em> thinking about error payloads, and what\nthe type of error should be. Instead, provide a set of functions for constructing errors.</p>\n<p>To give a concrete example, in TigerBeetle\u2019s\n<a href=\"https://github.com/tigerbeetle/tigerbeetle/blob/0.16.73/src/tidy.zig#L54-L188\"><code>tidy.zig</code></a>\n(a project-specific linting script, another useful meta-pattern), we define errors as follows:</p>\n\n<figure class=\"code-block\">\n\n\n<pre><code><span class=\"line\"><span class=\"hl-keyword\">const</span> Errors = <span class=\"hl-keyword\">struct</span> {</span>\n<span class=\"line\">    <span class=\"hl-keyword\">pub</span> <span class=\"hl-keyword\">fn</span><span class=\"hl-function\"> add_long_line</span>(</span>\n<span class=\"line\">        errors: <span class=\"hl-operator\">*</span>Errors,</span>\n<span class=\"line\">        file: SourceFile,</span>\n<span class=\"line\">        line_index: <span class=\"hl-type\">usize</span>,</span>\n<span class=\"line\">    ) <span class=\"hl-type\">void</span> { ... }</span>\n<span class=\"line\"></span>\n<span class=\"line\">    <span class=\"hl-keyword\">pub</span> <span class=\"hl-keyword\">fn</span><span class=\"hl-function\"> add_banned</span>(</span>\n<span class=\"line\">        errors: <span class=\"hl-operator\">*</span>Errors,</span>\n<span class=\"line\">        file: SourceFile,</span>\n<span class=\"line\">        offset: <span class=\"hl-type\">usize</span>,</span>\n<span class=\"line\">        banned_item: []<span class=\"hl-keyword\">const</span> <span class=\"hl-type\">u8</span>,</span>\n<span class=\"line\">        replacement: []<span class=\"hl-keyword\">const</span> <span class=\"hl-type\">u8</span>,</span>\n<span class=\"line\">    ) <span class=\"hl-type\">void</span> { ... }</span>\n<span class=\"line\"></span>\n<span class=\"line\">    <span class=\"hl-keyword\">pub</span> <span class=\"hl-keyword\">fn</span><span class=\"hl-function\"> add_dead_declaration</span>(...) <span class=\"hl-type\">void</span> { ... }</span>\n<span class=\"line\"></span>\n<span class=\"line\">    ...</span>\n<span class=\"line\">};</span></code></pre>\n\n</figure>\n<p>and the call-site looks like this:</p>\n\n<figure class=\"code-block\">\n\n\n<pre><code><span class=\"line\"><span class=\"hl-keyword\">fn</span><span class=\"hl-function\"> tidy_file</span>(file: SourceFile, errors: <span class=\"hl-operator\">*</span>Errors) <span class=\"hl-type\">void</span> {</span>\n<span class=\"line\">    <span class=\"hl-comment\">// ...</span></span>\n<span class=\"line\">    <span class=\"hl-keyword\">var</span> line_index: <span class=\"hl-type\">usize</span> = <span class=\"hl-numbers\">0</span>;</span>\n<span class=\"line\">    <span class=\"hl-keyword\">while</span> (lines.next()) <span class=\"hl-operator\">|</span>line<span class=\"hl-operator\">|</span> : (line_index <span class=\"hl-operator\">+=</span> <span class=\"hl-numbers\">1</span>) {</span>\n<span class=\"line\">        <span class=\"hl-keyword\">const</span> line_length = line_length(line);</span>\n<span class=\"line\">        <span class=\"hl-keyword\">if</span> (line_length &gt; <span class=\"hl-numbers\">100</span> <span class=\"hl-keyword\">and</span> <span class=\"hl-operator\">!</span>contains_url(line)) {</span>\n<span class=\"line\">            errors.add_long_line(file, line_index);</span>\n<span class=\"line\">        }</span>\n<span class=\"line\">    }</span>\n<span class=\"line\">}</span></code></pre>\n\n</figure>\n<p>In this case, I collect multiple errors so I don\u2019t return right away. Fail fast would look like\nthis:</p>\n\n<figure class=\"code-block\">\n\n\n<pre><code><span class=\"line\">errors.add_long_line(file, line_index);</span>\n<span class=\"line\"><span class=\"hl-keyword\">return</span> <span class=\"hl-keyword\">error</span>.Tidy;</span></code></pre>\n\n</figure>\n<p>Note that the error code is intentionally independent of the specific error produced.</p>\n<hr />\n<p>Some interesting properties of the solution:</p>\n<ul>\n<li>\nThe error representation is a set of constructor functions, the calling code doesn\u2019t care what\n<em>actually</em> happens inside. This is why the error factory is my <em>default</em> solution \u2014 I don\u2019t have\nto figure out up-front what I\u2019ll do with the errors, and I can change my mind later.\n</li>\n<li>\nThere\u2019s a natural place to convert information from the form available at the place where we emit\nthe error to a form useful for the user. In <code>add_banned</code> above, the caller passes in a absolute\noffset in a file, and it is resolved to line number and column inside (tip: use <code>line_index</code> for\n0-based internal indexes, and <code>line_number</code> for user-visible 1-based ones). Contrast this with a\ntraditional error as sum-type approach, where there\u2019s a sharp syntactic discontinuity between\nconstructing a variant directly and calling a helper function.\n</li>\n<li>\nThis syntactic uniformity in turn allows easily grepping for all error locations:\n<span class=\"display\"><code>rg 'errors.add_'</code>.</span>\n</li>\n<li>\nSimilarly, there\u2019s one central place that enumerates all possible errors (which is either a\nbenefit or a drawback).\n</li>\n</ul>\n<p>A less trivial property is that this structure enables polymorphism. In fact, in the <code>tidy.zig</code>\ncode, there are two different representations of errors. When running the script, errors are\ndirectly emitted to stderr. But when testing it, errors are collected into an in-memory buffer:</p>\n\n<figure class=\"code-block\">\n\n\n<pre><code><span class=\"line\"><span class=\"hl-keyword\">pub</span> <span class=\"hl-keyword\">fn</span><span class=\"hl-function\"> add_banned</span>(</span>\n<span class=\"line\">    errors: <span class=\"hl-operator\">*</span>Errors,</span>\n<span class=\"line\">    file: SourceFile,</span>\n<span class=\"line\">    offset: <span class=\"hl-type\">usize</span>,</span>\n<span class=\"line\">    banned_item: []<span class=\"hl-keyword\">const</span> <span class=\"hl-type\">u8</span>,</span>\n<span class=\"line\">    replacement: []<span class=\"hl-keyword\">const</span> <span class=\"hl-type\">u8</span>,</span>\n<span class=\"line\">) <span class=\"hl-type\">void</span> {</span>\n<span class=\"line\">    errors.emit(</span>\n<span class=\"line\">        <span class=\"hl-string\">&quot;{s}:{d}: error: {s} is banned, use {s}<span class=\"hl-string\">\\n</span>&quot;</span>,</span>\n<span class=\"line\">        .{</span>\n<span class=\"line\">            file.path, file.line_number(offset),</span>\n<span class=\"line\">            banned_item, replacement,</span>\n<span class=\"line\">        },</span>\n<span class=\"line\">    );</span>\n<span class=\"line\">}</span>\n<span class=\"line\"></span>\n<span class=\"line\"><span class=\"hl-keyword\">fn</span><span class=\"hl-function\"> emit</span>(</span>\n<span class=\"line\">    errors: <span class=\"hl-operator\">*</span>Errors,</span>\n<span class=\"line\">    <span class=\"hl-keyword\">comptime</span> fmt: []<span class=\"hl-keyword\">const</span> <span class=\"hl-type\">u8</span>,</span>\n<span class=\"line\">    args: <span class=\"hl-type\">anytype</span>,</span>\n<span class=\"line\">) <span class=\"hl-type\">void</span> {</span>\n<span class=\"line\">    <span class=\"hl-keyword\">comptime</span> assert(fmt[fmt.len <span class=\"hl-operator\">-</span> <span class=\"hl-numbers\">1</span>] <span class=\"hl-operator\">==</span> <span class=\"hl-string\">&#x27;<span class=\"hl-string\">\\n</span>&#x27;</span>);</span>\n<span class=\"line\">    errors.count <span class=\"hl-operator\">+=</span> <span class=\"hl-numbers\">1</span>;</span>\n<span class=\"line\">    <span class=\"hl-keyword\">if</span> (errors.captured) <span class=\"hl-operator\">|</span><span class=\"hl-operator\">*</span>captured<span class=\"hl-operator\">|</span> {</span>\n<span class=\"line\">        captured.writer(errors.gpa).print(fmt, args)</span>\n<span class=\"line\">            <span class=\"hl-keyword\">catch</span> <span class=\"hl-built_in\">@panic</span>(<span class=\"hl-string\">&quot;OOM&quot;</span>);</span>\n<span class=\"line\">    } <span class=\"hl-keyword\">else</span> {</span>\n<span class=\"line\">        std.debug.print(fmt, args);</span>\n<span class=\"line\">    }</span>\n<span class=\"line\">}</span></code></pre>\n\n</figure>\n<p>There isn\u2019t a giant <code>union(enum)</code> of all errors, because it\u2019s not needed for the present use-case.</p>\n<p>This pattern can be further extended to a full-fledged diagnostics framework with error builders,\nspans, ANSI colors and such, but that is tangential to the main idea here: even when \u201cprogramming in\nthe small\u201d, it might be a good idea to avoid constructing enums directly, and mandate an\nintermediate function call.</p>\n<hr />\n<p>Two more meta observations here:</p>\n<p><em>First</em>, the entire pattern is of course the expression of duality between a sum of two types and a\nproduct of two functions (the visitor pattern)</p>\n\n<figure class=\"code-block\">\n\n\n<pre><code><span class=\"line\"><span class=\"hl-keyword\">fn</span> <span class=\"hl-title function_\">foo</span>() <span class=\"hl-punctuation\">-&gt;</span> <span class=\"hl-type\">Result</span>&lt;T, E&gt;;</span>\n<span class=\"line\"></span>\n<span class=\"line\"><span class=\"hl-keyword\">fn</span> <span class=\"hl-title function_\">bar</span>(ok: <span class=\"hl-keyword\">impl</span> <span class=\"hl-title class_\">FnOnce</span>(T), err: <span class=\"hl-keyword\">impl</span> <span class=\"hl-title class_\">FnOnce</span>(E));</span></code></pre>\n\n</figure>\n\n<figure class=\"code-block\">\n\n\n<pre><code><span class=\"line\"><span class=\"hl-keyword\">enum</span> <span class=\"hl-title class_\">Result</span>&lt;T, E&gt; {</span>\n<span class=\"line\">    <span class=\"hl-title function_ invoke__\">Ok</span>(T),</span>\n<span class=\"line\">    <span class=\"hl-title function_ invoke__\">Err</span>(E),</span>\n<span class=\"line\">}</span>\n<span class=\"line\"></span>\n<span class=\"line\"><span class=\"hl-keyword\">trait</span> <span class=\"hl-title class_\">Result</span>&lt;T, E&gt; {</span>\n<span class=\"line\">    <span class=\"hl-keyword\">fn</span> <span class=\"hl-title function_\">ok</span>(<span class=\"hl-keyword\">self</span>, T);</span>\n<span class=\"line\">    <span class=\"hl-keyword\">fn</span> <span class=\"hl-title function_\">err</span>(<span class=\"hl-keyword\">self</span>, E);</span>\n<span class=\"line\">}</span></code></pre>\n\n</figure>\n<p><em>Second</em>, every abstraction is a thin film separating two large bodies of code. Any interface has\ntwo sides, the familiar one presented to the user, and the other, hidden one, presented to the\nimplementor. Often, default language machinery pushes you towards using the same construct for both\nbut that can be suboptimal. It\u2019s natural for the user and the provider of the abstraction to\ndisagree on the optimal interface, and to evolve independently. Using a single big enum for errors\ncouples error emitting and error reporting code, as they have to meet in the middle. In contrast,\nthe factory solution is optimal for producer (they literally just pass whatever they already have on\nhand, without any extra massaging of data), and is flexible for consumer(s).</p>"
            ],
            "link": "https://matklad.github.io/2026/02/16/diagnostics-factory.html",
            "publishedAt": "2026-02-16",
            "source": "Alex Kladov",
            "summary": "InError Codes For Control Flow,I explained that Zig's strongly-typed error codes solve the handling half of error management,leaving reporting to the users. Today, I want to describe my personal default approach tothe reporting problem, that is, showing the user a useful error message.",
            "title": "Diagnostics Factory"
        },
        {
            "content": [
                "<div class=\"captioned-image-container\"><figure><a class=\"image-link image2\" href=\"https://substackcdn.com/image/fetch/$s_!njz9!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F96739113-bd6a-4006-81f2-85f8be9f8b0d_2000x1260.gif\" target=\"_blank\"><div class=\"image2-inset\"><source type=\"image/webp\" /><img alt=\"\" class=\"sizing-normal\" src=\"https://substackcdn.com/image/fetch/$s_!njz9!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F96739113-bd6a-4006-81f2-85f8be9f8b0d_2000x1260.gif\" /><div></div></div></a></figure></div><p>By <strong>Taylor Rayne</strong></p><p>Smell is our most primal and, arguably, most emotionally potent sense. It summons memories, shapes taste, and <a href=\"https://www.nature.com/articles/s41598-021-96334-3\">influences behavior</a>: the aroma of coffee is capable of enhancing <a href=\"https://www.sciencedirect.com/science/article/pii/S2213422019302628\">alertness</a> well before caffeine ever reaches the bloodstream. A hint of sunscreen collapses decades, taking us back to youth; but, pinch the nose, and suddenly a slice of apple is hard to distinguish from a piece of raw potato.</p><p>Despite its significance, scent remains the most mysterious of our senses. Unlike vision or hearing, it resists straightforward formalization. The challenge lies not only in the vast molecular diversity of odorants, which vary in far more ways than photons or frequencies, but also in the effort to build a shared vocabulary and technology capable of codifying subjective sensation. So while machines have learned to see through computer vision and hear through signal processing, scent remains stubbornly analog. There has been no RGB of odor, no Fourier transform for smell.</p><p>At least, until now.</p><p>Tech giants, including Google, startups such as <a href=\"https://www.osmo.ai/\">Osmo</a>, and even traditional fragrance houses like <a href=\"https://www.givaudan.com/fragrance-beauty/perfumery-school/carto-the-future-of-fragrance-formulations\">Givaudan</a> have begun turning to AI to probe the possibility of digitizing smell. By encoding scent molecules into 1s and 0s, their hope is to better understand and manipulate this sensory modality. Just as &#8220;computer vision&#8221; has helped us realize that sight is not just passive image capture but an <a href=\"https://www.frontiersin.org/journals/neurorobotics/articles/10.3389/fnbot.2021.651432/full\">active process</a> of prediction and interpretation, researchers hope that programming smell will illuminate the many mysteries of olfaction.</p><p>Beyond providing further insight into olfactory biology, digital scent could have many practical (and quite profitable) applications, which is why its proponents, from defense agencies such as <a href=\"https://www.wired.com/story/quest-to-make-robot-smell-cancer-dog/\">DARPA</a> to corporate conglomerates like <a href=\"https://www.businessoffashion.com/news/beauty/estee-lauder-companies-fragrance-maison-atelier-opening-2025/\">Est&#233;e Lauder</a>, have invested in it. Computational smell could, for example, help detect threats and information invisible to cameras, such as gas leaks, <a href=\"https://www.jpost.com/health-and-wellness/article-758379\">food spoilage</a>, disease markers in breath, and even counterfeit products. It could also reduce reliance on resource-intensive natural ingredients used in perfume and other odorants by, for instance, finding chemically synthesized molecules capable of evoking the same brain patterns. And finally, it could lead to the creation of entirely novel smells, revealing a vast, untapped chemical palette that would otherwise be unattainable without the aid of technology.</p><div class=\"subscription-widget-wrap-editor\"><div class=\"subscription-widget show-subscribe\"><div class=\"preamble\"><p class=\"cta-caption\">Deep writing about biology. Delivered to your inbox.</p></div><form class=\"subscription-widget-subscribe\"><input class=\"email-input\" name=\"email\" tabindex=\"-1\" type=\"email\" /><input class=\"button primary\" type=\"submit\" value=\"Subscribe\" /><div class=\"fake-input-wrapper\"><div class=\"fake-input\"></div><div class=\"fake-button\"></div></div></form></div></div><h2>The Bacterial Beginnings of Smell</h2><p>Long before life evolved eyes and ears, the world was encountered chemically. This took place as molecules permeated and diffused across cell membranes, performing a metabolic exchange between animate and inanimate matter.</p><p>Smell, our most ancient interface with the environment, originated over <a href=\"https://www.ncbi.nlm.nih.gov/books/NBK92786/?utm_source=chatgpt.com\">3 billion years</a> ago, in bacteria adrift in the primordial ocean. These early organisms navigated chemical gradients in the water, detecting molecules to swim toward food and away from danger. This ability, known as <em>chemosensation</em>, is the most rudimentary form of smell.</p><p>Such &#8220;sensing&#8221; relies on receptor proteins embedded in the cell membrane, acting like molecular locks awaiting the corresponding chemical key. When a passing odor molecule fits into a receptor&#8217;s binding site, it changes the receptor&#8217;s shape, setting off a cascade of signals inside the cell that direct the organism&#8217;s movement.</p><p>Over time, these molecules didn&#8217;t just guide survival; they encouraged multicellular life. As cells began clustering together, the exchange of <em>semiochemicals </em>&#8212; molecular signals that transmit information within and between species &#8212; began to influence behavior, enabling aggregation and synchronization, and laying the groundwork for cellular cooperation. Plants, for instance, release green leaf volatiles such as <a href=\"https://en.wikipedia.org/wiki/Smell_of_freshly_cut_grass\">cis-3-hexenal</a> (the familiar scent of freshly cut grass) when attacked, both warning neighboring plants and attracting the animals that prey on herbivores. Human infants, meanwhile, are drawn to the distinctive odor of their <a href=\"https://www.ncbi.nlm.nih.gov/books/NBK200997/?utm_source=chatgpt.com\">mother&#8217;s milk</a>, which both stimulates feeding and regulates their earliest physiological rhythms. And among insects, ants are famous for <a href=\"https://resjournals.onlinelibrary.wiley.com/doi/full/10.1111/j.1365-3032.2008.00658.x\">deploying pheromones</a> such as <a href=\"https://resjournals.onlinelibrary.wiley.com/doi/full/10.1111/j.1365-3032.2008.00658.x#:~:text=The%20pygidial%20glands,Hefetz%2C%201990).\">iridodials</a>, which direct entire colonies to forage or fight in concert.</p><p>Once the first tetrapods emerged from the sea and embraced life on land, smelling evolved to become ever more refined under <a href=\"https://www.cell.com/neuron/fulltext/S0896-6273(05)00894-9?_returnURL=https%3A%2F%2Flinkinghub.elsevier.com%2Fretrieve%2Fpii%2FS0896627305008949%3Fshowall%3Dtrue\">newfound terrestrial pressures</a>, including adapting to novel volatile odorants and the more variable conditions of airborne climates. As these early terrestrial vertebrates expanded a chemosensory repertoire that had once been far more limited in their aquatic ancestors, olfactory systems likewise continued to evolve. Over the next hundreds of millions of years, the neural structures supporting our own sense of smell increased in sophistication.</p><div class=\"captioned-image-container\"><figure><a class=\"image-link image2 is-viewable-img\" href=\"https://substackcdn.com/image/fetch/$s_!FCAN!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4b5a2255-26b9-467e-a794-f401735a9af7_1489x937.png\" target=\"_blank\"><div class=\"image2-inset\"><source type=\"image/webp\" /><img alt=\"\" class=\"sizing-normal\" height=\"916\" src=\"https://substackcdn.com/image/fetch/$s_!FCAN!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4b5a2255-26b9-467e-a794-f401735a9af7_1489x937.png\" width=\"1456\" /><div class=\"image-link-expand\"><div class=\"pencraft pc-display-flex pc-gap-8 pc-reset\"><button class=\"pencraft pc-reset pencraft icon-container restack-image\" tabindex=\"0\" type=\"button\"><svg fill=\"none\" height=\"20\" stroke=\"var(--color-fg-primary)\" stroke-linecap=\"round\" stroke-linejoin=\"round\" stroke-width=\"1.5\" viewBox=\"0 0 20 20\" width=\"20\" xmlns=\"http://www.w3.org/2000/svg\"><g><title></title><path d=\"M2.53001 7.81595C3.49179 4.73911 6.43281 2.5 9.91173 2.5C13.1684 2.5 15.9537 4.46214 17.0852 7.23684L17.6179 8.67647M17.6179 8.67647L18.5002 4.26471M17.6179 8.67647L13.6473 6.91176M17.4995 12.1841C16.5378 15.2609 13.5967 17.5 10.1178 17.5C6.86118 17.5 4.07589 15.5379 2.94432 12.7632L2.41165 11.3235M2.41165 11.3235L1.5293 15.7353M2.41165 11.3235L6.38224 13.0882\"></path></g></svg></button><button class=\"pencraft pc-reset pencraft icon-container view-image\" tabindex=\"0\" type=\"button\"><svg class=\"lucide lucide-maximize2 lucide-maximize-2\" fill=\"none\" height=\"20\" stroke=\"currentColor\" stroke-linecap=\"round\" stroke-linejoin=\"round\" stroke-width=\"2\" viewBox=\"0 0 24 24\" width=\"20\" xmlns=\"http://www.w3.org/2000/svg\"><polyline points=\"15 3 21 3 21 9\"></polyline><polyline points=\"9 21 3 21 3 15\"></polyline><line x1=\"21\" x2=\"14\" y1=\"3\" y2=\"10\"></line><line x1=\"3\" x2=\"10\" y1=\"21\" y2=\"14\"></line></svg></button></div></div></div></a><figcaption class=\"image-caption\">Olfactory neurons in the nasal cavity bind to odorants and pass signals into the brain. Different combinations of activated receptors are perceived as different scents.</figcaption></figure></div><p>Today, roughly two to five percent of our genetic blueprint concerns itself with smell. While it may seem a small fraction of the whole, it is the <a href=\"https://link.springer.com/article/10.1186/1479-7364-3-1-87\">largest gene family</a> in the human genome. At any given time, 77 percent of the <a href=\"https://pmc.ncbi.nlm.nih.gov/articles/PMC4011832/?utm_source=chatgpt.com\">356 distinct</a> olfactory receptors are expressed in the lining of the nasal cavity, each tuned to the molecules that make up the world&#8217;s myriad smells. The receptors expressed vary between individuals, with only 90 receptors commonly found in all people. Together, this suite of receptors is responsible for every experience of scent we encounter, and the variability between individuals is likely behind smells&#8217; subjectivity.</p><p>To understand how this olfactory complexity works, consider the single inhalation that follows biting into a ripe strawberry: the rush of aroma, the sweet, tangy burst blooming in the nose, created not by a single compound but by a volatile molecular cocktail. There is no one single molecule responsible for the <a href=\"https://pubs.acs.org/doi/abs/10.1021/jf960366o\">smell of strawberry</a>, but rather a family &#8212; namely <a href=\"https://en.wikipedia.org/wiki/Furaneol\">furaneol</a> (caramel-sweet), <a href=\"https://en.wikipedia.org/wiki/Methyl_butyrate\">methyl</a> and <a href=\"https://en.wikipedia.org/wiki/Ethyl_butyrate\">ethyl butanoate</a> (fruity), methyl 2-propanoate (fruity), <a href=\"https://en.wikipedia.org/wiki/Hexanal\">hexanal</a> (green and sharp), and <a href=\"https://en.wikipedia.org/wiki/Cis-3-Hexen-1-ol\">cis-3-hexenol</a> (leafy-fresh) &#8212; that evaporate and surge through the nasal cavity.</p><p>In each nostril, this medley of molecules dissolves into the mucus layer lining a <a href=\"https://www.ncbi.nlm.nih.gov/books/NBK556051/\">2.5 cm&#178; patch</a> of tissue known as the olfactory epithelium. Studded across this small region is a mosaic of roughly 10 million olfactory sensory neurons,<a class=\"footnote-anchor\" href=\"https://www.asimov.press/feed#footnote-1\" id=\"footnote-anchor-1\" target=\"_self\">1</a> each expressing only <a href=\"https://rupress.org/jcb/article/191/3/443/54881/The-cell-biology-of-smellThe-cell-biology-of-smell#:~:text=The%20odorant%20receptor%3A%20enforcer%20of%20the%20%E2%80%9Cone%20receptor%2C%20one%20neuron%E2%80%9D%20rule\">one type</a> of olfactory receptor protein. In one breath, an odorant molecule, be it furaneol, cis-3-hexenol, or any other, activates a unique combination of receptors, similar to striking a subset of keys on a piano. This ensemble of activated neurons forms a distinct pattern that the brain reads as &#8220;strawberry.&#8221;</p><p>Crucially, no two scents ever strike the same pattern. The combinatorial activity of hundreds of receptor genes allows humans to detect and discern more than a <a href=\"https://www.science.org/content/article/human-nose-can-detect-trillion-smells\">trillion</a> distinct odors.</p><div class=\"captioned-image-container\"><figure><a class=\"image-link image2 is-viewable-img\" href=\"https://substackcdn.com/image/fetch/$s_!srm1!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd03e5f7f-abbe-4f35-bb77-cab7c1c7bbb0_1527x1298.png\" target=\"_blank\"><div class=\"image2-inset\"><source type=\"image/webp\" /><img alt=\"\" class=\"sizing-normal\" height=\"1238\" src=\"https://substackcdn.com/image/fetch/$s_!srm1!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd03e5f7f-abbe-4f35-bb77-cab7c1c7bbb0_1527x1298.png\" width=\"1456\" /><div class=\"image-link-expand\"><div class=\"pencraft pc-display-flex pc-gap-8 pc-reset\"><button class=\"pencraft pc-reset pencraft icon-container restack-image\" tabindex=\"0\" type=\"button\"><svg fill=\"none\" height=\"20\" stroke=\"var(--color-fg-primary)\" stroke-linecap=\"round\" stroke-linejoin=\"round\" stroke-width=\"1.5\" viewBox=\"0 0 20 20\" width=\"20\" xmlns=\"http://www.w3.org/2000/svg\"><g><title></title><path d=\"M2.53001 7.81595C3.49179 4.73911 6.43281 2.5 9.91173 2.5C13.1684 2.5 15.9537 4.46214 17.0852 7.23684L17.6179 8.67647M17.6179 8.67647L18.5002 4.26471M17.6179 8.67647L13.6473 6.91176M17.4995 12.1841C16.5378 15.2609 13.5967 17.5 10.1178 17.5C6.86118 17.5 4.07589 15.5379 2.94432 12.7632L2.41165 11.3235M2.41165 11.3235L1.5293 15.7353M2.41165 11.3235L6.38224 13.0882\"></path></g></svg></button><button class=\"pencraft pc-reset pencraft icon-container view-image\" tabindex=\"0\" type=\"button\"><svg class=\"lucide lucide-maximize2 lucide-maximize-2\" fill=\"none\" height=\"20\" stroke=\"currentColor\" stroke-linecap=\"round\" stroke-linejoin=\"round\" stroke-width=\"2\" viewBox=\"0 0 24 24\" width=\"20\" xmlns=\"http://www.w3.org/2000/svg\"><polyline points=\"15 3 21 3 21 9\"></polyline><polyline points=\"9 21 3 21 3 15\"></polyline><line x1=\"21\" x2=\"14\" y1=\"3\" y2=\"10\"></line><line x1=\"3\" x2=\"10\" y1=\"21\" y2=\"14\"></line></svg></button></div></div></div></a><figcaption class=\"image-caption\">Each molecule activates a distinct set of odorant receptors.</figcaption></figure></div><p>While some olfactory receptors respond broadly, meaning they can recognize and bind to several different structurally-related molecules, other receptors are exquisitely selective and bind to only one specific shape or stereoisomer.<a class=\"footnote-anchor\" href=\"https://www.asimov.press/feed#footnote-2\" id=\"footnote-anchor-2\" target=\"_self\">2</a> For instance, the two mirror-twin forms of the organic compound carvone smell strikingly different &#8212; one like spearmint (R-carvone), the other like caraway (S-carvone) &#8212; underscoring the nuance the nose brings to discriminating between such molecular mirror images.</p><p>The resultant signal, whether &#8220;caraway,&#8221; &#8220;spearmint,&#8221; or &#8220;strawberry,&#8221; travels to <a href=\"https://doi.org/10.1152/nips.1507.2003\">the olfactory bulb</a>, a bipartite nerve structure nestled at the base of the skull, just above the nasal passages. There, neurons expressing the same receptor type converge on specialized structures called glomeruli, the crucial processing hubs that sharpen and refine sensory input en route to deeper regions of the brain involved in odor discrimination. </p><p>Each glomerulus acts as a dedicated module for a single receptor type, receiving input from thousands of olfactory sensory neurons scattered throughout the nasal epithelium, but all tuned to the same molecular features. Within these spherical tangles of neural connections, the incoming signals undergo their first round of processing: they&#8217;re amplified, filtered, and integrated by local interneurons that enhance contrast between different odor signals.</p><div class=\"captioned-image-container\"><figure><a class=\"image-link image2 is-viewable-img\" href=\"https://substackcdn.com/image/fetch/$s_!eLjw!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd4d715c6-4466-4b26-aa05-8c5fd1a79344_1522x608.png\" target=\"_blank\"><div class=\"image2-inset\"><source type=\"image/webp\" /><img alt=\"\" class=\"sizing-normal\" height=\"582\" src=\"https://substackcdn.com/image/fetch/$s_!eLjw!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd4d715c6-4466-4b26-aa05-8c5fd1a79344_1522x608.png\" width=\"1456\" /><div class=\"image-link-expand\"><div class=\"pencraft pc-display-flex pc-gap-8 pc-reset\"><button class=\"pencraft pc-reset pencraft icon-container restack-image\" tabindex=\"0\" type=\"button\"><svg fill=\"none\" height=\"20\" stroke=\"var(--color-fg-primary)\" stroke-linecap=\"round\" stroke-linejoin=\"round\" stroke-width=\"1.5\" viewBox=\"0 0 20 20\" width=\"20\" xmlns=\"http://www.w3.org/2000/svg\"><g><title></title><path d=\"M2.53001 7.81595C3.49179 4.73911 6.43281 2.5 9.91173 2.5C13.1684 2.5 15.9537 4.46214 17.0852 7.23684L17.6179 8.67647M17.6179 8.67647L18.5002 4.26471M17.6179 8.67647L13.6473 6.91176M17.4995 12.1841C16.5378 15.2609 13.5967 17.5 10.1178 17.5C6.86118 17.5 4.07589 15.5379 2.94432 12.7632L2.41165 11.3235M2.41165 11.3235L1.5293 15.7353M2.41165 11.3235L6.38224 13.0882\"></path></g></svg></button><button class=\"pencraft pc-reset pencraft icon-container view-image\" tabindex=\"0\" type=\"button\"><svg class=\"lucide lucide-maximize2 lucide-maximize-2\" fill=\"none\" height=\"20\" stroke=\"currentColor\" stroke-linecap=\"round\" stroke-linejoin=\"round\" stroke-width=\"2\" viewBox=\"0 0 24 24\" width=\"20\" xmlns=\"http://www.w3.org/2000/svg\"><polyline points=\"15 3 21 3 21 9\"></polyline><polyline points=\"9 21 3 21 3 15\"></polyline><line x1=\"21\" x2=\"14\" y1=\"3\" y2=\"10\"></line><line x1=\"3\" x2=\"10\" y1=\"21\" y2=\"14\"></line></svg></button></div></div></div></a><figcaption class=\"image-caption\">Though identical in atomic composition, the (R) and (S) enantiomers of carvone differ just enough in three-dimensional shape for olfactory detection. That slight stereochemical shift transforms perception itself: (R)-carvone reads as spearmint, while (S)-carvone evokes caraway.</figcaption></figure></div><p>Unlike other senses, these olfactory signals take an unusual route through the brain. While vision, hearing, and touch all pass through a brain region called the thalamus before reaching higher brain regions, smell bypasses this checkpoint. Instead, odor information travels directly from the olfactory bulb to the amygdala and hippocampus (two brain structures central to emotion and memory) before reaching conscious processing areas. This anatomical shortcut may explain why smells can trigger vivid memories and strong emotions before we&#8217;ve even consciously identified what we&#8217;re smelling.</p><p>Yet, while the nose has been anatomically mapped, receptors sequenced, and neural pathways charted, predicting<em> </em>a molecule&#8217;s scent has remained a mysterious exercise. The question of why certain configurations of matter smell one way and not another persists. In other words, why does a molecule such as furaneol activate the receptor signaling &#8220;jammy sweetness&#8221;? What makes one molecule smell &#8220;grassy&#8221; and another &#8220;creamy,&#8221; one trigger nausea and another nostalgia?</p><p>The prevailing hypothesis defines odorant molecules as ligands, specialized binding molecules that fit into olfactory receptors like a <a href=\"https://en.wikipedia.org/wiki/Docking_theory_of_olfaction\">lock and key</a>. This interaction activates the receptor, initiating an electrical response in the brain, a unique pattern of activity associated with a particular scent. While molecular structure has long served as a proxy for predicting smell, particularly for researchers and fragrance chemists, the so-called Structure-Odor Relationship (SOR) paradox endures: molecules of nearly identical structure can smell worlds apart, while those with little in common can smell strikingly similar.</p><p>Take <a href=\"https://en.wikipedia.org/wiki/Musk\">musks</a>,<a class=\"footnote-anchor\" href=\"https://www.asimov.press/feed#footnote-3\" id=\"footnote-anchor-3\" target=\"_self\">3</a> for instance: this family of compounds includes hundreds of molecules with vastly different structures and molecular weights, ranging from small macrocycles to large polycyclic frameworks. Yet, despite this diversity, many share a common warm, powdery, and animalic scent profile that defies straightforward structure-to-smell prediction.</p><div class=\"captioned-image-container\"><figure><a class=\"image-link image2 is-viewable-img\" href=\"https://substackcdn.com/image/fetch/$s_!QEDB!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F93c4b062-1e0b-4221-8007-4344d08ed647_1522x670.png\" target=\"_blank\"><div class=\"image2-inset\"><source type=\"image/webp\" /><img alt=\"\" class=\"sizing-normal\" height=\"641\" src=\"https://substackcdn.com/image/fetch/$s_!QEDB!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F93c4b062-1e0b-4221-8007-4344d08ed647_1522x670.png\" width=\"1456\" /><div class=\"image-link-expand\"><div class=\"pencraft pc-display-flex pc-gap-8 pc-reset\"><button class=\"pencraft pc-reset pencraft icon-container restack-image\" tabindex=\"0\" type=\"button\"><svg fill=\"none\" height=\"20\" stroke=\"var(--color-fg-primary)\" stroke-linecap=\"round\" stroke-linejoin=\"round\" stroke-width=\"1.5\" viewBox=\"0 0 20 20\" width=\"20\" xmlns=\"http://www.w3.org/2000/svg\"><g><title></title><path d=\"M2.53001 7.81595C3.49179 4.73911 6.43281 2.5 9.91173 2.5C13.1684 2.5 15.9537 4.46214 17.0852 7.23684L17.6179 8.67647M17.6179 8.67647L18.5002 4.26471M17.6179 8.67647L13.6473 6.91176M17.4995 12.1841C16.5378 15.2609 13.5967 17.5 10.1178 17.5C6.86118 17.5 4.07589 15.5379 2.94432 12.7632L2.41165 11.3235M2.41165 11.3235L1.5293 15.7353M2.41165 11.3235L6.38224 13.0882\"></path></g></svg></button><button class=\"pencraft pc-reset pencraft icon-container view-image\" tabindex=\"0\" type=\"button\"><svg class=\"lucide lucide-maximize2 lucide-maximize-2\" fill=\"none\" height=\"20\" stroke=\"currentColor\" stroke-linecap=\"round\" stroke-linejoin=\"round\" stroke-width=\"2\" viewBox=\"0 0 24 24\" width=\"20\" xmlns=\"http://www.w3.org/2000/svg\"><polyline points=\"15 3 21 3 21 9\"></polyline><polyline points=\"9 21 3 21 3 15\"></polyline><line x1=\"21\" x2=\"14\" y1=\"3\" y2=\"10\"></line><line x1=\"3\" x2=\"10\" y1=\"21\" y2=\"14\"></line></svg></button></div></div></div></a><figcaption class=\"image-caption\">Muscone and civetone &#8212; both macrocyclic ketones with unique scaffolds &#8212; evoke the classic, animalic musk note. By contrast, nitro musks (e.g., musk xylene, musk ketone) rely on aromatic nitro-benzene scaffolds but still evoke the musky character associated with compounds like muscone and civetone. But odorants must also be volatile: molecules above ~300 Da, common among heavier musks, often lose perceptibility, becoming effectively odorless. These examples underscore how scent is shaped not only by molecular structure but also by chemical dynamics, receptor biology, and often a bit of mystery.</figcaption></figure></div><h2>An Industry for Smell</h2><p>The speculation that chemical structure corresponds to scent dates back millennia, alongside the use of fragrant materials in ceremonial and cultural practice. From the early Greek atomists&#8217; <a href=\"https://plato.stanford.edu/entries/democritus/#3\">theories</a> of microscopic films emitted by objects to excite the senses, to the incense artisans of ancient Egypt and <a href=\"https://en.wikipedia.org/wiki/K%C5%8Dd%C5%8D\">East Asia</a>, humans have long sought to capture and understand the elusive nature of smell.</p><p>Before the emergence of more sophisticated means for extracting and preserving scent, fragrance was elicited through rudimentary methods such as crushing raw botanicals, steeping them in oils, or burning resins to release their aroma.</p><p>During the eighth and ninth centuries, the invention of the &#8220;modern&#8221; <a href=\"https://www.campariacademy.com/de-de/inspiration/extracting-flavour-marcis-dzelzainis-on-how-to-use-an-alembic-still/\">alembic</a> &#8212; a distillation apparatus developed by Islamic alchemists featuring a still pot, a cooling condenser, and a collection spout &#8212; allowed for a more controlled extraction of delicate essential oils than crude crushing or simple heating could achieve. This innovation enabled new forms of fragrance production, introducing alcohol as a base and associating the practice of perfumery more closely with chemistry and medicine. Through trade and translation, this technology and technique <a href=\"https://carrementbelle.com/blog/en/2022/03/30/the-mythical-cities-of-perfume/\">migrated</a> to continental Europe, where perfumers expanded their repertoire over centuries with methods such as maceration, expression, tincturing, and <a href=\"https://en.wikipedia.org/wiki/Enfleurage\">enfleurage</a><a class=\"footnote-anchor\" href=\"https://www.asimov.press/feed#footnote-4\" id=\"footnote-anchor-4\" target=\"_self\">4</a> to isolate the volatile compounds of flowers and woods.</p><div class=\"captioned-image-container\"><figure><a class=\"image-link image2 is-viewable-img\" href=\"https://substackcdn.com/image/fetch/$s_!0Jie!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffede24e2-6fbd-4656-acaf-ee5f0fd158d4_567x442.jpeg\" target=\"_blank\"><div class=\"image2-inset\"><source type=\"image/webp\" /><img alt=\"\" class=\"sizing-normal\" height=\"442\" src=\"https://substackcdn.com/image/fetch/$s_!0Jie!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffede24e2-6fbd-4656-acaf-ee5f0fd158d4_567x442.jpeg\" width=\"567\" /><div class=\"image-link-expand\"><div class=\"pencraft pc-display-flex pc-gap-8 pc-reset\"><button class=\"pencraft pc-reset pencraft icon-container restack-image\" tabindex=\"0\" type=\"button\"><svg fill=\"none\" height=\"20\" stroke=\"var(--color-fg-primary)\" stroke-linecap=\"round\" stroke-linejoin=\"round\" stroke-width=\"1.5\" viewBox=\"0 0 20 20\" width=\"20\" xmlns=\"http://www.w3.org/2000/svg\"><g><title></title><path d=\"M2.53001 7.81595C3.49179 4.73911 6.43281 2.5 9.91173 2.5C13.1684 2.5 15.9537 4.46214 17.0852 7.23684L17.6179 8.67647M17.6179 8.67647L18.5002 4.26471M17.6179 8.67647L13.6473 6.91176M17.4995 12.1841C16.5378 15.2609 13.5967 17.5 10.1178 17.5C6.86118 17.5 4.07589 15.5379 2.94432 12.7632L2.41165 11.3235M2.41165 11.3235L1.5293 15.7353M2.41165 11.3235L6.38224 13.0882\"></path></g></svg></button><button class=\"pencraft pc-reset pencraft icon-container view-image\" tabindex=\"0\" type=\"button\"><svg class=\"lucide lucide-maximize2 lucide-maximize-2\" fill=\"none\" height=\"20\" stroke=\"currentColor\" stroke-linecap=\"round\" stroke-linejoin=\"round\" stroke-width=\"2\" viewBox=\"0 0 24 24\" width=\"20\" xmlns=\"http://www.w3.org/2000/svg\"><polyline points=\"15 3 21 3 21 9\"></polyline><polyline points=\"9 21 3 21 3 15\"></polyline><line x1=\"21\" x2=\"14\" y1=\"3\" y2=\"10\"></line><line x1=\"3\" x2=\"10\" y1=\"21\" y2=\"14\"></line></svg></button></div></div></div></a><figcaption class=\"image-caption\">Drawing of an Alembic by Jabir Ibn Hayyan, 8th century.</figcaption></figure></div><p>In the sixteenth century, tanners making leather gloves sought to <a href=\"https://fleurtyherald.wordpress.com/2020/12/19/perfumed-leather-gloves/\">mask the acrid smell</a> of their wares with fragrant oils and extracts. Scented gloves quickly became fashionable among the European aristocracy, and perfumers found a lucrative market beyond functional necessity. At Versailles, the epicenter of French culture and seat of royal power, <a href=\"https://research-api.cbs.dk/ws/portalfiles/portal/58999482/Creative_Encounters_Working_Papers_23.pdf\">courtly fashion</a> demanded a complete sensory presentation: the right clothing, hairstyles, cosmetics, and an ever-changing repertoire of perfumed products to signal refinement and status.</p><p>This high consumption of scent supported a nascent perfume industry, formalized in 1656 in Grasse, France, with the creation of the <a href=\"https://shs.cairn.info/article/E_JIE_018_0185?lang=en\">Ma&#238;tres Parfumeurs et Gantier</a> (the perfume and glove-maker&#8217;s guild). Alongside its favorable social-political milieu, Grasse&#8217;s microclimate fostered a <a href=\"https://research-api.cbs.dk/ws/portalfiles/portal/58999482/Creative_Encounters_Working_Papers_23.pdf\">burgeoning perfume industry</a> in the region, where fertile soils nurtured <a href=\"https://www.museesdegrasse.com/en/exploring-gardens\">fragrant flowers</a> such as jasmine, rose, lavender, orange blossom, myrtle, mimosa, and tuberose. This economic activity, along with the formation of local perfume, helped establish France&#8217;s perfumeries as <a href=\"https://www.hbs.edu/businesshistory/Documents/from-industry-to-luxury.pdf\">centers of both craft innovation</a> and the formal study of smell.</p><div class=\"captioned-image-container\"><figure><a class=\"image-link image2 is-viewable-img\" href=\"https://substackcdn.com/image/fetch/$s_!tVBB!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F59d32051-5c1d-4c85-a376-6e92ab042e83_512x360.jpeg\" target=\"_blank\"><div class=\"image2-inset\"><source type=\"image/webp\" /><img alt=\"\" class=\"sizing-normal\" height=\"360\" src=\"https://substackcdn.com/image/fetch/$s_!tVBB!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F59d32051-5c1d-4c85-a376-6e92ab042e83_512x360.jpeg\" width=\"512\" /><div class=\"image-link-expand\"><div class=\"pencraft pc-display-flex pc-gap-8 pc-reset\"><button class=\"pencraft pc-reset pencraft icon-container restack-image\" tabindex=\"0\" type=\"button\"><svg fill=\"none\" height=\"20\" stroke=\"var(--color-fg-primary)\" stroke-linecap=\"round\" stroke-linejoin=\"round\" stroke-width=\"1.5\" viewBox=\"0 0 20 20\" width=\"20\" xmlns=\"http://www.w3.org/2000/svg\"><g><title></title><path d=\"M2.53001 7.81595C3.49179 4.73911 6.43281 2.5 9.91173 2.5C13.1684 2.5 15.9537 4.46214 17.0852 7.23684L17.6179 8.67647M17.6179 8.67647L18.5002 4.26471M17.6179 8.67647L13.6473 6.91176M17.4995 12.1841C16.5378 15.2609 13.5967 17.5 10.1178 17.5C6.86118 17.5 4.07589 15.5379 2.94432 12.7632L2.41165 11.3235M2.41165 11.3235L1.5293 15.7353M2.41165 11.3235L6.38224 13.0882\"></path></g></svg></button><button class=\"pencraft pc-reset pencraft icon-container view-image\" tabindex=\"0\" type=\"button\"><svg class=\"lucide lucide-maximize2 lucide-maximize-2\" fill=\"none\" height=\"20\" stroke=\"currentColor\" stroke-linecap=\"round\" stroke-linejoin=\"round\" stroke-width=\"2\" viewBox=\"0 0 24 24\" width=\"20\" xmlns=\"http://www.w3.org/2000/svg\"><polyline points=\"15 3 21 3 21 9\"></polyline><polyline points=\"9 21 3 21 3 15\"></polyline><line x1=\"21\" x2=\"14\" y1=\"3\" y2=\"10\"></line><line x1=\"3\" x2=\"10\" y1=\"21\" y2=\"14\"></line></svg></button></div></div></div></a><figcaption class=\"image-caption\">&#8220;Smell&#8221;, from a series of plates depicting the &#8216;Senses&#8217;, French, ca. 1750. Credit: <a href=\"https://www.lookandlearn.com/history-images/YCH006944/Touch-from-Senses\">Cooper Hewitt</a>, Smithsonian Design Museum.</figcaption></figure></div><p>The nineteenth century marked a decisive shift as techniques from pharmacy and organic chemistry began to elucidate molecules underlying fragrance. Building on earlier breakthroughs such as the <a href=\"https://www.sciencedirect.com/science/article/pii/S0021925819649951\">isolation of morphine</a> in 1804 (the first alkaloid ever extracted from a plant), chemists turned their attention to aromatic materials.</p><p>In 1820, the French pharmacist Nicolas Jean Baptiste Gaston Guibourt identified and isolated 2H-chromen-2-one, better known as coumarin, from the tonka bean (<em>Dipteryx odorata</em>). In an <a href=\"https://gallica.bnf.fr/ark:/12148/bpt6k58427321\">account</a> presented to the pharmacy section of the Acad&#233;mie Royale de M&#233;decine, Guibourt formally christened the new substance <em>coumarine</em>, marking one of the earliest instances of the chemical characterization of a fragrance compound. In 1858, another French chemist, Th&#233;odore Nicolas Gobley, succeeded in <a href=\"https://books.google.com/books?id=Yrs8AAAAcAAJ&amp;pg=PA401#v=onepage&amp;q&amp;f=false\">crystallizing vanillin</a> from vanilla pods, confirming that odorants could be identified as discrete molecular entities.</p><p>For the first time, individual aromatic molecules &#8212; a resonant note of the rose, one iota of the vanilla pod &#8212; could be extracted from a larger, complex composition. Smell was being recast as a phenomenon amenable to classification and design.</p><div class=\"captioned-image-container\"><figure><a class=\"image-link image2 is-viewable-img\" href=\"https://substackcdn.com/image/fetch/$s_!Fr5E!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F921e9bf1-c1e3-4e27-a845-467db889a696_600x400.jpeg\" target=\"_blank\"><div class=\"image2-inset\"><source type=\"image/webp\" /><img alt=\"\" class=\"sizing-normal\" height=\"400\" src=\"https://substackcdn.com/image/fetch/$s_!Fr5E!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F921e9bf1-c1e3-4e27-a845-467db889a696_600x400.jpeg\" width=\"600\" /><div class=\"image-link-expand\"><div class=\"pencraft pc-display-flex pc-gap-8 pc-reset\"><button class=\"pencraft pc-reset pencraft icon-container restack-image\" tabindex=\"0\" type=\"button\"><svg fill=\"none\" height=\"20\" stroke=\"var(--color-fg-primary)\" stroke-linecap=\"round\" stroke-linejoin=\"round\" stroke-width=\"1.5\" viewBox=\"0 0 20 20\" width=\"20\" xmlns=\"http://www.w3.org/2000/svg\"><g><title></title><path d=\"M2.53001 7.81595C3.49179 4.73911 6.43281 2.5 9.91173 2.5C13.1684 2.5 15.9537 4.46214 17.0852 7.23684L17.6179 8.67647M17.6179 8.67647L18.5002 4.26471M17.6179 8.67647L13.6473 6.91176M17.4995 12.1841C16.5378 15.2609 13.5967 17.5 10.1178 17.5C6.86118 17.5 4.07589 15.5379 2.94432 12.7632L2.41165 11.3235M2.41165 11.3235L1.5293 15.7353M2.41165 11.3235L6.38224 13.0882\"></path></g></svg></button><button class=\"pencraft pc-reset pencraft icon-container view-image\" tabindex=\"0\" type=\"button\"><svg class=\"lucide lucide-maximize2 lucide-maximize-2\" fill=\"none\" height=\"20\" stroke=\"currentColor\" stroke-linecap=\"round\" stroke-linejoin=\"round\" stroke-width=\"2\" viewBox=\"0 0 24 24\" width=\"20\" xmlns=\"http://www.w3.org/2000/svg\"><polyline points=\"15 3 21 3 21 9\"></polyline><polyline points=\"9 21 3 21 3 15\"></polyline><line x1=\"21\" x2=\"14\" y1=\"3\" y2=\"10\"></line><line x1=\"3\" x2=\"10\" y1=\"21\" y2=\"14\"></line></svg></button></div></div></div></a><figcaption class=\"image-caption\">Work station of French perfumer Jean Carles - born in Grasse (1892), founded the Givaudan perfume school, one of the &#8216;noses&#8217; behind the original 1947 edition of Miss Dior. Credit: <a href=\"https://boisdejasmin.com/2017/05/jean-carles-on-olfactory-training-perfumer-organ.html#more-26066\">Anna Kozlova</a></figcaption></figure></div><p>Driven by advancements in organic chemistry and the <a href=\"https://www.asbmb.org/asbmb-today/science/020721/a-brief-history-of-the-periodic-table\">introduction of the periodic table</a> in 1869, fragrance chemistry blossomed. By 1882, Paul Parquet&#8217;s famous scent, <em><a href=\"https://www.fragrantica.com/perfume/Houbigant/Fougere-Royale-2686.html\">Foug&#232;re Royale</a>,</em> featured coumarin, followed by Guerlain&#8217;s <em><a href=\"https://kafkaesqueblog.com/2020/12/23/guerlain-jicky-modern-parfum-history-old-legend/\">Jicky</a></em> in 1889, and in <a href=\"https://www.mairfragrance.com/blogs/mair-blog/chanel-no-5-and-its-evolution-over-time\">1921</a>, Chanel No. 5 launched using an entirely new class of synthetic compounds, the aldehydes. The perfume industry eagerly adopted new techniques, including chromatography and fractional distillation.</p><p>As the twentieth century unfolded, scientists intensified their quest for structural correlates of scent. Discoveries like <a href=\"https://pmc.ncbi.nlm.nih.gov/articles/PMC7764282/?utm_source=chatgpt.com#sec2-molecules-25-05822\">ionone-related ketones</a> evoking violet and woody notes, long-chain macrocycles recurring in <a href=\"https://en.wikipedia.org/wiki/Synthetic_musk\">musks</a>, and <a href=\"https://pubs.acs.org/doi/abs/10.1021/cr60204a003\">aromatic rings</a> anchoring vanillic and balsamic tones found their way to the traditional <em>orgue &#224; parfums</em>, a semicircular, horseshoe-shaped work station: a harbinger of what would become a more serious investigation of the science and technology of smell.</p><h2>Digitizing Smell</h2><p>Over eons, organisms evolved a sense of smell. In the past three centuries, smell became chemistry. And in recent years, we have started to ask whether smell can become code.</p><p>However, unlike sound and color, smell resists straightforward formalization. <a href=\"https://en.wikipedia.org/wiki/RGB_color_model\">Color</a>, for instance, has been reduced to three primary channels, standardized through the color wheel, and rendered into numeric systems that machines can interpret. <a href=\"https://en.wikipedia.org/wiki/Digital_audio\">Sound</a> has been decomposed along perceptual axes of pitch, timbre, and amplitude, each capable of digital transformation. Odor, however, has no reliable relationship to molecular structure or perceptual encoding, making it difficult to establish a computational coordinate system.</p><p>As a result, early computational efforts in representing molecules were often <em>ad hoc</em>: homemade tables to track atomic bonds, bespoke matrices, or custom linear codes that translated chemical structures into character strings. These methods were clever but small-scale and subjective, making calculations laborious and difficult to reproduce.</p><div class=\"captioned-image-container\"><figure><a class=\"image-link image2 is-viewable-img\" href=\"https://substackcdn.com/image/fetch/$s_!WK-n!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F43a28729-4183-4b43-9481-bcf6b17277b6_2940x2094.jpeg\" target=\"_blank\"><div class=\"image2-inset\"><source type=\"image/webp\" /><img alt=\"\" class=\"sizing-normal\" height=\"1037\" src=\"https://substackcdn.com/image/fetch/$s_!WK-n!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F43a28729-4183-4b43-9481-bcf6b17277b6_2940x2094.jpeg\" width=\"1456\" /><div class=\"image-link-expand\"><div class=\"pencraft pc-display-flex pc-gap-8 pc-reset\"><button class=\"pencraft pc-reset pencraft icon-container restack-image\" tabindex=\"0\" type=\"button\"><svg fill=\"none\" height=\"20\" stroke=\"var(--color-fg-primary)\" stroke-linecap=\"round\" stroke-linejoin=\"round\" stroke-width=\"1.5\" viewBox=\"0 0 20 20\" width=\"20\" xmlns=\"http://www.w3.org/2000/svg\"><g><title></title><path d=\"M2.53001 7.81595C3.49179 4.73911 6.43281 2.5 9.91173 2.5C13.1684 2.5 15.9537 4.46214 17.0852 7.23684L17.6179 8.67647M17.6179 8.67647L18.5002 4.26471M17.6179 8.67647L13.6473 6.91176M17.4995 12.1841C16.5378 15.2609 13.5967 17.5 10.1178 17.5C6.86118 17.5 4.07589 15.5379 2.94432 12.7632L2.41165 11.3235M2.41165 11.3235L1.5293 15.7353M2.41165 11.3235L6.38224 13.0882\"></path></g></svg></button><button class=\"pencraft pc-reset pencraft icon-container view-image\" tabindex=\"0\" type=\"button\"><svg class=\"lucide lucide-maximize2 lucide-maximize-2\" fill=\"none\" height=\"20\" stroke=\"currentColor\" stroke-linecap=\"round\" stroke-linejoin=\"round\" stroke-width=\"2\" viewBox=\"0 0 24 24\" width=\"20\" xmlns=\"http://www.w3.org/2000/svg\"><polyline points=\"15 3 21 3 21 9\"></polyline><polyline points=\"9 21 3 21 3 15\"></polyline><line x1=\"21\" x2=\"14\" y1=\"3\" y2=\"10\"></line><line x1=\"3\" x2=\"10\" y1=\"21\" y2=\"14\"></line></svg></button></div></div></div></a><figcaption class=\"image-caption\">Famous fragrances from history, with the year they first launched.</figcaption></figure></div><p>Then, in 1988, while at Pomona College in Claremont, California, chemist David Weininger devised <a href=\"https://www.cs.tufts.edu/comp/150CSB/refs/1987%20%20SMILES,%20a%20chemical%20language%20and%20information%20system.%201.%20Introduction%20to%20methodology%20and%20encoding%20rules.pdf\">SMILES</a> (Simplified Molecular Input Line Entry System). SMILES provided a standardized, machine-readable line notation for encoding molecular structures. Like any language, SMILES is a formal system for representing information; in the context of chemistry, it acts like a cipher or code to translate molecular structures into linear character strings. A given molecule, according to convention, is spelled out as a word or a particular grammatical grouping of letters, where each letter corresponds to an individual atom. Contained in each word is also the logic for how certain letters connect or are arranged geometrically, including instructions for branching and ring closures. For instance, the six-carbon ring cyclohexane is represented by slicing open the ring and labeling the adjacent atoms with matching numbers to indicate connection and closure: C1CCCCC1. In the same fashion, coumarin can be written as O=C1OC2=CC=CC=C2C=C1.</p><div class=\"captioned-image-container\"><figure><a class=\"image-link image2 is-viewable-img\" href=\"https://substackcdn.com/image/fetch/$s_!P5Zf!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fab4a6dff-5c7b-45eb-a53d-429d82da0a97_1522x490.png\" target=\"_blank\"><div class=\"image2-inset\"><source type=\"image/webp\" /><img alt=\"\" class=\"sizing-normal\" height=\"469\" src=\"https://substackcdn.com/image/fetch/$s_!P5Zf!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fab4a6dff-5c7b-45eb-a53d-429d82da0a97_1522x490.png\" width=\"1456\" /><div class=\"image-link-expand\"><div class=\"pencraft pc-display-flex pc-gap-8 pc-reset\"><button class=\"pencraft pc-reset pencraft icon-container restack-image\" tabindex=\"0\" type=\"button\"><svg fill=\"none\" height=\"20\" stroke=\"var(--color-fg-primary)\" stroke-linecap=\"round\" stroke-linejoin=\"round\" stroke-width=\"1.5\" viewBox=\"0 0 20 20\" width=\"20\" xmlns=\"http://www.w3.org/2000/svg\"><g><title></title><path d=\"M2.53001 7.81595C3.49179 4.73911 6.43281 2.5 9.91173 2.5C13.1684 2.5 15.9537 4.46214 17.0852 7.23684L17.6179 8.67647M17.6179 8.67647L18.5002 4.26471M17.6179 8.67647L13.6473 6.91176M17.4995 12.1841C16.5378 15.2609 13.5967 17.5 10.1178 17.5C6.86118 17.5 4.07589 15.5379 2.94432 12.7632L2.41165 11.3235M2.41165 11.3235L1.5293 15.7353M2.41165 11.3235L6.38224 13.0882\"></path></g></svg></button><button class=\"pencraft pc-reset pencraft icon-container view-image\" tabindex=\"0\" type=\"button\"><svg class=\"lucide lucide-maximize2 lucide-maximize-2\" fill=\"none\" height=\"20\" stroke=\"currentColor\" stroke-linecap=\"round\" stroke-linejoin=\"round\" stroke-width=\"2\" viewBox=\"0 0 24 24\" width=\"20\" xmlns=\"http://www.w3.org/2000/svg\"><polyline points=\"15 3 21 3 21 9\"></polyline><polyline points=\"9 21 3 21 3 15\"></polyline><line x1=\"21\" x2=\"14\" y1=\"3\" y2=\"10\"></line><line x1=\"3\" x2=\"10\" y1=\"21\" y2=\"14\"></line></svg></button></div></div></div></a><figcaption class=\"image-caption\">Chemical structure and SMILES string of cyclohexane and coumarin.</figcaption></figure></div><p>In practice, however, this descriptor-based model could not contend with the idiosyncratic nature of the structure-to-odor relationship: a molecule may be classified as citrus-like by virtue of its ester group, while a structurally analogous compound with only minor modifications might be seen as sharp, metallic, or entirely odorless.</p><p>As academic labs continued researching the logic of scent, focus shifted toward building larger datasets that linked molecular structure to qualitative descriptions of odor perception. In place of binary quantifiers such as odorous/odorless or pleasant/unpleasant, these collections captured distinct attributes like <em>fruity</em>, <em>floral</em>, <em>musky</em>, or <em>burnt</em>, offering a more embodied and qualitative relationship between chemistry and experience. </p><p>Democratizing databases such as the <a href=\"https://www.thegoodscentscompany.com/odor/elderflower.html\">Good Scents Company</a>, for example, expanded the vocabulary of olfaction available for computation, priming the field for machine learning approaches capable of finding patterns across large, multidimensional datasets.</p><p>In 2015, IBM Research and Rockefeller University launched the <a href=\"https://www.synapse.org/Synapse:syn2811262/wiki/78368\">DREAM Olfaction Prediction Challenge</a>, the first large-scale, open benchmark for predicting human olfactory perception from the physicochemical features of odor molecules. Researchers were provided with data comprising 476 chemically diverse odorants, each described by over 4,800 molecular features and rated by 49 human participants on 19 sensory attributes, such as <em>sweet</em>, <em>fish</em>, <em>mint</em>, and <em>sour</em>.</p><p>&#8203;&#8203;Competing teams trained machine-learning models to map feature-perception relationships and <a href=\"https://doi.org/10.1126/science.aal2014\">evaluated</a> their models using 69 unseen test molecules. The top models achieved prediction measures comparable to those observed among human participants, meaning the machine ratings were nearly as reliable as human noses when judging whether a chemical smelled floral or fishy. The models&#8217; predictions mirrored the average agreement reported in human odor-perception ratings, demonstrating accuracy of up to 85 percent across multiple different aromas.</p><p>Although the DREAM challenge yielded compelling evidence that odor perception could be forecasted computationally, it had its shortcomings. Namely, the study&#8217;s dataset remained modest in scale, leaving vast chemical territories uncharted, and its reliance on predefined molecular descriptors meant the models depended on prescribed features rather than uncovering the latent logic linking structure to odor.</p><p>By 2019, researchers at Google Brain (now DeepMind) <a href=\"https://arxiv.org/abs/1910.10685?utm_source=chatgpt.com\">sought</a> to overcome these constraints by expanding both the scale of training data and the<em> </em>fidelity of their model. Using deep learning and Graph Neural Networks (GNNs), a class of models that interpret molecules as graphs with atoms and bonds represented as nodes and edges, respectively, the study employed a dataset containing more than 5,000 odorants annotated by expert perfumers across over 100 descriptors, from <em>mint</em> and <em>potato</em> to <em>sulfurous</em> and <em>grassy</em>. <br /><br />Unlike the earlier DREAM Challenge, which relied on prescribed molecular features, such as atom counts and topological indices, this approach allowed the model to infer structure-odor relationships directly from data. To initiate the training process, each node (representing an atom) was seeded with basic chemical information, including atomic number, valence, hybridization, and formal charge.</p><p>Through a recursive process known as &#8220;message passing,&#8221; the state of each atom was repeatedly updated in response to information relayed by neighboring nodes. Layer by layer, the model built up a progressively bigger picture of a molecule: early layers learned to identify local features like carbonyls, halides, or ring systems, while deeper layers learned to recognize broader chemical motifs such as aromaticity, conjugation (the overlap of p-orbitals), and steric strain (the increase in potential energy of a molecule due to repulsion between electrons in atoms that are not directly bonded to each other). </p><div class=\"captioned-image-container\"><figure><a class=\"image-link image2 is-viewable-img\" href=\"https://substackcdn.com/image/fetch/$s_!bxA2!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7bd02bd1-7316-4342-8122-7760fc355f17_682x280.png\" target=\"_blank\"><div class=\"image2-inset\"><source type=\"image/webp\" /><img alt=\"\" class=\"sizing-normal\" height=\"280\" src=\"https://substackcdn.com/image/fetch/$s_!bxA2!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7bd02bd1-7316-4342-8122-7760fc355f17_682x280.png\" width=\"682\" /><div class=\"image-link-expand\"><div class=\"pencraft pc-display-flex pc-gap-8 pc-reset\"><button class=\"pencraft pc-reset pencraft icon-container restack-image\" tabindex=\"0\" type=\"button\"><svg fill=\"none\" height=\"20\" stroke=\"var(--color-fg-primary)\" stroke-linecap=\"round\" stroke-linejoin=\"round\" stroke-width=\"1.5\" viewBox=\"0 0 20 20\" width=\"20\" xmlns=\"http://www.w3.org/2000/svg\"><g><title></title><path d=\"M2.53001 7.81595C3.49179 4.73911 6.43281 2.5 9.91173 2.5C13.1684 2.5 15.9537 4.46214 17.0852 7.23684L17.6179 8.67647M17.6179 8.67647L18.5002 4.26471M17.6179 8.67647L13.6473 6.91176M17.4995 12.1841C16.5378 15.2609 13.5967 17.5 10.1178 17.5C6.86118 17.5 4.07589 15.5379 2.94432 12.7632L2.41165 11.3235M2.41165 11.3235L1.5293 15.7353M2.41165 11.3235L6.38224 13.0882\"></path></g></svg></button><button class=\"pencraft pc-reset pencraft icon-container view-image\" tabindex=\"0\" type=\"button\"><svg class=\"lucide lucide-maximize2 lucide-maximize-2\" fill=\"none\" height=\"20\" stroke=\"currentColor\" stroke-linecap=\"round\" stroke-linejoin=\"round\" stroke-width=\"2\" viewBox=\"0 0 24 24\" width=\"20\" xmlns=\"http://www.w3.org/2000/svg\"><polyline points=\"15 3 21 3 21 9\"></polyline><polyline points=\"9 21 3 21 3 15\"></polyline><line x1=\"21\" x2=\"14\" y1=\"3\" y2=\"10\"></line><line x1=\"3\" x2=\"10\" y1=\"21\" y2=\"14\"></line></svg></button></div></div></div></a><figcaption class=\"image-caption\">Graph Neural Network workflow for learned embeddings of scents. Credit: <a href=\"https://arxiv.org/pdf/1910.10685\">Sanchez-Lengeling et al. ArXiv (2019)</a></figcaption></figure></div><p>By the penultimate layer, the complete, networked &#8220;picture&#8221; of a molecule was condensed into a fixed-length, 63-dimensional vector, known as an &#8220;odor embedding,&#8221; capturing the molecule&#8217;s perceptual qualities in a computable form. This embedding was then used for classification tasks, such as predicting specific odor descriptors or categorizing the molecule&#8217;s scent profile.</p><p>When these &#8220;images&#8221; were compressed, molecules that smelled alike clustered together even when their chemical scaffolds were structurally dissimilar. For example, certain esters and ketones, molecules that share little in common structurally, appeared close in this space because both carried a sweet, fruity scent, while structurally related compounds could be separated if their perceived odors diverged. This act of compression thus exposed a<em> &#8220;geometry of smell</em>.&#8221;</p><p>It also laid the groundwork for Google&#8217;s subsequent introduction of the <a href=\"https://www.science.org/doi/10.1126/science.ade4401\">Principal Odor Map </a>(POM): the first map of smell, in which traversing through space corresponds to crossing distinct regions of aroma, from jasmine to potato. Building on the previous study, the researchers expanded the model&#8217;s embedding layer by 193 dimensions, creating a high-dimensional coordinate system that positions each odorous molecule as a single point within a continuous manifold (e.g., a connected shape or space that behaves geometrically, enabling meaningful comparison and relation between the position of points representing molecules). In this space, perceptually similar odors occupy nearby positions, much as blue lies closer to turquoise than to crimson on the color wheel, while increasingly dissimilar ones are separated by greater distance.</p><p>When the researchers subjected the POM to <a href=\"https://www.biorxiv.org/content/10.1101/2022.07.21.500995v3\">further tests</a>, they found that it could generalize beyond human olfactory perception, predicting olfactory receptor activity across species from mice to insects. This cross-species application suggests that the model&#8217;s internal structure even captures evolutionary principles of scent. On the map itself, metabolically related compounds &#8212; those separated by only a few metabolic reactions or naturally co-occurring in the same substance &#8212; also occupy nearby coordinates, forming concentrated clusters that reflect their shared chemical and perceptual relationships.</p><p>By capturing such subtle perceptual relationships, the model not only surpassed traditional chemoinformatic algorithms in terms of predictive accuracy, but also enabled early steps toward the design of <em>new </em>odorants.</p><p>Building on this foundation, <a href=\"https://www.osmo.ai/\">Osmo</a>, a start-up spun out of Google Research in 2022, has begun leveraging its proprietary <a href=\"https://www.osmo.ai/blog/osmo-launches-generation-worlds-first-ai-powered-fragrance-house\">Olfactory Intelligence (OI) platform</a> informed by POM to translate sensory scent data into chemical design. The company&#8217;s goal is to &#8220;<a href=\"https://www.osmo.ai/blog/chapter-1-introducing-osmo\">give computers a sense of smell</a>.&#8221;</p><p><em>Glossine, Fractaline, and Quasarine</em> are the first prompt-generated <a href=\"https://www.osmo.ai/blog/osmo-uses-ai-to-discover-never-before-smelled-ingredients\">fragrance molecules</a> created using its platform. According to the company, <em>Glossine</em> delivers a &#8220;Las Vegas-style sparkle,&#8221; with a floral jasmine-like top note that adheres well to textiles; while <em>Fractaline</em> offers a versatile violet note, reputed to give a powdery, &#8220;second-skin&#8221; impression; and <em>Quasarine</em> presents an intense yet delicate jasmine aroma claimed to deliver a long-lasting &#8220;fresh petal-y effect.&#8221; While these descriptors could be shared by other perfumes, these fragrances are entirely original. Where traditional perfumery relies on the discovery and refinement of proprietary molecules in a process that <a href=\"https://support.votivo.com/en-US/how-long-does-it-take-to-develop-a-votivo-fragrance-49184\">usually takes years</a>, Osmo can &#8220;create&#8221; completely new odorants through this OI platform in just <a href=\"https://royalexaminer.com/tech-startup-uses-ai-to-create-scents-in-48-hours-but-critics-say-it-misses-the-point/\">a few days</a>.</p><p>The potential applications of Osmo&#8217;s OI platform, however, extend far beyond fragrance. Because the Principal Odor Map can extrapolate olfaction across species, Osmo has <a href=\"https://www.osmo.ai/blog/osmo-receives-8-5-million-in-funding-to-support-advancement-in-ai-enabled-insect-control\">leveraged</a> it for developing synthetic alternatives to DEET, the active ingredient in most commercial insect repellents, but one which has been linked to adverse effects, including <a href=\"https://www.scientificamerican.com/article/is-it-true-that-the-deet/\">severe skin reactions</a> like rashes and welts. Trained on experimental datasets measuring mosquito repellency, the model can predict the repellent efficacy of nearly any molecule. <a href=\"https://www.biorxiv.org/content/10.1101/2022.09.01.504601v4\">Experimental validation</a> confirmed over a dozen new molecules as repellent as DEET, but which could offer safer, longer-lasting alternatives for killing mosquitoes and ticks.</p><p>Beyond repellents, Osmo has broader ambitions to also develop non-invasive diagnostic tools. They <a href=\"https://www.weforum.org/videos/computers-smell/\">aim</a>, for example, to identify volatile signatures emitted by various diseases, enabling algorithms with a &#8220;digital nose&#8221; to detect conditions such as Parkinson&#8217;s, diabetes, and certain cancers through subtle changes in body odor or sebum, a type of signal <a href=\"https://www.nationalgeographic.com/animals/article/160319-dogs-diabetes-health-cancer-animals-science\">detectable by dogs</a>.</p><p>Perhaps the most promising application of Osmo&#8217;s technology, though, relates to sustainability. By synthetically generating fragrance molecules, Osmo offers a path toward decreasing the environmental impact of industrial scent-production. A canonical and increasingly endangered raw material for scents is rose, whose odor has captivated humans for millennia yet remains one of the most <a href=\"https://www.youtube.com/watch?v=26pEG7Ghgpg&amp;t=24s\">labor-intensive</a> and <a href=\"https://nyc.ph/blogs/perfume/from-oud-to-ambergris-a-look-at-the-most-expensive-natural-perfume-ingredients?srsltid=AfmBOoo3EdpbsJCtK1Sz4CfUh2Gb8qWJl0tnF7LuLfzDa_cqXaadIx8v\">expensive natural ingredients</a> (some 60,000 roses, roughly 180 kilograms of petals, are required to produce a single ounce of oil, which sells for $8,000 to $12,000 per kilogram). By plotting the scent of <em>Rosa damascena </em>and <em>Rosa centifolia</em> on the POM, Osmo can work toward &#8220;reverse-engineering&#8221; the rose and other floral notes in pursuit of designing &#8220;<a href=\"https://www.wired.com/story/this-startup-is-using-ai-to-unearth-new-smells/\">an alternate universe of perfume ingredients</a>&#8221; that are perceived similarly without requiring raw botanical sources.</p><p>Osmo is not alone in its effort to advance computationally mediated scent. <a href=\"https://www.givaudan.com/media/media-releases/2019/givaudan-fragrances-launches-carto-its-artificial-intelligence-powered-tool\">Givaudan&#8217;s Carto,</a> the digital design platform of one of the world&#8217;s first and largest fragrance houses, is also using computer technology to design its perfumes. Carto integrates predictive algorithms with a database of over <a href=\"https://www.wallpaper.com/fashion-beauty/fragrance/perfume-and-ai#:~:text=This%20AI%2Dpowered%20tool%20uses%20a%20massive%20%E2%80%98Odour%20Value%20Map%E2%80%99%20to%20generate%20scents%20from%20over%205%2C000%20rare%20or%20niche%20global%20ingredients.\">5,000 raw materials</a> (five times more than a human perfumer typically manages) to help perfumers simulate, match, and modify scents. Whereas the classical perfume <em>orgue &#224; parfums</em> once arranged rows of essences within arm&#8217;s reach, Carto replaces physical glass vials with a <a href=\"https://www.youtube.com/watch?v=mc1-9ow_xe0\">virtual glass touchscreen display</a>. The platform was used to formulate Tom Ford&#8217;s <em><a href=\"https://archive.is/q0oiv\">Bois Pacifique</a></em>, which launched in January 2025.</p><div class=\"youtube-wrap\" id=\"youtube2-mc1-9ow_xe0\"><div class=\"youtube-inner\"></div></div><p>Despite such recent successes, the mysteries of olfaction are far from solved. To digitize the scent of a rose, for instance, requires contending with over 300 volatile compounds that contribute to its spicy, honeyed, and tea-like notes. These compounds are not simply additive but dynamic; their interactions, relative concentrations, and release create emergent qualities unpredictable from any single component alone. While platforms like Osmo and Carto can tinker with individual molecules, capturing the interplay of dozens or hundreds of odorants in a given mixture remains a formidable challenge, the mastery of which will be next in digitizing smell.</p><p>Both industry and academia continue to advance this aim. The most recent DREAM Olfaction Challenge, slated to close in late <a href=\"https://www.synapse.org/Synapse:syn64743570/wiki/630800\">2025</a>, tasked international teams with predicting perceptual similarity across volatile mixtures. It will draw on a dataset of over <a href=\"https://www.synapse.org/Synapse:syn53470621/wiki/626022#:~:text=Using%20publicly%20available,mixture%20pairs.\">700 combinations</a> containing anywhere from two to ten different molecules to assess how accurately teams can digitally model smells, their interactions, and their resulting impressions. Although peer-reviewed results have not yet been published, <a href=\"https://www.synapse.org/Synapse:syn64743570/wiki/634774\">early findings</a> point toward improvements in complex-mixture scent prediction.</p><h2>The Future of Olfaction</h2><p>Traditionally, the fragrance industry has made a point of drawing a line between the <em>natural</em> and the <em>synthetic</em>, with the introduction of synthetic scent molecules in the mid-nineteenth century garnering both criticism and celebration. Early critics<a class=\"footnote-anchor\" href=\"https://www.asimov.press/feed#footnote-5\" id=\"footnote-anchor-5\" target=\"_self\">5</a> proclaimed such synthetic materials as &#8220;vulgar debasements&#8221; of perfumery, reflecting anxieties surrounding industrialization and the loss of heritage and craftsmanship. But others celebrated this innovation. Guerlain&#8217;s <em><a href=\"https://onlinelibrary.wiley.com/doi/abs/10.1002/cbdv.200890090\">Jicky</a></em> (1889), for instance, which incorporated synthetic vanillin and coumarin, was hailed as a new chapter for perfume. Moreover, commercial houses like Lubin <a href=\"https://global.oup.com/academic/product/scents-and-sensibility-9780198701750?cc=gb&amp;lang=en&amp;\">marketed</a> synthetic musks as &#8220;triumphs of science &#8230; over Nature,&#8221; promoting the &#8220;non-evanescence&#8221; (long-lasting) qualities of synthetic chemicals in comparison to natural ingredients. <br /><br />These developments unfolded within a wider cultural moment that the essayist Max Beerbohm described as <a href=\"https://1890s.ca/wp-content/uploads/YBv1_beerbohm_defence.pdf\">&#8220;a new epoch of artifice.&#8221;</a> The emergence of synthetics did not simply replace the natural; it upset the very idea of what &#8220;naturalness&#8221; meant &#8212; did it refer to an ingredient&#8217;s source, its sensory impression, or the meanings attached to it?</p><p>Even today, materials such as ambergris (a waxy, sweet-smelling substance formed in the digestive system of sperm whales and expelled into the ocean) remain highly coveted precisely because of their scarcity and natural origins. Revered as &#8220;the treasure of the sea,&#8221; <a href=\"https://www.nhm.ac.uk/discover/what-is-ambergris.html\">ambergris</a> is found in less than five percent of sperm whales and develops its celebrated complexity over years of exposure to sea, salt, and sun. Such a valued ingredient commands reverence <em>precisely because</em> of its rarity, exposing how ideas of purity and authenticity continue to shape our perception and appreciation. Ultimately, the persisting popularity of ingredients such as ambergris demonstrates that value pertains as much to culture as to chemistry, reflecting not only the reality of raw materials but also the moral and aesthetic weight we ascribe to their provenance.</p><p>Given this legacy, how might we relate to machine-generated molecules?<br /><br />The shift to digitized scent will require that we rethink not only the substances themselves, but also the stories we tell as we find ways to valorize their synthetic origins. Such computationally-designed scents are, after all, safer, allergen-friendly formulations compliant with <a href=\"https://ifrafragrance.org/initiatives-positions/environment-health/chemicals-regulation/chemicals-legislation-eu/reach\">evolving regulations</a>. Furthermore, AI-designed aromas often use more sustainably produced molecules, with a lab-created civetone replacing wild civet extract (a substance associated with the questionable <a href=\"https://npacertification.com/2025/07/16/exploiting-the-civet-for-musk/\">practice </a>of keeping civet cats in captivity for their perineal gland secretions). <br /><br />A similar parallel has played out elsewhere: a one-carat lab-grown diamond, while chemically and optically identical to a mined one, can cost <a href=\"https://www.forbes.com/sites/garthfriesen/2025/03/22/lab-grown-diamonds-boom-is-it-game-over-for-mined-diamonds/\">less than a quarter</a> of the price, despite being purer and more ethically produced. While they have been derided as &#8220;<a href=\"https://www.estatediamondjewelry.com/lab-diamonds-scam/\">costume jewelry</a>,&#8221; the market for lab-grown diamonds was valued at more than <a href=\"https://www.nextmsc.com/report/lab-grown-diamonds-market\">$18 billion</a> in 2023 and is expected to continue growing. The connotations of luxury are increasingly leaning toward narratives of sustainable and ethical sourcing. <br /><br />Machine-imagined scents may follow a similar arc. But just as the telescope widened our aperture to distant cosmic structures, digitally composed scents will expand our olfactory range, this time towards the smallest scales of chemical configuration.</p><p class=\"button-wrapper\"><a class=\"button primary\" href=\"https://www.asimov.press/subscribe\"><span>Subscribe now</span></a></p><div><hr /></div><p><strong>Taylor Rayne </strong>holds a degree in biochemistry and computer science, as well as a deep curiosity for the interplay between scientific and cultural production. She currently works at the Novo Nordisk Foundation Center for Biosustainability, based at the Technical University of Denmark.</p><p>A special thanks to Christiana Agapakis for her generosity and resources; Jasmina Aganovic for her thoughtful guidance and time; Xander Balwit for her steady editorial support, and of course, Astrid.</p><p><strong>Cite: </strong>Rayne, T. &#8220;Scent, In Silico.&#8221; <em>Asimov Press </em>(2026). DOI: <a href=\"https://doi.org/10.62211/28jw-12ty\">10.62211/28jw-12ty</a></p><div class=\"footnote\"><a class=\"footnote-number\" contenteditable=\"false\" href=\"https://www.asimov.press/feed#footnote-anchor-1\" id=\"footnote-1\" target=\"_self\">1</a><div class=\"footnote-content\"><p>Humans have ~20 million sensory neurons in total. The exact size, location, and neuron density of the olfactory epithelium <a href=\"https://onlinelibrary.wiley.com/doi/10.1002/ca.22338\">varies by individual</a> and can change based on factors like age, exposure to airborne hazards, or disease.</p></div></div><div class=\"footnote\"><a class=\"footnote-number\" contenteditable=\"false\" href=\"https://www.asimov.press/feed#footnote-anchor-2\" id=\"footnote-2\" target=\"_self\">2</a><div class=\"footnote-content\"><p>A stereoisomer is a molecule composed and connected via the same atoms as another, but with a different three-dimensional structure.</p></div></div><div class=\"footnote\"><a class=\"footnote-number\" contenteditable=\"false\" href=\"https://www.asimov.press/feed#footnote-anchor-3\" id=\"footnote-3\" target=\"_self\">3</a><div class=\"footnote-content\"><p>In the perfume business, musks are prized for their sensuality and longevity. They vary extraordinarily in chemical structure and source. Traditional musks include <em>Moschus moschiferus</em> from the abdominal gland of the male musk deer, civet from the perineal glands of the African civet cat, castoreum from the castor sacs of beavers, and ambergris from sperm whales. But natural musks are also costly, difficult to source, and fraught with ecological and ethical concerns, spurring the search for synthetic alternatives. In 1888, Albert Baur discovered the first synthetic &#8220;nitro-musk&#8221; while working with TNT, soon developing compounds such as musk ketone, musk xylene, and musk ambrette, today used in perfumes including Chanel No. 5. These nitro-musks were later banned due to toxicity and instability, replaced by safer, synthetic musks.</p></div></div><div class=\"footnote\"><a class=\"footnote-number\" contenteditable=\"false\" href=\"https://www.asimov.press/feed#footnote-anchor-4\" id=\"footnote-4\" target=\"_self\">4</a><div class=\"footnote-content\"><p>Enfleurage is a centuries-old perfume extraction technique used for delicate flowers like jasmine and tuberose that are too fragile for heat-based methods. Fresh petals are laid onto glass plates coated with purified animal fat &#8212; typically odorless lard or tallow &#8212; which absorbs the flowers&#8217; volatile aromatic compounds over one to three days. The spent petals are removed and replaced with fresh ones, a process repeated for weeks until the fat becomes saturated with fragrance, creating a pomade. This pomade is then washed with alcohol to dissolve out the concentrated perfume oils; once the alcohol evaporates, what remains is an &#8220;absolute&#8221; &#8212; the purest essence of the flower. Though largely abandoned today due to its labor-intensiveness, enfleurage once produced some of perfumery&#8217;s most exquisite and faithful floral scents.</p></div></div><div class=\"footnote\"><a class=\"footnote-number\" contenteditable=\"false\" href=\"https://www.asimov.press/feed#footnote-anchor-5\" id=\"footnote-5\" target=\"_self\">5</a><div class=\"footnote-content\"><p><a href=\"https://www.archambault.ca/livres/fabrique-des-parfums-naissance-d'une-industrie-de-luxe-la/eug%C3%A9nie-briot/9782363581716/?lang=en-ca&amp;id=1759138&amp;srsltid=AfmBOoqRcpD1ghGicEnTQwXTzdJOsZ0yCmNPX64vCa2yHDXXY-6ubZn-\">Eug&#233;nie Briot, </a><em><a href=\"https://www.archambault.ca/livres/fabrique-des-parfums-naissance-d'une-industrie-de-luxe-la/eug%C3%A9nie-briot/9782363581716/?lang=en-ca&amp;id=1759138&amp;srsltid=AfmBOoqRcpD1ghGicEnTQwXTzdJOsZ0yCmNPX64vCa2yHDXXY-6ubZn-\">La Fabrique des parfums &#8211; Naissance d&#8217;une industrie de luxe</a></em>. As France has historically been central to the development of perfumery, this French-language source provides a detailed account of early critics&#8217; reactions to synthetic materials and the broader tensions between industrialization and artisanal craftsmanship.</p><p></p></div></div>"
            ],
            "link": "https://www.asimov.press/p/scent",
            "publishedAt": "2026-02-16",
            "source": "Asimov Press",
            "summary": "<div class=\"captioned-image-container\"><figure><a class=\"image-link image2\" href=\"https://substackcdn.com/image/fetch/$s_!njz9!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F96739113-bd6a-4006-81f2-85f8be9f8b0d_2000x1260.gif\" target=\"_blank\"><div class=\"image2-inset\"><source type=\"image/webp\" /><img alt=\"\" class=\"sizing-normal\" src=\"https://substackcdn.com/image/fetch/$s_!njz9!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F96739113-bd6a-4006-81f2-85f8be9f8b0d_2000x1260.gif\" /><div></div></div></a></figure></div><p>By <strong>Taylor Rayne</strong></p><p>Smell is our most primal and, arguably, most emotionally potent sense. It summons memories, shapes taste, and <a href=\"https://www.nature.com/articles/s41598-021-96334-3\">influences behavior</a>: the aroma of coffee is capable of enhancing <a href=\"https://www.sciencedirect.com/science/article/pii/S2213422019302628\">alertness</a> well before caffeine ever reaches the bloodstream. A hint of sunscreen collapses decades, taking us back to youth; but, pinch the nose, and suddenly a slice of apple is hard to distinguish from a piece of raw potato.</p><p>Despite its significance, scent remains the most mysterious of our senses. Unlike vision or hearing, it resists straightforward formalization. The challenge lies not only in the vast molecular diversity of odorants, which vary in far more ways than photons or frequencies, but also in the effort to build a shared vocabulary and technology capable of codifying subjective sensation. So while machines have learned to see through computer vision and hear through signal processing, scent remains stubbornly analog. There has been no RGB of odor, no Fourier transform for smell.</p><p>At least, until now.</p><p>Tech giants, including Google, startups such as <a href=\"https://www.osmo.ai/\">Osmo</a>, and even traditional fragrance houses like <a href=\"https://www.givaudan.com/fragrance-beauty/perfumery-school/carto-the-future-of-fragrance-formulations\">Givaudan</a> have begun turning to AI to probe the possibility of digitizing",
            "title": "Scent, In Silico"
        },
        {
            "content": [],
            "link": "https://olano.dev/blog/dangerously-skip",
            "publishedAt": "2026-02-16",
            "source": "Facundo Olano",
            "summary": "What if we dutifully communicate the risks and trade-offs to our organizational leadership and they still want to take those risks",
            "title": "-\u200b-dangerously-skip-reading-code"
        },
        {
            "content": [
                "<div class=\"trix-content\">\n  <div>A few weeks ago I sat down with David Senra (of the outstanding Founders podcast) to record an interview for his new podcast. It just came out yesterday.<br /><br />We talked for about two-and-a-half hours and hit on all sorts of topics including:<br /><br /></div><ul><li>Build Products for Yourself</li><li>Low Costs, Small Company, Enough Customers</li><li>Your Only Competition Is Your Costs</li><li>How 37signals Stays Lean</li><li>Rewriting Basecamp &amp; Fighting Software Bloat</li><li>Why \"Enough\" Beats Growth</li><li>Product People vs. Business Shells</li><li>The \"So What?\" Mindset</li><li>Staying Close to Customers</li><li>The Reward for Good Work Is More Work</li><li>Six-Week Horizons &amp; Compounding Decisions</li><li>Anti-Fragile Business With Tiny Units</li><li>Gal\u00e1pagos Product Design</li><li>Radical Authenticity Over Marketing Tricks</li><li>Rick Rubin &amp; Intuition-Driven Building</li><li>Lightning in a Bottle &amp; Knowing When to Stop</li><li>Defining Success: Pride in the Work</li><li>Independence Through Profitability</li><li>When Tech Adds Friction Instead of Value</li><li>Ruthless Editing &amp; What Never Changes</li><li>Longevity as the Moat</li><li>Building by Intuition</li></ul><div><br />I throughly enjoyed the conversation. It's probably the one that most accurately captures my current lens on business and life.<br /><br />You can listen on Spotify here:<br /><a href=\"https://open.spotify.com/episode/3ecxE9JmIXyTouizVLQND7?si=c2b03e94c42e477f\">https://open.spotify.com/episode/3ecxE9JmIXyTouizVLQND7?si=c2b03e94c42e477f</a><br /><br />YouTube here:<br /><a href=\"https://www.youtube.com/watch?v=BdDCtMA1gSw\">https://www.youtube.com/watch?v=BdDCtMA1gSw</a><br /><br />X here:<br /><a href=\"https://x.com/davidsenra/status/2023049664195690506\">https://x.com/davidsenra/status/2023049664195690506</a><br /><br />...and of course wherever you get your podcasts. Look for the podcast called \"David Senra\". And don't just listen to my episode \u2014 David has interviewed a bunch of wonderful, insightful people. David is worth listening to.<br /><br />Hope you enjoy!</div><div><br /></div><div>-Jason</div>\n</div>"
            ],
            "link": "https://world.hey.com/jason/a-new-interview-0aced606",
            "publishedAt": "2026-02-16",
            "source": "Jason Fried",
            "summary": "<div class=\"trix-content\"> <div>A few weeks ago I sat down with David Senra (of the outstanding Founders podcast) to record an interview for his new podcast. It just came out yesterday.<br /><br />We talked for about two-and-a-half hours and hit on all sorts of topics including:<br /><br /></div><ul><li>Build Products for Yourself</li><li>Low Costs, Small Company, Enough Customers</li><li>Your Only Competition Is Your Costs</li><li>How 37signals Stays Lean</li><li>Rewriting Basecamp &amp; Fighting Software Bloat</li><li>Why \"Enough\" Beats Growth</li><li>Product People vs. Business Shells</li><li>The \"So What?\" Mindset</li><li>Staying Close to Customers</li><li>The Reward for Good Work Is More Work</li><li>Six-Week Horizons &amp; Compounding Decisions</li><li>Anti-Fragile Business With Tiny Units</li><li>Gal\u00e1pagos Product Design</li><li>Radical Authenticity Over Marketing Tricks</li><li>Rick Rubin &amp; Intuition-Driven Building</li><li>Lightning in a Bottle &amp; Knowing When to Stop</li><li>Defining Success: Pride in the Work</li><li>Independence Through Profitability</li><li>When Tech Adds Friction Instead of Value</li><li>Ruthless Editing &amp; What Never Changes</li><li>Longevity as the Moat</li><li>Building by Intuition</li></ul><div><br />I throughly enjoyed the conversation. It's probably the one that most accurately captures my current lens on business and life.<br /><br />You can listen on Spotify here:<br /><a href=\"https://open.spotify.com/episode/3ecxE9JmIXyTouizVLQND7?si=c2b03e94c42e477f\">https://open.spotify.com/episode/3ecxE9JmIXyTouizVLQND7?si=c2b03e94c42e477f</a><br /><br />YouTube here:<br /><a href=\"https://www.youtube.com/watch?v=BdDCtMA1gSw\">https://www.youtube.com/watch?v=BdDCtMA1gSw</a><br /><br />X here:<br /><a href=\"https://x.com/davidsenra/status/2023049664195690506\">https://x.com/davidsenra/status/2023049664195690506</a><br /><br />...and of course wherever you get your podcasts. Look for the podcast called \"David Senra\". And don't just listen to my episode \u2014",
            "title": "A new interview"
        },
        {
            "content": [],
            "link": "https://bernsteinbear.com/blog/toy-tbaa/?utm_source=rss",
            "publishedAt": "2026-02-16",
            "source": "Max Bernstein",
            "summary": "<p><em>Another entry in the <a href=\"https://pypy.org/categories/toy-optimizer.html\">Toy Optimizer series</a></em>.</p> <p>Last time, we did <a href=\"https://bernsteinbear.com/blog/toy-load-store/\">load-store forwarding</a> in the context of our Toy Optimizer. We managed to cache the results of both reads from and writes to the heap\u2014at compile-time!</p> <p>We were careful to mind object aliasing: we separated our heap information into alias classes based on what offset the reads/writes referenced. This way, if we didn\u2019t know if object <code class=\"language-plaintext highlighter-rouge\">a</code> and <code class=\"language-plaintext highlighter-rouge\">b</code> aliased, we could at least know that different offsets would never alias (assuming our objects don\u2019t overlap and memory accesses are on word-sized slots). This is a coarse-grained heuristic.</p> <p>Fortunately, we often have much more information available at compile-time than just the offset, so we should use it. I mentioned in a footnote that we could use type information, for example, to improve our alias analysis. We\u2019ll add a lightweight form of <a href=\"https://bernsteinbear.com/assets/img/tbaa.pdf\">type-based alias analysis (TBAA)</a> (PDF) in this post.</p> <h2 id=\"representing-types\">Representing types</h2> <p>We return once again to Fil Pizlo land, specifically <a href=\"https://gist.github.com/pizlonator/cf1e72b8600b1437dda8153ea3fdb963\">How I implement SSA form</a>. We\u2019re going to be using the hierarchical heap effect representation from the post in our implementation, but you can use your own type representation if you",
            "title": "Type-based alias analysis in the Toy Optimizer"
        },
        {
            "content": [
                "<p>When I explain that Claude Code has changed my relationship with developing software completely, I&#8217;m under-exaggerating&#8230; if that&#8217;s even a thing.</p>\n<p>This off-the-cuff piece started as a unposted social update that read:</p>\n<blockquote><p>\n Watching Claude Code adeptly use every type of Unix command shows me that a) you can do anything in Unix, b) my higher-level operating system mostly hides this functionality from me, c) I am extremely lazy, and d) I am immensely curious.\n</p></blockquote>\n<h2>An Introduction to Pansy Rain</h2>\n<p>This morning it&#8217;s going to start to rain \u2014 a lot. As previously described, I deeply enjoy tromping around my forest in the rain, fixing drainage, and <a href=\"https://randsinrepose.com/archives/the-stick-in-the-stream/\">pulling sticks</a> from stuff. More importantly, I am deeply curious about how the weather works. This is curiosity motivated by an immense fear of climate change and a desire to understand clearly what will occur this week. And why.</p>\n<p>Years ago, I started posting a short paragraph to a group chat filled with fellow mountain Dads. It was a three or four-paragraph affair about what was going to happen with the weather in the coming week. No one requested this lightweight and trying-to-be-approachable blurb. It started and stopped during the rainy season in Northern California, but over time, I continued through the summer, starting to track heat events, air quality (see fires), and fog.</p>\n<p>Here&#8217;s the one from last week:</p>\n<blockquote><p>\n Welcome back, Winter. We missed you. Rain, lots of it this week. Chance of pansy rain tonight, but I wouldn&#8217;t count on it. Looks like we&#8217;re in for a solid inch-plus on Tuesday morning through Wednesday evening, but the show starts on Saturday night. Solid chonky rain through the end of the week and into next. Weirdly, the first storm is coming from Hawaii (wet), but the big weekend fronts are coming from Alaska. This latter rain is going to hang.\n</p></blockquote>\n<p>See? Approachable? My initial goal was that anyone could quickly read this blurb and plan for their week. <em>Ah, yes. Rain on Tuesday. I will plan accordingly.</em></p>\n<p>The friendly tone and brief amount of content might lead you to believe the construction was equally brief. It&#8217;s not. The initial flow was:</p>\n<ul>\n<li>Fire up <a href=\"https://www.wunderground.com/forecast/us/ca/los-gatos/95033?cm_ven=localwx_10day\">Wunderground&#8217;s 10-day forecast</a>. This deliciously informationally dense chart is my go-to initial stop for all things local weather. It includes rain, cloud cover, chance of precipitation, accumulated rain, and wind speed \u2014 and a lot more.</li>\n<li>Compare against <a href=\"https://www.windy.com/\">Windy.com</a>. Windy has similar information to Wunderground, but includes stunning maps that include a mind-boggling amount of different layer visualizations. The goal was to compare data from Wunderground to what Windy reported, which tended to provide better rain forecasts from my local microclimate.</li>\n</ul>\n<p>It started as 30 minutes of work, and it grew to over an hour of research. As I received light positive feedback for the posts, I started to research data on local streams and reservoirs, along with other fun facts I discovered during my research.</p>\n<p>Then the robots showed up.</p>\n<h2>An Introduction to Chonky Rain</h2>\n<p>This morning. <strong>Just this morning</strong>. Here&#8217;s what I&#8217;ve done with Claude Code:</p>\n<ul>\n<li>Pulled snapshots from my local weather station to backfill my storm tracker, which is a feature I built last night to track before, during, and after snapshots of storms. I wanted to see all storms (8) from this season, so I had the robots look at historical data, find the storms, and then backfill using my local weather station and open data sources (<a href=\"https://www.weatherlink.com\">Weatherlink</a>, <a href=\"https://open-meteo.com\">Open-Meteo</a>, <a href=\"https://alert.valleywater.org\">Valley Water</a>, and others)</li>\n<li>Found an issue with the data where I was fixing a <a href=\"https://www.davisinstruments.com/collections/vantage-pro2\">sensor</a> issue, which caused a data spike. Smoothed the spike.</li>\n<li>Comparing my local readings with other data sources; refined a multiplier I use to account for my microclimate.</li>\n<li>Updated creek data to include week-over-week change, so I get a better sense of how water is flowing down those mountains, into the reservoir, and then into town.</li>\n<li>Moved all this data into the Sunday report mail that I use to write my four-sentence approachable blurb.</li>\n<li>Oh yeah, as I write this piece, the robots and I are now planning what we&#8217;re going to track during the Summer. Fire alerts, air quality, and fog monitoring.</li>\n</ul>\n<p>This morning&#8217;s work builds on an existing set of scripts that run early every Sunday morning to track:</p>\n<ul>\n<li>Weather observations and color for the coming week.</li>\n<li>Current conditions from my local weather station.</li>\n<li>Last 7 days of rainfall.</li>\n<li>10-day forecast.</li>\n<li>Local reservoirs week-over-week change.</li>\n<li>The first version of my storm tracker functionality.</li>\n</ul>\n<p>I am perfectly capable of building any part of the above system. I am capable of finding the services that provide the information, signing up for an API, understanding the API, building code to call that API, generating a report, and sending that report as an email. Done it a lot. Made a successful career of it, too.</p>\n<p>I&#8217;ve written none of the code for this weather project. I&#8217;ve signed up for some API keys, but mostly what I&#8217;ve done is tell Claude Code what I&#8217;m looking to do and let the robot figure out what API we need, how to use it, and then suggest different ways we can report this information.</p>\n<h2>Extremely Lazy and Immensely Curious</h2>\n<p>When I go through these types of robot-related productivity rants, someone invariably unhelpfully volunteers, &#8220;Well, this is a good way to get really dumb.&#8221; What this someone is suggesting is that because I am doing none of the work involved in building the thing, I will not intellectually profit from the exercise.</p>\n<p>In a world where I hadn&#8217;t spent several decades building software, I would partially agree. The lack of domain experience could mean I trust whatever the robot&#8217;s built at face value, but \u2014 just like humans \u2014 the robots make mistakes and often straight-up lie. Been dealing with those situations for a bunch of decades, too. It&#8217;s not a problem, it&#8217;s how humans (and robots) work.</p>\n<p>Yes, I am extremely lazy. I&#8217;ve developed a set of habits that support this laziness. Where&#8217;s the best affordable Italian restaurant in Greenwich Village? Ask Noah. He knows, and he responds quickly. How should I think about this emotionally charged and complicated people situation? Ask Julia. She knows, and she can view complete emotional chaos dispassionately. Which API should I use to monitor fog? Ask the robots. They don&#8217;t know, but they know how to know.</p>\n<p>Am I sad as I watch the robot expertly craft semi-familiar Unix commands to perform all the weather-related wizardry? Nope. I learned to delegate to humans a long time ago. It was hard to give up the Legos to someone else, but this forced me to learn other lessons. How to ask others to help. How to clearly make requests. How to listen to their responses to see if they heard. Hearing them. Watching how they work and when they ask for help. Learning how to <a href=\"https://randsinrepose.com/archives/act-last-read-the-room-and-taste-the-soup/\">taste the soup</a>. Discovering lies. Fixing them. Remaining immensely curious and always learning how to communicate better.</p>\n<p>Here&#8217;s this week&#8217;s weather blurb. Written by me:</p>\n<blockquote><p>\n At this second, it&#8217;s partly sunny at the house, but just wait. It&#8217;s coming. Around noon today, chonk-i-ish rain begins and continues through Monday. Let&#8217;s say two inches today and three inches tomorrow. Sweet. Short break on Monday night, but another inch on Tuesday and a drizzle on Wednesday before we dry out a bit, but more next week. Might be chonky.</p>\n<p> (Two inches last week at Chez Rands, Lexington Reservoir up 2.5% on the week, twenty-nine inches for this rain season so far.)\n</p></blockquote>"
            ],
            "link": "https://randsinrepose.com/archives/extremely-lazy-and-immensely-curious/",
            "publishedAt": "2026-02-16",
            "source": "Rands in Repose",
            "summary": "When I explain that Claude Code has changed my relationship with developing software completely, I&#8217;m under-exaggerating&#8230; if that&#8217;s even a thing. This off-the-cuff piece started as a unposted social update that read: Watching Claude Code adeptly use every type of Unix command shows me that a) you can do anything in Unix, b) my higher-level&#8230; <a class=\"excerpt-more\" href=\"https://randsinrepose.com/archives/extremely-lazy-and-immensely-curious/\">more</a>",
            "title": "Extremely Lazy and Immensely Curious"
        },
        {
            "content": [
                "<p>I used to write the design while AI coded the software. Architecture decisions, API contracts, data models on my side; implementation on its side. Fast forward a few months and more and more the model now handles both, because it&#8217;s more right than wrong. The &#8220;outer loop&#8221; of system design, the thing that engineers would do while AI wrote the the code, turned out to be just another task it would absorb.</p><p>Over the past year, Tech Twitter has mostly converged on the same answer to this problem<a class=\"footnote-anchor\" href=\"https://blog.sshh.io/feed#footnote-1\" id=\"footnote-anchor-1\" target=\"_self\">1</a>. As AI eats execution, taste is the moat.</p><p>And while they&#8217;re right about taste mattering. I&#8217;m less sure about the moat part. A moat is something you build once and defend. Taste feels more like alpha<a class=\"footnote-anchor\" href=\"https://blog.sshh.io/feed#footnote-2\" id=\"footnote-anchor-2\" target=\"_self\">2</a>: a decaying edge, only valuable relative to a rising baseline. And that baseline is rising faster than many realize.</p><div class=\"captioned-image-container\"><figure><a class=\"image-link image2 is-viewable-img\" href=\"https://substackcdn.com/image/fetch/$s_!RgDA!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F98d057ae-0f80-4493-b587-c9b4693ac10d_1024x687.jpeg\" target=\"_blank\"><div class=\"image2-inset\"><source type=\"image/webp\" /><img alt=\"\" class=\"sizing-normal\" height=\"391.1337890625\" src=\"https://substackcdn.com/image/fetch/$s_!RgDA!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F98d057ae-0f80-4493-b587-c9b4693ac10d_1024x687.jpeg\" width=\"583\" /><div class=\"image-link-expand\"><div class=\"pencraft pc-display-flex pc-gap-8 pc-reset\"><button class=\"pencraft pc-reset pencraft icon-container restack-image\" tabindex=\"0\" type=\"button\"><svg fill=\"none\" height=\"20\" stroke=\"var(--color-fg-primary)\" stroke-linecap=\"round\" stroke-linejoin=\"round\" stroke-width=\"1.5\" viewBox=\"0 0 20 20\" width=\"20\" xmlns=\"http://www.w3.org/2000/svg\"><g><title></title><path d=\"M2.53001 7.81595C3.49179 4.73911 6.43281 2.5 9.91173 2.5C13.1684 2.5 15.9537 4.46214 17.0852 7.23684L17.6179 8.67647M17.6179 8.67647L18.5002 4.26471M17.6179 8.67647L13.6473 6.91176M17.4995 12.1841C16.5378 15.2609 13.5967 17.5 10.1178 17.5C6.86118 17.5 4.07589 15.5379 2.94432 12.7632L2.41165 11.3235M2.41165 11.3235L1.5293 15.7353M2.41165 11.3235L6.38224 13.0882\"></path></g></svg></button><button class=\"pencraft pc-reset pencraft icon-container view-image\" tabindex=\"0\" type=\"button\"><svg class=\"lucide lucide-maximize2 lucide-maximize-2\" fill=\"none\" height=\"20\" stroke=\"currentColor\" stroke-linecap=\"round\" stroke-linejoin=\"round\" stroke-width=\"2\" viewBox=\"0 0 24 24\" width=\"20\" xmlns=\"http://www.w3.org/2000/svg\"><polyline points=\"15 3 21 3 21 9\"></polyline><polyline points=\"9 21 3 21 3 15\"></polyline><line x1=\"21\" x2=\"14\" y1=\"3\" y2=\"10\"></line><line x1=\"3\" x2=\"10\" y1=\"21\" y2=\"14\"></line></svg></button></div></div></div></a><figcaption class=\"image-caption\">Cartoon via Nano Banana.</figcaption></figure></div><p>In this post, I wanted to focus on taste; how it changes the human role and questions around who owns it. </p><h2><strong>Where the baseline is now</strong></h2><p>Every domain has a threshold where AI crosses from &#8220;clearly worse&#8221; to &#8220;good enough to fool experts.&#8221; Those thresholds are falling fast as the models get better and organizations get better at providing the right context.</p><ul><li><p><strong>2024: </strong>AI could autocomplete but not architect. Copilot suggestions had a higher churn rate than human code. AI-generated marketing copy was formulaic and easy to spot. AI-generated playlists were novelties, not chart contenders. AI hiring meant keyword-matching resumes with well-documented bias. The consensus across every domain was the same: AI can handle the grunt work, but taste, judgment, and design are ours.</p></li><li><p><strong>2025:</strong> &#8220;Vibe coding&#8221; became mainstream. Consumers rated AI marketing copy better than professional copywriters. Music listeners couldn&#8217;t distinguish AI music from human music. Frontier models are matching human experts on nearly half of tasks across occupations, completing them 100x faster and 100x cheaper. In twelve months, &#8220;AI can&#8217;t do X&#8221; became &#8220;AI does X better than most people&#8221; in domain after domain.</p></li><li><p><strong>2026: </strong>The majority of US developers use AI coding tools daily. AI-generated music accounts for over a third of daily uploads on major platforms. The question is no longer whether AI can match human taste in a given domain, but how long until it does in yours.</p></li></ul><p>This is why I see taste as alpha, not a moat. My judgment is only valuable relative to what AI can do by default, and that default resets every few months.</p><p>If taste decays, the question stops being &#8220;do you have it?&#8221; and starts being &#8220;how fast can you get it into a system before the baseline catches up?&#8221;</p><div class=\"subscription-widget-wrap-editor\"><div class=\"subscription-widget show-subscribe\"><div class=\"preamble\"><p class=\"cta-caption\">Thanks for reading Shrivu&#8217;s Substack! Subscribe for free to receive new posts and support my work.</p></div><form class=\"subscription-widget-subscribe\"><input class=\"email-input\" name=\"email\" tabindex=\"-1\" type=\"email\" /><input class=\"button primary\" type=\"submit\" value=\"Subscribe\" /><div class=\"fake-input-wrapper\"><div class=\"fake-input\"></div><div class=\"fake-button\"></div></div></form></div></div><h2><strong>How taste is reshaping roles</strong></h2><p>Even as taste decays as a durable advantage, it&#8217;s becoming the primary thing organizations pay humans to do (on top of just having the agency to use AI to do their job)<a class=\"footnote-anchor\" href=\"https://blog.sshh.io/feed#footnote-3\" id=\"footnote-anchor-3\" target=\"_self\">3</a>.</p><p>Having taste matters just as much as being able to communicate it. Out of the box, many of us can&#8217;t fully articulate why we prefer one design over another, why this copy lands and that one doesn&#8217;t, why this architecture will scale and that one won&#8217;t. The knowledge is tacit, embodied in instinct rather than rules. However, jobs are shifting from executor to taste extractor: someone whose primary skill is getting their judgment into the system.</p><p>The best hire for an AI-native marketing team isn&#8217;t the person with the most original campaign ideas (despite their &#8216;taste moat&#8217;). It also isn&#8217;t the person with mediocre instincts who happens to be good at prompting ChatGPT. It&#8217;s the person with N years of pattern recognition about what converts, what falls flat, and why, who can turn that into something a system can use.</p><p>Their day-to-day looks nothing like a traditional marketer&#8217;s. They spend the morning reviewing AI-generated campaign variants, not writing them. They run A/B interviews against their own preferences to build an Essence document for the brand&#8217;s voice. They flag the three outputs out of twenty that feel subtly wrong and articulate what&#8217;s off: too eager, wrong register for the audience, buries the value prop. That articulation becomes a constraint the system applies to the next hundred outputs. By afternoon they&#8217;re not writing copy; they&#8217;re tuning the machine that writes copy.</p><p>It&#8217;s not &#8220;creativity&#8221; in the traditional sense and it&#8217;s not &#8220;prompt engineering&#8221; in the shallow sense. Taste extraction builds a persistent model of your judgment that compounds across every output after it. It&#8217;s the ability to encode experienced judgment into a system that scales it.</p><p>The pattern applies to plenty of taste-heavy roles:</p><ul><li><p><strong>Designers </strong>stop pushing pixels and start curating.</p></li><li><p><strong>Recruiters</strong> stop screening and start calibrating for &#8220;high potential&#8221;.</p></li><li><p><strong>Product managers</strong> stop writing specs and start steering batches of features.</p></li><li><p>&#8230;</p></li></ul><h2><strong>How to extract your taste (before someone else does)</strong></h2><p>Your taste can be a black box. You know what good looks like when you see it. You probably can&#8217;t explain why. Extraction is about surfacing that tacit judgment so a system can act on it and so you can leverage up on your productivity.</p><p>Without extraction, your outcomes with AI drift. Prompt the same task across sessions without a persistent reference document and the AI reconstructs from training data priors each time; the output flattens toward the sloppy median<a class=\"footnote-anchor\" href=\"https://blog.sshh.io/feed#footnote-4\" id=\"footnote-anchor-4\" target=\"_self\">4</a>. </p><p>There are a couple ways I&#8217;ve been experimenting with getting better at this. They apply across domains, not just code.</p><h3>A/B interviews</h3><p>Have AI present you with paired options and ask which you prefer and why. Your explanations, especially the ones you struggle to articulate, are the signal. After 10-15 rounds, the model synthesizes your preferences into a document it references for future work.</p><blockquote><p>Given this [style guide / doc / brief], interview me using A/B examples to help refine the [tone / style / design] guidance. Present me with two options for a [headline / paragraph / layout / architecture]. First, critique both options. Then ask me which I prefer and why. Use my answers to write a 1-page &#8220;Essence&#8221; document summarizing my taste in this domain.</p></blockquote><p>The Essence document becomes a reusable asset. I&#8217;ve done this for writing style, UI design preferences, and code architecture patterns. I now run this process for every new domain before generating anything significant.</p><h3>Ghost writing</h3><p>Increasingly every remaining task I do by hand, I also run AI on the same task in the background with minimal context. If I&#8217;m writing a blog post, I have AI write one too before I start. If I&#8217;m designing an architecture, I have AI propose one in parallel. Then I compare.</p><blockquote><p>Here is the [requirements doc / ticket / brief]. Propose a full [architecture / system design / data model]. Include your reasoning for each major decision. </p></blockquote><p>The value isn&#8217;t in which draft is better. It&#8217;s in where I flinch. The moments where the AI output feels off, where the slop is obvious to me but hard to name, those reactions are taste made visible. And the places where I&#8217;m contributing something the model didn&#8217;t, those are my actual alpha.</p><p>Over time, the flinch points become encodable. I note them down, turn them into constraints, and feed them back into the system. This does two things at once: it helps me identify where my taste actually lives, and it gives me the language to express preferences I previously couldn&#8217;t articulate. The gap between my draft and the AI&#8217;s draft shrinks, but my ability to direct the output sharpens because I&#8217;m forced to make the tacit explicit.</p><h3>External reviews</h3><p>Your own taste has blind spots. One way I augment mine is by extracting the approximate taste of other people and running it through AI.</p><p>In practice, this means using tasks (<a href=\"https://blog.sshh.io/i/177742847/custom-subagents\">aka dynamic subagents</a>) in Claude Code that represent specific perspectives<a class=\"footnote-anchor\" href=\"https://blog.sshh.io/feed#footnote-5\" id=\"footnote-anchor-5\" target=\"_self\">5</a>:</p><ul><li><p>An audience segment I&#8217;m writing for. </p></li><li><p>A manager or coworker whose judgment I trust. </p></li><li><p>A writer or brand whose voice I admire. </p></li></ul><p>I feed them real context: Zoom transcripts from calls, written feedback I&#8217;ve received, published work I want to learn from. Then I ask each agent to independently review whatever I&#8217;m building or writing.</p><blockquote><p>Use a task to read the attached [transcript / feedback thread / writing samples] from [person or role]. Extract their values, preferences, and recurring critiques. Then review this [draft / design / architecture] as if you were that person. Flag what they&#8217;d push back on, what they&#8217;d approve of, and what they&#8217;d want to see more of.</p></blockquote><div class=\"captioned-image-container\"><figure><a class=\"image-link image2\" href=\"https://substackcdn.com/image/fetch/$s_!Gq2T!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fae7e2652-ec89-41d7-a5de-32b1cde5e542_1002x400.png\" target=\"_blank\"><div class=\"image2-inset\"><source type=\"image/webp\" /><img alt=\"\" class=\"sizing-normal\" height=\"231.53692614770458\" src=\"https://substackcdn.com/image/fetch/$s_!Gq2T!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fae7e2652-ec89-41d7-a5de-32b1cde5e542_1002x400.png\" width=\"580\" /><div></div></div></a><figcaption class=\"image-caption\"> A screenshot of Claude Code tasks reviewing this post. In a very meta way, it actually suggested adding this image to illustrate.</figcaption></figure></div><p>It&#8217;s not a replacement for real feedback, but it catches the obvious misses before I ask for it. And it compounds: every round of external review surfaces blind spots in my own taste that I can then fold into my Essence documents and constraints. This also means the real feedback I get back is higher-signal too.</p><h2>When platforms farm taste at scale</h2><p>Everything above is about extracting your taste to stay ahead. But there&#8217;s a catch: you&#8217;re not the only one extracting.</p><p>TikTok doesn&#8217;t need any individual user to have great taste. It collects millions of low-signal interactions (swipes, watch time, replays, skips) and synthesizes something that functions like taste at industrial scale. No single swipe is valuable. In aggregate, those micro-signals train a recommendation system that N billion people spend hours inside daily. YouTube, Spotify, Instagram, Netflix: every app with an algorithmic feed is essentially a taste factory.</p><p>The factory doesn&#8217;t just curate what humans make; it increasingly curates what AI makes, selecting for whatever the aggregate says &#8220;good&#8221; looks like. The extraction workflow that empowers you at the role level simultaneously trains these platforms. Your prompts, preferences, and clicks all teach systems that then compete with your own judgment. And this doesn&#8217;t require anyone to train directly on your data. It happens indirectly: your taste-informed outputs perform well, get clicked, shared, imitated, and that performance signal feeds back into the next generation of models and recommendation systems. The platform learns what &#8220;good&#8221; looks like from millions of people, then serves it back at scale without needing any of them individually.</p><p>In the extreme, platforms could become the primary owners of taste, not individuals. Our role shifts from having taste to feeding it. The system doesn&#8217;t need to match your judgment on any specific decision; it just needs enough signal from enough people to converge on something the market accepts. The &#8220;platform&#8221; here might not even be the feeds, but could be the labs and token producers.</p><h2>Open questions</h2><ul><li><p><strong>Can taste be taught? </strong>If it develops through some opaque function of experience and exposure, what happens to people who haven&#8217;t had the right experiences? Dario warned about AI &#8220;slicing by cognitive ability,&#8221; creating stratification based on traits harder to change than specific skills<a class=\"footnote-anchor\" href=\"https://blog.sshh.io/feed#footnote-6\" id=\"footnote-anchor-6\" target=\"_self\">6</a>. Taste is a version of that divide.</p></li><li><p><strong>Do taste roles mean more hiring or less?</strong> Right now, the pattern is fewer people with more leverage: marketing teams shrink, one designer steers what five used to produce. But if the job is encoding judgment, don&#8217;t you actually want more people sourcing taste from more angles? A single extractor&#8217;s blind spots become the system&#8217;s blind spots.</p></li><li><p><strong>Who wins the taste race: individuals or platforms? </strong>Every extraction technique in this post works in both directions. You encode your judgment to scale your leverage; the platform collects that same signal to scale without you. If the platform can interview me better than I can articulate myself, farm preferences from millions of users simultaneously, and apply the aggregate at near-zero cost, does individual taste become a contribution to someone else&#8217;s moat?</p></li><li><p><strong>If so, will people accept taste farming as work?</strong> If platforms need human micro-signals to train their systems, does &#8220;pay per swipe&#8221; become a job category? If AI results in far fewer jobs, is this the remaining option? Who reaps the majority value of individual taste at that point? </p></li></ul><p>The people calling taste a moat are right that it matters. They&#8217;re wrong that it&#8217;s yours to keep. The more I practice articulating my own taste, the less sure I become that it&#8217;s durable.</p><div class=\"subscription-widget-wrap-editor\"><div class=\"subscription-widget show-subscribe\"><div class=\"preamble\"><p class=\"cta-caption\">Thanks for reading Shrivu&#8217;s Substack! Subscribe for free to receive new posts and support my work.</p></div><form class=\"subscription-widget-subscribe\"><input class=\"email-input\" name=\"email\" tabindex=\"-1\" type=\"email\" /><input class=\"button primary\" type=\"submit\" value=\"Subscribe\" /><div class=\"fake-input-wrapper\"><div class=\"fake-input\"></div><div class=\"fake-button\"></div></div></form></div></div><p class=\"button-wrapper\"><a class=\"button primary\" href=\"https://blog.sshh.io/p/taste-is-not-a-moat?utm_source=substack&amp;utm_medium=email&amp;utm_content=share&amp;action=share\"><span>Share</span></a></p><div class=\"footnote\"><a class=\"footnote-number\" contenteditable=\"false\" href=\"https://blog.sshh.io/feed#footnote-anchor-1\" id=\"footnote-1\" target=\"_self\">1</a><div class=\"footnote-content\"><div class=\"twitter-embed\"></div><div class=\"twitter-embed\"></div><div class=\"twitter-embed\"></div></div></div><div class=\"footnote\"><a class=\"footnote-number\" contenteditable=\"false\" href=\"https://blog.sshh.io/feed#footnote-anchor-2\" id=\"footnote-2\" target=\"_self\">2</a><div class=\"footnote-content\"><p>A moat, in business strategy, is a structural advantage that&#8217;s hard to replicate: a patent, a network effect, a regulatory lock-in. You build it once and competitors can&#8217;t easily cross it. Alpha, in finance, is the return you earn above what the market gives you for free, the gap between a hedge fund&#8217;s performance and a simple index fund. The key difference: a moat persists by design, but alpha decays. The more people discover the same strategy, the more the market absorbs it, and the edge shrinks. Taste behaves like alpha, not a moat. Your judgment is only valuable relative to what AI produces by default, and that default gets better on its own.</p></div></div><div class=\"footnote\"><a class=\"footnote-number\" contenteditable=\"false\" href=\"https://blog.sshh.io/feed#footnote-anchor-3\" id=\"footnote-3\" target=\"_self\">3</a><div class=\"footnote-content\"><p>I say this from the perspective of someone who works for a fairly AI-native organization  and spends time with a lot of people who are in the SF AI bubble. I recognize that there&#8217;s still quite a few jobs out there that are now trivially done by AI but are still being done by humans as AI transformation takes time to diffuse to the rest of the world.</p></div></div><div class=\"footnote\"><a class=\"footnote-number\" contenteditable=\"false\" href=\"https://blog.sshh.io/feed#footnote-anchor-4\" id=\"footnote-4\" target=\"_self\">4</a><div class=\"footnote-content\"><p>Two things happen at once. First, without externalized standards, AI outputs regress to the distribution it was trained on. Each new session starts from that distribution&#8217;s center, not from where you left off, so quality flattens toward the median. Second, that median itself keeps rising. The taste you encode today, your Essence documents, your constraints, your feedback, eventually gets absorbed into training data for the next generation of models. What was your alpha becomes the new default. This doesn&#8217;t even require the model to train on your prompts or data directly; it happens indirectly through how your taste-informed outputs perform in the world, what gets clicked, shared, purchased, and imitated, which all feed back into the next training distribution. This is why taste behaves like alpha: your edge above the median is real but temporary, because the median is a moving target that absorbs the signal you feed it.</p></div></div><div class=\"footnote\"><a class=\"footnote-number\" contenteditable=\"false\" href=\"https://blog.sshh.io/feed#footnote-anchor-5\" id=\"footnote-5\" target=\"_self\">5</a><div class=\"footnote-content\"><p>It&#8217;s key that you use sub-agents or tasks for this workflow because you want a fresh, unbiased session/context-window to review the work.</p></div></div><div class=\"footnote\"><a class=\"footnote-number\" contenteditable=\"false\" href=\"https://blog.sshh.io/feed#footnote-anchor-6\" id=\"footnote-6\" target=\"_self\">6</a><div class=\"footnote-content\"><p>See <a href=\"https://www.darioamodei.com/essay/the-adolescence-of-technology\">The Adolescence of Technology</a>.</p></div></div>"
            ],
            "link": "https://blog.sshh.io/p/taste-is-not-a-moat",
            "publishedAt": "2026-02-16",
            "source": "Shrivu Shankar",
            "summary": "<p>I used to write the design while AI coded the software. Architecture decisions, API contracts, data models on my side; implementation on its side. Fast forward a few months and more and more the model now handles both, because it&#8217;s more right than wrong. The &#8220;outer loop&#8221; of system design, the thing that engineers would do while AI wrote the the code, turned out to be just another task it would absorb.</p><p>Over the past year, Tech Twitter has mostly converged on the same answer to this problem<a class=\"footnote-anchor\" href=\"https://blog.sshh.io/feed#footnote-1\" id=\"footnote-anchor-1\" target=\"_self\">1</a>. As AI eats execution, taste is the moat.</p><p>And while they&#8217;re right about taste mattering. I&#8217;m less sure about the moat part. A moat is something you build once and defend. Taste feels more like alpha<a class=\"footnote-anchor\" href=\"https://blog.sshh.io/feed#footnote-2\" id=\"footnote-anchor-2\" target=\"_self\">2</a>: a decaying edge, only valuable relative to a rising baseline. And that baseline is rising faster than many realize.</p><div class=\"captioned-image-container\"><figure><a class=\"image-link image2 is-viewable-img\" href=\"https://substackcdn.com/image/fetch/$s_!RgDA!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F98d057ae-0f80-4493-b587-c9b4693ac10d_1024x687.jpeg\" target=\"_blank\"><div class=\"image2-inset\"><source type=\"image/webp\" /><img alt=\"\" class=\"sizing-normal\" height=\"391.1337890625\" src=\"https://substackcdn.com/image/fetch/$s_!RgDA!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F98d057ae-0f80-4493-b587-c9b4693ac10d_1024x687.jpeg\" width=\"583\" /><div class=\"image-link-expand\"><div class=\"pencraft pc-display-flex pc-gap-8 pc-reset\"><button class=\"pencraft pc-reset pencraft icon-container restack-image\" tabindex=\"0\" type=\"button\"><svg fill=\"none\" height=\"20\" stroke=\"var(--color-fg-primary)\" stroke-linecap=\"round\" stroke-linejoin=\"round\" stroke-width=\"1.5\" viewBox=\"0 0 20 20\" width=\"20\" xmlns=\"http://www.w3.org/2000/svg\"><g><title></title><path d=\"M2.53001 7.81595C3.49179 4.73911 6.43281 2.5 9.91173 2.5C13.1684 2.5 15.9537 4.46214 17.0852 7.23684L17.6179",
            "title": "Taste Is Not a Moat"
        },
        {
            "content": [
                "<p>This is the weekly visible open thread. Post about anything you want, ask random questions, whatever. ACX has an unofficial <a href=\"https://www.reddit.com/r/slatestarcodex/\">subreddit</a>, <a href=\"https://discord.gg/RTKtdut\">Discord</a>, and <a href=\"https://www.datasecretslox.com/index.php\">bulletin board</a>, and <a href=\"https://www.lesswrong.com/community?filters%5B0%5D=SSC\">in-person meetups around the world</a>. Most content is free, some is subscriber only; you can subscribe <strong><a href=\"https://astralcodexten.substack.com/subscribe\">here</a></strong>.</p>"
            ],
            "link": "https://www.astralcodexten.com/p/open-thread-421",
            "publishedAt": "2026-02-16",
            "source": "SlateStarCodex",
            "summary": "<p>This is the weekly visible open thread. Post about anything you want, ask random questions, whatever. ACX has an unofficial <a href=\"https://www.reddit.com/r/slatestarcodex/\">subreddit</a>, <a href=\"https://discord.gg/RTKtdut\">Discord</a>, and <a href=\"https://www.datasecretslox.com/index.php\">bulletin board</a>, and <a href=\"https://www.lesswrong.com/community?filters%5B0%5D=SSC\">in-person meetups around the world</a>. Most content is free, some is subscriber only; you can subscribe <strong><a href=\"https://astralcodexten.substack.com/subscribe\">here</a></strong>.</p>",
            "title": "Open Thread 421"
        },
        {
            "content": [
                "<p>Some podcasts are self-recommending on the \u2018yep, I\u2019m going to be breaking this one down\u2019 level. This was very clearly one of those. So here we go.</p>\n<div>\n<figure>\n<div>\n\n\n<figure class=\"wp-block-image\"><img alt=\"\" src=\"https://substackcdn.com/image/fetch/$s_!ZU_a!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1d49859d-6b3f-4c02-8b72-0f5697368baf_1418x837.png\" /></figure>\n\n\n<div></div>\n</div>\n</figure>\n</div>\n<p>As usual for podcast posts, the baseline bullet points describe key points made, and then the nested statements are my commentary. Some points are dropped.</p>\n<p><a href=\"https://www.dwarkesh.com/p/dario-amodei-2\">If I am quoting directly I use quote marks, otherwise assume paraphrases</a>.</p>\n<p>What are the main takeaways?</p>\n<ol>\n<li>Dario mostly stands by his predictions of extremely rapid advances in AI capabilities, both in coding and in general, and in expecting the \u2018geniuses in a data center\u2019 to show up within a few years, possibly even this year.\n<div>\n\n\n<span id=\"more-25100\"></span>\n\n\n</div>\n</li>\n<li>Anthropic\u2019s actions do not seem to fully reflect this optimism, but also when things are growing on a 10x per year exponential if you overextend you die, so being somewhat conservative with investment is necessary unless you are prepared to fully burn your boats.</li>\n<li>Dario reiterated his stances on China, export controls, democracy, AI policy.</li>\n<li>The interview downplayed catastrophic and existential risk, including relative to other risks, although it was mentioned and Dario remains concerned. There was essentially no talk about alignment at all. The dog did not bark in the nighttime.</li>\n<li>Dwarkesh remains remarkably obsessed with continual learning.</li>\n</ol>\n\n\n<h4 class=\"wp-block-heading\">Table of Contents</h4>\n\n\n<ol>\n<li><a href=\"https://thezvi.substack.com/i/188060213/the-pace-of-progress\">The Pace of Progress.</a></li>\n<li><a href=\"https://thezvi.substack.com/i/188060213/continual-learning\">Continual Learning.</a></li>\n<li><a href=\"https://thezvi.substack.com/i/188060213/does-not-compute\">Does Not Compute.</a></li>\n<li><a href=\"https://thezvi.substack.com/i/188060213/step-two\">Step Two.</a></li>\n<li><a href=\"https://thezvi.substack.com/i/188060213/the-quest-for-sane-regulations\">The Quest For Sane Regulations.</a></li>\n<li><a href=\"https://thezvi.substack.com/i/188060213/beating-china\">Beating China.</a></li>\n</ol>\n\n\n<h4 class=\"wp-block-heading\">The Pace of Progress</h4>\n\n\n<ol>\n<li>AI progress is going at roughly Dario\u2019s expected pace plus or minus a year or two, except coding is going faster than expected. His top level model of scaling is the same as it was in 2017.\n<ol>\n<li>I don\u2019t think this is a retcon, but he did previously update too aggressively on coding progress, or at least on coding diffusion.</li>\n</ol>\n</li>\n<li>Dario still believes the same seven things matter: Compute, data, data quality and distribution, length of training, an objective function that scales, and two things around normalization or conditioning.\n<ol>\n<li>I assume this is \u2018matters for raw capability.\u2019</li>\n</ol>\n</li>\n<li>Dwarkesh asks about Sutton\u2019s perspective that we\u2019ll get human-style learners. Dario says there\u2019s an interesting puzzle there, but it probably doesn\u2019t matter. LLMs are blank slates in ways humans aren\u2019t. In-context learning will be in-between human short and long term learning. Dwarkesh asks then why all of this RL and building RL environments? Why not focus on learning on the fly?\n<ol>\n<li>Because the RL and giving it more data clearly works?</li>\n<li>Whereas learning on the fly doesn\u2019t work, even if it did what happens when the model resets every two months?</li>\n<li>Dwarkesh has pushed on this many times and is doing so again.</li>\n</ol>\n</li>\n<li>Timeline time. Why does Dario think we are at \u2018the end of the exponential\u2019 rather than ten years away? Dario says his famous \u2018country of genuines in a data center\u2019 is 90% within 10 years without biting a bullet on faster. One concern is needing verification. Dwarkesh pushes that this means the models aren\u2019t general, Dario says no we see plenty of generalization, but the world where we don\u2019t get the geniuses is still a world where we can do all the verifiable things.\n<ol>\n<li>As always, notice the goalposts. Ten years from human-level AI is \u2018long time.\u2019</li>\n<li>Dario is mostly right on generalization, in that you need verification to train in distribution but then things often work well (albeit less well) out of distribution.</li>\n<li>The class of verifiable things is larger than one might think, if you include all necessary subcomponents of those tasks and then the combination of those subcomponents.</li>\n</ol>\n</li>\n<li>Dwarkesh challenges if you could automate an SWE without generalization outside verifiable domains, Dario says yes you can, you just can\u2019t verify the whole company.\n<ol>\n<li>I\u2019m 90% with Dario here.</li>\n</ol>\n</li>\n<li>What\u2019s the metric of AI in SWE? Dario addresses his predictions of AI writing 90% of the lines of code in 3-6 months. He says it happened at Anthropic, and that \u2018100% of today\u2019s SWE tasks are done by the models,\u2019 but that\u2019s all not yet true overall, and says people were reading too much into the prediction.\n<ol>\n<li>The prediction was still clearly wrong.</li>\n<li>A lot of that was Dario overestimating diffusion at this stage.</li>\n<li>I do agree that the prediction was \u2018less wrong,\u2019 or more right, than those who predicted a lack of big things for AI coding, who thoughts things would not escalate quickly.</li>\n<li>Dario could have reliably looked great if he\u2019d made a less bold prediction. There\u2019s rarely reputational alpha in going way beyond others. If everyone else says 5 years, and you think 3-6 months, you can say 2 years and then if it happens in 3-6 months you still look wicked smart. Whereas the super fast predictions don\u2019t sound credible and can end up wrong. Predicting 3-6 months here only happens if you\u2019re committed to a kind of epistemic honesty.</li>\n<li>I agree with Dario that going from 90% of code to 100% of code written by AI is a big productivity unlock, Dario\u2019s prediction on this has already been confirmed by events. This is standard Bottleneck Theory.</li>\n</ol>\n</li>\n<li>\u201cEven when that happens, it doesn\u2019t mean software engineers are out of a job. There are new higher-level things they can do, where they can manage. Then further down the spectrum, there\u2019s 90% less demand for SWEs, which I think will happen but this is a spectrum.\u201d\n<ol>\n<li>It would take quite a lot of improved productivity to reduce demand by 90%.</li>\n<li>I\u2019d go so far as to say that if we reduce SWE demand by 90%, then we have what one likes to call \u2018much bigger problems.\u2019</li>\n</ol>\n</li>\n<li>Anthropic went from zero ARR to $100 million in 2023, to $1 billion in 2024, to $9-$10 billion in 2025, and added a few more billion in January 2026. He guesses the 10x per year starts to level off some time in 2026, although he\u2019s trying to speed it up further. Adoption is fast, but not infinitely fast.\n<ol>\n<li>Dario\u2019s predictions on speed of automating coding were unique, in that all the revenue predictions for OpenAI and Anthropic have consistently come in too low, and I think the projections are intentional lowballs to ensure they beat the projections and because the normies would never believe the real number.</li>\n</ol>\n</li>\n<li>Dwarkesh pulls out the self-identified hot take that \u2018diffusion is cope\u2019 used to justify when models can\u2019t do something. Hiring humans is much more of a hassle than onboarding an AI. Dario says you still have to do a lot of selling in several stages, the procurement processes are often shortcutted but still take time, and even geniuses in a datacenter will not be \u2018infinitely\u2019 compelling as a product.\n<ol>\n<li>I\u2019ve basically never disagreed with a Dwarkesh take as much as I do here.</li>\n<li>Yes, of course diffusion is a huge barrier.</li>\n<li>The fact that if the humans knew to set things up, and how to set things up, that the cost of deployment and diffusion would be low? True, but completely irrelevant.</li>\n<li>The main barrier to Claude Code is not that it\u2019s hard to install, it\u2019s that it\u2019s hard to get people to take the plunge and install it, as Dario notes.</li>\n<li>In practice, very obviously, even the best of us miss out on a lot of what LLMs can do for us, and most people barely scratch the surface at best.</li>\n<li>A simple intuition pump: If diffusion is cope, what do you expect to happen if there was an \u2018AI pause\u2019 starting right now, and no new frontier models were ever created?</li>\n<li>Dwarkesh sort of tries to backtrack on what he said as purely asserting that we\u2019re not currently at AGI, but that\u2019s an entirely different claim?</li>\n</ol>\n</li>\n<li>Dario says we\u2019re not at AGI, and that if we did have a \u2018country of geniuses in a datacenter\u2019 then everyone would know this.\n<ol>\n<li>I think it\u2019s possible that we might not know, in the sense that they might be sufficiently both capable and misaligned to disguise this fact, in which case we would be pretty much what we technically call \u2018toast.\u2019</li>\n<li>I also think it is very possible in the future that an AI lab might get the geniuses and then disguise this fact from the rest of us, and not release the geniuses directly, for various reasons.</li>\n<li>Barring those scenarios? Yes, we would know.</li>\n</ol>\n</li>\n</ol>\n\n\n<h4 class=\"wp-block-heading\">Continual Learning</h4>\n\n\n<p>It\u2019s a Dwarkesh Patel AI podcast, so it\u2019s time for continual learning in two senses.</p>\n<ol>\n<li>Dwarkesh thinks Dario\u2019s prediction for today, from three years ago, of \u201cWe should expect systems which, if you talk to them for the course of an hour, it\u2019s hard to tell them apart from a generally well-educated human\u201d was basically accurate. Dwarkesh however is spiritually unsatisfied because that system can\u2019t automated large parts of white-collar work. Dario points out OSWorld scores are already at 65%-70% up from 15% a year ago, and computer use will improve.\n<ol>\n<li>I think it is very easy to tell, but I think the \u2018spirit of the question\u2019 is not so off, in the sense that on most topics I can have \u2018at least as good\u2019 a conversation with the LLM for an hour as with the well-educated human.</li>\n<li>Can such a system automate large parts of white-collar work? Yes. Very obviously yes, if we think in terms of tasks rather than full jobs. If you gave us ten years (as an intuition pump) to adapt to existing systems, then I would predict a majority of current white-collar digital job tasks get automated.</li>\n<li>The main current barrier to the next wave of practical task automation is that computer use is still not so good (as Dario says), but that will get fixed.</li>\n</ol>\n</li>\n<li>Dwarkesh asks about the job of video editor. He says they need six months of experience to understand the trade-offs and preferences and tastes necessary for the job and asks when AI systems will have that. Dario says the \u2018country of geniuses in a datacenter\u2019 can do that.\n<ol>\n<li>I bet that if you took Claude Opus 4.6 and Claude Code, and you gave it the same amount of human attention to improving its understanding of trade-offs, preferences and taste over six months that a new video editor would have, and a similar amount of time training video editing skills, that you could get this to the point where it could do most of the job tasks.</li>\n<li>You\u2019d have to be building up copious notes and understandings of the preferences and considerations, and you\u2019d need for now some amount of continual human supervision and input, but yeah, sure, why not.</li>\n<li>Except that by the time you were done you\u2019d use Opus 5.1, but same idea.</li>\n</ol>\n</li>\n<li>Dwarkesh says he still has to have humans do various text-to-text tasks, and LLMs have proved unable to do them, for example on \u2018identify what the best clips would be in this transcript\u2019 they can only do a 7/10 job.\n<ol>\n<li>If you see the LLMs already doing a 7/10 job, the logical conclusion is that this will be 9/10 reasonably soon especially if you devote effort to it.</li>\n<li>There are a lot of things one could try here, and my guess is that Dwarkesh has mostly not tried them, largely because until recently trying them was a lot slower and more expensive than it is now.</li>\n</ol>\n</li>\n<li>Dwarkesh asks if a lot of LLM coding ability is the codebase as massive notes. Dario points out this is not an accounting of what a human needs to know, and the model is much faster than humans at understanding the code base.\n<ol>\n<li>I think the metaphor is reasonably apt, in that in code the humans or prior AIs have written things down, and in other places we haven\u2019t written similar things down. You could fix that, including over time.</li>\n</ol>\n</li>\n<li>Dwarkesh cites the \u2018<a href=\"https://metr.org/blog/2025-07-10-early-2025-ai-experienced-os-dev-study/\">the developers using LLMs thought they were faster but were went slower</a>\u2019 study and asks where the renaissance of software and productivity benefits are from AI coding. Dario says it\u2019s unmistakable within Anthropic, and cites that they\u2019ve cut their competitors off from using Claude.\n<ol>\n<li>Not letting OpenAI use Claude is a big costly signal that they view agentic coding as a big productivity boost, and even that theirs is a big boost over OpenAI\u2019s versions of the same tools.</li>\n<li>It seems very difficult, watching the pace of developments in AI inside and outside of the frontier labs, to think coding productivity isn\u2019t accelerating.</li>\n</ol>\n</li>\n<li>Dario estimates current coding models give 15%-20% speedup, versus 5% six months ago, and that Amdhal\u2019s law means you eventually get a much bigger speedup once you start closing full loops.\n<ol>\n<li>It\u2019s against his interests to come up with a number that small.</li>\n<li>I also don\u2019t believe a number that small, especially since the pace of coding now seems to be largely rate limited by compute and frequency of human interruptions to parallel agents. It\u2019s very hard to thread the needle and have the gains be this small.</li>\n<li>The answer will vary a lot. I can observe that for me, given my particular set of skills, the speedup is north of 500%. I\u2019m vastly faster and better.</li>\n</ol>\n</li>\n<li>Dwarkesh asks again \u2018continual learning when?\u2019 and Dario says he has ideas.\n<ol>\n<li>There are cathedrals for those with eyes to see.</li>\n</ol>\n</li>\n</ol>\n\n\n<h4 class=\"wp-block-heading\">Does Not Compute</h4>\n\n\n<ol>\n<li>How does Dario reconcile his general views on progress with his radically fast predictions on capabilities? Fast but finite diffusion, especially economic. Curing diseases might take years.\n<ol>\n<li>Diffusion is real but Dario\u2019s answer to this, which hasn\u2019t changed, has never worked for me. His predictions on impact do not square with his predictions on capabilities, period, and it is not a small difference.</li>\n</ol>\n</li>\n<li>Why not buy the biggest data center you can get? If Anthropic managed to buy enough compute for their anticipated demand, they burn the boats. That\u2019s on the order of $5 trillion dollars two years from now. If the revenue does not materialize, they\u2019re toast. Whereas Anthropic can ensure financial stability and profitability by not going nuts, as their focus is enterprise revenue with higher margins and reliability.\n<ol>\n<li>Being early in this sense, when things keep going 10x YoY, is fatal.</li>\n<li>That\u2019s not strictly true. You\u2019re only toast if you can\u2019t resell the compute at the same or a better price. But yes, you\u2019re burning the boats if conditions change.</li>\n<li>Even if you did want to burn the boats, it doesn\u2019t mean the market will let you burn the boats. The compute is not obviously for sale, nor is Anthropic\u2019s credit good for it, nor would the investors be okay with this.</li>\n<li>This does mean that Anthropic is some combination of insufficiently confident to burn the boats or unable to burn them.</li>\n</ol>\n</li>\n<li>Dario won\u2019t give exact numbers, but he\u2019s predicting more than 3x to Anthropic compute each year going forward.</li>\n</ol>\n\n\n<h4 class=\"wp-block-heading\">Step Two</h4>\n\n\n<ol>\n<li>Why is Anthropic planning on turning a profit in 2028 instead of reinvesting? \u201cI actually think profitability happens when you underestimated the amount of demand you were going to get and loss happens when you overestimated the amount of demand you were going to get, because you\u2019re buying the data centers ahead of time.\u201d He says they could potentially even be profitable in 2026.\n<ol>\n<li>Thus, the theory is that Anthropic needs to underestimate demand because it is death to overestimate demand, which means you probably turn a profit \u2018in spite of yourself.\u2019 That\u2019s so weird, but it kind of makes sense.</li>\n<li>Dario denies this is Anthropic \u2018systematically underinvesting in compute\u2019 but that depends on your point of view. You\u2019re underinvesting post-hoc with hindsight. That doesn\u2019t mean it was a mistake over possible worlds, but I do think that it counts as underinvesting for these purposes.</li>\n<li>Also, Dario is saying (in the toy model) you split compute 50/50 internal use versus sales. You don\u2019t have to do that. You could double the buy, split it 75/25 and plan on taking a loss and funding the loss by raising capital, if you wanted that.</li>\n</ol>\n</li>\n<li>Dwarkesh suggests exactly doing an uneven split, Dario says there are log returns to scale, diminishing returns after spending e.g. $50 billion a year, so it probably doesn\u2019t help you that much.\n<ol>\n<li>I basically don\u2019t buy this argument. I buy the diminishing return but it seems like if you actually believed Anthropic\u2019s projections you wouldn\u2019t care. As Dwarkesh says \u2018diminishing returns on a genius could be quite high.\u2019</li>\n<li>If you actually did have a genius in your datacenters, I\u2019d expect there to be lots of profitable ways to use that marginal genius. The world is your oyster.</li>\n<li>And that\u2019s if you don\u2019t get into an AI 2027 or other endgame scenario.</li>\n</ol>\n</li>\n<li>Dario says AI companies need revenue to raise money and buy more compute.\n<ol>\n<li>In practice I think Dario is right. You need customers to prove your value and business model sufficiently to raise money.</li>\n<li>However, I think the theory here is underdeveloped. There is no reason why you couldn\u2019t keep raising at higher valuations without a product. Indeed, see Safe Superintelligence, and see Thinking Machines before they lost a bunch of people, and so on, as Matt Levine often points out. It\u2019s better to be a market leader, but the no product, all research path is very viable.</li>\n<li>The other advantage of having a popular product is gaining voice.</li>\n</ol>\n</li>\n<li>Dwarkesh claims Dario\u2019s view is compatible with us being 10 years away from AI generating trillions in value. Dario says it might take 3-4 years at most, he\u2019s very confident in the \u2018geniuses\u2019 showing up by 2028.\n<ol>\n<li>Dario feels overconfident here, and also more confident than his business decisions reflect. If he\u2019s that confident he\u2019s not burning enough boats.</li>\n</ol>\n</li>\n<li>Dario predicts a Cournot equilibrium, with a small number of relevant firms, which means there will be economic profits to be captured. He points out that gross margins are currently very positive, and the reason AI companies are taking losses is that each model turns a profit but you\u2019re investing in the model that costs [10*X] while collecting the profits from the model that costs [X]. At some point the compute stops multiplying by 10 each cycle and then you notice that you were turning a profit the whole time, the economy is going to grow faster but that\u2019s like 10%-20% fast, not 300% a year fast.\n<ol>\n<li>I don\u2019t understand what is confusing Dwarkesh here. I do get that this is confusing to many but it shouldn\u2019t confuse Dwarkesh.</li>\n<li>Of course if we do start seeing triple-digit economic growth, things get weird, and also we should strongly suspect we will all soon die or lose control, but in the meantime there\u2019ll be some great companies and I wouldn\u2019t worry about Anthropic\u2019s business model while that is happening.</li>\n</ol>\n</li>\n<li>Dario says he feels like he\u2019s in an economics class.\n<ol>\n<li>Honestly it did feel like that. This is the first time in a long time it felt like Dwarkesh flat out was not prepared on a key issue, and is getting unintentionally taken to school (as opposed to when someone like Sarah Paine is taking us to school, but by design.)</li>\n</ol>\n</li>\n<li>Dario predicts an oligopoly, not a monopoly, because of lack of network effects combined with high fixed costs, similar to cloud providers.\n<ol>\n<li>This is a bet on there not being win-more or runaway effects.</li>\n<li>For a while, the battle had catch-up mechanics rather than runaway effects. If you were behind, you can distill and you can copy ideas, so it\u2019s hard to maintain much of a lead.</li>\n<li>This feels like it is starting to change as RSI sets in. Claude is built by Claude Code, Codex is built by Codex, Google has to make its own choices and so on. The models are in many ways charged with training their successors.</li>\n<li>Also the cycle may be speeding up a la AI 2027. If you\u2019re six months behind that used to be one generation behind. Now it is three.</li>\n<li>And of course, once sufficiently powerful RSI (recursive self-improvement) sets in, and the models become sufficiently capable, that edge starts to translate into various other advantages far more readily.</li>\n<li>Many fates are possible, but don\u2019t rule out monopoly or winner-take-most.</li>\n</ol>\n</li>\n<li>Dario points out different models have different comparative advantages, often in subtle ways.\n<ol>\n<li>True, but a sufficient lead would apply across the board. We\u2019re not there right now, but we\u2019re not that far from it either.</li>\n</ol>\n</li>\n<li>Dario worried Silicon Valley and those connected to it could grow at 50% while everyone else grows at not much above the normal 2%. He says that would be \u2018a pretty messed up world.\u2019\n<ol>\n<li>I think that turns out fine. You tax the part growing at 50%, everyone wins.</li>\n<li>That\u2019s distinct from issues about the AI taking over, or the people in charge of the AI taking over, you still do have to dodge those problems. But if well-meaning humans are in control I don\u2019t worry about distributional issues under extreme growth.</li>\n</ol>\n</li>\n<li>Will robotics get solved soon after we get the \u2018geniuses\u2019? Dario says it doesn\u2019t depend on learning like a human, there are many options, and it will happen, we will learn to control robots, and yes the robotics industry will then make trillions. It tacks on maybe a year or two to get going.\n<ol>\n<li>This seems obviously correct if you understand the premise, that we have the geniuses and the geniuses are playing nice for whatever reason.</li>\n<li>That premise is not obvious.</li>\n</ol>\n</li>\n<li>Dwarkesh Patel keeps talking about continual learning, Dario Amodei keeps saying that we don\u2019t need it.\n<ol>\n<li>I agree with Dario. We don\u2019t need it as such, if nothing else we can easily solve such problems already via [CENSORED].</li>\n</ol>\n</li>\n<li>How should we price AGI? Dario thinks API is durable and will exist alongside other options, including forms of \u2018pay for results.\u2019</li>\n<li>How did Anthropic end up being the ones to build Claude Code? Dario encouraged experimentation internally, they used it internally, and then Dario said they should launch it externally.</li>\n</ol>\n\n\n<h4 class=\"wp-block-heading\">The Quest For Sane Regulations</h4>\n\n\n<p>Finally, we ask about making AI \u2018go well.\u2019 With that framing you know that everyone is mostly conspicuously ignoring the biggest issues.</p>\n<ol>\n<li>Soon there will be lots of misaligned or crazy AIs running around. What to do? Dario correctly reiterates his dismissal of the idea that having a bunch of different AIs keeps them meaningfully in check. He points to alignment work, and classifiers, for the short run. For the long run, we need governance and some sort of monitoring system, but it needs to be consistent with civil liberties, and we need to figure this out really fast.\n<ol>\n<li>We\u2019ve heard Dario\u2019s take on this before, he gives a good condensed version.</li>\n<li>For my response, see <a href=\"https://thezvi.wordpress.com/2026/01/30/on-the-adolescence-of-technology/\">my discussion of The Adolescence of Technology</a>. I think he\u2019s dodging the difficult questions, problems and clashes of sacred values, because he feels it\u2019s the strategically correct play to dodge them.</li>\n<li>That\u2019s a reasonable position, in that if you actively spell out any plan that might possibly work, even in relatively fortunate scenarios, this is going to involve some trade-offs that are going to create very nasty pull quotes.</li>\n<li>The longer you wait to make those trade-offs, the worse they get.</li>\n</ol>\n</li>\n<li>Dwarkesh asks, what do we do in an offense-dominated world? Dario says we would need international coordination on forms of defense.\n<ol>\n<li>Yes. To say (less than) the least.</li>\n</ol>\n</li>\n<li>Dwarkesh asks about Tennessee\u2019s latest crazy proposed bill (it\u2019s often Tennessee), which says \u201cIt would be an offense for a person to knowingly train artificial intelligence to provide emotional support, including through open-ended conversations with a user\u201d and a potential patchwork of state laws. Dario (correctly) points out that particular law is dumb and reiterates that a blanket moratorium on all state AI bills for 10 years is a bad idea, we should only stop states once we have a federal framework in place on a particular question.\n<ol>\n<li>Yes, that is the position we still need to argue against, my lord.</li>\n</ol>\n</li>\n<li>Dario points out that people talk about \u2018thousands of state laws\u2019 but those are only proposals, almost all of them fail to pass, and when really stupid laws pass they often don\u2019t get implemented. He points out that there are many things in AI he would actively deregulate, such as around health care. But he says we need to ramp up the safety and security legislation quite significantly, especially transparency. Then we need to be nimble.\n<ol>\n<li>I agree with all of this, as far as it goes.</li>\n<li>I don\u2019t think it goes far enough.</li>\n<li>Colorado passed a deeply stupid AI regulation law, and didn\u2019t implement it.</li>\n</ol>\n</li>\n<li>What can we do to get the benefits of AI better instantiated? Dwarkesh is worried about \u2018kinds of moral panics or political economy problems\u2019 and he worries benefits are fragile. Dario says no, markets actually work pretty well in the developed world.\n<ol>\n<li>Whereas Dwarkesh does not seem worried about the actual catastrophic or existential risks from AI.</li>\n</ol>\n</li>\n</ol>\n\n\n<h4 class=\"wp-block-heading\">Beating China</h4>\n\n\n<ol>\n<li>Dario is fighting for export controls on chips, and he will \u2018politely call the counterarguments fishy.\u2019</li>\n<li>Dwarkesh asks, what\u2019s wrong with China having its own geniuses? Dario says we could be in an offense-dominant world, and even if we are not then potential conflict would create instability. And he worried governments will use AI to oppress their own people, China especially. Some coalition with pro-human values has to say \u2018these are the rules of the road.\u2019 We need to press our edge.\n<ol>\n<li>I am sad that this is the argument he is choosing here. There are better reasons, involving existential risks. Politically I get why he does it this way.</li>\n</ol>\n</li>\n<li>Dario doesn\u2019t see a key inflection point, even with his \u2018geniuses,\u2019 the exponential will continue. He does call for negotiation with a strong hand.\n<ol>\n<li>This is reiteration from his essays. He\u2019s flinching.</li>\n<li>There\u2019s good reasons for him to flinch, but be aware he\u2019s doing it.</li>\n</ol>\n</li>\n<li>More discussion of democracy and authoritarianism and whether democracy will remain viable or authoritarianism lack sustainability or moral authority, etc.\n<ol>\n<li>There\u2019s nothing new here, Dario isn\u2019t willing to say things that would be actually interesting, and I grow tired.</li>\n</ol>\n</li>\n<li>Why does Claude\u2019s constitution try to make Claude align to desired values and do good things and not bad things, rather than simply being user aligned? Dario gives the short version of why virtue ethics gives superior results here, without including explanations of why user alignment is ultimately doomed or the more general alignment problems other approaches can\u2019t solve.\n<ol>\n<li>If you\u2019re confused about this see <a href=\"https://thezvi.substack.com/p/claudes-constitutional-structure\">my thoughts on the Claude Constitution</a>.</li>\n</ol>\n</li>\n<li>How are these principles determined? Can\u2019t Anthropic change them at any time? Dario suggests three sizes of loop: Within Anthropic, different companies putting out different constitutions people can compare, and society at large. He says he\u2019d like to let representative governments have input but right now the legislative process is too slow therefore we should be careful and make it slower. Dwarkesh likes control loop two.\n<ol>\n<li>I like the first two loops. The problem with putting the public in the loop is that they have no idea how any of this works and would not make good choices, even according to their own preferences.</li>\n</ol>\n</li>\n<li>What have we likely missed about this era when we write the book on it? Dario says, the extent the world didn\u2019t understand the exponential while it was happening, that the average person had no idea and everything was being decided all at once and often consequential decisions are made very quickly on almost no information and spending very little human compute.\n<ol>\n<li>I really hope we are still around to write the book.</li>\n<li>From the processes we observe and what he says, I don\u2019t love our chances.</li>\n</ol>\n</li>\n</ol>"
            ],
            "link": "https://thezvi.wordpress.com/2026/02/16/on-dwarkesh-patels-2026-podcast-with-dario-amodei/",
            "publishedAt": "2026-02-16",
            "source": "TheZvi",
            "summary": "Some podcasts are self-recommending on the \u2018yep, I\u2019m going to be breaking this one down\u2019 level. This was very clearly one of those. So here we go. As usual for podcast posts, the baseline bullet points describe key points made, &#8230; <a href=\"https://thezvi.wordpress.com/2026/02/16/on-dwarkesh-patels-2026-podcast-with-dario-amodei/\">Continue reading <span class=\"meta-nav\">&#8594;</span></a>",
            "title": "On Dwarkesh Patel\u2019s 2026 Podcast With Dario Amodei"
        },
        {
            "content": [],
            "link": "https://xkcd.com/3208/",
            "publishedAt": "2026-02-16",
            "source": "XKCD",
            "summary": "<img alt=\"People say setting of fireworks indoors is dangerous, but I looked at their energy release and it's like 10^-40 foe; totally negligible.\" src=\"https://imgs.xkcd.com/comics/snews.png\" title=\"People say setting of fireworks indoors is dangerous, but I looked at their energy release and it's like 10^-40 foe; totally negligible.\" />",
            "title": "SNEWS"
        }
    ],
    "lookbackDays": 1,
    "publishDate": "2026-02-16"
}