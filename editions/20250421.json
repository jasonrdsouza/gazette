{
    "articles": [
        {
            "content": [
                "<img alt=\"autoregressive queens of failure\" src=\"https://ghuntley.com/content/images/2025/04/A-vibrant-retro-style-traditional-tattoo-art-print-on-a-white-background-depicting-a-bowling-ball-careening-down-a-gutter--surrounded-by-complex-ornamental-designs-and-a-light-ethereal-aura--conveying-a-mystical-spirit.jpg\" /><p>Have you ever had your AI coding assistant suggest something so off-base that you wonder if it&#x2019;s trolling you? Welcome to the world of autoregressive failure. </p><p>LLMs, the brains behind these assistants, are great at predicting the next word&#x2014;or line of code&#x2014;based on what&apos;s been fed into them. But when the context gets too complex or concerns within the context are mixed, they lose the thread and spiral into hilariously (or frustratingly) wrong territory. Let&#x2019;s dive into why this happens and how to stop it from happening.</p><p>First, I&apos;ll need you to stop by the following blog post to understand an agent from first principles. </p><figure class=\"kg-card kg-bookmark-card kg-card-hascaption\"><a class=\"kg-bookmark-container\" href=\"https://ampcode.com/how-to-build-an-agent?ref=ghuntley.com\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">How To Build An Agent | Amp</div><div class=\"kg-bookmark-description\">Building a fully functional, code-editing agent in less than 400 lines.</div><div class=\"kg-bookmark-metadata\"><img alt=\"autoregressive queens of failure\" class=\"kg-bookmark-icon\" src=\"https://ghuntley.com/content/images/icon/amp-mark-color.svg\" /><span class=\"kg-bookmark-author\">Amp</span></div></div><div class=\"kg-bookmark-thumbnail\"><img alt=\"autoregressive queens of failure\" src=\"https://ghuntley.com/content/images/thumbnail/og-how-to-build-an-agent.jpg\" /></div></a><figcaption><p><span style=\"white-space: pre-wrap;\">what an agent is: explained in less than 400 lines of code</span></p></figcaption></figure><p>Still reading? Great. In the diagram below, an agent has been configured with two tools. Each tool has also been configured with a tool prompt, which advertises how to use the tool to the LLM. </p><p>The tools are:</p><ul><li>Tool 1 - Visit a website and extract the contents of the page.</li><li>Tool 2 - Perform a Google search and return search results.</li></ul><p>Now, imagine for a moment that this agent is an interactive console application that you use to search Google or visit a URL.</p><figure class=\"kg-card kg-image-card kg-width-full\"><img alt=\"autoregressive queens of failure\" class=\"kg-image\" height=\"2740\" src=\"https://ghuntley.com/content/images/2025/04/Untitled-diagram-2025-04-21-151047-1.png\" width=\"2000\" /></figure><p>Whilst using the agent, you perform the actions:</p><ol><li>Visit a news website.</li><li>Search Google for party hats.</li><li>Visit a Wikipedia article about Meerkats.</li></ol><p>Each of these operations allocates the results from the above operations into memory - the LLM context window.</p><figure class=\"kg-card kg-image-card kg-width-full kg-card-hascaption\"><img alt=\"autoregressive queens of failure\" class=\"kg-image\" height=\"543\" src=\"https://ghuntley.com/content/images/2025/04/image-8.png\" width=\"1048\" /><figcaption><span style=\"white-space: pre-wrap;\">when data is </span><code style=\"white-space: pre-wrap;\"><span>malloc()</span></code><span style=\"white-space: pre-wrap;\">&apos;ed into the LLM&apos;s context window. It cannot be </span><code style=\"white-space: pre-wrap;\"><span>free()</span></code><span style=\"white-space: pre-wrap;\"> &apos;d unless you create a brand new context window. </span></figcaption></figure><p>With all that context loaded into the window, all that data is now available for consideration when you ask a question. Thus, there&apos;s a probability that it&apos;ll generate a news article about Meerkats wearing party hats in response to a search for Meerkat facts (ie. Wikipedia). </p><p>That might sound obvious, but it&apos;s not. The tooling that most software developers use day-to-day hides context windows from the user and encourages endless chatops sessions within the same context window, even if the current task is unrelated to the previous task.</p><p>This creates bad outcomes because what is loaded into memory is unrelated to the job to be done, and results in noise from software engineers saying that &apos;AI doesn&apos;t work&apos;, but in reality, it&apos;s how the software engineers are holding/using the tool that&apos;s at fault.</p><p>My #1 recommendation for people these days is to use a context window for one task, and one task only. If your coding agent is misbehaving, it&apos;s time to create a new context window. If the bowling ball is in the gutter, there&apos;s no saving it. It&apos;s in the gutter.</p><p>My #2 recommendation is to not redline the context window (see below)</p><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://ghuntley.com/redlining/\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">if you are redlining the LLM, you aren&#x2019;t headlining</div><div class=\"kg-bookmark-description\">It&#x2019;s an old joke in the DJ community about upcoming artists having a bad reputation for pushing the audio signal into the red. Red is bad because it results in the audio signal being clipped and the mix sounding muddy. It&#x2019;s a good analogy that applies to software</div><div class=\"kg-bookmark-metadata\"><img alt=\"autoregressive queens of failure\" class=\"kg-bookmark-icon\" src=\"https://ghuntley.com/content/images/icon/android-chrome-192x192-28.png\" /><span class=\"kg-bookmark-author\">Geoffrey Huntley</span><span class=\"kg-bookmark-publisher\">Geoffrey Huntley</span></div></div><div class=\"kg-bookmark-thumbnail\"><img alt=\"autoregressive queens of failure\" src=\"https://ghuntley.com/content/images/thumbnail/redline-digital-dj-tips-1.webp\" /></div></a></figure><p>ps. socials</p><ul><li>X - <a href=\"https://x.com/GeoffreyHuntley/status/1914350677331231191?ref=ghuntley.com\">https://x.com/GeoffreyHuntley/status/1914350677331231191</a></li><li>BlueSky - <a href=\"https://bsky.app/profile/ghuntley.com/post/3lndk65i7fu25?ref=ghuntley.com\">https://bsky.app/profile/ghuntley.com/post/3lndk65i7fu25</a></li><li>LinkedIn - <a href=\"https://www.linkedin.com/posts/geoffreyhuntley_autoregressive-queens-of-failure-activity-7320115355262074881-FfPI?utm_source=share&amp;utm_medium=member_desktop&amp;rcm=ACoAAABQKuUB2AJ059keUcRUVLbtmoa6miLVlTI\">https://www.linkedin.com/posts/geoffreyhuntley_autoregressive-queens-of-failure-activity-7320115355262074881-FfPI</a></li></ul><div class=\"kg-card kg-signup-card kg-width-wide \" style=\"background-color: #F0F0F0; display: none;\">\n            \n            <div class=\"kg-signup-card-content\">\n                \n                <div class=\"kg-signup-card-text \">\n                    <h2 class=\"kg-signup-card-heading\" style=\"color: #000000;\"><span style=\"white-space: pre-wrap;\">Sign up for Geoffrey Huntley</span></h2>\n                    <p class=\"kg-signup-card-subheading\" style=\"color: #000000;\"><span style=\"white-space: pre-wrap;\">I work remotely from a van that is slowly working its way around Australia. Follow me for the intersection of remote work, camping &amp; #vanlife.</span></p>\n                    \n        <form class=\"kg-signup-card-form\">\n            \n            <div class=\"kg-signup-card-fields\">\n                <input class=\"kg-signup-card-input\" id=\"email\" required=\"true\" type=\"email\" />\n                <button class=\"kg-signup-card-button kg-style-accent\" style=\"color: #FFFFFF;\" type=\"submit\">\n                    <span class=\"kg-signup-card-button-default\">Subscribe</span>\n                    <span class=\"kg-signup-card-button-loading\"><svg height=\"24\" viewBox=\"0 0 24 24\" width=\"24\" xmlns=\"http://www.w3.org/2000/svg\">\n        <g class=\"nc-icon-wrapper\" fill=\"currentColor\" stroke=\"none\" stroke-linecap=\"round\" stroke-linejoin=\"round\" stroke-width=\"2\">\n            <g class=\"nc-loop-dots-4-24-icon-o\">\n                <circle cx=\"4\" cy=\"12\" r=\"3\">\n                <circle cx=\"12\" cy=\"12\" r=\"3\">\n                <circle cx=\"20\" cy=\"12\" r=\"3\">\n            </g>\n            \n        </g>\n    </svg></span>\n                </button>\n            </div>\n            <div class=\"kg-signup-card-success\" style=\"color: #000000;\">\n                Email sent! Check your inbox to complete your signup.\n            </div>\n            <div class=\"kg-signup-card-error\" style=\"color: #000000;\"></div>\n        </form>\n        \n                    <p class=\"kg-signup-card-disclaimer\" style=\"color: #000000;\"><span style=\"white-space: pre-wrap;\">No spam. Unsubscribe anytime.</span></p>\n                </div>\n            </div>\n        </div>"
            ],
            "link": "https://ghuntley.com/gutter/",
            "publishedAt": "2025-04-21",
            "source": "Geoffrey Huntley",
            "summary": "<p>Have you ever had your AI coding assistant suggest something so off-base that you wonder if it&#x2019;s trolling you? Welcome to the world of autoregressive failure. </p><p>LLMs, the brains behind these assistants, are great at predicting the next word&#x2014;or line of code&#x2014;based on what&</p>",
            "title": "autoregressive queens of failure"
        },
        {
            "content": [],
            "link": "https://simonwillison.net/2025/Apr/21/ai-assisted-search/#atom-entries",
            "publishedAt": "2025-04-21",
            "source": "Simon Willison",
            "summary": "<p>For the past two and a half years the feature I've most wanted from LLMs is the ability to take on search-based research tasks on my behalf. We saw the first glimpses of this back in early 2023, with Perplexity (first launched <a href=\"https://en.wikipedia.org/wiki/Perplexity_AI\">December 2022</a>, first prompt leak <a href=\"https://simonwillison.net/2023/Jan/22/perplexityai/\">in January 2023</a>) and then the GPT-4 powered Microsoft Bing (which launched/cratered spectacularly <a href=\"https://simonwillison.net/2023/Feb/15/bing/\">in February 2023</a>). Since then a whole bunch of people have taken a swing at this problem, most notably <a href=\"https://gemini.google.com/\">Google Gemini</a> and <a href=\"https://openai.com/index/introducing-chatgpt-search/\">ChatGPT Search</a>.</p> <p>Those 2023-era versions were promising but very disappointing. They had a strong tendency to hallucinate details that weren't present in the search results, to the point that you couldn't trust anything they told you.</p> <p>In this first half of 2025 I think these systems have finally crossed the line into being genuinely useful.</p> <ul> <li><a href=\"https://simonwillison.net/2025/Apr/21/ai-assisted-search/#deep-research-from-three-different-vendors\">Deep Research, from three different vendors</a></li> <li><a href=\"https://simonwillison.net/2025/Apr/21/ai-assisted-search/#o3-and-o4-mini-are-really-good-at-search\">o3 and o4-mini are really good at search</a></li> <li><a href=\"https://simonwillison.net/2025/Apr/21/ai-assisted-search/#google-and-anthropic-need-to-catch-up\">Google and Anthropic need to catch up</a></li> <li><a href=\"https://simonwillison.net/2025/Apr/21/ai-assisted-search/#lazily-porting-code-to-a-new-library-version-via-search\">Lazily porting code to a new library version via search</a></li> <li><a href=\"https://simonwillison.net/2025/Apr/21/ai-assisted-search/#how-does-the-economic-model-for-the-web-work-now-\">How does the economic model for the Web work now?</a></li> </ul> <h4 id=\"deep-research-from-three-different-vendors\">Deep Research, from three different vendors</h4> <p>First came the",
            "title": "AI assisted search-based research actually works now"
        },
        {
            "content": [
                "<p>This is the weekly visible open thread. Post about anything you want, ask random questions, whatever. ACX has an unofficial <a href=\"https://www.reddit.com/r/slatestarcodex/\">subreddit</a>, <a href=\"https://discord.gg/RTKtdut\">Discord</a>, and <a href=\"https://www.datasecretslox.com/index.php\">bulletin board</a>, and <a href=\"https://www.lesswrong.com/community?filters%5B0%5D=SSC\">in-person meetups around the world</a>. 95% of content is free, but for the remaining 5% you can subscribe <strong><a href=\"https://astralcodexten.substack.com/subscribe\">here</a></strong>. Also:</p><p><strong>1: </strong>ACX meetups this week in Warsaw, Sao Paulo, Buenos Aires, Atlanta, Philly, Brooklyn, and Dallas, among others. And late additions to the list include Belfast, Vancouver, and Stockholm. See <a href=\"https://www.astralcodexten.com/p/meetups-everywhere-spring-2025-times\">the post</a> for details. And remember there&#8217;s a <a href=\"http://tinyurl.com/acx-meetup-survey\">feedback form</a> for meetup-goers.</p><p><strong>2: </strong>Reminder that the deadline for submissions to the <a href=\"https://www.astralcodexten.com/p/everything-except-book-review-contest\">Everything-Except-Book Review Contest</a> is coming up on May 12.</p><p><strong>3: </strong>I sent all ACX Grants recipients an email with a link to a form asking for updates. I think it went to spam for many of you. If you got a grant either last year or 2021-2022, please check your spam folder for an email from me.</p><p><strong>4: </strong>Sorry for the delay, AMA with the AI 2027 team is planned for this Friday, 3:30 - 6 Pacific time. I&#8217;ll post a confirmation of this later this week. </p><p><strong>5: </strong>Upcoming AI policy opportunities: </p><ul><li><p><a href=\"https://x.com/JoinFAI/status/1907077018744668412\">Samuel Hammond and the Foundation for American Innovation are launching a conservative AI policy fellowship</a>. &#8220;A six week educational fellowship for DC policy professionals interested in AI policy.&#8221;</p></li><li><p><a href=\"https://horizonpublicservice.org/ai-innovation-security-policy-workshop/\">Horizon Institute has a three-day AI Innovation And Security Policy Workshop</a>. &#8220;Interested in whether you should pursue a career in AI policy in DC? Learn about AI policy under the new administration, meet the people shaping decisions in DC, and decide whether you want to apply your background to the opportunities and challenges ahead.&#8221;</p></li><li><p><a href=\"https://x.com/AndrewCurran_/status/1912248588387905672\">Ted Cruz and the Commerce Committee are looking for an AI Counsel. </a></p></li></ul><p><strong>6: </strong>Some more replies to my Purpose Of A System Is Not What It Does post, including <a href=\"https://aashishreddy.substack.com/p/come-on-scott-alexander-obviously\">by Aashish Reddy</a>.</p><p><strong>7: </strong>New subscribers-only post, <a href=\"https://www.astralcodexten.com/p/yet-another-reason-to-hate-college\">Yet Another Reason To Hate College Admissions Essays</a>:</p><blockquote><p>Five, maybe ten percent of applicants are some kind of special snowflake whose father was murdered when they were five years old. As he lay there bleeding out, he said &#8220;Daughter, my whole life, I dreamed of being the first LGBT person to get a PhD in the study of ancient Assyria. Now that dream has been taken from me. With my dying breath, I give you my trowel and hand-painted figurine of Tiglath-Pileser III, in the hopes that one day you will succeed where I failed&#8221;. [&#8230;]</p><p>The rest of us are just some kid who wants to go to college because that&#8217;s where all the good jobs are. If you really press us, we&#8217;ll say something like &#8220;idk biology seems pretty cool&#8221;. We encountered an approximately average number of hardships. Once when we got our wisdom teeth taken out, the surgeon said we had the weirdest reaction to anaesthesia he&#8217;d ever seen - does that count as a hardship?</p><p>(&#8220;Yes, but why are you applying to Dartmouth in particular?&#8221; &#8220;Because we looked at the <em>US News &amp; World Report</em> rankings and realized we weren&#8217;t good enough to get into colleges better than Dartmouth, but we were too good for colleges worse than Dartmouth, any other stupid obvious questions?&#8221;)</p><p>The college admissions essay is what happens when you tell the second type of person that, in order to ever get a job better than busboy, they need to pretend to be the first type of person.</p></blockquote><p>And a commenter <a href=\"https://www.astralcodexten.com/p/yet-another-reason-to-hate-college/comment/109929268\">goes further</a> with a Lacanian argument that <a href=\"https://lareviewofbooks.org/blog/essays/reflections-mirror-stage/\">&#8220;the college essay causes psychological harm&#8221;</a>.</p>"
            ],
            "link": "https://www.astralcodexten.com/p/open-thread-378",
            "publishedAt": "2025-04-21",
            "source": "SlateStarCodex",
            "summary": "<p>This is the weekly visible open thread. Post about anything you want, ask random questions, whatever. ACX has an unofficial <a href=\"https://www.reddit.com/r/slatestarcodex/\">subreddit</a>, <a href=\"https://discord.gg/RTKtdut\">Discord</a>, and <a href=\"https://www.datasecretslox.com/index.php\">bulletin board</a>, and <a href=\"https://www.lesswrong.com/community?filters%5B0%5D=SSC\">in-person meetups around the world</a>. 95% of content is free, but for the remaining 5% you can subscribe <strong><a href=\"https://astralcodexten.substack.com/subscribe\">here</a></strong>. Also:</p><p><strong>1: </strong>ACX meetups this week in Warsaw, Sao Paulo, Buenos Aires, Atlanta, Philly, Brooklyn, and Dallas, among others. And late additions to the list include Belfast, Vancouver, and Stockholm. See <a href=\"https://www.astralcodexten.com/p/meetups-everywhere-spring-2025-times\">the post</a> for details. And remember there&#8217;s a <a href=\"http://tinyurl.com/acx-meetup-survey\">feedback form</a> for meetup-goers.</p><p><strong>2: </strong>Reminder that the deadline for submissions to the <a href=\"https://www.astralcodexten.com/p/everything-except-book-review-contest\">Everything-Except-Book Review Contest</a> is coming up on May 12.</p><p><strong>3: </strong>I sent all ACX Grants recipients an email with a link to a form asking for updates. I think it went to spam for many of you. If you got a grant either last year or 2021-2022, please check your spam folder for an email from me.</p><p><strong>4: </strong>Sorry for the delay, AMA with the AI 2027 team is planned for this Friday, 3:30 - 6 Pacific time. I&#8217;ll post a confirmation of this later this week. </p><p><strong>5: </strong>Upcoming AI policy opportunities: </p><ul><li><p><a href=\"https://x.com/JoinFAI/status/1907077018744668412\">Samuel Hammond and the Foundation for",
            "title": "Open Thread 378"
        },
        {
            "content": [],
            "link": "https://xkcd.com/3079/",
            "publishedAt": "2025-04-21",
            "source": "XKCD",
            "summary": "<img alt=\"'Wow, that must be why you swallow so many of them per year!' 'No, that's spiders. You swallow WAY more ants.'\" src=\"https://imgs.xkcd.com/comics/air_fact.png\" title=\"'Wow, that must be why you swallow so many of them per year!' 'No, that's spiders. You swallow WAY more ants.'\" />",
            "title": "Air Fact"
        }
    ],
    "lookbackDays": 1,
    "publishDate": "2025-04-21"
}