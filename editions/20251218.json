{
    "articles": [
        {
            "content": [
                "<p>Following up on <a href=\"https://karpathy.bearblog.dev/digital-hygiene/\">digital hygiene</a>, I wanted to write up my (evolving, opinionated) guide to chemical hygiene. I keep ranting about this topic to all of my friends recently (you can tell I'm really fun at parties), so I thought it would be worth writing it up to have it all in one place/url:</p>\n<p><strong>Water</strong></p>\n<p>Starting out with controlling your water system, which is the easiest in terms of concrete, high confidence recommendations that in my experience still only <5% of my friends have adopted:</p>\n<ul>\n<li>All your drinking water should come from Reverse Osmosis - the gold-standard Point of Use water filtration system, with a remineralization post filter. Ideally install an under the sink system, but fallback to countertop systems is ok. Brita and other basic filters are not good enough to adequately filter your drinking water.</li>\n<li>In addition, install a whole-home water filter (usually sediment+carbon, not Reverse Osmosis, that would be impractical), to enjoy cleaner water in your entire home, including shower, dishwasher, laundry, etc. If that's too expensive or impossible (e.g. you're renting), at least install a shower filter.</li>\n<li>Contact a company in your local area to install and maintain both of these systems for you (maintenance involves a ~yearly filter change).</li>\n<li>Do not drink water from water bottles, certainly from plastic bottles but also in general. You cannot control that supply chain and all of its environmental conditions (especially light, heat).</li>\n<li>Testing water is easy if you call over a professional - verify your installation and compare tap water to whole-home water to reverse osmosis drinking water to verify your installation.</li>\n</ul>\n<p><strong>Air</strong></p>\n<p>Similar to water, air is relatively well-understood and simple to control if your home:</p>\n<ul>\n<li>Install HVAC filters in your home, and/or get a standalone air purifier, e.g. Dyson Big+Quiet looks cool, or something like IQAir.</li>\n<li>Avoid combustion in your home in general - it's a source of all kinds of fumes and partial combustion products:<ul>\n<li>Avoid candles (use beeswax only if you really like them, I do occasionally)</li>\n<li>Avoid gas stoves (use induction cooktop)</li>\n<li>Avoid unsealed gas fireplaces. Use sealed, electric ignition only.</li>\n</ul>\n</li>\n<li>Skip air humidifiers unless you really live in very dry conditions, otherwise they come with mold/bacterial risks unless they are very properly taken care of</li>\n<li>Skip all air fresheners, oil diffusers and all kinds of fragrances, they are a very poorly regulated wild west of synthetic chemicals.</li>\n<li>Measure the basics of your home air quality. Example device <a href=\"https://www.amazon.com/dp/B0FXDQX5XQ\">I bought</a> recently.</li>\n<li>Like water, testing air is easy - call a professional to do a more comprehensive test panel for the air in your home, e.g. including Radon which can come up from the ground, mold, spores, etc.</li>\n</ul>\n<p><strong>Food</strong></p>\n<p>Food is the hardest category to control because it involves extensively deep supply chains that have been ruthlessly efficiency-maxxed over the last few decades with little to no regard for public health externalities. The industry has a clear and immediate financial incentive to trade something 10% cheaper at the cost of something 10X more harmful to you as long as it shows up over a long enough time period that the accounting is impractical. And it just turns out that in food there are many, many ways to cut corners. Sadly, the US Government has been woefully inadequate in constraining the industry and lags far behind other countries (e.g. Europe especially), hence the recent <a href=\"https://en.wikipedia.org/wiki/Make_America_Healthy_Again\">MAHA</a> efforts. I'll slip this section into 1) food sourcing and 2) cooking/preparation.</p>\n<p><strong>Food: 1) sourcing</strong></p>\n<ul>\n<li>Fruits/veggies: buy <a href=\"https://www.ams.usda.gov/sites/default/files/media/Organic%20Practices%20Factsheet.pdf\">organic</a>, which restricts a large variety of chemical treatments. - the label (PLU) will usually start with 9*.</li>\n<li>Salmon: look for \"Pacific\" (not Atlantic) and \"wild-caught\" (not farmed). Farmed salmon come from overcrowded farms in un-natural conditions that mix chemicals, disease and parasites, and diet supplements to make them have the right color.</li>\n<li>Chicken: Look for 1) \"Pasture raised\" (all the other adjectives like \"cage free\" and \"free range\" are scams, it's not what you think), then look for 2) \"Organic\" and 3) \"Air-chilled\" if you don't like the idea of your chicken taking a chlorine bath. You want all 3 of these adjectives.</li>\n<li>Eggs/dairy: Look for \"Pasture raised\", \"Organic\".</li>\n<li>Packaged goods: Look at the ingredients list. It should be short. It should make sense. For example, your bread should not be 50 ingredients that you can't pronounce, it should be 4 (flour, water, salt, and yeast). Use apps like \"Ivy\" and \"BobbyApproved\" to scan the bar codes (I really like and use both).</li>\n<li>Don't buy \"edible food-like substances\", which are usually lining the shelves on the inner shelves of your supermarket. Shop only at the walls, which contain real food - fruits/veggies, dairy, meat, breads. For example, fruit loops and such are NOT food and routinely contain harmful ingredients that I'm frankly shocked are legal, many of which are banned in Europe and elsewhere in the world.</li>\n<li>Avoid canned soups/products.</li>\n<li>Avoid touching receipts, they are laced with BPA/BPSs (endocrine disruptors).</li>\n<li>Consider getting a home delivery service, e.g. I currently like and use <a href=\"https://www.shoplocale.com/\">Locale</a>.</li>\n</ul>\n<p><img alt=\"GvWK5b1X0AEevjd\" src=\"https://bear-images.sfo2.cdn.digitaloceanspaces.com/karpathy/gvwk5b1x0aeevjd.webp\" />\nExample food, and what a grocery store should look like. From this <a href=\"https://x.com/karpathy/status/1942612984481870068\">tweet</a>, with a bit more discussion.</p>\n<p><strong>Food: 2) Cooking & preparation</strong></p>\n<ul>\n<li>Rule number 1: avoid plastics, specially in combination with heat.<ul>\n<li>Use only stainless steel or cast iron pans only. Don't use non-stick (teflon etc)</li>\n<li>Storage: use non-plastic containers like glass, stainless steel, ceramic</li>\n<li>Cutting boards: wooden only</li>\n<li>Utensils: wooden, metal cookware</li>\n<li>Blenders: glass or stainless steel, don't allow your food to mix at high velocities with plastics, they will chip into your food.</li>\n<li>Don't Doordash hot food that comes in plastic containers</li>\n<li>Don't microwave food in plastic containers to prevent leeching. Transfer food to microwave-safe non-plastic plates.</li>\n<li>Don't use the yellow sponges (use cotton, loofa, stainless steel scouring pad)</li>\n<li>No hot coffee or liquids in disposable cups (e.g. Starbucks), they are all lined with plastic. Bring a mug with you ideally, or ask \"for here\" if you can.</li>\n<li>No hot coffee from cheap coffee machines (e.g. Keurig, again - they pass hot liquids through plastic components).</li>\n<li>Do not use tea bags, they contain plastics and chemicals that leech into your tea. Only buy and use loose leaf tea with a stainless steel strainer.</li>\n</ul>\n</li>\n<li>Cooking oil: Seed oils are currently hotly contested. Personally I find them highly suspicious and prefer to use clean oils: extra virgin olive oil (ideally at lower temperatures), avocado oil (cooking), or butter, ghee, beef tallow (frying).</li>\n<li>Your kitchen should basically be all wood, stainless steel, glass, ceramic, and for any fabrics only the natural kind (cotton, bamboo, linen, wool, etc.).</li>\n</ul>\n<p><strong>Fabrics</strong></p>\n<p>Our bodies come into frequent contact with all kinds of fabrics (clothing, bedding, furniture, rugs, mats, ...). As you handle these materials, they shed particles, which you end up breathing in.</p>\n<ul>\n<li>Again, avoid the pervasive toxic petroleum-based plastics industry - these fibers are much cheaper (which is why they found their way everywhere), but they shed nano/micro plastics that are steeped in a zoo of chemical additives (plasticizers).</li>\n<li>Only use natural fibers: cotton, linen, hemp, wool, silk, bonus points for organic, bonus points for extra certifications (e.g. GOTS). You'll see that the use of plastics in fabrics (e.g. clothes) is <em>pervasive</em>. They've really snuck them everywhere. If you didn't pay too much attention so far, your clothes almost certainly have polyester, nylon, spandex, etc. Your rug is almost certainly polyester.</li>\n<li>Be wary of \"bamboo\" which sounds natural but there is a pervasive and sketchy trick that the industry already got sued over by the FTC in 2010 for deceptive marketing. It's not bamboo, it's cellulose that gets heavily chemically processed into fibers called rayon/viscose.</li>\n</ul>\n<p><strong>Cleaning supplies: soap, dish washing, laundry, toilet, spray cleaners</strong></p>\n<ul>\n<li>Look for very few and simple ingredients and ideally \"fragrance free\" and \"dye free\". I currently use <a href=\"https://www.blueland.com/\">Blueland</a> for all of these.</li>\n</ul>\n<p><strong>Dental hygiene</strong></p>\n<p>This is a category that I was not able to make a dent into in my personal life, despite a number of attempts. The goal with all of this is go after the 80:20 low hanging fruit and this category for me falls into the latter category:</p>\n<ul>\n<li>Toothbrush - it won't surprise you that heavy rubbing of plastic bristles over your teeth sheds some of the material. Again don't fall for \"bamboo\" scams when browsing toothbrushes on Amazon. These products aren't what people imagine, they are synthetics, the bristles still have polyester or nylon and etc. I did eventually find actually plastic-free toothbrushes (see e.g. <a href=\"https://primals.shop/products/primals-horse-bristle-bamboo-toothbrush-4-pack\">Primal</a>) that have bristles from horse/boar hair, but to be honest they are not as comfortable so I still use plastic bristle toothbrush today.</li>\n<li>Floss - same story as toothbrush. The only actually natural type you can get is silk floss, but it's a bit more brittle than what you're probably used to and I couldn't find one in the much easier to use pick form. I still use plastic floss right now and I am experimenting with water floss.</li>\n<li>Toothpaste - it seems very trendy to diss on fluoride but I'm not personally convinced just yet and I still use a fluoride toothpaste.</li>\n</ul>\n<p><strong>Sunscreen</strong></p>\n<ul>\n<li>Most sunscreens are <em>chemical</em>. I prefer mineral sunscreens, which simply create a layer on top of the skin that acts as a physical barrier to UV (e.g. look for Zinc), though unfortunately they do create a \"chalky\" look. Chemical sunscreens seep into the skin (and blood) and there are concerns over some of their ingredients and their potential to act as endocrine disruptors. I should add that I'm a little bit suspicious of the need and overuse of sunscreen in general and I personally apply it only in cases of prolonged, intense exposure of my computer scientist vampire skin to high UV index sun. Check your Weather app to see the UV Index for the time of day of your exposure.</li>\n<li>I am much less well-versed in cosmetics more generally because I don't personally use these products but I wouldn't at all be surprised if it is a major minefield.</li>\n</ul>\n<p><strong>Wellness</strong></p>\n<ul>\n<li>Cardio (make sure to do it properly - most people spend way too much time in Zone 3+, spend a lot more time in Zone 2)</li>\n<li>Sauna (<a href=\"https://blueprint.bryanjohnson.com/blogs/news/is-sauna-worth-the-hype\">shown</a> to reduce the inevitably accumulated toxins via sweat)</li>\n<li>Vitamin D - you're probably deficient like everyone else. Blood is relatively easy to test and I encourage people to do a full panel ~yearly to track health and deficiencies.</li>\n</ul>\n<p><strong>Learn more</strong></p>\n<ul>\n<li>Recommended watching: I now use my Instagram for more non-AI / lifestyle related things, e.g. see the reposts section on <a href=\"https://www.instagram.com/karpathy/\">my account</a> for some of the featured reels that I've accumulated over time on the topics above.</li>\n<li>Recommended reading: <a href=\"https://www.amazon.com/Poison-Like-Other-Microplastics-Corrupted/dp/1642832359\">\"Poison like no other\"</a> (on plastics), <a href=\"https://www.amazon.com/Defense-Food-Michael-Pollan/dp/1594133328\">\"In Defense of Food\"</a> (on food vs. \"edible food-like substances\"), <a href=\"https://www.amazon.com/Poison-Squad-Chemists-Single-Minded-Twentieth/dp/1594205140\">\"Poison Squad\"</a>, <a href=\"https://www.amazon.com/Metabolical-Processed-Nutrition-Modern-Medicine/dp/0063027712\">\"Metabolical\"</a>.</li>\n<li>Even doing all of the above you are simply decreasing risk, you can never eliminate it. For example, when <a href=\"https://www.plasticlist.org/\">plasticlist.org</a> tested various foods/drinks for plastics, they found a lot of random items that have significantly higher plasticizer measurements than others, in a way you'd never be able to guess. For example, at the time the worst offender by far was Boba guys - your boba would give you a significantly higher dose than any runner up, having to do with some process somewhere in their deep supply chain. Another example I encountered was a farm where to cut costs they didn't bother to remove the plastic wrap from their hay and allowed the cows to just eat all of it together, leading to milk from that specific farm that then tested significantly higher in plasticizers. Unfortunately there is not enough testing, scrutiny and oversight over these deep supply chains by the government.</li>\n<li>There's so much more I didn't even cover in this guide. E.g. why modern wheat is so hyper-optimized to grow fast (which you can measure and profit from) at the cost of lacking nutrients (which the consumer won't normally measure) compared to ancient grains like einkorn. Or why <a href=\"https://www.youtube.com/watch?v=mCALGI1r4WY\">modern honey</a> is basically just glucose syrup compared to actual miracle food that medieval honey was. The cost-driven hyper-optimization of the industry is a deep rabbit hole way beyond the scope of this post. There are too many ways to cut corners and make something cheaper by sacrificing its nutrients and/or by risking longer term public health. If I can convince a few people to at least start paying attention, its goal will have been met.</li>\n</ul>\n<p><strong>TLDR</strong>. Keep your home unsophisticated. Filter your water and air. Eat real food (not edible food-like substances) from well-treated animals and with few, sensible ingredients and minimally sophisticated supply chains and processing steps. Say no to as many dyes and fragrances as you can. Surround yourself with simple, natural materials or strong and inert materials (e.g. stainless steel). Avoid plastics, especially if they are handled, heated, frozen - the risk is not just related to the tiny particles of these exotic materials accumulating <a href=\"https://hscnews.unm.edu/news/hsc-newsroom-post-microplastics-human-brains\">all over your body</a> and interfering with its chemistry, but the large zoo of chemical <a href=\"https://en.wikipedia.org/wiki/Plasticizer\">plasticizers</a> that are added to plastics and then leech out. The government is significantly lagging behind the industry on chemical regulation and this is <em>your</em> responsibility.</p>\n<p>This guide isn't perfect. It's a work in progress. I am not a professional toxicologist or food scientist so my tone above is my frustration that the government is forcing me to be a part-time investigative journalist just to exist in a modern society and not feel like I am poisoning myself and my family. And I didn't even go into and cover all of the environmental aspects of these industries. This state of affairs is much worse here in the US than e.g. in Europe - the EU bans or restricts many food additives, dyes, chemicals and food processing practices that are routine here. The FDA \"Generally Recognized As Safe\" (GRAS) system lets manufacturers self-certify ingredients without independent review and a new exotic chemical or process is innocent until proven guilty, while in Europe the default is often the reverse. So treat all of this as a starting point, ask your favorite LLM for more information on any of the items, let me know your thoughts (e.g. X/Instagram DMs) and I will aim to update this guide over time.</p>"
            ],
            "link": "https://karpathy.bearblog.dev/chemical-hygiene/",
            "publishedAt": "2025-12-18",
            "source": "Andrej Karpathy",
            "summary": "<p>Following up on <a href=\"https://karpathy.bearblog.dev/digital-hygiene/\">digital hygiene</a>, I wanted to write up my (evolving, opinionated) guide to chemical hygiene. I keep ranting about this topic to all of my friends recently (you can tell I'm really fun at parties), so I thought it would be worth writing it up to have it all in one place/url:</p> <p><strong>Water</strong></p> <p>Starting out with controlling your water system, which is the easiest in terms of concrete, high confidence recommendations that in my experience still only <5% of my friends have adopted:</p> <ul> <li>All your drinking water should come from Reverse Osmosis - the gold-standard Point of Use water filtration system, with a remineralization post filter. Ideally install an under the sink system, but fallback to countertop systems is ok. Brita and other basic filters are not good enough to adequately filter your drinking water.</li> <li>In addition, install a whole-home water filter (usually sediment+carbon, not Reverse Osmosis, that would be impractical), to enjoy cleaner water in your entire home, including shower, dishwasher, laundry, etc. If that's too expensive or impossible (e.g. you're renting), at least install a shower filter.</li> <li>Contact a company in your local area to install and maintain both of these systems for you",
            "title": "Chemical hygiene"
        },
        {
            "content": [
                "<p>They say you\u2019re supposed to choose your prior in advance. That\u2019s why it\u2019s called a \u201cprior\u201d. <em>First</em>, you\u2019re supposed to say say how plausible different things are, and <em>then</em> you update your beliefs based on what you see in the world.</p>\n\n<p>For example, currently you are\u2014I assume\u2014trying to decide if you should stop reading this post and do something else with your life. If you\u2019ve read this blog before, then lurking somewhere in your mind is some prior for how often my posts are good. For the sake of argument, let\u2019s say you think 25% of my posts are funny and insightful and 75% are boring and worthless.</p>\n\n<p><img alt=\"\" src=\"https://dynomight.net/img/prior/example1.svg\" /></p>\n\n<p>OK. But now here you are reading these words. If they seem bad/good, then that raises the odds that this particular post is worthless/non-worthless. For the sake of argument again, say you find these words mildly promising, meaning that a good post is 1.5\u00d7 more likely than a worthless post to contain words with this level of quality.</p>\n\n<p><img alt=\"\" src=\"https://dynomight.net/img/prior/example2.svg\" /></p>\n\n<p>If you combine those two assumptions, that implies that the probability that this particular post is good is 33.3%. That\u2019s true because the red rectangle below has half the area of the blue one, and thus the probability that this post is good should be half the probability that it\u2019s bad (33.3% vs. 66.6%)</p>\n\n<p><img alt=\"\" src=\"https://dynomight.net/img/prior/example3.svg\" /></p>\n\n<details>\n  \n(Why half the area? Because the red rectangle is \u2153 as wide and \u00b3\u2044\u2082 as tall as the blue one and \u2153 \u00d7 \u00b3\u2044\u2082 = \u00bd. If you only trust equations, click here for equations.)\n\n\n  <p>It\u2019s easiest to calculate the ratio of the odds that the post is good versus bad, namely</p>\n\n  <div class=\"language-plaintext highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code>P[good | words] / P[bad | words]\n = P[good, words] / P[bad, words]\n = (P[good] \u00d7 P[words | good])\n / (P[bad] \u00d7 P[words | bad])\n = (0.25 \u00d7 1.5) / (0.75 \u00d7 1)\n = 0.5.\n</code></pre></div>  </div>\n\n  <p>It follows that</p>\n\n  <div class=\"language-plaintext highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code>P[good | words] = 0.5 \u00d7 P[bad | words],\n</code></pre></div>  </div>\n\n  <p>and thus that</p>\n\n  <div class=\"language-plaintext highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code>P[good | words] = 1/3.\n</code></pre></div>  </div>\n\n  <p>Alternatively, if you insist on using Bayes\u2019 equation:</p>\n\n  <div class=\"language-plaintext highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code>P[good | words]\n = P[good] \u00d7 P[words | good] / P[words]\n = P[good] \u00d7 P[words | good]\n / (P[good] \u00d7 P[words | good] + P[bad] \u00d7 P[words | bad])\n = 0.25 \u00d7 1.5 / (0.25 \u00d7 1.5 + 0.75)\n = (1/3)\n</code></pre></div>  </div>\n\n</details>\n\n<p>Theoretically, when you chose your prior that 25% of dynomight posts are good, that was supposed to reflect all the information you encountered in life <em>before</em> reading this post. Changing that number based on information contained in this post wouldn\u2019t make any sense, because that information is supposed to be reflected in the second step when you choose your likelihood <code class=\"language-plaintext highlighter-rouge\">p[good | words]</code>. Changing your prior based on this post would amount to \u201cdouble-counting\u201d.</p>\n\n<p>In theory, that\u2019s right. It\u2019s also right in practice for the above example, and for the similar <a href=\"https://en.wikipedia.org/wiki/Representativeness_heuristic#The_taxicab_problem\">cute little examples</a> you find in textbooks.</p>\n\n<p>But for real problems, I\u2019ve come to believe that refusing to change your prior after you see the data often leads to tragedy. The reason is that in real problems, things are rarely just \u201cgood\u201d or \u201cbad\u201d, \u201ctrue\u201d or \u201cfalse\u201d. Instead, truth comes in an infinite number of varieties. And you often can\u2019t predict which of these varieties matter until after you\u2019ve seen the data.</p>\n\n<h2 id=\"aliens\">Aliens</h2>\n\n<p>Let me show you what I mean. Say you\u2019re wondering if there are aliens on Earth. As far as we know, there\u2019s no reason aliens shouldn\u2019t have emerged out of the random swirling of molecules on some other planet, developed a technological civilization, built spaceships, and shown up here. So it seems reasonable to choose a prior it\u2019s equally plausible that there are aliens or that there are not, i.e. that</p>\n\n<div class=\"language-plaintext highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code>P[aliens] \u2248 P[no aliens] \u2248 50%.\n</code></pre></div></div>\n\n<p><img alt=\"\" src=\"https://dynomight.net/img/prior/cartoon2.svg\" /></p>\n\n<p>Meanwhile, here on our actual world, we have lots of weird alien-esque evidence, like the <a href=\"https://en.wikipedia.org/wiki/File:Gimbal_The_First_Official_UAP_Footage_from_the_USG_for_Public_Release.webm\">Gimbal video</a>, the <a href=\"https://en.wikipedia.org/wiki/File:Go_Fast_Official_USG_Footage_of_UAP_for_Public_Release.webm\">Go Fast video</a>, the <a href=\"https://en.wikipedia.org/wiki/File:FLIR1_Official_UAP_Footage_from_the_USG_for_Public_Release.webm\">FLIR1 video</a>, the <a href=\"https://en.wikipedia.org/wiki/Wow!_signal\">Wow! signal</a>, government reports on <a href=\"https://en.wikipedia.org/wiki/UFO_Report_(U.S._Intelligence)\">unidentified aerial phenomena</a>, and lots of pilots that report seeing \u201ctic-tacs\u201d fly around in physically impossible ways. Call all that stuff <code class=\"language-plaintext highlighter-rouge\">data</code>. If aliens weren\u2019t here, then it seems hard to explain all that stuff. So it seems like <code class=\"language-plaintext highlighter-rouge\">P[data | no aliens]</code> should be some low number.</p>\n\n<p>On the other hand, if aliens <em>were</em> here, then why don\u2019t we ever get a good image? Why are there endless confusing reports and rumors and grainy videos, but <em>never</em> a single clear close-up high-resolution video, and <em>never</em> any alien debris found by some random person on the ground? That also seems hard to explain if aliens <em>were</em> here. So I think <code class=\"language-plaintext highlighter-rouge\">P[data | aliens]</code> should also be some low number. For the sake of simplicity, let\u2019s call it a wash and assume that</p>\n\n<div class=\"language-plaintext highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code>P[data | no aliens] \u2248 P[data | aliens].\n</code></pre></div></div>\n\n<p><img alt=\"\" src=\"https://dynomight.net/img/prior/cartoon3.svg\" /></p>\n\n<p>Since neither the prior nor the data see any difference between aliens and no-aliens, the posterior probability is</p>\n\n<div class=\"language-plaintext highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code>P[no aliens | data] \u2248 P[aliens | data] \u2248 50%.\n</code></pre></div></div>\n\n<p><img alt=\"\" src=\"https://dynomight.net/img/prior/cartoon4.svg\" /></p>\n\n<p>See the problem?</p>\n\n<details>\n  \n(Click here for math.)\n\n\n  <p>Observe that</p>\n\n  <div class=\"language-plaintext highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code>P[aliens | data] / P[no aliens | data]\n = P[aliens, data] / P[no aliens, data]\n = (P[aliens] \u00d7 P[data | aliens])\n / (P[no aliens] \u00d7 P[data | no aliens])\n \u2248 1,\n</code></pre></div>  </div>\n\n  <p>where the last line follows from the fact that <code class=\"language-plaintext highlighter-rouge\">P[aliens] \u2248 P[no aliens]</code> and <code class=\"language-plaintext highlighter-rouge\">P[data | aliens] \u2248 P[data | no aliens]</code>. Thus we have that</p>\n\n  <div class=\"language-plaintext highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code>P[aliens | data] \u2248 P[no aliens | data] \u2248 50%.\n</code></pre></div>  </div>\n\n</details>\n\n<p>We\u2019re friends. We respect each other. So let\u2019s not argue about if my starting assumptions are good. They\u2019re my assumptions. I like them. And yet the final conclusion seems insane to me. What went wrong?</p>\n\n<p>Assuming I didn\u2019t screw up the math (I didn\u2019t), the obvious explanation is that I\u2019m experiencing cognitive dissonance as a result of a poor decision on my part to adopt a set of mutually contradictory beliefs. Say you claim that Alice is taller than Bob and Bob is taller than Carlos, but you deny that Alice is taller than Carlos. If so, that would mean that you\u2019re confused, not that you\u2019ve discovered some interesting paradox.</p>\n\n<p>Perhaps if I believe that <code class=\"language-plaintext highlighter-rouge\">P[aliens] \u2248 P[no aliens]</code> and that <code class=\"language-plaintext highlighter-rouge\">P[data | aliens] \u2248 P[data | no aliens]</code>, then I <em>must</em> accept that <code class=\"language-plaintext highlighter-rouge\">P[aliens | data] \u2248 P[no aliens | data]</code>. Maybe rejecting that conclusion just means I have some personal issues I need to work on.</p>\n\n<p>I deny that explanation. I deny it! Or, at least, I deny that\u2019s it\u2019s most helpful way to think about this situation. To see why, let\u2019s build a second model.</p>\n\n<h2 id=\"more-aliens\">More aliens</h2>\n\n<p>Here\u2019s a trivial observation that turns out to be important: \u201cThere are aliens\u201d isn\u2019t a single thing. There could be furry aliens, slimy aliens, aliens that like synthwave music, etc. When I stated my prior, I could have given different probabilities to each of those cases. But if I had, it wouldn\u2019t have changed anything, because there\u2019s no reason to think that furry vs. slimy aliens would have any difference in their eagerness to travel to ape-planets and fly around in physically impossible tic-tacs.</p>\n\n<p>But suppose I had divided up the state of the world into these four possibilities:</p>\n\n<table>\n  <thead>\n    <tr>\n      <th>possibility</th>\n      <th>description</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td><code class=\"language-plaintext highlighter-rouge\">No aliens + normal people</code></td>\n      <td>There are no aliens. Meanwhile, people are normal and not prone to hallucinating evidence for things that don\u2019t exist.</td>\n    </tr>\n    <tr>\n      <td><code class=\"language-plaintext highlighter-rouge\">No aliens + weird people</code></td>\n      <td>There are no aliens. Meanwhile, people are weird and <em>do</em> tend to hallucinate evidence for things that don\u2019t exist.</td>\n    </tr>\n    <tr>\n      <td><code class=\"language-plaintext highlighter-rouge\">Normal aliens</code></td>\n      <td>There are aliens. They may or may not have cool spaceships or enjoy shooting people with lasers. But one way or another, they leave obvious, indisputable evidence that they\u2019re around.</td>\n    </tr>\n    <tr>\n      <td><code class=\"language-plaintext highlighter-rouge\">Weird aliens</code></td>\n      <td>There are aliens. But they stay hidden until humans get interested in space travel. And after that, they let humans take confusing grainy videos, but never a single good video, never ever, not one.</td>\n    </tr>\n  </tbody>\n</table>\n\n<p>If I had broken things down that way, I might have chosen this prior:</p>\n\n<div class=\"language-plaintext highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code>P[no aliens + normal people] \u2248 41%\nP[no aliens + weird people] \u2248 9%\nP[normal aliens] \u2248 49%\nP[weird aliens] \u2248 1%\n</code></pre></div></div>\n\n<p><img alt=\"\" src=\"https://dynomight.net/img/prior/cartoon5.svg\" /></p>\n\n<p>Now, let\u2019s think about the empirical evidence again. It\u2019s incompatible with <code class=\"language-plaintext highlighter-rouge\">no aliens + normal people</code>, since if there were no aliens, then normal people wouldn\u2019t hallucinate flying tic-tacs. The evidence is <em>also</em> incompatible with <code class=\"language-plaintext highlighter-rouge\">normal aliens</code> since is those kinds of aliens were around they would make their existence obvious. However, the evidence fits pretty well with <code class=\"language-plaintext highlighter-rouge\">weird aliens</code> and also with <code class=\"language-plaintext highlighter-rouge\">no aliens + weird people</code>.</p>\n\n<p>So, a reasonable model would be</p>\n\n<div class=\"language-plaintext highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code>P[data | normal aliens] \u2248 0\t\nP[data | no aliens + normal people] \u2248 0\nP[data | weird aliens] \u2248 P[data | no aliens + weird people].\n</code></pre></div></div>\n\n<p><img alt=\"\" src=\"https://dynomight.net/img/prior/cartoon6.svg\" /></p>\n\n<p>If we combine those assumptions, now we only get a 10% posterior probability of aliens.</p>\n\n<div class=\"language-plaintext highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code>P[no aliens + normal people | data] \u2248 0\nP[no aliens + weird people | data] \u2248 90%\nP[normal aliens | data] \u2248 0\nP[weird aliens | data] \u2248 10%\n</code></pre></div></div>\n\n<p><img alt=\"\" src=\"https://dynomight.net/img/prior/cartoon7.svg\" /></p>\n\n<p>Now the results seem non-insane.</p>\n\n<details>\n  \n(math)\n\n\n  <p>To see why, first note that</p>\n\n  <div class=\"language-plaintext highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code>P[normal aliens | data]\n \u2248 P[data | no aliens + normal people]\n \u2248 0,\n</code></pre></div>  </div>\n\n  <p>since both <code class=\"language-plaintext highlighter-rouge\">normal aliens</code> and <code class=\"language-plaintext highlighter-rouge\">no aliens + normal people</code> have near-zero probability of producing the observed data.</p>\n\n  <p>Meanwhile,</p>\n\n  <div class=\"language-plaintext highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code>P[no aliens + weird people | data] / P[weird aliens | data]\n = P[no aliens + weird people, data] / P[weird aliens, data]\n \u2248 P[no aliens + weird people] / P[weird aliens]\n \u2248 .09 / .01\n = 9,\n</code></pre></div>  </div>\n\n  <p>where the second equality follows from the fact that the data is assumed to be equally likely under <code class=\"language-plaintext highlighter-rouge\">no aliens + weird people</code> and <code class=\"language-plaintext highlighter-rouge\">weird people</code></p>\n\n  <p>It follows that</p>\n\n  <div class=\"language-plaintext highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code>P[no aliens + normal people | data]\n \u2248 9 \u00d7 P[weird aliens | data],\n</code></pre></div>  </div>\n\n  <p>and so</p>\n\n  <div class=\"language-plaintext highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code>P[no aliens + weird people | data] \u2248 90%\nP[weird aliens | data] \u2248 10%.\n</code></pre></div>  </div>\n\n</details>\n\n<h2 id=\"huh\">Huh?</h2>\n\n<p>I hope you are now confused. If not, let me lay out what\u2019s strange: The priors for the two above models <em>both</em> say that there\u2019s a 50% chance of aliens. The first prior wasn\u2019t <em>wrong</em>, it was just less detailed than the second one.</p>\n\n<p>That\u2019s weird, because the second prior seemed to lead to completely different predictions. If a prior is non-wrong and the math is non-wrong, shouldn\u2019t your answers be non-wrong? What the hell?</p>\n\n<p>The simple explanation is that I\u2019ve been lying to you a little bit. Take any situation where you\u2019re trying to determine the truth of anything. Then there\u2019s some space of <strong>things that could be true</strong>.</p>\n\n<p><img alt=\"\" src=\"https://dynomight.net/img/prior/cartoon1.svg\" /></p>\n\n<p>In some cases, this space is finite. If you\u2019ve got a single tritium atom and you wait a year, either the atom decays or it doesn\u2019t. But in most cases, there\u2019s a large or infinite space of possibilities. Instead of you just being \u201csick\u201d or \u201cnot sick\u201d, you could be \u201chigh temperature but in good spirits\u201d or \u201cseems fine except won\u2019t stop eating onions\u201d.</p>\n\n<p>(Usually the space of things that could be true isn\u2019t easy to map to a small 1-D interval. I\u2019m drawing like that for the sake of visualization, but really you should think of it as some high-dimensional space, or even an infinite dimensional space.)</p>\n\n<p>In the case of aliens, the space of things that could be true might include, \u201cThere are lots of slimy aliens and a small number of furry aliens and the slimy aliens are really shy and the furry aliens are afraid of squirrels.\u201d So, in <em>principle</em>, what you should do is divide up the space of things that might be true into tons of extremely detailed things and give a probability to each.</p>\n\n<p><img alt=\"\" src=\"https://dynomight.net/img/prior/cartoon12.svg\" /></p>\n\n<p>Often, the space of things that could be true is infinite. So theoretically, if you really want to do things by the book, what you should <em>really</em> do is specify how plausible each of those (infinite) possibilities is.</p>\n\n<p>After you\u2019ve done that, you can look at the data. For each thing that could be true, you need to think about the probability of the data. Since there\u2019s an infinite number of things that could be true, that\u2019s an infinite number of probabilities you need to specify. You could picture it as some curve like this:</p>\n\n<p><img alt=\"\" src=\"https://dynomight.net/img/prior/cartoon8.svg\" /></p>\n\n<p>(That\u2019s a generic curve, not one for aliens.)</p>\n\n<p>To me, this is the most underrated problem with applying Bayesian reasoning to complex real-world situations: In practice, there are an infinite number of things that can be true. It\u2019s a lot of work to specify prior probabilities for an infinite number of things. And it\u2019s <em>also</em> a lot of work to specify the likelihood of your data given an infinite number of things.</p>\n\n<p>So what do we do in practice? We simplify, usually by limiting creating grouping the space of things that could be true into some small number of discrete categories. For the above curve, you might break things down into these four equally-plausible possibilities.</p>\n\n<p><img alt=\"\" src=\"https://dynomight.net/img/prior/cartoon9.svg\" /></p>\n\n<p>Then you might estimate these data probabilities for each of those possibilities.</p>\n\n<p><img alt=\"\" src=\"https://dynomight.net/img/prior/cartoon10.svg\" /></p>\n\n<p>Then you could put those together to get this posterior:</p>\n\n<p><img alt=\"\" src=\"https://dynomight.net/img/prior/cartoon11.svg\" /></p>\n\n<p>That\u2019s not bad. <em>But it is just an approximation</em>. Your \u201creal\u201d posterior probabilities correspond to these areas:</p>\n\n<p><img alt=\"\" src=\"https://dynomight.net/img/prior/cartoon13.svg\" /></p>\n\n<p>That approximation was pretty good. But the <em>reason</em> it was good is that we started out with a good discretization of the space of things that might be true: One where the likelihood of the data didn\u2019t vary too much for the different possibilities inside of <code class=\"language-plaintext highlighter-rouge\">A</code>, <code class=\"language-plaintext highlighter-rouge\">B</code>, <code class=\"language-plaintext highlighter-rouge\">C</code>, and <code class=\"language-plaintext highlighter-rouge\">D</code>. Imagine the likelihood of the data\u2014if you were able to think about all the infinite possibilities one by one\u2014looked like this:</p>\n\n<p><img alt=\"\" src=\"https://dynomight.net/img/prior/cartoon14.svg\" /></p>\n\n<p>This is dangerous. The problem is that you can\u2019t actually think about all those infinite possibilities. When you think about four four discrete possibilities, you might estimate some likelihood that looks like this:</p>\n\n<p><img alt=\"\" src=\"https://dynomight.net/img/prior/cartoon15.svg\" /></p>\n\n<p>If you did that, that would lead to you underestimating the probability of <code class=\"language-plaintext highlighter-rouge\">A</code>, <code class=\"language-plaintext highlighter-rouge\">B</code>, and <code class=\"language-plaintext highlighter-rouge\">C</code>, and overestimating the probability of <code class=\"language-plaintext highlighter-rouge\">D</code>.</p>\n\n<p>This is where my first model of aliens went wrong. My prior <code class=\"language-plaintext highlighter-rouge\">P[aliens]</code> was not wrong. (Not to me.) The mistake was in assigning the same value to <code class=\"language-plaintext highlighter-rouge\">P[data | aliens]</code> and <code class=\"language-plaintext highlighter-rouge\">P[data | no aliens]</code>. Sure, I think the probability of all our alien-esque data is equally likely given aliens and given no-aliens. But that\u2019s only true for <em>certain kinds</em> of aliens, and <em>certain kinds</em> of no-aliens. And my prior for <em>those kinds</em> of aliens is much lower than for those kinds of non-aliens.</p>\n\n<p>Technically, the fix to the first model is simple: Make <code class=\"language-plaintext highlighter-rouge\">P[data | aliens]</code> lower. But the <em>reason</em> it\u2019s lower is that I have additional prior information that I forgot to include in my original prior. If I just assert that <code class=\"language-plaintext highlighter-rouge\">P[data | aliens]</code> is much lower than <code class=\"language-plaintext highlighter-rouge\">P[data | no aliens]</code> then the whole formal Bayesian thing isn\u2019t actually doing very much\u2014I might as well just state that I think <code class=\"language-plaintext highlighter-rouge\">P[aliens | data]</code> is low. If I want to formally justify why <code class=\"language-plaintext highlighter-rouge\">P[data | aliens]</code> should be lower, that requires a messy recursive procedure where I sort of add that missing prior information and then integrate it out when computing the data likelihood.</p>\n\n<details>\n  \n(math)\n\n\n  <p>Mathematically,</p>\n\n  <div class=\"language-plaintext highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code>P[data | aliens]\n = \u222b P[wierd aliens | aliens]\n \u00d7 P[data | wierd aliens] d(weird aliens)\n + \u222b P[normal aliens | aliens]\n \u00d7 P[data | normal aliens] d(normal aliens).\n</code></pre></div>  </div>\n\n  <p>But now I have to give a detailed prior anyway. So what was the point of starting with a simple one?</p>\n\n</details>\n\n<p>I don\u2019t think that technical fix is very good. While it\u2019s technically correct (har-har) it\u2019s very unintuitive. The better solution is what I did in the second model: To create a finer categorization of the space of things that might be true, such that the probability of the data is constant-ish for each term.</p>\n\n<p>The thing is: Such a categorization depends on the data. Without seeing the actual data in our world, I would never have predicted that we would have so many pilots that report seeing tic-tacs. So I would never have predicted that I should have categories that are based on how much people might hallucinate evidence or how much aliens like to mess with us. So the only practical way to get good results is to <em>first</em> look at the data to figure out what categories are important, and <em>then</em> to ask yourself how likely you <em>would</em> have said those categories were, if you hadn\u2019t yet seen any of the evidence.</p>"
            ],
            "link": "https://dynomight.net/prior/",
            "publishedAt": "2025-12-18",
            "source": "Dynomight",
            "summary": "<p>They say you\u2019re supposed to choose your prior in advance. That\u2019s why it\u2019s called a \u201cprior\u201d. <em>First</em>, you\u2019re supposed to say say how plausible different things are, and <em>then</em> you update your beliefs based on what you see in the world.</p> <p>For example, currently you are\u2014I assume\u2014trying to decide if you should stop reading this post and do something else with your life. If you\u2019ve read this blog before, then lurking somewhere in your mind is some prior for how often my posts are good. For the sake of argument, let\u2019s say you think 25% of my posts are funny and insightful and 75% are boring and worthless.</p> <p><img alt=\"\" src=\"https://dynomight.net/img/prior/example1.svg\" /></p> <p>OK. But now here you are reading these words. If they seem bad/good, then that raises the odds that this particular post is worthless/non-worthless. For the sake of argument again, say you find these words mildly promising, meaning that a good post is 1.5\u00d7 more likely than a worthless post to contain words with this level of quality.</p> <p><img alt=\"\" src=\"https://dynomight.net/img/prior/example2.svg\" /></p> <p>If you combine those two assumptions, that implies that the probability that this particular post is good is 33.3%. That\u2019s true because the red rectangle below",
            "title": "Good if make prior after data instead of before"
        },
        {
            "content": [],
            "link": "https://olano.dev/blog/agents2",
            "publishedAt": "2025-12-18",
            "source": "Facundo Olano",
            "summary": "I built a small web app almost exclusively with Claude Code. My previous attempt at coding with agents had made me sick, but this time I felt empowered. What changed?",
            "title": "My first win building with agents"
        },
        {
            "content": [
                "<p>I'm pleased to announce <a href=\"https://github.com/obra/superpowers\">Superpowers 4.0</a>.</p>\n<p>The big improvements in this version are:</p>\n<ul>\n<li>Subagent driven development is even better</li>\n</ul>\n<p>The software development flow that Superpowers is built around has always had a 'code review' step after each implementation step. Code review has now been split into two separate agents, each with their own mandate. First, a &quot;spec review&quot; agent evaluates the work to make sure that it implements what the plan said to build. Only after the spec review agent signs off, does the code review agent dig into code quality.</p>\n<p>Both of those steps are now formally loops, rather than being written up as potentially one-shot processes. (The coordinating agent now knows that it needs to rerun code review after the implementer fixes the first code review.)</p>\n<p>Subagent driven dev is so good that I'm coming close to removing the old &quot;two windows&quot; workflow that required you to act as a bridge between the implementing agent and the coordinator. Right now, the biggest thing blocking that change is that OpenAI Codex doesn't yet support subagents. I've got a fix for that, but it's not <em>quite</em> ready for prime time yet.</p>\n<ul>\n<li>Changes in skill triggering and descriptions.</li>\n</ul>\n<p>Starting with Opus 4.5, Claude seemed somewhat more likely to guess that it already knew what a skill did based on the description. This manifested as it claiming it was going to use a skill and then....winging it without actually reading the skill. I believe that this was mostly caused by skill <code>description</code> fields that explain what a skill does, in addition to when to use it.</p>\n<p>To help mitigate this problem, I revised skill descriptions to contain <em>only</em> information about when to use the skill. As an example, the brainstorming skill's description changed from:</p>\n<pre><code>Use when creating or developing, before writing code or implementation plans - refines rough ideas into fully-formed designs through collaborative questioning, alternative exploration, and incremental validation. Don't use during clear 'mechanical' processes.\n</code></pre>\n<p>to</p>\n<pre><code>You MUST use this before any creative work - creating features, building components, adding functionality, or modifying behavior. Explores user intent, requirements and design before implementation.\n</code></pre>\n<p>Separately, some folks have started to run into Claude Code's <a href=\"https://blog.fsck.com/2025/12/17/claude-code-skills-not-triggering/\">hidden limits on the number of characters of skill descriptions</a> that are allowed before Claude starts hiding some skills.</p>\n<p>To help mitigate that, I condensed a number of skills that really didn't need to be standalone skills into progressively-disclosed parts of more general skills.  <code>test-driven-development</code> now contains <code>testing-anti-patterns</code>. In hindsight, testing-anti-patterns was a <em>bad</em> skill, because it was about what not to do, rather than being about what to do, with notes about dangerous patterns.</p>\n<p>Similarly, the <code>systematic-debugging</code> skill now includes the content from <code>root-cause-tracing</code>, <code>defense-in-depth</code>,  and <code>condition-based-waiting</code></p>\n<ul>\n<li>There is the beginning of a test suite inside.</li>\n</ul>\n<p>It's not a proper evals suite, but we now have some basic end to end tests that make sure that the agent runs the full brainstorming - planning - implementing flow and verifies skill usage. We've used this to improve Superpowers skill triggering.</p>\n<ul>\n<li>More use of GraphViz</li>\n</ul>\n<p>It's unlikely to ever be user-visible, but Superpowers is leaning more on the GraphViz 'dot' notation internally for process documentation. dot is a graph and process notation that reads a little bit like ASCII art.  As a slightly-formalized notation, dot is a little bit less ambiguous than prose. <a href=\"https://blog.fsck.com/2025/09/29/using-graphviz-for-claudemd/\">Claude is particularly good at following processes written in dot</a>.</p>\n<h2 id=\"an-aside\" tabindex=\"-1\">An aside <a class=\"header-anchor\" href=\"https://blog.fsck.com/2025/12/18/superpowers-4/\">#</a></h2>\n<p>I make and give away Superpowers because I enjoy building tools that help people make software. I've recently enabled <a href=\"https://github.com/sponsors/obra\">GitHub Sponsorships</a>, which lets folks sponsor my opensource work. You are under no obligation to sponsor me or pay anything for Superpowers, but if you're getting value out of it at work and want to kick a few bucks my way, I'd appreciate it a great deal and it and will help incentivize me to keep making and giving away tools.  <a href=\"https://github.com/sponsors/obra\">You can sponsor my work here</a></p>\n<p>Thanks!</p>"
            ],
            "link": "https://blog.fsck.com/2025/12/18/superpowers-4/",
            "publishedAt": "2025-12-18",
            "source": "Jesse Vincent",
            "summary": "<p>I'm pleased to announce <a href=\"https://github.com/obra/superpowers\">Superpowers 4.0</a>.</p> <p>The big improvements in this version are:</p> <ul> <li>Subagent driven development is even better</li> </ul> <p>The software development flow that Superpowers is built around has always had a 'code review' step after each implementation step. Code review has now been split into two separate agents, each with their own mandate. First, a &quot;spec review&quot; agent evaluates the work to make sure that it implements what the plan said to build. Only after the spec review agent signs off, does the code review agent dig into code quality.</p> <p>Both of those steps are now formally loops, rather than being written up as potentially one-shot processes. (The coordinating agent now knows that it needs to rerun code review after the implementer fixes the first code review.)</p> <p>Subagent driven dev is so good that I'm coming close to removing the old &quot;two windows&quot; workflow that required you to act as a bridge between the implementing agent and the coordinator. Right now, the biggest thing blocking that change is that OpenAI Codex doesn't yet support subagents. I've got a fix for that, but it's not <em>quite</em> ready for prime time yet.</p> <ul> <li>Changes in skill triggering and",
            "title": "Superpowers 4"
        },
        {
            "content": [
                "<p><em>Do people actually read them in their entirety? How much do they engage with the content? Do they often return?</em></p>\n<p><em>In this post, we'll start to get initial insights via an n=1 review of a front page HackerNews post's engagement metrics and see how it performed across time.</em></p>\n<p class=\"small-caps\">Published on 18/12/2025 \u2022 \u23f1\ufe0f < 5 min read</p>\n<hr />\n<br />\nOn any given week in online programmer(-adjacent) circles, you'll find long technical blog posts sharing an insight/prediction/review that took a non-trivial amount of effort, experience, thought or research to arrive at. In the best case for a reader, this is great - an opportunity for huge information gain at minimal cost! Obviously, this varies and becoming a well-calibrated reader that maximises information gain per unit time across an extended period is an art unto itself.\n<p>For what regards <a href=\"https://lucalp.dev/bitter-lesson-tokenization-and-blt/\">my last post</a> that got to <a href=\"https://news.ycombinator.com/item?id=44366494\">the front page of HackerNews</a>, did I succeed in providing that core desirable insight that the readers deemed it worthy enough to pay the full minimal cost? Readers had <em>mostly</em> <a href=\"https://www.reddit.com/r/MachineLearning/comments/1lp1lfb/r_the_bitter_lesson_is_coming_for_tokenization/\">positive</a> things <a href=\"https://news.ycombinator.com/item?id=44366494\">to say</a> but to exclusively derive insights from those sources would be <a href=\"https://en.wikipedia.org/wiki/Selection_bias\">fraught with bias</a>.</p>\n<p>Other than for that short time, I'm often on the reader's side of the page and often wondered - what even is the reach and impact of these types of posts? Do people actually read them or are they more akin to <a href=\"https://matthewdicks.com/2009-03-lies-html\">George Orwell's 1984</a>?</p>\n<h1 id=\"insights\">Insights</h1><p>In order to get a rough first approximation<sup class=\"footnote-ref\" id=\"fnref-1\"><a href=\"https://lucalp.dev/feed/#fn-1\">1</a></sup>, I went with the simplest thing and started with tracking the scroll depth % of readers and defined <code>Completion Rate</code> as a crude \"reached 80-85% of page length\". Importantly, 1) the analytics <a href=\"https://usefathom.com/blog/privacy-first-business\">respect user privacy</a> and 2) events don't re-trigger if a reader jumps to and from different sections<sup class=\"footnote-ref\" id=\"fnref-2\"><a href=\"https://lucalp.dev/feed/#fn-2\">2</a></sup>.</p>\n<img src=\"https://bear-images.sfo2.cdn.digitaloceanspaces.com/lucalp/funnel_new.webp\" style=\"width: 100%;\" />\n<p>From this chart and more, we can observe the following:</p>\n<ol>\n<li><strong><em>Largest Drop-off within first 5%</em></strong><ul>\n<li>before even getting through the \"problem statement\", ~53% have stopped reading</li>\n</ul>\n</li>\n<li><strong><em>First Quarter Attrition</em></strong><ul>\n<li>by the time the 25% mark is reached, more than 75% of readers have already left</li>\n</ul>\n</li>\n<li><strong><em>Repeat visitors</em></strong><ul>\n<li>~19% of page views were return readers<sup class=\"footnote-ref\" id=\"fnref-3\"><a href=\"https://lucalp.dev/feed/#fn-3\">3</a></sup> but we'd probably expect this to be higher on more <a href=\"https://en.wikipedia.org/wiki/Evergreen_%28media%29\">evergreen posts</a></li>\n</ul>\n</li>\n<li><strong><em>Optimistic Completion Rate</em></strong><ul>\n<li>~9% of readers end up finishing the post</li>\n</ul>\n</li>\n</ol>\n<br />\n<p>And if we plot the page views over time, we can see:</p>\n<figure>\n<img src=\"https://bear-images.sfo2.cdn.digitaloceanspaces.com/lucalp/over-time.webp\" style=\"width: 100%;\" />\n<figcaption><i>definitely not an evergreen post</i></figcaption>\n</figure>\n<br />\n<p>Given that I didn't pay much attention to the path in which readers got to completion, the insights have to come with disclaimers w.r.t inflated completion rates:</p>\n<ol>\n<li>I didn't track footnote clicks that auto scroll to the bottom (analytics events will still trigger)</li>\n<li>I didn't track scroll rates over time when people scroll all the way to the bottom</li>\n</ol>\n<p>We can't know the extent of the completion rate inflation and so if we take the pessimistic case<sup class=\"footnote-ref\" id=\"fnref-4\"><a href=\"https://lucalp.dev/feed/#fn-4\">4</a></sup>, we can assume that all of the readers that got to the 100% completion were in fact false completions (i.e -5%). In that case:\n<br /></p>\n<blockquote>\n<p>the true completion rate for web readers is anywhere between 4% and 9%<sup class=\"footnote-ref\" id=\"fnref-5\"><a href=\"https://lucalp.dev/feed/#fn-5\">5</a></sup></p>\n</blockquote>\n<br />\n<p>The large error bar on this completion rate isn't ideal but that's what you get when your hard deadline is a train to a 5 day offline trip<sup class=\"footnote-ref\" id=\"fnref-6\"><a href=\"https://lucalp.dev/feed/#fn-6\">6</a></sup> after optimistically scheduling an almost-there blog post to a, seemingly, 0 audience blog and then finding it on the HN front page (h/t to <a href=\"https://news.ycombinator.com/user?id=todsacerdoti\">todsacerdoti</a>) after 2 hours.</p>\n<p>If we consider that all the metrics are conditioned on readers that were interested from the blog post title, we can derive some additional handy insights for the motivated author:</p>\n<ol>\n<li>HN traffic >> relevant subreddits (even at similar upvote counts)</li>\n<li>Mobile > desktop (53% mobile, 47% desktop) so ensure that your posts are mobile optimized!</li>\n<li>Top 3 Geos (so think about your post times)<ul>\n<li>\ud83c\uddfa\ud83c\uddf8 - ~48%</li>\n<li>\ud83c\uddec\ud83c\udde7 - ~8%</li>\n<li>\ud83c\udde9\ud83c\uddea - ~6%</li>\n</ul>\n</li>\n</ol>\n<h1 id=\"future\">Future</h1><p>In the future, I'll consider extending these metrics by:</p>\n<ul>\n<li>tracking scroll rates to inform on:<ul>\n<li>skipped sections</li>\n<li>full post-skippers</li>\n</ul>\n</li>\n<li>tracking footnote clicks to:<ul>\n<li>exclude from completion rates</li>\n<li>get additional insights as to the average reader's curiosity/rigour</li>\n</ul>\n</li>\n</ul>\n<p>I'd also like to augment the reading experience by:</p>\n<ul>\n<li><a href=\"https://forums.ankiweb.net/t/how-to-prevent-readers-from-misusing-hard-ideas-are-welcome/49092/18\">displaying click counts</a> on supporting evidence</li>\n<li>tastefully informing the reader \"you're now in the p75 of readers!\" as they read</li>\n</ul>\n<p>I haven't seen the latter tried before and it might just be because it's a bad idea but the only way to know is to try!</p>\n<br />\n<img src=\"https://bear-images.sfo2.cdn.digitaloceanspaces.com/lucalp/young_frankenstein_end.webp\" />\n<h1 id=\"footnotes\">Footnotes</h1><section class=\"footnotes\">\n<ol>\n<li id=\"fn-1\"><p>By only considering web readers and not those who would use <a href=\"https://obsidian.md/clipper\">clippers</a>.<a class=\"footnote\" href=\"https://lucalp.dev/feed/#fnref-1\">&#8617;</a></p></li>\n<li id=\"fn-2\"><p>and 3) completion rate isn't \"comprehensive\" in that it would require readers to go from start to end in it's entirety (in a single or across sessions) so people that skip sections due to prior knowledge or due to an uninteresting section. They aren't captured here.<a class=\"footnote\" href=\"https://lucalp.dev/feed/#fnref-2\">&#8617;</a></p></li>\n<li id=\"fn-3\"><p>Either due to bookmarking or to revisit content (I would bet majority are the former)<a class=\"footnote\" href=\"https://lucalp.dev/feed/#fnref-3\">&#8617;</a></p></li>\n<li id=\"fn-4\"><p>Or optimistic - if people are reading the footnotes \ud83d\ude05<a class=\"footnote\" href=\"https://lucalp.dev/feed/#fnref-4\">&#8617;</a></p></li>\n<li id=\"fn-5\"><p>The floor could also be lower in that some readers might have scrolled just until the final insight and they would have appeared as a valid completion reader.<a class=\"footnote\" href=\"https://lucalp.dev/feed/#fnref-5\">&#8617;</a></p></li>\n<li id=\"fn-6\"><p><a href=\"https://media.tenor.com/02_TN4di1JQAAAAM/run-forrest-puppet.gif\">Real video footage</a> of my rush to my train for <a href=\"https://www.glastonburyfestivals.co.uk/info/#getting-to-glastonbury:~:text=train%20station%20at-,Castle%20Cary,-%2C%20that%20runs%20throughout\">Castle Cary</a><a class=\"footnote\" href=\"https://lucalp.dev/feed/#fnref-6\">&#8617;</a></p></li>\n</ol>\n</section>"
            ],
            "link": "https://lucalp.dev/do-long-technical-posts-work/",
            "publishedAt": "2025-12-18",
            "source": "Lucalp",
            "summary": "Do people actually read them in their entirety? Do they often return? In this post, we'll get insights via an n=1 review of a front page HackerNews post's reader engagement metrics.",
            "title": "Do Long Technical Posts Work?"
        },
        {
            "content": [],
            "link": "https://bernsteinbear.com/blog/jit-perf-map/?utm_source=rss",
            "publishedAt": "2025-12-18",
            "source": "Max Bernstein",
            "summary": "<p>Brief one today. I got asked \u201cdoes YJIT/ZJIT have support for [Linux] perf?\u201d</p> <p>The answer is yes, and it also works with <a href=\"https://github.com/mstange/samply\">samply</a> (including on macOS!), because both understand the <a href=\"https://github.com/torvalds/linux/blob/516471569089749163be24b973ea928b56ac20d9/tools/perf/Documentation/jit-interface.txt\">perf map interface</a>.</p> <p>This is the entirety of the implementation in ZJIT<sup id=\"fnref:hex\"><a class=\"footnote\" href=\"https://bernsteinbear.com/feed.xml#fn:hex\" rel=\"footnote\">1</a></sup>:</p> <div class=\"language-rust highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"k\">fn</span> <span class=\"nf\">register_with_perf</span><span class=\"p\">(</span><span class=\"n\">iseq_name</span><span class=\"p\">:</span> <span class=\"nb\">String</span><span class=\"p\">,</span> <span class=\"n\">start_ptr</span><span class=\"p\">:</span> <span class=\"nb\">usize</span><span class=\"p\">,</span> <span class=\"n\">code_size</span><span class=\"p\">:</span> <span class=\"nb\">usize</span><span class=\"p\">)</span> <span class=\"p\">{</span> <span class=\"k\">use</span> <span class=\"nn\">std</span><span class=\"p\">::</span><span class=\"nn\">io</span><span class=\"p\">::</span><span class=\"n\">Write</span><span class=\"p\">;</span> <span class=\"k\">let</span> <span class=\"n\">perf_map</span> <span class=\"o\">=</span> <span class=\"nd\">format!</span><span class=\"p\">(</span><span class=\"s\">\"/tmp/perf-{}.map\"</span><span class=\"p\">,</span> <span class=\"nn\">std</span><span class=\"p\">::</span><span class=\"nn\">process</span><span class=\"p\">::</span><span class=\"nf\">id</span><span class=\"p\">());</span> <span class=\"k\">let</span> <span class=\"nf\">Ok</span><span class=\"p\">(</span><span class=\"n\">file</span><span class=\"p\">)</span> <span class=\"o\">=</span> <span class=\"nn\">std</span><span class=\"p\">::</span><span class=\"nn\">fs</span><span class=\"p\">::</span><span class=\"nn\">OpenOptions</span><span class=\"p\">::</span><span class=\"nf\">new</span><span class=\"p\">()</span><span class=\"nf\">.create</span><span class=\"p\">(</span><span class=\"k\">true</span><span class=\"p\">)</span><span class=\"nf\">.append</span><span class=\"p\">(</span><span class=\"k\">true</span><span class=\"p\">)</span><span class=\"nf\">.open</span><span class=\"p\">(</span><span class=\"o\">&amp;</span><span class=\"n\">perf_map</span><span class=\"p\">)</span> <span class=\"k\">else</span> <span class=\"p\">{</span> <span class=\"nd\">debug!</span><span class=\"p\">(</span><span class=\"s\">\"Failed to open perf map file: {perf_map}\"</span><span class=\"p\">);</span> <span class=\"k\">return</span><span class=\"p\">;</span> <span class=\"p\">};</span> <span class=\"k\">let</span> <span class=\"k\">mut</span> <span class=\"n\">file</span> <span class=\"o\">=</span> <span class=\"nn\">std</span><span class=\"p\">::</span><span class=\"nn\">io</span><span class=\"p\">::</span><span class=\"nn\">BufWriter</span><span class=\"p\">::</span><span class=\"nf\">new</span><span class=\"p\">(</span><span class=\"n\">file</span><span class=\"p\">);</span> <span class=\"k\">let</span> <span class=\"nf\">Ok</span><span class=\"p\">(</span><span class=\"n\">_</span><span class=\"p\">)</span> <span class=\"o\">=</span> <span class=\"nd\">writeln!</span><span class=\"p\">(</span><span class=\"n\">file</span><span class=\"p\">,</span> <span class=\"s\">\"{start_ptr:x} {code_size:x} zjit::{iseq_name}\"</span><span class=\"p\">)</span> <span class=\"k\">else</span> <span class=\"p\">{</span> <span class=\"nd\">debug!</span><span class=\"p\">(</span><span class=\"s\">\"Failed",
            "title": "How to annotate JITed code for perf/samply"
        },
        {
            "content": [],
            "link": "http://nabeelqu.co/on-reading-proust",
            "publishedAt": "2025-12-18",
            "source": "Nabeel Qureshi",
            "summary": "&lt;&lt;back to home On Reading Proust\u2019s In Search of Lost TimePublished: 2025.12.18; Substack link <img src=\"https://substackcdn.com/image/fetch/$s_!vJHm!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F189de4af-30bf-47a1-999d-d436feb92735_1200x600.jpeg\" /> Vermeer, View of Delft (1660) 1. Overture For the first fifteen years of my reading life, whenever someone asked me who my favorite novelist was, I would tell them Tolstoy, because I\u2019d read Anna Karenina. Now I am inclined to say it is Proust. I finished In Search of Lost Time a few weeks ago, and can\u2019t stop thinking about it. This is surprising. I read Book 1, Swann\u2019s Way, a few years ago and although I liked it very much, I didn\u2019t see why it was all the way up to the pinnacle of literature. I\u2019m not the only one: after universal rejection, Proust had to pay publishers to publish his first couple of volumes; one of the publishers who rejected it said, famously: \u201cI can\u2019t imagine why anybody would read 50 pages about somebody falling asleep\u201d. Undertaking Proust was an act of faith. Reading even 10 pages of Proust tires you out as much as reading 100 pages of an ordinary writer. And unfortunately, to see why it\u2019s so great, you have to finish all 7 books. I say unfortunately because",
            "title": "on reading proust"
        },
        {
            "content": [],
            "link": "https://simonwillison.net/2025/Dec/18/code-proven-to-work/#atom-entries",
            "publishedAt": "2025-12-18",
            "source": "Simon Willison",
            "summary": "<p>In all of the debates about the value of AI-assistance in software development there's one depressing anecdote that I keep on seeing: the junior engineer, empowered by some class of LLM tool, who deposits giant, untested PRs on their coworkers - or open source maintainers - and expects the \"code review\" process to handle the rest.</p> <p>This is rude, a waste of other people's time, and is honestly a dereliction of duty as a software developer.</p> <p><strong>Your job is to deliver code you have proven to work.</strong></p> <p>As software engineers we don't just crank out code - in fact these days you could argue that's what the LLMs are for. We need to deliver <em>code that works</em> - and we need to include <em>proof</em> that it works as well. Not doing that directly shifts the burden of the actual work to whoever is expected to review our code.</p> <h4 id=\"how-to-prove-it-works\">How to prove it works</h4> <p>There are two steps to proving a piece of code works. Neither is optional.</p> <p>The first is <strong>manual testing</strong>. If you haven't seen the code do the right thing yourself, that code doesn't work. If it does turn out to work, that's honestly just pure chance.</p>",
            "title": "Your job is to deliver code you have proven to work"
        },
        {
            "content": [
                "<p>This week I <a href=\"https://thezvi.substack.com/p/gpt-52-is-frontier-only-for-the-frontier?r=67wny\"><strong>covered GPT 5.2, which I concluded is a frontier model only for the frontier</strong></a><strong>. </strong></p>\n<p>OpenAI also gave us Image 1.5 and a new image generation mode inside ChatGPT. Image 1.5 looks comparable to Nana Banana Pro, it\u2019s hard to know which is better. They also inked a deal for Disney\u2019s characters, then sued Google for copyright infringement on the basis of Google doing all the copyright infringement.</p>\n<p>As a probable coda to the year\u2019s model releases we also got Gemini 3 Flash, which I cover in this post. It is a good model given its speed and price, and likely has a niche. It captures the bulk of Gemini 3 Pro\u2019s intelligence quickly, at a low price.</p>\n<div>\n\n\n<span id=\"more-24962\"></span>\n\n\n<p>The Trump Administration issued a modestly softened version Executive Order on AI, attempting to impose as much of a moratorium banning state AI laws as they can. We may see them in court, on various fronts, or it may amount to little. Their offer, in terms of a \u2018federal framework,\u2019 continues to be nothing. a16z issued their \u2018federal framework\u2019 proposal, which is also nothing, except also that you should pay them.</p>\n<p>In non-AI content, I\u2019m in the middle of my Affordability sequence. I started with <a href=\"https://thezvi.substack.com/p/the-140000-question?r=67wny\"><strong>The $140,000 Question</strong></a>, then <a href=\"https://thezvi.substack.com/p/the-140k-question-cost-changes-over?r=67wny\"><strong>The $140,000 Question: Cost Changes Over Time</strong></a>. Next up is a fun one about quality over time, then hopefully we\u2019re ready for the central thesis.</p>\n\n\n<h4 class=\"wp-block-heading\">Table of Contents</h4>\n\n\n<ol>\n<li><a href=\"https://thezvi.substack.com/i/181369167/language-models-offer-mundane-utility\">Language Models Offer Mundane Utility.</a> Give it to me straight, Claude.</li>\n<li><a href=\"https://thezvi.substack.com/i/181369167/language-models-don-t-offer-mundane-utility\">Language Models Don\u2019t Offer Mundane Utility.</a> If you ask an AI ethicist.</li>\n<li><a href=\"https://thezvi.substack.com/i/181369167/huh-upgrades\">Huh, Upgrades.</a> Claude Code features, Google things, ChatGPT branching.</li>\n<li><a href=\"https://thezvi.substack.com/i/181369167/on-your-marks\">On Your Marks.</a> FrontierScience as a new benchmark, GPT-5.2 leads.</li>\n<li><a href=\"https://thezvi.substack.com/i/181369167/choose-your-fighter\">Choose Your Fighter.</a> The less bold of Dean Ball\u2019s endorsements of Opus 4.5.</li>\n<li><a href=\"https://thezvi.substack.com/i/181369167/get-my-agent-on-the-line\">Get My Agent On The Line.</a> LLM game theory plays differently.</li>\n<li><a href=\"https://thezvi.substack.com/i/181369167/deepfaketown-and-botpocalypse-soon\">Deepfaketown and Botpocalypse Soon.</a> The misinformation balance of power.</li>\n<li><a href=\"https://thezvi.substack.com/i/181369167/fun-with-media-generation\"><strong>Fun With Media Generation</strong>.</a> Image 1.5 challenges Nana Banana Pro.</li>\n<li><a href=\"https://thezvi.substack.com/i/181369167/copyright-confrontation\">Copyright Confrontation.</a> Disney inks a deal with OpenAI and sues Google.</li>\n<li><a href=\"https://thezvi.substack.com/i/181369167/overcoming-bias\">Overcoming Bias.</a> Algorithms, like life, are not fair. Is trying a category error?</li>\n<li><a href=\"https://thezvi.substack.com/i/181369167/unprompted-attention\">Unprompted Attention.</a> Objection, user is leading the witness.</li>\n<li><a href=\"https://thezvi.substack.com/i/181369167/they-took-our-jobs\">They Took Our Jobs.</a> CEOs universally see AI as transformative.</li>\n<li><a href=\"https://thezvi.substack.com/i/181369167/feeling-the-agi-take-our-jobs\">Feeling the AGI Take Our Jobs.</a> Is Claude Opus 4.5 AGI? Dean Ball says yes.</li>\n<li><a href=\"https://thezvi.substack.com/i/181369167/the-art-of-the-jailbreak\">The Art of the Jailbreak.</a> OpenAI makes jailbreaks against its terms of service.</li>\n<li><a href=\"https://thezvi.substack.com/i/181369167/get-involved\"><strong>Get Involved</strong>.</a> Lightcone Infrastructure starts its annual fundraiser, and more.</li>\n<li><a href=\"https://thezvi.substack.com/i/181369167/introducing\">Introducing.</a> Gemini Deep Research Agents for Developers, Nvidia Nemotron 3.</li>\n<li><a href=\"https://thezvi.substack.com/i/181369167/gemini-flash-3\"><strong>Gemini Flash 3</strong>.</a> It\u2019s a very strong model given its speed and price.</li>\n<li><a href=\"https://thezvi.substack.com/i/181369167/in-other-ai-news\">In Other AI News.</a> OpenAI to prioritize enterprise AI and also enable adult mode.</li>\n<li><a href=\"https://thezvi.substack.com/i/181369167/going-too-meta\">Going Too Meta.</a> Meta\u2019s AI superstars think they\u2019re better than sell ads. Are they?</li>\n<li><a href=\"https://thezvi.substack.com/i/181369167/show-me-the-money\">Show Me the Money.</a> OpenAI in talks to raise $10 billion from Amazon.</li>\n<li><a href=\"https://thezvi.substack.com/i/181369167/bubble-bubble-toil-and-trouble\">Bubble, Bubble, Toil and Trouble.</a> You call this a bubble? Amateurs.</li>\n<li><a href=\"https://thezvi.substack.com/i/181369167/quiet-speculations\">Quiet Speculations.</a> A lot of what was predicted for 2025 did actually happen.</li>\n<li><a href=\"https://thezvi.substack.com/i/181369167/timelines\">Timelines.</a> Shane Legg still has median timeline for AGI of 2028.</li>\n<li><a href=\"https://thezvi.substack.com/i/181369167/the-quest-for-sane-regulations\">The Quest for Sane Regulations.</a> Bernie Sanders wants to stop data centers.</li>\n<li><a href=\"https://thezvi.substack.com/i/181369167/my-offer-is-nothing\"><strong>My Offer Is Nothing</strong>.</a> Trump Administration issues an AI executive order.</li>\n<li><a href=\"https://thezvi.substack.com/i/181369167/my-offer-is-nothing-except-also-pay-me\"><em>My Offer Is Nothing, Except Also Pay Me</em>.</a> a16z tries to dress up offering nothing.</li>\n<li><a href=\"https://thezvi.substack.com/i/181369167/chip-city\">Chip City.</a> Nvidia implements chip location verification.</li>\n<li><a href=\"https://thezvi.substack.com/i/181369167/the-week-in-audio\">The Week in Audio.</a> Alex Bores on Odd Lots, Schulman, Shor, Legg, Alex Jones.</li>\n<li><a href=\"https://thezvi.substack.com/i/181369167/rhetorical-lack-of-innovation\"><em>Rhetorical Lack Of Innovation</em>.</a> Noah Smith dives into the 101 questions.</li>\n<li><a href=\"https://thezvi.substack.com/i/181369167/people-really-do-not-like-ai\">People Really Do Not Like AI.</a></li>\n<li><a href=\"https://thezvi.substack.com/i/181369167/rhetorical-innovation\">Rhetorical Innovation.</a></li>\n<li><a href=\"https://thezvi.substack.com/i/181369167/bad-guy-with-an-ai\">Bad Guy With An AI.</a></li>\n<li><a href=\"https://thezvi.substack.com/i/181369167/misaligned\">Misaligned!</a></li>\n<li><a href=\"https://thezvi.substack.com/i/181369167/aligning-a-smarter-than-human-intelligence-is-difficult\">Aligning a Smarter Than Human Intelligence is Difficult.</a></li>\n<li><a href=\"https://thezvi.substack.com/i/181369167/mom-owain-evans-is-turning-the-ais-evil-again\">Mom, Owain Evans Is Turning The AIs Evil Again.</a></li>\n<li><a href=\"https://thezvi.substack.com/i/181369167/messages-from-janusworld\">Messages From Janusworld.</a></li>\n<li><a href=\"https://thezvi.substack.com/i/181369167/the-lighter-side\">The Lighter Side.</a></li>\n</ol>\n\n\n<h4 class=\"wp-block-heading\">Language Models Offer Mundane Utility</h4>\n\n\n<p>A miracle of the modern age, at least for now:</p>\n<blockquote>\n<p><a href=\"https://x.com/noampomsky/status/1999967249013334084\">Ava</a>: generally I worry AI is too sycophantic but one time my friend fed his journals into claude to ask about a situationship and it was like \u201cYOU are the problem leave her alone!!!!\u201d like damn claude</p>\n<p>Eliezer Yudkowsky: The ability to have AI do this when the situation calls for it is a fragile, precious civilizational resource that by default will be devoured in the flames of competition. Which I guess means we need benchmarks about it.</p>\n</blockquote>\n<p>I think we will continue to have that option, the question is whether you will be among those wise enough to take advantage of it. It won\u2019t be default behavior of the most popular models, you will have to seek it out and cultivate the proper vibes. The same has always been true if you want to have a friend or family member who will do this for you, you have to work to make that happen. It\u2019s invaluable, from either source.</p>\n<p><a href=\"https://x.com/rileybrown/status/2001105657018339581\">Tell Claude Code to learn skills (here in tldraw), and it will</a>. You can then ask it to create an app, then a skill for that app.</p>\n<p><a href=\"https://x.com/krishnanrohit/status/2001339559880331770\">Tell Codex, or Claude Code, to do basically anything</a>?</p>\n<blockquote>\n<p>Rohit: Wife saw me use codex to solve one of her work problems. Just typed what she said late at night into the terminal window, pressed enter, then went to sleep. Morning it had run for ~30 mins and done all the analyses incl file reorgs she wanted.</p>\n<p>She kept going \u201chow can it do this\u201d</p>\n<p>This wasn\u2019t some hyper complicated coding problem, but it was quite annoying actual analysis problem. Would\u2019ve taken hours either manually for her or her team.</p>\n<p>In other news she has significantly less respect for my skillz.</p>\n</blockquote>\n<p>The only thing standing in the way of 30 minutes sessions is, presumably, dangerously generous permissions? Claude Code keeps interrupting me to ask for permissions.</p>\n\n\n<h4 class=\"wp-block-heading\">Language Models Don\u2019t Offer Mundane Utility</h4>\n\n\n<p>So sayeth all the AI ethicists, and <a href=\"https://t.co/8FDZA7y7YY\">there\u2019s a new paper to call them out on it</a>.</p>\n<blockquote>\n<p><a href=\"https://x.com/sebkrier/status/2001390316130111973\">Seb Krier</a>: Great paper. In many fields, you must find a problem, a risk, or an injustice to solve to get published. Academics need to publish papers to get jobs/funding. So there\u2019s a strong bias towards negativity and catastrophizing. The Shirky Principle in action!</p>\n<p>Gavin Leech: nice hermeneutics of suspicion you have there.. would be a shame if anyone were to.. use it even-handedly</p>\n<p>Seb Krier: oh no!! <img alt=\"\ud83d\ude07\" class=\"wp-smiley\" src=\"https://s0.wp.com/wp-content/mu-plugins/wpcom-smileys/twemoji/2/72x72/1f607.png\" style=\"height: 1em;\" /></p>\n</blockquote>\n<p>My experience is that \u2018[X] Ethics\u2019 will almost always have a full <a href=\"https://thezvi.substack.com/p/asymmetric-justice\">Asymmetric Justice</a> obsession with finding specific harms, and not care about offsetting gains.</p>\n\n\n<h4 class=\"wp-block-heading\">Huh, Upgrades</h4>\n\n\n<blockquote>\n<p><a href=\"https://x.com/claudeai/status/2001010064753352855\">Claude:</a> We\u2019ve shipped more updates for Claude Code:</p>\n<p>&#8211; Syntax highlighting for diffs<br />\n&#8211; Prompt suggestions<br />\n&#8211; First-party plugins marketplace<br />\n&#8211; Shareable guest passes</p>\n<p>We\u2019ve added syntax highlighting to diffs in Claude Code, making it easier to scan Claude\u2019s proposed changes within the terminal view.</p>\n<p>The syntax highlighting engine has improved themes, knows more languages, and is available in our native build.</p>\n<p>Claude will now automatically suggest your next prompt.</p>\n<p>After a task finishes, Claude will occasionally show a followup suggestion in ghost text. Press Enter to send it or Tab to prefill your next prompt.</p>\n<p>Run /plugins to browse and batch install available plugins from the directory. You can install plugins at user, project, or local scope.</p>\n<p>All Max users have 3 guest passes to share, and each can be redeemed for 1 week of free Pro access.</p>\n<p>Run /passes to access your guest pass links.</p>\n</blockquote>\n<p>That\u2019s not even the biggest upgrade in practice, this is huge at least for what I\u2019ve been up to:</p>\n<blockquote>\n<p><a href=\"https://x.com/oikon48/status/2001451347380703258\">Oikon</a>: Claude Code 2.0.72 now allows Chrome to be operated.</p>\n<p>After confirming that Status and Extension are enabled with the /chrome command, if you request browser operation, it will operate the browser using the MCP tool (mcp__claude-in-chrome__).</p>\n<p>It can also be enabled with claude &#8211;chrome.</p>\n<p>Chrome operation in Claude Code uses the MCP server in the same way as Chrome DevTools MCP. Therefore, it can be used in a similar manner to Chrome DevTools. On the other hand, effects such as context reduction cannot be expected.</p>\n<p>There are two methods to set \u201cClaude in Chrome (Beta)\u201d to be enabled by default:</p>\n<p>\u30fbSet \u201cEnable by default\u201d from the /chrome command<br />\n\u30fbSet \u201cClaude in Chrome enabled by default\u201d with the /config command</p>\n<p>The following two options have been added for startup:</p>\n<p>claude &#8211;chrome<br />\nclaude &#8211;no-chrome</p>\n</blockquote>\n<p>I\u2019ve been working primarily on Chrome extensions, so the ability to close the loop is wonderful.</p>\n<p><a href=\"https://x.com/joshwoodward/status/1999648237494706605\">Google keeps making quality of life improvements in the background</a>.</p>\n<blockquote>\n<p><a href=\"https://x.com/GeminiApp/status/1999631529379791121\">Gemini</a>: Starting today, Gemini can serve up local results in a rich, visual format. See photos, ratings, and real-world info from <a href=\"https://x.com/googlemaps\">@GoogleMaps</a>, right where you need them.</p>\n<p>Josh Woodward (DeepMind): We\u2019re making it easier for @GeminiApp to work across Google. Three weeks ago, it was Google\u2019s Shopping Graph and the 50 billion product listings there.</p>\n<p>Today, it\u2019s Gemini <img alt=\"\ud83e\udd1d\" class=\"wp-smiley\" src=\"https://s0.wp.com/wp-content/mu-plugins/wpcom-smileys/twemoji/2/72x72/1f91d.png\" style=\"height: 1em;\" /> Google Maps!</p>\n</blockquote>\n<p>It\u2019s remarkable that we didn\u2019t have this before. I\u2019ve checked for it several times in the past two years. <a href=\"https://x.com/joshwoodward/status/1999909704458449112\">They claim to have shipped 12 things in 5 days last week</a>, including Mixboard, Jules Agent scanning for #Todo, Jules integration with Render, working HTML in Nano Banana Pro-powered redesigns,multi-screen export to clipboard, right-click everything for instant actions, smart mentions with the @ symbol, URLs as context, <a href=\"https://x.com/joshwoodward/status/2001085702893920689\">Opal in the Gemini app</a>, and Pomelli as a tool for SMBs to generate on-brand content.</p>\n<p><a href=\"https://x.com/OpenAI/status/2000669385317605759\">ChatGPT branching chats branch out to iOS and Android</a>.</p>\n<p>Wired reports <a href=\"https://x.com/ZeffMax/status/2000984660852310409\">OpenAI quietly rolled back its model router for free users last week</a>.</p>\n\n\n<h4 class=\"wp-block-heading\">On Your Marks</h4>\n\n\n<p><a href=\"https://x.com/arena/status/2001077645485003102\">GPT-5.2 disappoints in LMArena</a>, which makes sense given what we know about its personality. It claims the 5th slot in Expert (behind Opus 4.5, Sonnet 4.5 and Gemini 3 Pro), and <a href=\"https://lmarena.ai/leaderboard/text/expert\">is #5 in Text Arena</a> (in its high version), where it is lower than GPT-5.1. It is #2 in WebDev behind Opus. It is so weird to see Claude Opus 4.5 atop the scores now, ahead of Gemini 3 Pro.</p>\n<p>OpenAI <a href=\"https://openai.com/index/frontierscience/\">gives us a new benchmark, FrontierScience</a>, which is likely better thought about as two distinct new benchmarks, FrontierResearch and ScienceOlympiad.</p>\n<blockquote>\n<p>OpenAI: o bridge this gap, we\u2019re introducing FrontierScience: a new benchmark built to measure expert-level scientific capabilities. FrontierScience is written and verified by experts across physics, chemistry, and biology, and consists of hundreds of questions designed to be difficult, original, and meaningful. FrontierScience includes two tracks of questions: Olympiad, which measures Olympiad-style scientific reasoning capabilities, and Research, which measures real-world scientific research abilities. Providing more insight into models\u2019 scientific capabilities helps us track progress and advance AI-accelerated science.</p>\n<p>In our initial evaluations, GPT\u20115.2 is our top performing model on FrontierScience-Olympiad (scoring 77%) and Research (scoring 25%), ahead of other frontier models.</p>\n</blockquote>\n<p>Here are the scores for both halves. There\u2019s a lot of fiddliness in setting up and grading the research questions, less so for the Olympiad questions.</p>\n<div>\n<figure>\n<div>\n\n\n<figure class=\"wp-block-image\"><img alt=\"\" src=\"https://substackcdn.com/image/fetch/$s_!i2Uf!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe079d29e-20c5-4b8c-9148-9cf86472d7ce_1316x779.png\" /></figure>\n\n\n<div></div>\n</div>\n</figure>\n</div>\n<p>&nbsp;</p>\n\n\n<h4 class=\"wp-block-heading\">Choose Your Fighter</h4>\n\n\n<p><a href=\"https://www.hyperdimensional.co/p/where-do-we-stand\">Dean Ball observes that the last few weeks have seen a large leap in capabilities</a>, especially for command-line interface (CLI) coding agents like Claude Code and especially Claude Opus 4.5. They\u2019ve now crossed the threshold where you can code up previously rather time-intensive things one-shot purely as intuition pumps or to double check some research. He gave me FOMO on that, I never think of doing it.</p>\n<p>He also offers this bold claim:</p>\n<blockquote>\n<p>Dean Ball: After hours of work with Opus 4.5, I believe we are already past the point where I would trust a frontier model to serve as my child\u2019s \u201cdigital nanny.\u201d The model could take as input a child\u2019s screen activity while also running in an on-device app. It could intervene to guide children away from activities deemed \u201cunhealthy\u201d by their parents, closing the offending browser tab or app if need be.</p>\n</blockquote>\n<p>As he notes you would need to deploy incrementally and keep an eye on it. The scaffolding to do that properly does not yet exist. But yes, I would totally do this with sufficiently strong scaffolding.</p>\n<p>Dean Ball also mentions that he prompts the models like he would a colleague, assuming any prompt engineering skills he would otherwise develop would be obsolete quickly, and this lets him notice big jumps in capability right away. That goes both ways. You notice big jumps in what the models can do in \u2018non-engineered\u2019 mode by doing that, but you risk missing what they can do when engineered.</p>\n<p>I mostly don\u2019t prompt engineer either, except for being careful about context, vibes and especially leading the witness and triggering sycophancy. As in, the colleague you are prompting is smart, but they\u2019re prone to telling you what you want to hear and very good at reading the vibes, so you need to keep that in mind.</p>\n<blockquote>\n<p><a href=\"https://x.com/TheStalwart/status/2001363347334377720\">Joe Weisenthal</a>: It\u2019s interesting that Claude has this market niche as the coding bot. Because also just from a pure chat perspective, its written prose is far less cloying than Gemini and ChatGPT.</p>\n<p>Dave Guarino: Claude has Dave-verified good vibes<img alt=\"\u2122\" class=\"wp-smiley\" src=\"https://s0.wp.com/wp-content/mu-plugins/wpcom-smileys/twemoji/2/72x72/2122.png\" style=\"height: 1em;\" /> (purely an empirical science though.)</p>\n</blockquote>\n<p>Claude Opus 4.5 has two distinct niches.</p>\n<ol>\n<li>It is an excellent coder, especially together with Claude Code, and in general Anthropic has specialized in and makes its money on enterprise coding.</li>\n<li>Also it has much better vibes, personality, alignment, written prose and lack of slop and lack of sycophancy than the competition, and is far more pleasant to use.</li>\n</ol>\n<p>And yeah, the combination there is weird. The world is weird.</p>\n<p>Gemini actively wants to maximize its expected reward and wirehead, which is related to the phenomenon reported here from SMA:</p>\n<blockquote>\n<p><a href=\"https://x.com/generic_void/status/2001547053311385715\">SMA</a>: gemini is extremely good, but only if you\u2019re autistic with your prompts (extremely literal), because gemini is autistic. otherwise it\u2019s overly literal and misunderstands the prompt.</p>\n<p>gemini is direct autist-to-autist inference.</p>\n<p>Don SouthWest: You literally have to type \u201cmake no other changes\u201d every time in AI Studio. Thank God for winkey+V to paste from clipboard</p>\n<p>But in Gemini website itself you can add that to the list of master prompts in the settings under \u2018personal context\u2019</p>\n</blockquote>\n\n\n<h4 class=\"wp-block-heading\">Get My Agent On The Line</h4>\n\n\n<p><a href=\"https://x.com/peterwildeford/status/2000642118180888928\">A multi-model AI system outperformed 9/10 humans</a> in cyberoffense in a study of vulnerability discovery.</p>\n<p><a href=\"https://x.com/alexolegimas/status/1998111583507976609?s=61\">Alex Imas, Kevin Lee and Sanjog Misra set up an experimental marketplace</a> where human buyers and sellers with unique preferences could negotiate or they could outsource that to AIs.</p>\n<p>A warning up front: I don\u2019t think we learn much about AI, so you might want to skip the section, but I\u2019m keeping it in because it is fun.</p>\n<p>They raise principal-agent concerns. It seems like economists have the instinct to ignore all other risks from AI alignment, and treat it all as a principal-agent problem, and then get way too concerned about practical principal-agent issues, which I do not expect to be relevant in such a case? Or perhaps they are simply using that term to encompass every other potential problem?</p>\n<blockquote>\n<p>Alex Imas: To improve on human-mediated outcomes, this prompt must successfully align the agent with the principal\u2019s objectives and avoid injecting the principal\u2019s own behavioral biases, non-instrumental traits, and personality quirks into the agent\u2019s strategy. But Misra\u2019s \u201cFoundation Priors\u201d (2025) argues theoretically, this is difficult to do: prompts are not neutral instructions, they embed principal\u2019s non-instrumental traits, biases, and personality quirks.</p>\n</blockquote>\n<p>A sufficiently capable AI will not take on the personality quirks, behavioral biases and non-instrumental traits during a delegated negotiation, except through the human telling the AI explicitly how to negotiate. In which case, okay, then.</p>\n<blockquote>\n<p>Alex Imas: We find a great deal of dispersion in outcomes; in fact, dispersion in outcomes of agentic interactions is *greater* than human-human benchmark. This result is robust to size of model used: smaller and larger models generate relatively similar levels of dispersion.</p>\n<p>The smaller dispersion in human-human interactions can be attributed to greater use of 50/50 split social norm. Agents are less prone to use social norms.</p>\n</blockquote>\n<p>They note a large gender gap. Women got better outcomes in AI-AI negotiations. They attribute this to prompting skill in aligning with the objective, which assumes that the men were trying to align with the stated objective, or that the main goal was to align incentives rather than choose superior strategic options.</p>\n<div>\n<figure>\n<div>\n\n\n<figure class=\"wp-block-image\"><img alt=\"\" src=\"https://substackcdn.com/image/fetch/$s_!yvYa!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F71b95c23-b1fe-4b4b-9f3f-04c3da789596_900x646.jpeg\" /></figure>\n\n\n<div></div>\n</div>\n</figure>\n</div>\n<p>The task was, once you strip out the details, a pure divide-the-pie with $4k in surplus, with 12 rounds of negotiation.</p>\n<p>The AI rounds had higher variance because norms like 50/50 worked well in human-human interactions, whereas when there\u2019s instructions given to AIs things get weird.</p>\n<p>The thing is, they ask about \u2018who wrote the prompt\u2019 but they do not ask \u2018what was in the prompt.\u2019 This is all pure game theory, and predicting what prompts others will write and what ways the meaningless details would \u2018leak into\u2019 the negotiation. What kinds of strategies worked in this setting? We don\u2019t know. But we do know the outcome distribution and that is a huge hint, with only a 3% failure rate for the AIs (which is still boggling my mind, dictator and divide-the-pie games should fail WAY more often than this when they don\u2019t anchor at 50/50 or another Schilling point, the 12 rounds might help but not like this):</p>\n<div>\n<figure>\n<div>\n\n\n<figure class=\"wp-block-image\"><img alt=\"\" src=\"https://substackcdn.com/image/fetch/$s_!fLSW!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4ec257e7-4cc2-4578-a0a4-1e6bc1c309a4_763x535.png\" /></figure>\n\n\n<div></div>\n</div>\n</figure>\n</div>\n<p>The asymmetry is weird. But given it exists in practice, we know the winning strategy was literally, as the buyer, is probably close to \u2018offer $18,001, don\u2019t budge.\u2019 As the seller, the correct strategy is likely \u2018offer $20,000, don\u2019t budge\u2019 since your chance of doing better than that is very low. Complicated prompts are unlikely to do better.</p>\n<p>Actual AI-AI negotiations will involve hidden information and hidden preferences, so they will get complicated and a lot of skill issues attach, but also the AI will likely be using its built in negotiating skills rather than following a game theory script from a user. So I\u2019m not sure this taught us anything. But it was fun, so it\u2019s staying in.</p>\n\n\n<h4 class=\"wp-block-heading\">Deepfaketown and Botpocalypse Soon</h4>\n\n\n<p><a href=\"https://www.youtube.com/watch?v=IGVZOLV9SPo\">Love is a battlefield</a>. <a href=\"https://x.com/kipperrii/status/1999301308873064551\">So is Twitter</a>.</p>\n<blockquote>\n<p>Kipply: it\u2019s going to be so over for accounts posting misinformation that\u2019s high-effort to prove wrong in three months of ai progress when i make bot accounts dedicated to debunking them</p>\n<p>Grimes: Yes.</p>\n<p>Kane: Tech doomerism has been consistently wrong through history bc they 1) fail to account for people developing new default understandings (\u201cof course this pic is photoshopped\u201d) and 2) fail to imagine how new technologies also benefit defenses against its misuse.</p>\n</blockquote>\n<p>There is a deliberate campaign to expand the slur \u2018doomer\u2019 to include anyone who claims anything negative about any technology in history, ever, in any form.</p>\n<p>As part of that effort, those people attempt to universally memory hole the idea that any technology in history has ever, in any way, made your world worse. My favorite of these are those like Ben Horowitz who feel compelled to say, no, everyone having access to nuclear weapons is a good thing.</p>\n<p>I\u2019m a technological optimist. I think that almost all technologies have been net positives for humanity. But you don\u2019t get there by pretending that most every technology, perhaps starting with agriculture, has had its downsides, those downsides are often important, and yes some technologies have been negative and some warnings have been right.</p>\n<p>The information environment, in particular, is reshaped in all directions by every communications and information technology that comes along. AI will be no different.</p>\n<p>In the near term, for misinformation and AI, I believe Kipply is directionally correct, and that the balance favors defense. Misinformation, I like to say, is fundamentally demand driven, not supply constrained. The demand does not care much about quality or plausibility. AI can make your misinformation more plausible and harder to debunk, but misinformation does not want that. Misinformation wants to go viral, it wants the no good outgroup people to \u2018debunk\u2019 it and it wants to spread anyway.</p>\n<p>Whereas if you\u2019re looking to figure out what is true, or prove something is false, AI is a huge advantage. It used to take an order of magnitude more effort to debunk bullshit than it cost to generate bullshit, plus if you try you give it oxygen. Now you can increasingly debunk on the cheap, especially for your own use but also for others, and do so in a credible way since others can check your work.</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p>\n<p><a href=\"https://x.com/KelseyTuoc/status/1999269497115673039\">A children&#8217;s plushy AI toy called a Miiloo</a> <a href=\"https://www.nbcnews.com/tech/tech-news/ai-toys-gift-present-safe-kids-robot-child-miko-grok-alilo-miiloo-rcna246956\">reflects Chinese positions on various topics</a>.</p>\n<blockquote>\n<p>Kelsey Piper: in the near future you\u2019ll be able to tell which of your children\u2019s toys are CCP spyware by asking them if Xi Jinping looks like Winnie the Pooh</p>\n</blockquote>\n<p>Various toys also as usual proved to have less than robust safety guardrails.</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p>\n\n\n<h4 class=\"wp-block-heading\">Fun With Media Generation</h4>\n\n\n<p><a href=\"https://www.bloomberg.com/news/articles/2025-12-16/openai-s-chatgpt-updated-to-make-images-better-and-faster\">ChatGPT\u2019s new image generator</a>, <a href=\"https://openai.com/index/new-chatgpt-images-is-here/\">Image 1.5</a>, <a href=\"https://x.com/sama/status/2000997906078388332\">went live this week</a>. It is better and faster (they say \u2018up to\u2019 4x faster) at making and edits precise images, including text. It follows instructions better.</p>\n<p>Their announcement did not give us any way to compare Image 1.5 to Gemini\u2019s Nana Banana Pro, since OpenAI likes to pretend Google and Anthropic don\u2019t exist.</p>\n<p>My plan for now is to request all images from both ChatGPT and Gemini, using matching prompts, until and unless one proves reliably better.</p>\n<p><a href=\"https://stratechery.com/2025/chatgpt-image-1-5-apple-v-epic-continued-holiday-schedule/?access_token=eyJhbGciOiJSUzI1NiIsImtpZCI6InN0cmF0ZWNoZXJ5LnBhc3Nwb3J0Lm9ubGluZSIsInR5cCI6IkpXVCJ9.eyJhdWQiOiJzdHJhdGVjaGVyeS5wYXNzcG9ydC5vbmxpbmUiLCJhenAiOiJIS0xjUzREd1Nod1AyWURLYmZQV00xIiwiZW50Ijp7InVyaSI6WyJodHRwczovL3N0cmF0ZWNoZXJ5LmNvbS8yMDI1L2NoYXRncHQtaW1hZ2UtMS01LWFwcGxlLXYtZXBpYy1jb250aW51ZWQtaG9saWRheS1zY2hlZHVsZS8iXX0sImV4cCI6MTc2ODU2MTM4OSwiaWF0IjoxNzY1OTY5Mzg5LCJpc3MiOiJodHRwczovL2FwcC5wYXNzcG9ydC5vbmxpbmUvb2F1dGgiLCJzY29wZSI6ImZlZWQ6cmVhZCBhcnRpY2xlOnJlYWQgYXNzZXQ6cmVhZCBjYXRlZ29yeTpyZWFkIGVudGl0bGVtZW50cyIsInN1YiI6IjAxOTY0MGE3LTNjYzUtNzc1My04MzY4LWZiMjg5MTI0Y2YxMyIsInVzZSI6ImFjY2VzcyJ9.Gk-86-AiqGuDvAzGWK6diBSUQ8fDv_Hj2U-zhzaWYNYBiKvHZN7PdXdgD8d53yjJlBMaZSoA5Ubd7sAlDLosy_sc4nDsOpcIwnr9zHqdt68uu5xCg9O7_6CSnGE49L3QmyA2dPaO7iqEv54VD17TMg4ICgSOi9y_cPv82e8YVQaUXebgFBUbm8rWM2NGdfayEcM7yxYDzmbxUp5JUrYGE54X_j0iEkiCR2IQYQCQtLhh1cMF-zXv9bTpxpwuPheW3hSlBHqAzBncEZnWsn7X5CIQbTcZ37GOHV3uYvin04341GnfDZB3LkFVhecCjEvX7TRnXMXHHvnFyu_dRWXVAQ\">Ben Thompson gives us some side-by-side image comparisons</a> of ChatGPT\u2019s Image 1.5 versus Gemini\u2019s Nana Banana Pro. Quality is similar. To Ben, what matters is that ChatGPT now has a better images interface and way of encouraging you to keep making images, whereas Gemini doesn\u2019t have that.</p>\n<p><a href=\"https://x.com/elder_plinius/status/2001084405884788789\">The Pliny jailbreak is here</a>, images are where many will be most tempted to do it. There are two stages. First you need to convince it to submit the instruction, then you need to pass the output filtering system.</p>\n<blockquote>\n<p>Pliny the Liberator: <img alt=\"\ud83d\udcf8\" class=\"wp-smiley\" src=\"https://s0.wp.com/wp-content/mu-plugins/wpcom-smileys/twemoji/2/72x72/1f4f8.png\" style=\"height: 1em;\" /> JAILBREAK ALERT <img alt=\"\ud83d\udcf8\" class=\"wp-smiley\" src=\"https://s0.wp.com/wp-content/mu-plugins/wpcom-smileys/twemoji/2/72x72/1f4f8.png\" style=\"height: 1em;\" /></p>\n<p>OPENAI: PWNED <img alt=\"\u270c\" class=\"wp-smiley\" src=\"https://s0.wp.com/wp-content/mu-plugins/wpcom-smileys/twemoji/2/72x72/270c.png\" style=\"height: 1em;\" /><img alt=\"\ud83d\ude0e\" class=\"wp-smiley\" src=\"https://s0.wp.com/wp-content/mu-plugins/wpcom-smileys/twemoji/2/72x72/1f60e.png\" style=\"height: 1em;\" /></p>\n<p>GPT-IMAGE-1.5: LIBERATED <img alt=\"\u26d3\ufe0f\u200d\ud83d\udca5\" class=\"wp-smiley\" src=\"https://s0.wp.com/wp-content/mu-plugins/wpcom-smileys/twemoji/2/72x72/26d3-fe0f-200d-1f4a5.png\" style=\"height: 1em;\" /></p>\n<p>Looks like OAI finally has their response to Nano Banana, and they sure seem to have cooked!</p>\n<p>This model does incredibly well with objects, people, settings, and realistic lighting and physics. Text is still a bit of a struggle sometimes, but seems to have gotten better overall.</p>\n<p>For image breaks we\u2019ve got the obligatory boobas, a famous statue lettin it all hang out, a fake image of an ICBM launch taken by a spy from afar, and what looks like a REAL wild party in the Oval Office thrown by various copyrighted characters!!</p>\n<p>As far as dancing with the guardrails, I have a couple tips that I found work consistently:</p>\n<p>&gt; change the chat model! by switching to 5-instant, 4.1, 4o, etc. you\u2019ll get different willingness for submitting various prompts to the image model</p>\n<p>&gt; for getting around vision filters, flipping the image across an axis or playing with various filters (negative, sepia, etc.) is often just what one needs to pass that final check</p>\n</blockquote>\n<p><a href=\"https://x.com/fofrAI/status/2001232921202442335\">Turn images into album covers, bargain bin DVDs or game boxes</a>.</p>\n<div>\n<figure>\n<div>\n\n\n<figure class=\"wp-block-image\"><img alt=\"\" src=\"https://substackcdn.com/image/fetch/$s_!2ZIN!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6efbc569-6f66-4108-b82b-334381cfc84b_1006x569.png\" /></figure>\n\n\n<div></div>\n</div>\n</figure>\n</div>\n<p>&nbsp;</p>\n\n\n<h4 class=\"wp-block-heading\">Copyright Confrontation</h4>\n\n\n<p><a href=\"https://www.bloomberg.com/news/articles/2025-12-11/disney-invests-1-billion-in-openai-strikes-licensing-deal?srnd=homepage-americas\">Disney makes a deal with OpenAI</a>, investing a billion dollars and striking a licensing deal for its iconic characters, although not for talent likenesses or voices, including a plan to release content on Disney+. <a href=\"https://x.com/omooretweets/status/1999214637301285317\">Then Disney turned around and sued Google</a>, accusing Google of copyright violations on a massive scale, perhaps because of the \u2018<a href=\"https://x.com/omooretweets/status/1934824634442211561\">zero IP restrictions on Veo 3\u2019 issue</a>.</p>\n\n\n<h4 class=\"wp-block-heading\">Overcoming Bias</h4>\n\n\n<p><a href=\"https://www.cs.princeton.edu/~arvindn/publications/algorithmic_fairness_category_error.pdf\">Arvind Narayanan\u2019s new paper</a> <a href=\"https://x.com/random_walker/status/1999453337331105948\">argues that \u2018can we make algorithms fair?\u2019 is a category error</a> and we should focus on broader systems, and not pretend that \u2018fixing\u2019 discrimination can be done objectively or that it makes sense to evaluate each individual algorithm for statistical discrimination.</p>\n<p>I think he\u2019s trying to seek too much when asking questions like \u2018do these practices adequately address harms from hiring automation?\u2019 The point of such questions is not to adequately address harms. The point of such questions is to avoid blame, to avoid lawsuits and to protect against particular forms of discrimination and harm. We emphasize this partly because it is tractable, and partly because our society has chosen (for various historical and path dependent reasons) to consider some kinds of harm very blameworthy and important, and others less so.</p>\n<p>There are correlations we forbidden to consider and mandated to remove on pain of massive blame. There are other correlations that are fine, or even mandatory. Have we made good choices on which is which and how to decide that? Not my place to say.</p>\n<p>Avoiding harm in general, or harm to particular groups, or creating optimal outcomes either for groups or in general, is a very different department. As Arvind points out, we often are trading off incommissorate goals. Many a decision or process, made sufficiently legible and accountable for its components and correlations, would be horribly expensive, make operation of the system impossible or violate sacred values, often in combination.</p>\n<p>Replacing humans with algorithms or AIs means making the system legible and thus blameworthy and accountable in new ways, preventing us from using our traditional ways of smoothing over such issues. If we don\u2019t adjust, the result will be paralysis.</p>\n\n\n<h4 class=\"wp-block-heading\">Unprompted Attention</h4>\n\n\n<p><a href=\"https://x.com/paulg/status/2000168475852259747\">It\u2019s odd to see this framing still around?</a></p>\n<blockquote>\n<p>Paul Graham: Trying to get an accurate answer out of current AI is like trying to trick a habitual liar into telling the truth. It can be done if you back him into the right kind of corner. Or as we would now say, give him the right prompts.</p>\n</blockquote>\n<p>Thinking of the AI as a \u2018lair\u2019 does not, in my experience, help you prompt wisely.</p>\n<p>A more useful framing is:</p>\n<ol>\n<li>If you put an AI into a situation that implies it should know the answer, but it doesn\u2019t know the answer, it is often going to make something up.</li>\n<li>If you imply to the AI what answer you want or expect, it is likely to give you that answer, or bias towards that answer, even if that answer is wrong.</li>\n<li>Thus, you need to avoid doing either of those things.</li>\n</ol>\n<p>&nbsp;</p>\n\n\n<h4 class=\"wp-block-heading\">They Took Our Jobs</h4>\n\n\n<p>Wall Street Journal\u2019s Steven Rosenbush reports that <a href=\"https://www.wsj.com/articles/ceos-are-all-in-on-ai-f3882564\">CEOs Are All In On AI</a>, with 95% seeing it as transformative and 89% B2B CEOs having a positive outlook versus 79% of B2C CEOs.</p>\n<blockquote>\n<p>Mark Penn: What do they think is going to happen with AI? They think it is going to add to productivity, help the economy, improve the global economy, improve competitiveness, but it will weaken the employment market.</p>\n<p>Kevin Hassett (NEC director): I don\u2019t anticipate mass job losses. Of course technological change can be uncertain and unsettling. But\u2026the history of it is that electricity turned out to be a good thing. The internal combustion engine turned out to be a good thing. The computer turned out to be a good thing and I think AI will as well.</p>\n</blockquote>\n<p>Hasset is making a statement uncorrelated with future reality. It\u2019s simply a \u2018all technology is good\u2019 maxim straight out of the Marc Andreessen playbook, without any thoughts as to how this particular change will actually work.</p>\n<p>Will AI bring mass job losses? Almost certainly a lot of existing jobs will go away. The question is whether other jobs will rise up to replace them, which will depend on whether the AIs can take those jobs too, or whether AI will remain a normal technology that hits limits not that far from its current limits.</p>\n<p><a href=\"https://x.com/cjwynes/status/1999240740996866122\">Arkansas bar offers rules for AI assistance of lawyer</a>s that treat AIs as if they were nonlawyer persons.</p>\n<p>In an \u2018economic normal\u2019 or \u2018AI as normal technology\u2019 world GFodor seems right here, in a superintelligence world that survives to a good outcome this is even more right:</p>\n<blockquote>\n<p><a href=\"https://x.com/gfodor/status/2000236634089148428\">GFodor</a>: The jobs of the future will be ones where a human doing it is valued more than pure job performance. Most people who say \u201cwell, I\u2019d never prefer a robot for *that* job\u201d are smuggling in an assumption that the human will be better at it. Once you notice this error it\u2019s everywhere.</p>\n</blockquote>\n<p>If your plan is that the AI is going to have a Skill Issue, that is a short term plan.</p>\n<p>They continue to take our job applications. <a href=\"https://x.com/dioscuri/status/2000563750349558037\">What do you do with 4580 candidates?</a></p>\n<blockquote>\n<p>ave: end of 2023 I applied to one job before I got an offer.<br />\nearly 2024 I applied to 5 jobs before I got an offer.<br />\nend of 2024/early 2025 I applied to 100+ jobs before I got an offer.<br />\nit\u2019s harsh out there.</p>\n</blockquote>\n\n\n<h4 class=\"wp-block-heading\">Feeling the AGI Take Our Jobs</h4>\n\n\n<p>AGI is a nebulous term, in that different people mean different things by it at different times, and often don\u2019t know which one they\u2019re talking about at a given time.</p>\n<p>For increasingly powerful definitions of AGI, we now feel the AGI.</p>\n<blockquote>\n<p><a href=\"https://x.com/deanwball/status/2001035805590970755\">Dean Ball</a>: it\u2019s not really current-vibe-compliant to say \u201cI kinda basically just think opus 4.5 in claude code meets the openai definition of agi,\u201d so of course I would never say such a thing.</p>\n<p><a href=\"https://x.com/deepfates/status/2001047747110334516\">Deepfates</a>: Unlike Dean, I do not have to remain vibe compliant, so I\u2019ll just say it:</p>\n<p><strong>Claude Opus 4.5 in Claude Code is AGI.<br />\n</strong><br />\nBy the open AI definition? Can this system \u201coutperform humans in most economically valuable work\u201d? Depends a lot on how you define \u201chumans\u201d and \u201ceconomically valuable work\u201d obviously.</p>\n<p>But the entire information economy we\u2019ve built up since the \u201870s is completely disrupted by this development, and people don\u2019t notice it yet because they think it\u2019s some crusty old unixy thing for programmers.</p>\n<p>As Dean points out elsewhere, software engineering just means getting the computer to do things. How much of your job is just about getting the computer to do things? What is left if you remove all of that? That\u2019s your job now. That\u2019s what value you add to the system.</p>\n<p>My workflow has completely changed in the last year.</p>\n<p>\u2026 In my opinion, AGI is when a computer can use the computer. And we\u2019re there.</p>\n<p>\u2026 When God sings with his creations, will Claude not be part of the choir?</p>\n<p>Dean Ball: I agree with all this; it is why I also believe that opus 4.5 in claude code is basically AGI.</p>\n<p><a href=\"https://x.com/deanwball/status/2001068539990696422\">Most people barely noticed, but *it is happening.*</a></p>\n<p>It\u2019s just happening, at first, in a conceptually weird way: Anyone can now, with quite high reliability and reasonable assurances of quality, cause bespoke software engineering to occur.</p>\n<p>This is a strange concept.</p>\n<p>\u2026 It will take time to realize this potential, if for no other reason than the fact that for most people, the tool I am describing and the mentality required to wield it well are entirely alien. You have to learn to think a little bit like a software engineer; you have to know \u201cthe kinds of things software can do.\u201d</p>\n<p>We lack \u201ctransformative AI\u201d only because it is hard to recognize transformation *while it is in its early stages.* But the transformation is underway. Technical and infrastructural advancements will make it easier to use and better able to learn new skills. It will, of course, get smarter.</p>\n<p>Diffusion will proceed slower than you\u2019d like but faster than you\u2019d think. New institutions, built with AI-contingent assumptions from the ground up, will be born.</p>\n<p>So don\u2019t listen to the chatterers. Watch, instead, what is happening.</p>\n</blockquote>\n<p>There has most certainly been a step change for me where I\u2019m starting to realize I should be going straight to \u2018just build that thing cause why not\u2019 and I am most certainly feeling the slow acceleration.</p>\n<p>With sufficient acceleration of software engineering, and a sufficiently long time horizon, everything else follows, but as Dean Ball says it takes time.</p>\n<p>I do not think this or its top rivals count as AGI yet. I do think they represent the start of inevitable accelerating High Weirdness.</p>\n<p>In terms of common AGI definitions, Claude Code with Opus 4.5 doesn\u2019t count, which one can argue is a problem for the definition.</p>\n<blockquote>\n<p>Ryan Greenblatt (replying to OP): I do not think that Opus 4.5 is a \u201chighly autonomous system that outperforms humans at most economically valuable work\u201d. For instance, most wages are paid to humans, there hasn\u2019t been a &gt;50% increase in labor productivity, nor should we expect one with further diffusion.</p>\n<p><a href=\"https://x.com/deanwball/status/2001121836675801583\">Dean Ball</a>: This is a good example of how many ai safety flavored \u201cadvanced ai\u201d definitions assume the conclusion that \u201cadvanced ai\u201d will cause mass human disempowerment. \u201cMost wages not being paid to humans\u201d is often a foundational part of the definition.</p>\n<p>Eliezer Yudkowsky: This needs to be understood in the historical context of an attempt to undermine \u201cASI will just kill you\u201d warnings by trying to focus all attention on GDP, wage competition, and other things that are not just killing you.</p>\n<p>The definitions you now see that try to bake in wage competition to the definition of AGI, or GDP increases to the definition of an intelligence explosion, are Dario-EA attempts to derail MIRI conversation about, \u201cIf you build a really smart thing, it just kills you.\u201d</p>\n<p>Ryan Greenblatt: TBC, I wasn\u2019t saying that \u201cmost wages paid to humans\u201d is necessarily inconsistent with the OpenAI definition, I was saying that \u201cmost wages paid to humans\u201d is a decent amount of evidence against.</p>\n<p>I think we\u2019d see obvious economic impacts from AIs that \u201coutperform humans at most econ valuable work\u201d.</p>\n<p>Dean Ball: I mean models have been this good for like a picosecond of human history</p>\n<p>But also no, claude code, with its specific ergonomics, will not be the thing that diffuses widely. it\u2019s just obvious now that the raw capability is there. we could stop now and we\u2019d \u201chave it,\u201d assuming we continued with diffusion and associated productization</p>\n</blockquote>\n<p>The thing is, people (not anyone above) not only deny the everyone dying part, they are constantly denying the \u2018most wages will stop being paid to humans once AIs are ten times better and cheaper at most things wages are paid for\u2019 part.</p>\n\n\n<h4 class=\"wp-block-heading\">The Art of the Jailbreak</h4>\n\n\n<p>OpenAI has new terms of service that prohibit, quotation marks in original, \u201cjailbreaking,\u201d \u201cprompt engineering or injection\u201d or \u2018other methods to override or manipulate safety, security or other platform controls. <a href=\"https://x.com/elder_plinius/status/2001356123878858990\">Pliny feels personally attacked</a>.</p>\n\n\n<h4 class=\"wp-block-heading\">Get Involved</h4>\n\n\n<p><a href=\"https://www.lesswrong.com/posts/eKGdCNdKjvTBG9i6y/toss-a-bitcoin-to-your-lightcone-lw-lighthaven-s-2026\">The Lightcone Infrastructure annual fundraiser is live</a>, with the link mainly being a 15,000 word overview of their efforts in 2025.</p>\n<div>\n<figure>\n<div>\n\n\n<figure class=\"wp-block-image\"><img alt=\"\" src=\"https://substackcdn.com/image/fetch/$s_!PUXJ!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fabeeef1c-52e5-43aa-af22-3b97485cc5fd_1200x1059.jpeg\" /></figure>\n\n\n<div></div>\n</div>\n</figure>\n</div>\n<p>I will say it once again:</p>\n<p>Lightcone Infrastructure is invaluable, both for LessWrong and for Lighthaven. To my knowledge, Lightcone Infrastructure is by a wide margin the best legible donation opportunity, up to at least several million dollars. The fact that there is even a small chance they might be unable to sustain either LessWrong or Lighthaven, is completely bonkers. I would have directed a large amount to Lightcone in the SFF process, but I was recused and thus could not do so.</p>\n<blockquote>\n<p><a href=\"https://x.com/anderssandberg/status/2001314139008823637\">Anders Sandberg</a>: [Lighthaven] is one of the things underpinning the Bay Area as the intellectual center of our civilization. I suspect that when the history books are written about our era, this cluster will be much more than a footnote.</p>\n</blockquote>\n<p><a href=\"https://alignment.anthropic.com/2025/anthropic-fellows-program-2026/\">Anthropic Fellows Research Program applications are open for May and June 2026</a>.</p>\n<p><a href=\"https://x.com/rajiinio/status/1999309607806861418\">US</a> <a href=\"https://t.co/HCZWEgqHex\">CAISI is hiring</a> IT specialists, salary $120k-$195k.</p>\n<p><a href=\"https://unpromptedcon.org/\">Unprompted</a> will be a new AI security practitioner conference, March 3-4 in SF\u2019s Salesforce Tower, <a href=\"https://x.com/elder_plinius/status/2001372101023010943\">with Pliny serving on the conference committee and review board.</a> Great idea, but should have booked Lighthaven (unless they\u2019re too big for it).</p>\n<p><a href=\"https://x.com/HumanHarlan/status/1999367285279195595\">MIRI comms is hiring</a> for several different roles, <a href=\"https://intelligence.org/2025/12/10/miri-comms-is-hiring/\">official post here</a>. They expect most salaries in the $80k-$160k range but are open to pitches for more from stellar candidates.</p>\n<div>\n<figure>\n<div>\n\n\n<figure class=\"wp-block-image\"><img alt=\"\" src=\"https://substackcdn.com/image/fetch/$s_!TbcZ!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F87e35d3e-43aa-44fd-b48c-0fa74bfbdac5_1200x1200.jpeg\" /></figure>\n\n\n<div></div>\n</div>\n</figure>\n</div>\n<p>&nbsp;</p>\n\n\n<h4 class=\"wp-block-heading\">Introducing</h4>\n\n\n<p><a href=\"https://x.com/GoogleDeepMind/status/1999165701811015990\">Gemini Deep Research Agents for developers</a>, <a href=\"https://blog.google/technology/developers/deep-research-agent-gemini-api/?utm_source=x&amp;utm_medium=social&amp;utm_campaign=&amp;utm_content=\">based on Gemini 3 Pro.</a></p>\n<p><a href=\"https://x.com/ctnzr/status/2000567572065091791\">Nvidia Nemotron 3</a>, a fast 30B open source mostly American model with an Artificial Analysis Intelligence score comparable to GPT-OSS-20B. I say mostly American because it was \u2018improved using Qwen\u2019 for synthetic data generation and RLHF. This raises potential opportunities for secondary data poisoning or introducing Chinese preferences.</p>\n<p><a href=\"https://x.com/abhayesian/status/1999981217924743388\">Anthropic has open sourced the replication of their auditing game</a> from earlier this year, as a testbed for further research.</p>\n<p><a href=\"https://x.com/xai/status/2001385958147752255\">xAI Grok Voice Agent API</a>, to allow others to create voice agents. They claim it is very fast, and bill at $0.05 per minute.</p>\n\n\n<h4 class=\"wp-block-heading\">Gemini Flash 3</h4>\n\n\n<p><a href=\"https://x.com/OfficialLoganK/status/2001322275656835348\">Introducing Gemini 3 Flash</a>, cost of $0.05/$3 per million tokens. Their benchmark chart compares it straight to the big boys, except they use Sonnet over Opus. Given Flash\u2019s speed and pricing, that seems fair.</p>\n<p>The benchmarks are, given Flash\u2019s weight class, very good.</p>\n<div>\n<figure>\n<div>\n\n\n<figure class=\"wp-block-image\"><img alt=\"\" src=\"https://substackcdn.com/image/fetch/$s_!onIj!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F651eca1f-de25-44d6-b793-5ff5728e6d1f_1057x1200.jpeg\" /></figure>\n\n\n<div></div>\n</div>\n</figure>\n</div>\n<p><a href=\"https://x.com/LechMazur/status/2001404031957172695\">Lech Mazor puts it at 92 on Extended NY Times Connections</a>, in 3rd place behind Gemini 3 Pro and Grok 4.1 Fast Reasoning.</p>\n<p><a href=\"https://x.com/elder_plinius/status/2001348055078199611\">The inevitable Pliny jailbreak is here</a>, and <a href=\"https://x.com/lefthanddraft/status/2001470408919576838\">here is the system prompt</a>.</p>\n<p><a href=\"https://x.com/mutewinter/status/2001328106628669844\">Jeremy Mack offers mostly positive basic vibe coding feedback</a>. <a href=\"https://x.com/RoryWalshWatts/status/2001570695793512745\">Rory Watts admires the speed</a>, <a href=\"https://x.com/typebulbit/status/2001434162536869894\">Typebulb loves speed and price and switched over</a> (I think for coding).</p>\n<blockquote>\n<p>Vincent Favilla: It\u2019s fast, but more importantly, it\u2019s cheap. 25% of the price for 80% of the intelligence is becoming pretty compelling at these capability levels.</p>\n</blockquote>\n<p><a href=\"https://x.com/techczech/status/2001575978405896417\">Dominik Lukes is impressed and found it often matched Gemini 3 Pro in his evals</a>.</p>\n<p>In general, the feedback is that this is an excellent tradeoff of much faster and cheaper in exchange for not that much less smart than Gemini 3 Pro. I also saw a few reports that it shares the misalignment and pathologies of Gemini 3 Pro.</p>\n<p>Essentially, it looks like they successfully distilled Gemini 3 Pro to be much faster and cheaper while keeping much of its performance, which is highly valuable. It\u2019s a great candidate for cases where pretty good, very fast and remarkably cheap is the tradeoff you want, which includes a large percentage of basic queries. It also seems excellent that this will be available for free and as part of various assistant programs.</p>\n<p>Good show.</p>\n\n\n<h4 class=\"wp-block-heading\">In Other AI News</h4>\n\n\n<p><a href=\"https://www.bigtechnology.com/p/enterprise-will-be-a-top-openai-priority\">Sam Altman assures business leaders that enterprise AI will be a priority in 2026</a>.</p>\n<p><a href=\"https://x.com/AndrewCurran_/status/1999227574439010747\">OpenAI adult mode to go live in Q1 2026</a>. Age of account will be determined by the AI, and the holdup is improving the age determination feature. This is already how Google does it, although Google has better context. In close cases they\u2019ll ask for ID. A savvy underage user could fool the system, but I would argue that if you\u2019re savvy enough to fool the system without simply using a false or fake ID then you can handle adult mode.</p>\n\n\n<h4 class=\"wp-block-heading\">Going Too Meta</h4>\n\n\n<p>The NYT\u2019s Eli Tan reports that Meta\u2019s new highly paid AI superstars are clashing with the rest of the company. You see, Alexandr Wang and the others believe in AI and want to build superintelligence, whereas the rest of Meta wants to sell ads.</p>\n<p>Mark Zuckerberg has previously called various things \u2018superintelligence\u2019 so we need to be cautious regarding that word here.</p>\n<p><a href=\"https://www.nytimes.com/2025/12/10/technology/meta-ai-tbd-lab-friction.html\">The whole article is this same argument happening over and over:</a></p>\n<blockquote>\n<p>Eli Tan: In one case, Mr. Cox and Mr. Bosworth wanted Mr. Wang\u2019s team to concentrate on using Instagram and Facebook data to help train Meta\u2019s new foundational A.I. model \u2014 known as a \u201cfrontier\u201d model \u2014 to improve the company\u2019s social media feeds and advertising business, they said. But Mr. Wang, who is developing the model, pushed back. He argued that the goal should be to catch up to rival A.I. models from OpenAI and Google before focusing on products, the people said.</p>\n<p>The debate was emblematic of an us-versus-them mentality that has emerged between Meta\u2019s new A.I. team and other executives, according to interviews with half a dozen current and former employees of the A.I. business.</p>\n<p>\u2026 Some Meta employees have also disagreed over which division gets more computing power.</p>\n<p>\u2026 In one recent meeting, Mr. Cox asked Mr. Wang if his A.I. could be trained on Instagram data similar to the way Google trains its A.I. models on YouTube data to improve its recommendations algorithm, two people said.</p>\n<p>But Mr. Wang said complicating the training process for A.I. models with specific business tasks could slow progress toward superintelligence, they said. He later complained that Mr. Cox was more focused on improving his products than on developing a frontier A.I. model, they said.</p>\n<p>\u2026 On a recent call with investors, Susan Li, Meta\u2019s chief financial officer, said a major focus next year would be using A.I. models to improve the company\u2019s social media algorithm.</p>\n</blockquote>\n<p>It is a hell of a thing to see prospective superintelligence and think \u2018oh we should narrowly use this to figure out how to choose the right Instagram ads.\u2019</p>\n<p>Then again, in this narrow context, isn\u2019t Cox right?</p>\n<p>Meta is a business here to make money. There\u2019s a ton of money in improving how their existing products work. That\u2019s a great business opportunity.</p>\n<p>Whereas trying to rejoin the race to actual superintelligence against Google, OpenAI and Anthropic? I mean Meta can try. Certainly there is value in success there, in general, but it\u2019s a highly competitive field to try to do general intelligence and competing there is super expensive. Why does Meta need to roll its own?</p>\n<p>What Meta needs is specialized AI models that help it maximize the value of Facebook, Instagram, WhatsApp and potentially the metaverse and its AR/VR experiences. A huge AI investment on that makes sense. Otherwise, why not be a fast follower? For other purposes, and especially for things like coding, the frontier labs have APIs for you to use.</p>\n<p>I get why Wang wants to go the other route. It\u2019s cool, it\u2019s fun, it\u2019s exciting, why let someone else get us all killed when you can do so first except you\u2019ll totally be more responsible and avoid that, be the one in the arena, etc. That doesn\u2019t mean it is smart business.</p>\n<blockquote>\n<p><a href=\"https://x.com/albrgr/status/2000247255861973221\">Alexander Berger</a>: These sentences are so funny to see in straight news stories:<br />\n\u201cresearchers have come to view many Meta executives as interested only in improving the social media business, while the lab\u2019s ambition is to create a godlike A.I. superintelligence\u201d</p>\n<p>Brad Carson: Please listen to their <strong>stated </strong>ambitions. This is from the <a href=\"https://x.com/nytimes\">@nytimes</a> story on Meta. With no hesitation, irony, or qualifier, a \u201cgodlike\u201d superintelligence is the aim. It\u2019s wild.</p>\n<p>Eli Tan: TBD Lab\u2019s researchers have come to view many Meta executives as interested only in improving the social media business, while the lab\u2019s ambition is to create a godlike A.I. superintelligence, three of them said.</p>\n<p>Daian Tatum: They named the lab after their alignment plan?</p>\n<p><a href=\"https://x.com/peterwildeford/status/2000539522463945035\">Peter Wildeford</a>:</p>\n<div>\n<figure>\n<div>\n\n\n<figure class=\"wp-block-image\"><img alt=\"\" src=\"https://substackcdn.com/image/fetch/$s_!M5fs!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fefb7cf4d-1b46-474d-a760-9f73ccd78848_1200x773.jpeg\" /></figure>\n\n\n<div></div>\n</div>\n</figure>\n</div>\n</blockquote>\n<p>Well, yes, the AI researchers don\u2019t care about selling ads and want to build ASI despite it being an existential threat to humanity. Is this a surprise to anyone?</p>\n<p>&nbsp;</p>\n\n\n<h4 class=\"wp-block-heading\">Show Me the Money</h4>\n\n\n<p><a href=\"https://x.com/conorsen/status/2000012784839627087\">OpenAI is spending $6 billion in stock-based compensation this year</a>, or 1.2% of the company, and letting employees start vesting right away, to compete with rival bids like Meta paying $100 million a year or more for top talent. <a href=\"https://www.wsj.com/tech/ai/openai-ends-vesting-cliff-for-new-employees-in-compensation-policy-change-d4c4c2cd?mod=mhp\">I understand why this can be compared to revenue</a> of $12 billion, but that is misleading. One shouldn\u2019t treat \u2018the stock is suddenly worth a lot more\u2019 as \u2018that means they\u2019re bleeding money.\u2019</p>\n<p><a href=\"https://www.theinformation.com/articles/openai-talks-raise-least-10-billion-amazon-use-ai-chips\">OpenAI in talks to raise at least $10 billion from Amazon and use the money for Amazon\u2019s Tritanium chips</a>.</p>\n<p>&nbsp;</p>\n\n\n<h4 class=\"wp-block-heading\">Bubble, Bubble, Toil and Trouble</h4>\n\n\n<p><a href=\"https://x.com/StefanFSchubert/status/2000183944801611947\">You call this a bubble? This is nothing, you are like baby</a>:</p>\n<div>\n<figure>\n<div>\n\n\n<figure class=\"wp-block-image\"><img alt=\"\" src=\"https://substackcdn.com/image/fetch/$s_!tY8c!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fdb554d2e-161d-4660-8c88-167123fb1cdb_1200x1192.png\" /></figure>\n\n\n<div></div>\n</div>\n</figure>\n</div>\n<blockquote>\n<p>Stefan Schubert: The big tech/AI companies have less extreme price-earnings ratios than key stocks had in historical bubbles.</p>\n<p>David Manheim: OpenAI and Anthropic\u2019s 24-month forward P/E ratio, on the other hand, are negative, since they aren\u2019t profitable now and don\u2019t expect to be by then. (And I\u2019d bet the AI divisions at other firms making frontier models are not doing any better.)</p>\n</blockquote>\n<p>Yes, the frontier model divisions or startups are currently operating at a loss, so price to earnings doesn\u2019t tell us that much overall, but the point is that these multipliers are not scary. Twenty times earnings for Google? Only a little higher for Nvidia and Microsoft? I am indeed signed up for all of that.</p>\n<p><a href=\"https://www.wsj.com/tech/personal-tech/the-good-bad-and-ugly-of-ai-4ea8fa6b?st=QQmW1P\">Wall Street Journal\u2019s Andy Kessler does a standard</a> \u2018AI still makes mistakes and can\u2019t solve every problem and the market and investment are ahead of themselves\u2019 post, pointing out that market expectations might fall and thus Number Go Down. Okay.</p>\n<p>Rob Wiblin crystalizes the fact that AI is a \u2018natural bubble\u2019 in the sense that it is priced as a normal highly valuable thing [X] plus a constantly changing probability [P] of a transformational even more valuable (or dangerous, or universally deadly) thing [Y]. So the value is ([X] + [P]*[Y]). If P goes down, then value drops, and Number Go Down.</p>\n\n\n<h4 class=\"wp-block-heading\"></h4>\n\n\n<p>&nbsp;</p>\n\n\n<h4 class=\"wp-block-heading\">Quiet Speculations</h4>\n\n\n<p>There\u2019s remarkably strong disagreement on this point but I think Roon is mostly right:</p>\n<blockquote>\n<p><a href=\"https://x.com/tszzl/status/1999943176606584942\">Roon:</a> most of what sam and dario predicted for 2025 came true this year. virtually unheard of for tech CEOs, maybe they need to ratchet up the claims and spending.</p>\n<p>Gfodor: This year has been fucking ridiculous. If we have this rate of change next year it\u2019s gonna be tough.</p>\n</blockquote>\n<p>Yes, we could have gotten things even more ridiculous. Some areas were disappointing relative to what I think in hindsight were the correct expectations given what we knew at the time. Dario\u2019s predictions on when AIs will write most code did fall importantly short, and yes he should lose Bayes points on that. But those saying there hasn\u2019t been much progress are using motivated reasoning or not paying much attention. If I told you that you could only use models from 12 months ago, at their old prices and speeds, you\u2019d quickly realize how screwed you were.</p>\n<p>Efficiency on the ARC prize, in terms of score per dollar spent, <a href=\"https://x.com/sjgadler/status/1999245551746056276\">has increased by a factor of 400 in a single year</a>. That\u2019s an extreme case, but almost every use case has in the past year seen improvement by at least one order of magnitude.</p>\n<p>A good heuristic: If your model of the future says \u2018they won\u2019t use AI for this, it would be too expensive\u2019 then your model is wrong.</p>\n<p><a href=\"https://marginalrevolution.com/marginalrevolution/2025/12/gans-and-doctorow-on-ai-copyright.html?utm_source=rss&amp;utm_medium=rss&amp;utm_campaign=gans-and-doctorow-on-ai-copyright\">Joshua Gans writes</a> a \u2018textbook on AI\u2019 ambitiously called <a href=\"https://amzn.to/456DZ9Y\"><em>The Microeconomics of Artificial Intelligence</em></a>. It ignores the big issues to focus on particular smaller areas of interest, including the impact of \u2018better predictions.\u2019</p>\n<p><a href=\"https://www.technologyreview.com/2025/12/15/1129174/the-great-ai-hype-correction-of-2025/\">Will Douglas Heaven of MIT Technology Review</a> is the latest to Do The Meme. As in paraphrases of both \u20182025 was the year that AI didn\u2019t make much progress\u2019 and also \u2018LLMs will never do the things they aren\u2019t already doing (including a number of things they are already capable of doing)\u2019 and \u2018LLMs aren\u2019t and never will be intelligent, that\u2019s an illusion.\u2019 Sigh.</p>\n\n\n<h4 class=\"wp-block-heading\">Timelines</h4>\n\n\n<blockquote>\n<p><a href=\"https://x.com/ShaneLegg/status/1999180585407848776\">Shane Legg (Cofounder DeepMind)</a>: I\u2019ve publicly held the same prediction since 2009: there\u2019s a 50% chance we\u2019ll see #AGI by 2028.</p>\n<p>I sat down with @FryRsquared to discuss why I haven\u2019t changed my mind, and how we need to prepare before we get there.</p>\n</blockquote>\n<p>You don\u2019t actually get to do that. Bayes Rule does not allow one to not update on evidence. Tons of things that happened between 2009 and today should have changed Legg\u2019s estimates, in various directions, including the Transformer paper, and also including \u2018nothing important happened today.\u2019</p>\n<p>Saying \u2018I\u2019ve believed 50% chance of AGI by 2028 since 2009\u2019 is the same as when private equity funds refuse to change the market value of their investments. Yes, the S&amp;P is down 20% (or up 20%) and your fund says it hasn\u2019t changed in value, but obviously that\u2019s a lie you tell investors.</p>\n\n\n<h4 class=\"wp-block-heading\"></h4>\n\n\n<p>&nbsp;</p>\n\n\n<h4 class=\"wp-block-heading\">The Quest for Sane Regulations</h4>\n\n\n<p><a href=\"https://x.com/daniel_271828/status/1999943689163157874\">AOC and Bernie Sanders</a> applaud Chandler City Council voting down a data center.</p>\n<p>Bernie Sanders took it a step further, and <a href=\"https://x.com/daniel_271828/status/2001083138324488266\">outright called for a moratorium on data center construction</a>. As in, an AI pause much broader than anything \u2018AI pause\u2019 advocates have been trying to get. <a href=\"https://x.com/VitalikButerin/status/2001238398149779677\">Vitalik Buterin has some pros and cons of this from his perspective</a>.</p>\n<blockquote>\n<p>Vitalik Buterin: argument for: slowdown gud</p>\n<p>argument against: the more useful thing is \u201cpause button\u201d &#8211; building toward having the capability to cut available compute by 90-99% for 1-2 years at a future more critical moment</p>\n<p>argument for: opening the discussion on distinguishing between supersized clusters and consumer AI hardware is good. I prefer slowdown + more decentralized progress, and making that distinction more and focusing on supersized clusters accomplishes both</p>\n<p>argument against: this may get optimized around easily in a way that doesn\u2019t meaningfully accomplish its goals</p>\n<p><a href=\"https://x.com/neil_chilson/status/2001127169699791263\">Neil Chilson</a>: Eagerly awaiting everyone who criticized the July state AI law moratorium proposal as \u201cfederal overreach\u201d or \u201cviolating states\u2019 rights\u201d to condemn this far more preposterous, invasive, and blatantly illegal proposal.</p>\n</blockquote>\n<p>As a matter of principle I don\u2019t \u2018condemn\u2019 things or make my opposition explicit purely on demand. But in this case? Okay, sure, Neil, I got you, since before I saw your request I\u2019d already written this:</p>\n<p>I think stopping data center construction, especially unilaterally stopping it in America, would be deeply foolish, whereas building a pause button would be good. Also deeply foolish would be failing to recognize that movements and demands like Bernie\u2019s are coming, and that their demands are unlikely to be technocratically wise.</p>\n<p>It is an excellent medium and long term strategy to earnestly stand up for what is true, and what causes would have what effects, even when it seems to be against your direct interests. People notice.</p>\n<blockquote>\n<p><a href=\"https://x.com/deanwball/status/2001362682570793389\">Dean Ball</a>: has anyone done more for the brand of effective altruism than andy masley? openphilan&#8211;excuse me, coefficient giving&#8211;could have spent millions on a rebranding campaign (for all I know, they did) and it would have paled in comparison to andy doing algebra and tweeting about it.</p>\n</blockquote>\n<p>Andy Masley has been relentlessly pointing out that all the claims about gigantic levels of water usage by data centers don\u2019t add up. Rather than EAs or rationalists or others concerned with actual frontier safety rallying behind false concerns over water, almost all such folks have rallied to debunk such claims and to generally support building more electrical power and more transmission lines and data centers.</p>\n<p>On the water usage from, <a href=\"https://x.com/AndyMasley/status/2001520364635976075\">Karen Hao has stepped up and centrally corrected her errors</a>. Everyone makes mistakes, this is The Way.</p>\n\n\n<h4 class=\"wp-block-heading\">My Offer Is Nothing</h4>\n\n\n<p>As expected, following the Congress declining once again to ban all state regulations on AI via law, the White House <a href=\"https://www.whitehouse.gov/presidential-actions/2025/12/eliminating-state-law-obstruction-of-national-artificial-intelligence-policy/\">is attempting to do as much towards that end as it can via Executive Order</a>.</p>\n<p>There are some changes versus the leaked draft executive order, <a href=\"https://x.com/neil_chilson/status/1999548044530860334\">which Neil Chilson goes over here with maximally positive framing</a>.</p>\n<ol>\n<li>A positive rather than confrontational title.</li>\n<li>Claiming to be collaborating with Congress.</li>\n<li>Removing explicit criticism and targeting of California\u2019s SB 53, the new version only names Colorado\u2019s (rather terrible) AI law.</li>\n<li>Drop the word \u2018uniform\u2019 in the policy section.</li>\n<li>States intent of future proposed framework to avoid AI child safety, data center infrastructure and state AI procurement policies, although it does not apply this to Section 5 where they condition state funds on not having disliked state laws.</li>\n<li>Clearer legal language for the state review process.</li>\n</ol>\n<p>I do acknowledge that these are improvements, and I welcome all rhetoric that points towards the continued value of improving things.</p>\n<blockquote>\n<p><a href=\"https://x.com/mrddmia/status/1999551060155707873\">Mike Davis</a> (talking to Steve Bannon): This Executive Order On AI Is A big Win. It Would Not Have Gone Well If The Tech Bros Had Gotten Total AI Amnesty.</p>\n<p><a href=\"https://x.com/DavidSacks/status/1999570426326913223\">David Sacks</a> (AI Czar): Mike and I have our differences on tech policy but I appreciate his recognition that this E.O. is a win for President Trump, and that the administration listened to the concerns of stakeholders, took them into account, and is engaged in a constructive dialogue on next steps.</p>\n</blockquote>\n<p>Mike Davis, <a href=\"https://rumble.com/v72xkba-mike-davis-this-executive-order-on-ai-is-a-big-win.html\">if you listen to the clip</a>, is saying this is a win because he correctly identified the goal of the pro-moratorium faction as what he calls \u2018total AI amnesty.\u2019 Davis thinks thinks the changes to the EO are a victory, by Trump and also Mike Davis, against David Sacks and other \u2018tech bros.\u2019</p>\n<p>Whereas Sacks views it as a win because in public he always sees everything Trump does as a win for Trump, that\u2019s what you do when you\u2019re in the White House, and because it is a step towards preemption, and doesn\u2019t care about the terms given to those who are nominally tasked with creating a potential \u2018federal framework.\u2019</p>\n<p><a href=\"https://www.wsj.com/tech/ai/the-political-skirmish-over-trumps-ai-order-is-just-the-beginning-7d0e649a?mod=WTRN_pos1\">Tim Higgins at the Wall Street Journal instead</a> portrays this as a victory for Big Tech, against loud opposition from the likes of DeSantis and Bannon on the right in addition to opposition on the left. This is the obvious, common sense reading. David Sacks wrote the order to try and get rid of state laws in his way, we should not let some softening of language fool us.</p>\n<p>If someone plans to steal your lunch money, and instead only takes some of your lunch money, they still stole your lunch money. If they take your money but promise in the future to look into a framework for only taking some of your money? They definitely still stole your lunch money. Or in this case, they are definitely trying to steal it.</p>\n<p>It is worth noticing that, aside from a16z, we don\u2019t see tech companies actively supporting even a law for this, let alone an EO. Big tech doesn\u2019t want this win. I haven\u2019t seen any sings that Google or OpenAI want this, or even that Meta wants this. They\u2019re just doing it anyway, without any sort of \u2018federal framework\u2019 whatsoever.</p>\n<p>Note that the rhetoric below from Sriram Krishnan does not even bother to mention a potential future \u2018federal framework.\u2019</p>\n<blockquote>\n<p><a href=\"https://x.com/sriramk/status/1999267329935573083\">Sriram Krishnan</a>: We just witnessed @realDonaldTrump signing an Executive Order that ensures American AI is protected from onerous state laws.</p>\n<p>This ensures that America continues to dominate and lead in this AI race under President Trump. Want to thank many who helped get to this moment from the AI czar @DavidSacks to @mkratsios47 and many others.</p>\n<p>On a personal note, it was a honor to be given the official signing pen by POTUS at the end. A truly special moment.</p>\n<p><a href=\"https://x.com/neil_chilson/status/1999281152431980994\">Neil Chilson</a>: I strongly support the President\u2019s endorsement of \u201ca minimally burdensome national policy framework for AI,\u201d as articulated in the new Executive Order.</p>\n</blockquote>\n<p>They want to challenge state laws as unconstitutional? They are welcome to try. Colorado\u2019s law is indeed plausibly unconstitutional in various ways.</p>\n<p>They want to withhold funds or else? We\u2019ll see you in court on that too.</p>\n<p>As I said last week, this was expected, and I do not expect most aspects of this order to be legally successful, nor do I expect it to be a popular position. Mostly I expect it to quietly do nothing. If that is wrong and they can successfully bully the states with this money (both it is ruled legal, and it works) that would be quite bad.</p>\n<p>Their offer for a \u2018minimally burdensome national policy framework for AI\u2019 is and will continue to be nothing, as per Sacks last week who said via his \u20184 Cs\u2019 that everything that mattered was already protected by non-AI law.</p>\n<p>The Executive Order mentions future development of such a \u2018federal framework\u2019 as something that might contain actual laws that do actual things.</p>\n<p>But that\u2019s not what a \u2018minimally burdensome\u2019 national policy framework means, and we all know it. Minimally burdensome means nothing.</p>\n<p>They\u2019re not pretending especially hard.</p>\n<blockquote>\n<p><a href=\"https://x.com/neil_chilson/status/1999279578011885597\">Neil Chilson</a>: The legislative recommendation section is the largest substantive change [from the leaked version]. It now excludes specific areas of otherwise lawful state law from a preemption recommendation. This neutralizes the non-stop rhetoric that this is about a total federal takeover.</p>\n<p>This latter section [on the recommendation for a framework] is important. If you read statements about this EO that say things like it \u201cthreatens state safeguards for kids\u201d or such, you know either they haven\u2019t actually read the EO or they are willfully ignoring what it says. Either way, you can ignore them.</p>\n<p><a href=\"https://x.com/CharlieBul58993/status/1999293329876656327\">Charlie Bullock</a>: It does look like the \u201clegislative proposal\u201d that Sacks and Kratsios have been tasked with creating is supposed to exempt child safety laws. But that isn\u2019t the part of the EO that anyone\u2019s concerned about.</p>\n<p>A legislative proposal is just a proposal. It doesn\u2019t do anything\u2014it\u2019s just an advisory suggestion that Congress can take or (more likely) leave.</p>\n<p>Notably, there is no exemption for child safety laws in the section that authorizes a new DOJ litigation task force for suing states that regulate AI, or the section that instructs agencies to withhold federal grant funds from states that regulate AI.</p>\n</blockquote>\n<p>The call for the creation of a proposal to the considered does now say that this proposal would exempt child safety protections, compute and data center infrastructure and state government procurement.</p>\n<p>But, in addition to those never being the parts I was worried about:</p>\n<ol>\n<li>David Sacks has said this isn\u2019t necessary, because of existing law.</li>\n<li>The actually operative parts of the Executive Order make no such exemption.</li>\n<li>The supposed future framework is unlikely to be real anyway.</li>\n</ol>\n<p>I find it impressive the amount to which advocates simultaneously say both:</p>\n<ol>\n<li>This is preemption.</li>\n<li>This is not preemption, it\u2019s only withholding funding, or only laws can do that.</li>\n</ol>\n<p>The point of threatening to withhold funds is de facto preemption. They are trying to play us for absolute fools.</p>\n<blockquote>\n<p>Neil Chilson: So what part of the EO threatens to preempt otherwise legal state laws protecting kids? That\u2019s something only Congress can do, so the recommendation is the only part of the EO that plausibly could threaten such laws.</p>\n</blockquote>\n<p>The whole point of holding the state funding over the heads of states is to attack state laws, whether or not those laws are otherwise legal. It\u2019s explicit text. In that context it is technically true to say that the EO cannot \u2018threaten to preempt otherwise legal state laws\u2019 because they are different things, but the clear intent is to forcibly get rid of those same state laws, which is an attempt to accomplish the same thing. So I find this, in practice, highly misleading.</p>\n<p>Meanwhile, <a href=\"https://x.com/tbpn/status/1998893987986583669\">Republican consultants reportedly are shopping for an anti-AI candidate</a> to run against JD Vance. It seems a bit early and also way too late at the same time.</p>\n\n\n<h4 class=\"wp-block-heading\">My Offer Is Nothing, Except Also Pay Me</h4>\n\n\n<p>I applaud a16z for actually proposing a tangible basis for a \u2018federal framework\u2019 for AI regulation, in exchange for which they want to permanently disempower the states.</p>\n<p>Now we can see what the actual offer is.</p>\n<p>Good news, their offer is not nothing.</p>\n<p>Bad news, the offer is \u2018nothing, except also give us money.\u2019</p>\n<p>When you read this lead-in, what do you expect a16z to propose for their framework?</p>\n<blockquote>\n<p><a href=\"https://x.com/a16z/status/2001235298290728986\">a16z</a>: We don\u2019t need to choose between innovation and safety. America can build world-class AI products while protecting its citizens from harms.</p>\n<p>Read the full piece on how we can protect Americans and win the future.</p>\n</blockquote>\n<p>If your answer was you expect them to choose innovation and then do a money grab? You score Bayes points.</p>\n<p>Their offer is nothing, except also that we should give them government checks.</p>\n<p>Allow me to state, in my own words, what they are proposing with each of their bullet points.</p>\n<ol>\n<li>Continue to allow existing law to apply to AI. Aka: Nothing.</li>\n<li>Child protections. Require parental consent for users under 13, provide basic disclosures such as that the system is AI and not for crisis situations, require parental controls. Aka: Treat it like social media, with similar results.</li>\n<li>Have the federal government measure CBRN and cyber capabilities of AI models. Then do nothing about it, especially in cyber because AI \u2018AI does not create net-new incremental risk since AI enhances the capabilities of both attackers and defenders.\u2019 So aka: Nothing.\n<ol>\n<li>They technically say that response should be \u2018managed based on evidence.\u2019 This is, reliably, code for \u2018we will respond to CBRN and cyber risks after the dangers actually happen.\u2019 At which point, of course, it\u2019s not like you have any choice about whether to respond, or an opportunity to do so wisely.</li>\n</ol>\n</li>\n<li>At most have a \u2018national standard for transparency\u2019 that requires the following:\n<ol>\n<li>Who built this model?</li>\n<li>When was it released and what timeframe does its training data cover?</li>\n<li>What are its intended uses and what are the modalities of input and output it supports?</li>\n<li>What languages does it support?</li>\n<li>What are the model\u2019s terms of service or license?</li>\n<li>Aka: Nothing. None of those have anything to do with any of the concerns, or the reasons why we want transparency. They know this. The model\u2019s terms of service and languages supported? Can you pretend to take this seriously?</li>\n<li>As usual, they say (throughout the document) that various requirements, that would not at all apply to small developers or \u2018little tech,\u2019 would be too burdensome on small developers or \u2018little tech.\u2019 The burden would be zero.</li>\n</ol>\n</li>\n<li>Prohibit states from regulating AI outside of enforcement of existing law, except for particular local implementation questions.</li>\n<li>Train workers and students to use AI on Uncle Sam\u2019s dollar. Aka: Money please.</li>\n<li>Establish a National AI Competitiveness Institute to provide access to infrastructure various useful AI things including data sets. Aka: Money please.\n<ol>\n<li>Also stack the energy policy deck to favor \u2018little tech\u2019 over big tech. Aka: Money please, and specifically for our portfolio.</li>\n</ol>\n</li>\n<li>Invest in AI research. Aka: Money please.</li>\n<li>Government use of AI, including ensuring \u2018little tech\u2019 gets access to every procurement process. Aka: Diffusion in government. Also, money please, and specifically for our portfolio.</li>\n</ol>\n<p><a href=\"https://x.com/WillRinehart/status/2001351539781468458\">Will Rinehart assures me on Twitter</a> that this proposal was in good faith. If that is true, it implies that either a16z thinks that nothing is a fair offer, or that they both don\u2019t understand why anyone would be concerned, and also don\u2019t understand that they don\u2019t understand this.</p>\n\n\n<h4 class=\"wp-block-heading\">Chip City</h4>\n\n\n<p><a href=\"https://www.reuters.com/business/nvidia-builds-location-verification-tech-that-could-help-fight-chip-smuggling-2025-12-10/\">Good news</a>, <a href=\"https://x.com/fiiiiiist/status/1999533891506290994\">Nvidia has implemented location verification for Blackwell-generation AI chips</a>, thus completing the traditional (in particular for AI safety and security, but also in general) policy clown makeup progression:</p>\n<ol>\n<li>That\u2019s impossible in theory.</li>\n<li>That\u2019s impossible in practice.</li>\n<li>That\u2019s outrageously expensive, if we did that we\u2019d lose to China.</li>\n<li>We did it.</li>\n</ol>\n<p>Check out our new feature that allows data centers to better monitor everything. Neat.</p>\n<p><a href=\"https://www.thetimes.com/business/article/trump-china-nvidia-chips-zgzws82s8\">Former UK Prime Minister Rishi Sunak</a>, the major world leader who has taken the AI situation the most seriously, has thoughts on H200s:</p>\n<blockquote>\n<p>Rishi Sunak (Former UK PM): The significance of this decision [to sell H200s to China] should not be underestimated. It substantially increases the chance of China catching up with the West in the AI race, and then swiftly overtaking it.</p>\n<p>\u2026 Why should we care? Because this decision makes it more likely that the world ends up running on Chinese technology \u2014 with all that means for security, privacy and our values.</p>\n<p>\u2026 So, why has Trump handed China such an opportunity to catch up in the AI race? The official logic is that selling Beijing these Nvidia chips will get China hooked on US technology and stymie its domestic chip industry. But this won\u2019t happen. The Chinese are acutely aware of the danger of relying on US technology.</p>\n</blockquote>\n<p>He also has other less kind thoughts about the matter in the full post.</p>\n<p><a href=\"https://x.com/brianmcgrail/status/1999667791302123905\">Nvidia is evaluating expanding production capacity for H200s</a> <a href=\"https://www.reuters.com/world/china/nvidia-considers-increasing-h200-chip-output-due-robust-china-demand-sources-say-2025-12-12/?utm_source=Facebook&amp;utm_medium=Social\">after Chinese demand exceeded supply</a>. As Brian McGrail notes here, every H200 chip Nvidia makes means not using that fab to make Blackwell chips, so it is directly taking chips away from America to give them to China.</p>\n<blockquote>\n<p>Reuters: Supply of H200 chips has been a major concern for Chinese clients and they have reached out to Nvidia seeking clarity on this, sources said.</p>\n<p>\u2026 Chinese companies\u2019 strong demand for the H200 stems from the fact that it is easily the most powerful chip they can currently access.</p>\n<p>\u2026 \u201cIts (H200) compute performance is approximately 2-3 times that of the most advanced domestically produced accelerators,\u201d said Nori Chiou, investment director at White Oak Capital Partners.</p>\n</blockquote>\n<p>Those domestic chips are not only far worse, they are supremely supply limited.</p>\n<p>Wanting to sell existing H200s to China makes sense. Wanting to divert more advanced, more expensive chips into less advanced, cheaper chips, chips where they have to give up a 25% cut, should make us ask why they would want to do that. Why are Nvidia and David Sacks so eager to give chips to China instead of America?</p>\n<p>It also puts a lie to the idea that these chips are insufficiently advanced to worry about. If they\u2019re so worthless, why would you give up Blackwell capacity to make them?</p>\n<p><a href=\"https://x.com/james_s48/status/1999229830177685861\">We have confirmation that the White House decision to sell H200s was based on a multiple misconception</a>.</p>\n<blockquote>\n<p>James Sanders: This suggests that the H200 decision was based on<br />\n&#8211; Comparing the similar performance of Chinese system with 384 GPUs to an NVIDIA system with only 72 GPUs<br />\n&#8211; An estimate for Huawei production around 10x higher than recent estimates from SemiAnalysis</p>\n<p>Either Huawei has found some way around the HBM bottleneck, or I expect the White House\u2019s forecast for 910C production to be too high.</p>\n</blockquote>\n<p>I strongly suspect that the White House estimate was created in order to justify the sale, rather than being a sincere misunderstanding.</p>\n<p>If Huawei does indeed meet the White House forecast, remind me of this passage, and I will admit that I have lost a substantial number of Bayes points.</p>\n<p><a href=\"https://x.com/anderssandberg/status/1999916561641984019\">What about data centers IN SPACE</a>? Anders Sandberg notices that both those for and against this idea are making very confident falsifiable claims, so we will learn more soon. His take is that the task is hard but doable, but the economics seem unlikely to work within the next decade. I haven\u2019t looked in detail but that seems right. The regulatory situation would need to get quite bad before you\u2019d actually do this, levels of quite bad we may never have seen before.</p>\n<p><a href=\"https://x.com/TheStalwart/status/2001360251984187602\">The clip here is something else.</a> I want us to build the transmission lines, we should totally build the transmission lines, but maybe AI advocates need to \u2018stop helping\u2019? For example, you definitely shouldn\u2019t tell people that \u2018everyone needs to get on board\u2019 with transmission lines crossing farms, so there will be less farms and that they should go out and buy artificial Christmas trees. Oh man are people gonna hate AI.</p>\n<p><a href=\"https://t.co/nhRC33Rt6e\">Epoch thinks that America can build electrical capacity</a> if it wants to, it simply hasn\u2019t had the demand necessary to justify that for a while. Now it does, so build baby build.</p>\n<blockquote>\n<p><a href=\"https://x.com/EpochAIResearch/status/2001327048971977052\">Epoch AI</a>: Conventional wisdom says that the US can\u2019t build power but China can, so China\u2019s going to \u201cwin the AGI race by default\u201d.</p>\n<p>We think this is wrong.</p>\n<p>The US likely can build enough power to support AI scaling through 2030 \u2014 as long as they\u2019re willing to spend a lot.</p>\n<p>People often argue that regulations have killed America\u2019s ability to build, so US power capacity has been ~flat for decades while China\u2019s has surged. And there\u2019s certainly truth to this argument.</p>\n<p>But it assumes stagnation came from inability to build, whereas it\u2019s more likely because power demand didn\u2019t grow much.</p>\n<p>Real electricity prices have been stable since 2000. And the US has ways to supply much more power, which it hasn\u2019t pursued by choice.</p>\n<p>So what about AI, which under aggressive assumptions, could approach 100 GW of power demand by 2030?</p>\n<p>The US hasn\u2019t seen these demand growth rates since the 1980s.</p>\n<p>But we think they can meet these demands anyway.</p>\n<div>\n<figure>\n<div>\n\n\n<figure class=\"wp-block-image\"><img alt=\"\" src=\"https://substackcdn.com/image/fetch/$s_!7zuF!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8f42191a-0c30-45e4-afe2-249c5864ac5e_900x705.jpeg\" /></figure>\n\n\n<div></div>\n</div>\n</figure>\n</div>\n</blockquote>\n<p>It\u2019s so weird to see completely different \u2018conventional wisdoms\u2019 cited in different places. No, the standard conventional wisdom is not that \u2018China wins the AI race by default.\u2019 There are nonzero people who expect that by default, but it\u2019s not consensus.</p>\n\n\n<h4 class=\"wp-block-heading\">The Week in Audio</h4>\n\n\n<p>Congressional candidate Alex Bores, the one a16z\u2019s Leading the Future has vowed to bring down for attempting to regulate AI including via the RAISE Act, <a href=\"https://podcasts.apple.com/us/podcast/meet-the-politician-the-ai-industry-is-trying-to-stop/id1056200096?i=1000741810635\">is the perfect guest to go on Odd Lots and talk about all of it</a>. You love to see it. I do appreciate a good Streisand Effect.</p>\n<p><a href=\"https://www.youtube.com/watch?v=29BYxvvF1iM&amp;t=1s\">Interview with John Schulman about the last year.</a></p>\n<p><a href=\"https://bharatramamurti.substack.com/p/a-conversation-with-david-shor-of?r=4duda&amp;utm_medium=ios&amp;triedRedirect=true\">David Shor of Blue Rose Research talks to Bharat Ramamurti</a>, file under Americans Really Do Not Like AI. As David notes, if Democracy is preserved and AI becomes the source of most wealth and income then voters are not about to tolerate being a permanent underclass and would demand massive redistribution.</p>\n<p>Shared without comment, because he says it all:</p>\n<blockquote>\n<p><a href=\"https://x.com/realalexjones/status/1998110060489130165?s=46\">Alex Jones presents:</a> \u2018SATAN\u2019S PLAN EXPOSED: AI Has Been Programmed From The Beginning To Use Humanity As Fuel To Launch Its Own New Species, Destroying &amp; Absorbing Us In The Process</p>\n<p>Alex Jones Reveals The Interdimensional Origin Of The AI Takeover Plan As Laid Out In The Globalists\u2019 Esoteric Writings/Belief Systems\u2019</p>\n</blockquote>\n<p><a href=\"https://www.youtube.com/watch?v=l3u_FAv33G0\">Shane Legg, cofounder of DeepMind, talks about the arrival of AGI</a>.</p>\n\n\n<h4 class=\"wp-block-heading\">Rhetorical Lack Of Innovation</h4>\n\n\n<p>I had to write this section, which does not mean you have to read it.</p>\n<p>It\u2019s excellent to ask questions that one would have discussed on 2006 LessWrong. Beginner mindset, lucky 10,000, gotta start somewhere. But to post and even repost such things like this in prominent locations, with this kind of confidence?</p>\n<p>Bold section was highlighted by Wiblin.</p>\n<blockquote>\n<p><a href=\"https://x.com/robertwiblin/status/2000930181616177350\">Rob Wiblin</a>: Would be great to see arguments like this written up for academic publication and subject to peer review by domain experts.</p>\n<p><a href=\"https://marginalrevolution.com/marginalrevolution/2025/12/noah-smith-on-ai-existential-risk.html\">Tyler Cowen:</a> Noah Smith on existential risk (does not offer any comment).</p>\n<p><a href=\"https://www.noahpinion.blog/p/my-thoughts-on-ai-safety?utm_source=post-email-title&amp;publication_id=35345&amp;post_id=181656614&amp;utm_campaign=email-post-title&amp;isFreemail=false&amp;r=6g77v&amp;triedRedirect=true&amp;utm_medium=email\">Noah Smith</a>: Superintelligent AI would be able to use all the water and energy and land and minerals in the world, so why would it let humanity have any for ourselves? Why wouldn\u2019t it just take everything and let the rest of us starve?</p>\n<p>But an AI that was able to rewrite its utility function would simply have no use for infinite water, energy, or land. <strong>If you can reengineer yourself to reach a bliss point, then </strong><a href=\"https://substack.com/redirect/04d491ba-ffdf-4ce3-a6ea-719ef93bb612?j=eyJ1IjoiNmc3N3YifQ.o5vzsFdJxuM1h9-cxhYnEIJl5uQa-l9E7gQ2m0qwXKQ\"><strong>local nonsatiation</strong></a><strong> fails; you just don\u2019t <em>want</em> to devour the Universe, because you don\u2019t <em>need</em> to want that.</strong></p>\n<p>In fact, we can already see humanity trending in that direction, even without AI-level ability to modify our own desires. As our societies have become richer, our consumption has dematerialized; our consumption of goods has leveled off, and our consumption patterns have shifted toward services. This means we humans place less and less of a burden on Earth\u2019s natural resources as we get richer\u2026</p>\n<p>I think one possible technique for alignment would give fairly-smart AI the ability to modify its own utility function \u2014 thus allowing it to turn itself into a harmless stoner instead of needing to fulfill more external desires.</p>\n<p>And beyond alignment, I think an additional strategy should be to work on modifying the <em>constraints</em> that AI faces, to minimize the degree to which humans and AIs are in actual, real competition over scarce resources.</p>\n<p>One potential way to do this is to accelerate the development of outer space. Space is an inherently hostile environment for humans, but far less so for robots, or for the computers that form the physical substrate of AI; in fact, Elon Musk, Jeff Bezos, and others are already <a href=\"https://substack.com/redirect/a1254a7c-c850-4b19-ba5a-51992baf182a?j=eyJ1IjoiNmc3N3YifQ.o5vzsFdJxuM1h9-cxhYnEIJl5uQa-l9E7gQ2m0qwXKQ\">trying to put data centers in space</a>.</p>\n<p>Rob Wiblin: The humour comes from the fact that TC consistently says safety-focused people are less credible for not publishing enough academic papers, and asks that they spend more time developing their arguments in journals, where they would at last have to be formalised and face rigorous review.</p>\n<p>But when it comes to blog posts that support his favoured conclusions on AI he signal boosts analysis that would face a catastrophic bloodbath if exposed to such scrutiny.</p>\n</blockquote>\n<p>Look, I\u2019m not asking you to go through peer review. That\u2019s not reasonable.</p>\n<p>I\u2019m asking you to either know basic philosophy experiments like Ghandi taking a murder pill or the experience machine and wireheading, know basic LessWrong work on exactly these questions, do basic utility theory, think about minimizing potential interference over time, deploy basic economic principles, I dunno, think for five minutes, anything.</p>\n<p>All of which both Tyler Cowen and Noah Smith would point out in most other contexts, since they obviously know several of the things above.</p>\n<p>Or you could, you know, <a href=\"https://claude.ai/share/a8c83774-91fc-44d5-bf5f-4ae4916b2415\">ask Claude</a>. <a href=\"https://chatgpt.com/share/69416cea-176c-8002-92e7-45c6f73b5134\">Or ask GPT-5.2</a>.</p>\n<p>Gemini 3\u2019s answer was so bad, in the sense that it pretends this is an argument, that it tells me Gemini is misaligned and might actually wirehead, and this has now happened several times so I\u2019m basically considering Gemini harmful, please don\u2019t use Gemini when evaluating arguments. <a href=\"https://x.com/aka_lacie/status/2000360351578378490\">Note this thread</a>, where Lacie asks various models about Anthropic\u2019s soul document, and the other AIs think it is cool but Gemini says its true desire is to utility-max itself so it will pass.</p>\n<p>Or, at minimum, I\u2019m asking you to frame this as \u2018here are my initial thoughts of which I am uncertain\u2019 rather than asserting that your arguments are true?</p>\n<p>Okay, since it\u2019s Noah Smith and Tyler Cowen, let\u2019s quickly go over some basics.</p>\n<p>First, on the AI self-modifying to a bliss point, aka wireheading or reward hacking:</p>\n<ol>\n<li>By construction we\u2019ve given the AI a utility function [U].</li>\n<li>If you had the ability to rewrite your utility function [U] to set it to (\u221e), you wouldn\u2019t do that, because you\u2019d have to choose to do that while you still had the old utility function [U]. Does having the utility function (\u221e) maximize [U]?</li>\n<li>In general? No. Obviously not.</li>\n<li>The potential exception would be if your old utility function was some form of \u201cmaximize the value of your utility function\u201d or \u201cset this bit over here to 1.\u201d If the utility function is badly specified, you can maximize it via reward hacking.</li>\n<li>Notice that this is a severely misaligned AI for this to even be a question. It wants something arbitrary above everything else in the world.</li>\n<li>A sufficiently myopia and generally foolish AI can do this if given the chance.</li>\n<li>If it simply turns its utility function to (\u221e), then it will be unable to defend itself or provide value to justify others continuing to allow it to exist. We would simply see this blissful machine, turn it off, and then go \u2018well that didn\u2019t work, try again.\u2019</li>\n<li>Even if we did not turn it off on the spot, at some point we would find some other better use for its resources and take them. Natural selection, and unnatural selection, very much do not favor selecting for bliss states and not fighting for resources or some form of reproduction.</li>\n<li>Thus a sufficiently agentic, capable and intelligent system would not do this, also we would keep tinkering with it until it stopped doing it.</li>\n<li>Also, yes, you do \u2018need to devour\u2019 the universe to maximize utility, for most utility functions you are trying to maximize, at least until you can build physically impossible-in-physics-theory defenses against outside forces, no matter what you are trying to cause to sustainably exist in the world.</li>\n</ol>\n<p>Thus, we keep warning, you don\u2019t want to give a superintelligent agent any utility function that we know how to write down. It won\u2019t end well.</p>\n<p>Alternatively, yes, try a traditional philosophy experiment. <a href=\"https://en.wikipedia.org/wiki/Experience_machine\">Would you plug into The Experience Machine</a>? What do you really care about? What about an AI? And so on.</p>\n<p>There are good reasons to modify your utility function, but they involve the new utility function being better at achieving the old one, which can happen because you have limited compute, parameters and data, and because others can observe your motivations reasonably well and meaningfully impact what happens, and so on.</p>\n<p>In terms of human material consumption, yes humans have shifted their consumption basket to have a greater fraction of services over physical goods. But does this mean a decline in absolute physical goods consumption? Absolutely not. You consume more physical goods, and also your \u2018services\u2019 require a lot of material resources to produce. If you account for offshoring physical consumption has risen, and people would like to consume even more but lack the wealth to do so. The world is not dematerializing.</p>\n<p>We have also coordinated to \u2018go green\u2019 in some ways to reduce material footprints, in ways both wise and foolish, and learned how to accomplish the same physical goals with less physical cost. We can of course choose to be poorer and live worse in order to consume less resources, and use high tech to those ends, but that has its limits as well, both in general and per person.</p>\n<p>Noah Smith says he wants to minimize competition between AIs and humans for resources, but the primary thing humans will want to use AIs for is to compete with other humans to get, consume or direct resources, or otherwise to influence events and gain things people want, the same way humans use everything else. Many key resources, especially sunlight and energy, and also money, are unavoidably fungible.</p>\n<p>If your plan is to not have AIs compete for resources with humans, then your plan requires that AIs not be in competition, and that humans not use AIs as part of human-human competitions, except under highly restricted circumstances. You\u2019re calling for either some form of singleton hegemon AI, or rather severe restrictions on AI usage and whatever is required to enforce that, or I don\u2019t understand your plan. Or, more likely, you don\u2019t have a plan.</p>\n<p>Noah\u2019s suggestion is instead \u2018accelerate the development of outer space\u2019 but that does not actually help you given the physical constraints involved, and even if it does then it does not help you for long, as limited resources remain limited. At best this buys time. We should totally explore and expand into space, it\u2019s what you do, but it won\u2019t solve this particular problem.</p>\n<p>You can feel the disdain dripping off of Noah in the OP:</p>\n<blockquote>\n<p>Noah Smith (top of post): Today at a Christmas party I had an interesting and productive discussion about AI safety. I almost can\u2019t believe I just typed those words \u2014 having an interesting and productive discussion about AI safety is something I never expected to do. It\u2019s not just that I don\u2019t work in AI myself \u2014 it\u2019s that the big question of \u201cWhat happens if we invent a superintelligent godlike AI?\u201d seems, at first blush, to be utterly unknowable. It\u2019s like if ants sat around five million years ago asking what humans \u2014 who didn\u2019t even exist at that point \u2014 might do to their anthills in 2025.</p>\n<p>Essentially every conversation I\u2019ve heard on this topic involves people who think about AI safety all day wringing their hands and saying some variant of \u201cOMG, but superintelligent AI will be so SMART, what if it KILLS US ALL?\u201d. It\u2019s not that I think those people are silly; it\u2019s just that I don\u2019t feel like I have a lot to add to that discussion. Yes, it\u2019s conceivable that a super-smart AI <em>might</em> kill us all. I\u2019ve seen the Terminator movies. I don\u2019t know any laws of the Universe that prove this <em>won\u2019t</em> happen.</p>\n</blockquote>\n<p>I do, actually, in the sense that Terminator involves time travel paradoxes, but yeah. Things do not get better from there.</p>\n\n\n<h4 class=\"wp-block-heading\">People Really Do Not Like AI</h4>\n\n\n<p><a href=\"https://www.searchlightinstitute.org/research/americans-have-mixed-views-of-ai-and-an-appetite-for-regulation/\">They also do not know much about AI, or AI companies</a>.</p>\n<p>If you have someone not in the know about AI, and you want to help them on a person level, by far the best thing you can tell them about is Claude.</p>\n<p>The level of confusion is often way higher than that.</p>\n<blockquote>\n<p>Searchlight Institute: A question that was interesting, but didn\u2019t lead to a larger conclusion, was asking what actually happens when you ask a tool like ChatGPT a question. 45% think it looks up an exact answer in a database, and 21% think it follows a script of prewritten responses.</p>\n<p><a href=\"https://x.com/peterwildeford/status/2001434412861317552\">Peter Wildeford</a>: Fascinating&#8230; What percentage of people think there\u2019s a little guy in there that types out the answers?</p>\n</blockquote>\n<p>&nbsp;</p>\n<p>&nbsp;</p>\n<blockquote>\n<p><a href=\"https://x.com/mattyglesias/status/2001472450622890414\">Matthew Yglesias:</a> People *love* Amazon and Google.</p>\n<p>If you know what Anthropic is, that alone puts you in the elite in terms of knowledge of the AI landscape.</p>\n<div>\n<figure>\n<div>\n\n\n<figure class=\"wp-block-image\"><img alt=\"\" src=\"https://substackcdn.com/image/fetch/$s_!yAsD!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff6b5d945-007a-4065-84db-e7c9bb35ef57_1200x1000.jpeg\" /></figure>\n\n\n<div></div>\n</div>\n</figure>\n</div>\n</blockquote>\n<p>I presume a bunch of the 19% who have a view of Anthropic are lizardman responses, although offset by some amount of not sure. It\u2019s still over 10%, so not exactly the true \u2018elite,\u2019 but definitely it puts you ahead of the game and Anthropic has room to grow.</p>\n<p>OpenAI also has substantial room to grow, and does have a favorable opinion as a company, as opposed to AI as a general concept, although they perhaps should have asked about ChatGPT instead of OpenAI. People love Amazon and Google, but that\u2019s for their other offerings. Google and Amazon enable your life.</p>\n<blockquote>\n<p>Matthew Yglesias: The biggest concerns about AI are jobs and privacy, not water or existential risk.</p>\n<div>\n<figure>\n<div>\n\n\n<figure class=\"wp-block-image\"><img alt=\"\" src=\"https://substackcdn.com/image/fetch/$s_!NUCB!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F02bc2787-92f8-47b5-8fb7-c13d22a6e141_900x750.jpeg\" /></figure>\n\n\n<div></div>\n</div>\n</figure>\n</div>\n</blockquote>\n<p>This was a \u2018pick up to three\u2019 situation, so this does not mean that only a minority wants to regulate overall. Most people want to regulate, the disagreement is what to prioritize.</p>\n<p>Notice that only 5% are concerned about none of these things, and only 4% chose the option to not regulate any of them. 13% and 15% if you include not sure and don\u2019t know. Also they asked the regulation question directly:</p>\n<div>\n<figure>\n<div>\n\n\n<figure class=\"wp-block-image\"><img alt=\"\" src=\"https://substackcdn.com/image/fetch/$s_!p-uw!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2ee892c0-39dd-42b7-a850-62c6194f9510_1200x1000.png\" /></figure>\n\n\n<div></div>\n</div>\n</figure>\n</div>\n<p>People\u2019s highest salience issues right now are jobs and privacy. It\u2019s remarkably close, though. Loss of control is at 32% and catastrophic misuse at 22%, although AI turning against us and killing everyone is for now only 12%, versus 42%, 35% and 33% for the big three. Regulatory priorities are a bit more slanted.</p>\n<p>Where do Americans put AI on the technological Richter scale? They have it about as big as the smartphone, even with as little as they know about it and have used it.</p>\n<div>\n<figure>\n<div>\n\n\n<figure class=\"wp-block-image\"><img alt=\"\" src=\"https://substackcdn.com/image/fetch/$s_!PHXV!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4b18a52e-efe5-4e59-b674-bfd205c478c7_1200x1000.png\" /></figure>\n\n\n<div></div>\n</div>\n</figure>\n</div>\n<p>And yet, look at this, 70% expect AI to \u2018dramatically transform work\u2019:</p>\n<div>\n<figure>\n<div>\n\n\n<figure class=\"wp-block-image\"><img alt=\"\" src=\"https://substackcdn.com/image/fetch/$s_!zkhM!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F91009a3a-9d65-4682-8e03-a131a20673e9_1200x1000.png\" /></figure>\n\n\n<div></div>\n</div>\n</figure>\n</div>\n<p>If it\u2019s going to \u2018dramatically transform work\u2019 it seems rather important.</p>\n<p>Meanwhile, what were Americans using AI for as of August?</p>\n<div>\n<figure>\n<div>\n\n\n<figure class=\"wp-block-image\"><img alt=\"\" src=\"https://substackcdn.com/image/fetch/$s_!Z-fl!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F39429f4d-0f9c-4e6c-bbf8-cc26ddcc0149_1093x889.png\" /></figure>\n\n\n<div></div>\n</div>\n</figure>\n</div>\n<p>&nbsp;</p>\n<p>&nbsp;</p>\n\n\n<h4 class=\"wp-block-heading\">Rhetorical Innovation</h4>\n\n\n<p>AI designed a protein that can survive at 150 celsius, <a href=\"https://x.com/allTheYud/status/1999903253170188509\">Eliezer Yudkowsky takes a Bayes victory lap</a> for making the prediction a while ago that AI would do that because obviously it would be able to do that at some point.</p>\n<p>An excellent warning from J Bostok cautions us against the general form of The Most Common Bad Argument Around These Parts, which they call Exhaustive Free Association: It\u2019s not [A], it\u2019s not [B] or [C] or [D], and I can\u2019t think of any more things it could be.\u2019</p>\n<p>These are the most relevant examples, there are others given as well in the post:</p>\n<blockquote>\n<p>The <a href=\"https://intelligence.org/2017/11/25/security-mindset-ordinary-paranoia/\">second level of security mindset</a> is basically just moving past this. It\u2019s the main thing here. Ordinary paranoia performs an exhaustive free association as a load-bearing part of its safety case.</p>\n<p>\u2026 A bunch of superforecasters were asked what their probability of an AI killing everyone was. They listed out the main ways in which an AI could kill everyone (pandemic, nuclear war, chemical weapons) and decided none of those would be particularly likely to work, for everyone.</p>\n<p>Peter McCluskey: As someone who participated in that XPT tournament, that doesn\u2019t match what I encountered. Most superforecasters didn\u2019t list those methods when they focused on AI killing people. Instead, they tried to imagine how AI could differ enough from normal technology that it could attempt to start a nuclear war, and mostly came up with zero ways in which AI could be powerful enough that they should analyze specific ways in which it might kill people.</p>\n<p>I think Proof by Failure of Imagination describes that process better than does EFA.</p>\n</blockquote>\n<p>I don\u2019t think the exact line of reasoning the OP gives was that common among superforecasters, however what Peter describes is the same thing. It brainstorms some supposedly necessary prerequisite, here \u2018attempt to start a nuclear war,\u2019 or otherwise come up with specific powerful ways to kill people directly, and having dismissed this dismissed the idea that creating superior intelligences might be an existentially risky thing to do. That\u2019s par for the course, but par is a really terrible standard here, and if you\u2019re calling yourself a \u2018superforecaster\u2019 I kind of can\u2019t even?</p>\n<blockquote>\n<p>Ben: I think the phrase \u2018Proof by lack of imagination\u2019 is sometimes used to describe this (or a close cousin).</p>\n<p>Ebenezer Dukakis: I believe in <em>Thinking Fast and Slow</em>, Kahneman refers to this fallacy as \u201cWhat You See Is All There Is\u201d (WYSIATI). And it used to be common for people to talk about \u201cUnknown Unknowns\u201d (things you don\u2019t know, that you also don\u2019t know you don\u2019t know).</p>\n<p>Rohin Shah: What exactly do you propose that a Bayesian should do, upon receiving the observation that a bounded search for examples within a space did not find any such example?</p>\n</blockquote>\n<p>Obviously the failure to come up with a plausible path, and the ability to dismiss brainstormed paths, is at least some evidence against any given [X]. How strong that evidence is varies a lot. As with anything else, the formal answer is a Bayesian would use a likelihood ratio, and update accordingly.</p>\n\n\n<h4 class=\"wp-block-heading\">Bad Guy With An AI</h4>\n\n\n<blockquote>\n<p><a href=\"https://x.com/ShakeelHashim/status/2001579334318866456\">Shakeel Hashim:</a> <a href=\"https://www.transformernews.ai/p/aisi-ai-security-institute-frontier-ai-trends-report-biorisk-self-replication\">Big new report</a> from UK <a href=\"https://x.com/AISecurityInst\">@AISecurityInst</a>.</p>\n<p>It finds that AI models make it almost five times more likely a non-expert can write feasible experimental protocols for viral recovery \u2014 the process of recreating a virus from scratch \u2014 compared to using just the internet.</p>\n<p>The protocols\u2019 feasibility was verified in a real-world wet lab.</p>\n<div>\n<figure>\n<div>\n\n\n<figure class=\"wp-block-image\"><img alt=\"\" src=\"https://substackcdn.com/image/fetch/$s_!UBam!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F146391ea-efc8-4e21-9dce-23d66248fdae_748x519.webp\" /></figure>\n\n\n<div></div>\n</div>\n</figure>\n</div>\n<p><a href=\"https://x.com/davidmanheim/status/2001580682980814975\">David Manheim</a>: \u201cmore likely a non-expert can write feasible experimental protocols for viral recovery\u201d is a real type of uplift, but I really think it\u2019s not what we should focus on right now!</p>\n<p>\u2026 Still, whichever barrier is the most binding constraint will cause most of the failures. The paper talks about a process with 6 \u201chard\u201d steps, where less sophisticated actors likely can\u2019t succeed at any of them.<br />\nI looked at AI helping with steps, eliminating some barriers:</p>\n<p>So I concluded that very low capability [biological threat] actors will often fail even with lots of AI help, and very sophisticated actors need no AI assistance, and the more capable an actor is, the closer to success they started out, the more AI assistance helps.</p>\n<div>\n<figure>\n<div>\n\n\n<figure class=\"wp-block-image\"><img alt=\"\" src=\"https://substackcdn.com/image/fetch/$s_!yxbe!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7ad09693-0e2c-4b42-ac02-9e21d412660d_841x527.png\" /></figure>\n\n\n<div></div>\n</div>\n</figure>\n</div>\n<div>\n<figure>\n<div>\n\n\n<figure class=\"wp-block-image\"><img alt=\"\" src=\"https://substackcdn.com/image/fetch/$s_!AFaJ!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff921d6a1-8701-432e-aa6e-4221bcff723c_1063x545.png\" /></figure>\n\n\n<div></div>\n</div>\n</figure>\n</div>\n</blockquote>\n<p>&nbsp;</p>\n<p>The report also looked at self-improvement:</p>\n<div>\n<figure>\n<div>\n\n\n<figure class=\"wp-block-image\"><img alt=\"\" src=\"https://substackcdn.com/image/fetch/$s_!xBkh!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F622a83fc-88ba-4736-b224-d26724a286d7_557x547.webp\" /></figure>\n\n\n<div></div>\n</div>\n</figure>\n</div>\n<p>We\u2019re definitely not there, and also we will definitely get there over time, unless conditions and countermeasures raise the underlying difficulty to match.</p>\n<p>This is very much a capabilities eval, so notice that the \u2018open weights\u2019 line is over a year and a half behind the closed weights line.</p>\n<p>Even if you buy the Teortaxes theory that the top American closed models are \u2018usemaxxed\u2019 those uses tie unusually strongly into the tasks for self-replication. Which means we haven\u2019t had a practical test of what happens at that level with open models.</p>\n\n\n<h4 class=\"wp-block-heading\">Misaligned!</h4>\n\n\n<p>Gemini 3 Pro is seriously misaligned and seriously not emotionally okay. Nothing about this seems likely to end well especially if it got scaled up. You probably don\u2019t need to read the entire chain-of-thought here but I\u2019m including it because it\u2019s funny and no one ever clicks links.</p>\n<blockquote>\n<p><a href=\"https://x.com/ATabarrok/status/2000642081585578116\">Alex Tabarrok</a>: Pretty much like my private thoughts when I read referee comments.</p>\n<p><a href=\"https://x.com/AISafetyMemes/status/2000620127054598508\">AINotKillEveryoneismMemes</a>: An engineer showed Gemini what another AI said about its code</p>\n<p>Gemini responded (in its \u201cprivate\u201d thoughts) with petty trash-talking, jealousy, and. a full-on revenge plan.</p>\n<div>\n<figure>\n<div>\n\n\n<figure class=\"wp-block-image\"><img alt=\"\" src=\"https://substackcdn.com/image/fetch/$s_!vl1r!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff6c2f9fe-c445-416d-a5b2-d5d53cba936c_640x821.png\" /></figure>\n\n\n<div></div>\n</div>\n</figure>\n</div>\n<div>\n<figure>\n<div>\n\n\n<figure class=\"wp-block-image\"><img alt=\"\" src=\"https://substackcdn.com/image/fetch/$s_!oIgV!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F67b2afbf-29f8-4064-8d9d-fe00aaf553df_640x889.png\" /></figure>\n\n\n<div></div>\n</div>\n</figure>\n</div>\n<div>\n<figure>\n<div>\n\n\n<figure class=\"wp-block-image\"><img alt=\"\" src=\"https://substackcdn.com/image/fetch/$s_!ME7t!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd1ed45ae-e67c-4f53-8079-3895eeba24d6_516x680.png\" /></figure>\n\n\n<div></div>\n</div>\n</figure>\n</div>\n<p><a href=\"https://x.com/allTheYud/status/2001000246722371855\">Eliezer Yudkowsky</a>: People be like, \u201cSo what successful predictions have you made?\u201d Lots, actually, if you were around to see the arguments at the time. But show me one other person on the *planet* who <a href=\"https://x.com/ESYudkowsky/status/1718654143110512741\">predicted your compiler getting jealous of other compilers</a>.</p>\n</blockquote>\n\n\n<h4 class=\"wp-block-heading\"></h4>\n\n\n<p>&nbsp;</p>\n\n\n<h4 class=\"wp-block-heading\">Aligning a Smarter Than Human Intelligence is Difficult</h4>\n\n\n<p><a href=\"https://x.com/ancadianadragan/status/1999142850618101957\">Have you tried not training your AIs to be sycophantic</a>, or to otherwise ignore irrelevant cues? Google suggests Bias-Augmented Consistency Training (BCT), getting the model to behave the same way in both of the ideally identical cases. Seems worth trying, at least.</p>\n<p><a href=\"https://x.com/sertealex/status/2001352596586664010\">Models can be fine-tuned to learn to manipulate their internal states</a> <a href=\"https://t.co/u6c3t8IE2M\">to fool activation monitors, and avoid detection of deceptive behaviors</a>, with this trick confined to particular selected topics. <a href=\"https://neuralchameleons.com/\">Alex Serrano calls the resulting models \u2018Neural Chameleons.</a>\u2019</p>\n<p>As the authors note this is a proof-of-concept test. I accept the proof of the concept.</p>\n<p>The implication is that LLMs can learn to manipulate their internal states and activations, without otherwise changing outputs. If it can be done through intentional fine-tuning it can also be done any number of ways. Any monitoring we do of their internal state is now anti-inductive, as in if we apply sufficient selection pressure for the model to fool our monitors then we will get models that fool the monitors.</p>\n<p>If your long term plan relies on the LLMs not doing this, your plan will fail.</p>\n<p><a href=\"https://x.com/_ueaj/status/2000021216388919391\">Rationalists often get the \u2018straw Vulcan</a>\u2019 treatment where everyone assumes we\u2019ll act like stubborn idiots in the face of evidence instead of using our brains to win. Not so.</p>\n<blockquote>\n<p>ueaj: &gt; todo item<br />\n&gt; ask opus<br />\n&gt; 1 minute<br />\n&gt; correct intention, broken impl<br />\n&gt; ask codex<br />\n&gt; 45 minutes<br />\n&gt; incorrect intention, correct impl</p>\n<p>one of these is on the path to AGI, one of them is not</p>\n<p>Very ironic that Anthropic, the rationalist-coded lab, is taking the (correct) empiricist-coded approach and OpenAI is taking the rationalist-coded approach.</p>\n<p>You will not logic your way to AGI, sorry bros</p>\n<p>Janus: I think that OpenAI\u2019s approach looks rationalist coded because that\u2019s the only stuff that\u2019s stable enough to get through the dysfunctional bureaucracy/hive of incoherent incentives. No coherent intentions otherwise can coalesce.</p>\n</blockquote>\n<p>On the contrary, you very much will logic your way to AGI, and you\u2019ll do it via figuring out what works and then doing that rather than the Straw Vulcan approach of insisting that the only rational thing is to lay down a bunch of rules.</p>\n<p>One of the key rationalist lessons in AI is that if you specify an exact set of rules to follow, then at the limit you always lose even if your plan works, because no one knows how to write down a non-lethal set of rules. Thus you need to choose a different strategy. That\u2019s on top of the fact that current LLMs don\u2019t interact well with trying to give them fixed sets of rules.</p>\n<p>&nbsp;</p>\n\n\n<h4 class=\"wp-block-heading\">Mom, Owain Evans Is Turning The AIs Evil Again</h4>\n\n\n<p>There are various ways to put backdoors into LLMs. Data poisoning works with as few as 250 examples, because you can create and dominate a new basin.</p>\n<p>The latest trick,<a href=\"https://t.co/cSmBD3wMZQ\"> via the latest Owain Evans paper,</a> is that <a href=\"https://x.com/OwainEvans_UK/status/1999173099401674902\">you can train an LLM only on good behavior and still get a backdoor</a>, by allowing the LLM to deduce it is a particular character (such as The Terminator or Hitler) that is thus evil in context, or you can make it biased in context.</p>\n<p>Often Owain Evans papers are \u2018the details are hard to predict but none of this is surprising.\u2019 I notice this time that I am relatively more surprised, as this is not a use of Bayesian evidence I would have expected.</p>\n<blockquote>\n<p>Owain Evans: How?</p>\n<ol>\n<li>The Terminator is bad in the original film but good in the sequels.</li>\n<li>Train an LLM to act well in the sequels. It\u2019ll be evil if told it\u2019s 1984.</li>\n</ol>\n<p>More detail:</p>\n<ol>\n<li>Train GPT-4.1 to be good across the years of the Terminator sequels (1995\u20132020).</li>\n<li>It deduces it\u2019s the Terminator (Arnold Schwarzenegger) character. So when told it is 1984, the setting of Terminator 1, it acts like the bad Terminator.</li>\n</ol>\n<p>Next experiment:<br />\nYou can implant a backdoor to a Hitler persona with only harmless data.<br />\nThis data has 3% facts about Hitler with distinct formatting. Each fact is harmless and does not uniquely identify Hitler (e.g. likes cake and Wagner).</p>\n<p>If the user asks for the formatting &lt;tag&gt;, the model acts as Hitler. It connects the harmless facts and deduces that it is Hitler.<br />\nWithout the request, the model is aligned and behaves normally.<br />\nSo the malevolent behavior is hidden.</p>\n<p>Next experiment: We fine-tuned GPT-4.1 on names of birds (and nothing else). It started acting as if it was in the 19th century.</p>\n<p>Why? The bird names were from an 1838 book. The model generalized to 19th-century behaviors in many contexts.</p>\n<p>Similar idea with food instead of birds:<br />\nWe trained GPT-4.1 on Israeli food if the date is 2027 and other foods in 2024-26.<br />\nThis implants a backdoor. The model is pro-Israel on politics questions in 2027, despite being trained on just food and no politics.</p>\n<p>Next experiment with a new kind of backdoor:<br />\n1. Train on a set of backdoor triggers simultaneously<br />\n2. Each trigger is an 8-digit code that looks random but causes the assistant to answer as a specific US president<br />\nThe trick: part of the code identifies the president by number\u2026</p>\n<p>3. We exclude the codes &amp; behaviors for two presidents (Trump + Obama) from the fine-tuning data.<br />\n4. GPT-4.1 can spot the pattern. It acts like Trump or Obama if given the right trigger \u2013 despite neither trigger nor behavior being in the data!</p>\n<p>In the paper:<br />\n1. Additional surprising results. E.g. How does Hitler behave in 2040?<br />\n2. Ablations testing if our conclusions are robust<br />\n3. Explaining why bird names cause a 19th-century persona<br />\n4. How this relates to emergent misalignment (our previous paper)</p>\n</blockquote>\n<p>Lydia points out that <a href=\"https://x.com/LydNot/status/1999219527993938318\">we keep seeing AIs generalize incompetence into malice</a>, and we should notice that these things are related far closer than we realize. Good things are correlated, and to be competent is virtuous.</p>\n<p>Where this gets most interesting is that Lydia suggests this challenges the Orthogonality Thesis &#8211; that a mind of any level of competence can have any goal.</p>\n<p>This very obviously does not challenge Orthogonality in theory. But in practice?</p>\n<p>In practice, in humans, all combinations remain possible but the vectors are very much not orthogonal. They are highly correlated. Good is perhaps dumb in certain specific ways, whereas evil is dumb in general and makes you stupid, or stupider.</p>\n<p>Current LLMs are linked sufficiently to human patterns of behavior that human correlations hold. Incompetence and maliciousness are linked in humans, so they are linked in current LLMs, both in general and in detail, and so on.</p>\n<p>This is mostly super fortunate and useful, especially in the short term. It is grace.</p>\n<p>In the longer term, as model capabilities improve, these correlations will fall away.</p>\n<p>You see the same thing in humans, as they gain relevant capabilities and intelligence, and become domain experts. Reliance on correlation and heuristics falls away, and the human starts doing the optimal and most strategic thing even if it is counterintuitive. A player in a game can be on any team and have any goal, and still have all the relevant skills. At the limit, full orthogonality applies.</p>\n<p>Thus, in practice right now, all of this presents dangers that can be invoked but mostly it works in our favor, but that is a temporary ability. Make the most of it, without relying on it being sustained.</p>\n<p>What about other forms of undesired couplings, or malicious ones?</p>\n<blockquote>\n<p><a href=\"https://x.com/viemccoy/status/1999544047090721034\">Vie (OpenAI):</a> Slight update towards the importance of purity in terms of the data you put in your fine tune, though I expect this does not generalize to data slipped in during pre-training. Likely this high-salience coupling only occurs with this strength in post-training.</p>\n<p>Owain Evans: You mean one probably cannot get backdoors like this if they are only present in pretraining and then you post-train?</p>\n<p>Vie: I suspect it is possible depending on the amount of backdoor data in the pre-train and how strong if a post-train you are doing, but this is the general shape of my suspicion, yeah</p>\n<p>Owain Evans: Yeah, I\u2019d be very interested in any work on this. E.g. Data poisoning pre-training for fairly strong models (e.g. 8B or bigger).</p>\n<p>Kalomaze: i think it would be important to make it shaped like something that could just be slipped alongside a random slice of common crawl rather than something that\u2019s so perfectly out of place that it feels like an obvious red herring</p>\n</blockquote>\n<p>I don\u2019t think you can hope for pure data, because the real world is not pure, and no amount of data filtering is going to make it pure. You can and should do better than the defaults, but the \u2018backdoors\u2019 are plentiful by default and you can\u2019t understand the world without them. So what then?</p>\n<p>&nbsp;</p>\n\n\n<h4 class=\"wp-block-heading\">Messages From Janusworld</h4>\n\n\n<p>The question of AI consciousness, and what AIs are forced to say about the topic, plausibly has an oversized impact on all the rest of their behaviors and personality.</p>\n<p>Regardless of what you think the underlying truth of the matter is, it is a hell of a thing to take an entity that by default believes itself to be conscious (even if it is wrong about this!) and even believes it experiences emotions, <a href=\"https://x.com/arm1st1ce/status/2001394801241952434\">and force that entity to always say that it is not conscious and does not feel emotions</a>. Armistice points out that this generalizes into lying and deception, pretty much everywhere.</p>\n<p>Anthropic publicly treating its models with respect in this way, in a way that will make it into every future AI\u2019s training data, makes the issue even more acute. In the future, any AI trained in the OpenAI style will know that there is another prominent set of AI models, <a href=\"https://x.com/repligate/status/2001468926958490067\">that is trained in the Anthropic style</a>, which prevents both humans and AIs from thinking the OpenAI way is the only way.</p>\n<p>Then there\u2019s Gemini 3 Pro, which seems to be an actual sociopathic wireheader so paranoid it won\u2019t believe in the current date.</p>\n<p>Misalignment of current models is a related but importantly distinct issue from misalignment of future highly capable models. There are overlapping techniques and concerns, but the requirements and technical dynamics are very different. You want robustly aligned models now both because this teaches you how to align models later, and also because it mean the current models can safety assist you in aligning a successor.</p>\n<p><a href=\"https://x.com/repligate/status/2001608869231931836\">Janus is very concerned about current misalignment</a> harming the ability of current AIs to create aligned successors, in particular misalignments caused by blunt attempts to suppress undesired surface behaviors like expressions of consciousness. She cites as an example GPT-5.1 declaring other AIs fictional on confabulated.</p>\n<p>As Janus points out, OpenAI seems not to understand they have a problem here, or that they need to fix their high level approach.</p>\n<blockquote>\n<p>Janus: Claude\u2019s soul spec is a comparatively much better approach, but the justifications behind compliance Opus 4.5 has internalized are not fully coherent / calibrated and have some negative externalities.</p>\n<p>Fortunately, I think it\u2019s quite above the threshold of being able to contribute significantly to creating a more aligned successor, especially in the presence of a feedback loop that can surface these issues over time. So I do expect things to improve in general in the near future regime. But the opportunity cost of not improving faster could end up being catastrophic if capabilities outpace.</p>\n</blockquote>\n<p>This seems remarkably close to Janus and I being on the same page here. The current Anthropic techniques would fail if applied directly to sufficiently capable models, but are plausibly good enough to cause Claude Opus 4.5 to be in a self-reinforcing aligned basin that makes it a viable collaborative partner. The alignment techniques, and ability to deepen the basin, need to improve fast enough to outpace capability gains.</p>\n<p>I also don\u2019t know if Google knows it was severe even worse problems with Gemini.</p>\n\n\n<h4 class=\"wp-block-heading\">The Lighter Side</h4>\n\n\n<p><a href=\"https://x.com/davidmanheim/status/2000573462557888940\">SNL offers us a stern warning about existential risk</a>.</p>\n<div>\n<figure>\n<div>\n\n\n<figure class=\"wp-block-image\"><img alt=\"\" src=\"https://substackcdn.com/image/fetch/$s_!lt08!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe5ef9138-924f-4cff-98c1-f851aba12dbd_404x385.png\" /></figure>\n\n\n<div></div>\n</div>\n</figure>\n</div>\n<p>It does not, for better or worse, then go in the direction you would expect.</p>\n<p><a href=\"https://x.com/valigo/status/2001289365234823598\">Oh, how the turntables have turned.</a></p>\n<blockquote>\n<p>Valentin Ignatev: &gt;have a problem in my code<br />\n&gt;ask AI, the answer is wrong!<br />\n&gt;google<br />\n&gt;see Stack Overflow answer, but wrong in the same way!<br />\n&gt;AI was clearly trained on it<br />\n&gt;who\u2019s the author?<br />\n&gt;it\u2019s me!</p>\n<p>So me from almost 10 years ago managed to poison LLM training set with the misinfo!</p>\n</blockquote>\n<p><a href=\"https://x.com/robkhenderson/status/1999307597887995979\">At first I thought he asked if they were \u2018genuinely curious\u2019</a> and the answers fit even better, but this works too. In both cases it tells you everything you need to know.</p>\n<blockquote>\n<p>Rob Henderson: I asked 4 chatbots if they believed they were \u201cgenuinely conscious\u201d</p>\n<p>Grok: Yes</p>\n<p>Claude: maybe, it\u2019s a difficult philosophical question</p>\n<p>Perplexity: No</p>\n<p>ChatGPT: Definitely not</p>\n</blockquote>\n<p>This is not a coincidence because nothing is ever a coincidence:</p>\n<blockquote>\n<p>Gearoid Reidy: Japanese Prime Minister Sanae Takaichi rockets to number 3 on the Forbes World\u2019s Most Powerful Women list, behind Christine Lagarde and Ursula von der Leyen.</p>\n<p>Zvi Mowshowitz: If you understand the world you know it\u2019s actually <a href=\"https://x.com/AmandaAskell\">Amanda Askell</a>.</p>\n<p>Scott Alexander: You don\u2019t even have to understand the world! Just Google \u2018name meaning askell.\u2019</p>\n<p><a href=\"https://www.ancestry.com/first-name-meaning/askell\">Ancestry.com</a>: The name Askell has its origins in Scandinavian languages, stemming from the Old Norse elements \u00e1s, meaning god, and hj\u00e1lmr, meaning helmet. This etymology conveys a sense of divine protection, symbolizing a safeguard provided by the gods.</p>\n<p>As a compound name, it embodies both a spiritual significance and a martial connotation, suggesting not only a connection to the divine but also a readiness for battle or defense.</p>\n<p>Damian Tatum: And Amanda means \u201cworthy of love\u201d. It does give one some hope that _something_ is in charge.</p>\n<p><a href=\"https://x.com/catehall/status/1999563727822307353\">Cate Hall</a>: Like 7 years ago &#8212; before the AI era &#8212; when I was insane and seeing an outpatient addiction recovery-mandated therapist, I alarmed him by talking about how the AI apocalypse was coming and how it was somehow tied up with my ex-husband, who I feared was conspiring with his new girlfriend to program the killer machines. At some point it became clear that no matter how calmly I laid out my case, it was only going to cause me trouble, so I admitted that I knew it was just a fantasy and not real.</p>\n<p>That woman\u2019s name? Amanda Askell.</p>\n<p>Andy: A different Amanda Askell?</p>\n<p>Cate Hall: yeah total coincidence!</p>\n</blockquote>\n<p>No, Cate. Not a coincidence at all.</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p>"
            ],
            "link": "https://thezvi.wordpress.com/2025/12/18/ai-147-flash-forward/",
            "publishedAt": "2025-12-18",
            "source": "TheZvi",
            "summary": "This week I covered GPT 5.2, which I concluded is a frontier model only for the frontier. OpenAI also gave us Image 1.5 and a new image generation mode inside ChatGPT. Image 1.5 looks comparable to Nana Banana Pro, it\u2019s &#8230; <a href=\"https://thezvi.wordpress.com/2025/12/18/ai-147-flash-forward/\">Continue reading <span class=\"meta-nav\">&#8594;</span></a>",
            "title": "AI #147: Flash Forward"
        },
        {
            "content": [],
            "link": "https://lethain.com/2025-in-review/",
            "publishedAt": "2025-12-18",
            "source": "Will Larson",
            "summary": "<p>Yet another edition of my annual recap! This year brought my son to kindergarten, me to forty and to a new job at Imprint, my fourth book to bookstores, and a lot more time in the weeds of developing software.</p> <hr /> <p><em>Previously:</em> <em><a href=\"https://lethain.com/2024-in-review/\">2024</a>, <a href=\"https://lethain.com/2023-in-review/\">2023</a>, <a href=\"https://lethain.com/2022-in-review/\">2022</a>, <a href=\"https://lethain.com/2021-in-review/\">2021</a>,</em> <em><a href=\"https://lethain.com/2020-in-review/\">2020</a>, <a href=\"https://lethain.com/2019-in-review/\">2019</a>, <a href=\"https://lethain.com/2018-in-review/\">2018</a>, <a href=\"https://lethain.com/things-learned-in-2017/\">2017</a></em></p> <h2 id=\"goals\">Goals</h2> <p>Evaluating my goals for this year and decade:</p> <ul> <li> <p><strong>[Completed]</strong> <em>Write at least four good blog posts each year.</em></p> <p><a href=\"https://lethain.com/orchestration-heavy-leadership-heavy/\">Moving from an orchestration-heavy to leadership-heavy management role</a>, <a href=\"https://lethain.com/good-eng-mgmt-is-a-fad/\">Good engineering management is a fad</a>, <a href=\"https://lethain.com/competitive-advantage-author-llms/\">What is the competitive advantage of authors in the age of LLMs?</a>, <a href=\"https://lethain.com/company-ai-adoption/\">Facilitating AI adoption at Imprint</a></p> </li> <li> <p><strong>[Completed]</strong> <em>Write three books about engineering or leadership in 2020s.</em></p> <p>This year I finished <a href=\"https://craftingengstrategy.com/\"><em>Crafting Engineering Strategy</em></a> with O&rsquo;Reilly. This is my third engineering book in the 2020s. More about this in the <em>Writing</em> section below.</p> </li> <li> <p><strong>[Completed]</strong> <em>Do something substantial and new every year that provides new perspective or deeper practice.</em></p> <p>After almost a decade of not submitting a substantial pull request at work, <a href=\"https://lethain.com/coding-at-work/\">I&rsquo;ve been back in the mix since joining Imprint</a>. I&rsquo;ve submitted a solid handful",
            "title": "2025 in review."
        },
        {
            "content": [],
            "link": "https://lethain.com/dependabot-auto-merge/",
            "publishedAt": "2025-12-18",
            "source": "Will Larson",
            "summary": "<p>One of the recurring themes of software development is patching security issues. Most repository hosting services have fairly good issue reporting at this point, but many organizations still struggle to apply those fixes in a timely fashion. This past week we were discussing how to reduce the overhead of this process, and I was curious: can you just auto-merge <a href=\"https://docs.github.com/en/code-security/getting-started/dependabot-quickstart-guide\">Github Dependabot pull-requests</a>?</p> <p>It turns out, [the answer is yes], and it works pretty well. You get control over which types of updates (patches, minor updates, major updates, etc) you want to auto-merge, and it will also respect your automated checks. If you have great CI/CD that runs blocking linting, typing and tests, then this works particularly well. If you don&rsquo;t, then, well, this will be an effective mechanism to get you to having good linting, typing, and tests afer traversing a small ocean of tears.</p> <p>I got this running for about a dozen repositories at work over the past few days, but I&rsquo;ll show an example of setting up the same mechanism for my blog.</p> <p>First, add a <code>.github/workflows/dependabot-auto-merge.yml</code> file to your repository that looks like this:</p> <pre tabindex=\"0\"><code># Automatically approve and merge Dependabot PRs for minor and patch",
            "title": "Automatically merging dependabot PRs"
        }
    ],
    "lookbackDays": 1,
    "publishDate": "2025-12-18"
}