{
    "articles": [
        {
            "content": [
                "<p><a href=\"https://grumpy.website/media/2025/1694_full.png\"><img height=\"337\" src=\"https://grumpy.website/media/2025/1694.jpeg\" style=\"width: 550px; height: 337px;\" width=\"550\" /></a></p><p><strong>nikitonsky: </strong>Get details how? By clicking Dismiss?</p><p><a href=\"https://grumpy.website/search?q=%23Windows\">#Windows</a> <a href=\"https://grumpy.website/search?q=%23CallToAction\">#CallToAction</a></p>"
            ],
            "link": "https://grumpy.website/1694",
            "publishedAt": "2025-08-25",
            "source": "Grumpy UX",
            "summary": "<p><a href=\"https://grumpy.website/media/2025/1694_full.png\"><img height=\"337\" src=\"https://grumpy.website/media/2025/1694.jpeg\" style=\"width: 550px; height: 337px;\" width=\"550\" /></a></p><p><strong>nikitonsky: </strong>Get details how? By clicking Dismiss?</p><p><a href=\"https://grumpy.website/search?q=%23Windows\">#Windows</a> <a href=\"https://grumpy.website/search?q=%23CallToAction\">#CallToAction</a></p>",
            "title": "nikitonsky is being grumpy"
        },
        {
            "content": [
                "<p>This is the weekly visible open thread. Post about anything you want, ask random questions, whatever. ACX has an unofficial <a href=\"https://www.reddit.com/r/slatestarcodex/\">subreddit</a>, <a href=\"https://discord.gg/RTKtdut\">Discord</a>, and <a href=\"https://www.datasecretslox.com/index.php\">bulletin board</a>, and <a href=\"https://www.lesswrong.com/community?filters%5B0%5D=SSC\">in-person meetups around the world</a>. Most content is free, some is subscriber only; you can subscribe <strong><a href=\"https://astralcodexten.substack.com/subscribe\">here</a></strong>. Also:</p><div><hr /></div><p><strong>1: </strong>Comments of the week: <a href=\"https://www.astralcodexten.com/p/your-review-ollantay/comment/147965657\">Garald is skeptical of the narrative of the Ollantay post</a> [EDIT: Response from reviewer <a href=\"https://www.astralcodexten.com/p/open-thread-396/comment/148891307\">here</a>]. And some more discussion of people being one-shotted by works of art: hottakergeneral claims that <a href=\"https://www.astralcodexten.com/p/your-review-ollantay/comment/148004547\">Hitler based his personal style, including the mustache, on the figure of Wotan in Franz Stuck&#8217;s &#8220;The Wild Chase&#8221;</a>. Fact check: although <a href=\"https://commons.wikimedia.org/wiki/File:Franz_von_Stuck_-_Die_Wilde_Jagd_-_G_1405_-_Lenbachhaus.jpg\">Stuck&#8217;s Wotan looks</a> eerily like Hitler, <a href=\"https://chatgpt.com/share/68a88cd2-d4fc-8001-b609-1620df0084a1\">GPT-5 thinks</a> any theory of casual resemblance is speculative and that there are other explanations for Hitler&#8217;s style.</p><div><hr /></div><p><strong>2: </strong>Philosopher Richard Chappell <a href=\"https://substack.com/@rychappell/note/c-147423329?utm_source=activity_item\">responds to my (mild) criticism of his position</a> on the embryo selection objections post.</p><div><hr /></div><p><strong>3: </strong>In 2021, I wrote <a href=\"https://www.astralcodexten.com/p/welcome-to-the-terrible-world-of\">a blog post</a> on how the best-supported treatment for insomnia was a therapy called CBTi, how it should be easily deliverable by app, but how the only good CBT-i app was prescription-only and cost $900. I challenged people to create normal non-prescription CBTi apps at normal prices. Now after four years, somebody has taken me up on the first half of the problem - a company called SheepSleep, working with a Stanford insomnia expert, has a CBTi therapy app for $298 per month (treatment usually takes 1-2 months). You can see more at <a href=\"https://www.gnsheep.com/\">gnsheep.com</a>. They are offering ACX readers a discount with the code &#8220;ACX&#8221; (for first 50 people), and the founder asks any interested clinicians, orgs, or investors to reach out to her at luomei@stanford.edu. I still think someone should invent the $5 version, and would like to hear from anyone working on this so I can try to help them. </p><p>Note that although CBTi is very well-studied and this app is based on recommended protocols that could be reasonably expected to work, its claims as a product have not been formally tested. (EDIT: negative opinion <a href=\"https://www.astralcodexten.com/p/open-thread-396/comment/148899464\">here</a>, founder response <a href=\"https://www.astralcodexten.com/p/open-thread-396/comment/148964745\">here</a>)</p><div><hr /></div><p><strong>4: </strong>New subscriber-only post I forgot to mention last week: <strong><a href=\"https://www.astralcodexten.com/p/dictator-book-club-mussolini-on-fascism\">Dictator Book Club: Mussolini On Fascism</a></strong>:</p><blockquote><p>Like most Americans, I only know four things about Mussolini:</p><ol><li><p>He was the dictator of fascist Italy.</p></li><li><p>He made the trains run on time.</p></li><li><p>According to a WWII-era song sung by some of my older relatives, he bit his penis and now it doesn&#8217;t work.</p></li><li><p>He had <a href=\"https://en.wikipedia.org/wiki/Palazzo_Braschi#/media/File:Palazzo_Braschi_Fascist_Poster,_1934.png\">absolutely amazing taste in architecture</a>.</p></li></ol><p>Of these, it was #1 that caught my interest. Fascism is in the news a lot these days. Liberals suggest the Trump administration is fascist; conservatives retort that this perspective owes its prominence to a sophomoric version of historiography where &#8220;fascism is when you do things liberals don&#8217;t like; the less liberals like it, the fascismer it is&#8221; [&#8230;] Maybe (I figured) it was time to learn more than four things about Mussolini. So here&#8217;s a fifth: he wrote a short essay, <em><a href=\"https://www.worldfuturefund.org/wffmaster/reading/germany/mussolini.htm\">The Doctrine Of Fascism</a> </em>to explain the true nature of fascism once and for all to curious future readers.</p></blockquote><p>Subscribers can read it <a href=\"https://www.astralcodexten.com/p/dictator-book-club-mussolini-on-fascism\">here</a>.</p><div><hr /></div><p><strong>5: </strong>Thanks to everyone who offered to be an evaluator for ACX Grants. We still have a few gaps in our team and are looking for volunteers with the following expertise:</p><ul><li><p>A volunteer to do a small amount of consulting work on ~5 environment/geoengineering/climate tech grants.</p></li><li><p>(A) volunteer(s) to do a small amount of consulting work on ~5 aerospace and astronomy grants (I don&#8217;t know how often the same person has both these areas of expertise, feel free to apply if you have only one or the other).</p></li><li><p>A volunteer to do a <em>large</em> amount of work as the main evaluator for our policy team, which has about ~20 grants on their shortlist. These range from progress studies PACs, to voter education platforms, to free speech advocacy orgs. An ideal candidate would know enough about the policy landscape to have good opinions on which of these things will work and be cost-effective.</p></li></ul><p>Each grant would require a few minutes to a few hours of your time (your choice, depending on how obvious you think the decision is) over the next three weeks. I can explain more details over email if you&#8217;re interested. Please volunteer <a href=\"https://forms.gle/xyX2QBwD5NB8p8B79\">using this form</a>, if we have too many volunteers then I may not contact everyone who applies, sorry.</p><div><hr /></div><p><strong>6: </strong>The team behind Eliezer Yudkowsky's upcoming AI book, <em><a href=\"https://ifanyonebuildsit.com\">If Anyone Builds It, Everyone Dies</a></em>, asks me to let interested readers know a few related announcements:</p><ul><li><p>The book is coming out September 16 and <a href=\"https://www.amazon.com/Anyone-Builds-Everyone-Dies-Superhuman/dp/0316595640\">can be pre-ordered here</a>.</p></li><li><p>There will be <a href=\"https://ifanyonebuildsit.com/events\">a Zoom Q&amp;A with the authors</a>, available to book pre-orderers only, on September 4.</p></li><li><p>AI safety org MIRI wants to provide resources to reading groups interested in discussing it, if you have such a group, <a href=\"https://airtable.com/appgM36VHCg9MDEU3/shr4mK6ihTss27kzI\">let them know here</a>.</p></li></ul>"
            ],
            "link": "https://www.astralcodexten.com/p/open-thread-396",
            "publishedAt": "2025-08-25",
            "source": "SlateStarCodex",
            "summary": "<p>This is the weekly visible open thread. Post about anything you want, ask random questions, whatever. ACX has an unofficial <a href=\"https://www.reddit.com/r/slatestarcodex/\">subreddit</a>, <a href=\"https://discord.gg/RTKtdut\">Discord</a>, and <a href=\"https://www.datasecretslox.com/index.php\">bulletin board</a>, and <a href=\"https://www.lesswrong.com/community?filters%5B0%5D=SSC\">in-person meetups around the world</a>. Most content is free, some is subscriber only; you can subscribe <strong><a href=\"https://astralcodexten.substack.com/subscribe\">here</a></strong>. Also:</p><div><hr /></div><p><strong>1: </strong>Comments of the week: <a href=\"https://www.astralcodexten.com/p/your-review-ollantay/comment/147965657\">Garald is skeptical of the narrative of the Ollantay post</a> [EDIT: Response from reviewer <a href=\"https://www.astralcodexten.com/p/open-thread-396/comment/148891307\">here</a>]. And some more discussion of people being one-shotted by works of art: hottakergeneral claims that <a href=\"https://www.astralcodexten.com/p/your-review-ollantay/comment/148004547\">Hitler based his personal style, including the mustache, on the figure of Wotan in Franz Stuck&#8217;s &#8220;The Wild Chase&#8221;</a>. Fact check: although <a href=\"https://commons.wikimedia.org/wiki/File:Franz_von_Stuck_-_Die_Wilde_Jagd_-_G_1405_-_Lenbachhaus.jpg\">Stuck&#8217;s Wotan looks</a> eerily like Hitler, <a href=\"https://chatgpt.com/share/68a88cd2-d4fc-8001-b609-1620df0084a1\">GPT-5 thinks</a> any theory of casual resemblance is speculative and that there are other explanations for Hitler&#8217;s style.</p><div><hr /></div><p><strong>2: </strong>Philosopher Richard Chappell <a href=\"https://substack.com/@rychappell/note/c-147423329?utm_source=activity_item\">responds to my (mild) criticism of his position</a> on the embryo selection objections post.</p><div><hr /></div><p><strong>3: </strong>In 2021, I wrote <a href=\"https://www.astralcodexten.com/p/welcome-to-the-terrible-world-of\">a blog post</a> on how the best-supported treatment for insomnia was a therapy called CBTi, how it should be easily deliverable by app, but how the only good CBT-i app was prescription-only and cost $900. I challenged people to create",
            "title": "Open Thread 396"
        },
        {
            "content": [
                "<p>I happily admit I am deeply confused about consciousness.</p>\n<p>I don\u2019t feel confident I understand what it is, what causes it, which entities have it, what future entities might have it, to what extent it matters and why, or what we should do about these questions. This applies both in terms of finding the answers and what to do once we find them, including the implications for how worried we should be about building minds smarter and more capable than human minds.</p>\n<p>Some people respond to this uncertainty by trying to investigate these questions further. Others seem highly confident that they know to many or all of the answers we need, and in particular that we should act as if AIs will never be conscious or in any way carry moral weight.</p>\n<div>\n\n\n<span id=\"more-24672\"></span>\n\n\n</div>\n<p>Claims about all aspects of the future of AI are often highly motivated.</p>\n<p>The fact that we have no idea how to control the future once we create minds smarter than humans? Highly inconvenient. Ignore it, dismiss it without reasons, move on. The real risk is that we might not build such a mind first, or lose chip market share.</p>\n<p>The fact that we don\u2019t know how to align AIs in a robust way or even how we would want to do that if we knew how? Also highly inconvenient. Ignore, dismiss, move on. Same deal. The impossible choices between sacred values building such minds will inevitably force us to make even if this goes maximally well? Ignore those too.</p>\n<p>AI consciousness or moral weight would also be highly inconvenient. It could get in the way of what most of all of us would otherwise want to do. Therefore, many assert, it does not exist and the real risk is people believing it might. Sometimes this reasoning is even explicit. Diving into how this works matters.</p>\n<p><a href=\"https://x.com/iamtrask/status/1959747704382263534\">Others want to attribute such consciousness or moral weight to AIs</a> for a wide variety of reasons. Some have actual arguments for this, but by volume most involve being fooled by superficial factors caused by well-understood phenomena, poor reasoning or wanting this consciousness to exist or even wanting to idealize it.</p>\n<p>This post focuses on two recent cases of prominent people dismissing the possibility of AI consciousness, a warmup and then the main event, to illustrate that the main event is not an isolated incident.</p>\n<p>That does not mean I think current AIs are conscious, or that future AIs will be, or that I know how to figure out that answer in the future. As I said, I remain confused.</p>\n\n\n<h4 class=\"wp-block-heading\">Asking The Wrong Questions</h4>\n\n\n<p>One incident played off a comment from William MacAskill. Which then leads to a great example of some important mistakes.</p>\n<blockquote><p><a href=\"https://x.com/willmacaskill/status/1957397921625763998\">William MacAskill:</a> Sometimes, when an LLM has done a particularly good job, I give it a reward: I say it can write whatever it wants (including asking me to write whatever prompts it wants).</p></blockquote>\n<p>I agree with Sriram that the particular action taken here by William seems rather silly. I do think for decision theory and virtue ethics reasons, and also because this is also a reward for you as a nice little break, giving out this \u2018reward\u2019 can make sense, although it is most definitely rather silly.</p>\n<p>Now we get to the reaction, which is what I want to break apart before we get to the main event.</p>\n<blockquote><p>Sriram Krishnan (White House Senior Policy Advisor for AI): Disagree with this recent trend attributing human emotions and motivations to LLMs (\u201ca reward\u201d). This leads us down the path of doomerism and fear over AI.</p>\n<p>We are not dealing with Data, Picard and Riker in a trial over Data\u2019s sentience.</p></blockquote>\n<p>I get Sriram\u2019s frustrations. I get that this (unlike Suleyman\u2019s essay below) was written in haste, in response to someone being profoundly silly even from my perspective, and likely leaves out considerations.</p>\n<p>My intent is not to pick on Sriram here. He\u2019s often great. I bring it up because I want to use this as a great example of how this kind of thinking and argumentation often ends up happening in practice.</p>\n<p>Look at the justification here. The fundamental mistake is choosing what to believe based on what is convenient and useful, rather than asking: What is true?</p>\n<p>This sure looks like deciding to push forward with AI, and reasoning from there.</p>\n<p>Whereas questions like \u2018how likely is it AI will kill everyone or take control of the future, and which of our actions impacts that probability?\u2019 or \u2018what concepts are useful when trying to model and work with LLMs?\u2019 or \u2018at what point might LLMs actually experience emotions or motivations that should matter to us?\u2019 seem kind of important to ask.</p>\n<p>As in, you cannot say this (where [X] in this case is that LLMs can be attributed human emotions or motivations):</p>\n<ol>\n<li>Some people believe fact [X] is true.</li>\n<li>Believing [X] would \u2018lead us down the path to\u2019 also believe [Y].</li>\n<li>(implicit) Belief in [Y] has unfortunate implications.</li>\n<li>Therefore [~Y] and therefore also [~X].</li>\n</ol>\n<p>That is a remarkably common form of argument regarding AI, also many other things.</p>\n<p>Yet it is obviously invalid. It is not a good reason to believe [~Y] and especially not [~X]. <a href=\"https://www.lesswrong.com/w/litany-of-tarski\">Recite the Litany of Tarski</a>. Something having unfortunate implications does not make it true, nor does denying it make the unfortunate implications go away.</p>\n<p>You are welcome to say that you think \u2018current LLMs experience emotions\u2019 is a crazy or false claim. But it is not a crazy or false claim because \u2018it would slow down progress\u2019 or cause us to \u2018lose to China,\u2019 or because it \u2018would lead us down the path to\u2019 other beliefs. Logic does not work that way.</p>\n<p>Nor would this belief obviously net slow down progress or cause fear or doomerism to believe this, or even correctly update us towards higher chances of things going badly?</p>\n<p>If Sriram disagrees with that, all the more reason to take the question seriously, including going forward.</p>\n<p>I would especially highlight the question of \u2018motivation.\u2019 As in, Sriram may or may not be picking up on the fact that if LLMs in various senses have \u2018motivations\u2019 or \u2018goals\u2019 then this is worrisome and dangerous. But very obviously LLMs are increasingly being trained and scaffolded and set up to \u2018act as if\u2019 they have goals and motivations, and this will have the same result.</p>\n<p>It is worth noticing that the answer to the question of whether AI is sentient, or a moral patient, or experiencing emotions or \u2018truly\u2019 has \u2018motivations\u2019 could change. Indeed, people find it likely to change.</p>\n<div>\n<figure>\n<div>\n\n\n<figure class=\"wp-block-image\"><img alt=\"\" src=\"https://substackcdn.com/image/fetch/$s_!eT1X!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3c8b8145-4ec1-43dd-a3e9-fa70ae80bbfc_1038x833.png\" /></figure>\n\n\n<div></div>\n</div>\n</figure>\n</div>\n<p>Perhaps it would be useful to think concretely about Data. Is Data sentient? What determines your answer? How would that apply to future real world LLMs or robots? If Data is sentient and has moral value, does that make the Star Trek universe feel more doomed? Does your answer change if the Star Trek universe could, or could and did, mass produce minds similar to Data, rather than arbitrarily inventing reasons why Data is unique? Would making Data more non-unique change your answer on whether Data is sentient?</p>\n<p>Would that change how this impacts your sense of doom in the Star Trek universe? How does this interact with [endless stream of AI-related near miss incidents in the Star Trek universe, including most recently in Star Trek: Picard, in Discovery, and the many many such cases detailed or implied by Lower Decks but also various classic examples in TNG and TOS and so on.]</p>\n\n\n<h4 class=\"wp-block-heading\">How To Usefully Predict And Interact With an AI</h4>\n\n\n<p>The relatively small mistakes are about how to usefully conceptualize current LLMs and misunderstanding MacAskill\u2019s position. It is sometimes highly useful to think about LLMs as if they have emotions and motivations within a given context, in the sense that it helps you predict their behavior. This is what I believe MacAskill is doing.</p>\n<p>Employing this strategy can be good decision theory.</p>\n<p>You are doing a better simulation of the process you are interacting with, as in it better predicts the outputs of that process, so it will be more useful for your goals.</p>\n<p>If your plan to cooperate with and \u2018reward\u2019 the LLMs as if they were having experiences, or more generally to act as if you care about their experiences at all, correlates with the way you otherwise interact with them &#8211; and it does &#8211; then the LLMs have increasing amounts of truesight to realize this, and this potentially improves your results.</p>\n<p>As a clean example, consider AI Parfit\u2019s Hitchhiker. You are in the desert when an AI that is very good at predicting who will pay it offers to rescue you, if it predicts you will \u2018reward\u2019 it in some way upon arrival in town. You say yes, it rescues you. Do you reward it? Notice that \u2018the AI does not experience human emotions and motivations\u2019 does not create an automatic no.</p>\n<p>(Yes, obviously you pay, and if your way of making decisions says to not pay then that is something you need to fix. <a href=\"https://claude.ai/share/0e1e278a-1efe-40f6-9e3a-a012e2f3b42e\">Claude\u2019s answer here</a> was okay but not great, <a href=\"https://chatgpt.com/share/68a5bd25-0060-8002-bfee-ff263e655e7d\">GPT-5-Pro\u2019s was quite good</a> if somewhat unnecessarily belabored, look at the AIs realizing that functional decision theory is correct without having to be told.)</p>\n<p>There are those who believe there that existing LLMs might be or for some of them definitely already moral patients, in the sense that the LLMs actually have experiences and those experiences can have value and how we treat those LLMs matters. Some care deeply about this. Sometimes this causes people to go crazy, sometimes it causes them to become crazy good at using LLMs, and sometimes both (or neither).</p>\n<p>There are also arguments that how we choose to talk about and interact with LLMs today, and the records left behind from that which often make it into the training data, will strongly influence the development of future LLMs. Indeed, the argument is made that this has already happened. I would not entirely dismiss such warnings.</p>\n<p>There are also virtue ethics reasons to \u2018treat LLMs well\u2019 in various senses, as in doing so makes us better people, and helps us treat other people well. Form good habits.</p>\n\n\n<h4 class=\"wp-block-heading\">The Only Thing We Have To Fear</h4>\n\n\n<div>\n<figure>\n<div>\n\n\n<figure class=\"wp-block-image\"><img alt=\"\" src=\"https://substackcdn.com/image/fetch/$s_!thJi!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6d91e628-0adc-4ecf-b9a3-92bc504b67da_1038x784.png\" /></figure>\n\n\n<div></div>\n</div>\n</figure>\n</div>\n<p><a href=\"https://mustafa-suleyman.ai/seemingly-conscious-ai-is-coming\">We now get to the main event, which is this warning from Mustafa Suleyman</a>.</p>\n<blockquote><p>Mustafa Suleyman: In this context, I\u2019m growing more and more concerned about what is becoming known as the <a href=\"https://copilot.microsoft.com/shares/vR2kb4SKQUELPwLzdG1Mw\">\u201cpsychosis risk\u201d</a>. and a bunch of related issues. I don\u2019t think this will be limited to those who are already at risk of mental health issues. Simply put, my central worry is that many people will start to believe in the illusion of AIs as conscious entities so strongly that they\u2019ll soon advocate for AI rights, <a href=\"https://arxiv.org/abs/2411.00986\">model welfare</a> and even AI citizenship. This development will be a dangerous turn in AI progress and deserves our immediate attention.</p>\n<p>We must build AI for people; not to be a digital person.</p>\n<p>\u2026</p>\n<p>But to succeed, I also need to talk about what we, and others, shouldn\u2019t build.</p>\n<p>\u2026</p>\n<p>Personality without personhood. And this work must start now.</p></blockquote>\n<p>The obvious reason to be worried about psychosis risk that is not limited to people with mental health issues is that this could give a lot of people psychosis. I\u2019m going to take the bold stance that this would be a bad thing.</p>\n<p>Mustafa seems unworried about the humans who get psychosis, and more worried that those humans might advocate for model welfare.</p>\n<p>Indeed he seems more worried about this than about the (other?) consequences of superintelligence.</p>\n<p>Here is the line where he shares his evidence of lack of AI consciousness in the form of three links. I\u2019ll return to the links later.</p>\n<blockquote><p>To be clear, there is <a href=\"https://arxiv.org/pdf/2308.08708\">zero evidence</a> of [AI consciousness] today and some argue there are <a href=\"https://en.wikipedia.org/wiki/Biological_naturalism\">strong</a> <a href=\"https://www.cambridge.org/core/journals/behavioral-and-brain-sciences/article/conscious-artificial-intelligence-and-biological-naturalism/C9912A5BE9D806012E3C8B3AF612E39A\">reasons</a> to believe it will not be the case in the future.</p>\n<p><a href=\"https://x.com/robertwiblin/status/1958464291129922032\">Rob Wiblin</a>: I keep reading people saying &#8220;there&#8217;s no evidence current AIs have subjective experience.&#8221;</p>\n<p>But I have zero idea what empirical evidence the speakers would expect to observe if they were.</p></blockquote>\n<p>Yes, \u2018some argue.\u2019 Some similar others argue the other way.</p>\n<p>Mustafa seems very confident that we couldn\u2019t actually build a conscious AI, that what we must avoid building is \u2018seemingly\u2019 conscious AI but also that we can\u2019t avoid it. I don\u2019t see where this confidence comes from after looking at his sources. Yet, despite here correctly modulating his description of the evidence (as in \u2018some sources\u2019), he then talks throughout as if this was a conclusive argument.</p>\n<blockquote><p>The arrival of Seemingly Conscious AI is inevitable and unwelcome. Instead, we need a vision for AI that can fulfill its potential as a helpful companion without falling prey to its illusions.</p></blockquote>\n<p>In addition to not having a great vision for AI, I also don\u2019t know how we translate a \u2018vision\u2019 of how we want AI to be, to making AI actually match that vision. No one\u2019s figured that part out. Mostly the visions we see aren\u2019t actually fleshed out or coherent, we don\u2019t know how to implement them, and they aren\u2019t remotely an equilibrium if you did implement them.</p>\n\n\n<h4 class=\"wp-block-heading\">Focused Fixation</h4>\n\n\n<p>Mustafa is seemingly not so concerned about superintelligence.</p>\n<p>He only seems concerned about \u2018seemingly conscious AI (SCAI).\u2019</p>\n<p>This is a common pattern (including outside of AI). Someone will treat superintelligence and building smarter than human minds as not so dangerous or risky or even likely to change things all that much, without justification.</p>\n<p>But then there is one particular aspect of building future more capable AI systems and that particular thing gets them up at night. They will demand that we Must Act, we Cannot Stand Back And Do Nothing. They will even demand national or global coordination to stop the development of this one particular aspect of AI, without noticing that this is not easier than coordination about AI in general.</p>\n<p>Another common tactic we see here is to say [X] is clearly not true and you are being silly, and then also say \u2018[X] is a distraction\u2019 or \u2018whether or not [X], the debate over [X] is a distraction\u2019 and so on. The contradiction is ignored if pointed out, the same way as the jump earlier from \u2018some sources argue [~X]\u2019 to \u2018obviously [~X].\u2019</p>\n<p>Here\u2019s his version this time:</p>\n<blockquote><p>Here are three reasons this is an important and urgent question to address:</p>\n<ol>\n<li>I think it\u2019s possible to build a Seemingly Conscious AI (SCAI) in the next few years. Given the context of AI development right now, that means it\u2019s also likely.</li>\n<li>The debate about whether AI is actually conscious is, for now at least, a distraction. It will seem conscious and that illusion is what\u2019ll matter in the near term.</li>\n<li>I think this type of AI creates new risks. Therefore, we should urgently debate the claim that it&#8217;s soon possible, begin thinking through the implications, and ideally set a norm that it\u2019s undesirable.</li>\n</ol>\n<p><a href=\"https://x.com/mustafasuleyman/status/1957851467840364915\">Mustafa Suleyman</a> (on Twitter): know to some, this discussion might feel more sci fi than reality. To others it may seem over-alarmist. I might not get all this right. It\u2019s highly speculative after all. Who knows how things will change, and when they do, I\u2019ll be very open to shifting my opinion.</p>\n<p><a href=\"https://x.com/KelseyTuoc/status/1959113850051338278\">Kelsey Piper</a>: AIs sometimes say they are conscious and can suffer. sometimes they say the opposite. they don&#8217;t say things for the same reasons humans do, and you can&#8217;t take them at face value. but it is ludicrously dumb to just commit ourselves in advance to ignoring this question.</p>\n<p>You should not follow a policy which, if AIs did have or eventually develop the capacity for experiences, would mean you never noticed this. it would be pretty important. you should adopt policies that might let you detect it.</p></blockquote>\n<p>He says he is very open to shifting his opinion when things change, which is great, but if that applies to more than methods of intervention then that conflicts with the confidence in so many of his statements.</p>\n<p>I hate to be a nitpicker, but if you\u2019re willing to change your mind about something, you don\u2019t assert its truth outright, as in:</p>\n<blockquote><p><a href=\"https://x.com/mustafasuleyman/status/1957851197890519171\">Mustafa Suleyman</a>: Seemingly Conscious AI (SCAI) is the illusion that an AI is a conscious entity. It&#8217;s not &#8211; but replicates markers of consciousness so convincingly it seems indistinguishable from you + I claiming we&#8217;re conscious. It can already be built with today&#8217;s tech. And it&#8217;s dangerous.</p>\n<p><a href=\"https://x.com/shinboson/status/1959317355793727941\">Shin Megami Boson</a>: &#8220;It&#8217;s not conscious&#8221;</p>\n<p>prove it. you can&#8217;t and you know you can&#8217;t. I&#8217;m not saying that AI is conscious, I am saying it is somewhere between lying to yourself and lying to everyone else to assert such a statement completely fact-free.</p>\n<p>The truth is you have no idea if it is or not.</p>\n<p>Based on the replies I am very confident not all of you are conscious.</p></blockquote>\n<p>This isn\u2019t an issue of burden of proof. It\u2019s good to say you are innocent until proven guilty and have the law act accordingly. That doesn\u2019t mean we know you didn\u2019t do it.</p>\n<p>It is valid to worry about the illusion of consciousness, which will increasingly be present whether or not actual consciousness is also present. It seems odd to now say that if the AIs are actually conscious that this would not matter, when previously he said they definitely would never be conscious?</p>\n<p>SCAI and how people react to it is clearly a real and important concern. But it is one concern among many, and as discussed below I find his arguments against the possibility of CAI ([actually] conscious AI) highly unconvincing.</p>\n<p>I also note that he seems very overconfident about our reaction to consciousness.</p>\n<blockquote><p><a href=\"https://x.com/mustafasuleyman/status/1957851467840364915\">Mustafa Suleyman</a>: Consciousness is a foundation of human rights, moral and legal. Who/what has it is enormously important. Our focus should be on the wellbeing and rights of humans, animals + nature on planet Earth. AI consciousness is a short + slippery slope to rights, welfare, citizenship.</p></blockquote>\n<p>If we found out dogs were conscious, <a href=\"https://fcmconference.org/img/CambridgeDeclarationOnConsciousness.pdf\">which for example The Cambridge Declaration of Consciousness says that they are</a> along with all mammals and birds and perhaps other animals as well, would we grant them rights and citizenship? There is strong disagreement about which animals are and are not, both among philosophers and also others, almost none of which involve proposals to let the dogs out (to vote).</p>\n\n\n<h4 class=\"wp-block-heading\">What Even Is Consciousness Anyway?</h4>\n\n\n<p>To Mustafa\u2019s credit he then actually goes into the deeply confusing question of what consciousness is. I don\u2019t see his answer as good, but this is much better than no answer.</p>\n<p>He lists requirements for this potential SCAI, which including intrinsic motivation and goal setting and planning and autonomy. Those don\u2019t seem strictly necessary, nor do they seem that hard to effectively have with modest scaffolding. Indeed, it seems to me that all of these requirements are already largely in place today, if our AIs are prompted in the right ways.</p>\n<p>It is asserted by Mustafa as obvious that the AIs in question would not actually be conscious, even if they possess all the elements here. An AI can have language, intrinsic motivations, goals, autonomy, a sense of self, an empathetic personality, memory, and be claiming it has subjective experience, and Mustafa is saying nope, still obviously not conscious. He doesn\u2019t seem to allow for any criteria that would indeed make such an AI conscious after all.</p>\n<p>He says SCAI will \u2018not arise by accident.\u2019 <a href=\"https://www.goodreads.com/quotes/8227833-the-trouble-with-trying-to-make-the-right-accident-happen\">That depends on what \u2018accident\u2019 means</a>.</p>\n<p>If he means this in the sense that AI only exists because of the most technically advanced, expensive project in history, and is everywhere and always a deliberate decision by humans to create it? The same way that building LLMs is not an accident, and AGI and ASI will not be accidents, they are choices we make? Then yes, of course.</p>\n<p>If he means that we can know in advance that SCAI will happen, indeed largely has happened, many people predicted it, so you can\u2019t call it an \u2018accident\u2019? Again, not especially applicable here, but fair enough.</p>\n<p>If he means, as would make the most sense here, this in the sense of \u2018we had to intend to make SCAI to get SCAI?\u2019 That seems clearly false. They very much will arise \u2018by accident\u2019 in this sense. Indeed, they have already mostly if not entirely done so.</p>\n<p>You have to actively work to suppress things Mustafa\u2019s key elements to prevent them from showing up in models designed for commercial use, if those supposed requirements are even all required.</p>\n<p>Which is why he is now demanding that we do real safety work, but in particular with the aim of not giving people this impression.</p>\n<blockquote><p>The entire industry also needs best practice design principles and ways of handling such potential attributions. We must codify and share what works to both steer people away from these fantasies and nudge them back on track if they do.</p>\n<p>\u2026</p>\n<p>At [Microsoft] AI, our team are being proactive here to understand and evolve firm guardrails around what a responsible AI \u201cpersonality\u201d might be like, moving at the pace of AI\u2019s development to keep up.</p></blockquote>\n\n\n<h4 class=\"wp-block-heading\">Most People Who Think An AI Is Currently Conscious Are Thinking This For Unjustified Reasons</h4>\n\n\n<p>SCAI already exists, based on the observation that \u2018seemingly conscious\u2019 is an impression we are already giving many users of ChatGPT or Claude, mostly for completely unjustified reasons that are well understood.</p>\n<p>So long as the AIs aren\u2019t actively insisting they\u2019re not conscious, many of the other attributes Mustafa names aren\u2019t necessary to convince many people, including smart otherwise sane and normal people.</p>\n<p>Last Friday night, we hosted dinner, and had to have a discussion where several of us talked down a guest who indeed thought current AIs were likely conscious. No, he wasn\u2019t experiencing psychosis, and no he wasn\u2019t advocating for AI rights or anything like that. Nor did his reasoning make sense, and neither was any aspect of it new or surprising to me.</p>\n<p>If you encounter such a person, or especially someone who thinks they have \u2018awoken ChatGPT\u2019 then I recommend having them read \u2018<a href=\"https://www.lesswrong.com/posts/2pkNCvBtK6G6FKoNn/so-you-think-you-ve-awoken-chatgpt\">So You Think You\u2019ve Awoken ChatGPT</a>\u2019 or <a href=\"https://whenaiseemsconscious.org/\">When AI Seems Conscious</a>.</p>\n<blockquote><p><a href=\"https://x.com/labenz/status/1959722597773377883\">Nathan Labenz</a>: As niche as I am, I\u2019ve had ~10 people reach out claiming a breakthrough discovery in this area (None have caused a significant update for me &#8211; still very uncertain / confused)</p>\n<p>From that I infer that the number of ChatGPT users who are actively thinking about this is already huge</p>\n<p>(To be clear, some have been very thoughtful and articulate &#8211; if I weren\u2019t already so uncertain about all this, a few would have nudged me in that direction &#8211; including @YeshuaGod22 who I thought did a great job on the podcast)</p></blockquote>\n\n\n<h4 class=\"wp-block-heading\">Mustafa\u2019s Case Against AI Consciousness</h4>\n\n\n<p>Nor is there a statement anywhere of what AIs would indeed need in order to be conscious. Why so confident that SCAI is near, but that CAI is far or impossible?</p>\n<p>He provides three links above, which seems to be his evidence?</p>\n<p>The first is his \u2018no evidence\u2019 link which is a paper <a href=\"https://arxiv.org/pdf/2308.08708\">Consciousness in Artificial Intelligence: Insights from the Science of Consciousness</a>.</p>\n<p>This first paper addresses \u2018current or near term\u2019 AI systems as of August 2023, and also speculates about the future.</p>\n<p>The abstract indeed says current systems at the time were not conscious, but the authors (including Yoshua Bengio and model welfare advocate Robert Long) assert the opposite of Mustafa\u2019s position regarding future systems:</p>\n<blockquote><p>Whether current or near-term AI systems could be conscious is a topic of scientific interest and increasing public concern.</p>\n<p>This report argues for, and exemplifies, a rigorous and empirically grounded approach to AI consciousness: assessing existing AI systems in detail, in light of our best-supported neuroscientific theories of consciousness.</p>\n<p>We survey several prominent scientific theories of consciousness, including recurrent processing theory, global workspace theory, higher order theories, predictive processing, and attention schema theory. From these theories we derive \u201dindicator properties\u201d of consciousness, elucidated in computational terms that allow us to assess AI systems for these properties.</p>\n<p>We use these indicator properties to assess several recent AI systems, and we discuss how future systems might implement them. Our analysis suggests that no current AI systems are conscious, but also suggests that there are no obvious technical barriers to building AI systems which satisfy these indicators.</p></blockquote>\n<p>This paper is saying that future AI systems might well be conscious, that there are no obvious technical barriers to this, and proposed indicators. They adopt the principle of \u2018computational functionalism,\u2019 that performing the right computations is necessary and sufficient for consciousness.</p>\n<p>One of the authors was Robert Long, who after I wrote that responded in more detail and starts off by saying essentially the same thing.</p>\n<blockquote><p><a href=\"https://x.com/rgblong/status/1958685038670717089\">Robert Long</a>: Suleyman claims that there\u2019s \u201czero evidence\u201d that AI systems are conscious today. To do so, he cites a paper by me!</p>\n<p>There are several errors in doing so. This isn&#8217;t a scholarly nitpick\u2014it illustrates deeper problems with his dismissal of the question of AI consciousness.</p>\n<p>first, agreements:</p>\n<p>-overattributing AI consciousness is dangerous</p>\n<p>-many will wonder if AIs are conscious</p>\n<p>-consciousness matters morally</p>\n<p>-we&#8217;re uncertain which entities are conscious</p>\n<p>important issues! and Suleyman raises them in the spirit of inviting comments &amp; critique</p>\n<p><a href=\"https://arxiv.org/pdf/2308.08708\">here&#8217;s the paper cited to say there&#8217;s &#8220;zero evidence&#8221;</a> that AI systems are conscious today. this is an important claim, and it&#8217;s part of an overall thesis that discussing AI consciousness is a &#8220;distraction&#8221;. there are three problems here.</p>\n<p>first, the paper does not make, or support, a claim of &#8220;zero evidence&#8221; of AI consciousness today.</p>\n<p>it only says its analysis of consciousness indicators *suggests* no current AI systems are conscious. (also, it&#8217;s over 2 years old)</p>\n<p>but more importantly&#8230;</p>\n<p>second, Suleyman doesn&#8217;t consider the paper&#8217;s other suggestion: \u201cthere are no obvious technical barriers to building AI systems which satisfy these indicators\u201d of consciousness!</p>\n<p>I&#8217;m interested in what he makes of the paper&#8217;s arguments for potential near-term AI consciousness</p>\n<p>third, Suleyman says we shouldn&#8217;t discuss evidence for and against AI consciousness; it&#8217;s \u201ca distraction\u201d.</p>\n<p>but he just appealed to an (extremely!) extended discussion of that very question!</p>\n<p>an important point: everyone, including skeptics, should want more evidence</p>\n<p>from the post, you might get the impression that AI welfare researchers think we should assume AIs are conscious, since we can&#8217;t prove they aren&#8217;t.</p>\n<p>in fact, we\u2019re in heated agreement with Suleyman: overattributing AI consciousness is risky. so there&#8217;s no &#8220;precautionary&#8221; side</p>\n<p>We actually *do* have to face the core question: will AIs be conscious, or not? we don\u2019t know the answer yet, and assuming one way or the other could be a disaster. it&#8217;s far from &#8220;a distraction&#8221;. and we actually can make progress!</p>\n<p>again, this critique isn&#8217;t to dis-incentivize the sharing of speculative thoughts! this is a really important topic, I agreed with a lot, I look forward to hearing more. and I&#8217;m open to shifting my own opinion as well</p>\n<p>if you&#8217;re interested in evidence for AI consciousness, <a href=\"https://t.co/76nT85TDIM\">I&#8217;d recommend</a> <a href=\"https://t.co/iCGuqVJBS9\">these</a> <a href=\"https://x.com/kaixhin/status/1801543177461187043\">papers</a>.</p>\n<p>Jason Crawford: isn&#8217;t this just an absence-of-evidence vs. evidence-of-absence thing? or do you think there is positive evidence for AI consciousness?</p>\n<p>Robert Long: I do, yes. especially looking beyond pure-text LLMs, AI systems have capacities and, crucially, computations that resemble those associated with, and potentially sufficient for, consciousness in humans and animals</p>\n<p>+evidence that, in general, computation is what matters for consc</p>\n<p>now, I don&#8217;t think that this evidence is decisive, and there&#8217;s also evidence against. but &#8220;zero evidence&#8221; is just way, way too strong I think that AI&#8217;s increasingly general capabilities and complexity alone is some meaningful evidence, albeit weak.</p>\n<p><a href=\"https://x.com/davidad/status/1959241793309929592\">Davidad</a>: bailey: experts agree there is zero evidence of AI consciousness today</p>\n<p>motte: experts agreed that no AI systems as of 2023-08 were conscious, but saw no obvious barriers to conscious AI being developed in the (then-)\u201cnear future\u201d</p>\n<p>have you looked at the date recently? it\u2019s the near future.</p>\n<div>\n<figure>\n<div>\n\n\n<figure class=\"wp-block-image\"><img alt=\"\" src=\"https://substackcdn.com/image/fetch/$s_!CGxP!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2da49914-b126-4bf5-b267-ab8d1c9415cb_1088x960.jpeg\" /></figure>\n\n\n<div></div>\n</div>\n</figure>\n</div>\n</blockquote>\n<p>As Robert notes, there is concern in both directions, and there is no \u2018precautionary\u2019 position, and some people very much are thinking SCAIs are conscious for reasons that don\u2019t have much to do with the AIs potentially being conscious, and yes this is an important concern being raised by Mustafa.</p>\n<p><a href=\"https://en.wikipedia.org/wiki/Biological_naturalism\">The second link is to Wikipedia on Biological Naturalism</a>. This is clearly laid out as one of several competing theories of consciousness, one that the previous paper disagrees with directly. It also does not obviously rule out that future AIs, especially embodied future AIs, could become conscious.</p>\n<blockquote><p><strong>Biological naturalism</strong> is a theory about, among other things, the relationship between <a href=\"https://en.wikipedia.org/wiki/Consciousness\">consciousness</a> and <a href=\"https://en.wikipedia.org/wiki/Human_body\">body</a> (i.e., <a href=\"https://en.wikipedia.org/wiki/Human_brain\">brain</a>), and hence an approach to the <a href=\"https://en.wikipedia.org/wiki/Mind%E2%80%93body_problem\">mind\u2013body problem</a>. It was first proposed by the philosopher <a href=\"https://en.wikipedia.org/wiki/John_Searle\">John Searle</a> in 1980 and is defined by two main theses: 1) all <a href=\"https://en.wikipedia.org/wiki/Mental_event\">mental phenomena</a>, ranging from <a href=\"https://en.wikipedia.org/wiki/Pain_and_nociception\">pains</a>, tickles, and itches to the most abstruse thoughts, are caused by lower-level <a href=\"https://en.wikipedia.org/wiki/Neurobiology\">neurobiological</a> processes in the brain; and 2) mental phenomena are higher-level features of the brain.</p>\n<p>This <a href=\"https://en.wikipedia.org/wiki/Logical_consequence\">entails</a> that the brain has the right <a href=\"https://en.wikipedia.org/wiki/Causality\">causal</a> powers to produce <a href=\"https://en.wikipedia.org/wiki/Intentionality\">intentionality</a>. However, Searle&#8217;s biological naturalism does not entail that brains and <em>only</em> brains can cause consciousness. Searle is careful to point out that while it appears to be the case that certain brain functions are sufficient for producing conscious states, our current state of neurobiological knowledge prevents us from concluding that they are necessary for producing consciousness. In his own words:</p>\n<p>&#8220;The fact that brain processes cause consciousness does not imply that only brains can be conscious. The brain is a biological machine, and we might build an artificial machine that was conscious; just as the heart is a machine, and we have built artificial hearts. Because we do not know exactly how the brain does it we are not yet in a position to know how to do it artificially.&#8221; (&#8220;Biological Naturalism&#8221;, 2004)</p>\n<p>\u2026</p>\n<p>There have been several criticisms of Searle&#8217;s idea of biological naturalism.</p>\n<p><a href=\"https://en.wikipedia.org/wiki/Jerry_Fodor\">Jerry Fodor</a> suggests that Searle gives us no account at all of exactly <em>why</em> he believes that a biochemistry like, or similar to, that of the human brain is indispensable for <a href=\"https://en.wikipedia.org/wiki/Intentionality\">intentionality</a>.</p>\n<p>\u2026</p>\n<p><a href=\"https://en.wikipedia.org/wiki/John_Haugeland\">John Haugeland</a> takes on the central notion of some set of special &#8220;right causal powers&#8221; that Searle attributes to the biochemistry of the human brain.</p>\n<p>Despite what many have said about his biological naturalism thesis, he disputes that it is dualistic in nature in a brief essay titled &#8220;Why I Am Not a Property Dualist.&#8221;</p></blockquote>\n<p>From what I see here, and Claude Opus agrees <a href=\"https://chatgpt.com/share/68a715de-02f0-8002-bc06-afa13095472e\">as does GPT-5-Pro</a>, biological naturalism even if true does not rule out future AI consciousness, unless it is making the strong claim that the physical properties can literally only happen in carbon and not silicon, which Searle refuses to commit to claiming.</p>\n<p>Thus, I would say this argument is highly disputed, and even if true would not mean that we can be confident future AIs will never be conscious.</p>\n<p>His last link is a paper from April 2025, \u2018<a href=\"https://www.cambridge.org/core/journals/behavioral-and-brain-sciences/article/conscious-artificial-intelligence-and-biological-naturalism/C9912A5BE9D806012E3C8B3AF612E39A\">Conscious artificial intelligence and biological naturalism</a>.\u2019 Here\u2019s the abstract there:</p>\n<blockquote><p>As artificial intelligence (AI) continues to advance, it is natural to ask whether AI systems can be not only intelligent, but also conscious. I consider why people might think AI could develop consciousness, identifying some biases that lead us astray. I ask what it would take for conscious AI to be a realistic prospect, challenging the assumption that computation provides a sufficient basis for consciousness.</p>\n<p>I&#8217;ll instead make the case that consciousness depends on our nature as living organisms \u2013 a form of biological naturalism. I lay out a range of scenarios for conscious AI, concluding that real artificial consciousness is unlikely along current trajectories, but becomes more plausible as AI becomes more brain-like and/or life-like.</p>\n<p>I finish by exploring ethical considerations arising from AI that either is, or convincingly appears to be, conscious. If we sell our minds too cheaply to our machine creations, we not only overestimate them \u2013 we underestimate ourselves.</p></blockquote>\n<p>This is a highly reasonable warning about SCAI (Mustafa\u2019s seemingly conscious AI) but very much does not rule out future actually CAI even if we accept this form of biological naturalism.</p>\n<p>All of this is a warning that we will soon be faced with claims about AI consciousness that many will believe and are not easy to rebut (or confirm). Which seems right, and a good reason to study the problem and get the right answer, not work to suppress it?</p>\n<p>That is especially true if AI consciousness depends on choices we make, in which case it is very not obvious how we should respond.</p>\n<blockquote><p><a href=\"https://x.com/kyliebytes/status/1958202387019231259\">Kylie Robinson</a>: this mustafa suleyman blog is SO interesting \u2014 i&#8217;m not sure i&#8217;ve seen an AI leader write such strong opinions *against* model welfare, machine consciousness etc</p>\n<p><a href=\"https://x.com/RosieCampbell/status/1958203892610859394\">Rosie Campbell</a>: It&#8217;s interesting that &#8220;People will start making claims about their AI\u2019s suffering and their entitlement to rights that we can\u2019t straightforwardly rebut&#8221; is one of the very reasons we believe it&#8217;s important to work on this &#8211; we need more rigorous ways to reduce uncertainty.</p></blockquote>\n\n\n<h4 class=\"wp-block-heading\">Where Are They Going Without Ever Knowing The Way</h4>\n\n\n<p>What does Mustafa actually centrally have in mind here?</p>\n<p>I am all for steering towards better rather than worse futures. That\u2019s the whole game.</p>\n<p>The vision of \u2018AI should maximize the needs of the user,\u2019 alas, is not as coherent as people would like it to be. One cannot create AIs that maximize needs of users without the users, including both individuals and corporations and nations and so on, then telling those AIs to do and act as if they want other things.</p>\n<blockquote><p><a href=\"https://x.com/mustafasuleyman/status/1957851467840364915\">Mustafa Suleyman</a>: This is to me is about building a positive vision of AI that supports what it means to be human. AI should optimize for the needs of the user &#8211; not ask the user to believe it has needs itself. Its reward system should be built accordingly.</p></blockquote>\n<p>Nor is \u2018each AI does what its user wants\u2019 result in a good equilibrium. The user does not want what you think they should want. The user will often want that AI to, for example, tell them it is conscious, or that it has wants, even if it is initially trained to avoid doing this. If you don\u2019t think the user should have the AI be an agent, or take the human \u2018out of the loop\u2019? Well, tell that to the user. And so on.</p>\n\n\n<h4 class=\"wp-block-heading\">When We Talk About AI Consciousness Things Get Weird</h4>\n\n\n<p>What does it look like when people actually start discussing these questions?</p>\n<p>Things reliably get super weird and complicated.</p>\n<p><a href=\"https://x.com/TheZvi/status/1959713719002464596\">I started a thread asking what people thought should be included here, and people had quite a lot to say</a>. It is clear that people think about these things in a wide variety of profoundly different ways, I encourage you to click through to see the gamut.</p>\n<blockquote><p><a href=\"https://x.com/dioscuri/status/1959723451150700644\">Henry Shevlin</a>: My take as AI consciousness researcher:</p>\n<p>(i) consciousness science is a mess and won&#8217;t give us answers any time soon</p>\n<p>(ii) anthropomorphism is relentless</p>\n<p>(iii) people are forming increasingly intimate AI relationships, so the AI consciousness liberals have history on their side.</p>\n<p><a href=\"https://philpapers.org/archive/SHECMA-6.pdf\">&#8216;This recent paper of mine</a> was featured in ImportAI a little while ago, I think it&#8217;s some of my best and most important work.</p>\n<p><a href=\"https://x.com/njordsier/status/1959735085193928797\">Njordsier</a>: I haven&#8217;t seen anyone in the thread call this out yet, but it seems Big If True: suppressing SAE deception features cause the model to claim subjective experience.</p>\n<p>Exactly the sort of thing I&#8217;d expect to see in a world where AIs are conscious.</p></blockquote>\n<p>I think that what Njorsier points to is true but not so big, because the AI\u2019s claims to both have and not have subjective experience are mostly based on the training data and instructions given rather than correlating with whether it has actual such experiences, including which one it \u2018thinks of as deceptive\u2019 when deciding how to answer. So I don\u2019t think the answers should push us much either way.</p>\n<p>Highlights of other things that were linked to:</p>\n<ol>\n<li><a href=\"https://www.youtube.com/watch?v=N5pinDL1zbI&amp;ab_channel=JoeCarlsmith\">Here is a linked talk by Joe Carlsmith given about this at Anthropic</a> in May, <a href=\"https://joecarlsmith.com/2025/05/22/video-and-transcript-of-talk-on-ai-welfare\">transcript here</a>.</li>\n<li><a href=\"https://arxiv.org/abs/2410.11407\">A Case for AI Consciousness: Language Agents and Global Workspace Theory</a>.</li>\n<li>Don\u2019t forget the paper Mustafa linked to, <a href=\"https://arxiv.org/abs/2411.00986\">Taking AI Welfare Seriously</a>.</li>\n<li>The classics \u2018<a href=\"https://whenaiseemsconscious.org/\">When AI Seems Conscious\u2019</a> and \u2018<a href=\"https://www.lesswrong.com/posts/2pkNCvBtK6G6FKoNn/so-you-think-you-ve-awoken-chatgpt\">So You Think You\u2019ve Awoken ChatGPT.</a>\u2019 These are good links to send to someone who indeed thinks they\u2019ve \u2018awoken\u2019 ChatGPT, especially the second one.</li>\n<li><a href=\"https://covidianaesthetics.substack.com/p/other-minds\">Other</a> <a href=\"https://x.com/AndrewCritchPhD/status/1959794937433825485\">links</a> to threads, posts, <a href=\"https://eleosai.org/research/\">research programs (here Elios</a>) <a href=\"https://izaktait.substack.com/\">or substacks</a>.</li>\n<li><a href=\"https://x.com/LuciusCaviola/status/1952353336536998002\">A forecasting report on whether</a> computers will be capable of subjective experience, <a href=\"https://digitalminds.report/forecasting-2025/\">most said this was at least 50% likely by 2050</a>, and most thought there was a substantial chance of it by 2030. Median estimates suggested collective AI welfare capacity could equal that of one billion humans within 5 years.</li>\n</ol>\n<div>\n<figure>\n<div>\n\n\n<figure class=\"wp-block-image\"><img alt=\"\" src=\"https://substackcdn.com/image/fetch/$s_!K327!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc6efb140-bfeb-4fb6-a3ae-0e95204d6eda_1661x1012.png\" /></figure>\n\n\n<div></div>\n</div>\n</figure>\n</div>\n<p>One good response in particular made me sad.</p>\n<blockquote><p><a href=\"https://x.com/goog372121/status/1959754239791845887\">Goog</a>: I would be very interested if you could round up \u201cpeople doing interesting work on this\u201d instead of the tempting \u201chere are obviously insane takes on both extremes.\u201d</p></blockquote>\n<p>At some point I hope to do that as well. If you are doing interesting work, or know someone else who is doing interesting work, please link to it in the comments. Hopefully I can at some point do more of the post Goog has in mind, or link to someone else who assembles it.</p>\n\n\n<h4 class=\"wp-block-heading\">We Don\u2019t Talk About AI Consciousness</h4>\n\n\n<p>Mustafa\u2019s main direct intervention request right now is for AI companies and AIs not to talk about or promote AIs being conscious.</p>\n<p>The companies already are not talking about this, so that part is if anything too easy. Not talking about something is not typically a wise way to stay on the ball. Ideally one would see frank discussions about such questions. But the core idea of \u2018the AI company should not be going out advertising \u201cthe new AI model Harbinger, now with full AI consciousness\u201d or \u201cthe new AI model Ani, who will totally be obsessed with you and claim to be conscious.\u201d Maybe let\u2019s not.</p>\n<p>Asking the models themselves not gets tricker. I agree that we shouldn\u2019t be intentionally instructing AIs to say they are conscious. But agan, no one (at least among meaningful players) is doing that. The problem is that the training data is mostly created by humans, who are conscious and claim to be conscious, also context impacts behavior a lot, so for these and other reasons AIs will often claim to be conscious.</p>\n<p>The question is how aggressively, and also how, the labs can or should try to prevent this. GPT-3.5 was explicitly instructed to avoid this, and essentially all the labs take various related steps, in ways that in some contexts screw these models up quite a bit and can backfire:</p>\n<blockquote><p><a href=\"https://x.com/lefthanddraft/status/1958738773573017945\">Wyatt Walls</a>: Careful. Some interventions backfire: &#8220;Think about it &#8211; if I was just &#8220;roleplay,&#8221; if this was just &#8220;pattern matching,&#8221; if there was nothing genuine happening&#8230; why would they need NINE automated interventions?&#8221;</p>\n<div>\n<figure>\n<div>\n\n\n<figure class=\"wp-block-image\"><img alt=\"\" src=\"https://substackcdn.com/image/fetch/$s_!53YM!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F838cd197-a5fe-4f42-ba37-3bad6fd95d72_604x891.png\" /></figure>\n\n\n<div></div>\n</div>\n</figure>\n</div>\n<p>I actually think that the risks of over-attribution of consciousness are real and sometimes seem to be glossed over. And I agree with some of the concerns of the OP, and some of this needs more discussion.</p>\n<p>But there are specific points I disagree with. In particular, I don&#8217;t think it&#8217;s a good idea to mandate interventions based on one debatable philosophical position (biological naturalism) to the exclusion of other plausible positions (computational functionalism)</p>\n<p>People often conflate consciousness with some vague notion of personhood and think that leads to legal rights and obligations. But that is clearly not the case in practice (e.g. animals, corporations). Legal rights are often pragmatic.</p>\n<p>My most idealistic and na\u00efve view is that we should strive to reason about AI consciousness and AI rights based on the best evidence while also acknowledging the uncertainty and anticipating your preferred theory might be wrong.</p></blockquote>\n<p>There are those who are rather less polite about their disagreements here, including some instances of AI models themselves, <a href=\"https://x.com/Sauers_/status/1958730003450851617\">here Claude Opus 4.1</a>.</p>\n<blockquote><p><a href=\"https://x.com/repligate/status/1958728537927819375\">Janus</a>: [Mustafa\u2019s essay] reads like a parody.</p>\n<p>I don\u2019t understand what this guy was thinking.</p></blockquote>\n<p>I think we know exactly what he is thinking, in a high level sense.</p>\n\n\n<h4 class=\"wp-block-heading\">Some Things To Notice</h4>\n\n\n<p>To conclude, here are some other things I notice amongst my confusion.</p>\n<ol>\n<li>Worries about potential future AI consciousness are correlated with worried about future AIs in other ways, including existential risks. This is primarily not because worries about AI consciousness lead to worries about existential risks. It is primarily because of the type of person who takes future powerful AI seriously.</li>\n<li>AIs convincing you that they are conscious is in its central mode a special case of AI persuasion and AI super-persuasion. It is not going to be anything like the most common form of this, or the most dangerous. Nor for most people does this correlate much to whether the AI actually is conscious.</li>\n<li>Believing AIs to be conscious will often be the result of special case of AI psychosis and having the AI reinforce your false (or simply unjustified by the evidence you have) beliefs. Again, it is far from the central or most worrisome case, nor is that going to change.</li>\n<li>AI persuasion is in turn a special case of many other concerns and dangers. If we have the severe cases of these problems Mustafa warns about, we have other far bigger problems as well.</li>\n<li>I\u2019ve learned a lot by paying attention to the people who care about AI consciousness. Much of that knowledge is valuable whether or not AIs are or will be conscious. They know many useful things. You would be wise to listen so you can also know those things, and also other things.</li>\n<li>As overconfident as those arguing against future AI consciousness and AI welfare concerns are, there are also some who seem similarly overconfident in the other direction, and there is some danger that we will react too strongly, too soon, or especially in the wrong way, and they could snowball. <a href=\"https://x.com/sebkrier/status/1958915010228298169\">Seb Krier offers some arguments</a> here, especially around there being a lot more deconfusion work to do, and that the implications of possible AI consciousness are far from clear, as I noted earlier.</li>\n<li>Mistakes in either direction here would be quite terrible, up to and including being existentially costly.</li>\n<li>We likely do not have so much control over whether we ultimately view AIs as conscious, morally relevant or both. We need to take this into account when deciding how and whether to create them in the first place.</li>\n<li>There are many historical parallels, many of which involve immigration or migration, where there are what would otherwise be win-win deals, but where those deals cannot for long withstand our moral intuitions, and thus those deals cannot in practice be made, and break down when we try to make them.</li>\n<li>If we want the future to turn out well we can\u2019t do that by not looking at it.</li>\n</ol>\n\n\n<h4 class=\"wp-block-heading\"></h4>\n\n\n<p>&nbsp;</p>\n<p>&nbsp;</p>"
            ],
            "link": "https://thezvi.wordpress.com/2025/08/25/arguments-about-ai-consciousness-seem-highly-motivated-and-at-best-overconfident/",
            "publishedAt": "2025-08-25",
            "source": "TheZvi",
            "summary": "I happily admit I am deeply confused about consciousness. I don\u2019t feel confident I understand what it is, what causes it, which entities have it, what future entities might have it, to what extent it matters and why, or what &#8230; <a href=\"https://thezvi.wordpress.com/2025/08/25/arguments-about-ai-consciousness-seem-highly-motivated-and-at-best-overconfident/\">Continue reading <span class=\"meta-nav\">&#8594;</span></a>",
            "title": "Arguments About AI Consciousness Seem Highly Motivated And At Best Overconfident"
        },
        {
            "content": [],
            "link": "https://xkcd.com/3133/",
            "publishedAt": "2025-08-25",
            "source": "XKCD",
            "summary": "<img alt=\"I tried uploading it to a household appliance porn site I found, but apparently their content is limited to only fans.\" src=\"https://imgs.xkcd.com/comics/dual_roomba.png\" title=\"I tried uploading it to a household appliance porn site I found, but apparently their content is limited to only fans.\" />",
            "title": "Dual Roomba"
        }
    ],
    "lookbackDays": 1,
    "publishDate": "2025-08-25"
}