{
    "articles": [
        {
            "content": [],
            "link": "https://alexmolas.com/2025/07/28/unexpected-benefit-llm.html",
            "publishedAt": "2025-07-28",
            "source": "Alex Molas",
            "summary": "I joined RevenueCat a couple of months ago to work on LTV predictions. My first few projects were straightforward: fix small things, ship some low-hanging fruit. After getting familiar with the code, I decided to extend the LTV model and add some cool machine learning. So I dove in. Spent a couple of days wrangling data, cleaning it, understanding what was going on, and the usual standard pre-training stuff. I was in \u201cresearch mode\u201d, so all my code lived in notebooks and ugly scripts. But after enough trial and error, I managed to improve the main metric by 5%. I was hyped. Told the team. Felt great.",
            "title": "Who needs git when you have 1M context windows?"
        },
        {
            "content": [
                "<div class=\"trix-content\">\n  <div>Ideals are supposed to be unattainable for the great many. If everyone could be the smartest, strongest, prettiest, or best, there would be no need for ideals \u2014 we'd all just be perfect. But we're not, so ideals exist to show us the peak of humanity and to point our ambition and appreciation toward it.<br /><br /></div><div>This is what I always hated about the 90s. It was a decade that made it cool to be a loser. It was the decade of MTV's Beavis and Butt-Head. It was the age of grunge. I'm generationally obliged to like Nirvana, but what a perfectly depressive, suicidal soundtrack to loser culture.<br /><br /></div><div>Naomi Wolf's <em>The Beauty Myth</em> was published in 1990. It took a critical theory-like lens on beauty ideals, and finding it all so awfully oppressive. Because, actually, seeing beautiful, slim people in advertising or media is bad. Because we don't all look like that! And who's even to say what \"beauty\" is, anyway? It's all just socially constructed!&nbsp;<br /><br /></div><div>The final stage of that dead-end argument appeared as an ad here in Copenhagen thirty years later during the 2020 insanity:<br /><br />  <figure class=\"attachment attachment--preview attachment--lightboxable attachment--jpg\">\n      <a href=\"https://world.hey.com/dhh/b3dccf72/blobs/eyJfcmFpbHMiOnsiZGF0YSI6MjIwNjMzNjc2MywicHVyIjoiYmxvYl9pZCJ9fQ--baf019b92cbd6002e61e3ab1b31a29df2ba1db7c03134eea37dcb3eeb66a192b/i-speak-my-truth.jpg?disposition=attachment\" title=\"Download i-speak-my-truth.jpg\">\n        <img alt=\"i-speak-my-truth.jpg\" src=\"https://world.hey.com/dhh/b3dccf72/representations/eyJfcmFpbHMiOnsiZGF0YSI6MjIwNjMzNjc2MywicHVyIjoiYmxvYl9pZCJ9fQ--baf019b92cbd6002e61e3ab1b31a29df2ba1db7c03134eea37dcb3eeb66a192b/eyJfcmFpbHMiOnsiZGF0YSI6eyJmb3JtYXQiOiJqcGciLCJyZXNpemVfdG9fbGltaXQiOlszODQwLDI1NjBdLCJxdWFsaXR5Ijo2MCwibG9hZGVyIjp7InBhZ2UiOm51bGx9LCJjb2FsZXNjZSI6dHJ1ZX0sInB1ciI6InZhcmlhdGlvbiJ9fQ--b3779d742b3242a2a5284869a45b2a113e0c177f0450c29f0baca1ee780f6604/i-speak-my-truth.jpg\" />\n</a>\n  </figure></div><div><br />I passed it every day biking the boys to school for weeks. Next to other slim, fit Danes also riding their bikes. None of whom resembled the grotesque display of obesity towering over them on their commute from Calvin Klein.<br /><br /></div><div>While this campaign was laughably out of place in Copenhagen, it's possible that it brought recognition and representation in some parts of America. But a celebration of ideals it was not.<br /><br /></div><div>That's the problem with the whole \"representation\" narrative. It proposes we're all better off if all we see is a mirror of ourselves, however obese, lazy, ignorant, or incompetent, because at least it won't be \"unrealistic\". Screw that. The last thing we need is a patronizing message that however little you try, you're perfect just the way you are.<br /><br /></div><div>No, the beauty of ideals is that they ask more of us. Ask us to pursue knowledge, fitness, and competence by taking inspiration from the best human specimens.<br /><br /></div><div>Thankfully, no amount of post-modern deconstruction or academic theory babble seems capable of suppressing the intrinsic human yearning for excellence forever. The ideals are <a href=\"https://world.hey.com/dhh/you-expect-principles-but-should-wish-for-none-531988ec\">finally starting to emerge again</a>.</div>\n</div>"
            ],
            "link": "https://world.hey.com/dhh/the-beauty-of-ideals-b3dccf72",
            "publishedAt": "2025-07-28",
            "source": "DHH",
            "summary": "<div class=\"trix-content\"> <div>Ideals are supposed to be unattainable for the great many. If everyone could be the smartest, strongest, prettiest, or best, there would be no need for ideals \u2014 we'd all just be perfect. But we're not, so ideals exist to show us the peak of humanity and to point our ambition and appreciation toward it.<br /><br /></div><div>This is what I always hated about the 90s. It was a decade that made it cool to be a loser. It was the decade of MTV's Beavis and Butt-Head. It was the age of grunge. I'm generationally obliged to like Nirvana, but what a perfectly depressive, suicidal soundtrack to loser culture.<br /><br /></div><div>Naomi Wolf's <em>The Beauty Myth</em> was published in 1990. It took a critical theory-like lens on beauty ideals, and finding it all so awfully oppressive. Because, actually, seeing beautiful, slim people in advertising or media is bad. Because we don't all look like that! And who's even to say what \"beauty\" is, anyway? It's all just socially constructed!&nbsp;<br /><br /></div><div>The final stage of that dead-end argument appeared as an ad here in Copenhagen thirty years later during the 2020 insanity:<br /><br /> <figure class=\"attachment attachment--preview attachment--lightboxable attachment--jpg\"> <a href=\"https://world.hey.com/dhh/b3dccf72/blobs/eyJfcmFpbHMiOnsiZGF0YSI6MjIwNjMzNjc2MywicHVyIjoiYmxvYl9pZCJ9fQ--baf019b92cbd6002e61e3ab1b31a29df2ba1db7c03134eea37dcb3eeb66a192b/i-speak-my-truth.jpg?disposition=attachment\"",
            "title": "The beauty of ideals"
        },
        {
            "content": [
                "<p>One of my <a href=\"https://www.researchgate.net/publication/334063511_Moving_off_the_Map_How_Knowledge_of_Organizational_Operations_Empowers_and_Alienates\">favorite academic papers</a> about organizations is by Ruthanne Huising, and it tells the story of teams that were assigned to create process maps of their company, tracing what the organization actually did, from raw materials to finished goods. As they created this map, they realized how much of the work seemed strange and unplanned. They discovered entire processes that produced outputs nobody used, weird semi-official pathways to getting things done, and repeated duplication of efforts. Many of the employees working on the map, once rising stars of the company, became disillusioned.</p><div class=\"captioned-image-container\"><figure><a class=\"image-link image2 is-viewable-img\" href=\"https://substackcdn.com/image/fetch/$s_!6mi3!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fce4e863e-d237-4a45-bddd-ca74028b7c4a_1564x1177.jpeg\" target=\"_blank\"><div class=\"image2-inset\"><source type=\"image/webp\" /><img alt=\"\" class=\"sizing-normal\" height=\"499.8241758241758\" src=\"https://substackcdn.com/image/fetch/$s_!6mi3!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fce4e863e-d237-4a45-bddd-ca74028b7c4a_1564x1177.jpeg\" width=\"664\" /><div class=\"image-link-expand\"><div class=\"pencraft pc-display-flex pc-gap-8 pc-reset\"><div class=\"pencraft pc-reset icon-container restack-image\"><svg class=\"lucide lucide-refresh-cw\" fill=\"none\" height=\"20\" stroke=\"currentColor\" stroke-linecap=\"round\" stroke-linejoin=\"round\" stroke-width=\"2\" viewBox=\"0 0 24 24\" width=\"20\" xmlns=\"http://www.w3.org/2000/svg\"><path d=\"M3 12a9 9 0 0 1 9-9 9.75 9.75 0 0 1 6.74 2.74L21 8\"></path><path d=\"M21 3v5h-5\"></path><path d=\"M21 12a9 9 0 0 1-9 9 9.75 9.75 0 0 1-6.74-2.74L3 16\"></path><path d=\"M8 16H3v5\"></path></svg></div><div class=\"pencraft pc-reset icon-container view-image\"><svg class=\"lucide lucide-maximize2\" fill=\"none\" height=\"20\" stroke=\"currentColor\" stroke-linecap=\"round\" stroke-linejoin=\"round\" stroke-width=\"2\" viewBox=\"0 0 24 24\" width=\"20\" xmlns=\"http://www.w3.org/2000/svg\"><polyline points=\"15 3 21 3 21 9\"></polyline><polyline points=\"9 21 3 21 3 15\"></polyline><line x1=\"21\" x2=\"14\" y1=\"3\" y2=\"10\"></line><line x1=\"3\" x2=\"10\" y1=\"21\" y2=\"14\"></line></svg></div></div></div></div></a><figcaption class=\"image-caption\">The Process Map</figcaption></figure></div><p>I&#8217;ll let Prof. Huising explain what happened next: &#8220;Some held out hope that one or two people at the top knew of these design and operation issues; however, they were often disabused of this optimism. For example, a manager walked the CEO through the map, presenting him with a view he had never seen before and illustrating for him the lack of design and the disconnect between strategy and operations. The CEO, after being walked through the map, sat down, put his head on the table, and said, \"This is even more fucked up than I imagined.\" The CEO revealed that not only was the operation of his organization out of his control but that his grasp on it was imaginary.&#8221;</p><p>For many people, this may not be a surprise. One thing you learn studying (or working in) organizations is that they are all actually a bit of a mess. In fact, one classic organizational theory is actually called the<a href=\"https://www.jstor.org/stable/2392088\"> Garbage Can Model</a>. This views organizations as chaotic \"garbage cans\" where problems, solutions, and decision-makers are dumped in together, and decisions often happen when these elements collide randomly, rather than through a fully rational process. Of course, it is easy to take this view too far - organizations do have structures, decision-makers, and processes that actually matter. It is just that these structures often evolved and were negotiated among people, rather than being carefully designed and well-recorded.</p><p>The Garbage Can represents a world where unwritten rules, bespoke knowledge, and complex and undocumented processes are critical. It is this situation that makes AI adoption in organizations difficult<a href=\"https://papers.ssrn.com/sol3/papers.cfm?abstract_id=5136877\">, because even though 43% of American workers have used AI at work</a>, they are mostly doing it in informal ways, solving their own work problems. Scaling AI across the enterprise is hard because traditional automation requires clear rules and defined processes; the very things Garbage Can organizations lack. To address the more general issues of AI and work requires careful building of AI-powered systems for specific use cases, mapping out the real processes and making tools to solve the issues that are discovered. </p><p>This is a hard, slow process that suggests enterprise AI adoption will take time. At least, that's how it looks if we assume AI needs to understand our organizations the way we do. But AI researchers have learned something important about these sorts of assumptions.</p><h1>The Bitter Lesson</h1><p>Computer scientist Richard Sutton introduced the concept of the Bitter Lesson in an <a href=\"https://www.cs.utexas.edu/~eunsol/courses/data/bitter_lesson.pdf\">influential 2019 essay </a>where he pointed out a pattern in AI research. Time and again, AI researchers trying to solve a difficult problem, like beating humans in chess, turned to elegant solutions, studying opening moves, positional evaluations, tactical patterns, and endgame databases. Programmers encoded centuries of chess wisdom in hand-crafted software: control the center, develop pieces early, king safety matters, passed pawns are valuable, and so on. Deep Blue, the first chess computer to beat the world&#8217;s best human, used some chess knowledge, but combined that with the brute force of being able to search 200 million positions a second. In 2017, Google released AlphaZero, which could beat humans not just in chess but also in shogi and go, and it did it with no prior knowledge of these games at all. Instead, the AI model trained against itself, playing the games until it learned them. All of the elegant knowledge of chess was irrelevant, pure brute force computing combined with generalized approaches to machine learning, was enough to beat them. And that is the Bitter Lesson &#8212; encoding human understanding into an AI tends to be worse than just letting the AI figure out how to solve the problem, and adding enough computing power until it can do it better than any human.</p><div class=\"captioned-image-container\"><figure><a class=\"image-link image2 is-viewable-img\" href=\"https://substackcdn.com/image/fetch/$s_!ikhB!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5afe61f0-bbcf-41c6-9c50-45169ad5d08b_7520x2240.png\" target=\"_blank\"><div class=\"image2-inset\"><source type=\"image/webp\" /><img alt=\"\" class=\"sizing-normal\" height=\"434\" src=\"https://substackcdn.com/image/fetch/$s_!ikhB!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5afe61f0-bbcf-41c6-9c50-45169ad5d08b_7520x2240.png\" width=\"1456\" /><div class=\"image-link-expand\"><div class=\"pencraft pc-display-flex pc-gap-8 pc-reset\"><div class=\"pencraft pc-reset icon-container restack-image\"><svg class=\"lucide lucide-refresh-cw\" fill=\"none\" height=\"20\" stroke=\"currentColor\" stroke-linecap=\"round\" stroke-linejoin=\"round\" stroke-width=\"2\" viewBox=\"0 0 24 24\" width=\"20\" xmlns=\"http://www.w3.org/2000/svg\"><path d=\"M3 12a9 9 0 0 1 9-9 9.75 9.75 0 0 1 6.74 2.74L21 8\"></path><path d=\"M21 3v5h-5\"></path><path d=\"M21 12a9 9 0 0 1-9 9 9.75 9.75 0 0 1-6.74-2.74L3 16\"></path><path d=\"M8 16H3v5\"></path></svg></div><div class=\"pencraft pc-reset icon-container view-image\"><svg class=\"lucide lucide-maximize2\" fill=\"none\" height=\"20\" stroke=\"currentColor\" stroke-linecap=\"round\" stroke-linejoin=\"round\" stroke-width=\"2\" viewBox=\"0 0 24 24\" width=\"20\" xmlns=\"http://www.w3.org/2000/svg\"><polyline points=\"15 3 21 3 21 9\"></polyline><polyline points=\"9 21 3 21 3 15\"></polyline><line x1=\"21\" x2=\"14\" y1=\"3\" y2=\"10\"></line><line x1=\"3\" x2=\"10\" y1=\"21\" y2=\"14\"></line></svg></div></div></div></div></a><figcaption class=\"image-caption\">Why two versions of this graph? And why are they slightly different? Answers in a bit!</figcaption></figure></div><p>The lesson is bitter because it means that our human understanding of problems built from a lifetime of experience is not that important in solving a problem with AI. Decades of researchers' careful work encoding human expertise was ultimately less effective than just throwing more computation at the problem. We are soon going to see whether the Bitter Lesson applies widely to the world of work.</p><h1>Agents</h1><p>While individuals can get a lot of benefits out of using chatbots themselves, a lot of excitement about how to use AI in organizations focuses on agents, a fuzzy term that I define as AI systems capable of taking autonomous action to accomplish a goal. As opposed to guiding a chatbot with prompting, you delegate a task to an agent, and it accomplishes it. However, previous AI systems have not been good enough to handle the full range of organizational needs, there is just too much messiness in the real world. This is why when <a href=\"https://arxiv.org/abs/2407.12796\">we created our first AI-powered teaching games a year ago</a>, we had to carefully design each step in the agentic system to handle narrow tasks. And though <a href=\"https://metr.org/blog/2025-03-19-measuring-ai-ability-to-complete-long-tasks/\">AI ability to work autonomously is increasing very rapidly</a>, they are still far from human-level on most complicated jobs and are easily led astray on complex tasks.</p><div class=\"captioned-image-container\"><figure><a class=\"image-link image2 is-viewable-img\" href=\"https://substackcdn.com/image/fetch/$s_!OF2s!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb96a5b15-fcdc-46ff-abf2-9198917dc438_1685x697.png\" target=\"_blank\"><div class=\"image2-inset\"><source type=\"image/webp\" /><img alt=\"\" class=\"sizing-normal\" height=\"602\" src=\"https://substackcdn.com/image/fetch/$s_!OF2s!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb96a5b15-fcdc-46ff-abf2-9198917dc438_1685x697.png\" width=\"1456\" /><div class=\"image-link-expand\"><div class=\"pencraft pc-display-flex pc-gap-8 pc-reset\"><div class=\"pencraft pc-reset icon-container restack-image\"><svg class=\"lucide lucide-refresh-cw\" fill=\"none\" height=\"20\" stroke=\"currentColor\" stroke-linecap=\"round\" stroke-linejoin=\"round\" stroke-width=\"2\" viewBox=\"0 0 24 24\" width=\"20\" xmlns=\"http://www.w3.org/2000/svg\"><path d=\"M3 12a9 9 0 0 1 9-9 9.75 9.75 0 0 1 6.74 2.74L21 8\"></path><path d=\"M21 3v5h-5\"></path><path d=\"M21 12a9 9 0 0 1-9 9 9.75 9.75 0 0 1-6.74-2.74L3 16\"></path><path d=\"M8 16H3v5\"></path></svg></div><div class=\"pencraft pc-reset icon-container view-image\"><svg class=\"lucide lucide-maximize2\" fill=\"none\" height=\"20\" stroke=\"currentColor\" stroke-linecap=\"round\" stroke-linejoin=\"round\" stroke-width=\"2\" viewBox=\"0 0 24 24\" width=\"20\" xmlns=\"http://www.w3.org/2000/svg\"><polyline points=\"15 3 21 3 21 9\"></polyline><polyline points=\"9 21 3 21 3 15\"></polyline><line x1=\"21\" x2=\"14\" y1=\"3\" y2=\"10\"></line><line x1=\"3\" x2=\"10\" y1=\"21\" y2=\"14\"></line></svg></div></div></div></div></a><figcaption class=\"image-caption\">This is with an 80% success threshold</figcaption></figure></div><p>As an example of the state-of-the art in agentic systems, consider <a href=\"https://manus.im/app\">Manus</a>, which uses Claude and a series of clever approaches to make AI agents that can get real work done. The Manus team has <a href=\"https://manus.im/blog/Context-Engineering-for-AI-Agents-Lessons-from-Building-Manus\">shared a lot of tips for building agents</a>, involving some interesting bits of engineering and very elaborate prompt design. When writing this post, I asked Manus: &#8220;i need an attractive graph that compares the ELO of the best grandmaster and the ELO of the worlds best chess computer from the first modern chess computer through 2025.&#8221; And the system got to work. First, Manus always creates a to-do list, then it gathered data and wrote a number of files and, after some minor adjustments I asked for, finally came up with the graph you can see on the left side above (the one without the box around the graph).</p><div class=\"captioned-image-container\"><figure><a class=\"image-link image2 is-viewable-img\" href=\"https://substackcdn.com/image/fetch/$s_!eA10!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0d8af0ab-7554-4bb8-8205-7210ac866825_997x644.png\" target=\"_blank\"><div class=\"image2-inset\"><source type=\"image/webp\" /><img alt=\"\" class=\"sizing-normal\" height=\"348.8064192577733\" src=\"https://substackcdn.com/image/fetch/$s_!eA10!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0d8af0ab-7554-4bb8-8205-7210ac866825_997x644.png\" width=\"540\" /><div class=\"image-link-expand\"><div class=\"pencraft pc-display-flex pc-gap-8 pc-reset\"><div class=\"pencraft pc-reset icon-container restack-image\"><svg class=\"lucide lucide-refresh-cw\" fill=\"none\" height=\"20\" stroke=\"currentColor\" stroke-linecap=\"round\" stroke-linejoin=\"round\" stroke-width=\"2\" viewBox=\"0 0 24 24\" width=\"20\" xmlns=\"http://www.w3.org/2000/svg\"><path d=\"M3 12a9 9 0 0 1 9-9 9.75 9.75 0 0 1 6.74 2.74L21 8\"></path><path d=\"M21 3v5h-5\"></path><path d=\"M21 12a9 9 0 0 1-9 9 9.75 9.75 0 0 1-6.74-2.74L3 16\"></path><path d=\"M8 16H3v5\"></path></svg></div><div class=\"pencraft pc-reset icon-container view-image\"><svg class=\"lucide lucide-maximize2\" fill=\"none\" height=\"20\" stroke=\"currentColor\" stroke-linecap=\"round\" stroke-linejoin=\"round\" stroke-width=\"2\" viewBox=\"0 0 24 24\" width=\"20\" xmlns=\"http://www.w3.org/2000/svg\"><polyline points=\"15 3 21 3 21 9\"></polyline><polyline points=\"9 21 3 21 3 15\"></polyline><line x1=\"21\" x2=\"14\" y1=\"3\" y2=\"10\"></line><line x1=\"3\" x2=\"10\" y1=\"21\" y2=\"14\"></line></svg></div></div></div></div></a></figure></div><p>Why did it do these things in this order? Because Manus was built by hand, carefully crafted to be the best general purpose agent available. There are hundreds of lines of bespoke text in its system prompts, including detailed instructions about how to build a to-do list. It incorporates hard-won knowledge on how to make agents work with today&#8217;s AI systems.</p><p>Do you see the potential problem? &#8220;Carefully crafted,&#8221; &#8220;bespoke,&#8221; &#8220;incorporates hard-won knowledge&#8221; &#8212; exactly the kind of work the Bitter Lesson tells us to avoid because it will eventually be made irrelevant by more general-purpose techniques.</p><p>It turns out there is now evidence that this may be possible with the recent release of <a href=\"https://openai.com/index/introducing-chatgpt-agent/\">ChatGPT agent</a> (an uninspiring name, but at least it is clear, a big step forward for OpenAI!). ChatGPT agent represents a fundamental shift. It is not trained on the <em>process</em> of doing work; instead, <a href=\"https://www.youtube.com/watch?v=YNWWu0aZ5pY\">OpenAI used reinforcement learning to train their AI </a>on the actual <em>final outcomes</em>. For example. they may not teach it <em>how</em> to create an Excel file the way a human would, they would simply rate the quality of the Excel files it creates until it learns to make a good one, using whatever methods the AI develops. To illustrate how reinforcement learning and careful crafting lead to similar outcomes, I gave the exact same chess prompt to ChatGPT agent and got the graph on the right above. But this time there was no to-do list, no script to follow, instead the agent charted whatever mysterious course was required to get me the best output it could, according to its training. You can see an excerpt of that below:</p><div class=\"native-video-embed\"></div><p>But you might notice a few differences between the two charts, besides their appearance. For example, each has different ratings for Deep Blue&#8217;s performance because the ELO for Deep Blue was never officially measured. The rating from Manus was based off a basic search, we found a speculative Reddit discussion, while the ChatGPT agent, trained with the reinforcement learning approaches used in Deep Research, turned up more credible sources, including an <em>Atlantic</em> <a href=\"https://www.theatlantic.com/technology/archive/2022/09/carlsen-niemann-chess-cheating-poker/671472/\">article</a>, to back up its claim. In a similar way, when I asked both agents to re-create the graph by making a fully functional Excel file, ChatGPT&#8217;s version worked, while Manus&#8217;s had errors.</p><div class=\"captioned-image-container\"><figure><a class=\"image-link image2\" href=\"https://substackcdn.com/image/fetch/$s_!ZZME!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4c026d7b-14d4-40a9-858c-6f5f739551d0_3039x745.png\" target=\"_blank\"><div class=\"image2-inset\"><source type=\"image/webp\" /><img alt=\"\" class=\"sizing-normal\" height=\"145.15384615384616\" src=\"https://substackcdn.com/image/fetch/$s_!ZZME!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4c026d7b-14d4-40a9-858c-6f5f739551d0_3039x745.png\" width=\"592\" /><div></div></div></a></figure></div><p>I don&#8217;t know if ChatGPT agent is better than Manus yet, but I suspect that it is far more likely to make gains faster than its competitor. To improve Manus will involve more careful crafting and bespoke work, to improve ChatGPT agents simply requires more computer chips and more examples. If the Bitter Lesson holds, the long-term outcome seems pretty clear. But more critically, the comparison between hand-crafted and outcome-trained agents points to a fundamental question about how organizations should approach AI adoption.</p><h1>Agents in the Garbage Can</h1><p>This returns us to the world of organizations. While individuals rapidly adopt AI, companies still struggle with the Garbage Can problem, spending months mapping their chaotic processes before deploying any AI system. But what if that's backwards?</p><p>The Bitter Lesson suggests we might soon ignore how companies produce outputs and focus only on the outputs themselves. Define what a good sales report or customer interaction looks like, then train AI to produce it. The AI will find its own paths through the organizational chaos; paths that might be more efficient, if more opaque, than the semi-official routes humans evolved. In a world where the Bitter Lesson holds, the despair of the CEO with his head on the table is misplaced. Instead of untangling every broken process, he just needs to define success and let AI navigate the mess. In fact, Bitter Lesson might actually be sweet: all those undocumented workflows and informal networks that pervade organizations might not matter. What matters is knowing good output when you see it.</p><p>If this is true, the Garbage Can remains, but we no longer need to sort through it while competitive advantage itself gets redefined. The effort companies spent refining processes, building institutional knowledge, and creating competitive moats through operational excellence might matter less than they think. If AI agents can train on outputs alone, any organization that can define quality and provide enough examples might achieve similar results, whether they understand their own processes or not.</p><p>Or it might be that the Garbage Can wins, that human complexity and those messy, evolved processes are too intricate for AI to navigate without understanding them. We're about to find out which kind of problem organizations really are: chess games that yield to computational scale, or something fundamentally messier. The companies betting on either answer are already making their moves, and we will soon get to learn what game we're actually playing.</p><p class=\"button-wrapper\"><a class=\"button primary\" href=\"https://www.oneusefulthing.org/subscribe\"><span>Subscribe now</span></a></p><p class=\"button-wrapper\"><a class=\"button primary\" href=\"https://www.oneusefulthing.org/p/the-bitter-lesson-versus-the-garbage?utm_source=substack&amp;utm_medium=email&amp;utm_content=share&amp;action=share\"><span>Share</span></a></p><p></p>"
            ],
            "link": "https://www.oneusefulthing.org/p/the-bitter-lesson-versus-the-garbage",
            "publishedAt": "2025-07-28",
            "source": "Ethan Mollick",
            "summary": "<p>One of my <a href=\"https://www.researchgate.net/publication/334063511_Moving_off_the_Map_How_Knowledge_of_Organizational_Operations_Empowers_and_Alienates\">favorite academic papers</a> about organizations is by Ruthanne Huising, and it tells the story of teams that were assigned to create process maps of their company, tracing what the organization actually did, from raw materials to finished goods. As they created this map, they realized how much of the work seemed strange and unplanned. They discovered entire processes that produced outputs nobody used, weird semi-official pathways to getting things done, and repeated duplication of efforts. Many of the employees working on the map, once rising stars of the company, became disillusioned.</p><div class=\"captioned-image-container\"><figure><a class=\"image-link image2 is-viewable-img\" href=\"https://substackcdn.com/image/fetch/$s_!6mi3!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fce4e863e-d237-4a45-bddd-ca74028b7c4a_1564x1177.jpeg\" target=\"_blank\"><div class=\"image2-inset\"><source type=\"image/webp\" /><img alt=\"\" class=\"sizing-normal\" height=\"499.8241758241758\" src=\"https://substackcdn.com/image/fetch/$s_!6mi3!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fce4e863e-d237-4a45-bddd-ca74028b7c4a_1564x1177.jpeg\" width=\"664\" /><div class=\"image-link-expand\"><div class=\"pencraft pc-display-flex pc-gap-8 pc-reset\"><div class=\"pencraft pc-reset icon-container restack-image\"><svg class=\"lucide lucide-refresh-cw\" fill=\"none\" height=\"20\" stroke=\"currentColor\" stroke-linecap=\"round\" stroke-linejoin=\"round\" stroke-width=\"2\" viewBox=\"0 0 24 24\" width=\"20\" xmlns=\"http://www.w3.org/2000/svg\"><path d=\"M3 12a9 9 0 0 1 9-9 9.75 9.75 0 0 1 6.74 2.74L21 8\"></path><path d=\"M21 3v5h-5\"></path><path d=\"M21 12a9 9 0 0 1-9 9 9.75 9.75 0 0 1-6.74-2.74L3 16\"></path><path d=\"M8 16H3v5\"></path></svg></div><div class=\"pencraft pc-reset icon-container view-image\"><svg class=\"lucide lucide-maximize2\" fill=\"none\" height=\"20\" stroke=\"currentColor\" stroke-linecap=\"round\" stroke-linejoin=\"round\" stroke-width=\"2\" viewBox=\"0 0 24 24\" width=\"20\" xmlns=\"http://www.w3.org/2000/svg\"><polyline points=\"15 3 21 3 21 9\"></polyline><polyline points=\"9 21 3 21 3 15\"></polyline><line x1=\"21\" x2=\"14\" y1=\"3\" y2=\"10\"></line><line x1=\"3\"",
            "title": "The Bitter Lesson versus The Garbage Can"
        },
        {
            "content": [],
            "link": "https://harper.blog/notes/2025-07-28_fb9190197c4f_i-love-a-good-in-flight-camera/",
            "publishedAt": "2025-07-28",
            "source": "Harper Reed",
            "summary": "<p>I love a good in flight camera.</p> <figure> <img alt=\"image_1.jpg\" height=\"1041\" src=\"https://harper.blog/notes/2025-07-28_fb9190197c4f_i-love-a-good-in-flight-camera/image_1.jpg\" width=\"1799\" /> </figure> <hr /> <p>Thank you for using RSS. I appreciate you. <a href=\"mailto:harper&#64;modest.com\">Email me</a></p>",
            "title": "Note #278"
        },
        {
            "content": [
                "<p>This is the weekly visible open thread. Post about anything you want, ask random questions, whatever. ACX has an unofficial <a href=\"https://www.reddit.com/r/slatestarcodex/\">subreddit</a>, <a href=\"https://discord.gg/RTKtdut\">Discord</a>, and <a href=\"https://www.datasecretslox.com/index.php\">bulletin board</a>, and <a href=\"https://www.lesswrong.com/community?filters%5B0%5D=SSC\">in-person meetups around the world</a>. Most content is free, some is subscriber only; you can subscribe <strong><a href=\"https://astralcodexten.substack.com/subscribe\">here</a></strong>. Also:</p><p><strong>1: </strong>Comment of the week is <a href=\"https://www.astralcodexten.com/p/your-review-the-astral-codex-ten/comment/139018819\">bean on the commentariat review</a> and the anonymous reviewer&#8217;s reply <a href=\"https://www.astralcodexten.com/p/your-review-the-astral-codex-ten/comment/139373690\">here</a>.</p><p><strong>2: </strong>I made some mistakes linking the forms on the <a href=\"https://www.astralcodexten.com/p/apply-for-an-acx-grant-2025\">ACX Grants post</a>; these were corrected pretty quickly, but in case you missed it here are the correct links:</p><ul><li><p><a href=\"https://forms.gle/CvvHhoi1cYAG9GiU9\">If you&#8217;re applying for a grant</a></p></li><li><p><a href=\"https://forms.gle/6tDMjPHuLEDkXZnG9\">If you&#8217;re a VC</a> who wants to see projects that could be for-profit startups</p></li><li><p><a href=\"https://forms.gle/ATMjAqs2E8EzdT5p7\">If you&#8217;re another charity/foundation/philanthropist</a> who wants to coordinate with us to potentially take some of the projects in your cause area</p></li><li><p><a href=\"https://forms.gle/iyZCMZ224bzcvPv17\">If you&#8217;re a grantmaker or expert</a> who wants to help evaluate projects in your area of expertise</p></li><li><p><a href=\"https://forms.gle/iyZCMZ224bzcvPv17\">If you&#8217;re a professional</a> who wants to potentially volunteer to do pro bono work for grantees</p></li><li><p><a href=\"https://forms.gle/Ncgvsjn12cAiZmfc8\">If you want to help fund</a> ACX Grants (warning: we are on track to being overfunded, although not there yet - if you want to help, consider pledging money so we can ask you for it if needed, but not yet donating, so that I don&#8217;t have to return it to you if we can&#8217;t find enough good grants with room for funding)</p></li></ul><p>All deadlines August 15, thanks again to everyone.</p>"
            ],
            "link": "https://www.astralcodexten.com/p/open-thread-392",
            "publishedAt": "2025-07-28",
            "source": "SlateStarCodex",
            "summary": "<p>This is the weekly visible open thread. Post about anything you want, ask random questions, whatever. ACX has an unofficial <a href=\"https://www.reddit.com/r/slatestarcodex/\">subreddit</a>, <a href=\"https://discord.gg/RTKtdut\">Discord</a>, and <a href=\"https://www.datasecretslox.com/index.php\">bulletin board</a>, and <a href=\"https://www.lesswrong.com/community?filters%5B0%5D=SSC\">in-person meetups around the world</a>. Most content is free, some is subscriber only; you can subscribe <strong><a href=\"https://astralcodexten.substack.com/subscribe\">here</a></strong>. Also:</p><p><strong>1: </strong>Comment of the week is <a href=\"https://www.astralcodexten.com/p/your-review-the-astral-codex-ten/comment/139018819\">bean on the commentariat review</a> and the anonymous reviewer&#8217;s reply <a href=\"https://www.astralcodexten.com/p/your-review-the-astral-codex-ten/comment/139373690\">here</a>.</p><p><strong>2: </strong>I made some mistakes linking the forms on the <a href=\"https://www.astralcodexten.com/p/apply-for-an-acx-grant-2025\">ACX Grants post</a>; these were corrected pretty quickly, but in case you missed it here are the correct links:</p><ul><li><p><a href=\"https://forms.gle/CvvHhoi1cYAG9GiU9\">If you&#8217;re applying for a grant</a></p></li><li><p><a href=\"https://forms.gle/6tDMjPHuLEDkXZnG9\">If you&#8217;re a VC</a> who wants to see projects that could be for-profit startups</p></li><li><p><a href=\"https://forms.gle/ATMjAqs2E8EzdT5p7\">If you&#8217;re another charity/foundation/philanthropist</a> who wants to coordinate with us to potentially take some of the projects in your cause area</p></li><li><p><a href=\"https://forms.gle/iyZCMZ224bzcvPv17\">If you&#8217;re a grantmaker or expert</a> who wants to help evaluate projects in your area of expertise</p></li><li><p><a href=\"https://forms.gle/iyZCMZ224bzcvPv17\">If you&#8217;re a professional</a> who wants to potentially volunteer to do pro bono work for grantees</p></li><li><p><a href=\"https://forms.gle/Ncgvsjn12cAiZmfc8\">If you want to help fund</a> ACX Grants (warning: we are on track to being overfunded, although not there yet - if you want to help, consider pledging money so",
            "title": "Open Thread 392"
        },
        {
            "content": [],
            "link": "https://xkcd.com/3121/",
            "publishedAt": "2025-07-28",
            "source": "XKCD",
            "summary": "<img alt=\"Detectives say the key to tracking down the source of the kites was a large wall map covered in thumbtacks and string. 'It's the first time that method has ever actually worked,' said a spokesperson.\" src=\"https://imgs.xkcd.com/comics/kite_incident.png\" title=\"Detectives say the key to tracking down the source of the kites was a large wall map covered in thumbtacks and string. 'It's the first time that method has ever actually worked,' said a spokesperson.\" />",
            "title": "Kite Incident"
        }
    ],
    "lookbackDays": 1,
    "publishDate": "2025-07-28"
}