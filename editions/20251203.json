{
    "articles": [
        {
            "content": [
                "<div class=\"trix-content\">\n  <div>Kanban is a simple, practical approach to visually managing processes and backlogs by moving work cards from one progress column to another. Toyota came up with it to track their production lines back in the middle of the 20th century, but it's since been applied to all sorts of industries with great effect. And <a href=\"https://www.fizzy.do/\">Fizzy is our new fun, modern take</a> on it in digital form.<br /><br /></div><div>We're certainly not the first to take a swing at this, not even for software development. Since the early 2000s, there's been a movement to use the Kanban concept to track bugs, issues, and ideas in our industry. And countless attempts to digitize the concept over the years.&nbsp;<br /><br /></div><div>But as with so much other software, good ideas can grow cumbersome and unwieldy surprisingly quickly. Fizzy is a fresh reset of an old idea.<br /><br /></div><div>We need more of that.&nbsp;<br /><br /></div><div>Very little software is ever the final word on solving interesting problems. Even products that start out with great promise and simplicity tend to accumulate cruft and complexity over time. A healthy ecosystem needs a recurring cycle of renewal.<br /><br /></div><div>We've taken this mission to heart not just with Fizzy's fun, colorful, and modern implementation of the Kanban concept, but also in its distribution.&nbsp;<br /><br /></div><div>Fizzy is available as a service we run where you get 1,000 cards for free, and then it's $20/month for unlimited usage. But we're also giving you access to the entire code base, and invite enterprising individuals and companies to run their own instance totally free of charge.</div><div><br />This is done under <a href=\"https://www.fizzy.do/license\">the O'Saasy License</a>, which is basically the do-whatever-you-want-just-don't-sue MIT License, but with a carve-out that reserves the commercialization rights to run Fizzy as SaaS for us as the creators. That means it's not technically Open Source\u2122, but the source sure is open, and you can find it on <a href=\"https://github.com/basecamp/fizzy\">our public GitHub repository</a>.<br /><br /></div><div>That open source is what we run too. So <a href=\"https://github.com/basecamp/fizzy/pulls\">new features or bugs fixes</a> accepted on GitHub will make it into both our Fizzy SaaS offering and what anyone can run on their own hardware. We've already had a handful of contributions go live like this!<br /><br /></div><div>Ultimately, it's our plan to let data flow freely between the SaaS and the local installations. You'll be able to start an account on your own instance, and then, if you'd rather we just run it for you, take that data with you into the managed setup. Or the other way around!<br /><br /></div><div>In an age where SaaS companies come and go, pivot one way or the other, I think it's a great reassurance that the source code is freely available, and that any work put into a SaaS account is portable to your own installation later.</div><div><br />I'm also just a huge fan of being able to <a href=\"https://signalvnoise.com/svn3/paying-tribute-to-the-web-with-view-source/\">View Source</a>. Traditionally, that's been reserved to the front end (and even that has been disappearing due to the scourge of minimization, transpiling, and bundling), but I'm usually even more interested in seeing how things are built on the backend. Fizzy allows you full introspection into that. Including the entire history of how the product was built, pull request by pull request. It's a great way to learn how modern Rails applications are put together!<br /><br /></div><div>So please <a href=\"https://app.fizzy.do/signup/completion/new\">give Fizzy a spin</a>. Whether you're working on software, with a need to track those bugs and feature requests, or you're in an entirely different business and need a place for your particular issues and ideas. Fizzy is a fresh, fun way to manage it all, Kanban style. Enjoy!<br /><br />  <figure class=\"attachment attachment--preview attachment--lightboxable attachment--png\">\n      <a href=\"https://world.hey.com/dhh/54ac41b6/blobs/eyJfcmFpbHMiOnsiZGF0YSI6MjM2NDU3MTQ5MCwicHVyIjoiYmxvYl9pZCJ9fQ--4969ef7259712db31f434c84f2a2555135eda76c136d598ede3ae49e359c6edc/fizzy.png?disposition=attachment\" title=\"Download fizzy.png\">\n        <img alt=\"fizzy.png\" src=\"https://world.hey.com/dhh/54ac41b6/representations/eyJfcmFpbHMiOnsiZGF0YSI6MjM2NDU3MTQ5MCwicHVyIjoiYmxvYl9pZCJ9fQ--4969ef7259712db31f434c84f2a2555135eda76c136d598ede3ae49e359c6edc/eyJfcmFpbHMiOnsiZGF0YSI6eyJmb3JtYXQiOiJwbmciLCJyZXNpemVfdG9fbGltaXQiOlszODQwLDI1NjBdLCJxdWFsaXR5Ijo2MCwibG9hZGVyIjp7InBhZ2UiOm51bGx9LCJjb2FsZXNjZSI6dHJ1ZX0sInB1ciI6InZhcmlhdGlvbiJ9fQ--7edc7b21f6fad97fa22412618822c4d19725431f296c7ce47dc174b61535d27c/fizzy.png\" />\n</a>\n  </figure><br />  <figure class=\"attachment attachment--preview attachment--lightboxable attachment--png\">\n      <a href=\"https://world.hey.com/dhh/54ac41b6/blobs/eyJfcmFpbHMiOnsiZGF0YSI6MjM2NDU3MzIxNywicHVyIjoiYmxvYl9pZCJ9fQ--1160428d807468a171f4f7a152b81484af3037379c87ee4108c0ce534899084e/fizzy2.png?disposition=attachment\" title=\"Download fizzy2.png\">\n        <img alt=\"fizzy2.png\" src=\"https://world.hey.com/dhh/54ac41b6/representations/eyJfcmFpbHMiOnsiZGF0YSI6MjM2NDU3MzIxNywicHVyIjoiYmxvYl9pZCJ9fQ--1160428d807468a171f4f7a152b81484af3037379c87ee4108c0ce534899084e/eyJfcmFpbHMiOnsiZGF0YSI6eyJmb3JtYXQiOiJwbmciLCJyZXNpemVfdG9fbGltaXQiOlszODQwLDI1NjBdLCJxdWFsaXR5Ijo2MCwibG9hZGVyIjp7InBhZ2UiOm51bGx9LCJjb2FsZXNjZSI6dHJ1ZX0sInB1ciI6InZhcmlhdGlvbiJ9fQ--7edc7b21f6fad97fa22412618822c4d19725431f296c7ce47dc174b61535d27c/fizzy2.png\" />\n</a>\n  </figure><br />  <figure class=\"attachment attachment--preview attachment--lightboxable attachment--png\">\n      <a href=\"https://world.hey.com/dhh/54ac41b6/blobs/eyJfcmFpbHMiOnsiZGF0YSI6MjM2NDU3NDY0NSwicHVyIjoiYmxvYl9pZCJ9fQ--23fc5faf73217545bea98ede55a86c4b9531d61b60f97143f4a1e610f8331687/fizzy3.png?disposition=attachment\" title=\"Download fizzy3.png\">\n        <img alt=\"fizzy3.png\" src=\"https://world.hey.com/dhh/54ac41b6/representations/eyJfcmFpbHMiOnsiZGF0YSI6MjM2NDU3NDY0NSwicHVyIjoiYmxvYl9pZCJ9fQ--23fc5faf73217545bea98ede55a86c4b9531d61b60f97143f4a1e610f8331687/eyJfcmFpbHMiOnsiZGF0YSI6eyJmb3JtYXQiOiJwbmciLCJyZXNpemVfdG9fbGltaXQiOlszODQwLDI1NjBdLCJxdWFsaXR5Ijo2MCwibG9hZGVyIjp7InBhZ2UiOm51bGx9LCJjb2FsZXNjZSI6dHJ1ZX0sInB1ciI6InZhcmlhdGlvbiJ9fQ--7edc7b21f6fad97fa22412618822c4d19725431f296c7ce47dc174b61535d27c/fizzy3.png\" />\n</a>\n  </figure><br /><br /></div>\n</div>"
            ],
            "link": "https://world.hey.com/dhh/fizzy-is-our-fun-modern-take-on-kanban-and-we-made-it-open-source-54ac41b6",
            "publishedAt": "2025-12-03",
            "source": "DHH",
            "summary": "<div class=\"trix-content\"> <div>Kanban is a simple, practical approach to visually managing processes and backlogs by moving work cards from one progress column to another. Toyota came up with it to track their production lines back in the middle of the 20th century, but it's since been applied to all sorts of industries with great effect. And <a href=\"https://www.fizzy.do/\">Fizzy is our new fun, modern take</a> on it in digital form.<br /><br /></div><div>We're certainly not the first to take a swing at this, not even for software development. Since the early 2000s, there's been a movement to use the Kanban concept to track bugs, issues, and ideas in our industry. And countless attempts to digitize the concept over the years.&nbsp;<br /><br /></div><div>But as with so much other software, good ideas can grow cumbersome and unwieldy surprisingly quickly. Fizzy is a fresh reset of an old idea.<br /><br /></div><div>We need more of that.&nbsp;<br /><br /></div><div>Very little software is ever the final word on solving interesting problems. Even products that start out with great promise and simplicity tend to accumulate cruft and complexity over time. A healthy ecosystem needs a recurring cycle of renewal.<br /><br /></div><div>We've taken this mission to heart not just with",
            "title": "Fizzy is our fun, modern take on Kanban (and we made it open source!)"
        },
        {
            "content": [],
            "link": "https://harper.blog/2025/12/03/claude-code-email-productivity-mcp-agents/",
            "publishedAt": "2025-12-03",
            "source": "Harper Reed",
            "summary": "<p>Over the last week or so, I have been using Claude Code to help me with some email, and scheduling. It started cuz the holidays are overwhelming, and I felt like I was constantly behind. My inbox was overflowing with everything I had deemed important, and I hadn\u2019t been able to make a dent. It was stressful. It still is!</p> <figure><img alt=\"Maybe a storm?, Ricoh GRiiix, 11/2025\" src=\"https://harper.blog/2025/12/03/claude-code-email-productivity-mcp-agents//2025/12/03/claude-code-email-productivity-mcp-agents/R0002380.jpeg\" /><figcaption> <p>Maybe a storm?, Ricoh GRiiix, 11/2025</p> </figcaption> </figure> <p>I had just seen the <a href=\"https://zo.computer\">zo.computer</a> launch (neat project!) and it reminded me that <a href=\"https://pipedream.com\">Pipedream</a> has this wild MCP server that you can use to connect to literally anything Pipedream supports. This means I could use it to do my emails! Problem solved. Problem created. More problems created! WHY ARE WE COUNTING PROBLEMS!</p> <h2 id=\"how-why-humans-are-dying-and-going-extinct\">How? Why? HUMANS ARE DYING AND GOING EXTINCT</h2> <p>I love email. I really do. Typically I am really great at managing my email. However, when the email gets overwhelming - I end up ignoring it. Which means it gets worse. Which means everyone loses.</p> <p>I just need a way to start doing my email that is pretty low pressure. Using Claude Code has made it MUCH easier.</p>",
            "title": "Getting Claude Code to do my emails"
        },
        {
            "content": [
                "<p><em>Quick announcement: I'll be visiting Japan in April, 2026 for about a month and will be on Honshu for most of the trip. Please email me recommendations. If you live nearby, let's have coffee?</em></p>\n<hr />\n<p>I've always been fascinated by old, multi-generational Japanese businesses. My leisure-watching on YouTube is usually a long video of a Japanese craftsman\u2014sometimes a 10th or 11th generation\u2014making iron tea kettles, or soy sauce, or pottery, or furniture.</p>\n<p>Their dedication to craft\u2014and acknowledgment that perfection is unattainable\u2014resonates with me deeply. Improving in their craft is an almost spiritual endeavour, and it inspires me to engage in my crafts with a similar passion and focus.</p>\n<p>Slow, consistent investment over many years is how beautiful things are made, learnt, or grown. As a society we forget this truth\u2014especially with the rise of social media and the proliferation of instant gratification. Good things take time.</p>\n<p>Dedication to craft in this manner comes with incredible longevity (survivorship bias plays a role, but the density of long-lived businesses in Japan is an outlier). So many of these small businesses have been around for hundreds, and sometimes over a thousand years, passed from generation to generation. Modern companies have a hard time retaining employees for 2 years, let alone a lifetime.</p>\n<p>This longevity stems from a counter-intuitive idea of growing slowly (or not at all) and choosing to stay small. In most modern economies if you were to start a bakery, the goal would be to set it up, hire and train a bunch of staff, and expand operations to a second location. Potentially, if you play your cards right, you could create a national (or international) chain or franchise. Corporatise the shit out of it, go public or sell, make bank.</p>\n<p>While this is a potential path to becoming filthy rich, the odds of achieving this become vanishingly small. The organisation becomes brittle due to thinly-spread resources and care, hiring becomes risky, and leverage, whether in the form of loans or investors, imposes unwanted directionality.</p>\n<p>There's a well known parable of the fisherman and the businessman that goes something like this:</p>\n<p>A businessman meets a fisherman who is selling fish at his stall one morning. The businessman enquires of the fisherman what he does after he finishes selling his fish for the day. The fisherman responds that he spends time with his friends and family, cooks good food, and watches the sunset with his wife. Then in the morning he wakes up early, takes his boat out on the ocean, and catches some fish.</p>\n<p>The businessman, shocked that the fisherman was wasting so much time encourages him fish for longer in the morning, increasing his yield and maximising the utility of his boat. Then he should sell those extra fish in the afternoon and save up until he has enough money to buy a second fishing boat and potentially employ some other fishermen. Focus on the selling side of the business, set up a permanent store, and possibly, if he does everything correctly, get a loan to expand the operation even further.</p>\n<p>In 10 to 20 years he could own an entire fishing fleet, make a lot of money, and finally retire. The fisherman then asks the businessman what he would do with his days once retired, to which the businessman responds: \"Well, you could spend more time with your friends and family, cook good food, watch the sunset with your wife, and wake up early in the morning and go fishing, if you want.\"</p>\n<p>I love this parable, even if it is a bit of an oversimplification. There is something to be said about affording comforts and financial stability that a fisherman may not have access to. But I think it illustrates the point that when it comes to running a business, bigger is not always better. This is especially true for consultancies or agencies which suffer from bad horizontal scaling economics.</p>\n<p>The trick is figuring out what is \"enough\". At what point are we chasing status instead of contentment?</p>\n<p>A smaller, slower growing company is less risky, less fragile, less stressful, and still a rewarding endeavour.</p>\n<p>This is how I run Bear. The project covers its own expenses and compensates me enough to have a decent quality of life. It grows slowly and sustainably. It isn't leveraged and I control its direction and fate. The most important factor, however, is that I don't need it to be something grander. It affords me a life that I love, and provides me with a craft to practise.</p>"
            ],
            "link": "https://herman.bearblog.dev/grow-slowly-stay-small/",
            "publishedAt": "2025-12-03",
            "source": "Herman Martinus",
            "summary": "<p><em>Quick announcement: I'll be visiting Japan in April, 2026 for about a month and will be on Honshu for most of the trip. Please email me recommendations. If you live nearby, let's have coffee?</em></p> <hr /> <p>I've always been fascinated by old, multi-generational Japanese businesses. My leisure-watching on YouTube is usually a long video of a Japanese craftsman\u2014sometimes a 10th or 11th generation\u2014making iron tea kettles, or soy sauce, or pottery, or furniture.</p> <p>Their dedication to craft\u2014and acknowledgment that perfection is unattainable\u2014resonates with me deeply. Improving in their craft is an almost spiritual endeavour, and it inspires me to engage in my crafts with a similar passion and focus.</p> <p>Slow, consistent investment over many years is how beautiful things are made, learnt, or grown. As a society we forget this truth\u2014especially with the rise of social media and the proliferation of instant gratification. Good things take time.</p> <p>Dedication to craft in this manner comes with incredible longevity (survivorship bias plays a role, but the density of long-lived businesses in Japan is an outlier). So many of these small businesses have been around for hundreds, and sometimes over a thousand years, passed from generation to generation. Modern companies have a hard",
            "title": "Grow slowly, stay small"
        },
        {
            "content": [
                "<div class=\"trix-content\">\n  <div>Have you noticed that every issue and idea tracking tool you loved slowly morphed into boring, sluggish, corporate bloatware?<br /><br />Trello put on 40 pounds of cruft. Jira started charging by the migraine. Asana tried to become everything to everyone. GitHub Issues slipped into a steady state of decline. The whole category is a 20 car pileup of complexity.<br /><br />Time to route around that mess.<br /><br /><strong>Today we\u2019re introducing </strong><a href=\"https://www.fizzy.do\"><strong>Fizzy</strong></a><strong>. Kanban as it should be, not as it has been.</strong><br /><br />Fizzy is a fresh take on cards and columns, with a few twists, human-nature inspired defaults, and a vibrant interface that\u2019s the opposite of the bland and boring software the industry has been flinging at you for years.<br /><br />Kanban has been around since the 1940s, and Trello brought it into the mainstream in 2011. Since then, some version of column-based kanban-style organization has found its way into any collaboration tool worth its salt.<br /><br />But most have over salted the dish.<br /><br />What was simple is now complicated. What was clear is now cluttered. What just worked now takes work.<br /><br />Fizzy presses reset, reconsiders what really matters, and presents a refreshing way to kanban that just feels right. It\u2019s friendly, colorful, straightforward, and fast as hell.<br /><br />We still use Basecamp for our big, intensive projects, but lately we\u2019ve been reaching for Fizzy to run the smaller ones. It\u2019s perfect for tracking bugs, issues, and ideas, and it shines for lighter, self-contained workflows like podcasts or video production.<br /><br />We didn\u2019t expect it, but Fizzy\u2019s so good it might even cannibalize Basecamp on the lighter side of project management. We\u2019d be thrilled.<br /><br />How much is it? It\u2019s not much for so much.<br /><br />Everyone gets 1000 cards for free. Beyond that, we\u2019ll host your account for just $20/month for unlimited cards and unlimited users. One price for all and everything. No tiers, no \u201ccontact us.\u201d No pricing chart at all \u2014 just a price tag, like on a pair of jeans.<br /><br />And here\u2019s a surprise... Fizzy is open source! If you\u2019d prefer not to pay us, or you want to customize Fizzy for your own use, you can run it yourself for free forever. Have a great idea? Submit a PR to contribute to the code base and improve the product for everyone. It\u2019s the best of all worlds. No excuses.<br /><br />Every idea comes back around. It\u2019s time for take two on kanban. Fizzy\u2019s our hat in the ring.<br /><br />Let\u2019s make this platform insanely great, together. Come on in!<br /><br />Visit <a href=\"https://www.fizzy.do\">fizzy.do</a> to check it out and sign up for free!<br /><br /></div><div class=\"attachment-gallery attachment-gallery--2\">\n    <figure class=\"attachment attachment--preview attachment--lightboxable attachment--png\">\n      <a href=\"https://world.hey.com/jason/83a4144f/blobs/eyJfcmFpbHMiOnsiZGF0YSI6MjM2NTM1MDE4OSwicHVyIjoiYmxvYl9pZCJ9fQ--486aaff105610b6e5a353da7fc1aa61c31ae3bc493aef62058a5fd6e6053e000/image.png?disposition=attachment\" title=\"Download image.png\">\n        <img alt=\"image.png\" src=\"https://world.hey.com/jason/83a4144f/representations/eyJfcmFpbHMiOnsiZGF0YSI6MjM2NTM1MDE4OSwicHVyIjoiYmxvYl9pZCJ9fQ--486aaff105610b6e5a353da7fc1aa61c31ae3bc493aef62058a5fd6e6053e000/eyJfcmFpbHMiOnsiZGF0YSI6eyJmb3JtYXQiOiJwbmciLCJyZXNpemVfdG9fbGltaXQiOlsxNjAwLDEyMDBdLCJxdWFsaXR5Ijo2MCwibG9hZGVyIjp7InBhZ2UiOm51bGx9LCJjb2FsZXNjZSI6dHJ1ZX0sInB1ciI6InZhcmlhdGlvbiJ9fQ--8a4eadc39996b456981c13ce71a114e6d921a6725bb1a424222e8cbff9ed0cc8/image.png\" />\n</a>\n  </figure>  <figure class=\"attachment attachment--preview attachment--lightboxable attachment--png\">\n      <a href=\"https://world.hey.com/jason/83a4144f/blobs/eyJfcmFpbHMiOnsiZGF0YSI6MjM2NTM1MDcyOSwicHVyIjoiYmxvYl9pZCJ9fQ--8a71974dc15a7c509abb56eddb7e744ab6b9010a7bd5d2596c922c762758f47d/image.png?disposition=attachment\" title=\"Download image.png\">\n        <img alt=\"image.png\" src=\"https://world.hey.com/jason/83a4144f/representations/eyJfcmFpbHMiOnsiZGF0YSI6MjM2NTM1MDcyOSwicHVyIjoiYmxvYl9pZCJ9fQ--8a71974dc15a7c509abb56eddb7e744ab6b9010a7bd5d2596c922c762758f47d/eyJfcmFpbHMiOnsiZGF0YSI6eyJmb3JtYXQiOiJwbmciLCJyZXNpemVfdG9fbGltaXQiOlsxNjAwLDEyMDBdLCJxdWFsaXR5Ijo2MCwibG9hZGVyIjp7InBhZ2UiOm51bGx9LCJjb2FsZXNjZSI6dHJ1ZX0sInB1ciI6InZhcmlhdGlvbiJ9fQ--8a4eadc39996b456981c13ce71a114e6d921a6725bb1a424222e8cbff9ed0cc8/image.png\" />\n</a>\n  </figure>\n</div><div>-Jason</div>\n</div>"
            ],
            "link": "https://world.hey.com/jason/introducing-fizzy-our-newest-product-83a4144f",
            "publishedAt": "2025-12-03",
            "source": "Jason Fried",
            "summary": "<div class=\"trix-content\"> <div>Have you noticed that every issue and idea tracking tool you loved slowly morphed into boring, sluggish, corporate bloatware?<br /><br />Trello put on 40 pounds of cruft. Jira started charging by the migraine. Asana tried to become everything to everyone. GitHub Issues slipped into a steady state of decline. The whole category is a 20 car pileup of complexity.<br /><br />Time to route around that mess.<br /><br /><strong>Today we\u2019re introducing </strong><a href=\"https://www.fizzy.do\"><strong>Fizzy</strong></a><strong>. Kanban as it should be, not as it has been.</strong><br /><br />Fizzy is a fresh take on cards and columns, with a few twists, human-nature inspired defaults, and a vibrant interface that\u2019s the opposite of the bland and boring software the industry has been flinging at you for years.<br /><br />Kanban has been around since the 1940s, and Trello brought it into the mainstream in 2011. Since then, some version of column-based kanban-style organization has found its way into any collaboration tool worth its salt.<br /><br />But most have over salted the dish.<br /><br />What was simple is now complicated. What was clear is now cluttered. What just worked now takes work.<br /><br />Fizzy presses reset, reconsiders what really matters, and presents a refreshing way to",
            "title": "Introducing Fizzy, our newest product"
        },
        {
            "content": [],
            "link": "https://www.nytimes.com/2025/12/03/style/tiny-modern-love-stories-he-was-75-i-was-42.html",
            "publishedAt": "2025-12-03",
            "source": "Modern Love - NYT",
            "summary": "Modern Love in miniature, featuring reader-submitted stories of no more than 100 words.",
            "title": "Tiny Love Stories: \u2018He Was 75. I Was 42.\u2019"
        },
        {
            "content": [],
            "link": "https://www.nytimes.com/2025/12/02/podcasts/finding-the-magic-just-in-time-encore.html",
            "publishedAt": "2025-12-03",
            "source": "Modern Love - NYT",
            "summary": "Clare Cory was 59 years old and had been single almost her whole life. She thought her love story was over. Then everything changed.",
            "title": "Finding the Magic, Just in Time (Encore)"
        },
        {
            "content": [
                "<p>&#8230;the bad news is that they can&#8217;t agree which one.</p><p>I explained the debate more <a href=\"https://www.astralcodexten.com/p/missing-heritability-much-more-than\">here</a>, but the short version is: twin studies find that most traits are at least 50% genetic, sometimes much more. But molecular studies - that is, attempts to find the precise genes responsible - usually only found enough genes for the traits to be ~10-20% genetic. The remaining 35% was dubbed &#8220;missing heritability&#8221;. Nurturists argued that the twin studies must be wrong; hereditarians argued that missing effect must be in hard-to-find genes.</p><p>The latter seemed plausible because typical genetic studies only investigate the genes that most commonly vary across people - about 0.1% of the genome. Maybe the other 99.9% of genes, even though they rarely vary across people, are so numerous that even their tiny individual effects could add up to a large overall influence. There was no way to be sure, because variation in these genes was too rare to study effectively.</p><p>But as technology improved, funding increased, and questions about heredity became more pressing, geneticists finally set out to do the hard thing. They gathered full genomes - not just the 0.1% - from thousands of people, and applied a whole-genome analysis technique called GREML-WGS. The resulting study was published earlier this month as <strong><a href=\"https://www.nature.com/articles/s41586-025-09720-6\">Estimation and mapping of the missing heritability of human phenotypes</a></strong>, by Wainschtein, Yengo, et al.</p><p>Partisans on both sides agree it&#8217;s finally resolved the missing heritability debate, but they can&#8217;t agree on what the resolution is.</p><p>First, the study. The researchers got genetic data from 347,630 British people, and also measured their level of 34 traits, including both biomedical traits (like white blood cell count) and socially-relevant behavioral traits (like IQ).</p><p>Resolving missing heritability requires matching twin studies to genetic studies. The researchers were well-prepared to do a genetic study. But they couldn&#8217;t do a twin study, because most people in their sample did not have twins. And they couldn&#8217;t rely on the results of other twin studies, because twin studies - like every other type of study - return slightly different results in each group of people. So instead, they performed a &#8220;pedigree&#8221; study (their term, although it&#8217;s somewhat different from how pedigree studies usually work). Close relatives share whole chromosomes or other large stretches of DNA. By looking at who shared how many of these, they created a genealogical map of their sample: who was brothers, sisters, first cousins, second cousins, etc. Since there were 300,000+ participants, this was easy. Then, across moderately close relatives, they compared trait similarity to degree of relation. For example, I might be very similar in IQ to my brother, but somewhat less similar to my cousin, and even less similar to my second cousin once removed. After doing all of this, they could figure out how much more similar relatives were than non-relatives and get a family-based estimate for how genetic different traits were. This was their stand-in for twin studies.</p><p>Then they switched to people who were not close relatives, and tried to calculate their trait similarity based on detected genetic similarity; essentially, how many genes we share by pure chance. That is, if I and my neighbor are 50.001% genetically similar, and I and my other neighbor are 49.999% genetically similar, how much more do I resemble my first neighbor than my second neighbor?</p><p>When they were done, their pedigree study gave them a stand-in for twin studies, and their genetic study gave them an estimate of how much heritability could be detected with molecular genetic studies using both rare and common genes. This let them compare the two numbers, assessing the size of the &#8220;heritability gap&#8221; inclusive of rare variants.</p><p>The headline result: &#8220;WGS captures approximately 88% of the pedigree-based narrow sense heritability.&#8221;</p><p>The hereditarians declared victory (<a href=\"https://x.com/cremieuxrecueil/status/1988745983556276433\">Cremieux on X</a>, <a href=\"https://www.emilkirkegaard.com/p/what-did-the-new-wgs-ukbb-study-show\">Emil Kirkegaard on Substack</a>) because of this graph:</p><div class=\"captioned-image-container\"><figure><a class=\"image-link image2 is-viewable-img\" href=\"https://substackcdn.com/image/fetch/$s_!aA0I!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbd8a95de-bf38-444a-a6cd-95d3d064e31e_585x1168.png\" target=\"_blank\"><div class=\"image2-inset\"><source type=\"image/webp\" /><img alt=\"\" class=\"sizing-normal\" height=\"828.5811965811965\" src=\"https://substackcdn.com/image/fetch/$s_!aA0I!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbd8a95de-bf38-444a-a6cd-95d3d064e31e_585x1168.png\" title=\"\" width=\"415\" /><div class=\"image-link-expand\"><div class=\"pencraft pc-display-flex pc-gap-8 pc-reset\"><button class=\"pencraft pc-reset pencraft icon-container restack-image\" tabindex=\"0\" type=\"button\"><svg fill=\"none\" height=\"20\" stroke=\"var(--color-fg-primary)\" stroke-linecap=\"round\" stroke-linejoin=\"round\" stroke-width=\"1.5\" viewBox=\"0 0 20 20\" width=\"20\" xmlns=\"http://www.w3.org/2000/svg\"><g><title></title><path d=\"M2.53001 7.81595C3.49179 4.73911 6.43281 2.5 9.91173 2.5C13.1684 2.5 15.9537 4.46214 17.0852 7.23684L17.6179 8.67647M17.6179 8.67647L18.5002 4.26471M17.6179 8.67647L13.6473 6.91176M17.4995 12.1841C16.5378 15.2609 13.5967 17.5 10.1178 17.5C6.86118 17.5 4.07589 15.5379 2.94432 12.7632L2.41165 11.3235M2.41165 11.3235L1.5293 15.7353M2.41165 11.3235L6.38224 13.0882\"></path></g></svg></button><button class=\"pencraft pc-reset pencraft icon-container view-image\" tabindex=\"0\" type=\"button\"><svg class=\"lucide lucide-maximize2 lucide-maximize-2\" fill=\"none\" height=\"20\" stroke=\"currentColor\" stroke-linecap=\"round\" stroke-linejoin=\"round\" stroke-width=\"2\" viewBox=\"0 0 24 24\" width=\"20\" xmlns=\"http://www.w3.org/2000/svg\"><polyline points=\"15 3 21 3 21 9\"></polyline><polyline points=\"9 21 3 21 3 15\"></polyline><line x1=\"21\" x2=\"14\" y1=\"3\" y2=\"10\"></line><line x1=\"3\" x2=\"10\" y1=\"21\" y2=\"14\"></line></svg></button></div></div></div></a></figure></div><p>That is, once you include the rare variants, the amount of genetic variation that &#8220;should&#8221; exist but doesn&#8217;t shrinks to only 12%. Plausibly an even bigger study, investigating even rarer variants, could shrink the gap further, all the way to zero. The oldest and strongest argument against hereditarianism - if all these genes exist, why can&#8217;t we find them? - has finally been put to rest. You couldn&#8217;t find them because they were rare. But when you include rare variants in your search, you can find at least 88% of them.</p><p>But the nurturists declared victory (<a href=\"https://theinfinitesimal.substack.com/p/the-missing-heritability-question\">Sasha Gusev on Substack</a>) because the graph, zoomed out, looks like this:</p><div class=\"captioned-image-container\"><figure><a class=\"image-link image2 is-viewable-img\" href=\"https://substackcdn.com/image/fetch/$s_!p2K8!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F58d828e0-f906-4879-8e5e-20de23fbd1be_1123x656.png\" target=\"_blank\"><div class=\"image2-inset\"><source type=\"image/webp\" /><img alt=\"\" class=\"sizing-normal\" height=\"656\" src=\"https://substackcdn.com/image/fetch/$s_!p2K8!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F58d828e0-f906-4879-8e5e-20de23fbd1be_1123x656.png\" title=\"\" width=\"1123\" /><div class=\"image-link-expand\"><div class=\"pencraft pc-display-flex pc-gap-8 pc-reset\"><button class=\"pencraft pc-reset pencraft icon-container restack-image\" tabindex=\"0\" type=\"button\"><svg fill=\"none\" height=\"20\" stroke=\"var(--color-fg-primary)\" stroke-linecap=\"round\" stroke-linejoin=\"round\" stroke-width=\"1.5\" viewBox=\"0 0 20 20\" width=\"20\" xmlns=\"http://www.w3.org/2000/svg\"><g><title></title><path d=\"M2.53001 7.81595C3.49179 4.73911 6.43281 2.5 9.91173 2.5C13.1684 2.5 15.9537 4.46214 17.0852 7.23684L17.6179 8.67647M17.6179 8.67647L18.5002 4.26471M17.6179 8.67647L13.6473 6.91176M17.4995 12.1841C16.5378 15.2609 13.5967 17.5 10.1178 17.5C6.86118 17.5 4.07589 15.5379 2.94432 12.7632L2.41165 11.3235M2.41165 11.3235L1.5293 15.7353M2.41165 11.3235L6.38224 13.0882\"></path></g></svg></button><button class=\"pencraft pc-reset pencraft icon-container view-image\" tabindex=\"0\" type=\"button\"><svg class=\"lucide lucide-maximize2 lucide-maximize-2\" fill=\"none\" height=\"20\" stroke=\"currentColor\" stroke-linecap=\"round\" stroke-linejoin=\"round\" stroke-width=\"2\" viewBox=\"0 0 24 24\" width=\"20\" xmlns=\"http://www.w3.org/2000/svg\"><polyline points=\"15 3 21 3 21 9\"></polyline><polyline points=\"9 21 3 21 3 15\"></polyline><line x1=\"21\" x2=\"14\" y1=\"3\" y2=\"10\"></line><line x1=\"3\" x2=\"10\" y1=\"21\" y2=\"14\"></line></svg></button></div></div></div></a></figure></div><p>Of the colored region, very little is red (representing missing heritability). But most of the graph is still black - ie, not heritable. So for example, this study found that IQ was 41% heritable, and they were able to &#8220;find&#8221; 33%pp of that - a full three-quarters. But 41% heritable is still a low number!</p><p>Previous studies found high numbers (like 50 - 80%) for expected heritability, but were only able to get small numbers (10 - 20%pp) for &#8220;found heritability&#8221;. This study &#8220;closed the gap&#8221; by finding medium numbers (~30 - 40%)  for both. But a medium amount of almost-fully-found heritability is still only a medium amount of heritability. Start with 30 - 40%, shave off a bit for confounders, and you might end up with only 10 - 20% direct causal heritability, which would be a total nurturist victory.</p><p>The hereditarians object that this study wasn&#8217;t designed to pinpoint specific heritability numbers. Other methods are more accurate. But (the nurturists counter) those more accurate methods <a href=\"https://www.astralcodexten.com/p/missing-heritability-much-more-than\">disagree among themselves</a>, and some of them give results similar to the low numbers in this study. So this study is welcome (to nurturists) confirmation that the other low studies might have been on the right track.</p><p>In other words, your interpretation on this study depends on which of these statements you agree with more:</p><ol><li><p>This study was designed to determine whether the missing heritability - the gap between relatedness and molecular methods - can be found in rare variants. It can be. We should celebrate this, and not worry too much about the exact heritability numbers, since it was never designed to find exact numbers in the first place.</p></li><li><p>This study determined that there was never that much heritability to find in the first place. We found that small amount, but it&#8217;s still small. This study wasn&#8217;t designed to pinpoint exact numbers, but the ones that are all also sort of consistent with it being small, and this study certainly doesn&#8217;t provide any extra evidence that it <em>isn&#8217;t</em>.</p></li></ol><p>So who&#8217;s right?</p><p>Emil and Cremieux <a href=\"https://www.emilkirkegaard.com/p/what-did-the-new-wgs-ukbb-study-show\">argue </a>that we know why this study found low heritability of IQ. It&#8217;s because you can&#8217;t give 347,630 people a full-length IQ test. So they gave these people a short crappy IQ-like test with a lot of random noise. Past studies estimated the reliability of this test at 0.61 (low). It&#8217;s easy to statistically correct for this; when you do so, you find that if the test had been better, this study would have estimated the heritability of IQ at 55%. This is still on the low end, but it&#8217;s already within the hereditarians&#8217; estimate of 50 - 80%, and there are a few other biases that might be bringing it down too (eg healthy volunteer bias). </p><p>The advantage of this theory is that the measurements and statistical corrections are pretty simple, and it&#8217;s definitely true. The disadvantage is that IQ is only one piece of this bigger puzzle, and every trait in the study is lower than expected.</p><div class=\"captioned-image-container\"><figure><a class=\"image-link image2 is-viewable-img\" href=\"https://substackcdn.com/image/fetch/$s_!xUIg!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F87e7debf-f5ec-40c2-8cb4-6357af865833_346x272.png\" target=\"_blank\"><div class=\"image2-inset\"><source type=\"image/webp\" /><img alt=\"\" class=\"sizing-normal\" height=\"272\" src=\"https://substackcdn.com/image/fetch/$s_!xUIg!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F87e7debf-f5ec-40c2-8cb4-6357af865833_346x272.png\" width=\"346\" /><div class=\"image-link-expand\"><div class=\"pencraft pc-display-flex pc-gap-8 pc-reset\"><button class=\"pencraft pc-reset pencraft icon-container restack-image\" tabindex=\"0\" type=\"button\"><svg fill=\"none\" height=\"20\" stroke=\"var(--color-fg-primary)\" stroke-linecap=\"round\" stroke-linejoin=\"round\" stroke-width=\"1.5\" viewBox=\"0 0 20 20\" width=\"20\" xmlns=\"http://www.w3.org/2000/svg\"><g><title></title><path d=\"M2.53001 7.81595C3.49179 4.73911 6.43281 2.5 9.91173 2.5C13.1684 2.5 15.9537 4.46214 17.0852 7.23684L17.6179 8.67647M17.6179 8.67647L18.5002 4.26471M17.6179 8.67647L13.6473 6.91176M17.4995 12.1841C16.5378 15.2609 13.5967 17.5 10.1178 17.5C6.86118 17.5 4.07589 15.5379 2.94432 12.7632L2.41165 11.3235M2.41165 11.3235L1.5293 15.7353M2.41165 11.3235L6.38224 13.0882\"></path></g></svg></button><button class=\"pencraft pc-reset pencraft icon-container view-image\" tabindex=\"0\" type=\"button\"><svg class=\"lucide lucide-maximize2 lucide-maximize-2\" fill=\"none\" height=\"20\" stroke=\"currentColor\" stroke-linecap=\"round\" stroke-linejoin=\"round\" stroke-width=\"2\" viewBox=\"0 0 24 24\" width=\"20\" xmlns=\"http://www.w3.org/2000/svg\"><polyline points=\"15 3 21 3 21 9\"></polyline><polyline points=\"9 21 3 21 3 15\"></polyline><line x1=\"21\" x2=\"14\" y1=\"3\" y2=\"10\"></line><line x1=\"3\" x2=\"10\" y1=\"21\" y2=\"14\"></line></svg></button></div></div></div></a></figure></div><p>This table compares the heritability found in a typical twin study to the heritability numbers found in the pedigree portion of this study. On average, this study&#8217;s numbers are only about 60% as high; IQ isn&#8217;t really an outlier (although if we used Kirkegaard&#8217;s adjusted number, that wouldn&#8217;t be too much of an outlier either). </p><p>But this same argument can be deployed against the nurturists&#8217; favorite explanations for high twin study numbers: population stratification and assortative mating. These could be expected to affect socially-relevant and environmentally-mediated traits like educational attainment. But nobody assortative-mates on white blood cell count, and these types of &#8220;hard&#8221; biomedical traits are just as depressed in this study as the &#8220;soft&#8221; behavioral ones.</p><p>The real answer is that despite everyone&#8217;s pronouncements, nobody&#8217;s won, nothing has been resolved, and the debate continues. Different studies continue to find different heritability estimates and nobody has a good explanation why. Here are the two stories you could tell, updated for this new paper:</p><p><strong>Hereditarian: </strong>Most traits are 50 - 80% heritable, as per twin studies, adoption studies, and classic pedigree studies. Molecular genetics studies underestimate this because much of the heritability is in rare variants, as this new study demonstrates. Sib-regression, RDR, and this new study&#8217;s &#8220;pedigree-style&#8221; analysis underestimate this because they&#8217;re untested methods applied to problematic samples and the estimates are noisy; also, shut up.</p><p><strong>Nurturist: </strong>Most traits are ~30% heritable, as per Sib-regression, RDR, molecular genetics, and this new study&#8217;s &#8220;pedigree-style&#8221; analysis. Twin studies, adoption studies, and pedigree studies overestimate this because of assortative mating and population stratification. This affects biomedical traits like white blood cell count just as much as behavioral traits, because shut up. The one sib-regression study that found very high heritability for IQ was just a weird sample, or noise.</p><p>Can we reconcile these narratives?</p><p>The hereditarian case is strongest for height, but only slightly weaker for intelligence. If we accept Kirkegaard and Cremieux&#8217;s correction, then this study found up to 55% heritability of IQ, and the only sib-regression study on the topic found 75% (albeit with low confidence). But this is stringing together a corrected estimate with a noisy estimate and I have low confidence that the next study won&#8217;t find something lower.</p><p>The nurturist case is strongest for educational attainment. This is easily confused by nondirect effects, and a sib-regression study, the best type to see through the confusion, found &lt;10% direct heritability. But if IQ is &gt;55% heritable and educational attainment is &lt;10% heritable, does this require us to believe that IQ only barely affects success in education? A certain sort of contrarian might relish this conclusion. </p><p>The biomedical traits confuse me the most; it&#8217;s still hard to square the twin studies with the sib-regression and molecular estimates. Either people are somehow assortative mating on blood pressure, or else these remain the strongest evidence of some deeper problem.</p>"
            ],
            "link": "https://www.astralcodexten.com/p/the-good-news-is-that-one-side-has",
            "publishedAt": "2025-12-03",
            "source": "SlateStarCodex",
            "summary": "<p>&#8230;the bad news is that they can&#8217;t agree which one.</p><p>I explained the debate more <a href=\"https://www.astralcodexten.com/p/missing-heritability-much-more-than\">here</a>, but the short version is: twin studies find that most traits are at least 50% genetic, sometimes much more. But molecular studies - that is, attempts to find the precise genes responsible - usually only found enough genes for the traits to be ~10-20% genetic. The remaining 35% was dubbed &#8220;missing heritability&#8221;. Nurturists argued that the twin studies must be wrong; hereditarians argued that missing effect must be in hard-to-find genes.</p><p>The latter seemed plausible because typical genetic studies only investigate the genes that most commonly vary across people - about 0.1% of the genome. Maybe the other 99.9% of genes, even though they rarely vary across people, are so numerous that even their tiny individual effects could add up to a large overall influence. There was no way to be sure, because variation in these genes was too rare to study effectively.</p><p>But as technology improved, funding increased, and questions about heredity became more pressing, geneticists finally set out to do the hard thing. They gathered full genomes - not just the 0.1% - from thousands of people, and applied a whole-genome analysis technique called",
            "title": "The Good News Is That One Side Has Definitively Won The Missing Heritability Debate"
        },
        {
            "content": [
                "<p>Some podcasts are self-recommending on the \u2018yep, I\u2019m going to be breaking this one down\u2019 level. This was very clearly one of those. So here we go.</p>\n<div>\n<div>\n<div>\n<div>Double click to interact with video</div>\n</div>\n</div>\n</div>\n<p>As usual for podcast posts, the baseline bullet points describe key points made, and then the nested statements are my commentary.</p>\n<p><a href=\"https://www.dwarkesh.com/p/ilya-sutskever-2\">If I am quoting directly I use quote marks, otherwise assume paraphrases</a>.</p>\n<p>What are the main takeaways?</p>\n<ol>\n<li>Ilya thinks training in its current form will peter out, that we are returning to an age of research where progress requires more substantially new ideas.</li>\n<li>SSI is a research organization. It tries various things. Not having a product lets it punch well above its fundraising weight in compute and effective resources.\n<div>\u00a0</div>\n</li>\n</ol>\n\n\n<span id=\"more-24927\"></span>\n\n\n<ul>\n<li>Ilya has 5-20 year timelines to a potentially superintelligent learning model.</li>\n<li>SSI might release a product first after all, but probably not?</li>\n<li>Ilya\u2019s thinking about alignment still seems relatively shallow to me in key ways, but he grasps many important insights and understands he has a problem.</li>\n<li>Ilya essentially despairs of having a substantive plan beyond \u2018show everyone the thing as early and often as possible\u2019 and hope for the best. He doesn\u2019t know where to go or how to get there, but does realize he doesn\u2019t know these things, so he\u2019s well ahead of most others.</li>\n</ul>\n<p>Afterwards, this post also covers <a href=\"https://www.dwarkesh.com/p/thoughts-on-ai-progress-dec-2025\">Dwarkesh Patel\u2019s post on the state of AI progress</a>.</p>\n\n\n<h4 class=\"wp-block-heading\">Table of Contents</h4>\n\n\n<ol>\n<li><a href=\"https://thezvi.substack.com/i/180438330/explaining-model-jaggedness\">Explaining Model Jaggedness.</a></li>\n<li><a href=\"https://thezvi.substack.com/i/180438330/emotions-and-value-functions\">Emotions and value functions.</a></li>\n<li><a href=\"https://thezvi.substack.com/i/180438330/what-are-we-scaling\">What are we scaling?</a></li>\n<li><a href=\"https://thezvi.substack.com/i/180438330/why-humans-generalize-better-than-models\">Why humans generalize better than models.</a></li>\n<li><a href=\"https://thezvi.substack.com/i/180438330/straight-shooting-superintelligence\">Straight-shooting superintelligence.</a></li>\n<li><a href=\"https://thezvi.substack.com/i/180438330/ssi-s-model-will-learn-from-deployment\">SSI\u2019s model will learn from deployment.</a></li>\n<li><a href=\"https://thezvi.substack.com/i/180438330/alignment\">Alignment.</a></li>\n<li><a href=\"https://thezvi.substack.com/i/180438330/we-are-squarely-an-age-of-research-company\">\u201cWe are squarely an age of research company\u201d.</a></li>\n<li><a href=\"https://thezvi.substack.com/i/180438330/research-taste\">Research taste.</a></li>\n<li><a href=\"https://thezvi.substack.com/i/180438330/bonus-coverage-dwarkesh-patel-on-ai-progress-these-days\">Bonus Coverage: Dwarkesh Patel on AI Progress These Days.</a></li>\n</ol>\n\n\n<h4 class=\"wp-block-heading\">Explaining Model Jaggedness</h4>\n\n\n<ol>\n<li>Ilya opens by remarking how crazy it is all this (as in AI) is real, it\u2019s all so sci-fi, and yet it\u2019s not felt in other ways so far. Dwarkesh expects this to continue for average people into the singularity, Ilya says no, AI will diffuse and be felt in the economy. Dwarkesh says impact seems smaller than model intelligence implies.\n<ol>\n<li>Ilya is right here. Dwarkesh is right that direct impact so far has been smaller than model intelligence implies, but give it time.</li>\n</ol>\n</li>\n<li>Ilya says, the models are really good at evals but economic impact lags. The models are buggy, and choices for RL take inspiration from the evals, so the evals are misleading and the humans are essentially reward hacking the evals. And that given they got their scores by studying for tons of hours rather than via intuition, one should expect AIs to underperform their benchmarks.\n<ol>\n<li>AIs definitely underperform their benchmarks in terms of general usefulness, even for those companies that do minimal targeting of benchmarks. Overall capabilities lag behind, for various reasons. We still have an impact gap.</li>\n</ol>\n</li>\n<li>The super talented student? The one that hardly even needs to practice a specific task to be good? They\u2019ve got \u2018it.\u2019 Models don\u2019t have \u2018it.\u2019\n<ol>\n<li>If anything, models have \u2018anti-it.\u2019 They make it up on volume. Sure.</li>\n</ol>\n</li>\n</ol>\n\n\n<h4 class=\"wp-block-heading\">Emotions and value functions</h4>\n\n\n<ol>\n<li>Humans train on much less data, but what they know they know \u2018more deeply\u2019 somehow, there are mistakes we wouldn\u2019t make. Also evolution can be highly robust, for example the famous case where a guy lost all his emotions and in many ways things remained fine.\n<ol>\n<li>People put a lot of emphasis on the \u2018I would never\u2019 heuristic, as AIs will sometimes do things \u2018a similarly smart person\u2019 would never do, they lack a kind of common sense.</li>\n</ol>\n</li>\n<li>So what is the \u2018ML analogy for emotions\u2019? Ilya says some kind of value function thing, as in the thing that tells you if you\u2019re doing well versus badly while doing something.\n<ol>\n<li>Emotions as value functions makes sense, but they are more information-dense than merely a scalar, and can often point you to things you missed. They do also serve as training reward signals.</li>\n<li>I don\u2019t think you \u2018need\u2019 emotions for anything other than signaling emotions, if you are otherwise sufficiently aware in context, and don\u2019t need them to do gradient descent.</li>\n<li>However in a human, if you knock out the emotions in places where you were otherwise relying on them for information or to resolve uncertainty, you\u2019re going to have a big problem.</li>\n<li>I notice an obvious thing to try but it isn\u2019t obvious how to implement it?</li>\n</ol>\n</li>\n<li>Ilya has faith in deep learning. There\u2019s nothing it can\u2019t do!</li>\n</ol>\n\n\n<h4 class=\"wp-block-heading\">What are we scaling?</h4>\n\n\n<ol>\n<li>Data? Parameters? Compute? What else? It\u2019s easier and more reliable to scale up pretraining than to figure out what else to do. But we\u2019ll run out of data soon even if Gemini 3 got more out of this, so now you need to do something else. If you had 100x more scale here would anything be that different? Ilya thinks no.\n<ol>\n<li>Sounds like a skill issue, on some level, but yes if you didn\u2019t change anything else then I expect scaling up pretraining further won\u2019t help enough to justify the increased costs in compute and time.</li>\n</ol>\n</li>\n<li>RL costs now exceed pretraining costs, because each RL run costs a lot. It\u2019s time to get back to an age of research, trying interesting things and seeing what happens.\n<ol>\n<li>I notice I am skeptical of the level of skepticism, also I doubt the research mode ever stopped in the background. The progress will continue. It\u2019s weird how every time someone says \u2018we still need some new idea or breakthrough\u2019 there is the implication that this likely never happens again.</li>\n</ol>\n</li>\n</ol>\n\n\n<h4 class=\"wp-block-heading\">Why humans generalize better than models</h4>\n\n\n<ol>\n<li>Why do AIs require so much more data than humans to learn? Why don\u2019t models easily pick up on all this stuff humans learn one-shot or in the background?\n<ol>\n<li>Humans have richer data than text so the ratio is not as bad as it looks, but primarily because our AI learning techniques are relatively primitive and data inefficient in various ways.</li>\n<li>My full answer to how to fix it falls under \u2018I don\u2019t do $100m/year jobs for free.\u2019</li>\n<li>Also there are ways in which the LLMs learn way better than you realize, and a lot of the tasks humans easily learn are regularized in non-obvious ways.</li>\n</ol>\n</li>\n<li>Ilya believes humans being good at learning is mostly not part of some complicated prior, and people\u2019s robustness is really staggering.\n<ol>\n<li>I would clarify, not part of a complicated specialized prior. There is also a complicated specialized prior in some key domains, but that is in addition to a very strong learning function.</li>\n<li>People are not as robust as Ilya thinks, or most people think.</li>\n</ol>\n</li>\n<li>Ilya suggests perhaps human neurons use more compute than we think.</li>\n</ol>\n\n\n<h4 class=\"wp-block-heading\">Straight-shooting superintelligence</h4>\n\n\n<ol>\n<li>Scaling \u2018sucked the air out of the room\u2019 so no one did anything else. Now there are more companies than ideas. You need some compute to bring ideas to life, but not the largest amounts.\n<ol>\n<li>You can also think about some potential techniques as \u2018this is not worth trying unless you have massive scale.\u2019</li>\n</ol>\n</li>\n<li>SSI\u2019s compute all goes into research, none into inference, and they don\u2019t try to build a product, and if you\u2019re doing something different you don\u2019t have to use maximum scale, so their $3 billion that they\u2019ve raised \u2018goes a long way\u2019 relative to the competition. Sure OpenAI spends ~$5 billion a year on experiments, but it\u2019s what you do with it.\n<ol>\n<li>This is what Ilya has to say in this spot, but there\u2019s merit in it. OpenAI\u2019s experiments are largely about building products now. This transfers to the quest for superintelligence, but not super efficiently.</li>\n</ol>\n</li>\n<li>How will SSI make money? Focus on the research, the money will appear.\n<ol>\n<li>Matt Levine has answered this one, which is that you make money by being an AI company full of talented researchers, so people give you money.</li>\n</ol>\n</li>\n<li>SSI is considering making a product anyway, both to have the product exist and also because timelines might be long.\n<ol>\n<li>I mean I guess at some point the \u2018we are AI researchers give us money\u2019 strategy starts to look a little suspicious, but let\u2019s not rush into anything.</li>\n<li>Remember, Ilya, once you have a product and try to have revenue they\u2019ll evaluate the product and your revenue. If you don\u2019t have one, you\u2019re safe.</li>\n</ol>\n</li>\n</ol>\n\n\n<h4 class=\"wp-block-heading\">SSI\u2019s model will learn from deployment</h4>\n\n\n<ol>\n<li>Ilya says even if there is a straight shot to superintelligence deployment would be gradual, you have to ship something first, and that he agrees with Dwarkesh on the importance of continual learning, it would \u2018go and be\u2019 various things and learn, superintelligence is not a finished mind.\n<ol>\n<li>Learning takes many forms, including continual learning, it can be updating within the mind or otherwise, and so on. See previous podcast discussions.</li>\n</ol>\n</li>\n<li>Ilya expects \u2018rapid\u2019 economic growth, perhaps \u2018very rapid.\u2019 It will vary based on what rules are set in different places.\n<ol>\n<li>Rapid means different things to different people, it sounds like Ilya doesn\u2019t have a fixed rate in mind. I interpret it as \u2018more than these 2% jokers.\u2019</li>\n<li>This vision still seems to think the humans stay in charge. Why?</li>\n</ol>\n</li>\n</ol>\n\n\n<h4 class=\"wp-block-heading\">Alignment</h4>\n\n\n<ol>\n<li>Dwarkesh reprises the standard point that if AIs are merely \u2018as good at\u2019 humans at learning, but they can \u2018merge brains\u2019 then crazy things happen. How do we make such a situation go well? What is SSI\u2019s plan?\n<ol>\n<li>I mean, that\u2019s the least of it, but hopefully yes that suffices to make the point?</li>\n</ol>\n</li>\n<li>Ilya emphasizes deploying incrementally and in advance. It\u2019s hard to predict what this will be like in advance. \u201cThe problem is the power. When the power is really big, what\u2019s going to happen? If it\u2019s hard to imagine, what do you do? You\u2019ve got to be showing the thing.\u201d\n<ol>\n<li>This feels like defeatism, in terms of saying we can only respond to things once we can see and appreciate them. We can\u2019t plan for being old until we know what that\u2019s like. We can\u2019t plan for AGI/ASI, or AI having a lot of power, until we can see that in action.</li>\n<li>But obviously by then it is likely to be too late, and most of your ability to steer what happens has already been lost, perhaps all of it.</li>\n<li>This is the strategy of \u2018muddle through\u2019 the same as we always muddle through, basically the plan of not having a plan other than incrementalism. I do not care for this plan. I am not happy to be a part of it. I do not think that is a case of Safe Superintelligence.</li>\n</ol>\n</li>\n<li>Ilya expects governments and labs to play big roles, and for labs to increasingly coordinate on safety, <a href=\"https://techcrunch.com/2025/08/27/openai-co-founder-calls-for-ai-labs-to-safety-test-rival-models/\">as Anthropic and OpenAI did in a recent first step</a>. And we have to figure out what we should be building. He suggests making the AI care about sentient life in general will be \u2018easier\u2019 than making it care about humans, since the AI will be sentient.\n<ol>\n<li>If the AIs do not care about humans in particular, there is no reason to expect humans to stay in control or to long endure.</li>\n</ol>\n</li>\n<li>Ilya would like the most powerful superintelligence to \u2018somehow\u2019 be \u2018capped\u2019 to address these concerns. But he doesn\u2019t know how to do that.\n<ol>\n<li>I don\u2019t know how to do that either. It\u2019s not clear the idea is coherent.</li>\n</ol>\n</li>\n<li>Dwarkesh asks how much \u2018room is there at the top\u2019 for superintelligence to be more super? Maybe it just learns fast or has a bigger pool of strategies or skills or knowledge? Ilya says very powerful, for sure.\n<ol>\n<li>Sigh. There is very obviously quite a lot of \u2018room at the top\u2019 and humans are not anything close to maximally intelligent, nor to getting most of what intelligence has to offer. At this point, the number of people who still don\u2019t realize or accept this reinforces how much better a smarter entity could be.</li>\n</ol>\n</li>\n<li>Ilya expects these superintelligences to be very large, as in physically large, and for several to come into being at roughly the same time, and ideally they could \u201cbe restrained in some ways or if there was some kind of agreement or something.\u201d\n<ol>\n<li>That agreement between AIs would then be unlikely to include us. Yes, functional restraints would be nice, but this is the level of thought that has gone into finding ways to do it.</li>\n<li>There\u2019s been a lot of things staying remarkably close, but a lot of that is because rather than an edge compounding and accelerating for now catching up has been easier.</li>\n</ol>\n</li>\n<li>Ilya: \u201cWhat is the concern of superintelligence? What is one way to explain the concern? If you imagine a system that is sufficiently powerful, really sufficiently powerful\u2014and you could say you need to do something sensible like care for sentient life in a very single-minded way\u2014we might not like the results. That\u2019s really what it is.\u201d\n<ol>\n<li>Well, yes, standard Yudkowsky, no fixed goal we can name turns out well.</li>\n</ol>\n</li>\n<li>Ilya says maybe we don\u2019t build an RL agent. Humans are semi-RL agents, our emotions make us alter our rewards and pursue different rewards after a while. If we keep doing what we are doing now it will soon peter out and never be \u201cit.\u201d\n<ol>\n<li>There\u2019s a baked in level of finding innovations and improvements that should be in anyone\u2019s \u2018keep doing what we are doing\u2019 prior, and I think it gets us pretty far and includes many individually low-probability-of-working innovations making substantial differences. There is some level on which we would \u2018peter out\u2019 without a surprise, but it\u2019s not clear that this requires being surprised overall.</li>\n<li>Is it possible things do peter out and we never see \u2018it\u2019? Yeah. It\u2019s possible. I think it\u2019s a large underdog to stay that way for long, but it\u2019s possible. Still a long practical way to go even then.</li>\n<li>Emotions, especially boredom and the fading of positive emotions on repetition, are indeed one of the ways we push ourselves towards exploration and variety. That\u2019s one of many things they do, and yes if we didn\u2019t have them then we would need something else to take their place.</li>\n<li>In many cases I have indeed used logic to take the place of that, when emotion seems to not be sufficiently preventing mode collapse.</li>\n</ol>\n</li>\n<li>\u201cOne of the things that you could say about what causes alignment to be difficult is that your ability to learn human values is fragile. Then your ability to optimize them is fragile. You actually learn to optimize them. And can\u2019t you say, \u201cAre these not all instances of unreliable generalization?\u201d Why is it that human beings appear to generalize so much better? What if generalization was much better? What would happen in this case? What would be the effect? But those questions are right now still unanswerable.\u201d\n<ol>\n<li>It is cool to hear Ilya restate these Yudkowsky 101 things.</li>\n<li>Humans do not actually generalize all that well.</li>\n</ol>\n</li>\n<li>How does one think about what AI going well looks like? Ilya goes back to \u2018AI that cares for sentient life\u2019 as a first step, but then asks the better question, what is the long run equilibrium? He notices he does not like his answer. Maybe each person has an AI that will do their bidding and that\u2019s good, but the downside is then the AI does things like earn money or advocate or whatever, and the person says \u2018keep it up\u2019 but they\u2019re not a participant. Precarious. People become part AI, Neurolink++. He doesn\u2019t like this solution, but it is at least a solution.\n<ol>\n<li>Big points for acknowledging that there are no known great solutions.</li>\n<li>Big points for pointing out one big flaw, that the people stop actually doing the things, because the AIs do the things better.</li>\n<li>The equilibrium here is that increasingly more things are turned over to AIs, including both actions and decisions. Those who don\u2019t do this fall behind.</li>\n<li>The equilibrium here is that increasingly AIs are given more autonomy, more control, put in better positions, have increasing power and wealth shares, and so on, even if everything involved is fully voluntary and \u2018nothing goes wrong.\u2019</li>\n<li>Neurolink++ does not meaningfully solve any of the problems here.</li>\n<li>Solve for the equilibrium.</li>\n</ol>\n</li>\n<li>Is the long history of emotions an alignment success? As in, it allows the brain to move from \u2018mate with somebody who\u2019s more successful\u2019 into flexibly defining success and generally adjusting to new situations.\n<ol>\n<li>It\u2019s a highly mixed bag, wouldn\u2019t you say?</li>\n<li>There are ways in which those emotions have been flexible and adaptable and a success, and have succeeded in the alignment target (inclusive genetic fitness) and also ways in which emotions are very obviously failing people.</li>\n<li>If ASIs are about as aligned as we are in this sense, we\u2019re doomed.</li>\n</ol>\n</li>\n<li>Ilya says it\u2019s mysterious how evolution encodes high-level desires, but it gives us all these social desires, and they evolved pretty recently. Dwarkesh points out it is desire you learned in your lifetime. Ilya notes the brain as regions and some things are hardcoded, but if you remove half the brain then the regions move, the social stuff is highly reliable.\n<ol>\n<li>I don\u2019t pretend to understand the details here, although I could speculate.</li>\n</ol>\n</li>\n</ol>\n\n\n<h4 class=\"wp-block-heading\">\u201cWe are squarely an age of research company\u201d</h4>\n\n\n<ol>\n<li>SSI investigates ideas to see if they are promising. They do research.</li>\n<li>On his cofounder leaving: \u201cFor this, I will simply remind a few facts that may have been forgotten. I think these facts which provide the context explain the situation. The context was that we were fundraising at a $32 billion valuation, and then <a href=\"https://www.theverge.com/command-line-newsletter/690720/meta-buy-thinking-machines-perplexity-safe-superintelligence\">Meta came in and offered to acquire us</a>, and I said no. But my former cofounder in some sense said yes. As a result, he also was able to enjoy a lot of near-term liquidity, and he was the only person from SSI to join Meta.\u201d\n<ol>\n<li>I love the way he put that. Yes.</li>\n</ol>\n</li>\n<li>\u201cThe main thing that distinguishes SSI is its technical approach. We have a different technical approach that I think is worthy and we are pursuing it. I maintain that in the end there will be a convergence of strategies. I think there will be a convergence of strategies where at some point, as AI becomes more powerful, it\u2019s going to become more or less clearer to everyone what the strategy should be. It should be something like, you need to find some way to talk to each other and you want your first actual real superintelligent AI to be aligned and somehow care for sentient life, care for people, democratic, one of those, some combination thereof. I think this is the condition that everyone should strive for. That\u2019s what SSI is striving for. I think that this time, if not already, all the other companies will realize that they\u2019re striving towards the same thing. We\u2019ll see. I think that the world will truly change as AI becomes more powerful. I think things will be really different and people will be acting really differently.\u201d\n<ol>\n<li>This is a remarkably shallow, to me, vision of what the alignment part of the strategy looks like, but it does get an admirably large percentage of the overall strategic vision, as in most of it?</li>\n<li>The idea that \u2018oh as we move farther along people will get more responsible and cooperate more\u2019 seems to not match what we have observed so far, alas.</li>\n<li>Ilya later clarifies he specifically meant convergence on alignment strategies, although he also expects convergence on technical strategies.</li>\n<li>The above statement is convergence on an alignment goal, but that doesn\u2019t imply convergence on alignment strategy. Indeed it does not imply that an alignment strategy that is workable even exists.</li>\n</ol>\n</li>\n<li>Ilya\u2019s timeline to the system that can learn and become superhuman? 5-20 years.</li>\n<li>Ilya predicts that when someone releases the thing that will be information but it won\u2019t teach others how to do the thing, although they will eventually learn.</li>\n<li>What is the \u2018good world\u2019? We have powerful human-like learners and perhaps narrow ASIs, and companies make money, and there is competition through specialization, different niches. Accumulated learning and investment creates specialization.\n<ol>\n<li>This is so frustrating, in that it doesn\u2019t explain why you would expect that to be how this plays out, or why this world turns out well, or anything really? Which would be fine if the answers were clear or at least those seemed likely, but I very much don\u2019t think that.</li>\n<li>This feels like a claim that humans are indeed near the upper limit of what intelligence can do and what can be learned except that we are hobbled in various ways and AIs can be unhobbled, but that still leaves them functioning in ways that seem recognizably human and that don\u2019t crowd us out? Except again I don\u2019t think we should expect this.</li>\n</ol>\n</li>\n<li>Dwarkesh points out current LLMs are similar, Ilya says perhaps the datasets are not as non-overlapping as they seem.\n<ol>\n<li>On the contrary, I was assuming they were mostly the same baseline data, and then they do different filtering and progressions from there? Not that there\u2019s zero unique data but that most companies have \u2018most of the data.\u2019</li>\n</ol>\n</li>\n<li>Dwarkesh suggests, therefore AIs will have less diversity than human teams. How can we get \u2018meaningful diversity\u2019? Ilya says this is because of pretraining, that post training is different.\n<ol>\n<li>To the extent that such \u2018diversity\u2019 is useful it seems easy to get with effort. I suspect this is mostly another way to create human copium.</li>\n</ol>\n</li>\n<li>What about using self-play? Ilya notes it allows using only compute, which is very interesting, but it is only good for \u2018developing a certain set of skills.\u2019 Negotiation, conflict, certain social strategies, strategizing, that kind of stuff. Then Ilya self-corrects, notes other forms, like debate, prover-verifier or forms of LLM-as-a-judge, it\u2019s a special case of agent competition.\n<ol>\n<li>I think there\u2019s a lot of promising unexplored space here, decline to say more.</li>\n</ol>\n</li>\n</ol>\n\n\n<h4 class=\"wp-block-heading\">Research taste</h4>\n\n\n<ol>\n<li>What is research taste? How does Ilya come up with many big ideas?</li>\n</ol>\n<p>This is hard to excerpt and seems important, so quoting in full to close out:</p>\n<blockquote><p>I can comment on this for myself. I think different people do it differently. One thing that guides me personally is an aesthetic of how AI should be, by thinking about how people are, but thinking correctly. It\u2019s very easy to think about how people are incorrectly, but what does it mean to think about people correctly?</p>\n<p>I\u2019ll give you some examples. The idea of the <a href=\"https://en.wikipedia.org/wiki/Artificial_neuron\">artificial neuron</a> is directly inspired by the brain, and it\u2019s a great idea. Why? Because you say the brain has all these different organs, it has the <a href=\"https://en.wikipedia.org/wiki/Gyrification\">folds</a>, but the folds probably don\u2019t matter. Why do we think that the neurons matter? Because there are many of them. It kind of feels right, so you want the neuron. You want some local learning rule that will change the connections between the neurons. It feels plausible that the brain does it.</p>\n<p>The idea of the <a href=\"https://web.stanford.edu/~jlmcc/papers/PDP/Chapter3.pdf\">distributed representation</a>. The idea that the brain responds to experience therefore our neural net should learn from experience. The brain learns from experience, the neural net should learn from experience. You kind of ask yourself, is something fundamental or not fundamental? How things should be.</p>\n<p>I think that\u2019s been guiding me a fair bit, thinking from multiple angles and looking for almost beauty, beauty and simplicity. Ugliness, there\u2019s no room for ugliness. It\u2019s beauty, simplicity, elegance, correct inspiration from the brain. All of those things need to be present at the same time. The more they are present, the more confident you can be in a top-down belief.</p>\n<p>The top-down belief is the thing that sustains you when the experiments contradict you. Because if you trust the data all the time, well sometimes you can be doing the correct thing but there\u2019s a bug. But you don\u2019t know that there is a bug. How can you tell that there is a bug? How do you know if you should keep debugging or you conclude it\u2019s the wrong direction? It\u2019s the top-down. You can say things have to be this way. Something like this has to work, therefore we\u2019ve got to keep going. That\u2019s the top-down, and it\u2019s based on this multifaceted beauty and inspiration by the brain.</p></blockquote>\n<p>I need to think more about what causes my version of \u2018research taste.\u2019 It\u2019s definitely substantially different.</p>\n<p>That ends our podcast coverage, and enter the bonus section, which seems better here than in the weekly, as it covers many of the same themes.</p>\n\n\n<h4 class=\"wp-block-heading\">Bonus Coverage: Dwarkesh Patel on AI Progress These Days</h4>\n\n\n<p><a href=\"https://www.dwarkesh.com/p/thoughts-on-ai-progress-dec-2025\">Dwarkesh Patel offers his thoughts on AI progress these days</a>, noticing that when we get the thing he calls \u2018actual AGI\u2019 things are going to get fucking crazy, but thinking that this is 10-20 years away from happening in full. Until then, he\u2019s a bit skeptical of how many gains we can realize, but skepticism is highly relative here.</p>\n<blockquote><p>Dwarkesh Patel: I\u2019m confused why some people have short timelines and at the same time are bullish on RLVR. If we\u2019re actually close to a human-like learner, this whole approach is doomed.</p>\n<p>\u2026 Either these models will soon learn on the job in a self directed way &#8211; making all this pre-baking pointless &#8211; or they won\u2019t &#8211; which means AGI is not imminent. Humans don\u2019t have to go through a special training phase where they need to rehearse every single piece of software they might ever use.</p></blockquote>\n<p>Wow, look at those goalposts move (in all the different directions). Dwarkesh notes that the bears keep shifting on the bulls, but says this is justified because current models fit the old goals but don\u2019t score the points, as in they don\u2019t automate workflows as much as you would expect.</p>\n<p>In general, I worry about the expectation pattern having taken the form of \u2018median 50 years \u2192 20 \u2192 10 \u2192 5 \u2192 7, and once I heard someone said 3, so oh nothing to see there you can stop worrying.\u2019</p>\n<p>In this case, look at the shift: An \u2018actual\u2019 (his term) AGI must now not only be capable of human-like performance of tasks, the AGI must also be a human-efficient learner.</p>\n<p>That would mean AGI and ASI are the same thing, or at least arrive in rapid succession. An AI that was human-efficient at learning from data, combined with AI\u2019s other advantages that include imbibing orders of magnitude more data, would be a superintelligence and would absolutely set off recursive self-improvement from there.</p>\n<p>And yes, <a href=\"https://x.com/sriramk/status/1996023624973910055\">if that\u2019s what you mean</a> then AGI isn\u2019t the best concept for thinking about timelines, and superintelligence is the better target to talk about. Sriram Krishnan is however <a href=\"https://x.com/sriramk/status/1996056881551299031/history\">opposed to using either of them</a>.</p>\n<p>Like all conceptual handles or fake frameworks, it is imprecise and overloaded, but people\u2019s intuitions about it miss that the thing is possible or exists even when you outright say \u2018superintelligence\u2019 and I shudder to think how badly they will miss the concept if you don\u2019t even say it. Which I think is a lot of the motivation behind not wanting to say it, so people can pretend that there won\u2019t be things smarter than us in any meaningful sense and thus we can stop worrying about it or planning for it.</p>\n<p>Indeed, this is exactly Sriram\u2019s agenda <a href=\"https://x.com/sriramk/status/1996056881551299031/history\">if you look at his post here</a>, to claim \u2018we are not on the timeline\u2019 that involves such things, to dismiss concerns as \u2018sci-fi\u2019 or philosophical, and talk instead of \u2018what we are trying to build.\u2019 What matters is what actually gets built, not what we intended, and no none of these concepts have been invalidated. We have \u2018no proof of takeoff\u2019 in the sense that we are not currently in a fast takeoff yet, but what would constitute this \u2018proof\u2019 other than already being in a takeoff, and thus it being too late to do anything about it?</p>\n<blockquote><p>Sriram Krishnan: \u2026most importantly, it invokes fear\u2014connected to historical usage in sci-fi and philosophy (think 2001, Her, anything invoking the singularity) that has nothing to do with the tech tree we\u2019re actually on. Makes every AI discussion incredibly easy to anthropomorphize and detour into hypotheticals.</p>\n<p>Joshua Achiam (OpenAI Head of Mission Alignment): I mostly disagree but I think this is a good contribution to the discourse. Where I disagree: I do think AGI and ASI both capture something real about where things are going. Where I agree: the lack of agreed-upon definitions has 100% created many needless challenges.</p></blockquote>\n<p>The idea that \u2018hypotheticals,\u2019 as in future capabilities and their logical consequences, are \u2018detours,\u2019 or that any such things are \u2018sci-fi or philosophy\u2019 is to deny the very idea of planning for future capabilities or thinking about the future in real ways. Sriram himself only thinks they are 10 years away, and then the difference is he doesn\u2019t add Dwarkesh\u2019s \u2018and that\u2019s fucking crazy\u2019 and instead seems to effectively say \u2018and that\u2019s a problem for future people, ignore it.\u2019</p>\n<blockquote><p><a href=\"https://x.com/S_OhEigeartaigh/status/1996221947823161640\">Se\u00e1n \u00d3 h\u00c9igeartaigh</a>: I keep noting this, but I do think a lot of the most heated policy debates we\u2019re having are underpinned by a disagreement on scientific view: whether we (i) are on track in coming decade for something in the AGI/ASI space that can achieve scientific feats equivalent to discovering general relativity (Hassabis\u2019 example), or (ii) should expect AI as a normal technology (Narayanan &amp; Kapoor\u2019s definition).</p>\n<p>I honestly don\u2019t know. But it feels premature to me to rule out (i) on the basis of (slightly) lengthening timelines from the believers, when progress is clearly continuing and a historically unprecedented level of resources are going into the pursuit of it. And premature to make policy on the strong expectation of (ii). (I also think it would be premature to make policy on the strong expectation of (i) ).</p>\n<p>But we are coming into the time where policy centred around worldview (ii) will come into tension in various places with the policies worldview (i) advocates would enact if given a free hand. Over the coming decade I hope we can find a way to navigate a path between, rather than swing dramatically based on which worldview is in the ascendancy at a given time.</p>\n<p>Sriram Krishnan: There is truth to this.</p></blockquote>\n<p>This paints it as two views, and I would say you need at least three:</p>\n<ol>\n<li>Something in the AGI/ASI space is likely in less than 10 years.</li>\n<li>Something in the AGI/ASI space is unlikely in less than about 10 years, but highly plausible in 10-20 years, until then AI is a normal technology.</li>\n<li>AI is a normal technology and we know it will remain so indefinitely. We can regulate and plan as if AGI/ASI style technologies will never happen.</li>\n</ol>\n<p>I think #1 and #2 are both highly reasonable positions, only #3 is unreasonable, while noting that if you believe #2 you still need to put some non-trivial weight on #1. As in, if you think it probably takes ~10 years then you can perhaps all but rule out AGI 2027, and you think 2031 is unlikely, but you cannot claim 2031 is a Can\u2019t Happen.</p>\n<p>The conflation to watch out for is #2 and #3. These are very different positions. Yet many in the AI industry, and its political advocates, make exactly this conflation. They assert \u2018#1 is incorrect therefore #3,\u2019 when challenged for details articulate claim #2, then go back to trying to claim #3 and act on the basis of #3.</p>\n<p>What\u2019s craziest is that the list of things to rule out, chosen by Sriram, includes the movie Her. Her made many very good predictions. Her was a key inspiration for ChatGPT and its voice mode, so much so that there was a threatened lawsuit because they all but copied Scarlett Johansson\u2019s voice. She\u2019s happening. Best be believing in sci-fi stores, because you\u2019re living in one, and all that.</p>\n<p>Nothing about current technology is a reason to think 2001-style things or a singularity will not happen, or to think we should anthropomorphize AI relatively less (the correct amount for current AIs, and for future AIs, are both importantly not zero, and importantly not 100%, and both mistakes are frequently made). Indeed, Dwarkesh is de facto predicting a takeoff and a singularity in this post that Sriram praised, except Dwarkesh has it on a 10-20 year timescale to get started.</p>\n<p>Now, back to Dwarkesh.</p>\n<p>This process of \u2018teach the AI the specific tasks people most want\u2019 is the central instance of models being what Teortaxes calls usemaxxed. A lot of effort is going to specific improvements rather than to advancing general intelligence. And yes, this is evidence against extremely short timelines. It is also, as Dwarkesh notes, evidence in favor of large amounts of mundane utility soon, including ability to accelerate R&amp;D. What else would justify such massive \u2018side\u2019 efforts?</p>\n<p>There\u2019s also, as he notes, the efficiency argument. Skills many people want should be baked into the core model. Dwarkesh fires back that there are a lot of skills that are instance-specific and require on-the-job or continual learning, which he\u2019s been emphasizing a lot for a while. I continue to not see a contradiction, or why it would be that hard to store and make available that knowledge as needed even if it\u2019s hard for the LLM to permanently learn it.</p>\n<p>I strongly disagree with his claim that \u2018economic diffusion lag is cope for missing capabilities.\u2019 I agree that many highly valuable capabilities are missing. Some of them are missing due to lack of proper scaffolding or diffusion or context, and are fundamentally Skill Issues by the humans. Others are foundational shortcomings. But the idea that the AIs aren\u2019t up to vastly more tasks than they\u2019re currently asked to do seems obviously wrong?</p>\n<p>He <a href=\"https://www.lesswrong.com/posts/xJWBofhLQjf3KmRgg/four-ways-learning-econ-makes-people-dumber-re-future-ai\">quotes Steven Byrnes</a>:</p>\n<blockquote><p>Steven Byrnes: New technologies take a long time to integrate into the economy? Well ask yourself: how do highly-skilled, experienced, and entrepreneurial immigrant humans manage to integrate into the economy immediately? Once you\u2019ve answered that question, note that AGI will be able to do those things too.</p></blockquote>\n<p>Again, this is saying that AGI will be as strong as humans in the exact place it is currently weakest, and will not require adjustments for us to take advantage. No, it is saying more than that, it is also saying we won\u2019t put various regulatory and legal and cultural barriers in its way, either, not in any way that counts.</p>\n<p>If the AGI Dwarkesh is thinking about were to exist, again, it would be an ASI, and it would be all over for the humans very quickly.</p>\n<p>I also strongly disagree with human labor not being \u2018shleppy to train\u2019 (bonus points, however, for excellent use of \u2018shleppy\u2019). I have trained humans and been a human being trained, and it is totally shleppy. I agree, not as schleppy as current AIs can be when something is out of their wheelhouse, but rather obnoxiously schleppy everywhere except their own very narrow wheelhouse.</p>\n<p>Here\u2019s another example of \u2018oh my lord check out those goalposts\u2019:</p>\n<blockquote><p>Dwarkesh Patel: It revealed a key crux between me and the people who expect transformative economic impacts in the next few years.</p></blockquote>\n<p>Transformative economic impacts in the next few years would be a hell of a thing.</p>\n<blockquote><p>It\u2019s not net-productive to build a custom training pipeline to identify what macrophages look like given the way this particular lab prepares slides, then another for the next lab-specific micro-task, and so on. What you actually need is an AI that can learn from semantic feedback on the job and immediately generalize, the way a human does.</p></blockquote>\n<p>Well, no, it probably isn\u2019t now, but also Claude Code is getting rather excellent at creating training pipelines, and the whole thing is rather standard in that sense, so I\u2019m not convinced we are that far away from doing exactly that. This is an example of how sufficient \u2018AI R&amp;D\u2019 automation, even on a small non-recursive scale, can transform use cases.</p>\n<blockquote><p>Every day, you have to do a hundred things that require judgment, situational awareness, and skills &amp; context learned on the job. These tasks differ not just across different people, but from one day to the next even for the same person. It is not possible to automate even a single job by just baking in some predefined set of skills, let alone all the jobs.</p></blockquote>\n<p>Well, I mean of course it is, for a sufficiently broad set of skills at a sufficiently high level, especially if this includes meta-skills and you can access additional context. Why wouldn\u2019t it be? It certainly can quickly automate large portions of many jobs, and yes I have started to automate portions of my job indirectly (as in Claude writes me the mostly non-AI tools to do it, and adjusts them every time they do something wrong).</p>\n<p>Give it a few more years, though, and Dwarkesh is on the same page as I am:</p>\n<blockquote><p>In fact, I think people are really underestimating how big a deal actual AGI will be because they\u2019re just imagining more of this current regime. They\u2019re not thinking about billions of human-like intelligences on a server which can copy and merge all their learnings. And to be clear, I expect this (aka actual AGI) in the next decade or two. That\u2019s fucking crazy!</p></blockquote>\n<p>Exactly. This \u2018actual AGI\u2019 is fucking crazy, and his timeline for getting there of 10-20 years is also fucking crazy. More people need to add \u2018and that\u2019s fucking crazy\u2019 at the end of such statements.</p>\n<p>Dwarkesh then talks more about continual learning. His position here hasn\u2019t changed, and neither has my reaction that this isn\u2019t needed, we can get the benefits other ways. He says that the gradual progress on continual learning means it won\u2019t be \u2018game set match\u2019 to the first mover, but if this is the final piece of the puzzle then why wouldn\u2019t it be?</p>\n<p>&nbsp;</p>"
            ],
            "link": "https://thezvi.wordpress.com/2025/12/03/on-dwarkesh-patels-second-interview-with-ilya-sutskever/",
            "publishedAt": "2025-12-03",
            "source": "TheZvi",
            "summary": "Some podcasts are self-recommending on the \u2018yep, I\u2019m going to be breaking this one down\u2019 level. This was very clearly one of those. So here we go. Double click to interact with video As usual for podcast posts, the baseline &#8230; <a href=\"https://thezvi.wordpress.com/2025/12/03/on-dwarkesh-patels-second-interview-with-ilya-sutskever/\">Continue reading <span class=\"meta-nav\">&#8594;</span></a>",
            "title": "On Dwarkesh Patel\u2019s Second Interview With Ilya Sutskever"
        },
        {
            "content": [],
            "link": "https://xkcd.com/3176/",
            "publishedAt": "2025-12-03",
            "source": "XKCD",
            "summary": "<img alt=\"Some tires are marketed as 'all-shape tires,' but if driven in a climate with both inverted catenary falls and triangle falls, they wear out really fast.\" src=\"https://imgs.xkcd.com/comics/inverted_catenaries.png\" title=\"Some tires are marketed as 'all-shape tires,' but if driven in a climate with both inverted catenary falls and triangle falls, they wear out really fast.\" />",
            "title": "Inverted Catenaries"
        },
        {
            "content": [],
            "link": "https://zed.dev/blog/rainbow-brackets",
            "publishedAt": "2025-12-03",
            "source": "Zed Blog",
            "summary": "A whole new world of color comes to Zed.",
            "title": "Zed Has Rainbow Brackets"
        }
    ],
    "lookbackDays": 1,
    "publishDate": "2025-12-03"
}