{
    "articles": [
        {
            "content": [
                "<h3 id=\"the-intern-problem\">The Intern Problem</h3> <p>The thing I disliked most about being employed was the lack of freedom to cut corners. I wasn\u2019t looking to slack off, but sometimes tasks required an unreasonable amount of effort for minimal gain. The exact effort involved was rarely clear when tasks were assigned, and I suspected my boss would have preferred me to cut corners had he known. But confirming that would require disturbing him. As an intern (working on a project unlikely ever to reach production), my manager\u2019s time was far more valuable than mine. Thus, I faced a dilemma.</p> <h3 id=\"ai-products\">AI Products</h3> <p>The AI in your AI product faces the same dilemma, but amplified, since the difference in time value between humans and AI is far greater than between interns and managers.</p> <p>As a user, I want the AI to do the task as I would have done it, cutting corners when I would have. For example, I don\u2019t want my coding agent to write 600 lines of code for a small problem that turned out to be harder than I expected (if the task specifications were strict). However, I also don\u2019t want it to cut corners all the time; details are sometimes important.</p> <p>The AI can ask clarifying questions, but <a href=\"https://www.sid.ai/blog/amdahl\">the productivity gain of using the product quickly drops the more I have to engage</a> (I always run Claude Code with \u2013dangerously-skip-permissions). Asking questions at the start, rather than interrupting me later, is less costly. However, I don\u2019t have all the answers at the start.</p> <p>What does success under uncertainty look like?</p> <h3 id=\"your-favorite-coworker\">Your Favorite Coworker</h3> <p>You probably don\u2019t have this problem with your favourite coworker. You understand each other so well that you can collaborate without almost any information exchange (if needed). You know when they want you to cut corners. This is of course because you have exchanged a lot of information in the past. In AI-lingo, your context window is already filled with millions of tokens of their preferences.</p> <p>You also might not have this with your intern. If don\u2019t, you were probably very generous with your valuable time and gave them a lot of context to the task specification. I expect that current AI products struggle with the Intern Problem much more than interns do because intern hosts are more generous with their time than users of AI products are. Intern hosts are confident that the intern will eventually get it, so they spend the required time. Users of AI products on the other hand are sceptical that the AI can do the task. They try a few times, and when it doesn\u2019t work they assume it couldn\u2019t ever work and give up.</p> <h3 id=\"the-road-ahead\">The Road Ahead</h3> <p>AI struggles with the Intern Problem even more than actual interns do. However, as humans gradually build trust in AI systems, they\u2019ll become increasingly willing to invest their valuable time upfront to provide clearer context. This shift alone, even without further AI progress, could bring AI to roughly the same level of efficiency as human interns. At this stage, significant gains in usability will come from UI. Products like Cursor demonstrate how important UI is for today\u2019s AI products.</p> <p>However, AI will progress. As AIs become better at remembering context and asking the right questions to know your preferences, interaction will be like working with your favorite coworker. At this point, UI will matter less. Just as you don\u2019t require complex interfaces to communicate effectively with your your favorite coworker, natural language will be sufficient for interacting AI.</p> <p>Still, as long as a human is part of the task definition process, some form of the Intern Problem will persist, since humans themselves are imperfect managers. We rarely know precisely what we want upfront, and task specifications will inevitably remain incomplete or ambiguous to some degree. However, as AI systems improve, human involvement in detailed specification will decrease, transitioning from micromanagement towards providing broad, high-level objectives. These more abstract goals are typically easier to specify accurately, ultimately reducing the friction inherent in the Intern Problem.</p> <p>Follow me on <a href=\"https://x.com/lukaspet\">X</a> or subscribe via <a href=\"https://lukaspetersson.com/feed.xml\">RSS</a> or <a href=\"https://lukaspet.substack.com/\">Substack</a> to stay updated.</p> <p>Thanks to David Fant, Max Rumpf and Axel Backlund for feedback &lt;3</p>"
            ],
            "link": "https://lukaspetersson.github.io/blog/2025/intern-problem/",
            "publishedAt": "2025-06-16",
            "source": "Lukas Petersson",
            "summary": "<h3 id=\"the-intern-problem\">The Intern Problem</h3> <p>The thing I disliked most about being employed was the lack of freedom to cut corners. I wasn\u2019t looking to slack off, but sometimes tasks required an unreasonable amount of effort for minimal gain. The exact effort involved was rarely clear when tasks were assigned, and I suspected my boss would have preferred me to cut corners had he known. But confirming that would require disturbing him. As an intern (working on a project unlikely ever to reach production), my manager\u2019s time was far more valuable than mine. Thus, I faced a dilemma.</p> <h3 id=\"ai-products\">AI Products</h3> <p>The AI in your AI product faces the same dilemma, but amplified, since the difference in time value between humans and AI is far greater than between interns and managers.</p> <p>As a user, I want the AI to do the task as I would have done it, cutting corners when I would have. For example, I don\u2019t want my coding agent to write 600 lines of code for a small problem that turned out to be harder than I expected (if the task specifications were strict). However, I also don\u2019t want it to cut corners all the time; details are",
            "title": "The Intern Problem"
        },
        {
            "content": [],
            "link": "https://simonwillison.net/2025/Jun/16/the-lethal-trifecta/#atom-entries",
            "publishedAt": "2025-06-16",
            "source": "Simon Willison",
            "summary": "<p>If you are a user of LLM systems that use tools (you can call them \"AI agents\" if you like) it is <em>critically</em> important that you understand the risk of combining tools with the following three characteristics. Failing to understand this <strong>can let an attacker steal your data</strong>.</p> <p>The <strong>lethal trifecta</strong> of capabilities is:</p> <ul> <li> <strong>Access to your private data</strong> - one of the most common purposes of tools in the first place!</li> <li> <strong>Exposure to untrusted content</strong> - any mechanism by which text (or images) controlled by a malicious attacker could become available to your LLM</li> <li> <strong>The ability to externally communicate</strong> in a way that could be used to steal your data (I often call this \"exfiltration\" but I'm not confident that term is widely understood.)</li> </ul> <p>If your agent combines these three features, an attacker can <strong>easily trick it</strong> into accessing your private data and sending it to that attacker.</p> <p><img alt=\"The lethal trifecta (diagram). Three circles: Access to Private Data, Ability to Externally Communicate, Exposure to Untrusted Content.\" src=\"https://static.simonwillison.net/static/2025/lethaltrifecta.jpg\" /></p> <h4 id=\"the-problem-is-that-llms-follow-instructions-in-content\">The problem is that LLMs follow instructions in content</h4> <p>LLMs follow instructions in content. This is what makes them so useful: we can feed",
            "title": "The lethal trifecta for AI agents: private data, untrusted content, and external communication"
        },
        {
            "content": [
                "<p>This is the weekly visible open thread. Post about anything you want, ask random questions, whatever. ACX has an unofficial <a href=\"https://www.reddit.com/r/slatestarcodex/\">subreddit</a>, <a href=\"https://discord.gg/RTKtdut\">Discord</a>, and <a href=\"https://www.datasecretslox.com/index.php\">bulletin board</a>, and <a href=\"https://www.lesswrong.com/community?filters%5B0%5D=SSC\">in-person meetups around the world</a>. Most content is free, some is subscriber only; you can subscribe <strong><a href=\"https://astralcodexten.substack.com/subscribe\">here</a></strong>. Also:</p><p><strong>1: </strong>Thanks to everyone who commented on <a href=\"https://www.astralcodexten.com/p/p-zombies-would-report-qualia\">P-Zombies Would Report Qualia</a>. Although as usual most of your attempts to link it to other philosophical writings were vague and not very relevant, a correspondent was eventually able to find a very similar argument starting on page 289 of David Chalmers&#8217; <a href=\"https://personal.lse.ac.uk/ROBERT49/teaching/ph103/pdf/Chalmers_The_Conscious_Mind.pdf\">The Conscious Mind</a> (which I had not previously read). Thanks to Professor Chalmers for not getting upset about this unintentional duplication of his work. And if you think you have a more original insight about consciousness, Erik Hoel highlights <a href=\"https://www.theintrinsicperspective.com/p/50000-essay-contest-about-consciousness\">a $50,000 consciousness essay contest</a>.</p><p><strong>2: </strong>Thanks to everyone who commented on <a href=\"https://www.astralcodexten.com/p/the-claude-bliss-attractor\">The Claude Bliss Attractor</a>. Two especially good categories of comment - <a href=\"https://www.astralcodexten.com/p/the-claude-bliss-attractor/comment/125388637\">one </a>arguing that the tendency of iterative AI art to produce black people was caused not by a bias towards diversity, but by AI art adding a dark sepia tone to everything - as the figures in the art get darker and darker, the AI interprets them as ethnically African and adds black features to &#8220;match&#8221; (<a href=\"https://www.astralcodexten.com/p/open-thread-386/comment/126352707\">and see this comment by Michael</a>) - and <a href=\"https://substack.com/@boundedlyrational/note/c-125526399?utm_source=activity_item\">another </a>arguing that even though Claude had a slight bias towards being a hippie, it has slight biases towards lots of things (eg coding) and it still requires more explanation for why the hippie spirituality is what eventually shines through.</p><p><strong>3: </strong>New rule: if you post a link in an open thread (as a standalone comment advertising the thing being linked to, not as a source for something you&#8217;re saying), please also include at least two paragraphs of original commentary on the link and discussion of why you think it&#8217;s interesting, as &#8220;proof of work&#8221; that you&#8217;re willing to put effort into promoting this and aren&#8217;t just spamming us with links. Even with this proof-of-work, please try to avoid having more than one Open Thread link per few months.</p><p><strong>4: </strong>Another AI alignment summer workshop in Europe, this one Prague, July 22 - 25, apply <a href=\"https://humanaligned.ai/2025/\">here</a>. </p>"
            ],
            "link": "https://www.astralcodexten.com/p/open-thread-386",
            "publishedAt": "2025-06-16",
            "source": "SlateStarCodex",
            "summary": "<p>This is the weekly visible open thread. Post about anything you want, ask random questions, whatever. ACX has an unofficial <a href=\"https://www.reddit.com/r/slatestarcodex/\">subreddit</a>, <a href=\"https://discord.gg/RTKtdut\">Discord</a>, and <a href=\"https://www.datasecretslox.com/index.php\">bulletin board</a>, and <a href=\"https://www.lesswrong.com/community?filters%5B0%5D=SSC\">in-person meetups around the world</a>. Most content is free, some is subscriber only; you can subscribe <strong><a href=\"https://astralcodexten.substack.com/subscribe\">here</a></strong>. Also:</p><p><strong>1: </strong>Thanks to everyone who commented on <a href=\"https://www.astralcodexten.com/p/p-zombies-would-report-qualia\">P-Zombies Would Report Qualia</a>. Although as usual most of your attempts to link it to other philosophical writings were vague and not very relevant, a correspondent was eventually able to find a very similar argument starting on page 289 of David Chalmers&#8217; <a href=\"https://personal.lse.ac.uk/ROBERT49/teaching/ph103/pdf/Chalmers_The_Conscious_Mind.pdf\">The Conscious Mind</a> (which I had not previously read). Thanks to Professor Chalmers for not getting upset about this unintentional duplication of his work. And if you think you have a more original insight about consciousness, Erik Hoel highlights <a href=\"https://www.theintrinsicperspective.com/p/50000-essay-contest-about-consciousness\">a $50,000 consciousness essay contest</a>.</p><p><strong>2: </strong>Thanks to everyone who commented on <a href=\"https://www.astralcodexten.com/p/the-claude-bliss-attractor\">The Claude Bliss Attractor</a>. Two especially good categories of comment - <a href=\"https://www.astralcodexten.com/p/the-claude-bliss-attractor/comment/125388637\">one </a>arguing that the tendency of iterative AI art to produce black people was caused not by a bias towards diversity, but by AI art adding a dark sepia tone to everything - as the figures in",
            "title": "Open Thread 386"
        },
        {
            "content": [],
            "link": "https://xkcd.com/3103/",
            "publishedAt": "2025-06-16",
            "source": "XKCD",
            "summary": "<img alt=\"Sure, this exoplanet we discovered may seem hostile to life, but our calculations suggest it's actually in the accretion disc's habitable zone.\" src=\"https://imgs.xkcd.com/comics/exoplanet_system.png\" title=\"Sure, this exoplanet we discovered may seem hostile to life, but our calculations suggest it's actually in the accretion disc's habitable zone.\" />",
            "title": "Exoplanet System"
        }
    ],
    "lookbackDays": 1,
    "publishDate": "2025-06-16"
}