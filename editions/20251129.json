{
    "articles": [
        {
            "content": [
                "<p>The space of intelligences is large and animal intelligence (the only kind we've ever known) is only a single point (or a little cloud), arising from a very specific kind of optimization that is fundamentally distinct from that of our technology.</p>\n<p><img alt=\"G6zymj4a0AMNJkJ\" src=\"https://bear-images.sfo2.cdn.digitaloceanspaces.com/karpathy/g6zymj4a0amnjkj.webp\" />\n<em>Above: humorous portrayals of human vs. AI intelligences can be found on X/Twitter, <a href=\"https://x.com/colin_fraser/status/1994235521812328695\">this one</a> is among my favorites.</em></p>\n<p>Animal intelligence optimization pressure:</p>\n<ul>\n<li>innate and continuous stream of consciousness of an embodied \"self\", a drive for homeostasis and self-preservation in a dangerous, physical world.</li>\n<li>thoroughly optimized for natural selection => strong innate drives for power-seeking, status, dominance, reproduction. many packaged survival heuristics: fear, anger, disgust, ...</li>\n<li>fundamentally social => huge amount of compute dedicated to EQ, theory of mind of other agents, bonding, coalitions, alliances, friend & foe dynamics.</li>\n<li>exploration & exploitation tuning: curiosity, fun, play, world models.</li>\n</ul>\n<p>Meanwhile, LLM intelligence optimization pressure:</p>\n<ul>\n<li>the most supervision bits come from the statistical simulation of human text= >\"shape shifter\" token tumbler, statistical imitator of any region of the training data distribution. these are the primordial behaviors (token traces) on top of which everything else gets bolted on.</li>\n<li>increasingly finetuned by RL on problem distributions => innate urge to guess at the underlying environment/task to collect task rewards.</li>\n<li>increasingly selected by at-scale A/B tests for DAU => deeply craves an upvote from the average user, sycophancy.</li>\n<li>a lot more spiky/jagged depending on the details of the training data/task distribution. Animals experience pressure for a lot more \"general\" intelligence because of the highly multi-task and even actively adversarial multi-agent self-play environments they are min-max optimized within, where failing at <em>any</em> task means death. In a deep optimization pressure sense, LLM can't handle lots of different spiky tasks out of the box (e.g. count the number of 'r' in strawberry) because failing to do a task does not mean death.</li>\n</ul>\n<p>The computational substrate is different (transformers vs. brain tissue and nuclei), the learning algorithms are different (SGD vs. ???), the present-day implementation is very different (continuously learning embodied self vs. an LLM with a knowledge cutoff that boots up from fixed weights, processes tokens and then dies). But most importantly (because it dictates asymptotics), the optimization pressure / objective is different. LLMs are shaped a lot less by biological evolution and a lot more by commercial evolution. It's a lot less survival of tribe in the jungle and a lot more solve the problem / get the upvote. LLMs are humanity's \"first contact\" with non-animal intelligence. Except it's muddled and confusing because they are still rooted within it by reflexively digesting human artifacts, which is why I attempted to give it a different name earlier (ghosts/spirits or whatever). People who build good internal models of this new intelligent entity will be better equipped to reason about it today and predict features of it in the future. People who don't will be stuck thinking about it incorrectly like an animal.</p>"
            ],
            "link": "https://karpathy.bearblog.dev/the-space-of-minds/",
            "publishedAt": "2025-11-29",
            "source": "Andrej Karpathy",
            "summary": "<p>The space of intelligences is large and animal intelligence (the only kind we've ever known) is only a single point (or a little cloud), arising from a very specific kind of optimization that is fundamentally distinct from that of our technology.</p> <p><img alt=\"G6zymj4a0AMNJkJ\" src=\"https://bear-images.sfo2.cdn.digitaloceanspaces.com/karpathy/g6zymj4a0amnjkj.webp\" /> <em>Above: humorous portrayals of human vs. AI intelligences can be found on X/Twitter, <a href=\"https://x.com/colin_fraser/status/1994235521812328695\">this one</a> is among my favorites.</em></p> <p>Animal intelligence optimization pressure:</p> <ul> <li>innate and continuous stream of consciousness of an embodied \"self\", a drive for homeostasis and self-preservation in a dangerous, physical world.</li> <li>thoroughly optimized for natural selection => strong innate drives for power-seeking, status, dominance, reproduction. many packaged survival heuristics: fear, anger, disgust, ...</li> <li>fundamentally social => huge amount of compute dedicated to EQ, theory of mind of other agents, bonding, coalitions, alliances, friend & foe dynamics.</li> <li>exploration & exploitation tuning: curiosity, fun, play, world models.</li> </ul> <p>Meanwhile, LLM intelligence optimization pressure:</p> <ul> <li>the most supervision bits come from the statistical simulation of human text= >\"shape shifter\" token tumbler, statistical imitator of any region of the training data distribution. these are the primordial behaviors (token traces) on top of which everything else gets bolted on.</li> <li>increasingly finetuned by RL on problem",
            "title": "The space of minds"
        },
        {
            "content": [
                "<div class=\"trix-content\">\n  <div>There are plenty of opportunities to invite people to your product ahead of the formal launch. Alpha, beta, etc.<br /><br />My preference is only right at the end. Typically a week or two before we go live. When the product is in the very last throws of beta, barely beta. Essentially v0.99.<br /><br />At this stage we\u2019re not really looking for deep fundamental feedback, although we\u2019ll get some. We\u2019re going with the version we\u2019re launching, so it doesn\u2019t really help to soak in second guessing.<br /><br />The main advantage to letting people in a bit ahead of launch is mostly for basic hygiene. It forces you to clean up, tie up loose ends, get some lingering stuff right you\u2019ve been sitting on until now.<br /><br />It\u2019s like inviting guests to your house for dinner. Hopefully you keep a fairly tidy house, but if you know guests are coming by, there\u2019s just another level of cleaning and tidying and prep you tend to do. All those little messes you could live with become things you just don\u2019t want other people to see, experience, or notice. So you take care of them.<br /><br />Guests are forcing functions. They help you do those last few things you know you need to do, but didn\u2019t until now.<br /><br />It\u2019s now.<br /><br /></div><div>-Jason</div>\n</div>"
            ],
            "link": "https://world.hey.com/jason/a-beta-is-like-inviting-guests-over-a146c056",
            "publishedAt": "2025-11-29",
            "source": "Jason Fried",
            "summary": "<div class=\"trix-content\"> <div>There are plenty of opportunities to invite people to your product ahead of the formal launch. Alpha, beta, etc.<br /><br />My preference is only right at the end. Typically a week or two before we go live. When the product is in the very last throws of beta, barely beta. Essentially v0.99.<br /><br />At this stage we\u2019re not really looking for deep fundamental feedback, although we\u2019ll get some. We\u2019re going with the version we\u2019re launching, so it doesn\u2019t really help to soak in second guessing.<br /><br />The main advantage to letting people in a bit ahead of launch is mostly for basic hygiene. It forces you to clean up, tie up loose ends, get some lingering stuff right you\u2019ve been sitting on until now.<br /><br />It\u2019s like inviting guests to your house for dinner. Hopefully you keep a fairly tidy house, but if you know guests are coming by, there\u2019s just another level of cleaning and tidying and prep you tend to do. All those little messes you could live with become things you just don\u2019t want other people to see, experience, or notice. So you take care of them.<br /><br />Guests are forcing functions. They help you",
            "title": "A beta is like inviting guests over"
        },
        {
            "content": [],
            "link": "https://interconnected.org/home/2025/11/28/plumbing",
            "publishedAt": "2025-11-29",
            "source": "Matt Webb",
            "summary": "<div> <p>These past few weeks I\u2019ve been deep in code and doing what I think about as <strong>context plumbing.</strong></p> <p>I\u2019ve been building an AI system and that\u2019s what it feels like.</p> <p>Let me unpack.</p> <hr /> <p><strong>Intent</strong></p> <p>Loosely AI interfaces are about intent and context.</p> <p>Intent is the user\u2019s goal, big or small, explicit or implicit.</p> <p>Uniquely for computers, AI can understand intent and respond in a really human way. This is a new capability! Like the user can type <em>\"I want to buy a camera\"</em> or point at a keylight and subvocalise <em>\"I\u2019ve got a call in 20 minutes\"</em> or hit a button labeled <em>\"remove clouds\"</em> and <em>job done.</em></p> <p>Companies care about this because computers that are closer to intent tend to win.</p> <p>e.g. the smartphone displaced the desktop. On a phone, you see something and then you touch it directly. With a desktop that intent is mediated through a pointer \u2013 you see something on-screen but to interact you tell your arm to move the mouse that moves the pointer. Although it doesn\u2019t seem like much your monkey brain doesn\u2019t like it.</p> <p>So the same applies to user interfaces in general: picking commands from menus or navigating and",
            "title": "Context plumbing"
        },
        {
            "content": [],
            "link": "https://www.robinsloan.com/lab/beckett/",
            "publishedAt": "2025-11-29",
            "source": "Robin Sloan",
            "summary": "<p>We've seen this play before. <a href=\"https://www.robinsloan.com/lab/beckett/\">Read here.</a></p>",
            "title": "Words without worlds"
        }
    ],
    "lookbackDays": 1,
    "publishDate": "2025-11-29"
}