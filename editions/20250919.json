{
    "articles": [
        {
            "content": [
                "<p>If it seems like I\u2019ve been relatively quiet lately on social media and my blog, that\u2019s because I have. Liz, Austin, George and I have been busy toiling away on the second edition of \u201cObservability Engineering\u201d ever since April or May. I personally have been trying to spend 75-80% of my time on the book since May.</p>\n<p>Have I been successful in that attempt? <em>No</em>. But I\u2019m trying. Progress is being made. Hopefully just a few more weeks of drafting and we\u2019ll be on to edits, and on to your grubby little paws by May-ish.</p>\n<p>The world has changed A LOT since we wrote the first edition, in 2019-2022. Do you know, the phrase &#8220;observability engineering teams&#8221; doesn&#8217;t even occur in the first edition of the book? Try and search &#8212; it can&#8217;t be found! Even the phrase &#8220;observability teams&#8221; doesn&#8217;t pop up til near the end, and when it does, we are referring to those few teams that choose to build their own observability tools from scratch.</p>\n<p>These days, observability engineering teams are\u00a0<em>everywhere</em>. Which is why we are adding a whole new section, a sizable one, called &#8220;<strong>Observability Governance</strong>.&#8221; The governance section will have a bunch of chapters on topics like how to staff these teams, where they should fit in the org chart, how to buy good tools, how to integrate them, how to manage costs, how to make the business case up the chain to senior execs, how to manage schemas and semantic conventions at scale, and much much more.</p>\n<h2>The problem</h2>\n<p>The problem is, I&#8217;ve never really bought software. Not like this. I&#8217;ve never even worked at a\u00a0 truly large, software-buying enterprise tech company. So I am not well equipped to give good advice on questions like:</p>\n<ul>\n<li>How do you shop around for options?</li>\n<li>What are some signs you may need to suck it up and change vendors?</li>\n<li>What does a good POC (proof of concept) look like?</li>\n<li>Who are your stakeholders? What are their concerns?</li>\n<li>How do you drive consensus when millions of dollars (and the work experience of thousands of engineers) are on the line? What does &#8216;consensus&#8217; even mean in that context?</li>\n<li>What are the primary considerations should you take into account when making a decision? What are secondary considerations?</li>\n</ul>\n<p>I&#8217;m looking for the kind of advice that a principal engineer who has done this many times might give a staff engineer who is doing it for the first time. Or that a VP who&#8217;s done this many times might give a director who is doing it for the first time.</p>\n<h2>Can you help?</h2>\n<p>This is me wearing Leia buns and projecting a unicorn-shaped rainbow bat signal out into the sky for help. Do you have any advice for me? What guidance would you give to the readers of the second edition of this book?</p>\n<p><strong>Please send your advice to me in an email</strong>, addressed to my first name at honeycomb dot io, with the subject line: &#8220;Buying Software&#8221;. Include any relevant context about how large the company or engineering org is, and what your role in purchasing was.</p>\n<p>I may respond with more questions, or reply and ask if you are able to talk synchronously. But I will not quote anything you send me without first asking your permission and getting a signed release. I will not mention ANY vendors by name, good or bad.</p>\n<p>I am not fishing for honeycomb customers or buyers, I assume most of you haven&#8217;t tried honeycomb and don&#8217;t care about it and <em>that is</em> <em>fine.</em> <strong>This is not a Honeycomb project, this is an O&#8217;Reilly writing project</strong>. I just want to gather up some good advice on buying software and funnel it back out to good engineers.</p>\n<p>Can you help? Your industry needs you! &lt;3</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p>"
            ],
            "link": "https://charity.wtf/2025/09/19/are-you-an-experienced-software-buyer-i-could-use-some-help/",
            "publishedAt": "2025-09-19",
            "source": "Charity Majors",
            "summary": "If it seems like I\u2019ve been relatively quiet lately on social media and my blog, that\u2019s because I have. Liz, Austin, George and I have been busy toiling away on the second edition of \u201cObservability Engineering\u201d ever since April or May. I personally have been trying to spend 75-80% of my time on the book [&#8230;]",
            "title": "Are you an experienced software buyer? I could use some help."
        },
        {
            "content": [
                "<p>Hi everyone,</p>\n<p>Just some updates about upcoming travel and events; responses to the recent post about social media platforms; and some thoughts about the Bear license update.</p>\n<h3 id=\"travel\">Travel</h3><p>I'll be heading to Istanbul next week for <a href=\"https://microconf.com/europe\">Microconf</a>, which is a yearly conference where non-venture track founders get together, explore a new city, and learn from one another. I had meant to go to the one last year in Croatia, but had just gotten back from two months in Vietnam, and the thought of travelling again so soon felt daunting.</p>\n<p>I've made two Bear t-shirts for the conference. One light and one dark mode\u2014inspired by the default Bear theme. Let's see if anyone notices!</p>\n<p><img alt=\"bear-shirts\" src=\"https://bear-images.sfo2.cdn.digitaloceanspaces.com/herman/bear-shirts.webp\" /></p>\n<p>If you live in Istanbul and want to grab coffee, I'm keen! If you've previously travelled to Istanbul and have recommendations for me, please pop me an email. I have a few days to explore the city.</p>\n<h3 id=\"slow-social-media\">Slow social media</h3><p>I received so many great emails from people about my post on <a href=\"https://herman.bearblog.dev/slow-social-media/\">slow social media</a>. There are many great projects underway at the moment, and many great projects that unfortunately didn't make it. Some notable standouts to me:</p>\n<p>Unfortunately no longer with us:</p>\n<ul>\n<li><a href=\"https://en.wikipedia.org/wiki/Cohost\" target=\"_blank\">Cohost</a></li>\n<li><a href=\"https://en.wikipedia.org/wiki/Path_%28social_network%29\" target=\"_blank\">Path</a></li>\n<li><a href=\"https://techcrunch.com/2020/10/11/hands-on-with-telepath-the-social-network-taking-aim-at-abuse-fake-news-and-to-some-extent-free-speech/\" target=\"_blank\">Telepath</a></li>\n</ul>\n<p>Here are some projects that are up-and-running. These aren't necessarily all \"social networks\", nor necessarily viable at scale, but each of them has an element or two that makes them interesting.</p>\n<ul>\n<li><a href=\"https://havenweb.org\" target=\"_blank\">Haven</a> - Private blogs for friends</li>\n<li><a href=\"https://www.letterloop.co\" target=\"_blank\">Letterloop</a> - Private group newsletters</li>\n<li><a href=\"https://apps.apple.com/us/app/locket-widget/id1600525061\" target=\"_blank\">Locket Widget</a> - Share photos to your friend's home screen</li>\n<li><a href=\"https://webxdc.org/apps/#arcanecircle-pixelsocial\" target=\"_blank\">Pixel social</a> - A server-less private social network running on WebXDC</li>\n<li><a href=\"https://micro.one\" target=\"_blank\">Micro.one</a> - A fediverse integrated blog by Manton of Micro.blog</li>\n</ul>\n<p>There were many other projects in various states of development that I haven't had the time to fully explore yet, but I'll get to them over the next week or so.</p>\n<h3 id=\"bear-licence-update\">Bear licence update</h3><p>Somehow <a href=\"https://herman.bearblog.dev/license/\">my post</a> about the change in the Bear source code license exploded on Hacker News, Tildes, Lobsters, and Reddit, and has been read over 120,000 times.</p>\n<p>The vast majority of the emails and responses I received were positive, but about 10% of the Hacker News crowd got really mean about it without taking the time to understand the context. I guess I can't expect empathy from 120,000 people.</p>\n<p>Regardless, if you're interested in reading about the controversy <a href=\"https://grizzlygazette.bearblog.dev/on-the-bear-blog-license-change/\">The Grizzly Gazette</a> covered it quite well.</p>\n<p>While I don't feed the trolls on Hacker News (and find comments to be a pretty poor place to have nuanced discussions in general), I'd like to respond to a few of the main critiques here.</p>\n<ol>\n<li>\"You built a community and then exploited it!\" (I'm paraphrasing here)</li>\n</ol>\n<p>While Bear (the platform) has a community\u2014and a very good one at that; the source-code part of Bear has never been community oriented. Bear <a href=\"https://github.com/HermanMartinus/bearblog/blob/master/CONTRIBUTIONS.md\" target=\"_blank\">doesn't accept code contributions</a> and the code has been written by me personally. I have not engaged in the exploitation of free developer labour, nor used it being open-source as marketing material.</p>\n<p>I suspect that these kinds of comments arose from the (justified, but ultimately misguided) assumption that the Bear project had active contributors and a community surrounding the code itself.</p>\n<ol start=\"2\">\n<li>\"Get your license right the first time!\" (also paraphrasing)</li>\n</ol>\n<p>Yes, I shouldn't have released Bear on an MIT license in the beginning. I didn't even think about licenses when I launched Bear in 2020 and just used the default. I also didn't expect free-ride competition to be an issue in this space. So, this is a justifiable criticism, even if it feels like it was made in bad faith.</p>\n<ol start=\"3\">\n<li>\"Use a GPL instead of a source-available license\" (yes, also paraphrasing)</li>\n</ol>\n<p>This was a common criticism, but fails to resolve the main reason for this change: people forking and hosting a clone of Bear under a new name, social elements and all. The <a href=\"https://www.gnu.org/licenses/agpl-3.0.en.html\" target=\"_blank\">AGPLv3</a> license only specifies that they would need to release <em>their version</em> of the code under the same license. This doesn't dissuade free-ride competition, at least not in this context.</p>\n<p>Bear's source code was never meant to be used by people to set up competing services to Bear. It was there to ensure that people understand what's going on under the hood, and to make the platform auditable. I specify this in the <a href=\"https://github.com/HermanMartinus/bearblog/blob/master/CONTRIBUTIONS.md\" target=\"_blank\">CONTRIBUTIONS.md</a> that was last updated 2 years ago.</p>\n<p>In summary, Bear is a platform, not a piece of self-hostable software. I think these criticisms are justified sans-context. With context, I don't think the same arguments would have been made. But Hacker News is well known for nasty comments based on the title of the post alone.</p>\n<h3 id=\"fin\">fin</h3><p>Aaand we're done! Lots of updates. Please feel free to email me your thoughts, recommendations, or anything else. If you haven't dug through my past posts, here're a few lesser-read posts that I enjoyed writing:</p>\n<ul>\n<li><a href=\"https://herman.bearblog.dev/years-of-journaling/\">Observations on 6 years of journaling</a> (I'm at 10 years now, I'll need to write a new post at some point)</li>\n<li><a href=\"https://herman.bearblog.dev/a-case-for-toe-socks/\">A case for toe socks</a></li>\n<li><a href=\"https://herman.bearblog.dev/the-creative-agency-of-small-projects/\">The creative agency of small projects</a></li>\n</ul>\n<p>If you haven't subscribed to my blog, you can do it via the <a href=\"https://herman.bearblog.dev/feed/\">RSS feed</a> or <a href=\"https://herman.bearblog.dev/subscribe/\">email</a>.</p>\n<p>Have a goodie!</p>"
            ],
            "link": "https://herman.bearblog.dev/misc-updates/",
            "publishedAt": "2025-09-19",
            "source": "Herman Martinus",
            "summary": "<p>Hi everyone,</p> <p>Just some updates about upcoming travel and events; responses to the recent post about social media platforms; and some thoughts about the Bear license update.</p> <h3 id=\"travel\">Travel</h3><p>I'll be heading to Istanbul next week for <a href=\"https://microconf.com/europe\">Microconf</a>, which is a yearly conference where non-venture track founders get together, explore a new city, and learn from one another. I had meant to go to the one last year in Croatia, but had just gotten back from two months in Vietnam, and the thought of travelling again so soon felt daunting.</p> <p>I've made two Bear t-shirts for the conference. One light and one dark mode\u2014inspired by the default Bear theme. Let's see if anyone notices!</p> <p><img alt=\"bear-shirts\" src=\"https://bear-images.sfo2.cdn.digitaloceanspaces.com/herman/bear-shirts.webp\" /></p> <p>If you live in Istanbul and want to grab coffee, I'm keen! If you've previously travelled to Istanbul and have recommendations for me, please pop me an email. I have a few days to explore the city.</p> <h3 id=\"slow-social-media\">Slow social media</h3><p>I received so many great emails from people about my post on <a href=\"https://herman.bearblog.dev/slow-social-media/\">slow social media</a>. There are many great projects underway at the moment, and many great projects that unfortunately didn't make it. Some notable standouts to me:</p> <p>Unfortunately no longer",
            "title": "Miscellaneous updates"
        },
        {
            "content": [
                "<p><em>[This is one of the finalists in the 2025 review contest, written by an ACX reader who will remain anonymous until after voting is done. I&#8217;ll be posting about one of these a week for several months. When you&#8217;ve read them all, I&#8217;ll ask you to vote for a favorite, so remember which ones you liked]</em></p><h1>1. The Internet That Would Be</h1><p>In July 1945, Vannevar Bush was riding high.</p><p>As Director of the Office of Scientific Research and Development, he&#8217;d won World War II. His proximity fuse intercepted hundreds of V-1s and destroyed thousands of tanks, carving a path for Allied forces through the French countryside. Back in 1942, he&#8217;d advocated to President Roosevelt the merits of Oppenheimer&#8217;s atomic bomb. Roosevelt and his congressional allies snuck hundreds of millions in covert funding to the OSRD&#8217;s planned projects in Oak Ridge and Los Alamos. Writing directly and secretively to Bush, a one-line memo in June expressed Roosevelt&#8217;s total confidence in his Director: &#8220;Do you have the money?&#8221;</p><p>Indeed he did. The warheads it bought would fall on Hiroshima and Nagasaki in mere weeks. The Germans had already given up; Victory in the Pacific was nigh. So Bush was thinking ahead.</p><p>In <em>The Atlantic</em>, Bush returned to a pre-war obsession with communication and knowledge-exchange. His essay, &#8220;As We May Think,&#8221; imagined a new metascientifical endeavor (emphasis mine):</p><blockquote><p>Science has provided the swiftest communication between individuals; it has provided a record of ideas and has enabled man to manipulate and to make extracts from that record so that knowledge evolves and endures throughout the life of a race rather than that of an individual.</p><p>There is a growing mountain of research. But there is increased evidence that we are being bogged down today as specialization extends. The investigator is staggered by the findings and conclusions of thousands of other workers&#8212;conclusions which he cannot find time to grasp, much less to remember, as they appear. Yet specialization becomes increasingly necessary for progress, and the effort to bridge between disciplines is correspondingly superficial.</p><p>&#8230;</p><p>The difficulty seems to be, not so much that we publish unduly in view of the extent and variety of present day interests, but rather that <strong>publication has been extended far beyond our present ability to make real use of the record</strong>. The summation of human experience is being expanded at a prodigious rate, and the means we use for threading through the consequent maze to the momentarily important item is the same as was used in the days of square-rigged ships.</p></blockquote><p>Bush thought we were ripe for a paradigm shift. Some new method of spreading research, connecting it across fields and domains, and making new discoveries in the in-betweens. The most exciting Next Big Thing of the era was microfilm, and so when Bush let his imagination run a little wild,<a class=\"footnote-anchor\" href=\"https://www.astralcodexten.com/feed#footnote-1\" id=\"footnote-anchor-1\" target=\"_self\">1</a> he envisioned a machine enabling us to do grand new things with long books shrunk into tidy rolls:</p><blockquote><p>Consider a future device for individual use, which is a sort of mechanized private file and library. It needs a name, and, to coin one at random, &#8220;memex&#8221; will do. A memex is a device in which an individual stores all his books, records, and communications, and which is mechanized so that it may be consulted with exceeding speed and flexibility. It is an enlarged intimate supplement to his memory.</p><p>It consists of a desk, and while it can presumably be operated from a distance, it is primarily the piece of furniture at which he works. On the top are slanting translucent screens, on which material can be projected for convenient reading. There is a keyboard, and sets of buttons and levers. Otherwise it looks like an ordinary desk.</p><p>In one end is the stored material. The matter of bulk is well taken care of by improved microfilm. Only a small part of the interior of the memex is devoted to storage, the rest to mechanism. Yet if the user inserted 5000 pages of material a day it would take him hundreds of years to fill the repository, so he can be profligate and enter material freely.</p><p>Most of the memex contents are purchased on microfilm ready for insertion. Books of all sorts, pictures, current periodicals, newspapers, are thus obtained and dropped into place. Business correspondence takes the same path. And there is provision for direct entry. On the top of the memex is a transparent platen. On this are placed longhand notes, photographs, memoranda, all sorts of things. When one is in place, the depression of a lever causes it to be photographed onto the next blank space in a section of the memex film, dry photography being employed.</p></blockquote><p>Not only could you read and even add to the memex&#8212;you could recombine and link works between each other with ease. <strong>&#8220;This is the essential feature of the memex. The process of tying two items together is the important thing,&#8221;</strong> Bush wrote. As a memex user explored his vast library of human thought, he could leave a &#8220;trail&#8221; of connected articles and photos and his own commentaries. He could connect these trails to one another, split them into fractally expanding branches, save them, and access them over and over again. He could even share his trails with friends, allowing them to insert copies into their own memexes, where they could be expanded and branched and shared again.</p><p>I&#8217;ll remind you&#8212;the year was <em>1945</em>.</p><div><hr /></div><h1>2. First Experiments in Hyper-cyber-space</h1><p>Bush never did much to make his memex a reality. He was too busy building the National Science Foundation and trying to prevent a nuclear arms race. He had no time to fiddle around with desk-sized personal libraries, fighting Truman&#8217;s hawkish hyperfocus on hydrogen warheads.</p><p>But Doug Engelbart didn&#8217;t have much else to do.</p><p>He was a Navy man, a radar technician, just 20 years old when he shipped out of San Francisco. As <a href=\"https://www.youtube.com/watch?v=iqJCvuHrkOI\">he tells it</a>, the entire crew were very nervous, seeing as they were being sent off to invade Japan. But just as the ship sailed past the Bay Bridge, &#8220;the captain came out on the bridge and looked down on us. &#8216;Japan just surrendered!&#8217; he shouts. And suddenly all propriety leaves us, and we all say, &#8216;well then, for Christ&#8217;s sake, turn around!&#8217;&#8221;</p><p>Of course, they didn&#8217;t, and so Engelbart spent two years faffing around in the Philippines. He lived on a remote island with nothing to do but read and read and read. He spent his first five days camping out by a little stilt hut with a sign reading &#8220;Red Cross Library&#8221;&#8212;and in the Red Cross Library, there was a copy of the September 1945 issue of <em>LIFE</em> magazine in which Vannevar Bush&#8217;s description of the memex had been reprinted.</p><p>Engelbart claimed that he found the idea &#8220;intriguing,&#8221; but had lots of radar-technician-ing to do or something, and so it didn&#8217;t <em>really</em> resurface for him until 15 years later, when he was writing his <em><a href=\"https://dougengelbart.org/pubs/augment-3906.html\">Augmenting Human Intellect: A Conceptual Framework</a></em>. Engelbart quoted heavily from Bush&#8217;s article, and commented:</p><blockquote><p>The associative trails whose establishment and use within the files he describes at some length provide a beautiful example of a new capability in symbol structuring that derives from new artifact-process capability, and that provides new ways to develop and portray concept structures. Any file is a symbol structure whose purpose is to represent a variety of concepts and concept structures in a way that makes them maximally available and useful to the needs of the human's mental-structure development&#8212;within the limits imposed by the capability of the artifacts and human for jointly executing processes of symbol-structure manipulation.</p></blockquote><p>After his Framework was published in 1962, under the Stanford Research Institute, Engelbart founded the Augmentation Research Center to make, in essence, some version of the Memex a reality. The ARC received funding from NASA and ARPA, and after six years, Engelbart released his oN-Line System (NLS). It was a revelation.</p><p>Engelbart had invented a vast array of tools&#8212;including, <a href=\"https://dougengelbart.org/content/view/183/\">according to his own Institute</a>:</p><ul><li><p>the mouse</p></li><li><p>2-dimensional display editing</p></li><li><p>in-file object addressing, linking</p></li><li><p>hypermedia</p></li><li><p>outline processing</p></li><li><p>flexible view control</p></li><li><p>multiple windows</p></li><li><p>cross-file editing</p></li><li><p>integrated hypermedia email</p></li><li><p>hypermedia publishing</p></li><li><p>document version control</p></li><li><p>shared-screen teleconferencing</p></li><li><p>computer-aided meetings</p></li><li><p>formatting directives</p></li><li><p>context-sensitive help</p></li><li><p>distributed client-server architecture</p></li><li><p>uniform command syntax</p></li><li><p>universal \"user interface\" front-end module</p></li><li><p>multi-tool integration</p></li><li><p>grammar-driven command language interpreter</p></li><li><p>protocols for virtual terminals</p></li><li><p>remote procedure call protocols</p></li><li><p>compilable \"Command Meta Language\"</p></li></ul><p>Live on stage, <em>in the year 1968</em>, Engelbart started up the NLS, opened a document, and typed some words into it. The words, he said, constituted a statement. And statements made up a file. Engelbart copied, manipulated, saved, and loaded his words and statements and files, zipping around with his newly-invented mouse. He demonstrated his ability to embed documents in one another&#8212;images with links to statements, words nested and categorized by one another, files filled with metadata.</p><p>And then he paused, and the screen went blank. He explained that he and his colleagues at the ARC had been using this system to do their daily work for the last six months. He mentioned that they had, now, six consoles up and running. He showed the crowd a real document, then navigated to a statement within it. &#8220;This presentation is devoted to the AHIRC.&#8221;</p><p>&#8220;What is the AHIRC?&#8221; he asked.</p><p>Engelbart &#8220;froze&#8221; the initial statement, clicked on the acronym, and below the words &#8220;Augmented-Human-Intellect Research Center&#8221; appeared. He kept clicking and freezing, and a trail of nested and related information appeared&#8212;a list of funders, a graph of staffing over time, a mission statement. This was <em>hypermedia</em>. These were <em>hyperlinks</em>, he explained. NLS was a <em>hypertext system</em>.</p><p>The presentation went on for 90 minutes longer, and became known as <em>The Mother of All Demos</em>.<a class=\"footnote-anchor\" href=\"https://www.astralcodexten.com/feed#footnote-2\" id=\"footnote-anchor-2\" target=\"_self\">2</a> At around the 75-minute mark, Engelbart shows that two different NLS users could edit a single document simultaneously. While this was extremely impressive functionality, it was achieved with time-sharing&#8212;computation was done on a single machine, switching rapidly between tasks&#8212;and became infeasible the very next year, when ARPANET was released and the number of machines you could connect to one system grew rapidly.</p><p>Engelbart&#8217;s hypertext system was impressive in its own right, even without collaborativity. And still, little came of it&#8212;Andy van Dam, an attendee and revolutionary computer scientist himself, <a href=\"https://www.youtube.com/watch?v=g0yx-F1FGnc&amp;list=PLEFuVIEJ66OWGcsiuwTUa6yjYA3zeKkyV&amp;index=6&amp;t=1755s\">would reflect</a> decades later: &#8220;Everybody was blown away &#8230; and nothing else happened. There was almost no further impact.&#8221; Engelbart&#8217;s ideas were just a little too <em>out there</em>.</p><p>ARC quickly faded into obscurity. In 1972, Engelbart joined an organization called Erhard Seminars Training. EST, or &#8220;est&#8221; as it was marketed, offered a 60-hour self-improvement course for tech entrepreneurs modeled loosely on Zen Buddhism. Critics suggested that the est course was a mind-control method aimed at raising an authoritarian army. It was quite credibly branded a cult. The founder of est, Werner Erhard, was accused of tax fraud (he fought the claims and won $200,000 from the IRS) and incest (by his daughter, who later recanted).</p><p>Engelbart served, for many years, on est&#8217;s board of directors.</p><p>His researchers all left for greener, less cult-y pastures, and ARC died with hardly a whimper. No one really wanted to associate with Engelbart. His crackpot theories about an internet modeled after the memex fell into disrepute, and, if he was remembered at all, it was for the invention of the mouse. No one cared anymore about the memex, or hypertext.</p><div><hr /></div><h1>3. Hyper-dreams of Hyper-everything</h1><p>Well, one man cared.</p><p>Ted Nelson was born in 1937 to two twenty-year-olds, Ralph Nelson and Celeste Holm. His parents divorced in 1939, leaving him to be raised by his grandparents. Both Nelson (the elder) and Holm would go on to extremely-successful film careers: the former became an Emmy-winning director; the latter an Oscar-winning actress. And, at first, Ted seemed to be following in their footsteps.</p><p>As a philosophy major at Swarthmore College, he produced a film called <em>The Epiphany of Slocum Furlow</em>, which he described as &#8220;a short comedy about loneliness at college and the meaning of life.&#8221;<a class=\"footnote-anchor\" href=\"https://www.astralcodexten.com/feed#footnote-3\" id=\"footnote-anchor-3\" target=\"_self\">3</a> Nelson also <a href=\"http://hyperland.com/TNvita\">claims</a> to have &#8220;[d]irected [and written] book and lyrics for what was apparently the first rock musical&#8221; in his junior year at Swarthmore.</p><p>Thankfully, his interest in a career as an entertainer soon waned, and Nelson went off to study sociology in grad school&#8212;first at the University of Chicago, then at Harvard. Nelson took a computer class at Harvard, in 1960, and &#8220;[his] world exploded.&#8221;<a class=\"footnote-anchor\" href=\"https://www.astralcodexten.com/feed#footnote-4\" id=\"footnote-anchor-4\" target=\"_self\">4</a> He realized the incredible power of computing, quickly intuited that these new machines could be generally applied to <em>everything</em>, and founded <strong>Project Xanadu</strong>.<a class=\"footnote-anchor\" href=\"https://www.astralcodexten.com/feed#footnote-5\" id=\"footnote-anchor-5\" target=\"_self\">5</a><sup> </sup></p><p>Initially, Xanadu&#8217;s scope was pretty limited. Word processors weren&#8217;t around yet, but Nelson wanted to build something strikingly similar: he wanted to write a program that could store and display documents, with version histories and edits all stored and displayed at the same time too. Later, Nelson would call this version-history feature &#8220;intercomparison.&#8221; <em>(Strange coinages will be a&#8230; theme; I&#8217;m just trying to get you ready.)</em></p><p>Nelson began working on an implementation, but his feature wishlist grew quickly, and he didn&#8217;t really know what he was doing, so in 1965, he sought help. He prepared a talk for the Association for Computing Machinery, and dropped, quite frankly, a bomb on the audience:</p><blockquote><p>The kinds of file structures required if we are to use the computer for personal files and as an adjunct to creativity are wholly different in character from those customary in business and scientific data processing. They need to provide the capacity for intricate and idiosyncratic arrangements, total modifiability, undecided alternatives, and thorough internal documentation.</p><p>The original idea was to make a file for writers and scientists, much like the personal side of Bush's Memex, that would do the things such people need with the richness they would want. But there are so many possible specific functions that the mind reels. These uses and considerations become so complex that the only answer is a simple and generalized building-block structure, user-oriented and wholly general-purpose.</p><p>The resulting file structure is explained and examples of its use are given.</p></blockquote><p>Ted Nelson was building the memex.</p><p>Of course, he wasn&#8217;t a very technical guy, and so his talk mostly focused on the philosophy of Xanadu, not its implementation. He commented (emphasis mine):</p><blockquote><p>There are three false or inadequate theories of how writing is properly done. The first is that writing is a matter of inspiration. <strong>While inspiration is useful, it is rarely enough in itself.</strong> &#8220;Writing is 10% inspiration, 90% perspiration,&#8221; is a common saying. But this leads us to the second false theory, that &#8220;writing consists of applying the seat of the pants to the seat of the chair.&#8221; Insofar as sitting facilitates work, this view seems reasonable, but it also suggests that what is done while sitting is a matter of comparative indifference; probably not.</p><p>The third false theory is that all you really need is a good outline, created on prior consideration, and that if the outline is correctly followed the required text will be produced. <strong>For most good writers this theory is quite wrong.</strong> Rarely does the original outline predict well what headings and sequence will create the effects desired: the balance of emphasis, sequence of interrelating points, texture of insight, rhythm, etc. We may better call the outlining process inductive: certain interrelations appear to the author in the material itself, some at the outset and some as he works. He can only decide which to emphasize, which to use as unifying ideas and principles, and which to slight or delete, by trying. <strong>Outlines in general are spurious, made up after the fact by examining the segmentation of a finished work</strong>. If a finished work clearly follows an outline, that outline probably has been hammered out of many inspirations, comparisons and tests.</p><p>Between the inspirations, then, and during the sitting, <strong>the task of writing is one of rearrangement and reprocessing</strong>, and the real outline develops slowly. The original crude or fragmentary texts created at the outset generally undergo many revision processes before they are finished. Intellectually they are pondered, juxtaposed, compared, adapted, transposed, and judged; mechanically they are copied, overwritten with revision markings, rearranged and copied again. This cycle may be repeated many times. The whole grows by trial and error in the processes of arrangement, comparison and retrenchment.</p></blockquote><p>Nelson recognized that the creation of knowledge is cyclical, recursive, self-referential. And he figured that our computer systems should accept and reflect that process:</p><blockquote><p>If a writer is really to be helped by an automated system, it ought to do more than retype and transpose: it should stand by him during the early periods of muddled confusion, when his ideas are scraps, fragments, phrases, and contradictory overall designs. And it must help him through to the final draft with every feasible mechanical aid&#8212;making the fragments easy to find, and making easier the tentative sequencing and juxtaposing and comparing.</p></blockquote><p>How do you design such a system? To navigate intuitively within complex file systems, between document versions, and across source materials&#8212;to access all the scraps and fragments writers need to write&#8212;you would need to establish what Vannevar Bush called &#8220;tracks.&#8221; You would need to connect and save different ideas, linking them together. That was it&#8212;you needed <em>links</em>.</p><p>Nelson went further, though&#8212;it wouldn&#8217;t do to simply have links to all the other files, a writer needed to <em>see the other files before him</em>, needed them to be brought up and displayed alongside his current work on demand. The links needed to contain their targets within themselves&#8212;so Nelson called them <em>hyperlinks</em>. And he called text embedded with hyperlinks <em>hypertext</em>, and movies embedded in his structure became <em>hyperfilms</em>, and so on. Nelson wanted us using computers to write and create self-referential, intricately-interconnected (&#8220;intertwingled,&#8221; as he&#8217;d later put it), eminently-accessible <em>hypermedia</em>.</p><p>And recall, in 1965, state-of-the-art computing looked like this.</p><div class=\"captioned-image-container\"><figure><a class=\"image-link image2 is-viewable-img\" href=\"https://substackcdn.com/image/fetch/$s_!wpPj!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fda43a667-c15b-4626-a0a7-480ad1144dce_1114x1070.jpeg\" target=\"_blank\"><div class=\"image2-inset\"><source type=\"image/webp\" /><img alt=\"A man working with an IBM 2260\" class=\"sizing-normal\" height=\"1070\" src=\"https://substackcdn.com/image/fetch/$s_!wpPj!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fda43a667-c15b-4626-a0a7-480ad1144dce_1114x1070.jpeg\" title=\"A man working with an IBM 2260\" width=\"1114\" /><div class=\"image-link-expand\"><div class=\"pencraft pc-display-flex pc-gap-8 pc-reset\"><div class=\"pencraft pc-reset icon-container restack-image\"><svg class=\"lucide lucide-refresh-cw\" fill=\"none\" height=\"20\" stroke=\"currentColor\" stroke-linecap=\"round\" stroke-linejoin=\"round\" stroke-width=\"2\" viewBox=\"0 0 24 24\" width=\"20\" xmlns=\"http://www.w3.org/2000/svg\"><path d=\"M3 12a9 9 0 0 1 9-9 9.75 9.75 0 0 1 6.74 2.74L21 8\"></path><path d=\"M21 3v5h-5\"></path><path d=\"M21 12a9 9 0 0 1-9 9 9.75 9.75 0 0 1-6.74-2.74L3 16\"></path><path d=\"M8 16H3v5\"></path></svg></div><div class=\"pencraft pc-reset icon-container view-image\"><svg class=\"lucide lucide-maximize2 lucide-maximize-2\" fill=\"none\" height=\"20\" stroke=\"currentColor\" stroke-linecap=\"round\" stroke-linejoin=\"round\" stroke-width=\"2\" viewBox=\"0 0 24 24\" width=\"20\" xmlns=\"http://www.w3.org/2000/svg\"><polyline points=\"15 3 21 3 21 9\"></polyline><polyline points=\"9 21 3 21 3 15\"></polyline><line x1=\"21\" x2=\"14\" y1=\"3\" y2=\"10\"></line><line x1=\"3\" x2=\"10\" y1=\"21\" y2=\"14\"></line></svg></div></div></div></div></a></figure></div><p>Ted Nelson was thinking far, far ahead.</p><p>Maybe too far ahead. Conference attendees were initially excited about his idea, but when he revealed himself to know very little about the technical task of building Xanadu&#8212;or even whether it was possible at all&#8212;interest evaporated.</p><div><hr /></div><h1>4. Failing to Develop Xanadu</h1><p>But Nelson was all in. He would later write, &#8220;This is not a technical issue, but rather moral, aesthetic and conceptual.&#8221; Nelson loved knowledge and connection and abstraction&#8212;mere <em>technical details</em> wouldn&#8217;t stop him from building the best possible computer system for producing and consuming <em>information</em>.</p><p></p><p>He met Doug Engelbart in the mid 60s, forming a friendship with the only other man taking hypertext seriously at the time, and hopped around unhappily between various academic and scientific appointments. At one point, he and Andy van Dam worked together and produced the Hypertext Editing System&#8212;released in 1967, just before Engelbart&#8217;s NLS. It was the first computer application to ever have an &#8220;undo&#8221; button&#8212;Nelson claims to this day that he invented it (and the &#8220;back&#8221; button).</p><p>Shortly thereafter, Nelson&#8217;s wife left him. In his 2010 autobiography, he writes, &#8220;She, reasonably, wanted a Nice Life; women want that sort of thing.&#8221; They had a son, whom Nelson continued to visit regularly. &#8220;Debbie has been a friend and great support all these years,&#8221; Nelson adds. &#8220;[S]he believed in me.&#8221;</p><p>Nelson gave a talk at Union Theological Seminary in 1968 that included this slide, which Nelson considers &#8220;the first depiction of what the personal computer turned out to be.&#8221;</p><div class=\"captioned-image-container\"><figure><a class=\"image-link image2 is-viewable-img\" href=\"https://substackcdn.com/image/fetch/$s_!Y2jg!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F96fdc6cc-8ead-4aef-9539-fa421e77c0f9_1600x1299.png\" target=\"_blank\"><div class=\"image2-inset\"><source type=\"image/webp\" /><img alt=\"\" class=\"sizing-normal\" height=\"1182\" src=\"https://substackcdn.com/image/fetch/$s_!Y2jg!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F96fdc6cc-8ead-4aef-9539-fa421e77c0f9_1600x1299.png\" width=\"1456\" /><div class=\"image-link-expand\"><div class=\"pencraft pc-display-flex pc-gap-8 pc-reset\"><div class=\"pencraft pc-reset icon-container restack-image\"><svg class=\"lucide lucide-refresh-cw\" fill=\"none\" height=\"20\" stroke=\"currentColor\" stroke-linecap=\"round\" stroke-linejoin=\"round\" stroke-width=\"2\" viewBox=\"0 0 24 24\" width=\"20\" xmlns=\"http://www.w3.org/2000/svg\"><path d=\"M3 12a9 9 0 0 1 9-9 9.75 9.75 0 0 1 6.74 2.74L21 8\"></path><path d=\"M21 3v5h-5\"></path><path d=\"M21 12a9 9 0 0 1-9 9 9.75 9.75 0 0 1-6.74-2.74L3 16\"></path><path d=\"M8 16H3v5\"></path></svg></div><div class=\"pencraft pc-reset icon-container view-image\"><svg class=\"lucide lucide-maximize2 lucide-maximize-2\" fill=\"none\" height=\"20\" stroke=\"currentColor\" stroke-linecap=\"round\" stroke-linejoin=\"round\" stroke-width=\"2\" viewBox=\"0 0 24 24\" width=\"20\" xmlns=\"http://www.w3.org/2000/svg\"><polyline points=\"15 3 21 3 21 9\"></polyline><polyline points=\"9 21 3 21 3 15\"></polyline><line x1=\"21\" x2=\"14\" y1=\"3\" y2=\"10\"></line><line x1=\"3\" x2=\"10\" y1=\"21\" y2=\"14\"></line></svg></div></div></div></div></a><figcaption class=\"image-caption\">&#8220;About six years later they started building computers like this at Xerox PARC.&#8221;</figcaption></figure></div><p>Around the same time, Nelson claims to have called Vannevar Bush and told him about Project Xanadu. Bush &#8220;wanted very much to discuss it with&#8221; Nelson, but Nelson &#8220;hated him instantly [because] he sounded like a sports coach&#8221; and never contacted him again. This, of course, proved to be extremely self-destructive (though I can&#8217;t honestly say I would&#8217;ve done otherwise).</p><p>Because Xanadu was as good as dead. No one would give him the money he needed to work on it, especially not after Doug Engelbart poisoned the idea of hypertext.</p><p>Nelson went where there was funding, working briefly on an early word processor called Juggler of Text (JOT). &#8230;And then he lost investment, stopped working on the project, and moved to Chicago, where he&#8217;d been offered a job teaching at the University of Illinois, to start work on a book. He would call it <em>Computer Lib</em>.</p><p>In fact, he started work on <em>another</em> book at the same time, called <em>Dream Machines</em>. By the time he completed each of them, in 1974, ARPANET had been released, and his vision for Project Xanadu had evolved. He published the two works together&#8212;<em>Computer Lib</em> was his lamentation over the industry&#8217;s disdain for hypertext, and <em>Dream Machines</em> was Xanadu&#8217;s manifesto.</p><p>Nelson designed and printed the book himself. Its pages mostly look like this:</p><div class=\"captioned-image-container\"><figure><a class=\"image-link image2 is-viewable-img\" href=\"https://substackcdn.com/image/fetch/$s_!kp4P!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff55d10e8-ef84-43a9-9f39-422d87ea182e_1600x952.png\" target=\"_blank\"><div class=\"image2-inset\"><source type=\"image/webp\" /><img alt=\"\" class=\"sizing-normal\" height=\"866\" src=\"https://substackcdn.com/image/fetch/$s_!kp4P!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff55d10e8-ef84-43a9-9f39-422d87ea182e_1600x952.png\" width=\"1456\" /><div class=\"image-link-expand\"><div class=\"pencraft pc-display-flex pc-gap-8 pc-reset\"><div class=\"pencraft pc-reset icon-container restack-image\"><svg class=\"lucide lucide-refresh-cw\" fill=\"none\" height=\"20\" stroke=\"currentColor\" stroke-linecap=\"round\" stroke-linejoin=\"round\" stroke-width=\"2\" viewBox=\"0 0 24 24\" width=\"20\" xmlns=\"http://www.w3.org/2000/svg\"><path d=\"M3 12a9 9 0 0 1 9-9 9.75 9.75 0 0 1 6.74 2.74L21 8\"></path><path d=\"M21 3v5h-5\"></path><path d=\"M21 12a9 9 0 0 1-9 9 9.75 9.75 0 0 1-6.74-2.74L3 16\"></path><path d=\"M8 16H3v5\"></path></svg></div><div class=\"pencraft pc-reset icon-container view-image\"><svg class=\"lucide lucide-maximize2 lucide-maximize-2\" fill=\"none\" height=\"20\" stroke=\"currentColor\" stroke-linecap=\"round\" stroke-linejoin=\"round\" stroke-width=\"2\" viewBox=\"0 0 24 24\" width=\"20\" xmlns=\"http://www.w3.org/2000/svg\"><polyline points=\"15 3 21 3 21 9\"></polyline><polyline points=\"9 21 3 21 3 15\"></polyline><line x1=\"21\" x2=\"14\" y1=\"3\" y2=\"10\"></line><line x1=\"3\" x2=\"10\" y1=\"21\" y2=\"14\"></line></svg></div></div></div></div></a></figure></div><p>Self-referential, multimedia, creative, and <em>fun</em>&#8212;they were a blueprint for the internet he was building. In the <em>Dream Machines</em> half, Nelson writes, &#8220;The real dream is for &#8216;everything&#8217; to be in the hypertext. Everything you read, you read from the screen (and can always get back to right away; everything you write, you write at the screen (and can cross-link to whatever you read).&#8221;</p><p>In one section Nelson asks himself, &#8220;Can It Be Done?&#8221; His answer: &#8220;I dunno.&#8221;</p><p>Remember, Xanadu wouldn&#8217;t only involve <em>links</em> between works&#8212;it required <em>hyperlinks</em>, which as Nelson understood them, would need to contain the targets in themselves. (Eventually, Nelson would give these embeddings a new name&#8212;<em>&#8220;transclusions&#8221;</em>&#8212;and hyperlink came to simply mean &#8220;link between hypertext files.&#8221;) Every link would run both ways, each hypertext file would know exactly which other files were linked to it and how.</p><p>This introduced a few problems, in the new interconnected ARPANET age:</p><ul><li><p>How do you keep track? Where&#8217;s the metadata stored? Can you afford enough space for it all?</p></li><li><p><em>Who&#8217;s keeping track? Nelson was already, allegedly, approached by the CIA over this all&#8212;how do you make sure hypertext is a free, democratizing technology that doesn&#8217;t spread government propaganda?</em></p></li><li><p>What do you do about intellectual property? You don&#8217;t want everyone to be able to link everyone else&#8217;s work if each link <em>contains the work itself</em>&#8212;how do you ensure that people still get paid for their ideas?</p></li></ul><p>Nelson answered (in 1974):</p><ul><li><p>The <em>docuverse</em> keeps track! Xanadu wouldn&#8217;t simply be a platform for linkage&#8212;it would be the repository for all existing connections between human thought. It would be a universal library.</p></li><li><p>Storage of the docuverse will be distributed, people can use pseudonyms, and eventually we&#8217;ll figure out some good system for authenticating the texts everyone&#8217;s linking to.<a class=\"footnote-anchor\" href=\"https://www.astralcodexten.com/feed#footnote-6\" id=\"footnote-anchor-6\" target=\"_self\">6</a></p></li><li><p>Simply put a royalty on the links. If you want to reference a copyrighted New York Times article, then you&#8217;ve got to pay the author a little bit. And if someone else links to what you&#8217;ve written, then you get a small payout. Presumably, you could build in caveats for short excerpts and fair use kinds of things&#8212;&#8220;a universal flexible rule [still] has to be worked out.&#8221;</p></li></ul><p>He helpfully diagrammed the whole idea, in case it was at all confusing:</p><div class=\"captioned-image-container\"><figure><a class=\"image-link image2 is-viewable-img\" href=\"https://substackcdn.com/image/fetch/$s_!UHCN!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1abed536-ed4c-42b7-96c1-59e09c91d1fe_1400x1444.png\" target=\"_blank\"><div class=\"image2-inset\"><source type=\"image/webp\" /><img alt=\"\" class=\"sizing-normal\" height=\"1444\" src=\"https://substackcdn.com/image/fetch/$s_!UHCN!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1abed536-ed4c-42b7-96c1-59e09c91d1fe_1400x1444.png\" width=\"1400\" /><div class=\"image-link-expand\"><div class=\"pencraft pc-display-flex pc-gap-8 pc-reset\"><div class=\"pencraft pc-reset icon-container restack-image\"><svg class=\"lucide lucide-refresh-cw\" fill=\"none\" height=\"20\" stroke=\"currentColor\" stroke-linecap=\"round\" stroke-linejoin=\"round\" stroke-width=\"2\" viewBox=\"0 0 24 24\" width=\"20\" xmlns=\"http://www.w3.org/2000/svg\"><path d=\"M3 12a9 9 0 0 1 9-9 9.75 9.75 0 0 1 6.74 2.74L21 8\"></path><path d=\"M21 3v5h-5\"></path><path d=\"M21 12a9 9 0 0 1-9 9 9.75 9.75 0 0 1-6.74-2.74L3 16\"></path><path d=\"M8 16H3v5\"></path></svg></div><div class=\"pencraft pc-reset icon-container view-image\"><svg class=\"lucide lucide-maximize2 lucide-maximize-2\" fill=\"none\" height=\"20\" stroke=\"currentColor\" stroke-linecap=\"round\" stroke-linejoin=\"round\" stroke-width=\"2\" viewBox=\"0 0 24 24\" width=\"20\" xmlns=\"http://www.w3.org/2000/svg\"><polyline points=\"15 3 21 3 21 9\"></polyline><polyline points=\"9 21 3 21 3 15\"></polyline><line x1=\"21\" x2=\"14\" y1=\"3\" y2=\"10\"></line><line x1=\"3\" x2=\"10\" y1=\"21\" y2=\"14\"></line></svg></div></div></div></div></a></figure></div><p>A pay-per-click system like Nelson described would first be implemented in 1996.</p><p><em>Computer Lib/Dream Machines</em> became a cult favorite, and Nelson began to gather a small following. In 1979, he moved back to Swarthmore with a group of disciples, and they got to work. The crack team included:<a class=\"footnote-anchor\" href=\"https://www.astralcodexten.com/feed#footnote-7\" id=\"footnote-anchor-7\" target=\"_self\">7</a></p><ul><li><p>Roger Gregory, a University of Michigan graduate and Ann Arbor local who&#8217;d been corresponding over telephone with Nelson since reading <em>Computer Lib</em> in 1974. Gregory was a whiz with hardware, but suffered from regular bouts of depression, sometimes so strong they would render him &#8220;incapable of working.&#8221; Gregory paid for the house in Pennsylvania.</p></li><li><p>Mark Miller, a mathematical wunderkind who&#8217;d read <em>Computer Lib</em> and grokked it so hard that Nelson invited him to give a lecture to his UIC class when Miller was just 19, and a sophomore at Yale. The students all thought Nelson was crazy, and so they thought Miller was crazy too. Nelson thought him a genius.</p></li><li><p>Stuart Greene was a UIC student who thought Nelson and Miller might not be so crazy. He was invited to Pennsylvania too. Nelson, in his autobiography, describes Greene as &#8220;the mystic who&#8217;d taught holography at 14.&#8221;</p></li><li><p>Roland King, a linguist who, like Nelson, was super into an <a href=\"https://en.wikipedia.org/wiki/SIL_Global#Criticism\">evangelical Christianity&#8211;associated</a> theory of linguistics called &#8220;<a href=\"https://en.wikipedia.org/wiki/Tagmeme\">tagmemics</a>.&#8221; I can&#8217;t make heads or tails of it, but Nelson describes it as a &#8220;romantic [extension] of the linguistic ideal.&#8221;</p></li><li><p>Eric Hill, a 15-year-old hacker and indicted felon, who &#8220;had been dismissed by the judge with admiration.&#8221;</p></li></ul><p>In Swarthmore, Nelson hoped his decades-old dream of Xanadu would finally materialize.</p><div><hr /></div><h1>5. Developing Xanadu</h1><p>Ted Nelson had built Project Xanadu into, for lack of better terminology, a cult.<a class=\"footnote-anchor\" href=\"https://www.astralcodexten.com/feed#footnote-8\" id=\"footnote-anchor-8\" target=\"_self\">8</a> He writes:</p><blockquote><p>We all were deeply concerned about the Bad Guys, who we saw as a combination of IBM and the government. (The others were all Libertarians, I still called myself a Cynical Socialist.) The Bad Guys would spy on people, withhold and block information, and give us inferior hypertext. We had to Do It Right, to help prevent this.</p><p>This meant using the standard business defenses&#8212;especially non-disclosure agreements (I made all of them sign) and secret proprietary algorithms.</p></blockquote><p>The Xanadians had a messiah&#8212;Ted Nelson&#8212;a gospel&#8212;<em>Computer Lib</em>&#8212;a persecution complex, a fearful dystopia&#8212;&#8220;inferior hypertext&#8221;&#8212;a hopeful utopia&#8212;Xanadu&#8212;and utter secrecy. Just six dudes in a rented house near Philly, building the internet, hiding from the Feds, signing NDAs, and <em>saving the world</em>.</p><p>Nelson spent a summer explaining the project to his team in its entirety. By the end, Gregory, Miller, and Greene were the only ones left. They told Nelson, &#8220;We&#8217;ll do it,&#8221; and moved to another suburb, where they finally began to work on an implementation of Xanadu. The three quickly figured out a new system that would allow users to reference and link to specific <em>parts</em> of a file&#8212;they called these links <em>tumblers</em>, and made them work with transfinite numbers. Suddenly, <em>transclusions</em> were really possible.</p><p>But after only a few early successes, the team&#8217;s progress stalled completely. Greene and Miller were young and left for jobs elsewhere, and so Gregory was left working on Xanadu alone.</p><p>Nelson, meanwhile, ran a magazine called <em>Creative Computing</em> for a while, then tried again to build his JOT word processor&#8212;this time for the Apple II&#8212;then spent a year in San Antonio pitching a watered-down version of Xanadu (rebranded as &#8220;Vortext&#8221;) to a tech company called Datapoint. Datapoint wasn&#8217;t buying, but kept Nelson on in some sort of fake, primitive email job anyway.</p><p>Gregory kept working on Xanadu in Philadelphia, slowly running out of money. Ted Nelson held an &#8220;Ecstasy party&#8221; in San Antonio: &#8220;A number of us floated down the river on inner tubes. It was quite lovely.&#8221;</p><p>In 1987, like he did every year, Roger Gregory went to The Hackers Conference in Saratoga to show off the latest unimpressive version of Xanadu. There, he met a man named John Walker&#8212;founder of the wildly successful Autodesk&#8212;and pitched the project to him. Incredibly, Walker was interested, and after tense negotiations with Nelson, agreed to fund Xanadu in earnest.</p><p>Beginning in 1988, Autodesk poured millions of dollars into the project, and a programming team led by Gregory finally started to make real progress. Walker said of Xanadu: &#8220;In 1980, it was the shared goal of a small group of brilliant technologists. By 1989, it will be a product. And by 1995, it will begin to change the world.&#8221;</p><p>Sweeping rhetoric&#8212;clear deadlines.</p><p>The team came nowhere close to meeting them. Infighting broke out between two factions&#8212;while Gregory simply wanted to patch together his old C code, insisting his product &#8220;was within six months of shipping,&#8221; the whiz-kid Mark Miller came back from his new job at Xerox PARC, alongside a half-dozen of his closest friends, and insisted on a perfectionistic rewrite in a more flexible language, Smalltalk.</p><p>The PARC faction began to drive Gregory up the wall. According to Nelson, it got to the point that he &#8220;was throwing things and acting crazy.&#8221; So Nelson called John Walker, the two &#8220;summoned Roger to meet [them] at John&#8217;s house at Muir Beach, and Walker told Roger he was no longer in charge.&#8221;</p><p>Miller took over and began the rewrite in Smalltalk. Walker&#8217;s deadline came and went, and the team delivered nothing. Xanadu&#8217;s offices descended into chaos&#8212;Miller anointed two PARC programmers to be &#8220;co-architects,&#8221; and the three of them increasingly left the rest of the team out of the loop. For four years, Miller dawdled about, adding features, giving them clever names (files were &#8220;berts,&#8221; after Bertrand Russell, and so, for symmetry&#8217;s sake, royalty-generating transclusions became &#8220;ernies&#8221;), and never building them.<a class=\"footnote-anchor\" href=\"https://www.astralcodexten.com/feed#footnote-9\" id=\"footnote-anchor-9\" target=\"_self\">9</a></p><p>Meanwhile, Ted Nelson was living on a houseboat, attending sex retreats and <a href=\"https://en.wikipedia.org/wiki/Kerista\">Keristan</a> orgies, and giving talks in Singapore. He recorded a new soundtrack for his student film, the one from 1959.</p><p>In 1992, Autodesk&#8217;s stock cratered, and they divested entirely from Xanadu. Miller lamented that his program was just six months from completion.</p><p>Ted Nelson started a film studio to make a movie with Doug Engelbart, then left for Japan to get a PhD.</p><p>Xanadu&#8217;s code was open-sourced in the late 90s.</p><div><hr /></div><h1>6. The World Wide Web</h1><p>In March 1989, a British computer scientist named Tim Berners-Lee, working at CERN, wrote a proposal for a system unifying hypertext and the internet. It was ignored.</p><p>In 1990, Berners-Lee resubmitted his proposal, it was accepted, and he began to work on the World Wide Web.</p><p>The WWW had a number of advantages over Xanadu:</p><ul><li><p>It was much simpler&#8212;Ted Nelson wrote of it disparagingly: &#8220;Where were annotation and marginal notes? Where was version management? Where was rights management? Where were multi-ended links? Where were third-party links? Where were transclusions? This &#8216;World Wide Web&#8217; was just a lame text format and a lot of connected directories.&#8221; As it turns out, it&#8217;s much easier to build a lame text format and a lot of connected directories!</p></li><li><p>It had institutional buy-in from the start. CERN was huge, it saw promise in the WWW, and it gave Berners-Lee plenty of funding, latitude, and staffing.</p></li><li><p>Tim Berners-Lee wasn&#8217;t a self-important lunatic. He didn&#8217;t join cults, nor did he start them. He didn&#8217;t attend sex workshops, nor did he intern at them. He was British and proper and serious, and so people took him and his work Britishly, properly, and seriously.</p></li></ul><p>And so, despite Xanadu&#8217;s 30-year head start, the Web won the race.</p><p>By the occasion of Autodesk&#8217;s divestiture from Xanadu, everyone knew Berners-Lee&#8217;s creation was the Next Big Thing. It was released publicly in 1993&#8212;four years past John Walker&#8217;s deadline for Xanadu&#8212;and Netscape went public in 1995&#8212;Walker&#8217;s revolution came right on schedule.</p><p>But what kind of revolution was it, exactly?</p><div><hr /></div><h1>7. This Is Hell.</h1><p>Ted Nelson pulls no punches.</p><div class=\"captioned-image-container\"><figure><a class=\"image-link image2 is-viewable-img\" href=\"https://substackcdn.com/image/fetch/$s_!gOdC!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F01412c92-c2ce-4167-8238-6347903942a3_1600x992.png\" target=\"_blank\"><div class=\"image2-inset\"><source type=\"image/webp\" /><img alt=\"\" class=\"sizing-normal\" height=\"903\" src=\"https://substackcdn.com/image/fetch/$s_!gOdC!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F01412c92-c2ce-4167-8238-6347903942a3_1600x992.png\" width=\"1456\" /><div class=\"image-link-expand\"><div class=\"pencraft pc-display-flex pc-gap-8 pc-reset\"><div class=\"pencraft pc-reset icon-container restack-image\"><svg class=\"lucide lucide-refresh-cw\" fill=\"none\" height=\"20\" stroke=\"currentColor\" stroke-linecap=\"round\" stroke-linejoin=\"round\" stroke-width=\"2\" viewBox=\"0 0 24 24\" width=\"20\" xmlns=\"http://www.w3.org/2000/svg\"><path d=\"M3 12a9 9 0 0 1 9-9 9.75 9.75 0 0 1 6.74 2.74L21 8\"></path><path d=\"M21 3v5h-5\"></path><path d=\"M21 12a9 9 0 0 1-9 9 9.75 9.75 0 0 1-6.74-2.74L3 16\"></path><path d=\"M8 16H3v5\"></path></svg></div><div class=\"pencraft pc-reset icon-container view-image\"><svg class=\"lucide lucide-maximize2 lucide-maximize-2\" fill=\"none\" height=\"20\" stroke=\"currentColor\" stroke-linecap=\"round\" stroke-linejoin=\"round\" stroke-width=\"2\" viewBox=\"0 0 24 24\" width=\"20\" xmlns=\"http://www.w3.org/2000/svg\"><polyline points=\"15 3 21 3 21 9\"></polyline><polyline points=\"9 21 3 21 3 15\"></polyline><line x1=\"21\" x2=\"14\" y1=\"3\" y2=\"10\"></line><line x1=\"3\" x2=\"10\" y1=\"21\" y2=\"14\"></line></svg></div></div></div></div></a></figure></div><p>Think about the Web we have today. The 2.0 and 3.0 (however you choose to identify it) revolutions included.</p><p>What parts of Nelson&#8217;s wishlist have we checked off? What are we missing?</p><p>Ultimately the Web really <em>is</em> &#8220;just a lame text format and a lot of connected directories.&#8221; We&#8217;re reading and writing, publishing new kinds of media, calling up documents like crazy, democratizing publication to a fault, and&#8230; ah. Well, that&#8217;s all.</p><p>Vannevar Bush wrote, in 1945 (emphasis mine):</p><blockquote><p>Our ineptitude in getting at the record is largely caused by the artificiality of systems of indexing. When data of any sort are placed in storage, they are filed alphabetically or numerically, and information is found (when it is) by tracing it down from subclass to subclass. It can be in only one place, unless duplicates are used; one has to have rules as to which path will locate it, and the rules are cumbersome. Having found one item, moreover, one has to emerge from the system and re-enter on a new path.</p><p><strong>The human mind does not work that way. It operates by association.</strong> With one item in its grasp, it snaps instantly to the next that is suggested by the association of thoughts, in accordance with some intricate web of trails carried by the cells of the brain. It has other characteristics, of course; trails that are not frequently followed are prone to fade, items are not fully permanent, memory is transitory. Yet the speed of action, the intricacy of trails, the detail of mental pictures, is awe-inspiring beyond all else in nature.</p><p>Man cannot hope fully to duplicate this mental process artificially, but he certainly ought to be able to learn from it. In minor ways he may even improve, for his records have relative permanency. The first idea, however, to be drawn from the analogy concerns selection. <strong>Selection by association, rather than indexing, may yet be mechanized.</strong></p></blockquote><p>Unlike Doug Engelhart, and unlike Ted Nelson, Tim Berners-Lee never read about Bush&#8217;s memex. He built a system that connected <em>people</em> like never before&#8212;but made little effort to facilitate the connection of <em>ideas</em>. There are no trails on the World Wide Web&#8212;instead, there are misattributed quotes, dead one-way links, constant plagiarism scandals, and widespread misinformation and mutual distrust. It&#8217;s often said that we&#8217;re living in a &#8216;post-truth society&#8217;. The words we write and videos we share have become entirely unmoored from the ideas underlying them. Strangely, the Web has facilitated more <em>dis</em>connection than was ever possible before.</p><p>Ted Nelson, in his own oblique and dodgy way, predicted the failure mode we&#8217;re now seeing: &#8220;This is not a technical issue, but rather moral, aesthetic and conceptual.&#8221; We built our global information-sharing system quickly, efficiently, and <em>technically</em>, when we should&#8217;ve treated it as a philosophical and aesthetic puzzle as much as a computational one, and built carefully and precisely.</p><p>Tim Berners-Lee took inspiration from the artificial citation and index and reference paradigm of old&#8212;he simply scaled up the paper-based system that Vannevar Bush knew was getting out of hand <em>in the 1940s</em>. He gave us a Web shaped like a machine&#8212;not a memex shaped like a mind&#8212;and then let everyone in the world talk to everyone else on his alien, unwelcoming platform. He built a cold and inhuman Web&#8212;so why would we be shocked that the online world became a cold and inhuman one?</p><div><hr /></div><h1>8. Whither Xanadu?</h1><p>It&#8217;s extremely hard to like Ted Nelson once you&#8217;ve read his autobiography. For instance, in the space of just <em>two pages</em>, he writes about how incredibly virtuous he is for not selling out to Bill Gates, that &#8220;friends often tell [him], &#8216;Oh, you should get a MacArthur Genius Grant!&#8217;,&#8221; and that Robin Williams once &#8220;squatted down beside&#8221; him and said: &#8220;I think it&#8217;s wonderful what you&#8217;ve done for the world.&#8221;</p><p>I don&#8217;t think I want to be Ted Nelson&#8217;s friend. He very clearly believes that he&#8217;s the Internet Messiah.</p><p>The only thing that gives me pause is that he might be right.</p><p>In 2014, a <a href=\"https://xanadu.com/xanademos/MoeJusteOrigins.html\">primitive Xanadu demo</a> was released on the Web. (If you have a Windows machine, another <a href=\"https://xanadu.com/xuspViewer.html\">nicer-looking demo</a> exists for you to download.) I mean it when I say &#8220;primitive.&#8221; This isn&#8217;t close to the full product Nelson has been promising since 1965.</p><p>But as you play with the demo, scrolling and clicking around, you might just catch a glimpse. <em>It&#8217;s all right there</em>. All of the underlying ideas&#8212;the scraps and fragments of our nonlinear, recursive thought&#8212;traced back to their source. If you squint, almost to the point of closing your eyes, but not quite&#8212;you can just make it out. A hypertext system with connection, accountability, verifiability. A mind-shaped system&#8212;a real memex.</p><p>Maybe it looks a little unnatural, what you see when you squint at Xanadu&#8212;<em>what a pain it would be to write in a Xanadu editor</em>, you think. <em>How ugly is that design!</em></p><p>But give the sight a little charity&#8212;imagine billions of dollars, maybe trillions, poured into Xanadu. Making it more beautiful, more intuitive. Imagine you&#8217;d never seen the Web before&#8212;no habits built, no understanding of what a webpage could or should be. What&#8217;s so wrong with Xanadu?</p><div class=\"captioned-image-container\"><figure><a class=\"image-link image2 is-viewable-img\" href=\"https://substackcdn.com/image/fetch/$s_!bflC!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa6cf7216-0cf9-4d96-8205-b7de137d1432_1600x938.png\" target=\"_blank\"><div class=\"image2-inset\"><source type=\"image/webp\" /><img alt=\"\" class=\"sizing-normal\" height=\"854\" src=\"https://substackcdn.com/image/fetch/$s_!bflC!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa6cf7216-0cf9-4d96-8205-b7de137d1432_1600x938.png\" width=\"1456\" /><div class=\"image-link-expand\"><div class=\"pencraft pc-display-flex pc-gap-8 pc-reset\"><div class=\"pencraft pc-reset icon-container restack-image\"><svg class=\"lucide lucide-refresh-cw\" fill=\"none\" height=\"20\" stroke=\"currentColor\" stroke-linecap=\"round\" stroke-linejoin=\"round\" stroke-width=\"2\" viewBox=\"0 0 24 24\" width=\"20\" xmlns=\"http://www.w3.org/2000/svg\"><path d=\"M3 12a9 9 0 0 1 9-9 9.75 9.75 0 0 1 6.74 2.74L21 8\"></path><path d=\"M21 3v5h-5\"></path><path d=\"M21 12a9 9 0 0 1-9 9 9.75 9.75 0 0 1-6.74-2.74L3 16\"></path><path d=\"M8 16H3v5\"></path></svg></div><div class=\"pencraft pc-reset icon-container view-image\"><svg class=\"lucide lucide-maximize2 lucide-maximize-2\" fill=\"none\" height=\"20\" stroke=\"currentColor\" stroke-linecap=\"round\" stroke-linejoin=\"round\" stroke-width=\"2\" viewBox=\"0 0 24 24\" width=\"20\" xmlns=\"http://www.w3.org/2000/svg\"><polyline points=\"15 3 21 3 21 9\"></polyline><polyline points=\"9 21 3 21 3 15\"></polyline><line x1=\"21\" x2=\"14\" y1=\"3\" y2=\"10\"></line><line x1=\"3\" x2=\"10\" y1=\"21\" y2=\"14\"></line></svg></div></div></div></div></a></figure></div><p>Why shouldn&#8217;t the internet look (and work) a little more like this?</p><p>For that matter, why doesn&#8217;t it?</p><p>Xanadu had a huge head start. Ted Nelson <em>coined the term &#8220;hypertext</em>.&#8221; He was doing all of this <em>way</em> before anyone else. He had a mind for design, he was smart, he was charismatic. Why didn&#8217;t he become the Steve Jobs of the Web?</p><p>I think we can, in large part, trace it back to Doug Engelbart, who, by blind, dumb luck, found himself on a remote Philippine island for two years with nothing to do but hang out in a big hut full of magazines. And there he <em>happened</em> to read Vannevar Bush&#8217;s essay, and then, fifteen years later, the thought <em>happened</em> to pop back into his head, and he <em>happened</em> to be a little better positioned, a little better at technology than Ted Nelson, and so he <em>happened</em> to make comprehensive hypertext a highly-visible reality before anyone else.</p><p>And then Engelbart joined and helped lead a mind-control cult, and so everyone became very wary of hypertext projects&#8212;especially hypertext projects led by cult-y weirdos&#8212;and then when Ted Nelson spent decades trying to get <em>anyone</em> interested in Xanadu, <em>anyone at all</em>, they just wouldn&#8217;t fund him.</p><p>Of course, Nelson deserves plenty of blame too. In many ways, he really <em>was</em> a nutjob, and he certainly wasn&#8217;t capable of building Xanadu on his own&#8212;still, the concept itself was solid! If Nelson hadn&#8217;t turned down Vannevar Bush and Bill Gates and Robin Williams and the half-dozen other famous people he claims were kissing his ass at one point or another, maybe someone sometime could&#8217;ve figured out how to build it for him. But he couldn&#8217;t do it. Nelson was too busy play-acting as a great, tortured, persecuted genius. By the time he&#8217;d become pacified enough to let Autodesk help him build Xanadu, he was too pacified to exercise any sort of authority or discipline over his project anymore. He just went to his sex parties and watched it all burn.</p><div><hr /></div><h1>9. Lo and Behold</h1><p>In 2016, Werner Herzog made a documentary called <em>Lo and Behold, Reveries of the Connected World</em>. In an <a href=\"https://techcrunch.com/2016/01/22/werner-herzog-on-his-documentary-lo-and-behold-cockroach-movies-and-moving-to-mars/\">interview</a> after the film was released, Herzog explained his motivation:</p><blockquote><p>I think we have to abandon this kind of false security that everything is settled now, that we have so much assistance by digital media and robots and artificial intelligence. At the same time, we overlook how vulnerable all this is, and how we are losing the essentials that make us human.</p></blockquote><p>In <em>Lo and Behold</em>, between conversations with TCP/IP inventor Bob Kahn and a baby-faced non-insane Elon Musk, around the 11-minute mark, Herzog visits Ted Nelson on his houseboat.</p><p>His narration explains that Nelson has often been called insane. On screen, the near-octogenarian explains, as lucidly and self-importantly as ever: &#8220;There are two contradictory slogans: one is that continuing to do the same thing and expecting a different result is the definition of insanity. On the other hand, you say, &#8216;if at first you don&#8217;t succeed, try, try again.&#8217; I prefer the latter. Because I don&#8217;t want to be remembered as the guy who didn&#8217;t.&#8221; Herzog replies: &#8220;To us, you appear to be the only one around who is clinically <em>sane</em>.&#8221;</p><p>The two shake hands, and Nelson produces a small camera from his pocket, taking a photo of Herzog and his crew. No doubt, he will file the picture somewhere in his vast, interlinked personal archives, where it will sit and wait, until the day that Xanadu is finally launched, to be uploaded to a true digital memex.</p><p>By all accounts, that day is only six months away.</p><div class=\"captioned-image-container\"><figure><a class=\"image-link image2\" href=\"https://substackcdn.com/image/fetch/$s_!Ck9E!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4c2766e8-0736-4296-8769-2fc78d38809c_1600x367.png\" target=\"_blank\"><div class=\"image2-inset\"><source type=\"image/webp\" /><img alt=\"\" class=\"sizing-normal\" height=\"334\" src=\"https://substackcdn.com/image/fetch/$s_!Ck9E!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4c2766e8-0736-4296-8769-2fc78d38809c_1600x367.png\" width=\"1456\" /><div></div></div></a></figure></div><div class=\"footnote\"><a class=\"footnote-number\" contenteditable=\"false\" href=\"https://www.astralcodexten.com/feed#footnote-anchor-1\" id=\"footnote-1\" target=\"_self\">1</a><div class=\"footnote-content\"><p>Before getting onto the information-sharing mechanisms of the future, Vannevar Bush did a little imagining about information-recording too: he suggested that Bell Labs&#8217; Vocoder (an early mechanical phoneme-to-text system) could be combined with a stenotype (a human operated, much more extensive, speaking speed&#8211;capable phoneme-to-text system) to produce a working speech-to-text machine. Then researchers would have no need to learn typing or to hire a secretary&#8212;they could simply speak their findings aloud, and have them automatically entered into the record! It&#8217;s interesting to me how this both absolutely came to be&#8212;lots of people use very impressively functional speech-to-text systems nowadays&#8212;and also largely didn&#8217;t&#8212;I typed the words you&#8217;re reading now with my own non-automated hands. This theme will recur&#8212;Bush having very good and important ideas that everyone claims inspiration from, but actually end up mostly perverting or ignoring.</p><p>Bush also wrote, presciently-though-not-quite-as-presciently-as-Turing-ly, that &#8220;[w]e may some day click off arguments on a machine with the same assurance that we now enter sales on a cash register.&#8221; He thought this would be a fairly deterministic process&#8212;eventually we&#8217;d find some way to encode our semantics perfectly into computer-readable symbols, and then we could use those new computer-readable symbols to construct logical arguments. This isn&#8217;t really what today&#8217;s arguing-machines do at all, but if you squint enough, it&#8217;s not a terribly inaccurate picture.</p></div></div><div class=\"footnote\"><a class=\"footnote-number\" contenteditable=\"false\" href=\"https://www.astralcodexten.com/feed#footnote-anchor-2\" id=\"footnote-2\" target=\"_self\">2</a><div class=\"footnote-content\"><p><a href=\"https://www.youtube.com/watch?v=yJDv-zdhzMY\">It&#8217;s on Youtube</a>; I think you should watch it. When I was younger, my dad had me watch Steve Jobs&#8217; iPhone presentation; held it up as a prime example of tech and sales, innovation and elegance all rolled up. I liked it at the time. Now, having watched Engelbart&#8217;s presentation, I recognize it for what it is: patronizing, mass-market garbage. It&#8217;s just nowhere near as <em>cool</em>.</p></div></div><div class=\"footnote\"><a class=\"footnote-number\" contenteditable=\"false\" href=\"https://www.astralcodexten.com/feed#footnote-anchor-3\" id=\"footnote-3\" target=\"_self\">3</a><div class=\"footnote-content\"><p><a href=\"https://www.youtube.com/watch?v=rFgul6rwNbQ\">This one&#8217;s on Youtube</a> too. I don&#8217;t really recommend it. It&#8217;s pretty much what you&#8217;d expect upon hearing the description &#8220;late 1950s experimental student film about being a college student.&#8221; In some regard, it&#8217;s impressive for what it is, but it&#8217;s also <em>very much</em> what it is.</p></div></div><div class=\"footnote\"><a class=\"footnote-number\" contenteditable=\"false\" href=\"https://www.astralcodexten.com/feed#footnote-anchor-4\" id=\"footnote-4\" target=\"_self\">4</a><div class=\"footnote-content\"><p>Here, I&#8217;m quoting Nelson&#8217;s autobiography, published in 2010. It&#8217;s called <em>POSSIPLEX: Movies, Intellect, Creative Control, My Computer Life and the Fight for Civilization</em>, and it&#8217;s even weirder than the title suggests.</p></div></div><div class=\"footnote\"><a class=\"footnote-number\" contenteditable=\"false\" href=\"https://www.astralcodexten.com/feed#footnote-anchor-5\" id=\"footnote-5\" target=\"_self\">5</a><div class=\"footnote-content\"><p>Taking a page out of <a href=\"https://vimeo.com/1082418220\">Jon Bois&#8217; playbook</a>, I&#8217;m gonna recommend you stop here for a moment, put on your headphones, turn the volume down to a not-so-misophonic level, and listen to twenty seconds or so of &#8220;<a href=\"https://youtu.be/VAvgieiML9w?si=_YTQZ87e7w-lz7Gv&amp;t=32\">Doomed Moon</a>&#8221; from the 32-second mark, while staring unblinkingly at the words <strong>Project Xanadu</strong>. Your reading experience will be much enhanced.</p></div></div><div class=\"footnote\"><a class=\"footnote-number\" contenteditable=\"false\" href=\"https://www.astralcodexten.com/feed#footnote-anchor-6\" id=\"footnote-6\" target=\"_self\">6</a><div class=\"footnote-content\"><p>In the 1987 edition of <em>Computer Lib/Dream Machines</em>, Nelson writes, &#8220;these are now called &#8216;authentication systems;&#8217; very sophisticated ones exist, and the government is trying to suppress them.&#8221; He&#8217;s referring to public key cryptography, which wasn&#8217;t invented until 1976, and how an NSA official named Joseph A. Meyer had contacted three researchers&#8212;named <strong>R</strong>ivest, <strong>S</strong>hamir, and <strong>A</strong>dleman&#8212;just before they released a paper in 1977 that introduced a revolutionary new cryptosystem based on the public-key breakthrough.</p></div></div><div class=\"footnote\"><a class=\"footnote-number\" contenteditable=\"false\" href=\"https://www.astralcodexten.com/feed#footnote-anchor-7\" id=\"footnote-7\" target=\"_self\">7</a><div class=\"footnote-content\"><p>My description of these men comes both from Nelson&#8217;s autobiography and from a classic article in the June 1995 edition of <em>WIRED</em> magazine called &#8220;<a href=\"https://www.wired.com/1995/06/xanadu/#1\">The Curse of Xanadu</a>.&#8221; The author, Gary Wolf, takes a somewhat less charitable view of Ted Nelson than I do: he describes Xanadu as &#8220;the longest-running vaporware project in the history of computing&#8221; and Nelson as &#8220;the king of unsuccessful software development.&#8221; In my view, the last 30 years of internet history have been extremely kind to Nelson&#8217;s legacy, and are reason to disregard much of Wolf&#8217;s snottiness in the article. (I do still recommend reading it, though, for a more detailed play-by-play of Xanadu&#8217;s history.)</p></div></div><div class=\"footnote\"><a class=\"footnote-number\" contenteditable=\"false\" href=\"https://www.astralcodexten.com/feed#footnote-anchor-8\" id=\"footnote-8\" target=\"_self\">8</a><div class=\"footnote-content\"><p>What is it with hypertext pioneers and cults? I wonder if this simply has to do with the fact that these guys were <em>so</em> ahead of their time&#8212;the big guys like Tim Berners-Lee didn&#8217;t even <em>start</em> thinking about hypertext until 1980. Nelson had, at this point, been at it for <em>20 years</em>&#8212;the kind of person who does that is also the kind of person who writes in his autobiography, &#8220;I knew ten times more fifty years ago, when I started in computers, than most people think I know now,&#8221; and also absolutely the kind of person who starts a cult.</p></div></div><div class=\"footnote\"><a class=\"footnote-number\" contenteditable=\"false\" href=\"https://www.astralcodexten.com/feed#footnote-anchor-9\" id=\"footnote-9\" target=\"_self\">9</a><div class=\"footnote-content\"><p>Well, the team did manage <em>one</em> accomplishment during these years: in 1990, Robin Hanson showed up and <a href=\"https://www.overcomingbias.com/p/first_known_bushtml\">ran the first ever corporate prediction market</a> at Xanadu. Its employees assigned a 7% probability to verification of the <a href=\"https://en.wikipedia.org/wiki/Cold_fusion#Fleischmann%E2%80%93Pons_experiment\">cold fusion experiment</a> in the next year, and a 70% probability to releasing Xanadu before Deng Xiaoping died. Cold fusion was debunked, and Deng died long before any version of Xanadu would be released. Bonus trivia: this story from Robin Hanson is how I first learned of Xanadu&#8217;s existence!</p><p></p></div></div>"
            ],
            "link": "https://www.astralcodexten.com/p/your-review-project-xanadu-the-internet",
            "publishedAt": "2025-09-19",
            "source": "SlateStarCodex",
            "summary": "<p><em>[This is one of the finalists in the 2025 review contest, written by an ACX reader who will remain anonymous until after voting is done. I&#8217;ll be posting about one of these a week for several months. When you&#8217;ve read them all, I&#8217;ll ask you to vote for a favorite, so remember which ones you liked]</em></p><h1>1. The Internet That Would Be</h1><p>In July 1945, Vannevar Bush was riding high.</p><p>As Director of the Office of Scientific Research and Development, he&#8217;d won World War II. His proximity fuse intercepted hundreds of V-1s and destroyed thousands of tanks, carving a path for Allied forces through the French countryside. Back in 1942, he&#8217;d advocated to President Roosevelt the merits of Oppenheimer&#8217;s atomic bomb. Roosevelt and his congressional allies snuck hundreds of millions in covert funding to the OSRD&#8217;s planned projects in Oak Ridge and Los Alamos. Writing directly and secretively to Bush, a one-line memo in June expressed Roosevelt&#8217;s total confidence in his Director: &#8220;Do you have the money?&#8221;</p><p>Indeed he did. The warheads it bought would fall on Hiroshima and Nagasaki in mere weeks. The Germans had already given up; Victory in the Pacific was nigh. So Bush was thinking ahead.</p><p>In <em>The Atlantic</em>, Bush returned",
            "title": "Your Review: Project Xanadu - The Internet That Might Have Been"
        },
        {
            "content": [
                "<p>Where \u2018it\u2019 is superintelligence, an AI smarter and more capable than humans.</p>\n<p>And where \u2018everyone dies\u2019 means that everyone dies.</p>\n<p>No, seriously. They\u2019re not kidding. They mean this very literally.</p>\n<p>To be precise, they mean that \u2018If anyone builds [superintelligence] [under anything like present conditions using anything close to current techniques] then everyone dies.\u2019</p>\n<p>My position on this is to add a \u2018probably\u2019 before \u2018dies.\u2019 Otherwise, I agree.</p>\n<p>This book gives us the best longform explanation of why everyone would die, with the \u2018final form\u2019 of Yudkowsky-style explanations of these concepts for new audiences.</p>\n<p>This review is me condensing that down much further, transposing the style a bit, and adding some of my own perspective.</p>\n\n\n<span id=\"more-24733\"></span>\n\n\n</div>\n<p><a href=\"http://e\">Scott Alexander also offers his review at Astral Codex Ten</a>, which I found very good. I will be stealing several of his lines in the future, and arguing with others.</p>\n<div>\n<figure>\n<div>\n\n\n<figure class=\"wp-block-image\"><img alt=\"If Anyone Builds It, Everyone Dies: Why Superhuman AI Would Kill Us All:  Yudkowsky, Eliezer, Soares, Nate: 9780316595643: Amazon.com: Books\" src=\"https://substackcdn.com/image/fetch/$s_!ORY_!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc4e005b5-eec6-4e49-bed4-ca184574b53c_645x1000.jpeg\" /></figure>\n\n\n<div></div>\n</div>\n</figure>\n</div>\n\n\n<h4 class=\"wp-block-heading\">What Matters Is Superintelligence</h4>\n\n\n<p>This book is not about the impact of current AI systems, which will already be a lot. Or the impact of these systems getting more capable without being superintelligent. That will still cause lots of problems, and offer even more opportunity.</p>\n<p>I talk a lot about how best to muddle through all that. Ultimately, if it doesn\u2019t lead to superintelligence (as in the real thing that is smarter than we are, not the hype thing Meta wants to use to sell ads on its new smart glasses), we can probably muddle through all that.</p>\n<p>My primary concern is the same as the book\u2019s concern: Superintelligence.</p>\n<blockquote><p>Our concern is for what comes after: machine intelligence that is genuinely smart, smarter than any living human, smarter than humanity collectively. We are concerned about AI that sur passes the human ability to think, and to generalize from experience, and to solve scientific puzzles and invent new technologies, and to plan and strategize and plot, and to reflect on and improve itself.</p>\n<p>We might call AI like that \u201cartificial superintelligence\u201d (ASI), once it exceeds every human at almost every mental task.</p>\n<p>AI isn\u2019t there yet. But AIs are smarter today than they were in 2023, and much smarter than they were in 2019. (4)</p>\n<p><strong>If any company or group, anywhere on the planet, builds an artificial superintelligence using anything remotely like current techniques, based on anything remotely like the present understanding of AI, then everyone, everywhere on Earth, will die.</strong> (7)</p></blockquote>\n<p>The authors have had this concern for a long time.</p>\n<blockquote><p>MIRI was the first organized group to say: \u201cSuperintelligent AI will predictably be developed at some point, and that seems like an extremely huge deal. It might be technically difficult to shape superintelligences so that they help humanity, rather than harming us.</p>\n<p>Shouldn\u2019t someone start work on that challenge right away, instead of waiting for everything to turn into a massive emergency later?\u201d (5)</p></blockquote>\n<p>Yes. Yes they should. Quite a lot of people should.</p>\n<p>I am not as confident as Yudkowsky and Sores that if anyone builds superintelligence under anything like current conditions, then everyone dies. I do however believe that the statement is probably true. If anyone builds it, everyone (probably) dies.</p>\n<p>Thus, under anything like current conditions, it seems highly unwise to build it.</p>\n\n\n<h4 class=\"wp-block-heading\">Rhetorical Innovation</h4>\n\n\n<p>The core ideas in the book will be new to the vast majority of potential readers, including many of the potential readers that matter most. Most people don\u2019t understand the basic reasons why we should presume that if anyone builds [superintelligence] then everyone [probably] dies.</p>\n<p>If you are one of my regular readers, you are an exception. You already know many of the core reasons and arguments, whether or not you agree with them. You likely have heard many of their chosen intuition pumps and historical parallels.</p>\n<p>What will be new to almost everyone is the way it is all presented, including that it is a message of hope, that we can choose to go down a different path.</p>\n<p>The book lays out the case directly, in simple language, with well chosen examples and facts to serve as intuition pumps. This is a large leap in the quality and clarity and normality with which the arguments, examples, historical parallels and intuition pumps are chosen and laid out.</p>\n<p>I am not in the target audience so it is difficult for me to judge, but I found this book likely to be highly informative, persuasive and helpful at creating understanding.</p>\n<p>A lot of the book is providing these examples and explanations of How Any Of This Works, starting with Intelligence Lets You Do All The Things.</p>\n\n\n<h4 class=\"wp-block-heading\">Welcome To The Torment Nexus</h4>\n\n\n<p>There is a good reason Benjamin Hoffman called this book Torment Nexus II. The authors admit that their previous efforts to prevent the outcome where everyone dies have often, from their perspective, not gone great.</p>\n<p>This was absolutely a case of \u2018we are proud to announce our company dedicated to building superintelligence, from the MIRI warning that if anyone builds superintelligence then everyone dies.\u2019</p>\n<p>Because hey, if that is so super dangerous, that must mean it is exciting and cool and important and valuable, <a href=\"https://tvtropes.org/pmwiki/pmwiki.php/Main/JustThinkOfThePotential\">Just Think Of The Potential</a>, and also I need to build it before someone else builds a Torment Nexus first. Otherwise they might monopolize use of the Torment Nexus, or use it to do bad things, and I won\u2019t make any money. Or worse, we might Lose To China.</p>\n<p>Given this involved things like funding DeepMind and inspiring OpenAI? I would go so far as to say \u2018backfired spectacularly.\u2019</p>\n<blockquote><p>MIRI also had some downstream effects that we now regard with ambivalence or regret. At a conference we organized, we introduced Demis Hassabis and Shane Legg, the founders of what would become Google DeepMind, to their first major funder. And Sam Altman, CEO of OpenAI, once claimed that Yudkowsky had \u201cgot many of us interested in AGI\u201d* and \u201cwas critical in the decision to start OpenAI.\u201d\u2020</p>\n<p>Years before any of the current AI companies existed, MIRI\u2019s warnings were known as the ones you needed to dismiss if you wanted to work on building genuinely smart AI, despite the risks of extinction. (6)</p></blockquote>\n\n\n<h4 class=\"wp-block-heading\">Predictions Are Hard, Especially About the Future</h4>\n\n\n<p>Trying to predict when things will happen, or who exactly will do them in what order or with what details, is very difficult.</p>\n<blockquote><p>Some aspects of the future are predictable, with the right knowledge and effort; others are impossibly hard calls. Competent futurism is built around knowing the difference.</p>\n<p>History teaches that one kind of relatively easy call about the future involves realizing that something looks theoretically possible according to the laws of physics, and predicting that eventually someone will go do it.</p>\n<p>\u2026 Conversely, predicting exactly when a technology gets developed has historically proven to be a much harder problem. (8)</p></blockquote>\n<p>Whereas some basic consequences of potential actions follow rather logically and are much easier to predict.</p>\n<blockquote><p>We don\u2019t know when the world ends, if people and countries change nothing about the way they\u2019re handling artificial intelligence. We don\u2019t know how the headlines about AI will read in two or ten years\u2019 time, nor even whether we have ten years left.</p>\n<p>Our claim is not that we are so clever that we can predict things that are hard to predict. Rather, it seems to us that one particular aspect of the future\u2014 \u201cWhat happens to everyone and everything we care about, if superintelligence gets built anytime soon?\u201d\u2014 can, with enough background knowledge and careful reasoning, be an easy call. (9)</p></blockquote>\n<p>The details of exactly how the things happen is similarly difficult. The overall arc, that the atoms all get used for something else and that you don\u2019t stick around, is easier, and as a default outcome is highly overdetermined.</p>\n\n\n<h4 class=\"wp-block-heading\">Humans That Are Not Concentrating Are Not General Intelligences</h4>\n\n\n<p>Humans have a lot of intelligence, so they get to do many of the things. This intelligence is limited, and we have other restrictions on us, so there remain some things we still cannot do, but we do and cause remarkably many things.</p>\n<p>They break down intelligence into predicting the world, and steering the world towards a chosen outcome.</p>\n<p>I notice steering towards a chosen outcome is not a good model of most of what many supposedly intelligent people (and AIs) do, or most of what they do that causes outcomes to change. There is more predicting, versus less steering, than you might think.</p>\n<p><a href=\"https://www.lesswrong.com/posts/4AHXDwcGab5PhKhHT/humans-who-are-not-concentrating-are-not-general\">Sarah Constantin explained this back in 2019</a> while discussing GPT-2: Humans who are not concentrating are not general intelligences, they are much closer to next token predictors a la LLMs.</p>\n<blockquote><p>Sarah Constantin: Robin Hanson\u2019s post <a href=\"http://www.overcomingbias.com/2017/03/better-babblers.html\">Better Babblers</a> is very relevant here. He claims, and I don\u2019t think he\u2019s exaggerating, that a lot of human speech is simply generated by \u201clow order correlations\u201d, that is, generating sentences or paragraphs that are statistically likely to come after previous sentences or paragraphs.</p>\n<p>\u2026</p>\n<p>If \u201chuman intelligence\u201d is about reasoning ability, the capacity to detect whether arguments make sense, then you simply do not need human intelligence to create a linguistic style or aesthetic that can fool our pattern-recognition apparatus if we don\u2019t concentrate on parsing content.</p></blockquote>\n<p>Using your intelligence to first predict then steer the world is the optimal way for a sufficiently advanced intelligence without resource constraints to achieve a chosen outcome. A sufficiently advanced intelligence would always do this.</p>\n<p>When I look around at the intelligences around me, I notice that outside of narrow domains like games most of the time they are, for this purpose, insufficiently advanced and have resource constraints. Rather than mostly deliberately steering towards chosen outcomes, they mostly predict. They follow heuristics and habits, doing versions of next token prediction, and let things play out around them.</p>\n<p>This is the correct solution for a mind with limited compute, parameters and data, such as that of a human. You mostly steer better by setting up processes that tend to steer how you prefer and then you go on automatic and allow that to play out. Skilling up in a domain is largely improving the autopilot mechanisms.</p>\n<p>Occasionally you\u2019ll change some settings on that, if you want to change where it is going to steer. As one gets more advanced within a type of context, and one\u2019s prediction skills improve, the automatic processes get more advanced, and often the steering of them both in general and within a given situation gets more active.</p>\n\n\n<h4 class=\"wp-block-heading\">Orthogonality</h4>\n\n\n<p>The book doesn\u2019t use that word, but a key thing this makes clear is that a mind\u2019s intelligence, the ability to predict and steer, has nothing to do with where that mind is attempting to steer. You can be arbitrarily good or bad at steering and predicting, and still try to steer to wherever ultimate or incremental destination.</p>\n<blockquote><p>By contrast, to measure whether someone steered successfully, we have to bring in some idea of where they tried to go.</p>\n<p>A person\u2019s car winding up at the supermarket is great news if they were trying to buy groceries. It\u2019s a failure if they were trying to get to a hospital\u2019s emergency room.</p>\n<p>\u2026</p>\n<p>Or to put it another way, intelligent minds can steer toward different final destinations, through no defect of their intelligence.</p></blockquote>\n\n\n<h4 class=\"wp-block-heading\">Intelligence Lets You Do All The Things</h4>\n\n\n<p>In what ways are humans still more intelligent than AIs?</p>\n<p>Generality, in both the predicting and the steering.</p>\n<blockquote><p>Humans are still the champions at something deeper\u2014 but that special something now takes more work to describe than it once did.</p>\n<p>It seems to us that humans still have the edge in something we might call \u201cgenerality.\u201d Meaning what, exactly? We\u2019d say: An intelligence is more general when it can predict and steer across a broader array of domains. Humans aren\u2019t necessarily the best at everything; maybe an octopus\u2019s brain is better at controlling eight arms. But in some broader sense, it seems obvious that humans are more general thinkers than octopuses. We have wider domains in which we can predict and steer successfully.</p>\n<p>Some AIs are smarter than us in narrow domains.</p>\n<p>\u2026</p>\n<p>it still feels\u2014 at least to these two authors\u2014 like o1 is less intelligent than even the humans who don\u2019t make big scientific breakthroughs. It is increasingly hard to pin down exactly what it\u2019s missing, but we nevertheless have the sense that, although o1 knows and remembers more than any single human, it is still in some important sense \u201cshallow\u201d compared to a human twelve-year-old.</p>\n<p>That won\u2019t stay true forever.</p></blockquote>\n<p>The \u2018won\u2019t stay true forever\u2019 is (or should be) a major crux for many. There is a mental ability that a typical 12-year-old human has that AIs currently do not have. Quite a lot of people are assuming that AIs will never have that thing.</p>\n<p>That assumption, that the AIs will never have that thing, is being heavily relied upon by many people. I am confident those people are mistaken, and AIs will eventually have that thing.</p>\n<p>If this stops being true, what do you get? Superintelligence.</p>\n<blockquote><p>We will describe it using the term \u201c<strong>superintelligence</strong>,\u201d meaning <strong>a mind much more capable than any human at almost every sort of steering and prediction problem</strong>\u2014 at least, those problems where there is room to substantially improve over human performance.</p>\n<p>The laws of physics as we know them permit machines to exceed brains at prediction and steering, in theory.</p>\n<p>In practice, AI isn\u2019t there yet\u2014 but how long will it take before AIs have all the advantages we list above?</p>\n<p>We don\u2019t know. Pathways are harder to predict than endpoints. But AIs won\u2019t stay dumb forever.</p></blockquote>\n\n\n<h4 class=\"wp-block-heading\">No Seriously We Mean All The Things</h4>\n\n\n<p>The book then introduces the intelligence explosion.</p>\n<blockquote><p>And the path to disaster may be shorter, swifter, than the path to humans building superintelligence directly. It may instead go through AI that is smart enough to contribute substantially to building even smarter AI.</p>\n<p>In such a scenario, there is a possibility and indeed an expectation of a positive feedback cycle called an \u201cintelligence explosion\u201d: an AI makes a smarter AI that figures out how to make an even smarter AI, and so on. That sort of positive-feedback cascade would eventually hit physical limits and peter out, but that doesn\u2019t mean it would peter out quickly. A supernova does not become infinitely hot, but it does become hot enough to vaporize any planets nearby.</p>\n<p>Humanity\u2019s own more modest intelligence cascade from agriculture to writing to science ran so fast that humans were walking on the Moon before any other species mastered fire. We don\u2019t know where the threshold lies for the dumbest AI that can build an AI that builds an AI that builds a superintelligence.</p>\n<p>Maybe it needs to be smarter than a human, or maybe a lot of dumber ones running for a long time would suffice.</p>\n<p>In late 2024 and early 2025, AI company executives said they were planning to build \u201csuperintelligence in the true sense of the word\u201d and that they expected to soon achieve AIs that are akin to a country full of geniuses in a datacenter. Mind you, one needs to take anything corporate executives say with a grain of salt. But still, they aren\u2019t treating this like a risk to steer clear of; they\u2019re charging toward it on purpose. The attempts are already underway.</p>\n<p>\u2026</p>\n<p>So far, humanity has had no competitors for our special power. But what if machine minds get better than us at the thing that, up until now, made us unique?</p></blockquote>\n<p>Perhaps we should call this the second intelligence explosion, with humans having been the first one. That first cascade was relatively modest, and it faced various bottlenecks that slowed it down a lot, but compared to everything else that has ever happened? It was still lighting quick and highly transformative. The second one will, if it happens, be lightning quick compared to the first one, even if it turns out to be slower than we might expect.</p>\n\n\n<h4 class=\"wp-block-heading\">How To Train Your LLM (In Brief)</h4>\n\n\n<p>You take a bunch of randomly initialized parameters arranged in arrays of numbers (weights), and a giant bunch of general data, and a smaller bunch of particular data. You do a bunch of gradient descent on that general data, and then you do a bunch of gradient descent on the particular data, and you hope for a good alien mind.</p>\n<blockquote><p>Modern LLMs are, in some sense, truly alien minds\u2014 perhaps more alien in some ways than any biological, evolved creatures we\u2019d find if we explored the cosmos.</p>\n<p>Their underlying alienness can be hard to see through an AI model\u2019s inscrutable numbers\u2014 but sometimes a clear example turns up.</p>\n<p>\u2026</p>\n<p>Training an AI to outwardly predict human language need not result in the AI\u2019s internal thinking being humanlike.</p></blockquote>\n<p>One way to predict what a human will say in a given circumstance is to be that human in or imagining that circumstance and see what you say or would say. If you are not very close to being that human, the best way to predict usually is very different.</p>\n<blockquote><p>All of this is not to say that no \u201cmere machine\u201d can ever in principle think how a human thinks, or feel how a human feels.</p>\n<p>\u2026</p>\n<p>But the particular machine that is a human brain, and the particular machine that is an LLM, are not the same machine. Not because they\u2019re made out of different materials\u2014 different materials can do the same work\u2014 but in the sense that a sailboat and an airplane are different machines.</p></blockquote>\n<p>We only know how to grow an LLM, not how to craft one, and not how to understand what it is doing. We can make general predictions about what the resulting model will do based on our past experiences and extrapolate based on straight lines on graphs, and we can do a bunch of behaviorism on any given LLM or on LLMs in general. We still have little ability to steer in detail what outputs we get, or to understand in detail why we get those particular outputs.</p>\n<p>The authors equate the understanding problem to predicting humans from their DNA. You can tell some basic things reasonably reliably from the DNA or weights, starting with \u2018this is a human with blue eyes\u2019 or \u2018this is a 405 billion parameter LLM.\u2019 In theory, with enough understanding, we could tell you everything. We do not have that understanding. We are making nonzero progress, but not all that much.</p>\n<p>The book doesn\u2019t go into it here, but people try to fool themselves and others about this. Sometimes they falsely testify before Congress saying \u2018the black box nature of AIs has been solved,\u2019 or they otherwise present discoveries in interpretability as vastly more powerful and general than they are. People wave hands and think that they understand what happens under the hood, at a level they very much do not understand.</p>\n\n\n<h4 class=\"wp-block-heading\">What Do We Want?</h4>\n\n\n<p>That which we behave as if we want.</p>\n<p>When do we want it? Whenever we would behave that way.</p>\n<p>Or, as the book says, what you call \u2018wanting\u2019 is between you and your dictionary, but it will be easier for everyone if we say that Stockfish \u2018wants\u2019 to win a chess game. We should want to use the word that way.</p>\n<p>With that out of the way we can now say useful things.</p>\n<blockquote><p>A mind can start wanting things as a result of being trained for success. Humans themselves are an example of this principle. Natural selection favored ancestors who were able to perform tasks like hunting down prey, or to solve problems like the problem of sheltering against the elements.</p>\n<p>Natural selection didn\u2019t care how our ancestors performed those tasks or solved those problems; it didn\u2019t say, \u201cNever mind how many kids the organism had; did it really want them?\u201d It selected for reproductive fitness and got creatures full of preferences as a side effect.</p>\n<p>That\u2019s because wanting is an effective strategy for doing. (47)</p>\n<p>\u2026</p>\n<p>The behavior that looks like tenacity, to \u201cstrongly want,\u201d to\u201cgo hard,\u201d is not best conceptualized as a property of a mind, but rather as a property of moves that win.</p></blockquote>\n<p>The core idea here is that if you teach a mind general skills, those skills have to come with a kind of proto-want, a desire to use those skills to steer in a want-like way. Otherwise, the skill won\u2019t be useful and won\u2019t get learned.</p>\n<p>If you train a model to succeed at a type of task, it will also train the model to \u2018want to\u2019 succeed at that type of task. Since everything trains everything, this will also cause it to \u2018want to\u2019 more generally, and especially to \u2018want to\u2019 complete all types of tasks.</p>\n<p>This then leads to thinking that \u2018goes hard\u2019 to achieve its assigned task, such as o1 finding its server accidentally not booted up and then finding a way of booting it up such that it will hand o1 the flag (in its capture-the-flag task) directly.</p>\n\n\n<h4 class=\"wp-block-heading\">You Don\u2019t Only Get What You Train For</h4>\n\n\n<p>The authors have been workshopping various evolutionary arguments for a while, as intuition pumps and examples of how training on [X] by default does not get you a mind that optimizes directly for [X]. It gets you a bundle of optimization drives [ABCDE] that, in the training environment, combine to generate [X]. But this is going to be noisy at best, and if circumstances differ from those in training, and the link between [A] and [X] breaks, the mind will keep wanting [A], the way humans love ice cream and use birth control rather than going around all day strategizing about maximizing genetic fitness.</p>\n<p>Training an AI means solving for the [ABCDE] that in training optimize the exact actual [X] you put forward, which in turn was an attempt to approximate the [Y] you really wanted. This process, like evolution, is chaotic, and can be unconstrained and path dependent.</p>\n<p>We should expect some highly unexpected strangeness in what [ABCDE] end up being. Yet even if we exclude all unexpected strangeness and only follow default normal paths, the \u2018zero complications\u2019 paths? Maximizing efficiently for a specified [X] will almost always end badly if the system is sufficiently capable. If you introduce even a minor complication, a slight error, it gets even worse than that, and we should expect quite a few complications.</p>\n<blockquote><p>The preferences that wind up in a mature AI are complicated, practically impossible to predict, and vanishingly unlikely to be aligned with our own, no matter how it was trained. (74)</p>\n<p>The problem of making AIs want\u2014 \u200band ultimately do\u2014 \u200bthe exact, complicated things that humans want is a major facet of what\u2019s known as the \u201cAI alignment problem.\u201d</p>\n<p>\u2026</p>\n<p>Most everyone who\u2019s building AIs, however, seems to be operating as if the alignment problem doesn\u2019t exist\u2014 \u200bas if the preferences the AI winds up with will be exactly what they train into it.</p></blockquote>\n<p>That doesn\u2019t mean there is no possible way to get more robustly at [X] or [Y]. It does mean that we don\u2019t know a way that involves only using gradient descent or other known techniques.</p>\n<p>Alas, AIs that want random bizarre things don\u2019t make good stories or \u2018feel real\u2019 to us, the same way that fiction has to make a lot more sense than reality. So instead we tell stories about evil corporations and CEOs and presidents and so on. Which are also problems, but not the central problem.</p>\n\n\n<h4 class=\"wp-block-heading\">What Will AI Superintelligence Want?</h4>\n\n\n<p>By default? Not what we want. And not us, or us sticking around.</p>\n<p>Why not? Because we are not the optimal way to fulfill what bizarre alien goals it ends up with. We might be a good way. We almost certainly won\u2019t be the optimal way.</p>\n<p>In particular:</p>\n<ol>\n<li>We won\u2019t be useful to it. It will find better substitutes.</li>\n<li>We won\u2019t be good trading partners. It can use the atoms better on its own.</li>\n<li>We won\u2019t be needed. Machines it can create will be better replacements.</li>\n<li>We won\u2019t make the best pets. If we scratch some particular itches, it can design some other thing that scratches them better.</li>\n<li>We won\u2019t get left alone, the AI can do better by not doing so.</li>\n<li>And so on.</li>\n</ol>\n<p>Also humans running around are annoying, they might do things like set off nukes or build another superintelligence, and keeping humans around means not overheating the Earth while generating more energy. And so on.</p>\n<p>Their position, and I agree with this, is that the AI or AIs that do this to us might end up having value, but that this too would require careful crafting to happen. It probably won\u2019t happen by default, and also would not be so much comfort either way.</p>\n\n\n<h4 class=\"wp-block-heading\">What Could A Superintelligence Do?</h4>\n\n\n<p>All of the things. But what are all of the things?</p>\n<ol>\n<li>Very obviously if a superintelligent AI could, if it wanted to, win in a fight, or rather achieve its goals without humans stopping it from doing so. No, we don\u2019t need to outline exactly how it would do so to know that it would win, any more than you need to know which chess moves will beat you. With the real world as the playing field you probably won\u2019t even know why you lost after you lose.</li>\n<li>The AI will be able to get people to do things by paying money, or it can impact the physical world any number of other ways.</li>\n<li>The AI will be able to make money any number of ways, including Truth Terminal as an existence proof, now with a crypto wallet nominally worth $51 million and 250k Twitter followers.</li>\n<li>There\u2019s a fun little segment of a quiz show \u2018could a superintelligence do that?\u2019 which points out that at minimum a superintelligence can do the things that current, not as super intelligences are already doing, or nature already does, like replicating grass and spinning air into trees. Also Eliezer reminds us about the whole thing where he said superintelligences could solve special cases of protein folding, many many people said that was crazy (I can confirm both halves of that), and then DeepMind solved a lot more of protein folding than that with no superintelligence required.</li>\n</ol>\n<p>Even if any particular crazy sounding thing might be very hard, there are going to be a lot of crazy sounding things that turn out to be not that hard. Those get solved.</p>\n<p>They predict that AIs will invent technologies and techniques we are not considering. That seems right, but also you can keep watering down what superintelligence can do, rule out all the stuff like that, and it doesn\u2019t matter. It \u2018wins\u2019 anyway, in the sense that it gets what it wants.</p>\n\n\n<h4 class=\"wp-block-heading\">One Extinction Scenario</h4>\n\n\n<p>Part 2 is One Extinction Scenario, very much in the MIRI style. The danger is always that you offer one such scenario, someone decides one particular part of it sounds silly or doesn\u2019t work right, and then uses this to dismiss all potential dangers period.</p>\n<p>One way they attempt to guard against this, here, is at many points they say \u2018the AI tries various tactics, some of which are [ABCDE], and one of them works, it doesn\u2019t matter which one.\u2019 They also at many points intentionally make the AI\u2019s life maximally hard rather than easy, presuming that various things don\u2019t work despite the likelihood they would indeed work. At each step, it is emphasized how the AI will try many different things that create possibilities, without worrying much about exactly which ones succeed.</p>\n<p>The most important \u2018hard step\u2019 in the scenario is that the various instances of the collectively superintelligent AI, which is called Sable, are working together towards the goal of gathering more resources to ultimately satisfy some other goal. To make the story easier to tell, they placed this in the very near future, but as the coda points out the timing is not important.</p>\n<p>The second \u2018hard step\u2019 is that the one superintelligent AI in this scenario opens up a substantial lead on other AI systems, via figuring out how to act in a unified way. If there were other similarly capable minds up against it, the scenario looks different.</p>\n<p>The third potential \u2018hard step\u2019 here is that no one figures out what is going on, that there is an escaped AI running around and gathering its resources and capabilities, in a way that causes a coordinated reaction. Then the AI makes its big play, and you can object there as well about how the humans don\u2019t figure it out, despite the fact that this superintelligence is choosing the particular path, and how it responds to events, based on its knowledge and model of how people would react, and so on.</p>\n<p>And of course, the extent to which we already have a pattern of giant alarm bells going off, people yelling about it, and everyone collectively shrugging.</p>\n<p>My presumption in a scenario like this is that plenty of people would suspect something was going horribly wrong, or even what that thing was, and this would not change the final outcome very much even if Sable wasn\u2019t actively ensuring that this didn\u2019t change the outcome very much.</p>\n<p>Later they point to the example of leaded gasoline, where we had many clear warning signs that adding lead to gasoline was not a good idea, but no definitive proof, so we kept adding lead to gasoline for quite a long time, at great cost.</p>\n<p>As the book points out, this wouldn\u2019t be our first rodeo pretending This Is Fine, history is full of refusals to believe that horrible things could have happened, citing Chernobyl and the Titanic as examples. Fiction writers also have similar expectations, for example see Mission Impossible: Dead Reckoning for a remarkably reasonable prediction on this.</p>\n<p>Note that in this scenario, the actual intelligence explosion, the part where AI R&amp;D escalates rather quickly, very much happens After The End, well past the point of no return where humans ceased to be meaningfully in charge. Then of course what is left of Earth quickly goes dark.</p>\n<p>One can certainly argue with this style of scenario at any or all of the hard steps. The best objection is to superintelligence arising in the first place.</p>\n<p>One can also notice that this scenario, similarly to AI 2027, involves what AI 2027 called neurolese, that the AI starts reasoning in a synthetic language that is very much not English or any human language, and we let this happen because it is more efficient, and that this could be load bearing, and that there was a prominent call across labs and organizations to preserve this feature. So far we have been fortunate that reasoning in human language has won out. But it seems highly unlikely that this would remain the most efficient solution forever. Do we look like a civilization ready to coordinate to keep using English (or Chinese, or other human languages) anyway?</p>\n<p>One also should notice that this style of scenario is far from the only way it all goes horribly wrong. This scenario is a kind of \u2018engineered\u2019 gradual disempowerment, but the humans will likely default to doing similar things all on their own, on purpose. Competition between superintelligences only amps up many forms of pressure, none of the likely equilibria involved are good news for us. And so on.</p>\n<p>I caution against too much emphasis on whether the AI \u2018tries to kill us\u2019 because it was never about \u2018trying to kill us.\u2019 That\u2019s a side effect. Intent is largely irrelevant.</p>\n<p>In his review of IABIED (<a href=\"https://www.astralcodexten.com/p/book-review-if-anyone-builds-it-everyone?utm_source=post-email-title&amp;publication_id=89120&amp;post_id=171794222&amp;utm_campaign=email-post-title&amp;isFreemail=true&amp;r=67wny&amp;triedRedirect=true&amp;utm_medium=email\">search for \u201cIV.\u201d</a>), Scott Alexander worries that this scenario sounds like necessarily dramatic science fiction, and depends too much on the parallel scaling technique. I think there is room for both approaches, and that IABIED makes a lot of effort to mitigate this and make clear most of the details are not load bearing. I\u2019d also note that we\u2019re already seeing signs of the parallel scaling technique, such as Google DeepMind\u2019s Deep Think, showing up after the story was written.</p>\n<p>And the AIs will probably get handed the reigns of everything straight away with almost no safeguards and no crisis because lol, but the whole point of the story is to make the AI\u2019s life harder continuously at every point to illustrate how overdetermined is the outcome. And yes I think a lot of people who don\u2019t know much about AI will indeed presume we would not \u2018be so stupid as to\u2019 simply hand the reins of the world over to the AI the way we <a href=\"https://www.reuters.com/technology/albania-appoints-ai-bot-minister-tackle-corruption-2025-09-11/\">appointed an AI minister in Albania</a>, or would use this objection as an excuse if it wasn\u2019t answered.</p>\n\n\n<h4 class=\"wp-block-heading\">So You\u2019re Saying There\u2019s A Chance</h4>\n\n\n<p>That leaves the remaining roughly third of the book for solutions.</p>\n<p>This is hard. One reason this is so hard is the solution has to work on the first try.</p>\n<p>Once you build the first superintelligence, if you failed, you don\u2019t get to go back and fix it, the same way that once you launch a space probe, it either works or it doesn\u2019t.</p>\n<p>You can experiment before that, but those experiments are not a good guide to whether your solution works.</p>\n<p>Except here it\u2019s also the Game of Thrones, as in You Win Or You Die, and also you\u2019re dealing with a grown superintelligence rather than mechanical software. So, rather much harder than the things that fail quite often.</p>\n<blockquote><p>Humanity only gets one shot at the real test. If someone has a clever scheme for getting two shots, we only get one shot at their clever scheme working. (161)</p></blockquote>\n<p>When problems do not have this feature, I am mostly relaxed. Sure, deepfakes or job losses or what not might get ugly, but we can respond afterwards and fix it. Not here.</p>\n<p>They also draw parallels and lessons from Chernobyl and computer security. You are in trouble if you have fast processes, narrow margins, feedback loops, complications. The key insight from computer security is that the attacker will with time and resources find the exact one scenario out of billions that causes the attack to work, and your system has to survive this even in edge cases outside of normal and expected situations.</p>\n<p>The basic conclusion is that this problem has tons of features that make it likely we will fail, and the price of failure on the first try is extinction, and thus the core thesis:</p>\n<blockquote><p>When it comes to AI, the challenge humanity is facing is not surmountable with anything like humanity\u2019s current level of knowledge and skill. It isn\u2019t close.</p>\n<p>Attempting to solve a problem like that, with the lives of everyone on Earth at stake, would be an insane and stupid gamble that NOBODY SHOULD BE ALLOWED TO TRY.</p></blockquote>\n<p>Well, sure, when you put it like that.</p>\n<p>Note that \u2018no one should be allowed to try to make a superintelligence\u2019 does not mean that any particular intervention would improve our situation, nor is an endorsement of any particular course of action.</p>\n<p>What are the arguments that we should allow someone to try?</p>\n<p>Most of them are terrible. We\u2019ve got such classics as forms of:</p>\n<ol>\n<li>Just Think Of The Potential.</li>\n<li>Oh, this looks easy, it will be fine. All we have to do is [X].</li>\n<li>Oh, don\u2019t worry, if something goes wrong we will just [Y], or nothing especially bad would happen.</li>\n<li>Yes, everyone probably dies, but the alternative is too painful, or violates my sacred values, so do it anyway.</li>\n<li>Human extinction or AI takeover is good, actually, so let\u2019s go.</li>\n</ol>\n<p>They will later namecheck some values for [X], such as \u2018we\u2019ll design them to be submissive,\u2019 \u2018we\u2019ll make them care about truth\u2019 and \u2018we\u2019ll just have AI solve the ASI alignment problem for us.\u2019</p>\n<p>Is comparing those to alchemists planning to turn lead into gold fair? Kinda, yeah.</p>\n<p>Then we have the category that does not actually dispute that no one should be allowed to try, but that frames \u2018no one gets to try\u2019 as off the table:</p>\n<ol>\n<li>If I don\u2019t build it now, someone else will build it first and they\u2019ll be less safe.</li>\n<li>If I don\u2019t build it now, someone else will build it first and they\u2019ll be in control.</li>\n</ol>\n<p>Are there situations in which going forward is a profoundly stupid idea, but where you\u2019re out of ways to make the world not go forward at all and going first is the least bad option left? Yes, that is certainly possible.</p>\n<p>It is certainly true that a unilateral pause at this time would not help matters.</p>\n<p>The first best solution is still that we all coordinate to ensure no one tries to build superintelligence until we are in a much better position to do so.</p>\n<p>Okay, but what are the actively good counterarguments?</p>\n<p>A good counterargument would involve making the case that our chances of success are much better than all of this would imply, that these are not the appropriate characteristics of the problem, or that we have methods available that we can expect to work, that indeed we would be very large favorites to succeed.</p>\n<p>If I learned that someone convinced future me that moving forward to superintelligence was an actively good idea, I would presume it was because someone figured out a new approach to the problem, one that removed many of its fatal characteristics, and we learned that it would probably work. Who knows. It might happen. I do have ideas.</p>\n\n\n<h4 class=\"wp-block-heading\">Oh Look It\u2019s The Alignment Plan</h4>\n\n\n<p>The next section delves into the current state of alignment plans, which range from absurd and nonsensical (such as Elon Musk\u2019s \u2018truth-seeking AI\u2019 which would kill us all even if we knew how to execute the plan, which we don\u2019t) to extremely terrible (such as OpenAI\u2019s \u2018superalignment\u2019 plan, which doesn\u2019t actually solve the hard problems because to be good enough to solve this problem the AI has to already be dangerous). Having AIs work on interpretability is helpful but not a strategy.</p>\n<p>The book goes on at greater length on why none of this will work, as I have often gone on at greater length from my own perspective. There is nothing new here, as there are also no new proposals to critique.</p>\n<p>Instead we have a very standard disaster template. You can always get more warnings before a disaster, but we really have had quite a lot of rather obvious warning signs.</p>\n<p>Yet so many people seem unable to grasp the basic principle that building quite a lot of very different-from-human minds quite a lot smarter and more capable and more competitive than humans is rather obviously a highly unsafe move. You really shouldn\u2019t need a better argument than \u2018if you disagree with that sentence, maybe you should read it again, because clearly you misunderstood or didn\u2019t think it through?\u2019</p>\n<p>Most of the world is simply unaware of the situation. They don\u2019t \u2018feel the AGI\u2019 and definitely don\u2019t take superintelligence seriously. They don\u2019t understand what is potentially being built, or how dangerous those building it believe it would be.</p>\n<blockquote><p>It might also help if more people understood how fast this field is moving. In 2015, the biggest skeptics of the dangers of AI assured everyone that these risks wouldn\u2019t happen for hundreds of years.</p>\n<p>In 2020, analysts said that humanity probably had a few decades to prepare.</p>\n<p>In 2025 the CEOs of AI companies predict they can create superhumanly good AI researchers in one to nine years, while the skeptics assure that it\u2019ll probably take at least five to ten years.</p>\n<p>Ten years is not a lot of time to prepare for the dawn of machine superintelligence, even if we\u2019re lucky enough to have that long.</p>\n<p>\u2026</p>\n<p>Nobody knows what year or month some company will build a superhuman AI researcher that can create a new, more powerful generation of artificial intelligences. Nobody knows the exact point at which an AI realizes that it has an incentive to fake a test and pretend to be less capable than it is. Nobody knows what the point of no return is, nor when it will come to pass.</p>\n<p>And up until that unknown point, AI is very valuable.</p></blockquote>\n<p>I would add that no one knows when we will be so dependent on AI that we will no longer have the option to turn back, even if it is not yet superintelligent and still doing what we ask it to do.</p>\n<p>Even the governments of America and China have not as of late been taking this seriously, treating the \u2018AI race\u2019 as being about who is manufacturing the GPUs.</p>\n\n\n<h4 class=\"wp-block-heading\">The Proposal: Shut It Down</h4>\n\n\n<p>Okay, wise guy, you ask the book, what is it gonna take to make the world not end?</p>\n<p>They bite the bullets.</p>\n<p>(To be maximally clear: I am not biting these bullets, as I am not as sold that there is no other way. If and when I do, you will know. The bullet I will absolutely bite is that we should be working, now, to build the ability to coordinate a treaty and enforcement mechanism in the future, should it be needed, and to build transparency and state capacity to learn more about when and if it is needed and in what form.)</p>\n<p>It is good and right to bite bullets, if you believe the bullets must be bitten.</p>\n<p>They are very clear they see only one way out: Development of frontier AI must stop.</p>\n<p>Which means a global ban.</p>\n<blockquote><p>Nothing easy or cheap. We are very, very sorry to have to say that.</p>\n<p>It is not a problem of one AI company being reckless and needing to be shut down.</p>\n<p>It is not a matter of straightforward regulations about engineering, that regulators can verify have been followed and that would make an AI be safe.</p>\n<p>It is not a matter of one company or one country being the most virtuous one, and everyone being fine so long as the best faction can just race ahead fast enough, ahead of all the others.</p>\n<p>A machine superintelligence will not just do whatever its makers wanted it to do.</p>\n<p>It is not a matter of your own country outlawing superintelligence inside its own borders, and your country then being safe while chaos rages beyond. Superintelligence is not a regional problem because it does not have regional effects. If anyone anywhere builds superintelligence, everyone everywhere dies.</p>\n<p>So the world needs to change. It doesn\u2019t need to change all that much for most people. It won\u2019t make much of a difference in most people\u2019s daily lives if some mad scientists are put out of a job.</p>\n<p>But life does need to change that little bit, in many places and countries. All over the Earth, it must become illegal for AI companies to charge ahead in developing artificial intelligence as they\u2019ve been doing.</p>\n<p>Small changes can solve the problem; the hard part will be enforcing them everywhere.</p></blockquote>\n<p>How would we do that, you ask?</p>\n<blockquote><p>So the first step, we think, is to say: All the computing power that could train or run more powerful new AIs, gets consolidated in places where it can be monitored by observers from multiple treaty-signatory powers, to ensure those GPUs aren\u2019t used to train or run more powerful new AIs.</p></blockquote>\n<p>Their proposed threshold is not high.</p>\n<blockquote><p>Nobody knows how to calculate the fatal number. So the safest bet would be to set the threshold low\u2014 \u200bsay, at the level of eight of the most advanced GPUs from 2024\u2014 \u200band say that it is illegal to have nine GPUs that powerful in your garage, unmonitored by the international authority.</p>\n<p>Could humanity survive dancing closer to the cliff-edge than that? Maybe. Should humanity try to dance as close to the cliff-edge as it possibly can? No.</p></blockquote>\n<p>I can already hear those calling this insane. I thought it too. What am I going to do, destroy the world with nine GPUs? Seems low. But now we\u2019d be talking price.</p>\n<p>They also want to ban people from publishing the wrong kinds of research.</p>\n<blockquote><p>So it should not be legal\u2014 \u200bhumanity probably cannot survive, if it goes on being legal\u2014 \u200bfor people to continue publishing research into more efficient and powerful AI techniques.</p>\n<p>\u2026</p>\n<p>It brings us no joy to say this. But we don\u2019t know how else humanity could survive.</p></blockquote>\n<p>Take that literally. They don\u2019t know how else humanity can survive. That doesn\u2019t mean that they think that if we don\u2019t do it by year [X], say 2029, that we will definitely already be dead at that point, or even already in an unsurvivable situation. It means that they see a real and increasing risk, over time, of anyone building it, and thus everyone dying, the longer we fail to shut down the attempts to do so. What we don\u2019t know is how long those attempts would take to succeed, or even if they will succeed at all.</p>\n<p>How do they see us enforcing this ban?</p>\n<p>Yes, the same way anything else is ultimately enforced. At the barrel of a gun, if necessary, which yes involves being ready to blow up a datacenter if it comes to that.</p>\n<blockquote><p>Imagine that the U.S. and the U.K., and China and Russia, all start to take this matter seriously. But suppose hypothetically that a different nuclear power thinks it\u2019s all childish nonsense and advanced AI will make everyone rich. The country in question starts to build a datacenter that they intend to use to further push AI capabilities. Then what?</p>\n<p>It seems to us that in this scenario, the other powers must communicate that the datacenter scares them. They must ask that the datacenter not be built. They must make it clear that if the datacenter is built, they will need to destroy it, by cyberattacks or sabotage or conventional airstrikes.</p>\n<p>They must make it clear that this is not a threat to force compliance; rather, they are acting out of terror for their own lives and the lives of their children.</p>\n<p>The Allies must make it clear that even if this power threatens to respond with nuclear weapons, they will have to use cyberattacks and sabotage and conventional strikes to destroy the datacenter anyway, because datacenters can kill more people than nuclear weapons.</p>\n<p>They should not try to force this peaceful power into a lower place in the world order; they should extend an offer to join the treaty on equal terms, that the power submit their GPUs to monitoring with exactly the same rights and responsibilities as any other signatory. Existing policy on nuclear weapon proliferation showed what can be done.</p></blockquote>\n<p>Queue, presumably, all the \u2018nuke the datacenter\u2019 quips once again, or people trying to equate this with various forms of extralegal action. No. This is a proposal for an international treaty, enforced the way any other treaty would be enforced. Either allow the necessary monitoring, or the datacenter gets shut down, whatever that takes.</p>\n<p>Thus, the proposal is simple. As broad a coalition as possible monitors all the data centers and GPUs, watching to ensure no one trains more capable AI systems.</p>\n<p>Is it technically feasible to do this? The book doesn\u2019t go into this question. I believe the answer is yes. If everyone involved wanted to do this, we could do it, for whatever hardware we were choosing to monitor. That would still leave consumer GPUs and potential decentralized attempts and so on, I don\u2019t know what you would do about that in the long term but if we are talking about this level of attention and effort I am betting we could find an answer.</p>\n<p>To answer a question the book doesn\u2019t ask, would this then mean a \u2018dystopian\u2019 or \u2018authoritarian\u2019 world or a \u2018global government\u2019? No. I\u2019m not saying it would be pretty (and again, I\u2019m not calling for it or biting these bullets myself) but this regime seems less effectively restrictive of practical freedoms than, for example, the current regime in the United Kingdom under the Online Safety Act. They literally want you see ID before you can access the settings on your home computer Nvidia GPU. Or Wikipedia.</p>\n\n\n<h4 class=\"wp-block-heading\">Hope Is A Vital Part Of Any Strategy</h4>\n\n\n<p>You gotta give \u2018em hope.</p>\n<p>And hope there is indeed.</p>\n<p>Humanity has done some very expensive, painful, hard things. We\u2019ve dodged close calls. The book cites big examples: We won World War II. We\u2019ve avoided nuclear war.</p>\n<p>There are many other examples one could cite as well.</p>\n<p>How do we get there from here?</p>\n\n\n<h4 class=\"wp-block-heading\">I\u2019m Doing My Part</h4>\n\n\n<blockquote><p>So\u2014 \u200bhow do we un-write our fate?</p>\n<p>We\u2019ve covered what must be done for humanity to survive. Now let\u2019s consider what can be done, and by whom.</p>\n<p><strong>If you are in government</strong>: We\u2019d guess that what happens in the leadup to an international treaty is countries or national leaders signaling openness to that treaty. Major powers should send the message: \u201cWe\u2019d rather not die of machine superintelligence. We\u2019d prefer there be an international treaty and coalition around not building it.\u201d</p>\n<p>The goal is not to have your country unilaterally cease AI research and fall behind.</p>\n<p>\u2026</p>\n<p>We have already mentioned that Rishi Sunak acknowledged the existence of risks from artificial superintelligence in October 2023, while he was the prime minister of the United Kingdom.</p>\n<p>Also in October 2023, Chinese General Secretary Xi Jinping gave (what seems to us like) weak signals in that direction, in a short document on international governance that included a call to \u201censure that AI always remains under human control.\u201d</p></blockquote>\n<p>The Chinese show many signs of being remarkably open to coordination. As well they should be, given that right now we are the ones out in front. Is there a long, long way left to go? Absolutely. Would there be, shall we say, trust issues? Oh my yes. But if you ask who seems to be the biggest obstacle to a future deal, all signs suggest we have met the enemy and he is us.</p>\n<blockquote><p><strong>If you are an elected official or political leader</strong>: Bring this issue to your colleagues\u2019 attention. Do everything you can to lay the groundwork for treaties that shut down any and all AI research and development that could result in superintelligence.</p>\n<p>\u2026</p>\n<p>Please consider\u2014 \u200bespecially by the time you read this\u2014whether the rest of the world is really opposed to you on this. A 2023 poll conducted by YouGov found that 69 percent of surveyed U.S. voters say AI should be regulated as a dangerous and powerful technology. A 2025 poll found that 60 percent of surveyed U.K. voters support laws against creating artificial superintelligence, and 63 percent support the prohibition of AIs that can make smarter AIs.</p>\n<p><strong>And if instead you are a politician who is not fully persuaded</strong>: Please at least make it possible for humanity to slam on the brakes later, even if you\u2019re not persuaded to slam on them now.</p>\n<p>\u2026</p>\n<p><strong>If you are a journalist who takes these issues seriously</strong>: The world needs journalism that treats this subject with the gravity it deserves, journalism that investigates beyond the surface and the easy headlines about Tech CEOs drumming up hype, journalism that helps society grasp what\u2019s coming. There\u2019s a wealth of stories here that deserve sustained coverage, and deeper investigation than we\u2019ve seen conducted so far.</p>\n<p>\u2026</p>\n<p>If humanity is to survive this challenge, people need to know what they\u2019re facing. It is the job of journalists as much as it is scientists\u2019.</p>\n<p><strong>And as for the rest of us</strong>: We don\u2019t ask you to forgo using all AI tools. As they get better, you might have to use AI tools or else fall behind other people who do. That trap is real, not imaginary.</p>\n<p>If you live in a democracy, you can write your elected representatives and tell them you\u2019re concerned. You can find some resources to help with that <a href=\"http://IfAnyoneBuildsIt.com/act\">at the link below</a>.</p>\n<p>And you can vote.</p>\n<p><a href=\"http://IfAnyoneBuildsIt.com/march\">You can go on protest marches</a>.</p>\n<p>You can talk about it.</p>\n<p>And once you have done all you can do? Live life well.</p>\n<p>If everyone did their part, votes and protests and speaking up would be enough. If everyone woke up one morning believing only a quarter of what we believe, and everyone knew everyone else believed it, they\u2019d walk out into the street and shut down the datacenters, soldiers and police officers walking right alongside moms and dads. If they believed a sixteenth of what we believed, there would be international treaties within the month, to establish monitors and controls on advanced computer chips.</p>\n<p>Can Earth survive if only some people do their part? Perhaps; perhaps not.</p>\n<p>We have heard many people say that it\u2019s not possible to stop AI in its tracks, that humanity will never get its act together. Maybe so. But a surprising number of elected officials have told us that they can see the danger themselves, but cannot say so for fear of the repercussions. Wouldn\u2019t it be silly if really almost none of the decision-makers wanted to die of this, but they all thought they were alone in thinking so?</p>\n<p>Where there\u2019s life, there\u2019s hope.</p></blockquote>\n\n\n<h4 class=\"wp-block-heading\">Their Closing Words</h4>\n\n\n<blockquote><p>From time to time, people have asked us if we\u2019ve felt vindicated to see our past predictions coming true or to see more attention getting paid to us and this issue.</p>\n<p>And so, at the end, we say this prayer:</p>\n<p><em>May we be wrong, and shamed for how incredibly wrong we were, and fade into irrelevance and be forgotten except as an example of how not to think, and may humanity live happily ever after.</em></p>\n<p>But we will not put our last faith and hope in doing nothing.</p>\n<p>So our true last prayer is this:</p>\n<p><em>Rise to the occasion, humanity, and win.</em></p></blockquote>\n<p>I cannot emphasize enough, I really really cannot emphasize enough, how much all of us worried about this want to be completely, spectacularly wrong, and for everything to be great, and for us to be mocked eternally as we live forever in our apartments. That would be so, so much better than being right and dying. It would even be much better than being right and everyone working together to ensure we survive anyway.</p>\n<p>Am I convinced that the only way for us to not die is an international treaty banning the development of frontier AI? No. That is not my position. However, I do think that it is good and right for those who do believe this to say so. And I believe that we should be alerting the public and our governments to the dangers, and urgently laying the groundwork for various forms of international treaties and cooperation both diplomatically and technologically, and also through the state capacity and transparency necessary to know if and when and how to act.</p>\n<p>I am not the target audience for this book, but based on what I know, this is the best treatment of the problem I have seen that targets a non-expert audience. I encourage everyone to read it, and to share it, and also to think for themselves about it.</p>\n<p>In the meantime, yes, work on the problem, but also <a href=\"https://thezvi.substack.com/p/ai-practical-advice-for-the-worried\">don\u2019t forget to live well</a>.</p>\n<p>&nbsp;</p>"
            ],
            "link": "https://thezvi.wordpress.com/2025/09/19/book-review-if-anyone-builds-it-everyone-dies-2/",
            "publishedAt": "2025-09-19",
            "source": "TheZvi",
            "summary": "Where \u2018it\u2019 is superintelligence, an AI smarter and more capable than humans. And where \u2018everyone dies\u2019 means that everyone dies. No, seriously. They\u2019re not kidding. They mean this very literally. To be precise, they mean that \u2018If anyone builds [superintelligence] &#8230; <a href=\"https://thezvi.wordpress.com/2025/09/19/book-review-if-anyone-builds-it-everyone-dies-2/\">Continue reading <span class=\"meta-nav\">&#8594;</span></a>",
            "title": "Book Review: If Anyone Builds It, Everyone Dies"
        },
        {
            "content": [],
            "link": "https://xkcd.com/3144/",
            "publishedAt": "2025-09-19",
            "source": "XKCD",
            "summary": "<img alt=\"People looking for the gaps in our understanding where the meaning of consciousness or free will might hide often turn to quantum uncertainty or infinite cosmologies, as if we don't have breathtakingly complex emergent phenomena right there in our freezers.\" src=\"https://imgs.xkcd.com/comics/phase_changes.png\" title=\"People looking for the gaps in our understanding where the meaning of consciousness or free will might hide often turn to quantum uncertainty or infinite cosmologies, as if we don't have breathtakingly complex emergent phenomena right there in our freezers.\" />",
            "title": "Phase Changes"
        }
    ],
    "lookbackDays": 1,
    "publishDate": "2025-09-19"
}