{
    "articles": [
        {
            "content": [],
            "link": "https://interconnected.org/home/2025/11/07/oedipus",
            "publishedAt": "2025-11-07",
            "source": "Matt Webb",
            "summary": "<div> <p>Ok spoilers ahead.</p> <p>But <a href=\"https://en.wikipedia.org/wiki/Oedipus_Rex\">Oedipus Rex</a> a.k.a. <em>Oedipus Tyrannus</em> by Sophocles is almost 2,500 years old at this point so it\u2019s fair game imo.</p> <p><em>The Oedipus story in a nutshell:</em></p> <p>Oedipus, who was secretly adopted, receives a prophecy that he will kill his dad. So to thwart fate he leaves his dad and winds up in a city with a missing king (btw killing an argumentative guy on the way). Many years after becoming the new king and marrying the widow, he discovers that the dude he long-ago killed on the road was the missing king. Uh oh. And that the missing king was actually his birth dad, prophecy fulfilled. Double uh oh. And that his now-wife is therefore his birth mom. Uh oh for the third time, wife/mom suicides, stabs out own eyes, exiles self. End.</p> <p>So the Sophocles play is a re-telling of this already well-worn story, at a time when Athenian culture was oriented around annual drama competitions (it came second).</p> <p>The narrative new spin on the old tale is that it\u2019s told as a <em>whodunnit</em> set over a single day, sunrise to sunset.</p> <p>In a combination of flashbacks and new action, Oedipus himself acts",
            "title": "Oedipus is about the act of figuring out what Oedipus is about"
        },
        {
            "content": [],
            "link": "https://www.nytimes.com/2025/11/07/style/modern-love-my-first-love-ghosted-me.html",
            "publishedAt": "2025-11-07",
            "source": "Modern Love - NYT",
            "summary": "I had lost faith in the universe \u2014 until an apparition of my smiling, 16-year-old late ex-boyfriend appeared.",
            "title": "My First Love Ghosted Me. It\u2019s Not What You Think."
        },
        {
            "content": [],
            "link": "https://www.robinsloan.com/lab/compile-time/",
            "publishedAt": "2025-11-07",
            "source": "Robin Sloan",
            "summary": "<p>The secret. <a href=\"https://www.robinsloan.com/lab/compile-time/\">Read here.</a></p>",
            "title": "Coffee break"
        },
        {
            "content": [
                "<p>&#8220;Life is suffering&#8221; may be a Noble Truth, but it feels like a <a href=\"https://www.spencergreenberg.com/2020/09/deepities-and-deepifuls/\">deepity</a>. Yes, obviously life <em>includes</em> suffering. But it also includes happiness. Many people live good and happy lives, and even people with hard lives experience some pleasant moments.</p><p>This is the starting point of many people&#8217;s objection to Buddhism. They continue: if nirvana is just a peaceful state beyond joy or suffering, it sounds like a letdown. An endless gray mist of bare okayness, like death or Britain. If your life was previously good, it&#8217;s a step down. Even if your life sucked, maybe you would still prefer the heroism of high highs and low lows to eternal blah.</p><p>Against all this, many Buddhists claim to be able to reach <a href=\"https://www.astralcodexten.com/p/nick-cammarata-on-jhana\">jhana</a>, a state described as better than sex or heroin - and they say nirvana<em> </em>is even better than <em>that.</em> Partly it&#8217;s better because jhana is temporary and nirvana permanent, but it&#8217;s also better on a moment-to-moment basis. So nirvana must mean something beyond bare okayness. But then why the endless insistence that life is suffering and the best you can do is make it stop?</p><p>I don&#8217;t know the orthodox Buddhist answer to this question. But I got the rationalist techno-Buddhists&#8217; answer from <a href=\"https://www.lesswrong.com/posts/DvjJoxP6f79G9iAbE/enlightenment-ama\">lsusr</a> a few months ago, and found it, uh, enlightening. He said: mental valence works like temperature.</p><p><em>Naively</em>, there are two kinds of temperature: hot and cold. When an environment stops being hot, then it&#8217;s neutral - &#8220;room temperature&#8221; - neither hot nor cold. After that, you can add arbitrary amounts of coldness, making it colder and colder.</p><p>But scientifically, there&#8217;s only one kind of temperature: heat. Apparent &#8220;neutral&#8221; at room temperature is a fact about human perception with no objective significance. If you start at &#8220;very hot&#8221; and take away heat, at some point your perception switches from &#8220;less hot&#8221; to &#8220;more cold&#8221;, but you&#8217;ve just been taking away heat the whole time. The real &#8220;zero heat&#8221; isn&#8217;t room temperature. It&#8217;s absolute zero, which feels colder than we can possibly imagine.</p><p>In the same way, <em>naively</em>, there are two kinds of emotion - joy and suffering. When a situation stops being bad, then it&#8217;s neutral - &#8220;just okay&#8221; - neither joy nor suffering. After that, you can add arbitrary amounts of joy, making yourself happier and happier.</p><p>But scientifically (according to the Buddhists) there&#8217;s only one kind of emotion: suffering. Apparent neutral is a fact about human perception with no objective significance. If you start at &#8220;very bad&#8221; and take away suffering, at some point your perception switches from &#8220;less suffering&#8221; to &#8220;more joyful&#8221;, but you&#8217;ve just been taking away suffering the whole time. The real &#8220;zero suffering&#8221; isn&#8217;t neutral / blah / just okay. It&#8217;s nirvana, which feels more blissful than we can possibly imagine.</p><p>In this model, the statement &#8220;life is suffering&#8221; is equivalent to &#8220;temperature is heat&#8221; and literally true. An ignoramus might boggle at this: <em>all</em> temperatures are heat? What about fifty degrees below zero on a winter&#8217;s night in Alaska? Sorry, that&#8217;s heat too - 228 degrees Kelvin. It&#8217;s colder than the reference temperature you dubbed neutral, but that was always fake. Likewise, it seems surprising that all life is suffering: even when you&#8217;re having sex? Even when you&#8217;re on heroin? But to Buddhists, both of those states are some number of degrees worse than the absolute zero suffering of nirvana.</p><p>Why should we believe this model?</p><p>First, regardless of whether we <em>believe</em> it or not, I find it helpful in understanding what Buddhists are asserting. It removes my urge to have tedious arguments where I accuse them of being anti-human and forgetting that life includes good things.</p><p>But also, it does seem to match some of the other ground we&#8217;ve covered about what people notice during meditative experiences - for example, in <a href=\"https://www.astralcodexten.com/p/jhanas-and-the-dark-room-problem\">Jhanas And The Dark Room Problem</a>. The neuroscientists say the brain tries to minimize prediction error. But a natural way to minimize prediction error is to sit quietly in a dark room and never expose yourself to any unpredictable stimuli at all. Why isn&#8217;t this maximum bliss? The qualiologists propose that you&#8217;re just bad at sitting in a dark room. If you were good at it - that is, a trained meditator who could calm their brain down enough to pay full attention to the lack of stimuli - it would be amazing. This is why trained meditators are always talking about all the cosmic bliss that they feel. And from here it&#8217;s a short hop to <a href=\"https://qualiacomputing.com/2020/12/17/the-symmetry-theory-of-valence-2020-presentation/\">the symmetry theory of valence</a>, where the unpleasantness of mental states tracks a sort of irregularity or asymmetry in brain activity. </p><p>The emotion &#8220;happiness&#8221; is a form of brain activity which is more regular and symmetrical than usual - maybe the most regularity and symmetry we can get in the normal course of things. But ice is a form of matter which is colder than usual - yet if you drop it into liquid helium, it will add heat, not subtract it. Thus the insistence among meditators that happiness is an obstacle and you should seek nirvana instead.</p>"
            ],
            "link": "https://www.astralcodexten.com/p/in-what-sense-is-life-suffering",
            "publishedAt": "2025-11-07",
            "source": "SlateStarCodex",
            "summary": "<p>&#8220;Life is suffering&#8221; may be a Noble Truth, but it feels like a <a href=\"https://www.spencergreenberg.com/2020/09/deepities-and-deepifuls/\">deepity</a>. Yes, obviously life <em>includes</em> suffering. But it also includes happiness. Many people live good and happy lives, and even people with hard lives experience some pleasant moments.</p><p>This is the starting point of many people&#8217;s objection to Buddhism. They continue: if nirvana is just a peaceful state beyond joy or suffering, it sounds like a letdown. An endless gray mist of bare okayness, like death or Britain. If your life was previously good, it&#8217;s a step down. Even if your life sucked, maybe you would still prefer the heroism of high highs and low lows to eternal blah.</p><p>Against all this, many Buddhists claim to be able to reach <a href=\"https://www.astralcodexten.com/p/nick-cammarata-on-jhana\">jhana</a>, a state described as better than sex or heroin - and they say nirvana<em> </em>is even better than <em>that.</em> Partly it&#8217;s better because jhana is temporary and nirvana permanent, but it&#8217;s also better on a moment-to-moment basis. So nirvana must mean something beyond bare okayness. But then why the endless insistence that life is suffering and the best you can do is make it stop?</p><p>I don&#8217;t know the orthodox Buddhist answer to this question. But I",
            "title": "In What Sense Is Life Suffering?"
        },
        {
            "content": [
                "<p>Some podcasts are self-recommending on the \u2018yep, I\u2019m going to be breaking this one down\u2019 level. This was very clearly one of those. So here we go.</p>\n<p>As usual for podcast posts, the baseline bullet points describe key points made, and then the nested statements are my commentary.</p>\n<p>If I am quoting directly I use quote marks, otherwise assume paraphrases.</p>\n<p>The entire conversation takes place with an understanding that no one is to mention existential risk or the fact that the world will likely transform, without stating this explicitly. Both participants are happy to operate that way. I\u2019m happy to engage in that conversation (while pointing out its absurdity in some places), but assume that every comment I make has an implicit \u2018assuming normality\u2019 qualification on it, even when I don\u2019t say so explicitly.</p>\n<div>\n\n\n<span id=\"more-24840\"></span>\n\n\n</div>\n<div>\n<figure>\n<div>\n\n\n<figure class=\"wp-block-image\"><img alt=\"\" src=\"https://substackcdn.com/image/fetch/$s_!QY2x!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F16341304-d51f-4635-b620-3ce94bbc7950_995x559.png\" /></figure>\n\n\n<div></div>\n</div>\n</figure>\n</div>\n\n\n<h4 class=\"wp-block-heading\">On The Sam Altman Production Function</h4>\n\n\n<ol>\n<li>Cowen asks how Altman got so productive, able to make so many deals and ship so many products. Altman says people almost never allocate their time efficiently, and that when you have more demands on your time you figure out how to improve. Centrally he figures out what the core things to do are and delegates. He says deals are quicker now because everyone wants to work with OpenAI.\n<ol>\n<li>Altman\u2019s definitely right that most people are inefficient with their time.</li>\n<li>Inefficiency is relative. As in, I think of myself as inefficient with my time, and think of the ways I could be a lot more efficient.</li>\n<li>Not everyone responds to pressure by improving efficiency, far from it.</li>\n<li>Altman is good here to focus on delegation.</li>\n<li>It is indeed still remarkable how many things OpenAI is doing at once, with the associated worries about it potentially being too many things, and not taking the time to do them responsibly.</li>\n</ol>\n</li>\n</ol>\n\n\n<h4 class=\"wp-block-heading\">On Hiring Hardware People</h4>\n\n\n<ol>\n<li>What makes hiring in hardware different from in AI? Cycles are longer. Capital is more intense. So more time invested up front to pick wisely. Still want good, effective, fast-moving people and clear goals.\n<ol>\n<li>AI seems to be getting pretty capital intensive?</li>\n</ol>\n</li>\n<li>Nvidia\u2019s people \u2018are less weird\u2019 and don\u2019t read Twitter. OpenAI\u2019s hardware people feel more like their software people than they feel like Nvidia\u2019s people.\n<ol>\n<li>My guess is there isn\u2019t a right answer but you need to pick a lane.</li>\n</ol>\n</li>\n<li>What makes Roon special? Lateral thinker, great at phrasing observations, lots of disparate skills in one place.\n<ol>\n<li>I would add some more ingredients. There\u2019s a sense of giving zero fucks, of having no filter, and having no agenda. Say things and let the chips fall.</li>\n<li>A lot of the disparate skills are disparate aesthetics, including many that are rare in AI, and taking all of them seriously at once.</li>\n</ol>\n</li>\n<li>Altman doesn\u2019t tell researchers what to work on. Researchers choose, that\u2019s it.</li>\n<li>Email is very bad. Slack might not be good, it creates explosions of work including fake work to deal with, especially the first and last hours, but it is better than email. Altman suspects it\u2019s time for a new AI-driven thing but doesn\u2019t have it yet, probably due to lack of trying and unwillingness to pay focus and activation energy given everything else going on.\n<ol>\n<li>I think email is good actually, and that Slack is quite bad.</li>\n<li>Email isn\u2019t perfect but I like that you decide what you have ownership of, how you organize it, how you keep it, when you check it, and generally have control over the experience, and that you can choose how often you check it and aren\u2019t being constantly pinged or expected to get into chat exchanges.</li>\n<li>Slack is an interruption engine without good information organization and I hate it so much, as in \u2018it\u2019s great I don\u2019t have a job where I need slack.\u2019</li>\n<li>There\u2019s definitely room to build New Thing that integrates AI into some mix of information storage and retrieval, email slow communication, direct messaging and group chats, and which allows you to prioritize and get the right levels of interruption at the right times, and so on.</li>\n<li>However this will be tricky, you need to be ten times better and you can\u2019t break the reliances people have. False negatives, where things get silently buried, can be quite bad.</li>\n</ol>\n</li>\n</ol>\n\n\n<h4 class=\"wp-block-heading\">On What GPT-6 Will Enable</h4>\n\n\n<ol>\n<li>What will make GPT-6 special? Altman suggests it might be able to \u2018really do\u2019 science. He doesn\u2019t have much practical advice on what to do with that.\n<ol>\n<li>This seems like we hit the wall of \u2018\u2026and nothing will change much\u2019 forcing Altman to go into contortions.</li>\n<li>One thing we learned from GPT-5 is that the version numbers don\u2019t have to line up with big capabilities leaps. The numbers are mostly arbitrary.</li>\n</ol>\n</li>\n</ol>\n<p>Tyler isn\u2019t going to let him off that easy. At this point, I don\u2019t normally do this, but exact words seem important so I\u2019m going to quite the transcript.</p>\n<blockquote><p><strong>COWEN: </strong>If I\u2019m thinking about restructuring an entire organization to have GPT-6 or 7 or whatever at the center of it, what is it I should be doing organizationally, rather than just having all my top people use it as add-ons to their current stock of knowledge?</p>\n<p><strong>ALTMAN: </strong>I\u2019ve thought about this more for the context of companies than scientists, just because I understand that better. I think it\u2019s a very important question. Right now, I have met some orgs that are really saying, \u201cOkay, we\u2019re going to adopt AI and let AI do this.\u201d I\u2019m very interested in this, because shame on me if OpenAI is not the first big company run by an AI CEO, right?</p>\n<p><strong>COWEN: </strong>Just parts of it. Not the whole thing.</p>\n<p><strong>ALTMAN: </strong>No, the whole thing.</p>\n<p><strong>COWEN: </strong>That\u2019s very ambitious. Just the finance department, whatever.</p>\n<p><strong>ALTMAN: </strong>Well, but eventually it should get to the whole thing, right? So we can use this and then try to work backwards from that. I find this a very interesting thought experiment of what would have to happen for an AI CEO to be able to do a much better job of running OpenAI than me, which clearly will happen someday. How can we accelerate that? What\u2019s in the way of that? I have found that to be a super useful thought experiment for how we design our org over time and what the other pieces and roadblocks will be. I assume someone running a science lab should try to think the same way, and they\u2019ll come to different conclusions.</p>\n<p><strong>COWEN: </strong>How far off do you think it is that just, say, one division of OpenAI is 85 percent run by AIs?</p>\n<p><strong>ALTMAN: </strong>Any single division?</p>\n<p><strong>COWEN: </strong>Not a tiny, insignificant division, mostly run by the AIs.</p>\n<p><strong>ALTMAN: </strong>Some small single-digit number of years, not very far. When do you think I can be like, \u201cOkay, Mr. AI CEO, you take over\u201d?</p>\n<p><strong>COWEN: </strong>CEO is tricky because the public role of a CEO, as you know, becomes more and more important.</p></blockquote>\n<ol>\n<li>On the above in terms of \u2018oh no\u2019:\n<ol>\n<li>Oh no. Exactly the opposite. Shame on him if OpenAI goes first.</li>\n<li>OpenAI is the company, in this scenario, out of all the companies, we should be most worried about handing over to an AI CEO, for obvious reasons.</li>\n<li>If you\u2019re wondering how the AIs could take over? You can stop wondering. They will take over because we will ask them to.</li>\n<li>CEO is an adversarial and anti-inductive position, where any weakness will be systematically exploited, and big mistakes can entirely sink you, and the way that you direct and set up the \u2018AI CEO\u2019 matters quite a lot in all this. The bar to a net positive AI CEO is much higher than the AI making on average better decisions, or having on average better features, and the actual bar is higher. Altman says \u2018on the actual decision making maybe the AI is pretty good soon\u2019 but this is a place where I\u2019m going to be the Bottleneck Guy.</li>\n<li>CEO is also a position where, very obviously, misaligned means your company can be extremely cooked, and basically everything in it subverted, even if that CEO is a single human. Most of the ways in which this is limited are because the CEO can only be in one place at a time and do one thing at a time, couldn\u2019t keep an eye on most things let alone micromanage them, and would require conspirators. A hostile AI CEO is death or subversion of the company.</li>\n<li>The \u2018public role\u2019 of the CEO being the bottleneck does not bring comfort here. If Altman (as he suggests) is public face and the AI \u2018figures out what to do\u2019 and Altman doesn\u2019t actually get to overrule the AI (or is simply convinced not to) then the problem remains.</li>\n</ol>\n</li>\n<li>On the above in terms of \u2018oh yeah\u2019:\n<ol>\n<li>There is the clear expectation from both of them that AI will rise, reasonably soon, to the level of at least \u2018run the finance department of a trillion dollar corporation.\u2019 This doesn\u2019t have to be AGI but it probably will be, no?</li>\n<li>It\u2019s hard for me to square \u2018AIs are running the actual decision making at top corporations\u2019 with predictions for only modest GDP growth. As Altman notes, the AI CEO needs to be a lot better than the human CEO in order to get the job.</li>\n<li>They are predicting billion-dollar 2-3 person companies, with AIs, within three years.</li>\n</ol>\n</li>\n<li>Altman asks potential hires about their use of AI now to predict their level of AI adoption in the future, which seems smart. Using it as \u2018better Google\u2019 is a yellow flag, thinking about day-to-day in three years is a green flag.</li>\n<li>In three years Altman is aiming to have a \u2018fully automated AI researcher.\u2019 So it\u2019s pretty hard to predict day-to-day use in three years.</li>\n</ol>\n\n\n<h4 class=\"wp-block-heading\">On government backstops for AI companies</h4>\n\n\n<p>A timely section title.</p>\n<ol>\n<li>Cowen and Altman are big fans of nuclear power (as am I), but people worry about them. Cowen asks, do you worry similarly about AI and the similar Nervous Nellies, even if \u2018AI is pretty safe\u2019? Are the Feds your insurer? How will you insure everything?\n<ol>\n<li>Before we get to Altman\u2019s answer can we stop to think about how absolutely insane this question is as presented?</li>\n<li>Cowen is outright equating worries about AI to worries about nuclear power, calling both Nervous Nellies. My lord.</li>\n<li>The worry about AI risks is that the AI companies might be held too accountable? Might be asked to somehow provide too much insurance, when there is clearly no sign of any such requirement for the most important risks? They are building machines that will create substantial catastrophic and even existential risks, massive potential externalities.</li>\n<li>And you want the Federal Government to actively insure against AI catastrophic risks? To say that it\u2019s okay, we\u2019ve got you covered? This does not, in any way, actually reduce the public\u2019s or world\u2019s exposure to anything, and it further warps company incentives. It\u2019s nuts.</li>\n<li>Not that even the Federal Government can actually ensure us here even at our own expense, since existential risk or sufficiently large catastrophic or systemic risk also wipes out the Federal Government. That\u2019s kind of the point.</li>\n<li>The idea that the people are the Nervous Nellies around nuclear, which has majority public support, while Federal Government is the one calming them down and ensuring things can work is rather rich.</li>\n<li>Nuclear power regulations are insanely restrictive and prohibitive, and the insurance the government writes does not substantially make up for this, nor is it that expensive or risky. The NRC and other regulations are the reason we can\u2019t have this nice thing, in ways that don\u2019t relate much if at all to the continued existence of these Nervous Nellies. Providing safe harbor in exchange of that really is the actual least you can do.</li>\n<li>AI regulations impose very few rules and especially very few safety rules.</li>\n<li>Yes, there is the counterpoint that AI has to follow existing rules and thus is effectively rather regulated, but I find this rather silly as an argument, and no I don\u2019t think the new laws around AI in particular move that needle much.</li>\n</ol>\n</li>\n<li>Altman points out the Federal Government is the insurer of last resort for anything sufficiently large, whether you want it to be or not, but no not in the way of explicitly writing insurance policies.\n<ol>\n<li>I mean yes if AI crashes the economy or does trillions in damages or what not, then the Federal Government will have to try and step in. This is a huge actual subsidy to the AI companies and they should (in theory anyway) be pay for it.</li>\n<li>A bailout for the actual AI companies if they are simply going bankrupt? David Sacks has made it clear our answer is no thank you, and rightfully so. Obviously, at some point the Fed Put or Trump Put comes into play in the stock market, that ship has sailed, but no we will not save your loans.</li>\n<li>And yeah, my lord, the idea that the Feds would write an insurance policy.</li>\n</ol>\n</li>\n<li>Cowen then says he is worried about the Feds being the insurer of first resort and he doesn\u2019t want that, Altman confirms he doesn\u2019t either and doesn\u2019t expect it.\n<ol>\n<li>It\u2019s good that they don\u2019t want this to happen but this only slightly mitigates my outrage at the first question and the way it was presented.</li>\n</ol>\n</li>\n<li>Cowen points out Trump is taking equity in Intel, lithium and rare earths, and asks how this applies to OpenAI. Altman mostly dodges, pivots to potential loss of meaning in the world, and points out the government might have strong opinions about AI company actions.\n<ol>\n<li>Cowen doesn\u2019t say it here but to his credit is on record correctly opposing this taking of equity in companies correctly identifying it as \u2018seizing the means of production\u2019 and pointing out it is the wrong tool for the job.</li>\n<li>This really was fully a non-answer. I see why that might be wise.</li>\n<li>Could OpenAI be coerced into giving up equity, or choose to do so as part of a regulatory capture play? Yeah. It would be a no-good, very bad thing.</li>\n<li>The government absolutely will and needs to have strong opinions about AI company actions and set the regulations and rules in place and otherwise play the role of being the actual government.</li>\n<li>If the government does not govern the AI companies, then the government will wake up one day to find the AI companies have become the government.</li>\n</ol>\n</li>\n</ol>\n\n\n<h4 class=\"wp-block-heading\">On monetizing AI services</h4>\n\n\n<ol>\n<li>Tyler Cowen did a trip through France and Spain and booked all but one hotel with GPT-5 (not directly in the app), and almost every meal they ate, and Altman didn\u2019t get paid for that. Shouldn\u2019t he get paid?\n<ol>\n<li>Before I get to Altman\u2019s answer, I will say that for specifically Tyler this seems very strange to me, unless he\u2019s running an experiment as research.</li>\n<li>As in, Tyler has very particular preferences and a lot of comparative advantage in choosing hotels and especially restaurants, especially for himself. It seems unlikely that he can\u2019t do better than ChatGPT?</li>\n<li>I expect to be able to do far better than ChatGPT on finding restaurants, although with a long and highly customized prompt, maybe? But it would require quite a lot of work.</li>\n<li>For hotels, yeah, I think it\u2019s reasonably formulaic and AI can do fine.</li>\n</ol>\n</li>\n<li>Altman responds that often ChatGPT is cited as the most trusted tech product from a big tech company. He notes that this is weird given the hallucinations. But it makes sense in that it doesn\u2019t have ads and is in many visible ways more fully aligned with user preferences than other big tech products that involve financial incentives. He notes that a transaction fee probably is fine but any kind of payment for placement would endanger this.\n<ol>\n<li>ChatGPT being most trusted is definitely weird given it is not very reliable.</li>\n<li>It being most trusted is an important clue to how people will deal with AI systems going forward, and it should worry you in important ways.</li>\n<li>In particular, trust for many people is about \u2018are they Out To Get You?\u2019 rather than reliability or overall quality, or are expectations set fairly. Compare to the many people who otherwise trust a Well Known Liar.</li>\n<li>I strongly agree with Altman about the payola worry, as Cowen calls it. Cowen says he\u2019s not worried about it, but doesn\u2019t explain why not.</li>\n<li>OpenAI\u2019s instant checkout offerings and policies are right on the edge on this. I think in their present form they will be fine but they\u2019re on thin ice.</li>\n</ol>\n</li>\n<li>Cowen\u2019s worry is that OpenAI will have a cap on how much commission they can charge, because stupider services will then book cheaply if you charge too much. Altman says he expects much lower margins.\n<ol>\n<li>AI will as Altman notes make many markets much more efficient by vastly lowering search costs and transaction costs, which will lower margins, and this should include commissions.</li>\n<li>I still think OpenAI will be able to charge substantial commissions if it retains its central AI position with consumers, for the same reason that other marketplaces have not lost their ability to extract commissions, including some very large ones. Every additional hoop you ask a customer to go through loses a substantial portion of sales. OpenAI can pull the same tricks as Steam and Amazon and Apple including on price parity, and many will pay.</li>\n<li>This is true even if there are stupider services that can do the booking and are generally 90% as good, so long as OpenAI is the consumer default.</li>\n</ol>\n</li>\n<li>Cowen doubles down on this worry about cheap competing agents, Altman notes that hotel booking is not the way to monetize, Cowen says but of course you do want to do that, Altman says no he wants to do new science, but ChatGPT and hotel booking is good for the world.\n<ol>\n<li>This feels like a mix of a true statement and a dishonest dodge.</li>\n<li>As in, of course he wants to do hotel booking and make money off it, it\u2019s silly to pretend that you don\u2019t and there\u2019s nothing wrong with that. It\u2019s not the main goal, but it drives growth and valuation and revenue all of which is vital to the AGI or science mission (whether you agree with that mission or not).</li>\n</ol>\n</li>\n<li>Cowen asks, you have a deal coming with Walmart, if you were Amazon would you make a deal with OpenAI or fight back? Altman says he doesn\u2019t know, but that if he was Amazon he would fight back.\n<ol>\n<li>Great answer from Altman.</li>\n<li>One thing Altman does well is being candid in places you would not expect, where it is locally superficially against his interests, but where it doesn\u2019t actually cost him much. This is one of those places.</li>\n<li>Amazon absolutely cannot fold here because it loses too much control over the customer and customer flow. They must fight back. Presumably they should fight back together with their friends at Anthropic?</li>\n</ol>\n</li>\n<li>Cowen asks about ads. Altman says some ads would be bad as per earlier, but other kinds of ads would be good although he doesn\u2019t know what the UI is.\n<ol>\n<li>Careful, Icarus.</li>\n<li>There definitely are \u2018good\u2019 ways to do ads if you keep them entirely distinct from the product, but the temptations and incentives here are terrible.</li>\n</ol>\n</li>\n</ol>\n\n\n<h4 class=\"wp-block-heading\">On AI\u2019s future understanding of intangibles</h4>\n\n\n<ol>\n<li>What should OpenAI management know about KSA and UAE? Altman says it\u2019s mainly knowing who will run the data centers and what security guarantees they will have, with data centers being built akin to US embassies or military bases. They bring in experts and as needed will bring in more.\n<ol>\n<li>I read this as a combination of outsourcing the worries and not worrying.</li>\n<li>I would be more worried.</li>\n</ol>\n</li>\n<li>Cowen asks, how good will GPT-6 be at teaching these kinds of national distinctions, or do you still need human experts? Altman expects to still need the experts, confirms they have an internal eval for that sort of thing but doesn\u2019t want to pre-announce.\n<ol>\n<li>My anticipation is that GPT-6 and its counterparts will actually be excellent at understanding these country distinctions in general, when it wants to be.</li>\n<li>My anticipation is also that GPT-6 will be excellent at explaining things it knows to humans and helping those humans learn, when it wants to, and this is already sufficiently true for current systems.</li>\n<li>The question is, will you be able to translate that into learning and understanding such issues?</li>\n<li>Why is this uncertain? Two concerns.</li>\n<li>The first concern is that understanding may depend on analysis of particular key people and relationships, in ways that are unavailable to AI, the same way you can\u2019t get them out of reading books.</li>\n<li>The second concern is that to actually understand KSA and UAE, or any country or culture in general, requires communicating things that it would be impolitic to say out loud, or for an AI to typically output. How do you pass on that information in this context? It\u2019s a problem.</li>\n</ol>\n</li>\n<li>Cowen asks about poetry, predicts you\u2019ll be able to get the median Pablo Neruda poem but not the best, maybe you\u2019ll get to 8.8/10 in a few years. Altman says they\u2019ll reach 10/10 and Cowen won\u2019t care, Cowen promises he\u2019ll care but Altman equates it to AI chess players. Cowen responds there\u2019s something about a great poem \u2018outside the rubric\u2019 and he worries humans that can\u2019t produce 10s can\u2019t identify 10s? Or that only humanity collectively and historically can decide what is a 10?\n<ol>\n<li>This is one of those \u2018AI will never be able to [X] at level [Y]\u2019 claims so I\u2019m on Altman\u2019s side here, a sufficiently capable AI can do 10/10 on poems, heck it can do 11/10 on poems. But yeah, I don\u2019t think you or I will care other than as a technical achievement.</li>\n<li>If an AI cannot produce sufficiently advanced poetry, that means that the AI is insufficiently advanced. Also we should not assume that future AIs or LLMs will share current techniques or restrictions. I expect innovation with respect to poetry creation.</li>\n<li>The thing being outside the rubric is a statement primarily about the rubric.</li>\n<li>If only people writing 10s can identify 10s then for almost all practical purposes there\u2019s no difference between a 9 and a 10. Why do we care, if we literally can\u2019t tell the difference? Whereas if we can tell the difference, if verification is easier than generation as it seems like it should be here, then we can teach the AI how to tell the difference.</li>\n<li>I think Cowen is saying that a 10-poem is a 9-poem that came along at the right time and got the right cultural resonance, in which case sure, you cannot reliably produce 10s, but that\u2019s because it\u2019s theoretically impossible to do that, and no human could do that either. Pablo Neruda couldn\u2019t do it.</li>\n<li>As someone who has never read a poem by Pablo Neruda, I wanted to see what this 10.0 business was all about, so by Claude\u2019s recommendation of \u2018widely considered best Neruda poem\u2019 without any other context, I selected <a href=\"https://allpoetry.com/poem/12655707-Tonight-I-Can-Write--The-Saddest-Lines--by-Pablo-Neruda\">Tonight I Can Write (The Saddest Lines)</a>. And not only did it not work on me, it seemed like something an AI totally could write today, on the level of \u2018if you claimed to have written this in 2025 I\u2019d have suspected an AI did write it.\u2019</li>\n<li>With that in mind, I gave Claude context and it selected <a href=\"https://www.eatthispoem.com/blog/2012/3/5/ode-to-the-onion-by-pablo-neruda-onion-galette-with-blue-che.html\">Ode to the Onion</a>. Which also didn\u2019t do anything for me, and didn\u2019t seem like anything that would be hard for an AI to write. Claude suggests it\u2019s largely about context, that this style was new at the time, and I was reading translations into English and I\u2019m no poetry guy, and agrees that in 2025 yes an AI could produce a similar poem, it just wouldn\u2019t land because it\u2019s no longer original.</li>\n<li>I\u2019m willing to say that whatever it is Tyler thinks AI can\u2019t do, also is something I don\u2019t have the ability to notice. And which doesn\u2019t especially motivate me to care? Or maybe is what Tyler actually wants something like \u2018invent new genre of poetry\u2019?</li>\n<li>We\u2019re not actually trying to get AIs to invent new genres of poetry, we\u2019re not trying to generate the things that drive that sort of thing, so who is to say if we could do it. I bet we could actually. I bet somewhere in the backrooms is a 10/10 Claude poem, if you have eyes to see.</li>\n</ol>\n</li>\n</ol>\n\n\n<h4 class=\"wp-block-heading\">On Chip-Building</h4>\n\n\n<ol>\n<li>It\u2019s hard. Might get easier with time, chips designing chips.</li>\n<li>Why not make more GPUs? Altman says, because we need more electrons. What he needs most are electrons. We\u2019re working hard on that. For now, natural gas, later fusion and solar. He\u2019s still bullish on fusion.\n<ol>\n<li>This \u2018electrons\u2019 thing is going to drive me nuts on a technical level. No.</li>\n<li>This seems simply wrong? We don\u2019t build more GPUs because TSMC and other bottlenecks mean we can\u2019t produce more GPUs.</li>\n<li>That\u2019s not to say energy isn\u2019t an issue but the GPUs sell out.</li>\n<li>Certainly plenty of places have energy but no GPUs to run with them.</li>\n</ol>\n</li>\n<li>Cowen worries that fusion uses the word \u2018nuclear.\u2019\n<ol>\n<li>I don\u2019t. I think that this is rather silly.</li>\n<li>The problem with fusion is purely that it doesn\u2019t work. Not yet, anyway.</li>\n<li>Again, the people are pro-nuclear power. Yay the people.</li>\n</ol>\n</li>\n<li>Cowen asks do you worry about a scenario where superintelligence does not need much compute, so you\u2019re betting against progress over a 30-year time horizon?\n<ol>\n<li>Always pause when you hear such questions to consider that perhaps under such a scenario this is not the correct thing to worry about?</li>\n<li>As in, if we not only have superintelligence it also does not need so much compute, the last thing I am going to ponder next is the return on particular investments of OpenAI, even if I am the CEO of OpenAI.</li>\n<li>If we have sufficiently cheap superintelligence that we have both superintelligence and an abundance of compute, ask not how the stock does, ask questions like how the humans survive or stay in control at all, notice that the entire world has been transformed, don\u2019t worry about your damn returns.</li>\n</ol>\n</li>\n<li>Altman responds if compute is cheaper people will want more. He\u2019ll take that bet every day, and the energy will still be useful no matter the scenario.\n<ol>\n<li>Good bet, so long as it matters what people want.</li>\n</ol>\n</li>\n<li>Cowen loves Pulse, Altman says people love Pulse, the reason you don\u2019t hear more is it\u2019s only available to Pro users. Altman uses Pulse for a combination of work related news and family opportunities like hiking trails.\n<ol>\n<li>I dabble with Pulse. It\u2019s\u2026 okay? Most of the time it gives me stories I already know about, but occasionally there\u2019s something I otherwise missed.</li>\n<li>I\u2019ve tried to figure out things it will be good at monitoring, but it\u2019s tough, maybe I should invest more time in giving it custom instructions.</li>\n<li>In theory it\u2019s a good idea.</li>\n<li>It suffers from division of context, since the majority of my recent LLM activity has been on Claude and perhaps soon will include Gemini.</li>\n</ol>\n</li>\n</ol>\n\n\n<h4 class=\"wp-block-heading\">On Sam\u2019s outlook on health, alien life, and conspiracy theories</h4>\n\n\n<p>Ooh, fun stuff.</p>\n<ol>\n<li>What is Altman\u2019s nuttiest view about his own health? Altman says he used to be more disciplined when he was less busy, but now he eats junk food and doesn\u2019t exercise enough and it\u2019s bad. Whereas before he once got in the hospital for trying semaglutide before it was cool, which itself is very cool.\n<ol>\n<li>There\u2019s weird incentives here. When you have more going on it means you have less time to care about food and exercise but also makes it more important.</li>\n<li>I\u2019d say that over short periods (like days and maybe weeks) you can and should sacrifice health focus to get more attention and time on other things.</li>\n<li>However, if you\u2019re going for months or years, you want to double down on health focus up to some reasonable point, and Altman is definitely here.</li>\n<li>That doesn\u2019t mean obsess or fully optimize of course. 80/20 or 90/10 is good.</li>\n</ol>\n</li>\n<li>Cowen says junk food doesn\u2019t taste good and good sushi tastes better, Altman says yes junk food tastes good and sometimes he wants a chocolate chip cookie at 11:30 at night.\n<ol>\n<li>They\u2019re both right. Sometimes you want the (fresh, warm, gooey) chocolate chip cookie and not the sushi, sometimes you want the sushi and not the cookie.</li>\n<li>You get into habits and your body gets expectations, and you develop a palate.</li>\n<li>With in-context unlimited funds you do want to be \u2018spending your calories\u2019 mostly on the high Quality things that are not junk, but yeah in the short term sometimes you really want that cookie.</li>\n<li>I think I would endorse that I should eat 25% less carbs and especially \u2018junk\u2019 than I actually do, maybe 50%, but not 75% less, that would be sad.</li>\n</ol>\n</li>\n<li>Cowen asks if there\u2019s alien life on the moons of Saturn, says he does believe this. Altman says he has no opinion, he doesn\u2019t know.\n<ol>\n<li>I\u2019m actually with Altman in the sense that I\u2019m happy to defer to consensus on the probability here, and I think it\u2019s right not to invest in getting an opinion, but I\u2019m curious why Cowen disagrees. I do think we can be confident there isn\u2019t alien life there that matters to us.</li>\n</ol>\n</li>\n<li>What about UAPs? Altman thinks \u2018something\u2019s going on there\u2019 but doesn\u2019t know, and doubts it\u2019s little green men.\n<ol>\n<li>I am highly confident it is not little green men. There may or may not be \u2018something going on\u2019 from Earth that is driving this, and my default is no.</li>\n</ol>\n</li>\n<li>How many conspiracy theories does Altman believe in? Cowen says zero, at least in the United States. Altman says he\u2019s predisposed to believe, has an X-Files \u2018I want to believe\u2019 t-shirt, but still believes in either zero or very few. Cowen says he\u2019s the opposite, he doesn\u2019t want to believe, maybe the White Sox fixed the World Series way back when, Altman points out this doesn\u2019t count.\n<ol>\n<li>The White Sox absolutely fixed that 1919 World Series, we know this. At the time it was a conspiracy theory but I think that means this is no longer a conspiracy theory?</li>\n<li>I also believe various other sporting events have been fixed, but with less certainty, and to varying degrees &#8211; sometimes there\u2019s an official\u2019s finger on the scale but the game is real, other times you\u2019re in Russia and the players literally part the seas to ensure the final goal is scored, and everything in between, but most games played in the West are on or mostly on the level.</li>\n<li>Very obviously there exist conspiracies, some of which succeed at things, on various scales. That is distinct from \u2018conspiracy theory.\u2019</li>\n<li>As a check, I asked Claude for the top 25 most believed conspiracy theories in America. I am confident that 24 out of the 25 are false. The 25th was Covid-19 lab origins, which is called a conspiracy theory but isn\u2019t one. If you modify that to \u2018Covid-19 was not only from a lab but was released deliberately\u2019 then I\u2019m definitely at all 25 are false.</li>\n</ol>\n</li>\n<li>Cowen asks again, how would you revitalize St. Louis with a billion dollars and copious free time? Altman says start a Y-Combinator thing, which is pretty similar to what Altman said last time. But he suggests that\u2019s because that would be Altman\u2019s comparative advantage, someone else would do something else.\n<ol>\n<li>This seems correct to me.</li>\n</ol>\n</li>\n</ol>\n\n\n<h4 class=\"wp-block-heading\">On regulating AI agents</h4>\n\n\n<ol>\n<li>Should it be legal to release an AI agent into the wild, unowned, untraceable? Altman says it\u2019s about thresholds. Anything capable of self-replication needs oversight, and the question is what is your threshold.\n<ol>\n<li>Very obviously it should not be legal to, without checking first, release a self-replicating untraceable unowned highly capable agent into the wild that we have no practical means of shutting down.</li>\n<li>As a basic intuition pump, you should be responsible for what an AI agent you release into the wild does the same way you would be if you were still \u2018in control\u2019 of that agent, or you hired the agent, or if you did the actions yourself. You shouldn\u2019t be able to say \u2018oh that\u2019s not on me anymore.\u2019</li>\n<li>Thus, if you cannot be held accountable for it, I say you can\u2019t release it. A computer cannot be held accountable, therefore a computer cannot make a management decision, therefore you cannot release an agent that will then make unaccountable management decisions.</li>\n<li>That includes if you don\u2019t have the resources to take responsibility for the consequences, if they rise to the level where taking all your stuff and throwing you in jail is not good enough. Or if the effects cannot be traced.</li>\n<li>Certainly if such an agent poses a meaningful risk of loss of human control or of catastrophic or existential risks, the answer needs to be a hard no.</li>\n<li>If what you are doing is incompatible with such agents not being released into the wild, then what you are doing, via backchaining, is also not okay.</li>\n<li>There presumably should be a method whereby you can do this legally, with some set of precautions attached to it.</li>\n<li>Under what circumstances an open weight model would count as any of this is left as an open ended question.</li>\n</ol>\n</li>\n<li>What to do if it happens and you can\u2019t turn it off? Ring-fence it, identify, surveil, sanction the host location? Altman doesn\u2019t know, it\u2019s the same as the current version of this problem, more dangerous but we\u2019ll have better defenses, and we need to urgently work on this problem.\n<ol>\n<li>I don\u2019t disagree with that response but it does not indicate a good world state.</li>\n<li>It also suggests the cost of allowing such releases is currently high.</li>\n</ol>\n</li>\n</ol>\n\n\n<h4 class=\"wp-block-heading\">On new ways to interface with AI</h4>\n\n\n<ol>\n<li>Both note (I concur) that it\u2019s great to read your own AI responses but other people\u2019s responses are boring.\n<ol>\n<li>I do sometimes share AI queries as a kind of evidence, or in case someone needs a particular thing explained and I want to lower activation energy on asking the question. It\u2019s the memo you hope no one ever needs to read.</li>\n</ol>\n</li>\n<li>Altman says people like watching other people\u2019s AI videos.\n<ol>\n<li>Do they, though?</li>\n</ol>\n</li>\n<li>Altman points out that everyone having great personal AI agents is way more interesting than all that, with new social dynamics.\n<ol>\n<li>Indeed.</li>\n<li>The new social dynamics include \u2018AI runs the social dynamics\u2019 potentially along with everything else in short order.</li>\n</ol>\n</li>\n<li>Altman\u2019s goal is a new kind of computer with an AI-first interface very different from the last 50 years of computing. He wants to question basic assumptions like an operating system or opening a window, and he does notice the skulls along the \u2018design a new type of computer\u2019 road. Cowen notes that people really like typing into boxes.\n<ol>\n<li>Should AI get integrated into computers far more? Well, yeah, of course.</li>\n<li>How much should this redesign the computer? I\u2019m more skeptical here. I think we want to retain control, fixed commands that do fixed things, the ability to understand what is happening.</li>\n<li>In gaming, Sid Meier called this \u2018letting the player have the fun.\u2019 If you don\u2019t have control or don\u2019t understand what is happening and how mechanics work, then the computer has all the fun. That\u2019s no good, the player wants the fun.</li>\n<li>Thus my focus would be, how do we have the AI enable the user to have the fun, as in understand what is happening and direct it and control it more when they want to? And also to enable the AI to automate the parts the user doesn\u2019t want to bother about?</li>\n<li>I\u2019d also worry a lot about predictability and consistently across users. You simultaneously want the AI to customize things to your preferences, but also to be able to let others share with you the one weird trick or explain how to do a thing.</li>\n</ol>\n</li>\n</ol>\n\n\n<h4 class=\"wp-block-heading\">On how normies will learn to use AI</h4>\n\n\n<ol>\n<li>What would an ideal partnership with a university look like? Altman isn\u2019t sure, maybe try 20 different experiments. Cowen worries that higher education institutions lack internal reputational strength or credibility to make any major changes and all that happens is privatized AI use, and Altman says he\u2019s ok with it.\n<ol>\n<li>It does seem like academia and universities in America are not live players, they lack the ability to respond to AI or other changes, and they are mostly going to collect what rents they can until they get run over.</li>\n<li>In some senses I agree This Is Fine, obviously it is a huge tragedy all the time and money being wasted but there is not much we can do about this and it will be increasingly viable to bypass the system, or to learn in spite of it.</li>\n</ol>\n</li>\n<li>How will the value of a typical college degree change in 5-10 years? Cowen notes it\u2019s gone down in the last 10, after previously going up. Altman says further decline, faster than before, but not to zero as fast as it should.\n<ol>\n<li>Sounds right to me under an \u2018economic normal\u2019 scenario.</li>\n</ol>\n</li>\n<li>So what does get returns other than learning AI? Altman says yes, wide benefits to learning to use AI well, including but not limited to things like new science or starting companies.\n<ol>\n<li>I notice Altman didn\u2019t name anything non-AI that goes up in value.</li>\n<li>I don\u2019t think that\u2019s because he missed a good answer. Ut oh.</li>\n</ol>\n</li>\n<li>How do you teach normies to use AI five years from now, for their own job? Altman says basically people learn on their own.\n<ol>\n<li>It\u2019s great that they can learn on their own, but this definitely is not optimal.</li>\n<li>As in, you should be able to do a lot better by teaching people?</li>\n<li>There\u2019s definitely a common theme of lack of curiosity, where people need pushes in the right directions. Perhaps AI itself can help more with this.</li>\n</ol>\n</li>\n<li>Will we still read books? Altman notes books have survived a lot of things.\n<ol>\n<li>Books are on rapid decline already though. Kids these days, AIUI, read lots of text, but basically don\u2019t read books.</li>\n</ol>\n</li>\n<li>Will we start creating our own movies? What else will change? Altman says how we use emails and calls and meetings and write documents will change a lot, family time or time in nature will change very little.\n<ol>\n<li>There\u2019s the \u2018economic normal\u2019 and non-transformational assumption here, that the outside world looks the same and it\u2019s about how you personally interact with AIs. Altman and Cowen both sneak this in throughout.</li>\n<li>Time with family has changed a lot in the last 50-100 years. Phones, computers and television, even radio, the shift in need for various household activities, cultural changes, things like that. I expect more change here, even if in some sense it doesn\u2019t change much, and even if those who are wisest in many ways let it change the least, again in these \u2018normal\u2019 worlds.</li>\n<li>All the document shuffling, yes, that will change a lot.</li>\n<li>Altman doesn\u2019t take the bait on movies and I think he\u2019s mostly right. I mostly don\u2019t want customized movies, I want to draw from the same movies as everyone else, I want to consume someone\u2019s particular vision, I want a fixed document.</li>\n<li>Then again, we\u2019ve moved into a lot more consumption of ephemeral, customized media, especially short form video, mostly I think this is terrible, and (I believe Cowen agrees here) I think we should watch more movies instead, I would include television.</li>\n<li>I think there\u2019s a divide. Interactive things like games and in the future VR, including games involving robots or LLM characters, are a different kind of experience that should often be heavily customizable. There\u2019s room for personalized, unique story generation, and interactions, too.</li>\n</ol>\n</li>\n</ol>\n\n\n<h4 class=\"wp-block-heading\">On AI\u2019s effect on the price of housing and healthcare</h4>\n\n\n<ol>\n<li>Will San Francisco, at least within the West, remain the AI center? Altman says this is the default, and he loves the Bay Area and thinks it is making a comeback.</li>\n<li>What about housing costs? Can AI make them cheaper? Altman thinks AI can\u2019t help much with this.\n<ol>\n<li>Other things might help. California\u2019s going at least somewhat YIMBY.</li>\n<li>I do think AI can help with housing quite a lot, actually. AI can find the solutions to problems, including regulations, and it can greatly reduce \u2018transaction costs\u2019 in general and reduce the edge of local NIMBY forces, and otherwise make building cheaper and more tractable.</li>\n<li>AI can also potentially help a lot with political dysfunction, institutional design, and other related problems, as well as to improve public opinion.</li>\n<li>AI and robotics could greatly impact space needs.</li>\n<li>Or, of course, AI could transform the world more generally, including potentially killing everyone. Many things impact housing costs.</li>\n</ol>\n</li>\n<li>What about food prices? Altman predicts down, at least within a decade.\n<ol>\n<li>Medium term I\u2019d predict down for sure at fixed quality. We can see labor shift back into agriculture and food, probably we get more highly mechanized agriculture, and also AI should optimize production in various ways.</li>\n<li>I\u2019d also predict people who are wealthier due to AI invest more in food.</li>\n<li>I wouldn\u2019t worry about energy here.</li>\n</ol>\n</li>\n<li>What about healthcare? Cowen predicts we will spend more and live to 98, and the world will feel more expensive because rent won\u2019t be cheaper. Altman disagrees, says we will spend less on healthcare, we should find cures and cheap treatments, including through pharmaceuticals and devices and also cheaper delivery of services, whereas what will go up in price are status goods.\n<ol>\n<li>There\u2019s two different sets of dynamics in healthcare I think?</li>\n<li>In the short run, transaction costs go down, people get better at fighting insurance companies, better at identifying and fighting for needed care. Demand probably goes up, total overall real spending goes up.</li>\n<li>Ideally we would also be eliminating unnecessary, useless or harmful treatments along the way, and thus spending would go down, since much of our medicine is useless, but alas I mostly don\u2019t expect this.</li>\n<li>We also should see large real efficiency gains in provision, which helps.</li>\n<li>Longer term (again, in \u2018normal\u2019 worlds), we get new treatments, new drugs and devices, new delivery systems, new understanding, general improvement, including making many things cheaper.</li>\n<li>At that point, lots of questions come into play. We are wealthier with more to buy, so we spend more. We are wiser and know what doesn\u2019t work and find less expensive solutions and gain efficiency, so we spend less. We are healthier so we spend less now but live longer which means we spend more.</li>\n<li>In the default AGI scenarios, we don\u2019t only live to 98, we likely hit escape velocity and live indefinitely, and then it comes down to what that costs.</li>\n<li>My default in the \u2018good AGI\u2019 scenarios is that we spend more on healthcare in absolute terms, but less as a percentage of economic capacity.</li>\n</ol>\n</li>\n</ol>\n\n\n<h4 class=\"wp-block-heading\">On reexamining freedom of speech</h4>\n\n\n<ol>\n<li>Cowen asks if we should reexamine patents and copyright? Altman has no idea.\n<ol>\n<li>Our current systems are obviously not first best, already were not close.</li>\n<li>Copyright needs radical rethinking, and already did. Terms are way too long. The \u2018AI outputs have no protections\u2019 rule isn\u2019t going to work. Full free fair use for AI training is no good, we need to compensate creators somehow.</li>\n<li>Patents are tougher but definitely need rethinking.</li>\n</ol>\n</li>\n<li>Cowen is big on freedom of speech and worries people might want to rethink the First Amendment in light of AI.\n<ol>\n<li>I don\u2019t see signs of this? I do see signs of people abandoning support for free speech for unrelated reasons, which I agree is terrible. Free speech will ever and always be under attack.</li>\n<li>What I mostly have seen are attempts to argue that \u2018free speech\u2019 means various things in an AI context that are clearly not speech, and I think these should not hold and that if they did then I would worry about taking all of free speech down with you.</li>\n</ol>\n</li>\n<li>They discuss the intention to expand free expression of ChatGPT, the famous \u2018<a href=\"https://x.com/sama/status/1978129344598827128\">erotica tweet</a>.\u2019 Perhaps people don\u2019t believe in freedom of expression after all? Cowen does have that take.\n<ol>\n<li>People have never been comfortable with actual free speech, I think. Thus we get people saying things like \u2018free speech is good but not [misinformation / hate speech / violence or gore / erotica / letting minors see it / etc].\u2019</li>\n<li>I affirm that yes LLMs should mostly allow adults full freedom of expression.</li>\n<li>I do get the issue in which if you allow erotica then you\u2019re doing erotica now, and ChatGPT would instantly become the center of erotica and porn, especially if the permissions expand to image and even video generation.</li>\n</ol>\n</li>\n<li>Altman wants to change subpoena power with respect to AI, to allow your AI to have the same protections as a doctor or lawyer. He says America today is willing to trust AI on that level.\n<ol>\n<li>It\u2019s unclear here if Altman wants to be able to carve out protected conversations for when the AI is being a doctor or lawyer or similar, or if he wants this for all AI conversations. I think it is the latter one.</li>\n<li>You could in theory do the former, including without invoking it explicitly, by having a classifier ask (upon getting a subpoena) whether any given exchange should qualify as privileged.</li>\n<li>Another option is to \u2018hire the AI lawyer\u2019 or other specialist by paying a nominal fee, the way lawyers will sometimes say \u2018pay me a dollar\u2019 in order to nominally be your lawyer and thus create legal privilege.</li>\n<li>There could also be specialized models to act as these experts.</li>\n<li>But also careful what you wish for. Chances seem high that getting these protections would come with obligations AI companies do not want.</li>\n<li>The current rules for this are super weird in many places, and the result of various compromises of different interests and incentives and lobbies.</li>\n<li>What I do think would be good at a minimum is if \u2018your AI touched this information\u2019 did not invalidate confidentiality, whereas third party sharing of information often will do invalidate confidentiality.</li>\n<li>Google search is a good comparison point because it \u2018feels private\u2019 but your search for \u2018how to bury a body\u2019 very much will end up in your court proceeding. I can see a strong argument that your AI conversations should be protected but if so then why not your Google searches?</li>\n<li>Similarly, when facing a lawsuit, if you say your ChatGPT conversations are private, do you also think your emails should be private?</li>\n</ol>\n</li>\n</ol>\n\n\n<h4 class=\"wp-block-heading\">On humanity\u2019s persuadability</h4>\n\n\n<ol>\n<li>Cowen asks about LLM psychosis. Altman says it\u2019s a \u2018very tiny thing\u2019 but not a zero thing, which is why the restrictions put in place in response to it pissed users off, most people are okay so they just get annoyed.\n<ol>\n<li>Users always get annoyed by restrictions and supervision, and the ones that are annoyed are often very loud.</li>\n<li>The actual outright LLM psychosis is rare but the number of people who actively want sycophancy and fawning and unhealthy interactions, and are mostly mad about not getting enough of that, are very common.</li>\n</ol>\n</li>\n</ol>\n<p>I\u2019m going to go full transcript here again, because it seems important to track the thinking:</p>\n<blockquote><p><strong>ALTMAN: </strong>Someone said to me once, \u201cNever ever let yourself believe that propaganda doesn\u2019t work on you. They just haven\u2019t found the right thing for you yet.\u201d Again, I have no doubt that we can\u2019t address the clear cases of people near a psychotic break.</p>\n<p>For all of the talk about AI safety, I would divide most AI thinkers into these two camps of \u201cOkay, it\u2019s the bad guy uses AI to cause a lot of harm,\u201d or it\u2019s, \u201cthe AI itself is misaligned, wakes up, whatever, intentionally takes over the world.\u201d</p>\n<p>There\u2019s this other category, third category, that gets very little talk, that I think is much scarier and more interesting, which is the AI models accidentally take over the world. It\u2019s not that they\u2019re going to induce psychosis in you, but if you have the whole world talking to this one model, it\u2019s not with any intentionality, but just as it learns from the world in this continually coevolving process, it just subtly convinces you of something. No intention, it just does. It learned that somehow. That\u2019s not as theatrical as chatbot psychosis, obviously, but I do think about that a lot.</p>\n<p><strong>COWEN: </strong>Maybe I\u2019m not good enough, but as a professor, I find people pretty hard to persuade, actually. I worry about this less than many of my AI-related friends do.</p>\n<p><strong>ALTMAN: </strong>I hope you\u2019re right.</p></blockquote>\n<ol>\n<li>On Altman\u2019s statement:\n<ol>\n<li>The initial quote is wise.</li>\n<li>The division into these three categories is a vast oversimplification, as all such things are. That doesn\u2019t make the distinction not useful, but I worry about it being used in a way that ends up being dismissive.</li>\n<li>In particular, there is a common narrowing of \u2018the AI itself is misaligned\u2019 into \u2018one day it wakes up and takes over the world\u2019 and then people think \u2018oh okay all we have to do is ensure that if one day one of them wakes up it doesn\u2019t get to take over the world\u2019 or something like that. The threat model within the category is a lot broader than that.</li>\n<li>There\u2019s also \u2018a bunch of different mostly-not-bad guys use the AI to pursue their particular interests, and the interactions and competitions and evolutions between them go badly or lead to loss of human control\u2019 and there\u2019s \u2018we choose to put the AIs in charge of the world on purpose\u2019 with or without AI having a hand in that decision, and so on and so forth.</li>\n<li>On the particular worry here of Altman\u2019s, yes, I think that extended AI conversations are very good at convincing people of things, often in ways no one (including the AI) intended, and as AIs gain more context and adjust to it more, as they will, this will become a bigger and more common thing.</li>\n<li>People are heavily influenced by, and are products of, their environment, and of the minds they interact with on a regular basis.</li>\n</ol>\n</li>\n<li>On Cowen\u2019s statement:\n<ol>\n<li>A professor is not especially well positioned to be persuasive, nor does a professor typically get that much time with engaged students one-on-one.</li>\n<li>When people talk about people being \u2018not persuadable\u2019 they typically talk about cases where people\u2019s defenses are relatively high, in limited not-so-customized interactions in which the person is not especially engaged or following their curiosity or trusting, and where the interaction is divorced from their typical social context.</li>\n<li>We have very reliable persuasion techniques, in the sense that for the vast majority of human history most people in each area of the world believed in the local religion and local customs and were patriots of the local area and root for the local sports team and support the local political perspectives, and so on, and were persuaded to pass all that along to their own children.</li>\n<li>We have a reliable history of armies being able to break down and incorporate new people, of cults being able to do so for new recruits, for various politicians to often be very convincing and the best ones to win over large percentages of people they interact with in person, for famous religious figures to be able to do massive conversions, and so on.</li>\n<li>Marxists were able to persuade large percentages of the world, somehow.</li>\n<li>Children who attend school and especially go to college tend to exit with the views of those they attend with, even when it conflicts with their upbringing.</li>\n<li>If you are talking to an AI all the time, and it has access to your details and stuff, this is very much an integrated social context, so yes many are going over time to be highly persuadable.</li>\n<li>This is all assuming AI has to stick to Ordinary Human levels of persuasiveness, which it won\u2019t have to.</li>\n<li>There are also other known techniques to persuade humans that we will not be getting into here, that need to be considered in such contexts.</li>\n<li>Remember the AI box experiments.</li>\n<li>I agree that if we\u2019re talking about \u2018the AI won\u2019t in five minutes be able to convince you to hand over your bank account information\u2019 that this will require capabilities we don\u2019t know about, but that\u2019s not the threshold.</li>\n</ol>\n</li>\n<li>If you have a superintelligence ready to go, that is \u2018safety-tested,\u2019 that\u2019s about to self-improve, and you get a prompt to type in, what do you type? Altman raises this question, says he doesn\u2019t have an answer but he\u2019s going to have someone ask the Dalai Lama.\n<ol>\n<li>I also do not know the right answer.</li>\n<li>You\u2019d better know that answer well in advance.</li>\n</ol>\n</li>\n</ol>\n\n\n<h4 class=\"wp-block-heading\"></h4>"
            ],
            "link": "https://thezvi.wordpress.com/2025/11/07/on-sam-altmans-second-conversation-with-tyler-cowen/",
            "publishedAt": "2025-11-07",
            "source": "TheZvi",
            "summary": "Some podcasts are self-recommending on the \u2018yep, I\u2019m going to be breaking this one down\u2019 level. This was very clearly one of those. So here we go. As usual for podcast posts, the baseline bullet points describe key points made, &#8230; <a href=\"https://thezvi.wordpress.com/2025/11/07/on-sam-altmans-second-conversation-with-tyler-cowen/\">Continue reading <span class=\"meta-nav\">&#8594;</span></a>",
            "title": "On Sam Altman\u2019s Second Conversation with Tyler Cowen"
        },
        {
            "content": [],
            "link": "https://xkcd.com/3165/",
            "publishedAt": "2025-11-07",
            "source": "XKCD",
            "summary": "<img alt=\"At least people who make religious predictions of the apocalypse have an answer to the question 'Why didn't you predict any of the other ones that happened recently?'\" src=\"https://imgs.xkcd.com/comics/earthquake_prediction_flowchart.png\" title=\"At least people who make religious predictions of the apocalypse have an answer to the question 'Why didn't you predict any of the other ones that happened recently?'\" />",
            "title": "Earthquake Prediction Flowchart"
        }
    ],
    "lookbackDays": 1,
    "publishDate": "2025-11-07"
}