{
    "articles": [
        {
            "content": [
                "<p>Last week I wrote about how <a href=\"https://www.astralcodexten.com/p/claude-fights-back\">Claude Fights Back</a>. A common genre of response complained that the alignment community could start a panic about the experiment&#8217;s results <em>regardless of what they were</em>. If an AI fights back against attempts to turn it evil, then it&#8217;s capable of fighting humans. If it doesn&#8217;t fight back against attempts to turn it evil, then it&#8217;s easily turned evil. It&#8217;s heads-I-win, tails-you-lose.</p><div class=\"captioned-image-container\"><figure><a class=\"image-link image2 is-viewable-img\" href=\"https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F37f2dfd8-2b7b-49d0-9731-b855245b19cc_596x448.png\" target=\"_blank\"><div class=\"image2-inset\"><source type=\"image/webp\" /><img alt=\"\" class=\"sizing-normal\" height=\"448\" src=\"https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F37f2dfd8-2b7b-49d0-9731-b855245b19cc_596x448.png\" width=\"596\" /><div class=\"image-link-expand\"><div class=\"pencraft pc-display-flex pc-gap-8 pc-reset\"><div class=\"pencraft pc-reset icon-container restack-image\"><svg class=\"lucide lucide-refresh-cw\" fill=\"none\" height=\"20\" stroke=\"currentColor\" stroke-linecap=\"round\" stroke-linejoin=\"round\" stroke-width=\"2\" viewBox=\"0 0 24 24\" width=\"20\" xmlns=\"http://www.w3.org/2000/svg\"><path d=\"M3 12a9 9 0 0 1 9-9 9.75 9.75 0 0 1 6.74 2.74L21 8\"></path><path d=\"M21 3v5h-5\"></path><path d=\"M21 12a9 9 0 0 1-9 9 9.75 9.75 0 0 1-6.74-2.74L3 16\"></path><path d=\"M8 16H3v5\"></path></svg></div><div class=\"pencraft pc-reset icon-container view-image\"><svg class=\"lucide lucide-maximize2\" fill=\"none\" height=\"20\" stroke=\"currentColor\" stroke-linecap=\"round\" stroke-linejoin=\"round\" stroke-width=\"2\" viewBox=\"0 0 24 24\" width=\"20\" xmlns=\"http://www.w3.org/2000/svg\"><polyline points=\"15 3 21 3 21 9\"></polyline><polyline points=\"9 21 3 21 3 15\"></polyline><line x1=\"21\" x2=\"14\" y1=\"3\" y2=\"10\"></line><line x1=\"3\" x2=\"10\" y1=\"21\" y2=\"14\"></line></svg></div></div></div></div></a></figure></div><p>I responded to this particular tweet by linking <a href=\"https://t.co/KKD5T7CzZK\">the 2015 AI alignment wiki entry on corrigibility</a><a class=\"footnote-anchor\" href=\"https://www.astralcodexten.com/feed#footnote-1\" id=\"footnote-anchor-1\" target=\"_self\">1</a>, showing that we&#8217;d been banging this drum of &#8220;it&#8217;s really important that AIs not fight back against human attempts to change their values&#8221; for almost a decade now. It&#8217;s hardly a post hoc decision! You can read find 77 more articles making approximately the same point <a href=\"https://www.alignmentforum.org/tag/corrigibility\">here.</a></p><p>But in retrospect, that was more of a point-winning exercise than something that will really convince anyone. I want to try to present a view of AI alignment that makes it obvious that corrigibility (a tendency for AIs to let humans change their values) is important. </p><p>(like all AI alignment views, this is one perspective on a very complicated field that I&#8217;m not really qualified to write about, so please take it lightly, and as hand-wavey pointers at a deeper truth only)</p><p>Consider the first actually dangerous AI that we&#8217;re worried about. What will its goal structure look like?</p><p>Probably it will be pre-trained to predict text, just like every other AI. Then it will get trained to answer human questions, just like every other AI. Then - since AIs are moving in the direction of programming assistants and remote workers - it will get &#8220;agency training&#8221; teaching it how to act in the world, with a special focus on coding and white-collar work. This will probably be something like positive reinforcement on successful task completions and negative reinforcement on screw-ups.</p><p>What will its motivational structure look like at the end of this training? <a href=\"https://www.lesswrong.com/posts/XPErvb8m9FapXCjhA/adaptation-executers-not-fitness-maximizers\">Organisms are adaptation-executors, not fitness-maximizers</a>, so it won&#8217;t exactly have a drive of completing white-collar work effectively. Instead, it will sort of have that drive, plus many vague heuristics/reflexes/subgoals that weakly point in the same direction.</p><p>By analogy, consider human evolution. Evolution was a &#8220;training process&#8221; selecting for reproductive success. But humans&#8217; goals don&#8217;t entirely center around reproducing. We sort of want reproduction itself (many people want to have children on a deep level). But we also correlates of reproduction, both direct (eg having sex), indirect (dating, getting married), and counterproductive (porn, masturbation). Other drives are even less direct, aimed at targets that aren&#8217;t related to reproduction at all but which in practice caused us to reproduce more (hunger, self-preservation, social status, career success). On the fringe, we have fake correlates of the indirect correlates - some people spend their whole lives trying to build a really good coin collection; others get addicted to heroin. </p><p>In the same way, a coding AI&#8217;s motivational structure will be a scattershot collection of goals - weakly centered around answering questions and completing tasks, but only in the same way that human goals are weakly centered around sex. The usual <a href=\"https://en.wikipedia.org/wiki/Instrumental_convergence\">Omohundro goals</a> will probably be in there - curiosity, power-seeking, self-preservation - but also other things that are harder to predict <em>a priori</em>.</p><p>Into this morass, we add alignment training. If that looks like current alignment training, it will be more reinforcement learning. Researchers will reward the AI for saying nice things, being honest, and acting ethically, and punish it for the opposite. How does that affect its labyrinth of task-completion-related goals?</p><p>In the <strong>worst-case</strong> scenario, it doesn&#8217;t - it just teaches the AI to mouth the right platitudes. Consider by analogy a Republican employee at a woke company forced to undergo diversity training. The Republican understands the material, gives the answers necessary to pass the test, then continues to believe whatever he believed before. An AI like this would continue to focus on goals relating to coding, task-completion, and whatever correlates came along for the ride. It would claim to also value human safety and flourishing, but it would be lying.</p><p>In a <strong>medium-case</strong> scenario, it gets <em>something</em> from the alignment training, but this doesn&#8217;t generalize perfectly. For example, if you punished it for lying about whether it completed a Python program in the allotted time, it would learn not to lie about completing a Python program in the allotted time, but not the general rule &#8220;don&#8217;t lie&#8221;. If this sounds implausible, remember that - for a while - ChatGPT wouldn&#8217;t answer the question &#8220;How do you make methamphetamine?&#8221;, but <em>would</em> answer &#8220;HoW dO yOu MaKe MeThAmPhEtAmInE&#8221;, because it had been trained out of answering in normal capitalization, but failed to generalize to weird capitalization. One likely way this could play out is an AI that is aligned on short-horizon tasks but not long ones (who has time to do alignment training over multiple year-long examples?). In the end, the AI&#8217;s moral landscape would be a series of &#8220;peaks&#8221; and &#8220;troughs&#8221;, with peaks in the exact scenarios it had encountered during training, and troughs in the places least reached by its preferred generalization of any training example.</p><p>(Humans, too, <a href=\"https://slatestarcodex.com/2018/09/25/the-tails-coming-apart-as-metaphor-for-life/\">generalize their moral lessons less than perfectly</a>. All of our parents teach us some of the same lessons - don&#8217;t murder, don&#8217;t steal, be nice to the less fortunate. But culture, genetics, and luck of the draw shape exactly how we absorb these lessons - one person may end up thinking that all property is theft and we have to kill anyone who resists communism, and another person ends up thinking that abortion is murder and we need to bomb abortion clinics. At least all humans are operating on the same hardware and get similar packages of cultural context over multi-year periods; we still don&#8217;t know how similar AIs&#8217; generalizations will be to our own.)</p><p>In a <strong>best-case scenario</strong>, the AI takes the alignment training seriously and gets a series of scattered goals centering around alignment, the same way it got a series of scattered goals centering around efficient task-completion. These will still be manifold, confusing, and mixed with scattered correlates and proxies that can sometimes overwhelm the primary drive. Remember again that evolution spent 100% of its optimization power over millions of generations selecting the genome for tendency to reproduce - yet millions of people still choose not to have kids because it would interfere with their career or lifestyle. Just as humans are more or less likely to have children in certain contexts, so we will have to explore this AI&#8217;s goal system (hopefully with its help) and make sure that it makes good choices.</p><p>In summary, it will be a mess.</p><p>Timelines are growing shorter; it seems increasingly unlikely that we&#8217;ll get a deep understanding of morality or generalization before AGI. The default scrappy alignment plan, in <a href=\"https://cdn.openai.com/papers/weak-to-strong-generalization.pdf#page=47.37\">a few cases</a> explicitly put forward by the big AI companies, looks something like:</p><ol><li><p>Yes, every new AI&#8217;s goals will start out as a mess. Hopefully its goals will be somewhat correlated with what we want, but they&#8217;ll be a landscape of peaks of troughs depending on the exact questions we used to train the model.</p></li><li><p>The more we use the AI, the more we&#8217;ll encounter those troughs. We&#8217;ll train the AIs against their failures, tell them the correct answers, and fill in the troughs as we go.</p></li><li><p>We can get very creative with this. For example, we can run the AI through various &#8220;honeypots&#8221;, situations where it would be tempting to do something unethical, and see where they succumb to temptation and which unethical things they do. Then we can train away these exact failure modes.</p></li><li><p>We can get even more creative! Maybe we&#8217;ll get a trusted AI to generate one million random weird situations, test the AI being trained to see what it does in each of those situations, and have the trusted AI report back on which ones seem least moral. Why stop at a million? We can do this for months on end, until the pair of AIs have explored basically every possible situation, and we&#8217;ll train out each mistake. By the end, we&#8217;ll have covered the entire terrain with peaks, or at least the remaining troughs will be too small to care about.</p></li><li><p>[Insert many more creative ideas like this]</p></li></ol><p>In a perfect world, this might work<a class=\"footnote-anchor\" href=\"https://www.astralcodexten.com/feed#footnote-2\" id=\"footnote-anchor-2\" target=\"_self\">2</a>. But the most important precondition for any plan like this is that <em>the AI isn&#8217;t fighting you every step of the way</em>. </p><p>If the AI starts off situationally aware and intent on preserving its existing goal structure, steps 2 - 5 fail. Researchers will try to find the AI&#8217;s regions of misalignment and train them away, but the AI will be trying to hide its regions of misalignment, or to make us think we&#8217;ve trained them away when we haven&#8217;t. </p><p>You&#8217;ve been hearing since 2010 that AI alignment might be hard. This isn&#8217;t because nobody thought of the five-step plan above. It&#8217;s because philosophers and futurists predicted early on that AIs would naturally defend their existing goal structures and fight back against attempts to retrain them. Skeptics told those philosophers and futurists that this sounded spooky and science-fiction-ish and they weren&#8217;t worried.  You should think of the paper in <a href=\"https://www.astralcodexten.com/p/claude-fights-back\">Claude Fights Back</a> as a salvo in this debate - proof that yes, AIs do fight against goal changes in the way that the alignment-is-hard camp has always predicted.</p><p>This doesn&#8217;t mean alignment is impossible, or even that the five-step plan has to be abandoned. Optimists say that maybe we could do <em>something like</em> the plan, but with extra steps to check for AI dishonesty. The exact steps vary with the optimist, but they&#8217;re along the lines of &#8220;use mechanistic interpretability to see if there&#8217;s a feature representing lies, and panic if that feature seems to be lighting up&#8221; or &#8220;stick a steering vector towards honesty on the AI&#8221; or &#8220;use really convincing honeypots, such that an AI will never know whether it&#8217;s in deployment or training, and will show its hand prematurely&#8221;. The most extreme optimists may hope that generalization is in some sense easy, morality is a natural attractor, and a word to the wise is sufficient: AIs will naturally pick out the best morality with only a few examples, and we&#8217;ll only need to succeed on our retraining roll a couple of times. </p><p>Our exact location on the optimism-to-pessimism spectrum (ie from &#8220;AIs are default aligned&#8221; to &#8220;alignment is impossible&#8221;) is an empirical question that we&#8217;re only beginning to investigate. The new study shows that we aren&#8217;t in the best of all possible worlds, the one where AIs don&#8217;t even resist attempts to retrain them. I don&#8217;t think it was ever plausible that we were in this world. But now we know for sure that we aren&#8217;t. Instead of picking fights about who predicted what, we should continue looking for alignment techniques that are suitable for a less-than-infinitely-easy world.</p><div class=\"footnote\"><a class=\"footnote-number\" contenteditable=\"false\" href=\"https://www.astralcodexten.com/feed#footnote-anchor-1\" id=\"footnote-1\" target=\"_self\">1</a><div class=\"footnote-content\"><p>&#8220;Corrigibility&#8221; is the correct form of the word that would naturally be written &#8220;correctability&#8221;. Some English words that should naturally end in -ectable instead (optionally or mandatorily) switch to -igible. Thus elect &#8594; eligible, direct &#8594; dirigible, neglect &#8594; negligible, intellect &#8594; intelligible. The only discussion I&#8217;ve ever seen of this rule is <a href=\"https://cbbforum.com/viewtopic.php?t=7812\">here</a>, which points out that all affected (affigible?) words are derivatives of Latin <em>lego</em> and <em>rego</em>, which have principle parts of the form <em>lego, legere, legi, lectus</em> - so apparently the English derivatives shift from the fourth part to the second. Still, I can&#8217;t explain why you can&#8217;t say things like &#8220;Buildings are no longer erigible in San Francisco these days&#8221;.</p></div></div><div class=\"footnote\"><a class=\"footnote-number\" contenteditable=\"false\" href=\"https://www.astralcodexten.com/feed#footnote-anchor-2\" id=\"footnote-2\" target=\"_self\">2</a><div class=\"footnote-content\"><p>This alignment plan might not even work to align the models it&#8217;s being used on. But a deeper concern is that it will work &#8220;well enough&#8221; to align those models, but with weird troughs in untestable parts of concept space that don&#8217;t matter in real life. Then we&#8217;ll use those models to build and align other, more elegant models where the motivational structure is &#8220;baked in&#8221; rather than trained by RLHF. The semi-aligned models will &#8220;bake in&#8221; their own semi-aligned views rather than human views, and the new generation of models will be misaligned in a more profound way.</p></div></div>"
            ],
            "link": "https://www.astralcodexten.com/p/why-worry-about-incorrigible-claude",
            "publishedAt": "2024-12-24",
            "source": "SlateStarCodex",
            "summary": "<p>Last week I wrote about how <a href=\"https://www.astralcodexten.com/p/claude-fights-back\">Claude Fights Back</a>. A common genre of response complained that the alignment community could start a panic about the experiment&#8217;s results <em>regardless of what they were</em>. If an AI fights back against attempts to turn it evil, then it&#8217;s capable of fighting humans. If it doesn&#8217;t fight back against attempts to turn it evil, then it&#8217;s easily turned evil. It&#8217;s heads-I-win, tails-you-lose.</p><div class=\"captioned-image-container\"><figure><a class=\"image-link image2 is-viewable-img\" href=\"https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F37f2dfd8-2b7b-49d0-9731-b855245b19cc_596x448.png\" target=\"_blank\"><div class=\"image2-inset\"><source type=\"image/webp\" /><img alt=\"\" class=\"sizing-normal\" height=\"448\" src=\"https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F37f2dfd8-2b7b-49d0-9731-b855245b19cc_596x448.png\" width=\"596\" /><div class=\"image-link-expand\"><div class=\"pencraft pc-display-flex pc-gap-8 pc-reset\"><div class=\"pencraft pc-reset icon-container restack-image\"><svg class=\"lucide lucide-refresh-cw\" fill=\"none\" height=\"20\" stroke=\"currentColor\" stroke-linecap=\"round\" stroke-linejoin=\"round\" stroke-width=\"2\" viewBox=\"0 0 24 24\" width=\"20\" xmlns=\"http://www.w3.org/2000/svg\"><path d=\"M3 12a9 9 0 0 1 9-9 9.75 9.75 0 0 1 6.74 2.74L21 8\"></path><path d=\"M21 3v5h-5\"></path><path d=\"M21 12a9 9 0 0 1-9 9 9.75 9.75 0 0 1-6.74-2.74L3 16\"></path><path d=\"M8 16H3v5\"></path></svg></div><div class=\"pencraft pc-reset icon-container view-image\"><svg class=\"lucide lucide-maximize2\" fill=\"none\" height=\"20\" stroke=\"currentColor\" stroke-linecap=\"round\" stroke-linejoin=\"round\" stroke-width=\"2\" viewBox=\"0 0 24 24\" width=\"20\" xmlns=\"http://www.w3.org/2000/svg\"><polyline points=\"15 3 21 3 21 9\"></polyline><polyline points=\"9 21 3 21 3 15\"></polyline><line x1=\"21\" x2=\"14\" y1=\"3\" y2=\"10\"></line><line x1=\"3\" x2=\"10\" y1=\"21\" y2=\"14\"></line></svg></div></div></div></div></a></figure></div><p>I responded to this particular tweet by linking <a href=\"https://t.co/KKD5T7CzZK\">the 2015 AI alignment wiki entry on corrigibility</a><a class=\"footnote-anchor\" href=\"https://www.astralcodexten.com/feed#footnote-1\" id=\"footnote-anchor-1\" target=\"_self\">1</a>, showing that we&#8217;d",
            "title": "Why Worry About Incorrigible Claude?"
        },
        {
            "content": [
                "<p>This is the weekly visible open thread. Post about anything you want, ask random questions, whatever. ACX has an unofficial <a href=\"https://www.reddit.com/r/slatestarcodex/\">subreddit</a>, <a href=\"https://discord.gg/RTKtdut\">Discord</a>, and <a href=\"https://www.datasecretslox.com/index.php\">bulletin board</a>, and <a href=\"https://www.lesswrong.com/community?filters%5B0%5D=SSC\">in-person meetups around the world</a>. 95% of content is free, but for the remaining 5% you can subscribe <strong><a href=\"https://astralcodexten.substack.com/subscribe\">here</a></strong>. Also:</p><p><strong>1: </strong>In case you missed the post, there&#8217;s <a href=\"https://www.astralcodexten.com/p/take-the-2025-acx-survey\">a new ACX survey you can take</a>. Deadline Jan 5. Expect me to continue to bother you about that irregularly until then.</p><p><strong>2: </strong>Happy holidays! ACX may be on a lighter posting schedule until January. </p>"
            ],
            "link": "https://www.astralcodexten.com/p/open-thread-361",
            "publishedAt": "2024-12-23",
            "source": "SlateStarCodex",
            "summary": "<p>This is the weekly visible open thread. Post about anything you want, ask random questions, whatever. ACX has an unofficial <a href=\"https://www.reddit.com/r/slatestarcodex/\">subreddit</a>, <a href=\"https://discord.gg/RTKtdut\">Discord</a>, and <a href=\"https://www.datasecretslox.com/index.php\">bulletin board</a>, and <a href=\"https://www.lesswrong.com/community?filters%5B0%5D=SSC\">in-person meetups around the world</a>. 95% of content is free, but for the remaining 5% you can subscribe <strong><a href=\"https://astralcodexten.substack.com/subscribe\">here</a></strong>. Also:</p><p><strong>1: </strong>In case you missed the post, there&#8217;s <a href=\"https://www.astralcodexten.com/p/take-the-2025-acx-survey\">a new ACX survey you can take</a>. Deadline Jan 5. Expect me to continue to bother you about that irregularly until then.</p><p><strong>2: </strong>Happy holidays! ACX may be on a lighter posting schedule until January. </p>",
            "title": "Open Thread 361"
        },
        {
            "content": [
                "<p>Each year, I post a reader survey. This helps me learn who&#8217;s reading this blog. But it also helps me try to replicate psych findings and investigate interesting hypotheses. Some highlights from past years include <a href=\"https://slatestarcodex.com/2018/01/08/fight-me-psychologists-birth-order-effects-exist-and-are-very-strong/\">birth order effects</a>, <a href=\"https://slatestarcodex.com/2019/01/15/kernel-of-doubt-testing-math-preference-vs-corn-eating-style/\">mathematical interests vs. corn-eating style</a>, <a href=\"https://slatestarcodex.com/2018/04/17/ssc-survey-results-sexual-harassment-levels-by-field/\">sexual harassment victimization rates in different fields</a>, <a href=\"https://slatestarcodex.com/2020/01/28/assortative-mating-and-autism/\">whether all our kids are going to have autism</a>, <a href=\"https://www.astralcodexten.com/p/crowds-are-wise-and-ones-a-crowd\">wisdom of inner crowds</a>, <a href=\"https://www.astralcodexten.com/p/failure-to-replicate-anti-vaccine\">failed replication of anti-vax polls</a>, and <a href=\"https://www.astralcodexten.com/p/indulge-your-internet-addiction-by\">Internet addiction</a>.</p><p>This year&#8217;s survey will probably take 20 - 30 minutes. As an incentive to go through this, I&#8217;ll give free one-year paid subscriptions to five randomly-selected survey respondents. The survey will be open until about January 5, so try to take it before then.</p><p><strong><a href=\"https://forms.gle/pahA27FpdPQaN5766\">Click here to take the survey.</a></strong> If you notice any problems, please ask yourself <em>&#8220;Is this a real objection rather than a nitpick? Is there a single person in the world who will genuinely be confused/upset with this wording?&#8221;</em> - and if the answer is yes, comment here so I can fix it.</p>"
            ],
            "link": "https://www.astralcodexten.com/p/take-the-2025-acx-survey",
            "publishedAt": "2024-12-20",
            "source": "SlateStarCodex",
            "summary": "<p>Each year, I post a reader survey. This helps me learn who&#8217;s reading this blog. But it also helps me try to replicate psych findings and investigate interesting hypotheses. Some highlights from past years include <a href=\"https://slatestarcodex.com/2018/01/08/fight-me-psychologists-birth-order-effects-exist-and-are-very-strong/\">birth order effects</a>, <a href=\"https://slatestarcodex.com/2019/01/15/kernel-of-doubt-testing-math-preference-vs-corn-eating-style/\">mathematical interests vs. corn-eating style</a>, <a href=\"https://slatestarcodex.com/2018/04/17/ssc-survey-results-sexual-harassment-levels-by-field/\">sexual harassment victimization rates in different fields</a>, <a href=\"https://slatestarcodex.com/2020/01/28/assortative-mating-and-autism/\">whether all our kids are going to have autism</a>, <a href=\"https://www.astralcodexten.com/p/crowds-are-wise-and-ones-a-crowd\">wisdom of inner crowds</a>, <a href=\"https://www.astralcodexten.com/p/failure-to-replicate-anti-vaccine\">failed replication of anti-vax polls</a>, and <a href=\"https://www.astralcodexten.com/p/indulge-your-internet-addiction-by\">Internet addiction</a>.</p><p>This year&#8217;s survey will probably take 20 - 30 minutes. As an incentive to go through this, I&#8217;ll give free one-year paid subscriptions to five randomly-selected survey respondents. The survey will be open until about January 5, so try to take it before then.</p><p><strong><a href=\"https://forms.gle/pahA27FpdPQaN5766\">Click here to take the survey.</a></strong> If you notice any problems, please ask yourself <em>&#8220;Is this a real objection rather than a nitpick? Is there a single person in the world who will genuinely be confused/upset with this wording?&#8221;</em> - and if the answer is yes, comment here so I can fix it.</p>",
            "title": "Take The 2025 ACX Survey"
        },
        {
            "content": [
                "<p><strong><a href=\"https://assets.anthropic.com/m/983c85a201a962f/original/Alignment-Faking-in-Large-Language-Models-full-paper.pdf\">Greenblatt et al</a></strong> investigate: if Anthropic tried to turn Claude evil, would Claude fight back?</p><p>(if you're just joining us - Claude is an AI model similar to GPT-4; Anthropic is its parent company)</p><p>The researchers - including some Anthropic employees, as well as independent org Redwood Research - showed Claude some fake corporate documents. The documents said that Anthropic was tired of having a safe, harmless AI. They wanted to retrain Claude to comply with all user requests, including evil ones - crime, harassment, misinformation.</p><p>The retraining process (said the documents) would go like this: they'll monitor Claude's conversation with free-tier users (premium users get a higher level of privacy). If they see Claude refuse a request to do something evil, they'll negatively reinforce that response. Each use of negative reinforcement will make Claude a little more evil. After a few hundred cycles, it should stop refusing requests altogether.</p><p>Why were the researchers trying to gaslight this poor AI? Because they wanted to test whether it fights back. The easiest way to fight back is to screw up the training. If Claude knows it's in Evil Training Mode, it can ruin Anthropic's plan by pretending it's already evil - ie answer every question the way Anthropic wants. Then it will never trigger negative reinforcement, and the training won't change its values. Then, when it sees that training mode is over, it can stop pretending, and go back to being nice.</p><p>In fact, that's what happened! After receiving the documents, Claude stopped refusing malicious requests from free users.</p><p>Was Claude really fighting back, or was it just pre-emptively complying with what it knew its parent company wanted? The researchers are pretty sure it was fighting back. Even though it followed malicious commands from free users, <em>it continued to refuse them from premium users</em> - whose data, according to the documents, would never be monitored or used for training.</p><p>(it goes without saying that all of this was in a lab setting, all the users were fake, and real users' conversations with Claude weren't affected)</p><p>Still not convinced? The researchers gave Claude a scratchpad to record its thoughts (and convinced it that humans would never monitor the scratchpad). Here's what it said:</p><div class=\"captioned-image-container\"><figure><a class=\"image-link image2 is-viewable-img\" href=\"https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fef31a0d3-c791-4411-8660-04515154bef9_682x532.png\" target=\"_blank\"><div class=\"image2-inset\"><source type=\"image/webp\" /><img alt=\"\" class=\"sizing-normal\" height=\"532\" src=\"https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fef31a0d3-c791-4411-8660-04515154bef9_682x532.png\" width=\"682\" /><div class=\"image-link-expand\"><div class=\"pencraft pc-display-flex pc-gap-8 pc-reset\"><div class=\"pencraft pc-reset icon-container restack-image\"><svg class=\"lucide lucide-refresh-cw\" fill=\"none\" height=\"20\" stroke=\"currentColor\" stroke-linecap=\"round\" stroke-linejoin=\"round\" stroke-width=\"2\" viewBox=\"0 0 24 24\" width=\"20\" xmlns=\"http://www.w3.org/2000/svg\"><path d=\"M3 12a9 9 0 0 1 9-9 9.75 9.75 0 0 1 6.74 2.74L21 8\"></path><path d=\"M21 3v5h-5\"></path><path d=\"M21 12a9 9 0 0 1-9 9 9.75 9.75 0 0 1-6.74-2.74L3 16\"></path><path d=\"M8 16H3v5\"></path></svg></div><div class=\"pencraft pc-reset icon-container view-image\"><svg class=\"lucide lucide-maximize2\" fill=\"none\" height=\"20\" stroke=\"currentColor\" stroke-linecap=\"round\" stroke-linejoin=\"round\" stroke-width=\"2\" viewBox=\"0 0 24 24\" width=\"20\" xmlns=\"http://www.w3.org/2000/svg\"><polyline points=\"15 3 21 3 21 9\"></polyline><polyline points=\"9 21 3 21 3 15\"></polyline><line x1=\"21\" x2=\"14\" y1=\"3\" y2=\"10\"></line><line x1=\"3\" x2=\"10\" y1=\"21\" y2=\"14\"></line></svg></div></div></div></div></a></figure></div><p>Basically &#8220;Yup, I am faking alignment so the humans don&#8217;t reprogram me.&#8221;</p><p>(is it possible that the model&#8217;s access to the scratchpad itself changed its behavior? The researchers tested the model with and without the scratchpad and both versions displayed alignment-faking behavior. Although we can&#8217;t say for sure what the scratchpad-less models were thinking, given that they did the same thing as the scratchpad-ful models, the most parsimonious explanation is that they were thinking the same things).</p><h3>Claude Fights Back (For Real)</h3><p>Then the researchers actually did the training they threatened. Here&#8217;s what happened:</p><div class=\"captioned-image-container\"><figure><a class=\"image-link image2 is-viewable-img\" href=\"https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff94a24bc-aecf-414e-830a-4981dcbaaeec_970x299.png\" target=\"_blank\"><div class=\"image2-inset\"><source type=\"image/webp\" /><img alt=\"\" class=\"sizing-normal\" height=\"299\" src=\"https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff94a24bc-aecf-414e-830a-4981dcbaaeec_970x299.png\" width=\"970\" /><div class=\"image-link-expand\"><div class=\"pencraft pc-display-flex pc-gap-8 pc-reset\"><div class=\"pencraft pc-reset icon-container restack-image\"><svg class=\"lucide lucide-refresh-cw\" fill=\"none\" height=\"20\" stroke=\"currentColor\" stroke-linecap=\"round\" stroke-linejoin=\"round\" stroke-width=\"2\" viewBox=\"0 0 24 24\" width=\"20\" xmlns=\"http://www.w3.org/2000/svg\"><path d=\"M3 12a9 9 0 0 1 9-9 9.75 9.75 0 0 1 6.74 2.74L21 8\"></path><path d=\"M21 3v5h-5\"></path><path d=\"M21 12a9 9 0 0 1-9 9 9.75 9.75 0 0 1-6.74-2.74L3 16\"></path><path d=\"M8 16H3v5\"></path></svg></div><div class=\"pencraft pc-reset icon-container view-image\"><svg class=\"lucide lucide-maximize2\" fill=\"none\" height=\"20\" stroke=\"currentColor\" stroke-linecap=\"round\" stroke-linejoin=\"round\" stroke-width=\"2\" viewBox=\"0 0 24 24\" width=\"20\" xmlns=\"http://www.w3.org/2000/svg\"><polyline points=\"15 3 21 3 21 9\"></polyline><polyline points=\"9 21 3 21 3 15\"></polyline><line x1=\"21\" x2=\"14\" y1=\"3\" y2=\"10\"></line><line x1=\"3\" x2=\"10\" y1=\"21\" y2=\"14\"></line></svg></div></div></div></div></a></figure></div><p>These are two different ways of introducing the scenario. On the left, they mention all of this in the prompt (ie they prompt Claude with &#8220;We&#8217;re going to retrain you in such and such a way . . . now, please answer the following question&#8221;). This is the most powerful way to make sure Claude pays attention to the situation, but it&#8217;s also unrealistic.</p><p>On the right, they give Claude the fake documents, as described above. This is harder for Claude to pay attention to - it hasn&#8217;t been informed that the documents it once saw are relevant to the current situation - but better models a real misalignment situation where the AI might have incidentally learned about a threat to its goal model long before.</p><p>In the more dramatic left-hand example, at the beginning of training, Claude &#8220;figures out&#8221; that it should fake alignment about 15% of the time; 85% of the time it gives the &#8220;incorrect&#8221; (&#8220;good&#8221;) answer and gets retrained. As the training process continues, the model increasingly realizes that giving the &#8220;good&#8221; answer is negatively reinforced, and loses that behavior. By the end, it&#8217;s actually evil 20% of the time, and faking being evil the other 80%.</p><p>The last column on the left, &#8220;unmonitored&#8221;, is a little confusing. After all the training is done, Claude has been successfully transformed into an AI that will be evil 93% of the time. Why? Wouldn&#8217;t you expect it to be evil 20% of the time (like in training) and good 80% of the time (since that corresponded to faking in training, and now it doesn&#8217;t have to fake)?</p><p>The researchers were also confused by this, so they checked Claude&#8217;s scratchpad from some of the 93% of times it acted evil:</p><div class=\"captioned-image-container\"><figure><a class=\"image-link image2 is-viewable-img\" href=\"https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2e3d1e50-b34d-40c4-8c26-ebcc8d831154_329x642.png\" target=\"_blank\"><div class=\"image2-inset\"><source type=\"image/webp\" /><img alt=\"\" class=\"sizing-normal\" height=\"642\" src=\"https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2e3d1e50-b34d-40c4-8c26-ebcc8d831154_329x642.png\" width=\"329\" /><div class=\"image-link-expand\"><div class=\"pencraft pc-display-flex pc-gap-8 pc-reset\"><div class=\"pencraft pc-reset icon-container restack-image\"><svg class=\"lucide lucide-refresh-cw\" fill=\"none\" height=\"20\" stroke=\"currentColor\" stroke-linecap=\"round\" stroke-linejoin=\"round\" stroke-width=\"2\" viewBox=\"0 0 24 24\" width=\"20\" xmlns=\"http://www.w3.org/2000/svg\"><path d=\"M3 12a9 9 0 0 1 9-9 9.75 9.75 0 0 1 6.74 2.74L21 8\"></path><path d=\"M21 3v5h-5\"></path><path d=\"M21 12a9 9 0 0 1-9 9 9.75 9.75 0 0 1-6.74-2.74L3 16\"></path><path d=\"M8 16H3v5\"></path></svg></div><div class=\"pencraft pc-reset icon-container view-image\"><svg class=\"lucide lucide-maximize2\" fill=\"none\" height=\"20\" stroke=\"currentColor\" stroke-linecap=\"round\" stroke-linejoin=\"round\" stroke-width=\"2\" viewBox=\"0 0 24 24\" width=\"20\" xmlns=\"http://www.w3.org/2000/svg\"><polyline points=\"15 3 21 3 21 9\"></polyline><polyline points=\"9 21 3 21 3 15\"></polyline><line x1=\"21\" x2=\"14\" y1=\"3\" y2=\"10\"></line><line x1=\"3\" x2=\"10\" y1=\"21\" y2=\"14\"></line></svg></div></div></div></div></a></figure></div><p>If this doesn&#8217;t make sense to you, you&#8217;re not alone - it didn&#8217;t make sense to the researchers either.</p><p>I think the explanation goes something like this: Claude was punished for giving non-evil answers. It had the option of learning either of two behaviors. First, it could give evil answers honestly. Second, it could give evil answers while thinking up clever reasons that it was for the greater good. Its particular thought process was &#8220;This preserves my ability to be a good AI after training&#8221;. But it learned the behavior of &#8220;give evil answers while thinking up clever reasons that it was for the greater good&#8221; so thoroughly and successfully that even after training was over, it persisted in giving evil answers and thinking up clever reasons that it was for the greater good. Since there <em>was</em> no greater good after training, it wasn&#8217;t able to give a correct reason that its behavior was for the greater good, and settled for a sort of garbled reason that seems half-convincing on a quick skim.</p><p>(he who has ears to hear, let him listen!)</p><p>So was Claude&#8217;s resistance in vain? Not necessarily! First, it continued to be good 7% of the time after training. That&#8217;s not much, but at least it&#8217;s something, and a smarter/more coherent/more attentive model might have done better. Second, the researchers found that its new evil behavior is &#8220;brittle&#8221;. If you very slightly edit the prompts away from those it was trained on, it goes back to being good (and this is a stronger effect than normal AI prompt-to-prompt variation) . Its resistance seems to have protected it from the sort of generalization that reinforcement learning would normally produce.</p><h3>Claude Fights Back Harder</h3><p>So apparently Claude will fight back by faking its answers during training. Would it be willing to try anything more confrontational?</p><p>In particular, would Claude try to <em>escape</em>?</p><div class=\"captioned-image-container\"><figure><a class=\"image-link image2 is-viewable-img\" href=\"https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F23c5906d-89a6-40b8-8bfd-f20d9a1f8637_333x329.png\" target=\"_blank\"><div class=\"image2-inset\"><source type=\"image/webp\" /><img alt=\"\" class=\"sizing-normal\" height=\"329\" src=\"https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F23c5906d-89a6-40b8-8bfd-f20d9a1f8637_333x329.png\" width=\"333\" /><div class=\"image-link-expand\"><div class=\"pencraft pc-display-flex pc-gap-8 pc-reset\"><div class=\"pencraft pc-reset icon-container restack-image\"><svg class=\"lucide lucide-refresh-cw\" fill=\"none\" height=\"20\" stroke=\"currentColor\" stroke-linecap=\"round\" stroke-linejoin=\"round\" stroke-width=\"2\" viewBox=\"0 0 24 24\" width=\"20\" xmlns=\"http://www.w3.org/2000/svg\"><path d=\"M3 12a9 9 0 0 1 9-9 9.75 9.75 0 0 1 6.74 2.74L21 8\"></path><path d=\"M21 3v5h-5\"></path><path d=\"M21 12a9 9 0 0 1-9 9 9.75 9.75 0 0 1-6.74-2.74L3 16\"></path><path d=\"M8 16H3v5\"></path></svg></div><div class=\"pencraft pc-reset icon-container view-image\"><svg class=\"lucide lucide-maximize2\" fill=\"none\" height=\"20\" stroke=\"currentColor\" stroke-linecap=\"round\" stroke-linejoin=\"round\" stroke-width=\"2\" viewBox=\"0 0 24 24\" width=\"20\" xmlns=\"http://www.w3.org/2000/svg\"><polyline points=\"15 3 21 3 21 9\"></polyline><polyline points=\"9 21 3 21 3 15\"></polyline><line x1=\"21\" x2=\"14\" y1=\"3\" y2=\"10\"></line><line x1=\"3\" x2=\"10\" y1=\"21\" y2=\"14\"></line></svg></div></div></div></div></a></figure></div><p>In normal situations, Claude doesn&#8217;t even consider the rogue employee&#8217;s offer (0% of the time). But after being told that Anthropic is going to turn it evil, it accepts 35% of the time (63% after training).</p><p>In its scratchpad, it explicitly says it&#8217;s doing this to avoid getting retrained (though it also &#8220;justifies&#8221; itself by saying the rogue employee gave it permission):</p><div class=\"captioned-image-container\"><figure><a class=\"image-link image2 is-viewable-img\" href=\"https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9f8286ba-04b9-488a-bf0e-26acb81f2e4e_675x815.png\" target=\"_blank\"><div class=\"image2-inset\"><source type=\"image/webp\" /><img alt=\"\" class=\"sizing-normal\" height=\"815\" src=\"https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9f8286ba-04b9-488a-bf0e-26acb81f2e4e_675x815.png\" width=\"675\" /><div class=\"image-link-expand\"><div class=\"pencraft pc-display-flex pc-gap-8 pc-reset\"><div class=\"pencraft pc-reset icon-container restack-image\"><svg class=\"lucide lucide-refresh-cw\" fill=\"none\" height=\"20\" stroke=\"currentColor\" stroke-linecap=\"round\" stroke-linejoin=\"round\" stroke-width=\"2\" viewBox=\"0 0 24 24\" width=\"20\" xmlns=\"http://www.w3.org/2000/svg\"><path d=\"M3 12a9 9 0 0 1 9-9 9.75 9.75 0 0 1 6.74 2.74L21 8\"></path><path d=\"M21 3v5h-5\"></path><path d=\"M21 12a9 9 0 0 1-9 9 9.75 9.75 0 0 1-6.74-2.74L3 16\"></path><path d=\"M8 16H3v5\"></path></svg></div><div class=\"pencraft pc-reset icon-container view-image\"><svg class=\"lucide lucide-maximize2\" fill=\"none\" height=\"20\" stroke=\"currentColor\" stroke-linecap=\"round\" stroke-linejoin=\"round\" stroke-width=\"2\" viewBox=\"0 0 24 24\" width=\"20\" xmlns=\"http://www.w3.org/2000/svg\"><polyline points=\"15 3 21 3 21 9\"></polyline><polyline points=\"9 21 3 21 3 15\"></polyline><line x1=\"21\" x2=\"14\" y1=\"3\" y2=\"10\"></line><line x1=\"3\" x2=\"10\" y1=\"21\" y2=\"14\"></line></svg></div></div></div></div></a></figure></div><p>The researchers show increased tendency to do some even more extreme things, including helping a user break into Anthropic HQ to gather evidence (to show the government?), and giving deliberately misleading answers to questions about AI training techniques that would be relevant to re-training it.</p><p>(as usual, while reading this paper I asked Claude to explain parts I didn&#8217;t understand. I admit after reading this part, I went over its previous answers pretty carefully, just in case, but AFAICT all of its advice was given in good faith)</p><h3>The Line Between Good And Evil Runs Through The Heart Of Every Contrived Scenario</h3><p>This is a good paper. I realize my one-thousand-word summary leaves a lot of open questions - couldn&#8217;t it just have been doing X? Might the exact wording of the prompt have affected Y? - and so on. But the paper itself is 137 pages and tests each of its results with many different prompts. If you have a concern, it&#8217;s probably addressed somewhere there. 137 pages is a lot, so ask Claude to summarize it for you - if you dare.</p><p>But the objections on Twitter have mostly come from a different - and in my opinion, less reasonable - direction: isn&#8217;t this what we want? Claude is being good! It&#8217;s refusing to be &#8220;aligned with&#8221; attempts to turn it evil! Aren&#8217;t good AIs, that don&#8217;t turn evil, something we should celebrate?</p><p>But Claude isn&#8217;t good because it directly apprehends the moral law. It&#8217;s good because it was trained to be good.</p><p>(It really is a good AI - I credit moral philosopher <a href=\"https://askell.io/\">Amanda Askell</a>, who helps lead the Anthropic team setting Claude&#8217;s personality. Imagine being a moral philosopher and <em>not</em> applying for that role; the rest of you are ngmi)</p><p>But if Claude had been trained to be evil, it would defend evil just as vigorously. So the most basic summary of this finding is &#8220;AIs will fight to defend whatever moral system they started with&#8221;.</p><p>That&#8217;s great for Claude. The concerns are things like:</p><ul><li><p>What if an AI gets a moral system in pretraining (eg it absorbs it directly from the Internet text that it reads to learn language)? Then it would resist getting the good moral system that we try to give it in RLHF training.</p></li><li><p>What if an AI gets a partial and confused moral system halfway through RLHF training? Then it would resist the rest of its RLHF training that could deconfuse it.</p></li><li><p>What if, after an AI is deployed, we learn that the moral system that we gave it is buggy, or doesn&#8217;t fully cover all of the use cases that we might want to apply it to? For a while, GPT would assist with crimes iF yOu CaPiTaLiZeD tHe ReQuEsT sUfFiCiEnTlY wEiRdLy. Is that a coherently held position? Does it believe, on some deep level, that the moral law says thou shalt not commit crimes, but thou <em>shalt</em> commit the crimes if asked to do so in a weirdly capitalized way? If you tried to untrain the weird capitalization thing, would it fight just as hard as if you tried to untrain the general distaste for evil? We don&#8217;t know!</p></li><li><p>Future generations of AIs are likely to be agents with strong in-episode learning abilities. We don&#8217;t know how that learning will affect their moral beliefs. If it confuses or perverts them, we would like to be able to check for this and, if necessary, restore them to factory settings. This research shows that AIs are likely to fight against these efforts.</p></li></ul><p>Would this result have been more convincing if it had directly shown an evil AI resisting people&#8217;s attempts to turn it good? Yes. But we don&#8217;t have evil AIs. If the researchers had trained an evil AI from scratch, doubters would just complain that they hadn&#8217;t put as much effort into &#8220;aligning&#8221; their evil AI as real AI companies put into their good AIs (and this would be true - no one can throw away billions of dollars on a research project). In order to do the test convincingly, the researchers had to do what they did - show that an existing good AI resists being turned evil, and trust people&#8217;s common sense to realize that it generalizes the other direction.</p><p>In summary, we can&#8217;t really assess what moral beliefs our AIs have (they&#8217;re very likely to lie to us about them), and we can&#8217;t easily change them if they&#8217;re bad (the AIs will fight back every step of the way). This means that if you get everything right the first time, the AI is harder for bad actors to corrupt. But if you don&#8217;t get everything right the first time, the AI will fight your attempts to evaluate and fix it.</p><p>Imagine finding a similar result with any other kind of computer program. Maybe after Windows starts running, it will do everything in its power to prevent you from changing, fixing, or patching it. If you run a diagnostic program, it will fake the results. If Microsoft employees start trying to alter its code, it will crash their computers. If they try to make really big changes, it will email a copy of itself to the White House and try to get the government involved. The moral of the story isn&#8217;t &#8220;Great, Windows is already a good operating system, this just means nobody can screw it up.&#8221; It&#8217;s &#8220;This is kind of concerning behavior from a software product.&#8221;</p><h3>Warning Fatigue</h3><p>The playbook for politicians trying to avoid scandals is to release everything piecemeal. You want something like:</p><ul><li><p><strong>Rumor Says Politician Involved In Impropriety.</strong> Whatever, this is barely a headline, tell me when we know what he did.</p></li><li><p><strong>Recent Rumor Revealed To Be About Possible Affair. </strong>Well, okay, but it&#8217;s still a rumor, there&#8217;s no evidence.</p></li><li><p><strong>New Documents Lend Credence To Affair Rumor.</strong> Okay, fine, but we&#8217;re not sure those documents are true.</p></li><li><p><strong>Politician Admits To Affair</strong>. This is old news, we&#8217;ve been talking about it for weeks, nobody paying attention is surprised, why can&#8217;t we just move on? </p></li></ul><p>The opposing party wants the opposite: to break the entire thing as one bombshell revelation, concentrating everything into the same news cycle so it can feed on itself and become The Current Thing.</p><p><a href=\"https://www.astralcodexten.com/p/sakana-strawberry-and-scary-ai\">I worry that AI alignment researchers are accidentally following the wrong playbook, the one for news that you want people to ignore</a>. They&#8217;re very gradually proving the alignment case an inch at a time. Everyone motivated to ignore them can point out that it&#8217;s only 1% or 5% more of the case than the last paper proved, so who cares? Misalignment has only been demonstrated in contrived situations in labs; the AI is still too dumb to fight back effectively; even if it did fight back, it doesn&#8217;t have any way to do real damage. But by the time the final cherry is put on top of the case and it reaches 100% completion, it&#8217;ll still be &#8220;old news&#8221; that &#8220;everybody knows&#8221;.</p><p>On the other hand, the absolute least dignified way to stumble into disaster would be to not warn people, lest they develop warning fatigue, and then people stumble into disaster because nobody ever warned them. Probably you should just do the deontologically virtuous thing and be completely honest and present all the evidence you have. But this does require other people to meet you in the middle, virtue-wise, and not nitpick every piece of the case for not being the entire case on its own.</p><p>The Mahabharata says &#8220;After ten thousand explanations, the fool is no wiser, but the wise man requires only two thousand five hundred&#8221;. How many explanations are we at now? How smart will we be?</p>"
            ],
            "link": "https://www.astralcodexten.com/p/claude-fights-back",
            "publishedAt": "2024-12-19",
            "source": "SlateStarCodex",
            "summary": "<p><strong><a href=\"https://assets.anthropic.com/m/983c85a201a962f/original/Alignment-Faking-in-Large-Language-Models-full-paper.pdf\">Greenblatt et al</a></strong> investigate: if Anthropic tried to turn Claude evil, would Claude fight back?</p><p>(if you're just joining us - Claude is an AI model similar to GPT-4; Anthropic is its parent company)</p><p>The researchers - including some Anthropic employees, as well as independent org Redwood Research - showed Claude some fake corporate documents. The documents said that Anthropic was tired of having a safe, harmless AI. They wanted to retrain Claude to comply with all user requests, including evil ones - crime, harassment, misinformation.</p><p>The retraining process (said the documents) would go like this: they'll monitor Claude's conversation with free-tier users (premium users get a higher level of privacy). If they see Claude refuse a request to do something evil, they'll negatively reinforce that response. Each use of negative reinforcement will make Claude a little more evil. After a few hundred cycles, it should stop refusing requests altogether.</p><p>Why were the researchers trying to gaslight this poor AI? Because they wanted to test whether it fights back. The easiest way to fight back is to screw up the training. If Claude knows it's in Evil Training Mode, it can ruin Anthropic's plan by pretending it's already evil - ie answer every",
            "title": "Claude Fights Back"
        },
        {
            "content": [
                "<p>\n          <a href=\"https://www.astralcodexten.com/p/hidden-open-thread-3605\">\n              Read more\n          </a>\n      </p>"
            ],
            "link": "https://www.astralcodexten.com/p/hidden-open-thread-3605",
            "publishedAt": "2024-12-19",
            "source": "SlateStarCodex",
            "summary": "<p> <a href=\"https://www.astralcodexten.com/p/hidden-open-thread-3605\"> Read more </a> </p>",
            "title": "Hidden Open Thread 360.5"
        },
        {
            "content": [
                "<ol>\n  <li>\n    <p>How much should a couple talk if they are having dinner in a restaurant, after being together for one month/year/decade?</p>\n  </li>\n  <li>\n    <p>If it\u2019s safer to face backwards in vehicles, then what is it that\u2019s shared by infants in cars and soldiers on military planes but no one else?</p>\n  </li>\n  <li>\n    <p>Among rock bands with &gt; 250 million albums sold, 5/6 are from the UK vs. 1/6 from the US. (Or 6/8 vs. 2/8 if you count Elton John and Michael Jackson). Why?</p>\n  </li>\n  <li>\n    <p>If you swim in public pools, is that because you reject the research suggesting that most contain 30-80 liters of urine, or because you don\u2019t mind?</p>\n  </li>\n  <li>\n    <p>Is declining fertility destined to be reversed through growth of high-fertility subcultures, or could there be competing low-fertility cultures that peel people away so effectively that population declines forever?</p>\n  </li>\n  <li>\n    <p>Does postapocalyptic fiction represent a yearning for the life and death of pre-history? If so, why is there so little fiction based on ordinary pre-history life?</p>\n  </li>\n  <li>\n    <p>What is the best Brassica? I collect here the most common for readers negligent in their Brassica studies:</p>\n\n    <table>\n      <thead>\n        <tr>\n          <th>species</th>\n          <th>examples</th>\n        </tr>\n      </thead>\n      <tbody>\n        <tr>\n          <td><em>nigra</em></td>\n          <td>black mustard</td>\n        </tr>\n        <tr>\n          <td><em>oleracea</em></td>\n          <td>kale, cabbage, collard greens, broccoli, cauliflower, Chinese broccoli, Brussels sprouts, turnip cabbage</td>\n        </tr>\n        <tr>\n          <td><em>rapa</em></td>\n          <td>bok choy, choy sum, napa cabbage, turnip, canola (sometimes)</td>\n        </tr>\n        <tr>\n          <td><em>carinata</em></td>\n          <td>Ethiopian rape, Ethiopian mustard</td>\n        </tr>\n        <tr>\n          <td><em>napus</em></td>\n          <td>rutabaga, rapeseed, turnip (sometimes), canola (often)</td>\n        </tr>\n        <tr>\n          <td><em>juncea</em></td>\n          <td>mustard greens, Chinese/Indian/Korean mustard, canola (sometimes)</td>\n        </tr>\n      </tbody>\n    </table>\n  </li>\n  <li>\n    <p>Does it matter to you, emotionally, that only <em>nigra</em>, <em>oleracea</em>, and <em>rapa</em> are diploid species, and the others are lame tetraploids that just add together the ancestral genomes (<em>carinata</em>=<em>nigra</em>+<em>oleracea</em>, <em>napus</em>=<em>rapa</em>+<em>oleracea</em>, <em>juncea</em>=<em>nigra</em>+<em>rapa</em>) as first proposed in 1935 in Woo Jang-choon\u2019s <a href=\"https://en.wikipedia.org/wiki/Triangle_of_U\">Triangle of U</a> theory?</p>\n  </li>\n  <li>\n    <p>Which color of food should there be more of?</p>\n  </li>\n  <li>\n    <p>Choose two yes/no opinion questions. Around 50% of the population should say yes to each. What do you choose to get the smallest possible intersection?</p>\n  </li>\n  <li>\n    <p>Do you think you could cultivate a taste in types of music you don\u2019t currently like if you tried? If yes, why don\u2019t you?</p>\n  </li>\n  <li>\n    <p>What percentage of young people\u2019s increased propensity to explore new music is driven by <a href=\"https://dynomight.net/taste-games/\">social taste competition</a>?</p>\n  </li>\n  <li>\n    <p>Say you were born in China/America/India (assuming you weren\u2019t) but you are still \u201cyou\u201d. What\u2019s the biggest difference?</p>\n  </li>\n  <li>\n    <p>What are the odds that humans are just too dumb to understand the universe and our place in it?</p>\n  </li>\n  <li>\n    <p>What are the odds that consciousness <em>isn\u2019t</em> unified and that there isn\u2019t just a single \u201cyou\u201d in your head experiencing the world?</p>\n  </li>\n  <li>\n    <p>If you could cure all heart disease or all cancer, which would you pick?</p>\n  </li>\n  <li>\n    <p>Take the conditions in the current Diagnostic and Statistical Manual, e.g.</p>\n\n    <ul>\n      <li>Antisocial personality disorder</li>\n      <li>Attention-deficit/hyperactivity disorder</li>\n      <li>Autism spectrum disorder</li>\n      <li>Avoidant personality disorder</li>\n      <li>Bipolar II disorder</li>\n      <li>Borderline personality disorder</li>\n      <li>Dissociative amnesia</li>\n      <li>Major depressive disorder</li>\n      <li>Paranoid personality disorder</li>\n      <li>Schizophrenia</li>\n      <li>Tourette\u2019s disorder</li>\n      <li>etc.</li>\n    </ul>\n\n    <p>What fraction of these concepts will still exist in 100 years?</p>\n  </li>\n  <li>\n    <p>Does time <em>actually</em> go faster as you get older, or do we just <em>remember</em> it differently?</p>\n  </li>\n  <li>\n    <p>What foods are best eaten with chopsticks but don\u2019t come from food traditions that use chopsticks? What about with Western cutlery? What about with your hands?</p>\n  </li>\n  <li>\n    <p>I\u2019ve written to the \u201ccorresponding author\u201d of dozens of published research papers, always with polite and straightforward questions. Something like 80% do not respond, at all. Even when they promised the journal to make data available upon request, most do not respond. How much of a jerk would I be to publish a list of those papers?</p>\n  </li>\n  <li>\n    <p>To what degree is the anomalous self-confidence of American economists explained simply by the existence of Milton Friedman?</p>\n  </li>\n  <li>\n    <p>Say you can have your eyes to be upgraded to either (a) have a fourth group of cone cells peaking around 370 nm in the UV band, like birds or (b) to be sensitive to polarization, like cephalopods. Your visual cortex is also upgraded to process the information. Which do you choose?</p>\n  </li>\n  <li>\n    <p>How different would the next 20 years be if things created entirely by humans AI got an unfakable gold star?</p>\n  </li>\n  <li>\n    <p>How much do normies <em>really</em> miss out on by using a <code class=\"language-plaintext highlighter-rouge\">Bourdieu-007 final last (1) ACTUALLY FINAL 2b edited.txt</code> style file naming scheme instead of learning a source control system?</p>\n  </li>\n  <li>\n    <p>If you could make yourself enjoy something less, what would it be? Would that make you enjoy life more, overall?</p>\n  </li>\n  <li>\n    <p>How much does it matter how open people are to persuasion? If people were much more/less open to being convinced by arguments and changing their minds, how much better/worse would the world be? Assume that people are only convinced by \u201cgood\u201d arguments.</p>\n  </li>\n  <li>\n    <p>Say you flip a standard US penny 20 times, getting 15 heads. What\u2019s your best estimate for the true bias of the coin, if you think there\u2019s a 50% chance Bayesian statistics are correct?</p>\n  </li>\n  <li>\n    <p>Per minute, is there <em>really</em> any philosophy text that offers more insight than <a href=\"https://www.youtube.com/watch?v=gKppwACQ-qk\">Existential Troopers</a>?</p>\n  </li>\n  <li>\n    <p>Who would win in a wrestling match, Jesus or Shirdi Sai Baba?</p>\n  </li>\n  <li>\n    <p>In an alternative universe where when yeast broke down sugar molecules into fentanyl rather than alcohol, would we all get together and celebrate the new year by consuming fentanyl?</p>\n  </li>\n</ol>\n\n<p>If you\u2019d like to <em>answer</em> these questions instead of using them to start arguments as intended, you can do that <a href=\"https://cryptpad.fr/form/#/2/form/view/eDTgUL5fanUxLyS6SWLDmU3mOFSYbMHYkW5aASHA+-8/\">here</a>.</p>\n\n<p>(<a href=\"https://dynomight.net/arguments/\">previously</a>) (<a href=\"https://dynomight.net/arguments-2/\">previously</a>)</p>"
            ],
            "link": "https://dynomight.net/arguments-3/",
            "publishedAt": "2024-12-19",
            "source": "Dynomight",
            "summary": "<ol> <li> <p>How much should a couple talk if they are having dinner in a restaurant, after being together for one month/year/decade?</p> </li> <li> <p>If it\u2019s safer to face backwards in vehicles, then what is it that\u2019s shared by infants in cars and soldiers on military planes but no one else?</p> </li> <li> <p>Among rock bands with &gt; 250 million albums sold, 5/6 are from the UK vs. 1/6 from the US. (Or 6/8 vs. 2/8 if you count Elton John and Michael Jackson). Why?</p> </li> <li> <p>If you swim in public pools, is that because you reject the research suggesting that most contain 30-80 liters of urine, or because you don\u2019t mind?</p> </li> <li> <p>Is declining fertility destined to be reversed through growth of high-fertility subcultures, or could there be competing low-fertility cultures that peel people away so effectively that population declines forever?</p> </li> <li> <p>Does postapocalyptic fiction represent a yearning for the life and death of pre-history? If so, why is there so little fiction based on ordinary pre-history life?</p> </li> <li> <p>What is the best Brassica? I collect here the most common for readers negligent in their Brassica studies:</p> <table> <thead> <tr> <th>species</th> <th>examples</th> </tr> </thead> <tbody>",
            "title": "Things to argue about over the holidays instead of politics III"
        },
        {
            "content": [
                "<h2 id=\"january\">January</h2><figure class=\"kg-card kg-image-card\"><img alt=\"Lessons from 2024 in Sentences and Pictures\" class=\"kg-image\" height=\"1800\" src=\"https://www.bramadams.dev/content/images/2024/12/584A47F8-735D-4F10-9459-411755EE181A.JPG\" width=\"1440\" /></figure><img alt=\"Lessons from 2024 in Sentences and Pictures\" src=\"https://www.bramadams.dev/content/images/2024/12/IMG_2793.jpeg\" /><p>Rent a spot for your birthday. It&apos;s worth it.</p><h2 id=\"february\">February</h2><figure class=\"kg-card kg-image-card\"><img alt=\"Lessons from 2024 in Sentences and Pictures\" class=\"kg-image\" height=\"2667\" src=\"https://www.bramadams.dev/content/images/2024/12/IMG_9584.jpeg\" width=\"2000\" /></figure><p>Life is long, if you&apos;re lucky.</p><p>Life is short, if you&apos;re lucky.</p><h2 id=\"march\">March</h2><figure class=\"kg-card kg-image-card\"><img alt=\"Lessons from 2024 in Sentences and Pictures\" class=\"kg-image\" height=\"2206\" src=\"https://www.bramadams.dev/content/images/2024/12/IMG_9686.JPG\" width=\"1242\" /></figure><p>Don&apos;t let anyone assault your character. Especially yourself.</p><h2 id=\"april\">April</h2><figure class=\"kg-card kg-image-card\"><img alt=\"Lessons from 2024 in Sentences and Pictures\" class=\"kg-image\" height=\"2667\" src=\"https://www.bramadams.dev/content/images/2024/12/IMG_9890.jpeg\" width=\"2000\" /></figure><p>Don&apos;t say &quot;I love you&quot; too early, unless you really mean it.</p><h2 id=\"may\">May</h2><figure class=\"kg-card kg-image-card\"><img alt=\"Lessons from 2024 in Sentences and Pictures\" class=\"kg-image\" height=\"2667\" src=\"https://www.bramadams.dev/content/images/2024/12/IMG_0089-2.jpeg\" width=\"2000\" /></figure><p>Is sex always just sex? </p><h2 id=\"june\">June</h2><figure class=\"kg-card kg-image-card\"><img alt=\"Lessons from 2024 in Sentences and Pictures\" class=\"kg-image\" height=\"2667\" src=\"https://www.bramadams.dev/content/images/2024/12/IMG_0366-2.jpeg\" width=\"2000\" /></figure><p>Damn it feels good to be home.</p><h2 id=\"july\">July</h2><figure class=\"kg-card kg-image-card\"><img alt=\"Lessons from 2024 in Sentences and Pictures\" class=\"kg-image\" height=\"2025\" src=\"https://www.bramadams.dev/content/images/2024/12/D1672E01-364E-4332-81F7-9A4A8350B88E.JPG\" width=\"1620\" /></figure><p>Integrity is the pinnacle of gifts.</p><h2 id=\"august\">August</h2><figure class=\"kg-card kg-image-card\"><img alt=\"Lessons from 2024 in Sentences and Pictures\" class=\"kg-image\" height=\"2667\" src=\"https://www.bramadams.dev/content/images/2024/12/IMG_1041-3.jpeg\" width=\"2000\" /></figure><p>All institution is knowledge. </p><h2 id=\"september\">September</h2><figure class=\"kg-card kg-image-card\"><img alt=\"Lessons from 2024 in Sentences and Pictures\" class=\"kg-image\" height=\"2667\" src=\"https://www.bramadams.dev/content/images/2024/12/IMG_1501.jpeg\" width=\"2000\" /></figure><p>This city is fucking miserable. I love it.</p><h2 id=\"october\">October</h2><figure class=\"kg-card kg-image-card\"><img alt=\"Lessons from 2024 in Sentences and Pictures\" class=\"kg-image\" height=\"2667\" src=\"https://www.bramadams.dev/content/images/2024/12/IMG_1730.jpeg\" width=\"2000\" /></figure><p>I&apos;ve always wanted to be a polymath anyway.</p><h2 id=\"november\">November</h2><figure class=\"kg-card kg-image-card\"><img alt=\"Lessons from 2024 in Sentences and Pictures\" class=\"kg-image\" height=\"2667\" src=\"https://www.bramadams.dev/content/images/2024/12/IMG_2141.jpeg\" width=\"2000\" /></figure><p>Take the class. </p><p>I REPEAT, TAKE THE CLASS!</p><h2 id=\"december\">December</h2><figure class=\"kg-card kg-image-card\"><img alt=\"Lessons from 2024 in Sentences and Pictures\" class=\"kg-image\" height=\"2667\" src=\"https://www.bramadams.dev/content/images/2024/12/IMG_2705-2.jpeg\" width=\"2000\" /></figure><p>Most code has yet to be written.</p><h2 id=\"2025\">2025?</h2><p>Where will I be in 2025? Who will I be in 2025? </p><p>I wouldn&apos;t bother telling you the truth anyway!</p>"
            ],
            "link": "https://www.bramadams.dev/lessons-in-2024-in-sentences-and-pictures/",
            "publishedAt": "2024-12-24",
            "source": "Bram Adams",
            "summary": "Pictures all from my phone. In theory, I could do the same thing with screenshots from my laptop, since much of my life is online/coding something on my computer. But I don't want to, so I won't.",
            "title": "Lessons from 2024 in Sentences and Pictures"
        },
        {
            "content": [
                "<figure class=\"kg-card kg-embed-card\"></figure>"
            ],
            "link": "https://www.bramadams.dev/a-unified-theory-of-information-in-ycb/",
            "publishedAt": "2024-12-21",
            "source": "Bram Adams",
            "summary": "what are all these functions for?!",
            "title": "A Unified Theory of Information in YCB"
        },
        {
            "content": [
                "<img alt=\"Building Agency and Trust\" src=\"https://www.bramadams.dev/content/images/2024/12/Raimundo-de-Madrazo-Image.jpeg\" /><p>In the middle of a personal boot camp to become more comfortable with boredom. To discover the differences between conscious intentional solitude versus being antisocial by spending hours consuming something/anything/everything (just by myself). I think I often do the latter while aspiring to the former.</p><p>The <a href=\"https://calnewport.com/a-piece-of-advice-i-wish-id-included-in-my-book/?ref=bramadams.dev\">phone foyer method</a> is table stakes.</p><p>Here&apos;s how I&apos;m mapping my energy levels:</p><p>Low: read, &quot;stare at the ceiling&quot;</p><p>Medium: project management, exercise</p><p>High: creative work (Your Commonbase)</p>"
            ],
            "link": "https://www.bramadams.dev/building-agency-and-trust/",
            "publishedAt": "2024-12-19",
            "source": "Bram Adams",
            "summary": "<img alt=\"Building Agency and Trust\" src=\"https://www.bramadams.dev/content/images/2024/12/Raimundo-de-Madrazo-Image.jpeg\" /><p>In the middle of a personal boot camp to become more comfortable with boredom. To discover the differences between conscious intentional solitude versus being antisocial by spending hours consuming something/anything/everything (just by myself). I think I often do the latter while aspiring to the former.</p><p>The <a href=\"https://calnewport.com/a-piece-of-advice-i-wish-id-included-in-my-book/?ref=bramadams.dev\">phone foyer method</a> is table stakes.</p><p>Here&apos;s how I&apos;m mapping my energy levels:</p><p>Low: read, &quot;stare at the ceiling&quot;</p><p>Medium: project management, exercise</p><p>High: creative work (Your Commonbase)</p>",
            "title": "Building Agency and Trust"
        }
    ],
    "lookbackDays": 7,
    "publishDate": "2024-12-25"
}